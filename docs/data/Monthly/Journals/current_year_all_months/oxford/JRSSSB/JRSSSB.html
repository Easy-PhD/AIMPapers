<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRSSSB</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrsssb">JRSSSB - 59</h2>
<ul>
<li><details>
<summary>
(2025). Correction to: Convexity and measures of statistical association. <em>JRSSSB</em>, <em>87</em>(4), 1307. (<a href='https://doi.org/10.1093/jrsssb/qkaf040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf040},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1307},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Convexity and measures of statistical association},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Parameterizing and simulating from causal models. <em>JRSSSB</em>, <em>87</em>(4), 1306. (<a href='https://doi.org/10.1093/jrsssb/qkaf039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf039},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1306},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Parameterizing and simulating from causal models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Consistent and fast inference in compartmental models of epidemics using poisson approximate likelihoods. <em>JRSSSB</em>, <em>87</em>(4), 1305. (<a href='https://doi.org/10.1093/jrsssb/qkaf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf013},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1305},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Consistent and fast inference in compartmental models of epidemics using poisson approximate likelihoods},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convexity and measures of statistical association. <em>JRSSSB</em>, <em>87</em>(4), 1281-1304. (<a href='https://doi.org/10.1093/jrsssb/qkaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent investigations on the measures of statistical association highlight essential properties such as zero-independence (the measure is zero if and only if the random variables are independent), monotonicity under information refinement, and max-functionality (the measure of association is maximal if and only if we are in the presence of a deterministic (noiseless) dependence). An open question concerns the reasons why measures of statistical associations satisfy one or more of those properties but not others. We show that convexity plays a central role in all properties. Convexity plus a form of strictness (that we are to define) are necessary and sufficient for zero-independence, and convexity and strict convexity on Dirac masses are necessary and sufficient for max-functionality. We apply the findings to study the families of measures of statistical association based on Csiszár divergences, optimal transport, kernels, as well as Chatterjee’s new correlation coefficient. We further discuss the role of convexity in guaranteeing the asymptotic unbiasedness of given data estimators, prove a central limit theorem for those estimators under independence, and show the rate of convergence under arbitrary dependence. We demonstrate the findings with numerical simulations in a multivariate response context.},
  archive      = {J_JRSSSB},
  author       = {Borgonovo, Emanuele and Figalli, Alessio and Ghosal, Promit and Plischke, Elmar and Savaré, Giuseppe},
  doi          = {10.1093/jrsssb/qkaf018},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1281-1304},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Convexity and measures of statistical association},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-resolution subsampling for linear classification with massive data. <em>JRSSSB</em>, <em>87</em>(4), 1260-1280. (<a href='https://doi.org/10.1093/jrsssb/qkaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling is one of the popular methods to balance statistical efficiency and computational efficiency in the big data era. Most approaches aim to select informative or representative sample points to achieve good overall information of the full data. The present work takes the view that sampling techniques are recommended for the region we focus on and summary measures are enough to collect the information for the rest according to a well-designed data partitioning. We propose a subsampling strategy that collects global information described by summary measures and local information obtained from selected subsample points. Thus, we call it multi-resolution subsampling. We show that the proposed method leads to a more efficient subsample-based estimator for general linear classification problems. Some asymptotic properties of the proposed method are established and connections to existing subsampling procedures are explored. Finally, we illustrate the proposed subsampling strategy via simulated and real-world examples.},
  archive      = {J_JRSSSB},
  author       = {Chen, Haolin and Dette, Holger and Yu, Jun},
  doi          = {10.1093/jrsssb/qkaf017},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1260-1280},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Multi-resolution subsampling for linear classification with massive data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence on the focal: Conformal prediction with selection-conditional coverage. <em>JRSSSB</em>, <em>87</em>(4), 1239-1259. (<a href='https://doi.org/10.1093/jrsssb/qkaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction builds marginally valid prediction intervals that cover the unknown outcome of a randomly drawn test point with a prescribed probability. However, in practice, data-driven methods are often used to identify specific test unit(s) of interest, requiring uncertainty quantification tailored to these focal units. In such cases, marginally valid conformal prediction intervals may fail to provide valid coverage for the focal unit(s) due to selection bias. This article presents a general framework for constructing a prediction set with finite-sample exact coverage, conditional on the unit being selected by a given procedure. The general form of our method accommodates arbitrary selection rules that are invariant to the permutation of the calibration units and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We also work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization-based selection, selection based on conformal p -values, and selection based on properties of preliminary conformal prediction sets. The performance of our methods is demonstrated via applications in drug discovery and health risk prediction.},
  archive      = {J_JRSSSB},
  author       = {Jin, Ying and Ren, Zhimei},
  doi          = {10.1093/jrsssb/qkaf016},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1239-1259},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Confidence on the focal: Conformal prediction with selection-conditional coverage},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased and consistent nested sampling via sequential monte carlo. <em>JRSSSB</em>, <em>87</em>(4), 1221-1238. (<a href='https://doi.org/10.1093/jrsssb/qkaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new class of sequential Monte Carlo methods which reformulates the essence of the nested sampling (NS) method of Skilling in terms of sequential Monte Carlo techniques. Two new algorithms are proposed: nested sampling via sequential Monte Carlo (NS-SMC) and adaptive nested sampling via sequential Monte Carlo (ANS-SMC). The new framework allows convergence results to be obtained in the setting when Markov chain Monte Carlo (MCMC) is used to produce new samples. An additional benefit is that marginal-likelihood (normalizing constant) estimates given by NS-SMC are unbiased. In contrast to NS, the analysis of our proposed algorithms does not require the (unrealistic) assumption that the simulated samples be independent. We show that a minor adjustment to our ANS-SMC algorithm recovers the original NS algorithm, which provides insights as to why NS seems to produce accurate estimates despite a typical violation of its assumptions. A numerical study is conducted where the performance of the proposed algorithms and temperature-annealed SMC is compared on challenging problems. Code for the experiments is made available online at https://github.com/LeahPrice/SMC-NS .},
  archive      = {J_JRSSSB},
  author       = {Salomone, Robert and South, Leah F and Drovandi, Christopher and Kroese, Dirk P and Johansen, Adam M},
  doi          = {10.1093/jrsssb/qkaf015},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1221-1238},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Unbiased and consistent nested sampling via sequential monte carlo},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential monte carlo testing by betting. <em>JRSSSB</em>, <em>87</em>(4), 1200-1220. (<a href='https://doi.org/10.1093/jrsssb/qkaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a Monte Carlo test, the observed dataset is fixed, and several resampled or permuted versions of the dataset are generated in order to test a null hypothesis that the original dataset is exchangeable with the resampled/permuted ones. Sequential Monte Carlo tests aim to save computational resources by generating these additional datasets sequentially one by one and potentially stopping early. While earlier tests yield valid inference at a particular prespecified stopping rule, our work develops a new anytime-valid Monte Carlo test that can be continuously monitored, yielding a p -value or e -value at any stopping time possibly not specified in advance. It generalizes the well-known method by Besag and Clifford, allowing it to stop at any time, but also encompasses new sequential Monte Carlo tests that tend to stop sooner under the null and alternative without compromising power. The core technical advance is the development of new test martingales for testing exchangeability against a very particular alternative based on a testing by betting technique. The proposed betting strategies are guided by the derivation of a simple log-optimal betting strategy, have closed-form expressions for the wealth process, provable guarantees on resampling risk, and display excellent power in practice.},
  archive      = {J_JRSSSB},
  author       = {Fischer, Lasse and Ramdas, Aaditya},
  doi          = {10.1093/jrsssb/qkaf014},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1200-1220},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Sequential monte carlo testing by betting},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework for cutting feedback within modularized bayesian inference. <em>JRSSSB</em>, <em>87</em>(4), 1171-1199. (<a href='https://doi.org/10.1093/jrsssb/qkaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard Bayesian inference enables building models that combine information from various sources, but this inference may not be reliable if components of the model are misspecified. Cut inference, a particular type of modularized Bayesian inference, is an alternative that splits a model into modules and cuts the feedback from any suspect module. Previous studies have focused on a two module case, but a more general definition of a ‘module’ remains unclear. We present a formal definition of a ‘module’ and discuss its properties. We formulate methods for identifying modules; determining the order of modules; and building the cut distribution that should be used for cut inference within an arbitrary directed acyclic graph structure. We justify the cut distribution by showing that it not only cuts the feedback but also is the best approximation to the joint distribution satisfying this condition in Kullback–Leibler divergence. We also extend cut inference for the two module case to a general multiple-module case via a sequential splitting technique and demonstrate this via illustrative applications.},
  archive      = {J_JRSSSB},
  author       = {Liu, Yang and Goudie, Robert J B},
  doi          = {10.1093/jrsssb/qkaf012},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1171-1199},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {A general framework for cutting feedback within modularized bayesian inference},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strong oracle guarantees for partial penalized tests of high-dimensional generalized linear models. <em>JRSSSB</em>, <em>87</em>(4), 1150-1170. (<a href='https://doi.org/10.1093/jrsssb/qkaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial penalized tests provide flexible approaches to testing linear hypotheses in high-dimensional generalized linear models. However, because the estimators used in these tests are local minimizers of potentially nonconvex folded-concave penalized objectives, the solutions one computes in practice may not coincide with the unknown local minima for which we have nice theoretical guarantees. To close this gap between theory and computation, we introduce local linear approximation (LLA) algorithms to compute the full and reduced model estimators for these tests and develop a theory specifically for the LLA solutions. We prove that our LLA algorithms converge to oracle estimators for the full and reduced models in two steps with overwhelming probability. We then leverage this strong oracle result and the asymptotic properties of the oracle estimators to show that the partial penalized test statistics evaluated at the LLA solutions are approximately chi-square in large samples, giving us guarantees for the tests using specific computed solutions and thereby closing the theoretical gap. In simulations, we find that our LLA tests closely agree with the oracle tests and compare favourably with alternative high-dimensional inference procedures. We demonstrate the flexibility of our LLA tests with two high-dimensional data applications.},
  archive      = {J_JRSSSB},
  author       = {Jacobson, Tate},
  doi          = {10.1093/jrsssb/qkaf010},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1150-1170},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Strong oracle guarantees for partial penalized tests of high-dimensional generalized linear models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian penalized empirical likelihood and markov chain monte carlo sampling. <em>JRSSSB</em>, <em>87</em>(4), 1127-1149. (<a href='https://doi.org/10.1093/jrsssb/qkaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce a novel methodological framework called Bayesian penalized empirical likelihood (BPEL), designed to address the computational challenges inherent in empirical likelihood (EL) approaches. Our approach has two primary objectives: (i) to enhance the inherent flexibility of EL in accommodating diverse model conditions, and (ii) to facilitate the use of well-established Markov Chain Monte Carlo sampling schemes as a convenient alternative to the complex optimization typically required for statistical inference using EL. To achieve the first objective, we propose a penalized approach that regularizes the Lagrange multipliers, significantly reducing the dimensionality of the problem while accommodating a comprehensive set of model conditions. For the second objective, our study designs and thoroughly investigates two popular sampling schemes within the BPEL context. We demonstrate that the BPEL framework is highly flexible and efficient, enhancing the adaptability and practicality of EL methods. Our study highlights the practical advantages of using sampling techniques over traditional optimization methods for EL problems, showing rapid convergence to the global optima of posterior distributions and ensuring the effective resolution of complex statistical inference challenges.},
  archive      = {J_JRSSSB},
  author       = {Chang, Jinyuan and Tang, Cheng Yong and Zhu, Yuanzheng},
  doi          = {10.1093/jrsssb/qkaf009},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1127-1149},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Bayesian penalized empirical likelihood and markov chain monte carlo sampling},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal prediction with conditional guarantees. <em>JRSSSB</em>, <em>87</em>(4), 1100-1126. (<a href='https://doi.org/10.1093/jrsssb/qkaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of constructing distribution-free prediction sets with finite-sample conditional guarantees. Prior work has shown that it is impossible to provide exact conditional coverage universally in finite samples. Thus, most popular methods only guarantee marginal coverage over the covariates or are restricted to a limited set of conditional targets, e.g. coverage over a finite set of prespecified subgroups. This paper bridges this gap by defining a spectrum of problems that interpolate between marginal and conditional validity. We motivate these problems by reformulating conditional coverage as coverage over a class of covariate shifts. When the target class of shifts is finite-dimensional, we show how to simultaneously obtain exact finite-sample coverage over all possible shifts. For example, given a collection of subgroups, our prediction sets guarantee coverage over each group. For more flexible, infinite-dimensional classes where exact coverage is impossible, we provide a procedure for quantifying the coverage errors of our algorithm. Moreover, by tuning interpretable hyperparameters, we allow the practitioner to control the size of these errors across shifts of interest. Our methods can be incorporated into existing split conformal inference pipelines, and thus can be used to quantify the uncertainty of modern black-box algorithms without distributional assumptions.},
  archive      = {J_JRSSSB},
  author       = {Gibbs, Isaac and Cherian, John J and Candès, Emmanuel J},
  doi          = {10.1093/jrsssb/qkaf008},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1100-1126},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Conformal prediction with conditional guarantees},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A conditioning tactic that increases design sensitivity in observational block designs. <em>JRSSSB</em>, <em>87</em>(4), 1085-1099. (<a href='https://doi.org/10.1093/jrsssb/qkaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an observational block design, there are I blocks of J individuals, typically with one treated individual and J − 1 controls; however, unlike a randomized block design, individuals were not randomly assigned to treatment or control. To be convincing, an observational block design must demonstrate that an ostensible treatment effect is not actually a consequence of small or moderate unmeasured biases of treatment assignment in the absence of a treatment effect. It is known that weighting to ignore blocks with a small range of responses increases the ability to distinguish a treatment effect from a bias in treatment assignment—that is, it increases the design sensitivity. Here, it is shown that a new tactic further increases design sensitivity. The new tactic involves a conditional statistic, such that blocks with moderately large ranges are considered conditionally given that the treated individual has either the largest or smallest response in the block. The new tactic is explored: (i) in terms of an asymptotic measure, the design sensitivity, (ii) in simulation of the power of a sensitivity analysis in finite samples, and (iii) in an example. Adaptive inference is briefly discussed. An R package weightedRank implements the method, contains the data, and reproduces the empirical results.},
  archive      = {J_JRSSSB},
  author       = {Rosenbaum, Paul R},
  doi          = {10.1093/jrsssb/qkaf007},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1085-1099},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {A conditioning tactic that increases design sensitivity in observational block designs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive experiments toward learning treatment effect heterogeneity. <em>JRSSSB</em>, <em>87</em>(4), 1055-1084. (<a href='https://doi.org/10.1093/jrsssb/qkaf006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding treatment effect heterogeneity has become an increasingly popular task in various fields, as it helps design personalized advertisements in e-commerce or targeted treatment in biomedical studies. However, most of the existing work in this research area focused on either analysing observational data based on strong causal assumptions or conducting post hoc analyses of randomized controlled trial data, and there has been limited effort dedicated to the design of randomized experiments specifically for uncovering treatment effect heterogeneity. In the manuscript, we develop a framework for designing and analysing response adaptive experiments toward better learning treatment effect heterogeneity. Concretely, we provide response adaptive experimental design frameworks that sequentially revise the data collection mechanism according to the accrued evidence during the experiment. Such design strategies allow for the identification of subgroups with the largest treatment effects with enhanced statistical efficiency. The proposed frameworks not only unify adaptive enrichment designs and response-adaptive randomization designs but also complement A/B test designs in e-commerce and randomized trial designs in clinical settings. We demonstrate the merit of our design with theoretical justifications and in simulation studies with synthetic e-commerce and clinical trial data.},
  archive      = {J_JRSSSB},
  author       = {Wei, Waverly and Ma, Xinwei and Wang, Jingshen},
  doi          = {10.1093/jrsssb/qkaf006},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1055-1084},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Adaptive experiments toward learning treatment effect heterogeneity},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric posterior corrections. <em>JRSSSB</em>, <em>87</em>(4), 1025-1054. (<a href='https://doi.org/10.1093/jrsssb/qkaf005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach to semiparametric inference using corrected posterior distributions. The method allows us to leverage the adaptivity, regularization, and predictive power of nonparametric Bayesian procedures to estimate low-dimensional functionals of interest without being restricted by the holistic Bayesian formalism. Starting from a conventional posterior on the whole data-generating distribution, we correct the marginal posterior for each functional of interest with the help of the Bayesian bootstrap. We provide conditions for the resulting one-step posterior to possess calibrated frequentist properties and specialize the results for several canonical examples: the integrated squared density, the mean of a missing-at-random outcome, and the average causal treatment effect on the treated. The procedure is computationally attractive, requiring only a simple, efficient postprocessing step that can be attached onto any arbitrary posterior sampling algorithm. Using the ACIC 2016 causal data analysis competition, we illustrate that our approach can outperform the existing state-of-the-art through the propagation of Bayesian uncertainty.},
  archive      = {J_JRSSSB},
  author       = {Yiu, Andrew and Fong, Edwin and Holmes, Chris and Rousseau, Judith},
  doi          = {10.1093/jrsssb/qkaf005},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1025-1054},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Semiparametric posterior corrections},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized empirical likelihood test for ultra-high dimensional means under general covariances. <em>JRSSSB</em>, <em>87</em>(4), 1001-1024. (<a href='https://doi.org/10.1093/jrsssb/qkaf004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a calibrated empirical likelihood test for ultra-high dimensional means that incorporates multiple projections. Under weak moment conditions on the distributions of data, we analyse all possible asymptotic distributions of the proposed test statistic in different scenarios. To determine the critical value and enhance test power, we employ the random symmetrization method based on the group of sign flips and use multiple selected projections. The test can still maintain the significance level asymptotically, even in the presence of heterogeneity in the data distribution. Moreover, the proposed test procedure allows for general covariance structures and ultra-high dimensional regimes. Further, the power function reveals the relation with the projection term in an asymptotic sense such that we can select suitable projections to achieve good power in various scenarios. A quasi-Newton algorithm is introduced to reduce the computational cost arising from the intensive optimizations required for computing empirical likelihood. Numerical studies evidence the promising performance of the proposed test compared with existing tests.},
  archive      = {J_JRSSSB},
  author       = {Chen, Yuexin and Zhu, Lixing and Xu, Wangli},
  doi          = {10.1093/jrsssb/qkaf004},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {1001-1024},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Randomized empirical likelihood test for ultra-high dimensional means under general covariances},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmentation invariant manifold learning. <em>JRSSSB</em>, <em>87</em>(4), 978-1000. (<a href='https://doi.org/10.1093/jrsssb/qkaf003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is a widely used technique and an essential ingredient in the recent advance in self-supervised representation learning. By preserving the similarity between augmented data, the resulting data representation can improve various downstream analyses and achieve state-of-the-art performance in many applications. Despite the empirical effectiveness, most existing methods lack theoretical understanding under a general nonlinear setting. To fill this gap, we develop a statistical framework on a low-dimensional product manifold to model the data augmentation transformation. Under this framework, we introduce a new representation learning method called augmentation invariant manifold learning and design a computationally efficient algorithm by reformulating it as a stochastic optimization problem. Compared with existing self-supervised methods, the new method simultaneously exploits the manifold’s geometric structure and invariant property of augmented data and has an explicit theoretical guarantee. Our theoretical investigation characterizes the role of data augmentation in the proposed method and reveals why and how the data representation learned from augmented data can improve the k -nearest neighbour classifier in the downstream analysis, showing that a more complex data augmentation leads to more improvement in downstream analysis. Finally, numerical experiments on simulated and real data sets are presented to demonstrate the merit of the proposed method.},
  archive      = {J_JRSSSB},
  author       = {Wang, Shulei},
  doi          = {10.1093/jrsssb/qkaf003},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {978-1000},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Augmentation invariant manifold learning},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-phase rejective sampling and its asymptotic properties. <em>JRSSSB</em>, <em>87</em>(4), 957-977. (<a href='https://doi.org/10.1093/jrsssb/qkaf002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rejective sampling improves design and estimation efficiency of single-phase sampling when auxiliary information in a finite population is available. When such auxiliary information is unavailable, we propose to use two-phase rejective sampling (TPRS), which involves measuring auxiliary variables for the sample of units in the first phase, followed by the implementation of rejective sampling for the outcome in the second phase. We explore the asymptotic design properties of double expansion and regression estimators under TPRS. We show that TPRS enhances the efficiency of the double-expansion estimator, rendering it comparable to a regression estimator. We further refine the design to accommodate varying importance of covariates and extend it to multi-phase sampling. We start with the theory for the population mean and then extend the theory to parameters defined by general estimating equations. Our asymptotic results for TPRS immediately cover the existing single-phase rejective sampling, under which the asymptotic theory has not been fully established.},
  archive      = {J_JRSSSB},
  author       = {Yang, Shu and Ding, Peng},
  doi          = {10.1093/jrsssb/qkaf002},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {957-977},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Two-phase rejective sampling and its asymptotic properties},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analytic natural gradient updates for cholesky factor in gaussian variational approximation. <em>JRSSSB</em>, <em>87</em>(4), 930-956. (<a href='https://doi.org/10.1093/jrsssb/qkaf001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural gradients can improve convergence in stochastic variational inference significantly but inverting the Fisher information matrix is daunting in high dimensions. Moreover, in Gaussian variational approximation, natural gradient updates of the precision matrix do not ensure positive definiteness. To tackle this issue, we derive analytic natural gradient updates of the Cholesky factor of the covariance or precision matrix and consider sparsity constraints representing different posterior correlation structures. Stochastic normalized natural gradient ascent with momentum is proposed for implementation in generalized linear mixed models and deep neural networks.},
  archive      = {J_JRSSSB},
  author       = {Tan, Linda S L},
  doi          = {10.1093/jrsssb/qkaf001},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {930-956},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Analytic natural gradient updates for cholesky factor in gaussian variational approximation},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selecting informative conformal prediction sets with false coverage rate control. <em>JRSSSB</em>, <em>87</em>(4), 909-929. (<a href='https://doi.org/10.1093/jrsssb/qkae120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised learning, including regression and classification, conformal methods provide prediction sets for the outcome/label with finite sample coverage for any machine learning predictor. We consider here the case where such prediction sets come after a selection process. The selection process requires that the selected prediction sets be ‘informative’ in a well-defined sense. We consider both the classification and regression settings where the analyst may consider as informative only the sample with prediction sets small enough, excluding null values, or obeying other appropriate ‘monotone’ constraints. We develop a unified framework for building such informative conformal prediction sets while controlling the false coverage rate (FCR) on the selected sample. While conformal prediction sets after selection have been the focus of much recent literature in the field, the new introduced procedures, called InfoSP and InfoSCOP , are to our knowledge the first ones providing FCR control for informative prediction sets. We show the usefulness of our resulting procedures on real and simulated data.},
  archive      = {J_JRSSSB},
  author       = {Gazin, Ulysse and Heller, Ruth and Marandon, Ariane and Roquain, Etienne},
  doi          = {10.1093/jrsssb/qkae120},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {9},
  number       = {4},
  pages        = {909-929},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Selecting informative conformal prediction sets with false coverage rate control},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: X-vine models for multivariate extremes. <em>JRSSSB</em>, <em>87</em>(3), 908. (<a href='https://doi.org/10.1093/jrsssb/qkaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  doi          = {10.1093/jrsssb/qkaf011},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {908},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: X-vine models for multivariate extremes},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust evaluation of longitudinal surrogate markers with censored data. <em>JRSSSB</em>, <em>87</em>(3), 891-907. (<a href='https://doi.org/10.1093/jrsssb/qkae119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of statistical methods to evaluate surrogate markers is an active area of research. In many clinical settings, the surrogate marker is not simply a single measurement but is instead a longitudinal trajectory of measurements over time, e.g. fasting plasma glucose measured every 6 months for 3 years. In general, available methods developed for the single-surrogate setting cannot accommodate a longitudinal surrogate marker. Furthermore, many of the methods have not been developed for use with primary outcomes that are time-to-event outcomes and/or subject to censoring. In this paper, we propose robust methods to evaluate a longitudinal surrogate marker in a censored time-to-event outcome setting. Specifically, we propose a method to define and estimate the proportion of the treatment effect on a censored primary outcome that is explained by the treatment effect on a longitudinal surrogate marker measured up to time t 0 ⁠ . We accommodate both potential censoring of the primary outcome and of the surrogate marker. A simulation study demonstrates a good finite-sample performance of our proposed methods. We illustrate our procedures by examining repeated measures of fasting plasma glucose, a surrogate marker for diabetes diagnosis, using data from the diabetes prevention programme.},
  archive      = {J_JRSSSB},
  author       = {Agniel, Denis and Parast, Layla},
  doi          = {10.1093/jrsssb/qkae119},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {891-907},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robust evaluation of longitudinal surrogate markers with censored data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial effect detection regression for large-scale spatio-temporal covariates. <em>JRSSSB</em>, <em>87</em>(3), 872-890. (<a href='https://doi.org/10.1093/jrsssb/qkae118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a Spatial Effect Detection Regression (SEDR) model to capture the nonlinear and irregular effects of high-dimensional spatio-temporal predictors on a scalar outcome. Specifically, we assume that both the component and the coefficient functions in the SEDR are unknown smooth functions of location and time. This allows us to leverage spatially and temporally correlated information, transforming the curse of dimensionality into a blessing, as confirmed by our theoretical and numerical results. Moreover, we introduce a set of 0–1 regression coefficients to automatically identify the boundaries of the spatial effect, implemented via a novel penalty. A simple iterative algorithm, with explicit forms at each update step, is developed, and we demonstrate that it converges from the initial values given in the paper. Furthermore, we establish the convergence rate and selection consistency of the proposed estimator under various scenarios involving dimensionality and the effect space. Through simulation studies, we thoroughly evaluate the superior performance of our method in terms of bias and empirical efficiency. Finally, we apply the method to analyse and forecast data from environmental monitoring and Alzheimer’s Disease Neuroimaging Initiative study, revealing interesting findings and achieving smaller out-of-sample prediction errors compared to existing methods.},
  archive      = {J_JRSSSB},
  author       = {Zhang, Chenlin and Zhou, Ling and Guo, Bin and Lin, Huazhen},
  doi          = {10.1093/jrsssb/qkae118},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {872-890},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Spatial effect detection regression for large-scale spatio-temporal covariates},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction sets for high-dimensional mixture of experts models. <em>JRSSSB</em>, <em>87</em>(3), 850-871. (<a href='https://doi.org/10.1093/jrsssb/qkae117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large datasets make it possible to build predictive models that can capture heterogenous relationships between the response variable and features. The mixture of high-dimensional linear experts model posits that observations come from a mixture of high-dimensional linear regression models, where the mixture weights are themselves feature-dependent. In this article, we show how to construct valid prediction sets for an ℓ 1 -penalized mixture of experts model in the high-dimensional setting. We make use of a debiasing procedure to account for the bias induced by the penalization and propose a novel strategy for combining intervals to form a prediction set with coverage guarantees in the mixture setting. Synthetic examples and an application to the prediction of critical temperatures of superconducting materials show our method to have reliable practical performance.},
  archive      = {J_JRSSSB},
  author       = {Javanmard, Adel and Shao, Simeng and Bien, Jacob},
  doi          = {10.1093/jrsssb/qkae117},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {850-871},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Prediction sets for high-dimensional mixture of experts models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic modelling of sparse longitudinal data and functional snippets with stochastic differential equations. <em>JRSSSB</em>, <em>87</em>(3), 833-849. (<a href='https://doi.org/10.1093/jrsssb/qkae116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse functional/longitudinal data have attracted widespread interest due to the prevalence of such data in social and life sciences. A prominent scenario where such data are routinely encountered are accelerated longitudinal studies, where subjects are enrolled in the study at a random time and are only tracked for a short amount of time relative to the domain of interest. The statistical analysis of such functional snippets is challenging since information for far-off-diagonal regions of the covariance structure is missing. Our main methodological contribution is to address this challenge by bypassing covariance estimation and instead modelling the underlying process as the solution of a data-adaptive stochastic differential equation. Taking advantage of the interface between Gaussian functional data and stochastic differential equations makes it possible to efficiently reconstruct the target process by estimating its dynamic distribution. The proposed approach allows one to consistently recover forward sample paths from functional snippets at the subject level. We establish the existence and uniqueness of the solution to the proposed data-driven stochastic differential equation and derive rates of convergence for the corresponding estimators. The finite sample performance is demonstrated with simulation studies and functional snippets arising from a growth study and spinal bone mineral density data.},
  archive      = {J_JRSSSB},
  author       = {Zhou, Yidong and Müller, Hans-Georg},
  doi          = {10.1093/jrsssb/qkae116},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {833-849},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Dynamic modelling of sparse longitudinal data and functional snippets with stochastic differential equations},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moving beyond population variable importance: Concept, theory and applications of individual variable importance. <em>JRSSSB</em>, <em>87</em>(3), 816-832. (<a href='https://doi.org/10.1093/jrsssb/qkae115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a non-parametric regression setting, we introduce a novel concept of ‘individual variable importance’, which assesses the relevance of certain covariates to an outcome variable among individuals with specific characteristics. This concept holds practical importance for both risk assessment and association identification. For example, it can represent (i) the usefulness of expensive biomarkers in risk prediction for individuals at a specified baseline risk, or (ii) age-specific associations between physiological indicators. We quantify individual variable importance using a ratio parameter between two conditional mean squared errors. To estimate and infer this parameter, we develop fully non-parametric estimators and establish their asymptotic properties. Our method performs well in simulation studies. Applying our approach to analyse a real dataset reveals a scientifically interesting result: the association between body shape and systolic blood pressure diminishes with increasing age. Our finding aligns with the medical literature based on standard parametric regression techniques, but our approach is more reliable due to its robustness to model misspecification. More importantly, the fully non-parametric nature of our method allows it to be applied in settings with complex relationships between variables, which cannot be correctly characterized by traditional parametric interaction analyses.},
  archive      = {J_JRSSSB},
  author       = {Dai, Guorong and Shao, Lingxuan and Chen, Jinbo},
  doi          = {10.1093/jrsssb/qkae115},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {816-832},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Moving beyond population variable importance: Concept, theory and applications of individual variable importance},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive conformal classification with noisy labels. <em>JRSSSB</em>, <em>87</em>(3), 796-815. (<a href='https://doi.org/10.1093/jrsssb/qkae114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a conformal prediction method for classification tasks that can adapt to random label contamination in the calibration sample, often leading to more informative prediction sets with stronger coverage guarantees compared to existing approaches. This is obtained through a precise characterization of the coverage inflation (or deflation) suffered by standard conformal inferences in the presence of label contamination, which is then made actionable through a new calibration algorithm. Our solution can leverage different modelling assumptions about the contamination process, while requiring no knowledge of the underlying data distribution or of the inner workings of the classification model. The empirical performance of the proposed method is demonstrated through simulations and an application to object classification with the CIFAR-10H image data set.},
  archive      = {J_JRSSSB},
  author       = {Sesia, Matteo and Wang, Y X Rachel and Tong, Xin},
  doi          = {10.1093/jrsssb/qkae114},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {796-815},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Adaptive conformal classification with noisy labels},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable couplings for the random walk metropolis algorithm. <em>JRSSSB</em>, <em>87</em>(3), 772-795. (<a href='https://doi.org/10.1093/jrsssb/qkae113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a recent surge of interest in coupling methods for Markov chain Monte Carlo algorithms: they facilitate convergence quantification and unbiased estimation, while exploiting embarrassingly parallel computing capabilities. Motivated by these, we consider the design and analysis of couplings of the random walk Metropolis algorithm which scale well with the dimension of the target measure. Methodologically, we introduce a low-rank modification of the synchronous coupling that is provably optimally contractive in standard high-dimensional asymptotic regimes. We expose a shortcoming of the reflection coupling, the state of the art at the time of writing, and we propose a modification which mitigates the issue. Our analysis bridges the gap to the optimal scaling literature and builds a framework of asymptotic optimality which may be of independent interest. We illustrate the applicability of our proposed couplings, and the potential for extending our ideas, with various numerical experiments.},
  archive      = {J_JRSSSB},
  author       = {Papp, Tamás P and Sherlock, Chris},
  doi          = {10.1093/jrsssb/qkae113},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {772-795},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Scalable couplings for the random walk metropolis algorithm},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). α-separability and adjustable combination of amplitude and phase model for functional data. <em>JRSSSB</em>, <em>87</em>(3), 746-771. (<a href='https://doi.org/10.1093/jrsssb/qkae112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider separating and joint modelling amplitude and phase variations for functional data in an identifiable manner. To rigorously address this separability issue, we introduce the notion of α-separability upon constructing a family of α -indexed metrics. We bridge α -separability with the uniqueness of Fréchet mean, leading to the proposed adjustable combination of amplitude and phase model. The parameter α allows user-defined modelling emphasis between vertical and horizontal features and provides a novel viewpoint on the identifiability issue. We prove the consistency of the sample Fréchet mean and variance, and the proposed estimators. Our method is illustrated in simulations and COVID-19 infection rate data.},
  archive      = {J_JRSSSB},
  author       = {Wang, Tian and Ding, Jimin},
  doi          = {10.1093/jrsssb/qkae112},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {746-771},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {α-separability and adjustable combination of amplitude and phase model for functional data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust angle-based transfer learning in high dimensions. <em>JRSSSB</em>, <em>87</em>(3), 723-745. (<a href='https://doi.org/10.1093/jrsssb/qkae111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning improves target model performance by leveraging data from related source populations, especially when target data are scarce. This study addresses the challenge of training high-dimensional regression models with limited target data in the presence of heterogeneous source populations. We focus on a practical setting where only parameter estimates of pretrained source models are available, rather than individual-level source data. For a single source model, we propose a novel angle-based transfer learning (angleTL) method that leverages concordance between source and target model parameters. AngleTL adapts to the signal strength of the target model, unifies several benchmark methods, and mitigates negative transfer when between-population heterogeneity is large. We extend angleTL to incorporate multiple source models, accounting for varying levels of relevance among them. Our high-dimensional asymptotic analysis provides insights into when a source model benefits the target model and demonstrates the superiority of angleTL over other methods. Extensive simulations validate these findings and highlight the feasibility of applying angleTL to transfer genetic risk prediction models across multiple biobanks.},
  archive      = {J_JRSSSB},
  author       = {Gu, Tian and Han, Yi and Duan, Rui},
  doi          = {10.1093/jrsssb/qkae111},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {723-745},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robust angle-based transfer learning in high dimensions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequentist inference for semi-mechanistic epidemic models with interventions. <em>JRSSSB</em>, <em>87</em>(3), 701-722. (<a href='https://doi.org/10.1093/jrsssb/qkae110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effect of public health interventions on an epidemic are often estimated by adding the intervention to epidemic models. During the Covid-19 epidemic, numerous papers used such methods for making scenario predictions. The majority of these papers use Bayesian methods to estimate the parameters of the model. In this article, we show how to use frequentist methods for estimating these effects which avoids having to specify prior distributions. We also use model-free shrinkage methods to improve estimation when there are many different geographic regions. This allows us to borrow strength from different regions while still getting confidence intervals with correct coverage and without having to specify a hierarchical model. Throughout, we focus on a semi-mechanistic model which provides a simple, tractable alternative to compartmental methods.},
  archive      = {J_JRSSSB},
  author       = {Bong, Heejong and Ventura, Valérie and Wasserman, Larry},
  doi          = {10.1093/jrsssb/qkae110},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {701-722},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Frequentist inference for semi-mechanistic epidemic models with interventions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal mediation analysis: Selection with asymptotically valid inference. <em>JRSSSB</em>, <em>87</em>(3), 678-700. (<a href='https://doi.org/10.1093/jrsssb/qkae109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers are often interested in learning not only the effect of treatments on outcomes, but also the mechanisms that transmit these effects. A mediator is a variable that is affected by treatment and subsequently affects outcome. Existing methods for penalized mediation analyses may lead to ignoring important mediators and either assume that finite-dimensional linear models are sufficient to remove confounding bias, or perform no confounding control at all. In practice, these assumptions may not hold. We propose a method that considers the confounding functions as nuisance parameters to be estimated using data-adaptive methods. We then use a novel regularization method applied to this objective function to identify a set of important mediators. We consider natural direct and indirect effects as our target parameters. We then proceed to derive the asymptotic properties of our estimators and establish the oracle property under specific assumptions. Asymptotic results are also presented in a local setting, which contrast the proposal with the standard adaptive lasso. We also propose a perturbation bootstrap technique to provide asymptotically valid postselection inference for the mediated effects of interest. The performance of these methods will be discussed and demonstrated through simulation studies.},
  archive      = {J_JRSSSB},
  author       = {Jones, Jeremiah and Ertefaie, Ashkan and Strawderman, Robert L},
  doi          = {10.1093/jrsssb/qkae109},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {678-700},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Causal mediation analysis: Selection with asymptotically valid inference},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Engression: Extrapolation through the lens of distributional regression. <em>JRSSSB</em>, <em>87</em>(3), 653-677. (<a href='https://doi.org/10.1093/jrsssb/qkae108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributional regression aims to estimate the full conditional distribution of a target variable, given covariates. Popular methods include linear and tree ensemble based quantile regression. We propose a neural network-based distributional regression methodology called ‘engression’. An engression model is generative in the sense that we can sample from the fitted conditional distribution and is also suitable for high-dimensional outcomes. Furthermore, we find that modelling the conditional distribution on training data can constrain the fitted function outside of the training support, which offers a new perspective to the challenging extrapolation problem in nonlinear regression. In particular, for ‘preadditive noise’ models, where noise is added to the covariates before applying a nonlinear transformation, we show that engression can successfully perform extrapolation under some assumptions such as monotonicity, whereas traditional regression approaches such as least-squares or quantile regression fall short under the same assumptions. Our empirical results, from both simulated and real data, validate the effectiveness of the engression method. The software implementations of engression are available in both R and Python.},
  archive      = {J_JRSSSB},
  author       = {Shen, Xinwei and Meinshausen, Nicolai},
  doi          = {10.1093/jrsssb/qkae108},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {653-677},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Engression: Extrapolation through the lens of distributional regression},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness, model checking, and hierarchical models. <em>JRSSSB</em>, <em>87</em>(3), 632-652. (<a href='https://doi.org/10.1093/jrsssb/qkae107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model checking is essential to evaluate the adequacy of statistical models and the validity of inferences drawn from them. Particularly, hierarchical models such as latent Gaussian models (LGMs) pose unique challenges as it is difficult to check assumptions on the latent parameters. Diagnostic statistics are often used to quantify the degree to which a model fit deviates from the observed data. We construct diagnostic statistics by (a) defining an alternative model with relaxed assumptions and (b) deriving the diagnostic statistic most sensitive to discrepancies induced by this alternative model. We also promote a workflow for model criticism that combines model checking with subsequent robustness analysis. As a result, we obtain a general recipe to check assumptions in hierarchical models and the impact of these assumptions on the results. We demonstrate the ideas by assessing the latent Gaussianity assumption, a crucial but often overlooked assumption in LGMs. We illustrate the methods via examples utilizing Stan and provide functions for easy usage of the methods for general models fitted through R-INLA.},
  archive      = {J_JRSSSB},
  author       = {Cabral, Rafael and Bolin, David and Rue, Håvard},
  doi          = {10.1093/jrsssb/qkae107},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {632-652},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robustness, model checking, and hierarchical models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive functional principal components analysis. <em>JRSSSB</em>, <em>87</em>(3), 603-631. (<a href='https://doi.org/10.1093/jrsssb/qkae106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis almost always involves smoothing discrete observations into curves, because they are never observed in continuous time and rarely without error. Although smoothing parameters affect the subsequent inference, data-driven methods for selecting these parameters are not well-developed, frustrated by the difficulty of using all the information shared by curves while being computationally efficient. On the one hand, smoothing individual curves in an isolated, albeit sophisticated way, ignores useful signals present in other curves. On the other hand, bandwidth selection by automatic procedures such as cross-validation after pooling all the curves together quickly become computationally unfeasible due to the large number of data points. In this paper, we propose a new data-driven, adaptive kernel smoothing, specifically tailored for functional principal components analysis through the derivation of sharp, explicit risk bounds for the eigen-elements. The minimization of these quadratic risk bounds provides refined, yet computationally efficient bandwidth rules for each eigen-element separately. Both common and independent design cases are allowed. Rates of convergence for the estimators are derived. An extensive simulation study, designed in a versatile manner to closely mimic the characteristics of real data sets supports our methodological contribution. An illustration on a real data application is provided.},
  archive      = {J_JRSSSB},
  author       = {Wang, Sunny G W and Patilea, Valentin and Klutchnikoff, Nicolas},
  doi          = {10.1093/jrsssb/qkae106},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {603-631},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Adaptive functional principal components analysis},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). X-vine models for multivariate extremes. <em>JRSSSB</em>, <em>87</em>(3), 579-602. (<a href='https://doi.org/10.1093/jrsssb/qkae105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular vine sequences permit the organization of variables in a random vector along a sequence of trees. Vine-based dependence models have become greatly popular as a way to combine arbitrary bivariate copulas into higher-dimensional ones, offering flexibility, parsimony, and tractability. In this project, we use regular vine sequences to decompose and construct the exponent measure density of a multivariate extreme value distribution, or, equivalently, the tail copula density. Although these densities pose theoretical challenges due to their infinite mass, their homogeneity property offers simplifications. The theory sheds new light on existing parametric families and facilitates the construction of new ones, called X-vines. Computations proceed via recursive formulas in terms of bivariate model components. We develop simulation algorithms for X-vine multivariate Pareto distributions as well as methods for parameter estimation and model selection on the basis of threshold exceedances. The methods are illustrated by Monte Carlo experiments and a case study on US flight delay data.},
  archive      = {J_JRSSSB},
  author       = {Kiriliouk, Anna and Lee, Jeongjin and Segers, Johan},
  doi          = {10.1093/jrsssb/qkae105},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {7},
  number       = {3},
  pages        = {579-602},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {X-vine models for multivariate extremes},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal prediction with local weights: Randomization enables robust guarantees. <em>JRSSSB</em>, <em>87</em>(2), 549-578. (<a href='https://doi.org/10.1093/jrsssb/qkae103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider the problem of building distribution-free prediction intervals with finite-sample conditional coverage guarantees. Conformal prediction (CP) is an increasingly popular framework for building such intervals with distribution-free guarantees, but these guarantees only ensure marginal coverage: the probability of coverage is averaged over both the training and test data, meaning that there might be substantial undercoverage within certain subpopulations. Instead, ideally we would want to have local coverage guarantees that hold for each possible value of the test point’s features. While the impossibility of achieving pointwise local coverage is well established in the literature, many variants of conformal prediction algorithm show favourable local coverage properties empirically. Relaxing the definition of local coverage can allow for a theoretical understanding of this empirical phenomenon. We propose randomly localized conformal prediction (RLCP), a method that builds on localized CP and weighted CP techniques to return prediction intervals that are not only marginally valid but also offer relaxed local coverage guarantees and validity under covariate shift. Through a series of simulations and real data experiments, we validate these coverage guarantees of RLCP while comparing it with the other local conformal prediction methods.},
  archive      = {J_JRSSSB},
  author       = {Hore, Rohan and Barber, Rina Foygel},
  doi          = {10.1093/jrsssb/qkae103},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {549-578},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Conformal prediction with local weights: Randomization enables robust guarantees},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A focusing framework for testing bi-directional causal effects in mendelian randomization. <em>JRSSSB</em>, <em>87</em>(2), 529-548. (<a href='https://doi.org/10.1093/jrsssb/qkae101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) is a powerful method that uses genetic variants as instrumental variables to infer the causal effect of a modifiable exposure on an outcome. We study inference for bi-directional causal relationships and causal directions with possibly pleiotropic genetic variants. We show that assumptions for common MR methods are often impossible or too stringent given the potential bi-directional relationships. We propose a new focusing framework for testing bi-directional causal effects and it can be coupled with many state-of-the-art MR methods. We provide theoretical guarantees for our proposal and demonstrate its performance using several simulated and real datasets.},
  archive      = {J_JRSSSB},
  author       = {Li, Sai and Ye, Ting},
  doi          = {10.1093/jrsssb/qkae101},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {529-548},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {A focusing framework for testing bi-directional causal effects in mendelian randomization},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale scanning with nuisance parameters. <em>JRSSSB</em>, <em>87</em>(2), 510-528. (<a href='https://doi.org/10.1093/jrsssb/qkae100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a multiscale scanning method to find anomalies in a d -dimensional random field in the presence of nuisance parameters. This covers the common situation that either the baseline-level or additional parameters such as the variance are unknown and have to be estimated from the data. We argue that state of the art approaches to determine asymptotically correct critical values for multiscale scanning statistics will in general fail when such parameters are naively replaced by plug-in estimators. Instead, we suggest to estimate the nuisance parameters on the largest scale and to use (only) smaller scales for multiscale scanning. We prove a uniform invariance principle for the resulting adjusted multiscale statistic, which is widely applicable and provides a computationally feasible way to simulate asymptotically correct critical values. We illustrate the implications of our theoretical results in a simulation study and in a real data example from super-resolution STED microscopy. This allows us to identify interesting regions inside a specimen in a pre-scan with controlled family-wise error rate.},
  archive      = {J_JRSSSB},
  author       = {König, Claudia and Munk, Axel and Werner, Frank},
  doi          = {10.1093/jrsssb/qkae100},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {510-528},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Multiscale scanning with nuisance parameters},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the role of surrogates in the efficient estimation of treatment effects with limited outcome data. <em>JRSSSB</em>, <em>87</em>(2), 480-509. (<a href='https://doi.org/10.1093/jrsssb/qkae099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many experimental and observational studies, the outcome of interest is often difficult or expensive to observe, reducing effective sample sizes for estimating average treatment effects (ATEs) even when identifiable. We study how incorporating data on units for which only surrogate outcomes not of primary interest are observed can increase the precision of ATE estimation. We refrain from imposing stringent surrogacy conditions, which permit surrogates as perfect replacements for the target outcome. Instead, we supplement the available, albeit limited, observations of the target outcome with abundant observations of surrogate outcomes, without any assumptions beyond unconfounded treatment assignment and missingness and corresponding overlap conditions. To quantify the potential gains, we derive the difference in efficiency bounds on ATE estimation with and without surrogates, both when an overwhelming or comparable number of units have missing outcomes. We develop robust ATE estimation and inference methods that realize these efficiency gains. We empirically demonstrate the gains by studying long-term-earning effects of job training.},
  archive      = {J_JRSSSB},
  author       = {Kallus, Nathan and Mao, Xiaojie},
  doi          = {10.1093/jrsssb/qkae099},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {480-509},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {On the role of surrogates in the efficient estimation of treatment effects with limited outcome data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic richardson extrapolation. <em>JRSSSB</em>, <em>87</em>(2), 457-479. (<a href='https://doi.org/10.1093/jrsssb/qkae098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For over a century, extrapolation methods have provided a powerful tool to improve the convergence order of a numerical method. However, these tools are not well-suited to modern computer codes, where multiple continua are discretized and convergence orders are not easily analysed. To address this challenge, we present a probabilistic perspective on Richardson extrapolation, a point of view that unifies classical extrapolation methods with modern multi-fidelity modelling, and handles uncertain convergence orders by allowing these to be statistically estimated. The approach is developed using Gaussian processes, leading to Gauss–Richardson Extrapolation . Conditions are established under which extrapolation using the conditional mean achieves a polynomial (or even an exponential) speed-up compared to the original numerical method. Further, the probabilistic formulation unlocks the possibility of experimental design, casting the selection of fidelities as a continuous optimization problem, which can then be (approximately) solved. A case study involving a computational cardiac model demonstrates that practical gains in accuracy can be achieved using the GRE method.},
  archive      = {J_JRSSSB},
  author       = {Oates, Chris J and Karvonen, Toni and Teckentrup, Aretha L and Strocchi, Marina and Niederer, Steven A},
  doi          = {10.1093/jrsssb/qkae098},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {457-479},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Probabilistic richardson extrapolation},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stableness of resistance model for nonresponse adjustment with callback data. <em>JRSSSB</em>, <em>87</em>(2), 433-456. (<a href='https://doi.org/10.1093/jrsssb/qkae097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonresponse arises frequently in surveys, and follow-ups are routinely made to increase the response rate. In order to monitor the follow-up process, callback data have been used in social sciences and survey studies for decades. In modern surveys, the availability of callback data is increasing because the response rate is decreasing, and follow-ups are essential to collect maximum information. Although callback data are helpful to reduce the bias in surveys, such data have not been widely used in statistical analysis until recently. We propose a stableness of resistance assumption for nonresponse adjustment with callback data. We establish the identification and the semiparametric efficiency theory under this assumption, and propose a suite of semiparametric estimation methods including doubly robust estimators, which generalize existing parametric approaches for callback data analysis. We apply the approach to a Consumer Expenditure Survey dataset. The results suggest an association between nonresponse and high housing expenditures.},
  archive      = {J_JRSSSB},
  author       = {Miao, Wang and Li, Xinyu and Zhang, Ping and Sun, Baoluo},
  doi          = {10.1093/jrsssb/qkae097},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {433-456},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {A stableness of resistance model for nonresponse adjustment with callback data},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Repulsion, chaos, and equilibrium in mixture models. <em>JRSSSB</em>, <em>87</em>(2), 389-432. (<a href='https://doi.org/10.1093/jrsssb/qkae096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture models are commonly used in applications with heterogeneity and overdispersion in the population, as they allow the identification of subpopulations. In the Bayesian framework, this entails the specification of suitable prior distributions for the weights and locations of the mixture. Despite their popularity, the flexibility of these models often does not translate into the interpretability of the clusters. To overcome this issue, repulsive mixture models have been recently proposed. The basic idea is to include a repulsive term in the distribution of the atoms of the mixture, favouring mixture locations far apart. This approach induces well-separated clusters, aiding the interpretation of the results. However, these models are usually not easy to handle due to unknown normalizing constants. We exploit results from equilibrium statistical mechanics, where the molecular chaos hypothesis implies that nearby particles spread out over time. In particular, we exploit the connection between random matrix theory and statistical mechanics and propose a novel class of repulsive prior distributions based on Gibbs measures associated with joint distributions of eigenvalues of random matrices. The proposed framework greatly simplifies computations thanks to the availability of the normalizing constant in closed form. We investigate the theoretical properties and clustering performance of the proposed distributions.},
  archive      = {J_JRSSSB},
  author       = {Cremaschi, Andrea and Wertz, Timothy M and De Iorio, Maria},
  doi          = {10.1093/jrsssb/qkae096},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {389-432},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Repulsion, chaos, and equilibrium in mixture models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-term causal inference under persistent confounding via data combination. <em>JRSSSB</em>, <em>87</em>(2), 362-388. (<a href='https://doi.org/10.1093/jrsssb/qkae095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the identification and estimation of long-term treatment effects by combining short-term experimental data and long-term observational data subject to unobserved confounding. This problem arises often when concerned with long-term treatment effects since experiments are often short-term due to operational necessity while observational data can be more easily collected over longer time frames but may be subject to confounding. In this paper, we tackle the challenge of persistent confounding: unobserved confounders that can simultaneously affect the treatment, short-term outcomes, and long-term outcome. In particular, persistent confounding invalidates identification strategies in previous approaches to this problem. To address this challenge, we exploit the sequential structure of multiple short-term outcomes and develop several novel identification strategies for the average long-term treatment effect. Based on these, we develop estimation and inference methods with asymptotic guarantees. To demonstrate the importance of handling persistent confounders, we apply our methods to estimate the effect of a job training program on long-term employment using semi-synthetic data.},
  archive      = {J_JRSSSB},
  author       = {Imbens, Guido and Kallus, Nathan and Mao, Xiaojie and Wang, Yuhao},
  doi          = {10.1093/jrsssb/qkae095},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {362-388},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Long-term causal inference under persistent confounding via data combination},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust model averaging prediction of longitudinal response with ultrahigh-dimensional covariates. <em>JRSSSB</em>, <em>87</em>(2), 337-361. (<a href='https://doi.org/10.1093/jrsssb/qkae094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model averaging is an attractive ensemble technique to construct fast and accurate prediction. Despite of having been widely practiced in cross-sectional data analysis, its application to longitudinal data is rather limited so far. We consider model averaging for longitudinal response when the number of covariates is ultrahigh. To this end, we propose a novel two-stage procedure in which variable screening is first conducted and then followed by model averaging. In both stages, a robust rank-based estimation function is introduced to cope with potential outliers and heavy-tailed error distributions, while the longitudinal correlation is modelled by a modified Cholesky decomposition method and properly incorporated to achieve efficiency. Asymptotic properties of our proposed methods are rigorously established, including screening consistency and convergence of the model averaging predictor, with uncertainties in the screening step and selected model set both taken into account. Extensive simulation studies demonstrate that our method outperforms existing competitors, resulting in significant improvements in screening and prediction performance. Finally, we apply our proposed framework to analyse a human microbiome dataset, showing the capability of our procedure in resolving robust prediction using massive metabolites.},
  archive      = {J_JRSSSB},
  author       = {Jiang, Binyan and Lv, Jing and Li, Jialiang and Cheng, Ming−Yen},
  doi          = {10.1093/jrsssb/qkae094},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {337-361},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Robust model averaging prediction of longitudinal response with ultrahigh-dimensional covariates},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric estimation via partial derivatives. <em>JRSSSB</em>, <em>87</em>(2), 319-336. (<a href='https://doi.org/10.1093/jrsssb/qkae093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional nonparametric estimation methods often lead to a slow convergence rate in large dimensions and require unrealistically large dataset sizes for reliable conclusions. We develop an approach based on partial derivatives, either observed or estimated, to effectively estimate the function at near-parametric convergence rates. This novel approach and computational algorithm could lead to methods useful to practitioners in many areas of science and engineering. Our theoretical results reveal behaviour universal to this class of nonparametric estimation problems. We explore a general setting involving tensor product spaces and build upon the smoothing spline analysis of variance framework. For d -dimensional models under full interaction, the optimal rates with gradient information on p covariates are identical to those for the ( d − p ) -interaction models without gradients and, therefore, the models are immune to the curse of interaction . For additive models, the optimal rates using gradient information are n ⁠ , thus achieving the parametric rate . We demonstrate aspects of the theoretical results through synthetic and real data applications.},
  archive      = {J_JRSSSB},
  author       = {Dai, Xiaowu},
  doi          = {10.1093/jrsssb/qkae093},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {319-336},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Nonparametric estimation via partial derivatives},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrected generalized cross-validation for finite ensembles of penalized estimators. <em>JRSSSB</em>, <em>87</em>(2), 289-318. (<a href='https://doi.org/10.1093/jrsssb/qkae092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized cross-validation (GCV) is a widely used method for estimating the squared out-of-sample prediction risk that employs scalar degrees of freedom adjustment (in a multiplicative sense) to the squared training error. In this paper, we examine the consistency of GCV for estimating the prediction risk of arbitrary ensembles of penalized least-squares estimators. We show that GCV is inconsistent for any finite ensemble of size greater than one. Towards repairing this shortcoming, we identify a correction that involves an additional scalar correction (in an additive sense) based on degrees of freedom adjusted training errors from each ensemble component. The proposed estimator (termed CGCV) maintains the computational advantages of GCV and requires neither sample splitting, model refitting, or out-of-bag risk estimation. The estimator stems from a finer inspection of the ensemble risk decomposition and two intermediate risk estimators for the components in this decomposition. We provide a non-asymptotic analysis of the CGCV and the two intermediate risk estimators for ensembles of convex penalized estimators under Gaussian features and a linear response model. Furthermore, in the special case of ridge regression, we extend the analysis to general feature and response distributions using random matrix theory, which establishes model-free uniform consistency of CGCV.},
  archive      = {J_JRSSSB},
  author       = {Bellec, Pierre C and Du, Jin-Hong and Koriyama, Takuya and Patil, Pratik and Tan, Kai},
  doi          = {10.1093/jrsssb/qkae092},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {4},
  number       = {2},
  pages        = {289-318},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Corrected generalized cross-validation for finite ensembles of penalized estimators},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Dynamic shrinkage processes. <em>JRSSSB</em>, <em>87</em>(1), 287-288. (<a href='https://doi.org/10.1093/jrsssb/qkae102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSB},
  author       = {Kowal, Daniel R and Matteson, David S and Ruppert, David},
  doi          = {10.1093/jrsssb/qkae102},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {287-288},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Correction to: Dynamic shrinkage processes},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rank-transformed subsampling: Inference for multiple data splitting and exchangeable p-values. <em>JRSSSB</em>, <em>87</em>(1), 256-286. (<a href='https://doi.org/10.1093/jrsssb/qkae091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many testing problems are readily amenable to randomized tests, such as those employing data splitting. However, despite their usefulness in principle, randomized tests have obvious drawbacks. Firstly, two analyses of the same dataset may lead to different results. Secondly, the test typically loses power because it does not fully utilize the entire sample. As a remedy to these drawbacks, we study how to combine the test statistics or p -values resulting from multiple random realizations, such as through random data splits. We develop rank-transformed subsampling as a general method for delivering large-sample inference about the combined statistic or p -value under mild assumptions. We apply our methodology to a wide range of problems, including testing unimodality in high-dimensional data, testing goodness-of-fit of parametric quantile regression models, testing no direct effect in a sequentially randomized trial and calibrating cross-fit double machine learning confidence intervals. In contrast to existing p -value aggregation schemes that can be highly conservative, our method enjoys Type I error control that asymptotically approaches the nominal level. Moreover, compared to using the ordinary subsampling, we show that our rank transform can remove the first-order bias in approximating the null under alternatives and greatly improve power.},
  archive      = {J_JRSSSB},
  author       = {Guo, F Richard and Shah, Rajen D},
  doi          = {10.1093/jrsssb/qkae091},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {256-286},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Rank-transformed subsampling: Inference for multiple data splitting and exchangeable p-values},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mode-wise principal subspace pursuit and matrix spiked covariance model. <em>JRSSSB</em>, <em>87</em>(1), 232-255. (<a href='https://doi.org/10.1093/jrsssb/qkae088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel framework called Mo de-wise P rincipal S u bspace P ursuit ( MOP-UP ) to extract hidden variations in both the row and column dimensions for matrix data. To enhance the understanding of the framework, we introduce a class of matrix-variate spiked covariance models that serve as inspiration for the development of the MOP-UP algorithm. The MOP-UP algorithm consists of two steps: Average Subspace Capture ( ASC ) and Alternating Projection. These steps are specifically designed to capture the row-wise and column-wise dimension-reduced subspaces which contain the most informative features of the data. ASC utilizes a novel average projection operator as initialization and achieves exact recovery in the noiseless setting. We analyse the convergence and non-asymptotic error bounds of MOP-UP , introducing a blockwise matrix eigenvalue perturbation bound that proves the desired bound, where classic perturbation bounds fail. The effectiveness and practical merits of the proposed framework are demonstrated through experiments on both simulated and real datasets. Lastly, we discuss generalizations of our approach to higher-order data.},
  archive      = {J_JRSSSB},
  author       = {Tang, Runshi and Yuan, Ming and Zhang, Anru R},
  doi          = {10.1093/jrsssb/qkae088},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {232-255},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Mode-wise principal subspace pursuit and matrix spiked covariance model},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuously indexed graphical models. <em>JRSSSB</em>, <em>87</em>(1), 211-231. (<a href='https://doi.org/10.1093/jrsssb/qkae086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let X = { X u } u ∈ U be a real-valued Gaussian process indexed by a set U ⁠ . We show that X can be viewed as a graphical model with an uncountably infinite graph, where each X u is a vertex. This graph is characterized by the reproducing property of X ’ s covariance kernel, without restricting U to be finite or countable, allowing the modelling of stochastic processes in continuous time/space. Unlike traditional methods, this characterization is not based on zero entries of an inverse covariance, posing challenges for structure estimation. We propose a plug-in methodology that targets graph recovery up to a finite resolution and shows consistency for graphs which are sufficiently regular and that can be applied to virtually any measurement regime. Furthermore, we derive convergence rates and finite-sample guarantees for the method, and demonstrate its performance through a simulation study and two data analyses.},
  archive      = {J_JRSSSB},
  author       = {Waghmare, Kartik G and Panaretos, Victor M},
  doi          = {10.1093/jrsssb/qkae086},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {211-231},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Continuously indexed graphical models},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alignment and comparison of directed networks via transition couplings of random walks. <em>JRSSSB</em>, <em>87</em>(1), 186-210. (<a href='https://doi.org/10.1093/jrsssb/qkae085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe and study a transport-based procedure called network optimal transition coupling (NetOTC) for the comparison and alignment of two networks. The networks of interest may be directed or undirected, weighted or unweighted, and may have distinct vertex sets of different sizes. Given two networks and a cost function relating their vertices, NetOTC finds a transition coupling of their associated random walks having minimum expected cost. The minimizing cost quantifies the difference between the networks, while the optimal transport plan itself provides alignments of both the vertices and the edges of the two networks. Coupling of the full random walks, rather than their marginal distributions, ensures that NetOTC captures local and global information about the networks and preserves edges. NetOTC has no free parameters and does not rely on randomization. We investigate a number of theoretical properties of NetOTC and present experiments establishing its empirical performance.},
  archive      = {J_JRSSSB},
  author       = {Yi, Bongsoo and O’Connor, Kevin and McGoff, Kevin and Nobel, Andrew B},
  doi          = {10.1093/jrsssb/qkae085},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {186-210},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Alignment and comparison of directed networks via transition couplings of random walks},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A nonparametric framework for treatment effect modifier discovery in high dimensions. <em>JRSSSB</em>, <em>87</em>(1), 157-185. (<a href='https://doi.org/10.1093/jrsssb/qkae084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous treatment effects are driven by treatment effect modifiers (TEMs), pretreatment covariates that modify the effect of a treatment on an outcome. Current approaches for uncovering these variables are limited to low-dimensional data, data with weakly correlated covariates, or data generated according to parametric processes. We resolve these issues by proposing a framework for defining model-agnostic TEM variable importance parameters (TEM-VIPs), deriving one-step, estimating equation, and targeted maximum likelihood estimators of these parameters, and establishing these estimators’ asymptotic properties. This framework is showcased by defining TEM-VIPs for data-generating processes with continuous, binary, and time-to-event outcomes with binary treatments, and deriving accompanying asymptotically linear estimators. Simulation experiments demonstrate that these estimators’ asymptotic guarantees are approximately achieved in realistic sample sizes in randomized and observational studies alike. This methodology is also applied to gene expression data collected in a clinical trial assessing the effect of a novel therapy on disease-free survival in breast cancer patients. Predicted TEMs have previously been linked to treatment resistance.},
  archive      = {J_JRSSSB},
  author       = {Boileau, Philippe and Leng, Ning and Hejazi, Nima S and van der Laan, Mark and Dudoit, Sandrine},
  doi          = {10.1093/jrsssb/qkae084},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {157-185},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {A nonparametric framework for treatment effect modifier discovery in high dimensions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Isotonic subgroup selection. <em>JRSSSB</em>, <em>87</em>(1), 132-156. (<a href='https://doi.org/10.1093/jrsssb/qkae083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a sample of covariate–response pairs, we consider the subgroup selection problem of identifying a subset of the covariate domain where the regression function exceeds a predetermined threshold. We introduce a computationally feasible approach for subgroup selection in the context of multivariate isotonic regression based on martingale tests and multiple testing procedures for logically structured hypotheses. Our proposed procedure satisfies a non-asymptotic, uniform Type I error rate guarantee with power that attains the minimax optimal rate up to poly-logarithmic factors. Extensions cover classification, isotonic quantile regression, and heterogeneous treatment effect settings. Numerical studies on both simulated and real data confirm the practical effectiveness of our proposal, which is implemented in the R package ISS .},
  archive      = {J_JRSSSB},
  author       = {Müller, Manuel M and Reeve, Henry W J and Cannings, Timothy I and Samworth, Richard J},
  doi          = {10.1093/jrsssb/qkae083},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {132-156},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Isotonic subgroup selection},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended fiducial inference: Toward an automated process of statistical inference. <em>JRSSSB</em>, <em>87</em>(1), 98-131. (<a href='https://doi.org/10.1093/jrsssb/qkae082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While fiducial inference was widely considered a big blunder by R.A. Fisher, the goal he initially set—‘inferring the uncertainty of model parameters on the basis of observations’—has been continually pursued by many statisticians. To this end, we develop a new statistical inference method called extended Fiducial inference (EFI). The new method achieves the goal of fiducial inference by leveraging advanced statistical computing techniques while remaining scalable for big data. Extended Fiducial inference involves jointly imputing random errors realized in observations using stochastic gradient Markov chain Monte Carlo and estimating the inverse function using a sparse deep neural network (DNN). The consistency of the sparse DNN estimator ensures that the uncertainty embedded in observations is properly propagated to model parameters through the estimated inverse function, thereby validating downstream statistical inference. Compared to frequentist and Bayesian methods, EFI offers significant advantages in parameter estimation and hypothesis testing. Specifically, EFI provides higher fidelity in parameter estimation, especially when outliers are present in the observations; and eliminates the need for theoretical reference distributions in hypothesis testing, thereby automating the statistical inference process. Extended Fiducial inference also provides an innovative framework for semisupervised learning.},
  archive      = {J_JRSSSB},
  author       = {Liang, Faming and Kim, Sehwan and Sun, Yan},
  doi          = {10.1093/jrsssb/qkae082},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {98-131},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Extended fiducial inference: Toward an automated process of statistical inference},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graphical criteria for the identification of marginal causal effects in continuous-time survival and event-history analyses. <em>JRSSSB</em>, <em>87</em>(1), 74-97. (<a href='https://doi.org/10.1093/jrsssb/qkae056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider continuous-time survival and event-history settings, where our aim is to graphically represent causal structures allowing us to characterize when a causal parameter is identified from observational data. This causal parameter is formalized as the effect on an outcome event of a (possibly hypothetical) intervention on the intensity of a treatment process. To establish identifiability, we propose novel graphical rules indicating whether the observed information is sufficient to obtain the desired causal effect by suitable reweighting. This requires a different type of graph than in discrete time. We formally define causal semantics for the corresponding dynamic graphs that represent local independence models for multivariate counting processes. Importantly, our work highlights that causal inference from censored data relies on subtle structural assumptions on the censoring process beyond independent censoring; these can be verified graphically. Put together, our results are the first to establish graphical rules for nonparametric causal identifiability in event processes in this generality for the continuous-time case, not relying on particular parametric survival models. We conclude with a data example on Human papillomavirus (HPV) testing for cervical cancer screening, where the assumptions are illustrated graphically and the desired effect is estimated by reweighted cumulative incidence curves.},
  archive      = {J_JRSSSB},
  author       = {Røysland, Kjetil and Ryalen, Pål C and Nygård, Mari and Didelez, Vanessa},
  doi          = {10.1093/jrsssb/qkae056},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {74-97},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Graphical criteria for the identification of marginal causal effects in continuous-time survival and event-history analyses},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Catch me if you can: Signal localization with knockoff e-values. <em>JRSSSB</em>, <em>87</em>(1), 56-73. (<a href='https://doi.org/10.1093/jrsssb/qkae042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider problems where many, somewhat redundant, hypotheses are tested and we are interested in reporting the most precise rejections, with false discovery rate (FDR) control. This is the case, for example, when researchers are interested both in individual hypotheses as well as group hypotheses corresponding to intersections of sets of the original hypotheses, at several resolution levels. A concrete application is in genome-wide association studies, where, depending on the signal strengths, it might be possible to resolve the influence of individual genetic variants on a phenotype with greater or lower precision. To adapt to the unknown signal strength, analyses are conducted at multiple resolutions and researchers are most interested in the more precise discoveries. Assuring FDR control on the reported findings with these adaptive searches is, however, often impossible. To design a multiple comparison procedure that allows for an adaptive choice of resolution with FDR control, we leverage e -values and linear programming. We adapt this approach to problems where knockoffs and group knockoffs have been successfully applied to test conditional independence hypotheses. We demonstrate its efficacy by analysing data from the UK Biobank.},
  archive      = {J_JRSSSB},
  author       = {Gablenz, Paula and Sabatti, Chiara},
  doi          = {10.1093/jrsssb/qkae042},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {56-73},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Catch me if you can: Signal localization with knockoff e-values},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Debiased inference for a covariate-adjusted regression function. <em>JRSSSB</em>, <em>87</em>(1), 33-55. (<a href='https://doi.org/10.1093/jrsssb/qkae041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study nonparametric inference for a covariate-adjusted regression function. This parameter captures the average association between a continuous exposure and an outcome after adjusting for other covariates. Under certain causal conditions, it also corresponds to the average outcome had all units been assigned to a specific exposure level, known as the causal dose–response curve. We propose a debiased local linear estimator of the covariate-adjusted regression function and demonstrate that our estimator converges pointwise to a mean-zero normal limit distribution. We use this result to construct asymptotically valid confidence intervals for function values and differences thereof. In addition, we use approximation results for the distribution of the supremum of an empirical process to construct asymptotically valid uniform confidence bands. Our methods do not require undersmoothing, permit the use of data-adaptive estimators of nuisance functions, and our estimator attains the optimal rate of convergence for a twice differentiable regression function. We illustrate the practical performance of our estimator using numerical studies and an analysis of the effect of air pollution exposure on cardiovascular mortality.},
  archive      = {J_JRSSSB},
  author       = {Takatsu, Kenta and Westling, Ted},
  doi          = {10.1093/jrsssb/qkae041},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {33-55},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Debiased inference for a covariate-adjusted regression function},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate, heteroscedastic empirical bayes via nonparametric maximum likelihood. <em>JRSSSB</em>, <em>87</em>(1), 1-32. (<a href='https://doi.org/10.1093/jrsssb/qkae040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate, heteroscedastic errors complicate statistical inference in many large-scale denoizing problems. Empirical Bayes is attractive in such settings, but standard parametric approaches rest on assumptions about the form of the prior distribution which can be hard to justify and which introduce unnecessary tuning parameters. We extend the nonparametric maximum-likelihood estimator (NPMLE) for Gaussian location mixture densities to allow for multivariate, heteroscedastic errors. NPMLEs estimate an arbitrary prior by solving an infinite-dimensional, convex optimization problem; we show that this convex optimization problem can be tractably approximated by a finite-dimensional version. The empirical Bayes posterior means based on an NPMLE have low regret, meaning they closely target the oracle posterior means one would compute with the true prior in hand. We prove an oracle inequality implying that the empirical Bayes estimator performs at nearly the optimal level (up to logarithmic factors) for denoizing without prior knowledge. We provide finite-sample bounds on the average Hellinger accuracy of an NPMLE for estimating the marginal densities of the observations. We also demonstrate the adaptive and nearly optimal properties of NPMLEs for deconvolution. We apply our method to two denoizing problems in astronomy and to two hierarchical linear modelling problems in social science and biology.},
  archive      = {J_JRSSSB},
  author       = {Soloff, Jake A and Guntuboyina, Adityanand and Sen, Bodhisattva},
  doi          = {10.1093/jrsssb/qkae040},
  journal      = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month        = {2},
  number       = {1},
  pages        = {1-32},
  shortjournal = {J. R. Stat. Soc. Ser. B},
  title        = {Multivariate, heteroscedastic empirical bayes via nonparametric maximum likelihood},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
