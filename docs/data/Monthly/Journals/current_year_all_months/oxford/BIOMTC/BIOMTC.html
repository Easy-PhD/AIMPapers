<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMTC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomtc">BIOMTC - 129</h2>
<ul>
<li><details>
<summary>
(2025). Binary regression and classification with covariates in metric spaces. <em>BIOMTC</em>, <em>81</em>(3), ujaf123. (<a href='https://doi.org/10.1093/biomtc/ujaf123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by logistic regression, we introduce a regression model for data tuples consisting of a binary response and a set of covariates residing in a metric space without vector structures. Based on the proposed model, we also develop a binary classifier for metric-space valued data. We propose a maximum likelihood estimator for the metric-space valued regression coefficient in the model, and provide upper bounds on the estimation error under various metric entropy conditions that quantify complexity of the underlying metric space. Matching lower bounds are derived for the important metric spaces commonly seen in statistics, establishing optimality of the proposed estimator in such spaces. A finer upper bound and a matching lower bound, and thus optimality of the proposed classifier, are established for Riemannian manifolds. To the best of our knowledge, the proposed regression model and the above minimax bounds are the first of their kind for analyzing a binary response with covariates residing in general metric spaces. We also investigate the numerical performance of the proposed estimator and classifier via simulation studies, and illustrate their practical merits via an application to task-related fMRI data.},
  archive      = {J_BIOMTC},
  author       = {Lin, Yinan and Lin, Zhenhua},
  doi          = {10.1093/biomtc/ujaf123},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf123},
  shortjournal = {Biometrics},
  title        = {Binary regression and classification with covariates in metric spaces},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the heterogeneity in recurrent episode lengths based on quantile regression. <em>BIOMTC</em>, <em>81</em>(3), ujaf122. (<a href='https://doi.org/10.1093/biomtc/ujaf122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent episode data frequently arise in chronic disease studies when an event of interest occurs repeatedly and each occurrence lasts for a random period of time. Understanding the heterogeneity in recurrent episode lengths can help guide dynamic and customized disease management. However, there has been relative sparse attention to methods tailored to this end. Existing approaches either do not confer direct interpretation on episode lengths or involve restrictive or unrealistic distributional assumptions, such as exchangeability of within-individual episode lengths. In this work, we propose a modeling strategy that overcomes these limitations through adopting quantile regression and sensibly incorporating time-dependent covariates. Treating recurrent episodes as clustered data, we develop an estimation procedure that properly handles the special complications, including dependent censoring, dependent truncation, and informative cluster size. Our estimation procedure is computationally simple and yields estimators with desirable asymptotic properties. Our numerical studies demonstrate the advantages of the proposed method over naive adaptations of existing approaches.},
  archive      = {J_BIOMTC},
  author       = {Liu, Yi and Umpierrez, Guillermo E and Peng, Limin},
  doi          = {10.1093/biomtc/ujaf122},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf122},
  shortjournal = {Biometrics},
  title        = {Exploring the heterogeneity in recurrent episode lengths based on quantile regression},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The cox-pólya-gamma algorithm for flexible bayesian inference of multilevel survival models. <em>BIOMTC</em>, <em>81</em>(3), ujaf121. (<a href='https://doi.org/10.1093/biomtc/ujaf121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Cox semiparametric regression is an important problem in many clinical settings. The elliptical information geometry of Cox models is underutilized in Bayesian inference but can effectively bridge survival analysis and hierarchical Gaussian models. Survival models should be able to incorporate multilevel modeling such as case weights, frailties, and smoothing splines, in a straightforward manner similar to Gaussian models. To tackle these challenges, we propose the Cox-Pólya-Gamma algorithm for Bayesian multilevel Cox semiparametric regression and survival functions. Our novel computational procedure succinctly addresses the difficult problem of monotonicity-constrained modeling of the nonparametric baseline cumulative hazard along with multilevel regression. We develop two key strategies based on the elliptical geometry of Cox models that allows computation to be implemented in a few lines of code. First, we exploit an approximation between Cox models and negative binomial processes through the Poisson process to reduce Bayesian computation to iterative Gaussian sampling. Next, we appeal to sufficient dimension reduction to address the difficult computation of nonparametric baseline cumulative hazards, allowing for the collapse of the Markov transition within the Gibbs sampler based on beta sufficient statistics. We explore conditions for uniform ergodicity of the Cox-Pólya-Gamma algorithm. We provide software and demonstrate our multilevel modeling approach using open-source data and simulations.},
  archive      = {J_BIOMTC},
  author       = {Ren, Benny and Morris, Jeffrey S and Barnett, Ian},
  doi          = {10.1093/biomtc/ujaf121},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf121},
  shortjournal = {Biometrics},
  title        = {The cox-pólya-gamma algorithm for flexible bayesian inference of multilevel survival models},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical significance of clustering for count data. <em>BIOMTC</em>, <em>81</em>(3), ujaf120. (<a href='https://doi.org/10.1093/biomtc/ujaf120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is widely used in biomedical research for meaningful subgroup identification. However, most existing clustering algorithms do not account for the statistical uncertainty of the resulting clusters and consequently may generate spurious clusters due to natural sampling variation. To address this problem, the Statistical Significance of Clustering (SigClust) method was developed to evaluate the significance of clusters in high-dimensional data. While SigClust has been successful in assessing clustering significance for continuous data, it is not specifically designed for discrete data, such as count data in genomics. Moreover, SigClust and its variations can suffer from reduced statistical power when applied to non-Gaussian high-dimensional data. To overcome these limitations, we propose SigClust-DEV, a method designed to evaluate the significance of clusters in count data. Through extensive simulations, we compare SigClust-DEV against other existing SigClust approaches across various count distributions and demonstrate its superior performance. Furthermore, we apply our proposed SigClust-DEV to Hydra single-cell RNA sequencing (scRNA) data and electronic health records (EHRs) of cancer patients to identify meaningful latent cell types and patient subgroups, respectively.},
  archive      = {J_BIOMTC},
  author       = {Dai, Yifan and Wu, Di and Liu, Yufeng},
  doi          = {10.1093/biomtc/ujaf120},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf120},
  shortjournal = {Biometrics},
  title        = {Statistical significance of clustering for count data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint disease mapping for bivariate count data with residual correlation due to unknown number of common cases. <em>BIOMTC</em>, <em>81</em>(3), ujaf119. (<a href='https://doi.org/10.1093/biomtc/ujaf119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint spatial distribution of two count outcomes (eg, counts of two diseases) is usually studied using a Poisson shared component model (P-SCM), which uses geographically structured latent variables to model spatial variations that are specific and shared by both outcomes. In this model, the correlation between the outcomes is assumed to be fully accounted for by the latent variables. However, in this article, we show that when the outcomes have an unknown number of cases in common, the bivariate counts exhibit a positive “residual” correlation, which the P-SCM wrongly attributes to the covariance of the latent variables, leading to biased inference and degraded predictive performance. Accordingly, we propose a new SCM based on the Bivariate-Poisson distribution (BP-SCM hereafter) to study such correlated bivariate data. The BP-SCM decomposes each count into counts of common and distinct cases, and then models each of these three counts (two distinct and one common) using Gaussian Markov Random Fields. The model is formulated in a Bayesian framework using Hamiltonian Monte Carlo inference. Simulations and a real-world application showed the good inferential and predictive performances of the BP-SCM and confirm the bias in P-SCM. BP-SCM provides rich epidemiological information, such as the mean levels of the unknown counts of common and distinct cases, and their shared and specific spatial variations.},
  archive      = {J_BIOMTC},
  author       = {Chatignoux, Edouard and Uhry, Zoé and Remontet, Laurent and Albert, Isabelle},
  doi          = {10.1093/biomtc/ujaf119},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf119},
  shortjournal = {Biometrics},
  title        = {Joint disease mapping for bivariate count data with residual correlation due to unknown number of common cases},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric bayesian approach for dynamic borrowing of historical control data. <em>BIOMTC</em>, <em>81</em>(3), ujaf118. (<a href='https://doi.org/10.1093/biomtc/ujaf118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When incorporating historical control data into the analysis of current randomized controlled trial data, it is critical to account for differences between the datasets. When the cause of difference is an unmeasured factor and adjustment for only observed covariates is insufficient, it is desirable to use a dynamic borrowing method that reduces the impact of heterogeneous historical controls. We propose a nonparametric Bayesian approach that addresses between-trial heterogeneity and allows borrowing historical controls homogeneous with the current control. Additionally, to emphasize conflict resolution between historical controls and the current control, we introduce a method based on the dependent Dirichlet process (DP) mixture. The proposed methods can be implemented using the same procedure, regardless of whether the outcome data comprise aggregated study-level data or individual participant data. We also develop a novel index of similarity between the historical and current control data, based on the posterior distribution of the parameter of interest. We conduct a simulation study and analyze clinical trial examples to evaluate the performance of the proposed methods compared to existing methods. The proposed method, based on the dependent DP mixture, can accurately borrow from homogeneous historical controls while reducing the impact of heterogeneous historical controls compared to the typical DP mixture. The proposed methods outperform existing methods in scenarios with heterogeneous historical controls, in which the meta-analytic approach is ineffective.},
  archive      = {J_BIOMTC},
  author       = {Ohigashi, Tomohiro and Maruo, Kazushi and Sozu, Takashi and Gosho, Masahiko},
  doi          = {10.1093/biomtc/ujaf118},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf118},
  shortjournal = {Biometrics},
  title        = {Nonparametric bayesian approach for dynamic borrowing of historical control data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A group distributional ICA method for decomposing multi-subject diffusion tensor imaging. <em>BIOMTC</em>, <em>81</em>(3), ujaf117. (<a href='https://doi.org/10.1093/biomtc/ujaf117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion tensor imaging (DTI) is a frequently used imaging modality to investigate white matter fiber connections of human brain. DTI provides an important tool for characterizing human brain structural organization. Common goals in DTI analysis include dimension reduction, denoising, and extraction of underlying structure networks. Blind source separation methods are often used to achieve these goals for other imaging modalities. However, there has been very limited work for multi-subject DTI data. Due to the special characteristics of the 3D diffusion tensor measured in DTI, existing methods such as standard independent component analysis (ICA) cannot be directly applied. We propose a Group Distributional ICA (G-DICA) method to fill this gap. G-DICA represents a fundamentally new blind source separation method that separates the parameters in the distribution function of the observed imaging data as a mixture of independent source signals. Decomposing multi-subject DTI using G-DICA uncovers structural networks corresponding to several major white matter fiber bundles in the brain. Through simulation studies and real data applications, the proposed G-DICA method demonstrates superior performance and improved reproducibility compared to the existing method.},
  archive      = {J_BIOMTC},
  author       = {Yang, Guangming and Wu, Ben and Kang, Jian and Guo, Ying},
  doi          = {10.1093/biomtc/ujaf117},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf117},
  shortjournal = {Biometrics},
  title        = {A group distributional ICA method for decomposing multi-subject diffusion tensor imaging},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating associations between cumulative exposure and health via generalized distributed lag non-linear models using penalized splines. <em>BIOMTC</em>, <em>81</em>(3), ujaf116. (<a href='https://doi.org/10.1093/biomtc/ujaf116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying associations between short-term exposure to ambient air pollution and health outcomes is an important public health priority. Many studies have investigated the association considering delayed effects within the past few days. Adaptive cumulative exposure distributed lag non-linear models (ACE-DLNMs) quantify associations between health outcomes and cumulative exposure that is specified in a data-adaptive way. While the ACE-DLNM framework is highly interpretable, it is limited to continuous outcomes and does not scale well to large datasets. Motivated by a large analysis of daily pollution and respiratory hospitalization counts in Canada between 2001 and 2018, we propose a generalized ACE-DLNM incorporating penalized splines, improving upon existing ACE-DLNM methods to accommodate general response types. We then develop a computationally efficient estimation strategy based on profile likelihood and Laplace approximate marginal likelihood with Newton-type methods. We demonstrate the performance and practical advantages of the proposed method through simulations. In application to the motivating analysis, the proposed method yields more stable inferences compared to generalized additive models with fixed exposures, while retaining interpretability.},
  archive      = {J_BIOMTC},
  author       = {Pan, Tianyi and Shin, Hwashin Hyun and McGee, Glen and Stringer, Alex},
  doi          = {10.1093/biomtc/ujaf116},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf116},
  shortjournal = {Biometrics},
  title        = {Estimating associations between cumulative exposure and health via generalized distributed lag non-linear models using penalized splines},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian inference for copy number intra-tumoral heterogeneity from single-cell RNA-sequencing data. <em>BIOMTC</em>, <em>81</em>(3), ujaf115. (<a href='https://doi.org/10.1093/biomtc/ujaf115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copy number alterations (CNA) are important drivers and markers of clonal structures within tumors. Understanding these structures at single-cell resolution is crucial to advancing cancer treatments. The objective is to cluster single cells into clones and identify CNA events in each clone. Early attempts often sacrifice the intrinsic link between cell clustering and clonal CNA detection for simplicity and rely heavily on human input for critical parameters such as the number of clones. Here, we develop a Bayesian model to utilize single-cell RNA sequencing (scRNA-seq) data for automatic analysis of intra-tumoral clonal structure concerning CNAs, without reliance on prior knowledge. The model clusters cells into sub-tumoral clones, identifies the number of clones, and simultaneously infers the clonal CNA profiles. It synergistically incorporates input from gene expression and germline single-nucleotide polymorphisms. A Gibbs sampling algorithm has been implemented and is available as an R package Chloris. We demonstrate that our new method compares strongly against existing software tools in terms of both cell clustering and CNA profile identification accuracy. Application to human metastatic melanoma and anaplastic thyroid tumor data demonstrates accurate clustering of tumor and non-tumor cells and reveals clonal CNA profiles that highlight functional gene expression differences between clones from the same tumor.},
  archive      = {J_BIOMTC},
  author       = {Qiao, PuXue and Kwok, Chun Fung and Qian, Guoqi and McCarthy, Davis J},
  doi          = {10.1093/biomtc/ujaf115},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf115},
  shortjournal = {Biometrics},
  title        = {Bayesian inference for copy number intra-tumoral heterogeneity from single-cell RNA-sequencing data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting optimal allocations for binary responses: Insights from considering type-I error rate control. <em>BIOMTC</em>, <em>81</em>(3), ujaf114. (<a href='https://doi.org/10.1093/biomtc/ujaf114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation—an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive 2 optimal allocation proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power, and the other minimizes the total number of failures in a trial while maintaining a fixed variance level. Through simulations based on an early phase and a confirmatory trial, we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials, and alternative measures of interest.},
  archive      = {J_BIOMTC},
  author       = {Pin, Lukas and Villar, Sofía S and Rosenberger, William F},
  doi          = {10.1093/biomtc/ujaf114},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf114},
  shortjournal = {Biometrics},
  title        = {Revisiting optimal allocations for binary responses: Insights from considering type-I error rate control},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised linear regression: Enhancing efficiency and robustness in high dimensions. <em>BIOMTC</em>, <em>81</em>(3), ujaf113. (<a href='https://doi.org/10.1093/biomtc/ujaf113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semi-supervised learning, the prevailing understanding suggests that observing additional unlabeled samples improves estimation accuracy for linear parameters only in the case of model misspecification. In this work, we challenge such a claim and show that additional unlabeled samples are beneficial in high-dimensional settings. Initially focusing on a dense scenario, we introduce robust semi-supervised estimators for the regression coefficient without relying on sparse structures in the population slope. Even when the true underlying model is linear, we show that leveraging information from large-scale unlabeled data helps reduce estimation bias, thereby improving both estimation accuracy and inference robustness. Moreover, we propose semi-supervised methods with further enhanced efficiency in scenarios with a sparse linear slope. The performance of the proposed methods is demonstrated through extensive numerical studies.},
  archive      = {J_BIOMTC},
  author       = {Chen, Kai and Zhang, Yuqian},
  doi          = {10.1093/biomtc/ujaf113},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf113},
  shortjournal = {Biometrics},
  title        = {Semi-supervised linear regression: Enhancing efficiency and robustness in high dimensions},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model robust designs for dose-response models. <em>BIOMTC</em>, <em>81</em>(3), ujaf112. (<a href='https://doi.org/10.1093/biomtc/ujaf112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An optimal experimental design is a structured data collection plan aimed at maximizing the amount of information gathered. Determining an optimal experimental design, however, relies on the assumption that a predetermined model structure, relating the response and covariates, is known a priori. In practical scenarios, such as dose-response modeling, the form of the model representing the “true” relationship is frequently unknown, although there exists a finite set or pool of potential alternative models. Designing experiments based on a single model from this set may lead to inefficiency or inadequacy if the “true” model differs from that assumed when calculating the design. One approach to minimize the impact of the uncertainty in the model on the experimental plan is known as model robust design . In this context, we systematically address the challenge of finding approximate optimal model robust experimental designs. Our focus is on locally optimal designs, so allowing some of the models in the pool to be nonlinear. We present three Semidefinite Programming-based formulations, each aligned with one of the classes of model robustness criteria introduced by Läuter. These formulations exploit the semidefinite representability of the robustness criteria, leading to the representation of the robust problem as a semidefinite program. To ensure comparability of information measures across various models, we employ standardized designs. To illustrate the application of our approach, we consider a dose-response study where, initially, seven models were postulated as potential candidates to describe the dose-response relationship.},
  archive      = {J_BIOMTC},
  author       = {Duarte, Belmiro P M and Atkinson, Anthony C and Oliveira, Nuno M C},
  doi          = {10.1093/biomtc/ujaf112},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf112},
  shortjournal = {Biometrics},
  title        = {Model robust designs for dose-response models},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Evaluating the effects of high-throughput structural neuroimaging predictors on whole-brain functional connectome outcomes via network-based matrix-on-vector regression. <em>BIOMTC</em>, <em>81</em>(3), ujaf111. (<a href='https://doi.org/10.1093/biomtc/ujaf111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1093/biomtc/ujaf111},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf111},
  shortjournal = {Biometrics},
  title        = {Correction to: Evaluating the effects of high-throughput structural neuroimaging predictors on whole-brain functional connectome outcomes via network-based matrix-on-vector regression},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mastering rare event analysis: Subsample-size determination in cox and logistic regressions. <em>BIOMTC</em>, <em>81</em>(3), ujaf110. (<a href='https://doi.org/10.1093/biomtc/ujaf110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of contemporary data analysis, the use of massive datasets has taken on heightened significance, albeit often entailing considerable demands on computational time and memory. While a multitude of existing works offer optimal subsampling methods for conducting analyses on subsamples with minimized efficiency loss, they notably lack tools for judiciously selecting the subsample size. To bridge this gap, our work introduces tools designed for choosing the subsample size. We focus on three settings: the Cox regression model for survival data with rare events, and logistic regression for both balanced and imbalanced datasets. Additionally, we present a new optimal subsampling procedure tailored to logistic regression with imbalanced data. The efficacy of these tools and procedures is demonstrated through an extensive simulation study and meticulous analyses of two sizable datasets: survival analysis of UK Biobank colorectal cancer data with about 350 million rows and logistic regression of linked birth and infant death data with about 28 million observations.},
  archive      = {J_BIOMTC},
  author       = {Agassi, Tal and Keret, Nir and Gorfine, Malka},
  doi          = {10.1093/biomtc/ujaf110},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf110},
  shortjournal = {Biometrics},
  title        = {Mastering rare event analysis: Subsample-size determination in cox and logistic regressions},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional multi-study multi-modality covariate-augmented generalized factor model. <em>BIOMTC</em>, <em>81</em>(3), ujaf107. (<a href='https://doi.org/10.1093/biomtc/ujaf107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent factor models that integrate data from multiple sources/studies or modalities have garnered considerable attention across various disciplines. However, existing methods predominantly focus either on multi-study integration or multi-modality integration, rendering them insufficient for analyzing the diverse modalities measured across multiple studies. To address this limitation and cater to practical needs, we introduce a high-dimensional generalized factor model that seamlessly integrates multi-modality data from multiple studies, while also accommodating additional covariates. We conduct a thorough investigation of the identifiability conditions to enhance the model’s interpretability. To tackle the complexity of high-dimensional nonlinear integration caused by 4 large latent random matrices, we utilize a variational lower bound to approximate the observed log-likelihood by employing a variational posterior distribution. By profiling the variational parameters, we establish the asymptotical properties of estimators for model parameters using M-estimation theory. Furthermore, we devise a computationally efficient variational expectation maximization (EM) algorithm to execute the estimation process and a criterion to determine the optimal number of both study-shared and study-specific factors. Extensive simulation studies and a real-world application show that the proposed method significantly outperforms existing methods in terms of estimation accuracy and computational efficiency.},
  archive      = {J_BIOMTC},
  author       = {Liu, Wei and Zhong, Qingzhi},
  doi          = {10.1093/biomtc/ujaf107},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf107},
  shortjournal = {Biometrics},
  title        = {High-dimensional multi-study multi-modality covariate-augmented generalized factor model},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A monotone single index model for spatially referenced multistate current status data. <em>BIOMTC</em>, <em>81</em>(3), ujaf105. (<a href='https://doi.org/10.1093/biomtc/ujaf105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessment of multistate disease progression is commonplace in biomedical research, such as in periodontal disease (PD). However, the presence of multistate current status endpoints, where only a single snapshot of each subject’s progression through disease states is available at a random inspection time after a known starting state, complicates the inferential framework. In addition, these endpoints can be clustered, and spatially associated, where a group of proximally located teeth (within subjects) may experience similar PD status, compared to those distally located. Motivated by a clinical study recording PD progression, we propose a Bayesian semiparametric accelerated failure time model with an inverse-Wishart proposal for accommodating (spatial) random effects, and flexible errors that follow a Dirichlet process mixture of Gaussians. For clinical interpretability, the systematic component of the event times is modeled using a monotone single index model, with the (unknown) link function estimated via a novel integrated basis expansion and basis coefficients endowed with constrained Gaussian process priors. In addition to establishing parameter identifiability, we present scalable computing via a combination of elliptical slice sampling, fast circulant embedding techniques, and smoothing of hard constraints, leading to straightforward estimation of parameters, and state occupation and transition probabilities. Using synthetic data, we study the finite sample properties of our Bayesian estimates and their performance under model misspecification. We also illustrate our method via application to the real clinical PD dataset.},
  archive      = {J_BIOMTC},
  author       = {Das, Snigdha and Chae, Minwoo and Pati, Debdeep and Bandyopadhyay, Dipankar},
  doi          = {10.1093/biomtc/ujaf105},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf105},
  shortjournal = {Biometrics},
  title        = {A monotone single index model for spatially referenced multistate current status data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric joint modeling to estimate the treatment effect on a longitudinal surrogate with application to chronic kidney disease trials. <em>BIOMTC</em>, <em>81</em>(3), ujaf104. (<a href='https://doi.org/10.1093/biomtc/ujaf104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials where long follow-up is required to measure the primary outcome of interest, there is substantial interest in using an accepted surrogate outcome that can be measured earlier in time or with less cost to estimate a treatment effect. For example, in clinical trials of chronic kidney disease, the effect of a treatment is often demonstrated on a longitudinal surrogate, the change of the longitudinal outcome (glomerular filtration rate, GFR) per year or GFR slope. However, estimating the effect of a treatment on the GFR slope is complicated by the fact that GFR measurement can be terminated by the occurrence of a terminal event, such as death or kidney failure. Thus, to estimate this effect, one must consider both the longitudinal GFR trajectory and the terminal event process. In this paper, we build a semiparametric framework to jointly model the longitudinal outcome and the terminal event, where the model for the longitudinal outcome is semiparametric, the relationship between the longitudinal outcome and the terminal event is nonparametric, and the terminal event is modeled via a semiparametric Cox model. The proposed semiparametric joint model is flexible and can be easily extended to include a nonlinear trajectory of the longitudinal outcome. An estimating equation based method is proposed to estimate the treatment effect on the longitudinal surrogate outcome (eg, GFR slope). Theoretical properties of the proposed estimators are derived, and finite sample performance is evaluated through simulation studies. We illustrate the proposed method using data from the Reduction of Endpoints in NIDDM with the Angiotensin II Antagonist Losartan (RENAAL) trial to examine the effect of Losartan on GFR slope.},
  archive      = {J_BIOMTC},
  author       = {Wang, Xuan and Zhou, Jie and Parast, Layla and Greene, Tom},
  doi          = {10.1093/biomtc/ujaf104},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf104},
  shortjournal = {Biometrics},
  title        = {Semiparametric joint modeling to estimate the treatment effect on a longitudinal surrogate with application to chronic kidney disease trials},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating longitudinal treatment effects for duchenne muscular dystrophy using dynamically enriched bayesian small sample, sequential, multiple assignment randomized trial (snSMART). <em>BIOMTC</em>, <em>81</em>(3), ujaf103. (<a href='https://doi.org/10.1093/biomtc/ujaf103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For progressive rare diseases like Duchenne muscular dystrophy (DMD), evaluating disease burden by measuring the totality of evidence from outcome data over time per patient can be highly informative, especially regarding how a new treatment impacts disease progression and functional outcomes. This paper focuses on new statistical approaches for analyzing data generated over time in a small sample, sequential, multiple assignment, randomized trial (snSMART), with an application to DMD. In addition, the use of external control data can enhance the statistical and operational efficiency in rare disease drug development by solving participant scarcity issues and ethical challenges. We employ a two-step robust meta-analytic approach to leverage external control data while adjusting for important baseline confounders and potential conflicts between external controls and trial data. Furthermore, our approach integrates important baseline covariates to account for patient heterogeneity and introduces a novel piecewise model to manage stage-wise treatment assignments. By applying this methodology to a case study in DMD research, we not only demonstrate the practical application and benefits of our approach but also highlight its potential to mitigate challenges in rare disease trials. Our findings advocate for a more nuanced and statistically robust analysis of treatment effects, thereby improving the reliability of clinical trial results.},
  archive      = {J_BIOMTC},
  author       = {Wang, Sidi and Roychoudhury, Satrajit and Kidwell, Kelley M},
  doi          = {10.1093/biomtc/ujaf103},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf103},
  shortjournal = {Biometrics},
  title        = {Evaluating longitudinal treatment effects for duchenne muscular dystrophy using dynamically enriched bayesian small sample, sequential, multiple assignment randomized trial (snSMART)},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analysis for attributable effects in case2 studies. <em>BIOMTC</em>, <em>81</em>(3), ujaf102. (<a href='https://doi.org/10.1093/biomtc/ujaf102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case 2 study, also referred to as the case–case study design, is a valuable approach for conducting inference for treatment effects. Unlike traditional case-control studies, the case 2 design compares treatment in cases of concern (the first type of case) to other cases (the second type of case). One of the quantities of interest is the attributable effect for the first type of case—that is, the number of the first type of case that would not have occurred had the treatment been withheld from all units. In some case 2 studies, a key quantity of interest is the attributable effect for the first type of case. Two key assumptions that are usually made for making inferences about this attributable effect in case 2 studies are (1) treatment does not cause the second type of case, and (2) the treatment does not alter an individual’s case type. However, these assumptions are not realistic in many real-data applications. In this article, we present a sensitivity analysis framework to scrutinize the impact of deviations from these assumptions on inferences for the attributable effect. We also include sensitivity analyses related to the assumption of unmeasured confounding, recognizing the potential bias introduced by unobserved covariates. The proposed methodology is exemplified through an investigation into whether having violent behavior in the last year of life increases suicide risk using the 1993 National Mortality Followback Survey dataset.},
  archive      = {J_BIOMTC},
  author       = {Chen, Kan and Ye, Ting and Small, Dylan S},
  doi          = {10.1093/biomtc/ujaf102},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf102},
  shortjournal = {Biometrics},
  title        = {Sensitivity analysis for attributable effects in case2 studies},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smooth and shape-constrained quantile distributed lag models. <em>BIOMTC</em>, <em>81</em>(3), ujaf101. (<a href='https://doi.org/10.1093/biomtc/ujaf101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposure to environmental pollutants during the gestational period can significantly impact infant health outcomes, such as birth weight and neurological development. Identifying critical windows of susceptibility, which are specific periods during pregnancy when exposure has the most profound effects, is essential for developing targeted interventions. Distributed lag models (DLMs) are widely used in environmental epidemiology to analyze the temporal patterns of exposure and their impact on health outcomes. However, traditional DLMs focus on modeling the conditional mean, which may fail to capture heterogeneity in the relationship between predictors and the outcome. Moreover, when modeling the distribution of health outcomes like gestational birth weight, it is the extreme quantiles that are of most clinical relevance. We introduce 2 new quantile distributed lag model (QDLM) estimators designed to address the limitations of existing methods by leveraging smoothness and shape constraints, such as unimodality and concavity, to enhance interpretability and efficiency. We apply our QDLM estimators to the Colorado birth cohort data, demonstrating their effectiveness in identifying critical windows of susceptibility and informing public health interventions.},
  archive      = {J_BIOMTC},
  author       = {Jin, Yisen and Molstad, Aaron J and Wilson, Ander and Antonelli, Joseph},
  doi          = {10.1093/biomtc/ujaf101},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf101},
  shortjournal = {Biometrics},
  title        = {Smooth and shape-constrained quantile distributed lag models},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regression analysis of interval-censored failure time data with change points and a cured subgroup. <em>BIOMTC</em>, <em>81</em>(3), ujaf100. (<a href='https://doi.org/10.1093/biomtc/ujaf100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There exists a substantial body of literature that discusses regression analysis of interval-censored failure time data and also many methods have been proposed for handling the presence of a cured subgroup. However, only limited research exists on the problems incorporating change points, with or without a cured subgroup, which can occur in various contexts such as clinical trials where disease risks may shift dramatically when certain biological indicators exceed specific thresholds. To fill this gap, we consider a class of partly linear transformation models within the mixture cure model framework and propose a sieve maximum likelihood estimation approach using Bernstein polynomials and piecewise linear functions for inference. Additionally, we provide a data-driven adaptive procedure to identify the number and locations of change points and establish the asymptotic properties of the proposed method. Extensive simulation studies demonstrate the effectiveness and practical utility of the proposed methods, which are applied to the real data from a breast cancer study that motivated this work.},
  archive      = {J_BIOMTC},
  author       = {Lou, Yichen and Du, Mingyue and Song, Xinyuan},
  doi          = {10.1093/biomtc/ujaf100},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf100},
  shortjournal = {Biometrics},
  title        = {Regression analysis of interval-censored failure time data with change points and a cured subgroup},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Negative binomial mixed effects location-scale models for intensive longitudinal count-type physical activity data provided by wearable devices. <em>BIOMTC</em>, <em>81</em>(3), ujaf099. (<a href='https://doi.org/10.1093/biomtc/ujaf099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of wearable devices, for example, accelerometers, have become increasingly prevalent. Wearable devices enable more accurate real-time tracking of a subject’s physical activity (PA) level, such as steps, number of activity bouts, or time in moderate-to-vigorous intensity PA (MVPA), which are important general health markers and can often be represented as counts. These intensive within-subject count data provided by wearable devices, for example, minutes in MVPA summarized per hour across days and even months, allow the possibility for modeling not only the mean PA level, but also the dispersion level for each subject. Especially in the context of daily PA, subjects’ dispersion levels are potentially informative in reflecting their exercise patterns: some subjects might exhibit consistent PA across time and can be considered “less dispersed” subjects; while others might have a large amount of PA at a particular time point, while being sedentary for most of the day, and can be considered “more dispersed” subjects. Thus, we propose a negative binomial mixed effects location-scale model to model these intensive longitudinal PA counts and to account for the heterogeneity in both the mean and dispersion level across subjects. Further, to handle the issue of inflated numbers of zeros in the PA data, we also propose a hurdle/zero-inflated version which additionally includes the modeling of the probability of having |$>$| 0 PA levels.},
  archive      = {J_BIOMTC},
  author       = {Ma, Qianheng and Dunton, Genevieve F and Hedeker, Donald},
  doi          = {10.1093/biomtc/ujaf099},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf099},
  shortjournal = {Biometrics},
  title        = {Negative binomial mixed effects location-scale models for intensive longitudinal count-type physical activity data provided by wearable devices},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal machine learning for heterogeneous treatment effects in the presence of missing outcome data. <em>BIOMTC</em>, <em>81</em>(3), ujaf098. (<a href='https://doi.org/10.1093/biomtc/ujaf098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When estimating heterogeneous treatment effects, missing outcome data can complicate treatment effect estimation, causing certain subgroups of the population to be poorly represented. In this work, we discuss this commonly overlooked problem and consider the impact that missing at random outcome data has on causal machine learning estimators for the conditional average treatment effect (CATE). We propose 2 de-biased machine learning estimators for the CATE, the mDR-learner, and mEP-learner, which address the issue of under-representation by integrating inverse probability of censoring weights into the DR-learner and EP-learner, respectively. We show that under reasonable conditions, these estimators are oracle efficient and illustrate their favorable performance through simulated data settings, comparing them to existing CATE estimators, including comparison to estimators that use common missing data techniques. We present an example of their application using the GBSG2 trial, exploring treatment effect heterogeneity when comparing hormonal therapies to non-hormonal therapies among breast cancer patients post surgery, and offer guidance on the decisions a practitioner must make when implementing these estimators.},
  archive      = {J_BIOMTC},
  author       = {Pryce, Matthew and Diaz-Ordaz, Karla and Keogh, Ruth H and Vansteelandt, Stijn},
  doi          = {10.1093/biomtc/ujaf098},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf098},
  shortjournal = {Biometrics},
  title        = {Causal machine learning for heterogeneous treatment effects in the presence of missing outcome data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariance-on-covariance regression. <em>BIOMTC</em>, <em>81</em>(3), ujaf097. (<a href='https://doi.org/10.1093/biomtc/ujaf097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A covariance-on-covariance regression model is introduced in this manuscript. It is assumed that there exists (at least) a pair of linear projections on outcome covariance matrices and predictor covariance matrices such that a log-linear model links the variances in the projection spaces, as well as additional covariates of interest. An ordinary least square type of estimator is proposed to simultaneously identify the projections and estimate model coefficients. Under regularity conditions, the proposed estimator is asymptotically consistent. The superior performance of the proposed approach over existing methods is demonstrated via simulation studies. Applying to data collected in the Human Connectome Project Aging study, the proposed approach identifies 3 pairs of brain networks, where functional connectivity within the resting-state network predicts functional connectivity within the corresponding task-state network. The 3 networks correspond to a global signal network, a task-related network, and a task-unrelated network. The findings are consistent with existing knowledge about brain function.},
  archive      = {J_BIOMTC},
  author       = {Zhao, Yi and Zhao, Yize},
  doi          = {10.1093/biomtc/ujaf097},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf097},
  shortjournal = {Biometrics},
  title        = {Covariance-on-covariance regression},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Valid and efficient inference for nonparametric variable importance in two-phase studies. <em>BIOMTC</em>, <em>81</em>(3), ujaf095. (<a href='https://doi.org/10.1093/biomtc/ujaf095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a common nonparametric regression setting, where the data consist of a response variable Y , some easily obtainable covariates X ⁠ , and a set of costly covariates Z ⁠ . Before establishing predictive models for Y , a natural question arises: Is it worthwhile to include Z as predictors, given the additional cost of collecting data on Z for both training the models and predicting Y for future individuals? Therefore, we aim to conduct preliminary investigations to infer importance of Z in predicting Y in the presence of X ⁠ . To achieve this goal, we propose a nonparametric variable importance measure for Z ⁠ . It is defined as a parameter that aggregates maximum potential contributions of Z in single or multiple predictive models, with contributions quantified by general loss functions. Considering two-phase data that provide a large number of observations for ( Y , X ) with the expensive Z measured only in a small subsample, we develop a novel approach to infer the proposed importance measure, accommodating missingness of Z in the sample by substituting functions of ( Y , X ) for each individual’s contribution to the predictive loss of models involving Z ⁠ . Our approach attains unified and efficient inference regardless of whether Z makes zero or positive contribution to predicting Y , a desirable yet surprising property owing to data incompleteness. As intermediate steps of our theoretical development, we establish novel results in two relevant research areas, semi-supervised inference and two-phase nonparametric estimation. Numerical results from both simulated and real data demonstrate superior performance of our approach.},
  archive      = {J_BIOMTC},
  author       = {Dai, Guorong and Carroll, Raymond J and Chen, Jinbo},
  doi          = {10.1093/biomtc/ujaf095},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf095},
  shortjournal = {Biometrics},
  title        = {Valid and efficient inference for nonparametric variable importance in two-phase studies},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved prediction and flagging of extreme random effects for non-gaussian outcomes using weighted methods. <em>BIOMTC</em>, <em>81</em>(3), ujaf094. (<a href='https://doi.org/10.1093/biomtc/ujaf094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigators often focus on predicting extreme random effects from mixed effects models fitted to longitudinal or clustered data, and on identifying or “flagging” outliers such as poorly performing hospitals or rapidly deteriorating patients. Our recent work with Gaussian outcomes showed that weighted prediction methods can substantially reduce mean square error of prediction for extremes and substantially increase correct flagging rates compared to previous methods, while controlling the incorrect flagging rates. This paper extends the weighted prediction methods to non-Gaussian outcomes such as binary and count data. Closed-form expressions for predicted random effects and probabilities of correct and incorrect flagging are not available for the usual non-Gaussian outcomes, and the computational challenges are substantial. Therefore, our results include the development of theory to support algorithms that tune predictors that we call “self-calibrated” (which control the incorrect flagging rate using very simple flagging rules) and innovative numerical methods to calculate weighted predictors as well as to evaluate their performance. Comprehensive numerical evaluations show that the novel weighted predictors for non-Gaussian outcomes have substantially lower mean square error of prediction at the extremes and considerably higher correct flagging rates than previously proposed methods, while controlling the incorrect flagging rates. We illustrate our new methods using data on emergency room readmissions for children with asthma.},
  archive      = {J_BIOMTC},
  author       = {Neuhaus, John and McCulloch, Charles and Boylan, Ross},
  doi          = {10.1093/biomtc/ujaf094},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf094},
  shortjournal = {Biometrics},
  title        = {Improved prediction and flagging of extreme random effects for non-gaussian outcomes using weighted methods},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage estimators for spatial confounding with point-referenced data. <em>BIOMTC</em>, <em>81</em>(3), ujaf093. (<a href='https://doi.org/10.1093/biomtc/ujaf093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public health data are often spatially dependent, but standard spatial regression methods can suffer from bias and invalid inference when the independent variable is associated with spatially correlated residuals. This could occur if, for example, there is an unmeasured environmental contaminant associated with the independent and outcome variables in a spatial regression analysis. Geoadditive structural equation modeling (gSEM), in which an estimated spatial trend is removed from both the explanatory and response variables before estimating the parameters of interest, has previously been proposed as a solution but there has been little investigation of gSEM’s properties with point-referenced data. We link gSEM to results on double machine learning and semiparametric regression based on two-stage procedures. We propose using these semiparametric estimators for spatial regression using Gaussian processes with Matèrn covariance to estimate the spatial trends and term this class of estimators double spatial regression (DSR). We derive regularity conditions for root- n asymptotic normality and consistency and closed-form variance estimation, and show that in simulations where standard spatial regression estimators are highly biased and have poor coverage, DSR can mitigate bias more effectively than competitors and obtain nominal coverage.},
  archive      = {J_BIOMTC},
  author       = {Wiecha, Nate and Hoppin, Jane A and Reich, Brian J},
  doi          = {10.1093/biomtc/ujaf093},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf093},
  shortjournal = {Biometrics},
  title        = {Two-stage estimators for spatial confounding with point-referenced data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using model-assisted calibration methods to improve efficiency of regression analyses using two-phase samples or pooled samples under complex survey designs. <em>BIOMTC</em>, <em>81</em>(3), ujaf092. (<a href='https://doi.org/10.1093/biomtc/ujaf092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase sampling designs are frequently applied in epidemiological studies and large-scale health surveys. In such designs, certain variables are collected exclusively within a second-phase random subsample of the initial first-phase sample, often due to factors such as high costs, response burden, or constraints on data collection or assessment. Consequently, second-phase sample estimators can be inefficient due to the diminished sample size. Model-assisted calibration methods have been used to improve the efficiency of second-phase estimators in regression analysis. However, limited literature provides valid finite population inferences of the calibration estimators that use appropriate calibration auxiliary variables while simultaneously accounting for the complex sample designs in the first- and second-phase samples. Moreover, no literature considers the “pooled design” where some covariates are measured exclusively in certain repeated survey cycles. This paper proposes calibrating the sample weights for the second-phase sample to the weighted first-phase sample based on score functions of the regression model that uses predictions of the second-phase variable for the first-phase sample. We establish the consistency of estimation using calibrated weights and provide variance estimation for the regression coefficients under the two-phase design or the pooled design nested within complex survey designs. Empirical evidence highlights the efficiency and robustness of the proposed calibration compared to existing calibration and imputation methods. Data examples from the National Health and Nutrition Examination Survey are provided.},
  archive      = {J_BIOMTC},
  author       = {Wang, Lingxiao},
  doi          = {10.1093/biomtc/ujaf092},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf092},
  shortjournal = {Biometrics},
  title        = {Using model-assisted calibration methods to improve efficiency of regression analyses using two-phase samples or pooled samples under complex survey designs},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to “Propensity weighting plus adjustment in proportional hazards model is not doubly robust,” by erin e. gabriel, michael c. sachs, ingeborg waernbaum, els goetghebeur, paul f. blanche, stijn vansteelandt, arvid sjölander, and thomas scheike; volume 80, issue 3, september 2024, https://doi.org/10.1093/biomtc/ujae069. <em>BIOMTC</em>, <em>81</em>(3), ujaf091. (<a href='https://doi.org/10.1093/biomtc/ujaf091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Gabriel, Erin E and Sachs, Michael C and Waernbaum, Ingeborg and Goetghebeur, Els and Blanche, Paul F and Vansteelandt, Stijn and Sjölander, Arvid and Scheike, Thomas},
  doi          = {10.1093/biomtc/ujaf091},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf091},
  shortjournal = {Biometrics},
  title        = {Correction to “Propensity weighting plus adjustment in proportional hazards model is not doubly robust,” by erin e. gabriel, michael c. sachs, ingeborg waernbaum, els goetghebeur, paul f. blanche, stijn vansteelandt, arvid sjölander, and thomas scheike; volume 80, issue 3, september 2024, https://doi.org/10.1093/biomtc/ujae069},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjusted predictions for generalized estimating equations. <em>BIOMTC</em>, <em>81</em>(3), ujaf090. (<a href='https://doi.org/10.1093/biomtc/ujaf090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized estimating equations (GEEs) are a popular statistical method for longitudinal data analysis, requiring specification of the first 2 marginal moments of the response along with a working correlation matrix to capture temporal correlations within a cluster. When it comes to prediction at future/new time points using GEEs, a standard approach adopted by practitioners and software is to base it simply on the marginal mean model. In this article, we propose an alternative approach to prediction for independent cluster GEEs. By viewing the GEE as solving an iterative working linear model, we borrow ideas from universal kriging to construct an adjusted predictor that exploits working cross-correlations between the current and new observations within the same cluster. We establish theoretical conditions for the adjusted GEE predictor to outperform the standard GEE predictor. Simulations and an application to longitudinal data on the growth of sitka spruces demonstrate that, even when we misspecify the working correlation structure, adjusted GEE predictors can achieve better performance relative to standard GEE predictors, the so-called “oracle” GEE predictor using all time points, and potentially even cluster-specific predictions from a generalized linear mixed model.},
  archive      = {J_BIOMTC},
  author       = {Hui, Francis K C and Muller, Samuel and Welsh, Alan H},
  doi          = {10.1093/biomtc/ujaf090},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf090},
  shortjournal = {Biometrics},
  title        = {Adjusted predictions for generalized estimating equations},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tree-based additive noise directed acyclic graphical models for nonlinear causal discovery with interactions. <em>BIOMTC</em>, <em>81</em>(3), ujaf089. (<a href='https://doi.org/10.1093/biomtc/ujaf089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed acyclic graphical models with additive noises are essential in nonlinear causal discovery and have numerous applications in various domains, such as social science and systems biology. Most such models further assume that structural causal functions are additive to ensure causal identifiability and computational feasibility, which may be too restrictive in the presence of causal interactions. Some methods consider general nonlinear causal functions represented by, for example, Gaussian processes and neural networks, to accommodate interactions. However, they are either computationally intensive or lack interpretability. We propose a highly interpretable and computationally feasible approach using trees to incorporate interactions in nonlinear causal discovery, termed tree-based additive noise models. The nature of the tree construction leads to piecewise constant causal functions, making existing causal identifiability results of additive noise models with continuous and smooth causal functions inapplicable. Therefore, we provide new conditions under which the proposed model is identifiable. We develop a recursive algorithm for source node identification and a score-based ordering search algorithm. Through extensive simulations, we demonstrate the utility of the proposed model and algorithms benchmarking against existing additive noise models, especially when there are strong causal interactions. Our method is applied to infer a protein–protein interaction network for breast cancer, where proteins may form protein complexes to perform their functions.},
  archive      = {J_BIOMTC},
  author       = {Zhou, Fangting and He, Kejun and Ni, Yang},
  doi          = {10.1093/biomtc/ujaf089},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf089},
  shortjournal = {Biometrics},
  title        = {Tree-based additive noise directed acyclic graphical models for nonlinear causal discovery with interactions},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simple simulation based reconstruction of incidence rates from death data. <em>BIOMTC</em>, <em>81</em>(3), ujaf088. (<a href='https://doi.org/10.1093/biomtc/ujaf088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Daily deaths from an infectious disease provide a means for retrospectively inferring daily incidence, given knowledge of the infection-to-death interval distribution. Existing methods for doing so rely either on fitting simplified non-linear epidemic models to the deaths data or on spline based deconvolution approaches. The former runs the risk of introducing unintended artefacts via the model formulation, while the latter may be viewed as technically obscure, impeding uptake by practitioners. This note proposes a simple simulation based approach to inferring fatal incidence from deaths that requires minimal assumptions, is easy to understand, and allows testing of alternative hypothesized incidence trajectories. The aim is that in any future situation similar to the COVID pandemic, the method can be easily, rapidly, transparently, and uncontroversially deployed as an input to management.},
  archive      = {J_BIOMTC},
  author       = {Wood, Simon N},
  doi          = {10.1093/biomtc/ujaf088},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf088},
  shortjournal = {Biometrics},
  title        = {Simple simulation based reconstruction of incidence rates from death data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A flexible framework for N-mixture occupancy models: Applications to breeding bird surveys. <em>BIOMTC</em>, <em>81</em>(3), ujaf087. (<a href='https://doi.org/10.1093/biomtc/ujaf087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating species abundance under imperfect detection is a key challenge in biodiversity conservation. The N -mixture model, widely recognized for its ability to distinguish between abundance and individual detection probability without marking individuals, is constrained by its stringent closure assumption, which leads to biased estimates when violated in real-world settings. To address this limitation, we propose an extended framework based on a development of the mixed Gamma-Poisson model, incorporating a community parameter that represents the proportion of individuals consistently present throughout the survey period. This flexible framework generalizes both the zero-inflated type occupancy model and the standard N -mixture model as special cases, corresponding to community parameter values of 0 and 1, respectively. The model’s effectiveness is validated through simulations and applications to real-world datasets, specifically with 5 species from the North American Breeding Bird Survey and 46 species from the Swiss Breeding Bird Survey, demonstrating its improved accuracy and adaptability in settings where strict closure may not hold.},
  archive      = {J_BIOMTC},
  author       = {Huynh, Huu-Dinh and Royle, J Andrew and Hwang, Wen-Han},
  doi          = {10.1093/biomtc/ujaf087},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf087},
  shortjournal = {Biometrics},
  title        = {A flexible framework for N-mixture occupancy models: Applications to breeding bird surveys},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple tests for restricted mean time lost with competing risks data. <em>BIOMTC</em>, <em>81</em>(3), ujaf086. (<a href='https://doi.org/10.1093/biomtc/ujaf086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Easy-to-interpret effect estimands are highly desirable in survival analysis. In the competing risks framework, one good candidate is the restricted mean time lost (RMTL). It is defined as the area under the cumulative incidence function up to a prespecified time point and, thus, it summarizes the cumulative incidence function into a meaningful estimand. While existing RMTL-based tests are limited to 2-sample comparisons and mostly to 2 event types, we aim to develop general contrast tests for factorial designs and an arbitrary number of event types based on a Wald-type test statistic. Furthermore, we avoid the often-made, rather restrictive continuity assumption on the event time distribution. This allows for ties in the data, which often occur in practical applications, for example, when event times are measured in whole days. In addition, we develop more reliable tests for RMTL comparisons that are based on a permutation approach to improve the small sample performance. In a second step, multiple tests for RMTL comparisons are developed to test several null hypotheses simultaneously. Here, we incorporate the asymptotically exact dependence structure between the local test statistics to gain more power. The small sample performance of the proposed testing procedures is analyzed in simulations and finally illustrated by analyzing a real-data example about leukemia patients who underwent bone marrow transplantation.},
  archive      = {J_BIOMTC},
  author       = {Munko, Merle and Dobler, Dennis and Ditzhaus, Marc},
  doi          = {10.1093/biomtc/ujaf086},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf086},
  shortjournal = {Biometrics},
  title        = {Multiple tests for restricted mean time lost with competing risks data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A positivity robust strategy to study effects of switching treatment. <em>BIOMTC</em>, <em>81</em>(3), ujaf085. (<a href='https://doi.org/10.1093/biomtc/ujaf085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In studies of medical treatments, individuals often experience post-treatment events that predict their future outcomes. In this work, we study how to use initial observations of a recurrent event—a type of post-treatment event—to offer updated treatment recommendations in settings where no, or few, individuals are observed to switch between treatment arms. Specifically, we formulate an estimand quantifying the average effect of switching treatment on subsequent events. We derive bounds on the value of this estimand under plausible conditions and propose non-parametric estimators of the bounds. Furthermore, we define a value and regret function for a dynamic treatment-switching regime, and use these to determine 3 types of optimal regimes under partial identification: the pessimist (maximin value), optimist (maximax value), and opportunist (minimax regret) regimes. The pessimist regime is guaranteed to perform at least as well as the standard of care. We apply our methods to data from the Systolic Blood Pressure Intervention Trial.},
  archive      = {J_BIOMTC},
  author       = {Janvin, Matias and Ryalen, Pål C and Sarvet, Aaron L and Stensrud, Mats J},
  doi          = {10.1093/biomtc/ujaf085},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf085},
  shortjournal = {Biometrics},
  title        = {A positivity robust strategy to study effects of switching treatment},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust nonparametric estimators of the predictive value of covariates for survival data. <em>BIOMTC</em>, <em>81</em>(3), ujaf084. (<a href='https://doi.org/10.1093/biomtc/ujaf084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The predictive value of a covariate is often of interest in studies with a survival endpoint. A common situation is that there are some well established predictors and a potential valuable new marker. The challenge is how to judge the potentially added predictive value of this new marker. We propose to use the positive predictive value (PPV) curve based on a nonparametric scoring rule. The estimand of interest is viewed as a single transformation of the underlying data generating probability measure, which allows us to develop a robust nonparametric estimator of the PPV by first calculating the corresponding efficient influence function. We provide asymptotic results and illustrate the approach with numerical studies and with 2 cancer data studies.},
  archive      = {J_BIOMTC},
  author       = {Martinussen, Torben and van der Laan, Mark J},
  doi          = {10.1093/biomtc/ujaf084},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf084},
  shortjournal = {Biometrics},
  title        = {Doubly robust nonparametric estimators of the predictive value of covariates for survival data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency band analysis of nonstationary multivariate time series. <em>BIOMTC</em>, <em>81</em>(3), ujaf083. (<a href='https://doi.org/10.1093/biomtc/ujaf083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information from frequency bands in biomedical time series provides useful summaries of the observed signal. Many existing methods consider summaries of the time series obtained over a few well-known, pre-defined frequency bands of interest. However, there is a dearth of data-driven methods for identifying frequency bands that optimally summarize frequency-domain information in the time series. A new method to identify partition points in the frequency space of a multivariate locally stationary time series is proposed. These partition points signify changes across frequencies in the time-varying behavior of the signal and provide frequency band summary measures that best preserve nonstationary dynamics of the observed series. An |$L_2$| -norm based discrepancy measure that finds differences in the time-varying spectral density matrix is constructed, and its asymptotic properties are derived. New nonparametric bootstrap tests are also provided to identify significant frequency partition points and to identify components and cross-components of the spectral matrix exhibiting changes over frequencies. Finite-sample performance of the proposed method is illustrated via simulations. The proposed method is used to develop optimal frequency band summary measures for characterizing time-varying behavior in resting-state electroencephalography time series, as well as identifying components and cross-components associated with each frequency partition point.},
  archive      = {J_BIOMTC},
  author       = {Sundararajan, Raanju R and Bruce, Scott A},
  doi          = {10.1093/biomtc/ujaf083},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf083},
  shortjournal = {Biometrics},
  title        = {Frequency band analysis of nonstationary multivariate time series},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse 2-stage bayesian meta-analysis for individualized treatments. <em>BIOMTC</em>, <em>81</em>(3), ujaf082. (<a href='https://doi.org/10.1093/biomtc/ujaf082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment rules tailor treatments to patients based on clinical, demographic, and other characteristics. Estimation of individualized treatment rules requires the identification of individuals who benefit most from the particular treatments and thus the detection of variability in treatment effects. To develop an effective individualized treatment rule, data from multisite studies may be required due to the low power provided by smaller datasets for detecting the often small treatment-covariate interactions. However, sharing of individual-level data is sometimes constrained. Furthermore, sparsity may arise in 2 senses: different data sites may recruit from different populations, making it infeasible to estimate identical models or all parameters of interest at all sites, and the number of non-zero parameters in the model for the treatment rule may be small. To address these issues, we adopt a 2-stage Bayesian meta-analysis approach to estimate individualized treatment rules which optimize expected patient outcomes using multisite data without disclosing individual-level data beyond the sites. Simulation results demonstrate that our approach can provide consistent estimates of the parameters which fully characterize the optimal individualized treatment rule. We estimate the optimal Warfarin dose strategy using data from the International Warfarin Pharmacogenetics Consortium, where data sparsity and small treatment-covariate interaction effects pose additional statistical challenges.},
  archive      = {J_BIOMTC},
  author       = {Shen, Junwei and Moodie, Erica E M and Golchi, Shirin},
  doi          = {10.1093/biomtc/ujaf082},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf082},
  shortjournal = {Biometrics},
  title        = {Sparse 2-stage bayesian meta-analysis for individualized treatments},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on age-specific fertility in ecology and evolution. learning from other disciplines and improving the state of the art. <em>BIOMTC</em>, <em>81</em>(3), ujaf081. (<a href='https://doi.org/10.1093/biomtc/ujaf081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the importance of age-specific fertility for ecology and evolution, the methods for modeling and inference have proven considerably limited. However, other disciplines have long focused on exploring and developing a vast number of models. Here, I provide an overview of the different models proposed since the 1940s by formal demographers, statisticians, and social scientists, most of which are unknown to the ecological and evolutionary communities. I describe how these fall into 2 main categories, namely polynomials and those based on probability density functions. I discuss their merits in terms of their overall behavior and how well they represent the different stages of fertility. Despite many alternative models, inference on age-specific fertility has usually been limited to simple least squares. Although this might be sufficient for human data, I hope to demonstrate that inference requires more sophisticated approaches for ecological and evolutionary datasets. To illustrate how inference and model choice can be achieved on different types of typical ecological and evolutionary data, I present the new R package Bayesian Fertility Trajectory Analysis, which I apply to published aggregated data for lions and baboons. I then conduct a simulation study to test its performance on individual-level data. I show that appropriate inference and model selection can be achieved even when a small number of parents are followed.},
  archive      = {J_BIOMTC},
  author       = {Colchero, Fernando},
  doi          = {10.1093/biomtc/ujaf081},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf081},
  shortjournal = {Biometrics},
  title        = {Inference on age-specific fertility in ecology and evolution. learning from other disciplines and improving the state of the art},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to environmental data science by jerry d. davis, chapman and Hall/CRC, 2023, ISBN: 9781003317821 https://www.routledge.com/Introduction-to-environmental-data-science/Davis/p/book/9781003317821. <em>BIOMTC</em>, <em>81</em>(3), ujaf079. (<a href='https://doi.org/10.1093/biomtc/ujaf079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Sheppard, Lianne and Blanco, Magali N},
  doi          = {10.1093/biomtc/ujaf079},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf079},
  shortjournal = {Biometrics},
  title        = {Introduction to environmental data science by jerry d. davis, chapman and Hall/CRC, 2023, ISBN: 9781003317821 https://www.routledge.com/Introduction-to-environmental-data-science/Davis/p/book/9781003317821},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Change point analysis for time series by lajos horváth and gregory rice, springer, 2024, ISBN: 9783031516085https://link.springer.com/book/10.1007/978-3-031-51609-2. <em>BIOMTC</em>, <em>81</em>(3), ujaf078. (<a href='https://doi.org/10.1093/biomtc/ujaf078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Han, Fang},
  doi          = {10.1093/biomtc/ujaf078},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf078},
  shortjournal = {Biometrics},
  title        = {Change point analysis for time series by lajos horváth and gregory rice, springer, 2024, ISBN: 9783031516085https://link.springer.com/book/10.1007/978-3-031-51609-2},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of gene therapies: Strategic, scientific, regulatory, and access considerations by avery McIntosh and oleksandr sverdlov, chapman and Hall/CRC, 2024, ISBN: 9781032136554 https://www.routledge.com/Development-of-gene-therapies-strategic-scientific-regulatory-and-access-Considerations/McIntosh-sverdlov/p/book/9781032136554. <em>BIOMTC</em>, <em>81</em>(3), ujaf070. (<a href='https://doi.org/10.1093/biomtc/ujaf070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yuan, Ying},
  doi          = {10.1093/biomtc/ujaf070},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf070},
  shortjournal = {Biometrics},
  title        = {Development of gene therapies: Strategic, scientific, regulatory, and access considerations by avery McIntosh and oleksandr sverdlov, chapman and Hall/CRC, 2024, ISBN: 9781032136554 https://www.routledge.com/Development-of-gene-therapies-strategic-scientific-regulatory-and-access-Considerations/McIntosh-sverdlov/p/book/9781032136554},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cumulative incidence function estimation using population-based biobank data. <em>BIOMTC</em>, <em>81</em>(3), ujaf049. (<a href='https://doi.org/10.1093/biomtc/ujaf049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many countries have established population-based biobanks, which are being used increasingly in epidemiological and clinical research. These biobanks offer opportunities for large-scale studies addressing questions beyond the scope of traditional clinical trials or cohort studies. However, using biobank data poses new challenges. Typically, biobank data are collected from a study cohort recruited over a defined calendar period, with subjects entering the study at various ages falling between c L and c U ⁠ . This work focuses on biobank data with individuals reporting disease-onset age upon recruitment, termed prevalent data, along with individuals initially recruited as healthy, and their disease onset observed during the follow-up period. We propose a novel cumulative incidence function (CIF) estimator that efficiently incorporates prevalent cases, in contrast to existing methods, providing two advantages: (1) increased efficiency and (2) CIF estimation for ages before the lower limit, c L ⁠ .},
  archive      = {J_BIOMTC},
  author       = {Gorfine, Malka and Zucker, David M and Shoham, Shoval},
  doi          = {10.1093/biomtc/ujaf049},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf049},
  shortjournal = {Biometrics},
  title        = {Cumulative incidence function estimation using population-based biobank data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precision generalized phase I-II designs. <em>BIOMTC</em>, <em>81</em>(3), ujaf043. (<a href='https://doi.org/10.1093/biomtc/ujaf043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new family of precision Bayesian dose optimization designs, PGen I-II, based on early efficacy, early toxicity, and long-term time to treatment failure is proposed. A PGen I-II design refines a Gen I-II design by accounting for patient heterogeneity characterized by subgroups that may be defined by prognostic levels, disease subtypes, or biomarker categories. The design makes subgroup-specific decisions, which may be to drop an unacceptably toxic or inefficacious dose, randomize patients among acceptable doses, or identify a best dose in terms of treatment success defined in terms of time to failure over long-term follow-up. A piecewise exponential distribution for failure time is assumed, including subgroup-specific effects of dose, response, and toxicity. Latent variables are used to adaptively cluster subgroups found to have similar dose-outcome distributions, with the model simplified to borrow strength between subgroups in the same cluster. Guidelines and user-friendly computer software for implementing the design are provided. A simulation study is reported that shows the PGen I-II design is superior to similarly structured designs that either assume patient homogeneity or conduct separate trials within subgroups.},
  archive      = {J_BIOMTC},
  author       = {Zhao, Saijun and Thall, Peter F and Yuan, Ying and Lee, Juhee and Msaouel, Pavlos and Zang, Yong},
  doi          = {10.1093/biomtc/ujaf043},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf043},
  shortjournal = {Biometrics},
  title        = {Precision generalized phase I-II designs},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture models: Parametric, semiparametric, and new directions by weixin yao and sijia xiang, chapman and Hall/CRC, 2024, ISBN: 9780367481827 https://www.routledge.com/Mixture-models-parametric-semiparametric-and-new-directions/Yao-xiang/p/book/9780367481827. <em>BIOMTC</em>, <em>81</em>(3), ujaf031. (<a href='https://doi.org/10.1093/biomtc/ujaf031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Chen, Li-Pang},
  doi          = {10.1093/biomtc/ujaf031},
  journal      = {Biometrics},
  month        = {9},
  number       = {3},
  pages        = {ujaf031},
  shortjournal = {Biometrics},
  title        = {Mixture models: Parametric, semiparametric, and new directions by weixin yao and sijia xiang, chapman and Hall/CRC, 2024, ISBN: 9780367481827 https://www.routledge.com/Mixture-models-parametric-semiparametric-and-new-directions/Yao-xiang/p/book/9780367481827},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging two-phase data for improved prediction of survival outcomes with application to nasopharyngeal cancer. <em>BIOMTC</em>, <em>81</em>(2), ujaf080. (<a href='https://doi.org/10.1093/biomtc/ujaf080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate survival predicting models are essential for improving targeted cancer therapies and clinical care among cancer patients. In this article, we investigate and develop a method to improve predictions of survival in cancer by leveraging two-phase data with expert knowledge and prognostic index. Our work is motivated by two-phase data in nasopharyngeal cancer (NPC), where traditional covariates are readily available for all subjects, but the primary viral factor, human papillomavirus (HPV), is substantially missing. To address this challenge, we propose an expert-guided method that incorporates prognostic index based on the observed covariates and clinical importance of key factors. The proposed method makes efficient use of available data, not simply discarding patients with unknown HPV status. We apply the proposed method and evaluate it against other existing approaches through a series of simulation studies and real data example of NPC patients. Under various settings, the proposed method consistently outperforms competing methods in terms of c-index, calibration slope, and integrated Brier score. By efficiently leveraging two-phase data, the model provides a more accurate and reliable predictive ability of survival models.},
  archive      = {J_BIOMTC},
  author       = {Oh, Eun Jeong and Ahn, Seungjun and Tham, Tristan and Qian, Min},
  doi          = {10.1093/biomtc/ujaf080},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf080},
  shortjournal = {Biometrics},
  title        = {Leveraging two-phase data for improved prediction of survival outcomes with application to nasopharyngeal cancer},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COCA: A randomized bayesian design integrating dose optimization and component contribution assessment for combination therapies. <em>BIOMTC</em>, <em>81</em>(2), ujaf077. (<a href='https://doi.org/10.1093/biomtc/ujaf077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cancer treatment, the development of combination therapies requires demonstrating the contribution of each individual drug and optimizing the dose during early-phase trials. This necessitates a large sample size, presenting formidable obstacles for drug developers. To address this issue, we propose a 2-stage randomized phase II design that seamlessly integrates combination dose optimization with component contribution assessment. In stage 1, the optimal combination dose is determined by maximizing the risk–benefit tradeoff across multiple candidate combination doses. In stage 2, a multi-arm randomized phase is initiated to evaluate the contribution of each component within the combination therapy. To increase trial efficiency and reduce the sample size, efficacy data from both stages are adaptively combined using a Bayesian logistic regression model with a spike-and-slab prior. The sample size and decision cutoffs of the proposed design are systematically determined based on a novel calibration procedure to achieve desired operating characteristics. Extensive simulation studies show that the proposed design achieves the dual goals of dose optimization and contribution assessment, while yielding substantial sample size savings compared to competing designs.},
  archive      = {J_BIOMTC},
  author       = {Chi, Xiaohan and Lin, Ruitao and Yuan, Ying},
  doi          = {10.1093/biomtc/ujaf077},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf077},
  shortjournal = {Biometrics},
  title        = {COCA: A randomized bayesian design integrating dose optimization and component contribution assessment for combination therapies},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularized principal spline functions to mitigate spatial confounding. <em>BIOMTC</em>, <em>81</em>(2), ujaf076. (<a href='https://doi.org/10.1093/biomtc/ujaf076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new approach to address the problem of unmeasured confounding in spatial designs. Spatial confounding occurs when some confounding variables are unobserved and not included in the model, leading to distorted inferential results about the effect of an exposure on an outcome. We show the relationship existing between the confounding bias of a non-spatial model and that of a semi-parametric model that includes a basis matrix to represent the unmeasured confounder conditional on the exposure. This relationship holds for any basis expansion; however, it is shown that using the semi-parametric approach guarantees a reduction in the confounding bias only under certain circumstances, which are related to the spatial structures of the exposure and the unmeasured confounder, the type of basis expansion utilized, and the regularization mechanism. To adjust for spatial confounding, and therefore try to recover the effect of interest, we propose a Bayesian semi-parametric regression model, where an expansion matrix of principal spline basis functions is used to approximate the unobserved factor, and spike-and-slab priors are imposed on the respective expansion coefficients in order to select the most important bases. From the results of an extensive simulation study, we conclude that our proposal is able to reduce the confounding bias more than competing approaches, and it also seems more robust to bias amplification.},
  archive      = {J_BIOMTC},
  author       = {Zaccardi, Carlo and Valentini, Pasquale and Ippoliti, Luigi and Schmidt, Alexandra M},
  doi          = {10.1093/biomtc/ujaf076},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf076},
  shortjournal = {Biometrics},
  title        = {Regularized principal spline functions to mitigate spatial confounding},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power calculation for cross-sectional stepped wedge cluster randomized trials with a time-to-event endpoint. <em>BIOMTC</em>, <em>81</em>(2), ujaf074. (<a href='https://doi.org/10.1093/biomtc/ujaf074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster randomized trials (SW-CRTs) are a form of randomized trial whereby clusters are progressively transitioned from control to intervention, with the timing of transition randomized for each cluster. An important task at the design stage is to ensure that the planned trial has sufficient power. While methods for determining power have been well-developed for SW-CRTs with continuous and binary outcomes, limited methods for power calculation are available for SW-CRTs with censored time-to-event outcomes. In this article, we propose a stratified marginal Cox model to analyze cross-sectional SW-CRTs and then derive an explicit expression of the robust sandwich variance to facilitate power calculations without the need for computationally intensive simulations. Power formulas based on both the Wald and robust score tests are developed, assuming constant within-period and between-period correlation parameters, and are further validated via simulation under different finite-sample scenarios. Finally, we illustrate our methods in the context of a SW-CRT testing the effect of a new electronic reminder system on time to catheter removal in hospital settings. We also offer an R Shiny application to facilitate sample size and power calculations using our proposed methods.},
  archive      = {J_BIOMTC},
  author       = {Ryan Baumann, Mary and Esserman, Denise and Taljaard, Monica and Li, Fan},
  doi          = {10.1093/biomtc/ujaf074},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf074},
  shortjournal = {Biometrics},
  title        = {Power calculation for cross-sectional stepped wedge cluster randomized trials with a time-to-event endpoint},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of platform trials with a change in the control treatment arm. <em>BIOMTC</em>, <em>81</em>(2), ujaf073. (<a href='https://doi.org/10.1093/biomtc/ujaf073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform trials are an efficient way of testing multiple treatments. We consider platform trials where, if a treatment is found to be superior to the control, it will become the new standard of care. The remaining treatments are then tested against this new control. In this setting, one can either keep the information on both the new standard of care and the other active treatments before the control is changed or discard this information when testing for benefit of the remaining treatments. We show analytically and numerically, retaining the information collected before the change in control can be detrimental to the power in a frequentist multi-arm multi-stage trial. Specifically, we consider the overall power, the probability that the active treatment with the greatest treatment effect is found during the trial, and the conditional power, the probability a given treatment is found superior against the current control. Also studied is the conditional type I error, the probability a given treatment is incorrectly found superior against the current control. We prove when retaining the information decreases both the overall and conditional power but also decreases the conditional type I error. A motivating example is then studied. Based on these observations, we discuss different aspects to consider when deciding whether to run a continuous platform trial or run an inherently new trial using the same trial infrastructure.},
  archive      = {J_BIOMTC},
  author       = {Greenstreet, Peter and Jaki, Thomas and Bedding, Alun and Mozgunov, Pavel},
  doi          = {10.1093/biomtc/ujaf073},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf073},
  shortjournal = {Biometrics},
  title        = {Design of platform trials with a change in the control treatment arm},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-parametric estimators of hazard ratios for comparing two survival curves. <em>BIOMTC</em>, <em>81</em>(2), ujaf072. (<a href='https://doi.org/10.1093/biomtc/ujaf072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose non-parametric estimators of the hazard ratio for comparing two survival curves using estimating equations defined in terms of group-specific cumulative hazard functions. We first describe the methods and their asymptotic properties in the case of a constant hazard ratio. We then extend the methods and the asymptotic results when the hazard ratio is time dependent and well approximated by a locally constant function. We propose a method to select the change points in the local hazard ratios. We extend the methods to stratified estimators and propose tests for heterogeneity of constant and time-dependent hazard ratios across strata. In a simulation study, we describe the finite sample properties of the proposed estimators and compare their performance with the Cox partial maximum likelihood estimator (MLE) in terms of efficiency and accuracy of coverage rates. An example is provided to illustrate an application of the proposed methods in practice.},
  archive      = {J_BIOMTC},
  author       = {Giurcanu, Mihai and Karrison, Theodore},
  doi          = {10.1093/biomtc/ujaf072},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf072},
  shortjournal = {Biometrics},
  title        = {Non-parametric estimators of hazard ratios for comparing two survival curves},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient and interpretable assumption-lean generalized linear modeling of continuous exposure effects. <em>BIOMTC</em>, <em>81</em>(2), ujaf071. (<a href='https://doi.org/10.1093/biomtc/ujaf071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in causal inference have largely ignored continuous exposures, apart from model-based approaches, which face criticism due to potential model misspecification. Model-free approaches based on modified treatment policies, such as uniformly shifting each subject’s observed exposure, have emerged as promising alternatives. However, because such interventions are impractical, it is necessary to evaluate a range of possible shifts to generate actionable insights. To address this, we introduce models that parameterize the effects of shift interventions across varying magnitudes, coupled with assumption-lean estimation strategies. To ensure validity and interpretability under model misspecification, we tailor these to minimize (squared) bias in estimating the effects of realistic shifts. We employ debiased machine learning procedures for this but observe them to exhibit erratic behavior under certain data-generating mechanisms, prompting two key innovations. First, we propose a broadly applicable debiasing procedure that yields estimators with significantly improved finite-sample properties and is of independent methodological interest. Second, we develop debiased machine learning estimators for estimands with a more favorable efficiency bound, but more nuanced interpretation when models are misspecified. Unlike existing projection estimators, our methods avoid inverse exposure density weighting and do not demand tailored shift interventions to address positivity violations. Extensive simulations and a re-analysis of the Bangladesh Wash Benefits study demonstrate the effectiveness, stability, and utility of our approach. This work advances assumption-lean methods that balance validity, interpretability, and efficiency.},
  archive      = {J_BIOMTC},
  author       = {Vansteelandt, Stijn},
  doi          = {10.1093/biomtc/ujaf071},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf071},
  shortjournal = {Biometrics},
  title        = {Towards efficient and interpretable assumption-lean generalized linear modeling of continuous exposure effects},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the finite-sample and asymptotic error control of a randomization-probability test for response-adaptive clinical trials. <em>BIOMTC</em>, <em>81</em>(2), ujaf069. (<a href='https://doi.org/10.1093/biomtc/ujaf069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is now commonly known that using response-adaptive designs for data collection offers great potential in terms of optimizing expected outcomes, but poses multiple challenges for inferential goals. In many settings, such as phase-II or confirmatory clinical trials, a main barrier to their practical use is the lack of type-I error guarantees and/or power efficiency, especially in finite samples. This work addresses this gap. Specifically, focusing on a novel test statistic defined on the randomization probabilities of the (randomized) adaptive design, we derive its finite-sample and asymptotic guarantees. Further theoretical properties are evaluated for Thompson sampling, a Bayesian response-adaptive design that is commonly used both in clinical applications and beyond (eg, recommendation systems or mobile health). The frequentist error control advantages of the proposed approach—also able to preserve expected outcome optimalities—are illustrated in a real-world phase-II oncology trial and in simulation experiments.},
  archive      = {J_BIOMTC},
  author       = {Deliu, Nina and Villar, Sofia S},
  doi          = {10.1093/biomtc/ujaf069},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf069},
  shortjournal = {Biometrics},
  title        = {On the finite-sample and asymptotic error control of a randomization-probability test for response-adaptive clinical trials},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variant specific treatment effects with applications in vaccine studies. <em>BIOMTC</em>, <em>81</em>(2), ujaf068. (<a href='https://doi.org/10.1093/biomtc/ujaf068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pathogens usually exist in heterogeneous variants, like subtypes and strains. Quantifying treatment effects on the different variants is important for guiding prevention policies and vaccine development. Here, we ground analyses of variant-specific effects on a formal framework for causal inference. This allows us to clarify the interpretation of existing methods and define new estimands. Unlike most of the existing literature, we explicitly consider the (realistic) setting with interference in the target population: even if individuals can be sensibly perceived as iid in randomized trial data, there will often be interference in the target population where treatments, such as vaccines, are rolled out. Thus, one of our contributions is to derive explicit conditions guaranteeing that commonly reported vaccine efficacy parameters quantify well-defined causal effects, also in the presence of interference. Furthermore, our results give alternative justifications for reporting estimands on the relative, rather than absolute, scale. We illustrate the findings with an analysis of a large HIV1 vaccine trial, where there is interest in distinguishing vaccine effects on viruses with different genome sequences.},
  archive      = {J_BIOMTC},
  author       = {Perényi, Gellért and Stensrud, Mats},
  doi          = {10.1093/biomtc/ujaf068},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf068},
  shortjournal = {Biometrics},
  title        = {Variant specific treatment effects with applications in vaccine studies},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating optimally tailored active surveillance strategy under interval censoring. <em>BIOMTC</em>, <em>81</em>(2), ujaf067. (<a href='https://doi.org/10.1093/biomtc/ujaf067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active surveillance (AS) using repeated biopsies to monitor disease progression has been a popular alternative to immediate surgical intervention in cancer care. However, a biopsy procedure is invasive and sometimes leads to severe side effects of infection and bleeding. To reduce the burden of repeated surveillance biopsies, biomarker-assistant decision rules are sought to replace the fix-for-all regimen with tailored biopsy intensity for individual patients. Constructing or evaluating such decision rules is challenging. The key AS outcome is often ascertained subject to interval censoring. Furthermore, patients will discontinue participation in the AS study once they receive a positive surveillance biopsy. Thus, patient dropout is affected by the outcomes of these biopsies. This work proposes a non-parametric kernel-based method to estimate a tailored AS strategy’s true positive rates (TPRs) and true negative rates (TNRs), accounting for interval censoring and immediate dropouts. We develop a weighted classification framework based on these estimates to estimate the optimally tailored AS strategy and further incorporate the cost-benefit ratio for cost-effectiveness in medical decision-making. Theoretically, we provide a uniform generalization error bound of the derived AS strategy, accommodating all possible trade-offs between TPRs and TNRs. Simulation and application to a prostate cancer surveillance study show the superiority of the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Liang, Muxuan and Zhao, Yingqi and Lin, Daniel W and Cooperberg, Matthew and Zheng, Yingye},
  doi          = {10.1093/biomtc/ujaf067},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf067},
  shortjournal = {Biometrics},
  title        = {Estimating optimally tailored active surveillance strategy under interval censoring},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric assessment of regimen response curve estimators. <em>BIOMTC</em>, <em>81</em>(2), ujaf066. (<a href='https://doi.org/10.1093/biomtc/ujaf066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the framework of dynamic marginal structural models, regimen-response curve is a function that describes the relation between the mean outcome and the parameters in the class of decision rules. The modeling choice of the regimen-response curve is crucial in constructing an optimal regime, as a misspecified model can lead to a biased estimate with questionable causal interpretability. However, the existing literature lacks methods to evaluate and compare different working models. To address this problem, we will leverage risk to assess the “goodness-of-fit” of an imposed working model. We consider the counterfactual risk as our target parameter and derive inverse probability weighting and canonical gradients to map it to the observed data. We provide asymptotic properties of the resulting risk estimators, considering both fixed and data-dependent target parameters. We will show that the inverse probability weighting estimator can be efficient and asymptotic linear when the weight functions are estimated using a sieve-based estimator. The proposed method is implemented on the LS1 study to estimate a regimen-response curve for patients with Parkinson’s disease.},
  archive      = {J_BIOMTC},
  author       = {Pham, Cuong T and Baer, Benjamin R and Ertefaie, Ashkan},
  doi          = {10.1093/biomtc/ujaf066},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf066},
  shortjournal = {Biometrics},
  title        = {Nonparametric assessment of regimen response curve estimators},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic exponential family inverse regression and its applications. <em>BIOMTC</em>, <em>81</em>(2), ujaf065. (<a href='https://doi.org/10.1093/biomtc/ujaf065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advances in high-throughput sequencing technologies have led to the fast accumulation of high-dimensional data, which is harnessed for understanding the implications of various factors on human disease and health. While dimension reduction plays an essential role in high-dimensional regression and classification, existing methods often require the predictors to be continuous, making them unsuitable for discrete data, such as presence-absence records of species in community ecology and sequencing reads in single-cell studies. To identify and estimate sufficient reductions in regressions with discrete predictors, we introduce probabilistic exponential family inverse regression (PrEFIR), assuming that, given the response and a set of latent factors, the predictors follow one-parameter exponential families. We show that the low-dimensional reductions result not only from the response variable but also from the latent factors. We further extend the latent factor modeling framework to the double exponential family by including an additional parameter to account for the dispersion. This versatile framework encompasses regressions with all categorical or a mixture of categorical and continuous predictors. We propose the method of maximum hierarchical likelihood for estimation, and develop a highly parallelizable algorithm for its computation. The effectiveness of PrEFIR is demonstrated through simulation studies and real data examples.},
  archive      = {J_BIOMTC},
  author       = {Pang, Daolin and Zhu, Ruoqing and Zhao, Hongyu and Wang, Tao},
  doi          = {10.1093/biomtc/ujaf065},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf065},
  shortjournal = {Biometrics},
  title        = {Probabilistic exponential family inverse regression and its applications},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric joint modeling for biomarker trajectory before disease onset. <em>BIOMTC</em>, <em>81</em>(2), ujaf064. (<a href='https://doi.org/10.1093/biomtc/ujaf064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how biomarkers change in relation to disease pathogenesis is a key area in biomedical research. We propose a semiparametric joint model to analyze the temporal evolution of biomarkers prior to the onset of disease. The model allows for a flexible biomarker trajectory that depends on two time scales: a natural time scale such as age and time to disease onset. In practice, the natural time scale often differs from time-on-study, leading to analytical challenges such as left-truncation bias. We introduce a profile kernel estimating equation approach to estimate regression coefficients and unspecified baseline mean trajectory functions. We establish the large-sample properties of the proposed estimators and conduct simulation studies to evaluate their finite-sample performance. Our method is applied to investigate brain biomarker trajectories before the onset of preclinical Alzheimer’s disease. We observed a decline in cortical thickness prior to disease onset across brain regions, with APOE4 carriers showing lower levels compared to non-carriers.},
  archive      = {J_BIOMTC},
  author       = {Sun, Yifei and Zhao, Xiwen and Chan, Kwun Chuen Gary and Xu, Wanwan and Allore, Heather and Zhao, Yize},
  doi          = {10.1093/biomtc/ujaf064},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf064},
  shortjournal = {Biometrics},
  title        = {Semiparametric joint modeling for biomarker trajectory before disease onset},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal predictive intervals in survival analysis: A resampling approach. <em>BIOMTC</em>, <em>81</em>(2), ujaf063. (<a href='https://doi.org/10.1093/biomtc/ujaf063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution-free method of conformal prediction has gained considerable attention in computer science, machine learning, and statistics. Candès et al. extended this method to right-censored survival data, addressing right-censoring complexity by creating a covariate shift setting, extracting a subcohort of subjects with censoring times exceeding a fixed threshold. Their approach only estimates the lower prediction bound for type I censoring, where all subjects have available censoring times regardless of their failure status. In medical applications, we often encounter more general right-censored data, observing only the minimum of failure time and censoring time. Subjects with observed failure times have unavailable censoring times. To address this, we propose a bootstrap method to construct 1- as well as 2-sided conformal predictive intervals for general right-censored survival data under different working regression models. Through simulations, our method demonstrates excellent average coverage for the lower bound and good coverage for the 2-sided predictive interval, regardless of working model is correctly specified or not, particularly under moderate censoring. We further extend the proposed method to several directions in medical applications. We apply this method to predict breast cancer patients’ future survival times based on tumor characteristics and treatment.},
  archive      = {J_BIOMTC},
  author       = {Qin, Jing and Piao, Jin and Ning, Jing and Shen, Yu},
  doi          = {10.1093/biomtc/ujaf063},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf063},
  shortjournal = {Biometrics},
  title        = {Conformal predictive intervals in survival analysis: A resampling approach},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous-time mediation analysis for repeatedly measured mediators and outcomes. <em>BIOMTC</em>, <em>81</em>(2), ujaf062. (<a href='https://doi.org/10.1093/biomtc/ujaf062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis aims to decipher the underlying causal mechanisms between an exposure, an outcome, and intermediate variables called mediators. Initially developed for fixed-time mediator and outcome, it has been extended to the framework of longitudinal data by discretizing the assessment times of mediator and outcome. Yet, processes in play in longitudinal studies are usually defined in continuous time and measured at irregular and subject-specific visits. This is the case in dementia research when cerebral and cognitive changes measured at planned visits in cohorts are of interest. We thus propose a methodology to estimate the causal mechanisms between a time-fixed exposure ( ⁠ X ⁠ ), a mediator process ( ⁠ M t ⁠ ), and an outcome process ( ⁠ Y t ⁠ ) both measured repeatedly over time in the presence of a time-dependent confounding process ( ⁠ L t ⁠ ). We consider 2 types of causal estimands, the natural effects and path-specific effects. We provide identifiability assumptions, and we employ a multivariate mixed model based on differential equations for their estimation. The performances of the method are assessed in simulations, and the method is illustrated in 2 real-world examples motivated by the 3C cerebral aging study to assess (1) the effect of educational level on functional dependency through depressive symptomatology and cognitive functioning and (2) the effect of a genetic factor on cognitive functioning potentially mediated by vascular brain lesions and confounded by neurodegeneration.},
  archive      = {J_BIOMTC},
  author       = {Kateline, Le Bourdonnec and Linda, Valeri and Cécile, Proust-Lima},
  doi          = {10.1093/biomtc/ujaf062},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf062},
  shortjournal = {Biometrics},
  title        = {Continuous-time mediation analysis for repeatedly measured mediators and outcomes},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learn-as-you-GO (LAGO) trials: Optimizing treatments and preventing trial failure through ongoing learning. <em>BIOMTC</em>, <em>81</em>(2), ujaf061. (<a href='https://doi.org/10.1093/biomtc/ujaf061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that changing the intervention package while a trial is ongoing does not lead to valid inference using standard statistical methods. However, it is often necessary to adapt, tailor, or tweak a complex intervention package in public health implementation trials, especially when the intervention package does not have the desired effect. This article presents conditions under which the resulting analyses remain valid even when the intervention package is adapted while a trial is ongoing. Our results on such Learn-As-you-GO (LAGO) trials extend the theory of LAGO for binary outcomes following a logistic regression model to LAGO for continuous outcomes under flexible conditional mean models. Because the mathematical methods for binary outcomes do not apply to continuous outcomes, the theory presented in this paper is entirely new. We derive point and interval estimators of the intervention effects and ensure the validity of hypothesis tests for an overall intervention effect. We develop a confidence set for the optimal intervention package, which achieves a pre-specified mean outcome while minimizing cost, and confidence bands for the mean outcome under all intervention package compositions. This work will be useful for the design and analysis of large-scale intervention trials where the intervention package is adapted, tailored, or tweaked while the trial is ongoing.},
  archive      = {J_BIOMTC},
  author       = {Bing, Ante and Spiegelman, Donna and Nevo, Daniel and Lok, Judith J},
  doi          = {10.1093/biomtc/ujaf061},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf061},
  shortjournal = {Biometrics},
  title        = {Learn-as-you-GO (LAGO) trials: Optimizing treatments and preventing trial failure through ongoing learning},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and efficient semi-supervised learning for ising model. <em>BIOMTC</em>, <em>81</em>(2), ujaf060. (<a href='https://doi.org/10.1093/biomtc/ujaf060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Wu, Daiqing and Liu, Molei},
  doi          = {10.1093/biomtc/ujaf060},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf060},
  shortjournal = {Biometrics},
  title        = {Robust and efficient semi-supervised learning for ising model},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving estimation efficiency for case-cohort studies with a cure fraction. <em>BIOMTC</em>, <em>81</em>(2), ujaf059. (<a href='https://doi.org/10.1093/biomtc/ujaf059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Zhou, Qingning and Cao, Xu},
  doi          = {10.1093/biomtc/ujaf059},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf059},
  shortjournal = {Biometrics},
  title        = {Improving estimation efficiency for case-cohort studies with a cure fraction},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rejoinder to the discussion on “Continuous-space occupancy models”. <em>BIOMTC</em>, <em>81</em>(2), ujaf058. (<a href='https://doi.org/10.1093/biomtc/ujaf058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discussions of our paper consider some assumptions of continuous-space occupancy models, alternative approaches, and directions for future research. In this short rejoinder, we expand on some of these ideas and provide additional comments.},
  archive      = {J_BIOMTC},
  author       = {Wright, Wilson J and Hooten, Mevin B},
  doi          = {10.1093/biomtc/ujaf058},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf058},
  shortjournal = {Biometrics},
  title        = {Rejoinder to the discussion on “Continuous-space occupancy models”},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion on “Continuous-space occupancy models” by wilson j. wright and mevin b. hooten. <em>BIOMTC</em>, <em>81</em>(2), ujaf057. (<a href='https://doi.org/10.1093/biomtc/ujaf057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Pautrel, Léa and Etienne, Marie-Pierre and Gimenez, Olivier},
  doi          = {10.1093/biomtc/ujaf057},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf057},
  shortjournal = {Biometrics},
  title        = {Discussion on “Continuous-space occupancy models” by wilson j. wright and mevin b. hooten},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion on “Continuous-space occupancy models” by wilson j. wright and mevin b. hooten. <em>BIOMTC</em>, <em>81</em>(2), ujaf056. (<a href='https://doi.org/10.1093/biomtc/ujaf056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Doser, Jeffrey W and Pacifici, Krishna},
  doi          = {10.1093/biomtc/ujaf056},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf056},
  shortjournal = {Biometrics},
  title        = {Discussion on “Continuous-space occupancy models” by wilson j. wright and mevin b. hooten},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous-space occupancy models. <em>BIOMTC</em>, <em>81</em>(2), ujaf055. (<a href='https://doi.org/10.1093/biomtc/ujaf055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occupancy models are used to infer species distributions over large spatial extents while accounting for imperfect detection. Current approaches, however, are unable to model species occurrence over continuous spatial domains while accounting for the discrete spatial domain of the observed data. We develop a new class of spatial occupancy models that embeds a change of spatial support between the observed data and occurrence process. We use a clipped Gaussian process to represent species occurrence in continuous space, which can provide inferences at a finer resolution than the observed occupancy data. Our approach is beneficial because it allows for more realistic models of species occurrence, can account for species occurring in only a portion of a surveyed site, and can relate detection probabilities to these within-site occurrence proportions. We show how our model can be fit using Bayesian methods and develop a computationally efficient MCMC algorithm. In particular, we rely on a Vecchia approximation to implement the spatial Gaussian process describing species occurrence and develop a surrogate data approach for jointly updating the spatial terms and spatial covariance parameters. We demonstrate our model using simulated data and compare our approach to alternative spatial occupancy models. We also use our model to analyze ovenbird occurrence data collected in New Hampshire, USA.},
  archive      = {J_BIOMTC},
  author       = {Wright, Wilson J and Hooten, Mevin B},
  doi          = {10.1093/biomtc/ujaf055},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf055},
  shortjournal = {Biometrics},
  title        = {Continuous-space occupancy models},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double robust variance estimation with parametric working models. <em>BIOMTC</em>, <em>81</em>(2), ujaf054. (<a href='https://doi.org/10.1093/biomtc/ujaf054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doubly robust estimators have gained popularity in the field of causal inference due to their ability to provide consistent point estimates when either an outcome or an exposure model is correctly specified. However, for nonrandomized exposures, the influence function based variance estimator frequently used with doubly robust estimators of the average causal effect is only consistent when both working models (ie, outcome and exposure models) are correctly specified. Here, the empirical sandwich variance estimator and the nonparametric bootstrap are demonstrated to be doubly robust variance estimators. That is, they are expected to provide valid estimates of the variance leading to nominal confidence interval coverage when only 1 working model is correctly specified. Simulation studies illustrate the properties of the influence function based, empirical sandwich, and nonparametric bootstrap variance estimators in the setting where parametric working models are assumed. Estimators are applied to data from the Improving Pregnancy Outcomes with Progesterone (IPOP) study to estimate the effect of maternal anemia on birth weight among women with HIV.},
  archive      = {J_BIOMTC},
  author       = {Shook-Sa, Bonnie E and Zivich, Paul N and Lee, Chanhwa and Xue, Keyi and Ross, Rachael K and Edwards, Jessie K and Stringer, Jeffrey S A and Cole, Stephen R},
  doi          = {10.1093/biomtc/ujaf054},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf054},
  shortjournal = {Biometrics},
  title        = {Double robust variance estimation with parametric working models},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian covariate-dependent graph learning with a dual group spike-and-slab prior. <em>BIOMTC</em>, <em>81</em>(2), ujaf053. (<a href='https://doi.org/10.1093/biomtc/ujaf053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate-dependent graph learning has gained increasing interest in the graphical modeling literature for the analysis of heterogeneous data. This task, however, poses challenges to modeling, computational efficiency, and interpretability. The parameter of interest can be naturally represented as a 3-dimensional array with elements that can be grouped according to 2 directions, corresponding to node level and covariate level, respectively. In this article, we propose a novel dual group spike-and-slab prior that enables multi-level selection at covariate-level and node-level, as well as individual (local) level sparsity. We introduce a nested strategy with specific choices to address distinct challenges posed by the various grouping directions. For posterior inference, we develop a full Gibbs sampler for all parameters, which mitigates the difficulties of parameter tuning often encountered in high-dimensional graphical models and facilitates routine implementation. Through simulation studies, we demonstrate that the proposed model outperforms existing methods in its accuracy of graph recovery. We show the practical utility of our model via an application to microbiome data where we seek to better understand the interactions among microbes as well as how these are affected by relevant covariates.},
  archive      = {J_BIOMTC},
  author       = {Zeng, Zijian and Li, Meng and Vannucci, Marina},
  doi          = {10.1093/biomtc/ujaf053},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf053},
  shortjournal = {Biometrics},
  title        = {Bayesian covariate-dependent graph learning with a dual group spike-and-slab prior},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian joint longitudinal-survival model with a latent stochastic process for intensive longitudinal data. <em>BIOMTC</em>, <em>81</em>(2), ujaf052. (<a href='https://doi.org/10.1093/biomtc/ujaf052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of mobile health (mHealth) technology has enabled increased collection of intensive longitudinal data (ILD). ILD have potential to capture rapid fluctuations in outcomes that may be associated with changes in the risk of an event. However, existing methods for jointly modeling longitudinal and event-time outcomes are not well-equipped to handle ILD due to the high computational cost. We propose a joint longitudinal and time-to-event model suitable for analyzing ILD. In this model, we summarize a multivariate longitudinal outcome as a smaller number of time-varying latent factors. These latent factors, which are modeled using an Ornstein-Uhlenbeck stochastic process, capture the risk of a time-to-event outcome in a parametric hazard model. We take a Bayesian approach to fit our joint model and conduct simulations to assess its performance. We use it to analyze data from an mHealth study of smoking cessation. We summarize the longitudinal self-reported intensity of 9 emotions as the psychological states of positive and negative affect. These time-varying latent states capture the risk of the first smoking lapse after attempted quit. Understanding factors associated with smoking lapse is of keen interest to smoking cessation researchers.},
  archive      = {J_BIOMTC},
  author       = {Abbott, Madeline R and Dempsey, Walter H and Nahum-Shani, Inbal and Potter, Lindsey N and Wetter, David W and Lam, Cho Y and Taylor, Jeremy M G},
  doi          = {10.1093/biomtc/ujaf052},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf052},
  shortjournal = {Biometrics},
  title        = {A bayesian joint longitudinal-survival model with a latent stochastic process for intensive longitudinal data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distance weighted directional regression for fréchet sufficient dimension reduction. <em>BIOMTC</em>, <em>81</em>(2), ujaf051. (<a href='https://doi.org/10.1093/biomtc/ujaf051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of non-Euclidean data accumulated from human longevity studies, brain functional network studies, and many other areas has become an important issue in modern statistics. Fréchet sufficient dimension reduction aims to identify dependencies between non-Euclidean object-valued responses and multivariate predictors while simultaneously reducing the dimensionality of the predictors. We introduce the distance weighted directional regression method for both linear and nonlinear Fréchet sufficient dimension reduction. We propose a new formulation of the classical directional regression method in sufficient dimension reduction. The new formulation is based on distance weighting, thus providing a unified approach for sufficient dimension reduction with Euclidean and non-Euclidean responses, and is further extended to nonlinear Fréchet sufficient dimension reduction. We derive the asymptotic normality of the linear Fréchet directional regression estimator and the convergence rate of the nonlinear estimator. Simulation studies are presented to demonstrate the empirical performance of the proposed methods and to support our theoretical findings. The application to human mortality modeling and diabetes prevalence analysis show that our proposal can improve interpretation and out-of-sample prediction.},
  archive      = {J_BIOMTC},
  author       = {Ying, Chao and Yu, Zhou and Zhang, Xin},
  doi          = {10.1093/biomtc/ujaf051},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf051},
  shortjournal = {Biometrics},
  title        = {Distance weighted directional regression for fréchet sufficient dimension reduction},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semiparametric quantile regression rank score test for zero-inflated data. <em>BIOMTC</em>, <em>81</em>(2), ujaf050. (<a href='https://doi.org/10.1093/biomtc/ujaf050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-inflated data commonly arise in various fields, including economics, healthcare, and environmental sciences, where measurements frequently include an excess of zeros due to structural or sampling mechanisms. Traditional approaches, such as Zero-Inflated Poisson and Zero-Inflated Negative Binomial models, have been widely used to handle excess zeros in count data, but they rely on strong parametric assumptions that may not hold in complex real-world applications. In this paper, we propose a zero-inflated quantile single-index rank-score-based test (ZIQ-SIR) to detect associations between zero-inflated outcomes and covariates, particularly when nonlinear relationships are present. ZIQ-SIR offers a flexible, semi-parametric approach that accounts for the zero-inflated nature of the data and avoids the restrictive assumptions of traditional parametric models. Through simulations, we show that ZIQ-SIR outperforms existing methods by achieving higher power and better Type I error control, owing to its flexibility in modeling zero-inflated and overdispersed data. We apply our method to the real-world dataset: microbiome abundance from the Columbian Gut study. In this application, ZIQ-SIR identifies more significant associations than alternative approaches, while maintaining accurate type I error control.},
  archive      = {J_BIOMTC},
  author       = {Wang, Zirui and Ling, Wodan and Wang, Tianying},
  doi          = {10.1093/biomtc/ujaf050},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf050},
  shortjournal = {Biometrics},
  title        = {A semiparametric quantile regression rank score test for zero-inflated data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probability modeling and statistical inference in cancer screening by dongfeng wu, chapman and Hall/CRC biostatistics series, 2024, ISBN: 9781032513300 https://www.routledge.com/Probability-modeling-and-statistical-inference-in-cancer-Screening/Wu/p/book/9781032513300. <em>BIOMTC</em>, <em>81</em>(2), ujaf048. (<a href='https://doi.org/10.1093/biomtc/ujaf048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Ankerst, Donna Pauler},
  doi          = {10.1093/biomtc/ujaf048},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf048},
  shortjournal = {Biometrics},
  title        = {Probability modeling and statistical inference in cancer screening by dongfeng wu, chapman and Hall/CRC biostatistics series, 2024, ISBN: 9781032513300 https://www.routledge.com/Probability-modeling-and-statistical-inference-in-cancer-Screening/Wu/p/book/9781032513300},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust omnibus sensitivity analysis of externally controlled trials with intercurrent events. <em>BIOMTC</em>, <em>81</em>(2), ujaf047. (<a href='https://doi.org/10.1093/biomtc/ujaf047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Externally controlled trials are crucial in clinical development when randomized controlled trials are unethical or impractical. These trials consist of a full treatment arm with the experimental treatment and a full external control arm. However, they present significant challenges in learning the treatment effect due to the lack of randomization and a parallel control group. Besides baseline incomparability, outcome mean non-exchangeability, caused by differences in conditional outcome distributions between external controls and counterfactual concurrent controls, is infeasible to test and may introduce biases in evaluating the treatment effect. Sensitivity analysis of outcome mean non-exchangeability is thus critically important to assess the robustness of the study’s conclusions against such assumption violations. Moreover, intercurrent events, which are ubiquitous and inevitable in clinical studies, can further confound the treatment effect and hinder the interpretation of the estimated treatment effects. This paper establishes a semi-parametric framework for externally controlled trials with intercurrent events, offering doubly robust and locally optimal estimators for primary and sensitivity analyses. We develop an omnibus sensitivity analysis that accounts for both outcome mean non-exchangeability and the impacts of intercurrent events simultaneously, ensuring root-n consistency and asymptotic normality under specified conditions. The performance of the proposed sensitivity analysis is evaluated in simulation studies and a real-data problem.},
  archive      = {J_BIOMTC},
  author       = {Gao, Chenyin and Zhang, Xiang and Yang, Shu},
  doi          = {10.1093/biomtc/ujaf047},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf047},
  shortjournal = {Biometrics},
  title        = {Doubly robust omnibus sensitivity analysis of externally controlled trials with intercurrent events},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering mortality patterns and hospital effects in COVID-19 heart failure patients: A novel multilevel logistic cluster-weighted modeling approach. <em>BIOMTC</em>, <em>81</em>(2), ujaf046. (<a href='https://doi.org/10.1093/biomtc/ujaf046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating hospital performance and its relationship to patients’ characteristics is of utmost importance to ensure timely, effective, and optimal treatment. This is particularly relevant in areas and situations where the healthcare system must deal with an unexpected surge in hospitalizations, such as heart failure patients in the Lombardy Region of Italy during the COVID-19 pandemic. Motivated by this issue, the paper introduces a novel multilevel logistic cluster-weighted model for predicting 45-day mortality following hospitalization due to COVID-19. The methodology flexibly accommodates dependence patterns among continuous and dichotomous variables; effectively accounting for group-specific effects in distinct subgroups showing different attributes. A tailored classification expectation-maximization algorithm is developed for parameter estimation, and extensive simulation studies are conducted to evaluate its performance against competing models. The novel approach is applied to administrative data from the Lombardy Region, with the aim of profiling heart failure patients hospitalized for COVID-19 and investigating the hospital-level impact on their overall mortality. A scenario analysis demonstrates the model’s efficacy in managing multiple sources of heterogeneity, thereby yielding promising results in aiding healthcare providers and policymakers in the identification of patient-specific treatment pathways.},
  archive      = {J_BIOMTC},
  author       = {Caldera, Luca and Masci, Chiara and Cappozzo, Andrea and Forlani, Marco and Antonelli, Barbara and Leoni, Olivia and Ieva, Francesca},
  doi          = {10.1093/biomtc/ujaf046},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf046},
  shortjournal = {Biometrics},
  title        = {Uncovering mortality patterns and hospital effects in COVID-19 heart failure patients: A novel multilevel logistic cluster-weighted modeling approach},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing confounding and continuous exposure measurement error using corrected score functions. <em>BIOMTC</em>, <em>81</em>(2), ujaf045. (<a href='https://doi.org/10.1093/biomtc/ujaf045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confounding and exposure measurement error can introduce bias when drawing inference about the marginal effect of an exposure on an outcome of interest. While there are broad methodologies for addressing each source of bias individually, confounding and exposure measurement error frequently co-occur, and there is a need for methods that address them simultaneously. In this paper, corrected score methods are derived under classical additive measurement error to draw inference about marginal exposure effects using only measured variables. Three estimators are proposed based on g-formula, inverse probability weighting, and doubly-robust estimation techniques. The estimators are shown to be consistent and asymptotically normal, and the doubly-robust estimator is shown to exhibit its namesake property. The methods, which are implemented in the R package mismex , perform well in finite samples under both confounding and measurement error as demonstrated by simulation studies. The proposed doubly-robust estimator is applied to study the effects of two biomarkers on HIV-1 infection using data from the HVTN 505 preventative vaccine trial.},
  archive      = {J_BIOMTC},
  author       = {Richardson, Brian D and Blette, Bryan S and Gilbert, Peter B and Hudgens, Michael G},
  doi          = {10.1093/biomtc/ujaf045},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf045},
  shortjournal = {Biometrics},
  title        = {Addressing confounding and continuous exposure measurement error using corrected score functions},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple bias calibration for valid statistical inference under nonignorable nonresponse. <em>BIOMTC</em>, <em>81</em>(2), ujaf044. (<a href='https://doi.org/10.1093/biomtc/ujaf044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Valid statistical inference is notoriously challenging when the sample is subject to nonresponse bias. We approach this difficult problem by employing multiple candidate models for the propensity score (PS) function combined with empirical likelihood. By incorporating multiple working PS models into the internal bias calibration constraint in the empirical likelihood, the selection bias can be safely eliminated as long as the working PS models contain the true model and their expectations are equal to the true missing rate. The bias calibration constraint for the multiple PS models is called the multiple bias calibration. The study delves into the asymptotic properties of the proposed method and provides a comparative analysis through limited simulation studies against existing methods. To illustrate practical implementation, we present a real data analysis on body fat percentage using the National Health and Nutrition Examination Survey dataset.},
  archive      = {J_BIOMTC},
  author       = {Cho, Seonghun and Kim, Jae Kwang and Qiu, Yumou},
  doi          = {10.1093/biomtc/ujaf044},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf044},
  shortjournal = {Biometrics},
  title        = {Multiple bias calibration for valid statistical inference under nonignorable nonresponse},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDC-MAKES: A conditional screening method for controlling false discoveries in high-dimensional multi-response setting. <em>BIOMTC</em>, <em>81</em>(2), ujaf042. (<a href='https://doi.org/10.1093/biomtc/ujaf042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coexistences of high dimensionality and strong correlation in both responses and predictors pose unprecedented challenges in identifying important predictors. In this paper, we propose a model-free conditional feature screening method with false discovery rate (FDR) control for ultrahigh-dimensional multi-response setting. The proposed method is built upon partial distance correlation, which measures the dependence between two random vectors while controlling effect for a multivariate random vector. This screening approach is robust against heavy-tailed data and can select predictors in instances of high correlation among predictors. Additionally, it can identify predictors that are marginally unrelated but conditionally related with the response. Leveraging the advantageous properties of partial distance correlation, our method allows for high-dimensional variables to be conditioned upon, distinguishing it from current research in this field. To further achieve FDR control, we apply derandomized knockoff- e -values to establish the threshold for feature screening more stably. The proposed FDR control method is shown to enjoy sure screening property while maintaining FDR control as well as achieving higher power under mild conditions. The superior performance of these methods is demonstrated through simulation examples and a real data application.},
  archive      = {J_BIOMTC},
  author       = {Xiong, Wei and Pan, Han and Shen, Tong},
  doi          = {10.1093/biomtc/ujaf042},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf042},
  shortjournal = {Biometrics},
  title        = {PDC-MAKES: A conditional screening method for controlling false discoveries in high-dimensional multi-response setting},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal dynamic treatment regime estimation in the presence of nonadherence. <em>BIOMTC</em>, <em>81</em>(2), ujaf041. (<a href='https://doi.org/10.1093/biomtc/ujaf041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes (DTRs) are sequences of functions that formalize the process of precision medicine. DTRs take as input patient information and output treatment recommendations. A major focus of the DTR literature has been on the estimation of optimal DTRs, the sequences of decision rules that result in the best outcome in expectation, across the complete population if they were to be applied. While there is a rich literature on optimal DTR estimation, to date, there has been minimal consideration of the impacts of nonadherence on these estimation procedures. Nonadherence refers to any process through which an individual’s prescribed treatment does not match their true treatment. We explore the impacts of nonadherence and demonstrate that, generally, when nonadherence is ignored, suboptimal regimes will be estimated. In light of these findings, we propose a method for estimating optimal DTRs in the presence of nonadherence. The resulting estimators are consistent and asymptotically normal, with a double robustness property. Using simulations, we demonstrate the reliability of these results, and illustrate comparable performance between the proposed estimation procedure adjusting for the impacts of nonadherence and estimators that are computed on data without nonadherence.},
  archive      = {J_BIOMTC},
  author       = {Spicker, Dylan and Wallace, Michael P and Yi, Grace Y},
  doi          = {10.1093/biomtc/ujaf041},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf041},
  shortjournal = {Biometrics},
  title        = {Optimal dynamic treatment regime estimation in the presence of nonadherence},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discrete-time competing-risks regression with or without penalization. <em>BIOMTC</em>, <em>81</em>(2), ujaf040. (<a href='https://doi.org/10.1093/biomtc/ujaf040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies employ the analysis of time-to-event data that incorporates competing risks and right censoring. Most methods and software packages are geared towards analyzing data that comes from a continuous failure time distribution. However, failure-time data may sometimes be discrete either because time is inherently discrete or due to imprecise measurement. This paper introduces a new estimation procedure for discrete-time survival analysis with competing events. The proposed approach offers a major key advantage over existing procedures and allows for straightforward integration and application of widely used regularized regression and screening-features methods. We illustrate the benefits of our proposed approach by a comprehensive simulation study. Additionally, we showcase the utility of the proposed procedure by estimating a survival model for the length of stay of patients hospitalized in the intensive care unit, considering 3 competing events: discharge to home, transfer to another medical facility, and in-hospital death. A Python package, PyDTS , is available for applying the proposed method with additional features.},
  archive      = {J_BIOMTC},
  author       = {Meir, Tomer and Gorfine, Malka},
  doi          = {10.1093/biomtc/ujaf040},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf040},
  shortjournal = {Biometrics},
  title        = {Discrete-time competing-risks regression with or without penalization},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and analysis of pragmatic trials by song zhang, chul ahn, hong zhu, chapman and Hall/CRC, 2023, ISBN: 9781003126010https://www.routledge.com/Design-and-analysis-of-pragmatic-Trials/Zhang-ahn-zhu/p/book/9781003126010. <em>BIOMTC</em>, <em>81</em>(2), ujaf039. (<a href='https://doi.org/10.1093/biomtc/ujaf039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Blette, Bryan},
  doi          = {10.1093/biomtc/ujaf039},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf039},
  shortjournal = {Biometrics},
  title        = {Design and analysis of pragmatic trials by song zhang, chul ahn, hong zhu, chapman and Hall/CRC, 2023, ISBN: 9781003126010https://www.routledge.com/Design-and-analysis-of-pragmatic-Trials/Zhang-ahn-zhu/p/book/9781003126010},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating weighted quantile treatment effects with missing outcome data by double sampling. <em>BIOMTC</em>, <em>81</em>(2), ujaf038. (<a href='https://doi.org/10.1093/biomtc/ujaf038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal weighted quantile treatment effects (WQTEs) complement standard mean-focused causal contrasts when interest lies at the tails of the counterfactual distribution. However, existing methods for estimating and inferring causal WQTEs assume complete data on all relevant factors, which is often not the case in practice, particularly when the data are not collected for research purposes, such as electronic health records (EHRs) and disease registries. Furthermore, these data may be particularly susceptible to the outcome data being missing-not-at-random (MNAR). This paper proposes to use double sampling, through which the otherwise missing data are ascertained on a sub-sample of study units, as a strategy to mitigate bias due to MNAR data in estimating causal WQTEs. With the additional data, we present identifying conditions that do not require missingness assumptions in the original data. We then propose a novel inverse-probability weighted estimator and derive its asymptotic properties, both pointwise at specific quantiles and uniformly across quantiles over some compact subset of (0,1), allowing the propensity score and double-sampling probabilities to be estimated. For practical inference, we develop a bootstrap method that can be used for both pointwise and uniform inference. A simulation study is conducted to examine the finite sample performance of the proposed estimators. We illustrate the proposed method using EHR data examining the relative effects of 2 bariatric surgery procedures on BMI loss 3 years post-surgery.},
  archive      = {J_BIOMTC},
  author       = {Sun, Shuo and Haneuse, Sebastien and Levis, Alexander W and Lee, Catherine and Arterburn, David E and Fischer, Heidi and Shortreed, Susan and Mukherjee, Rajarshi},
  doi          = {10.1093/biomtc/ujaf038},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf038},
  shortjournal = {Biometrics},
  title        = {Estimating weighted quantile treatment effects with missing outcome data by double sampling},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vine copula mixed models for meta-analysis of diagnostic accuracy studies without a gold standard. <em>BIOMTC</em>, <em>81</em>(2), ujaf037. (<a href='https://doi.org/10.1093/biomtc/ujaf037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous statistical models have been proposed for conducting meta-analysis of diagnostic accuracy studies when a gold standard is available. However, in real-world scenarios, the gold standard test may not be perfect due to several factors such as measurement error, non-availability, invasiveness, or high cost. A generalized linear mixed model (GLMM) is currently recommended to account for an imperfect reference test. We propose vine copula mixed models for meta-analysis of diagnostic test accuracy studies with an imperfect reference standard. Our general models include the GLMM as a special case, can have arbitrary univariate distributions for the random effects, and can provide tail dependencies and asymmetries. Our general methodology is demonstrated with an extensive simulation study and illustrated by insightfully re-analyzing the data of a meta-analysis of the Papanicolaou test that diagnoses cervical neoplasia. Our study suggests that there can be an improvement on GLMM and makes the argument for moving to vine copula random effects models.},
  archive      = {J_BIOMTC},
  author       = {Nikoloulopoulos, Aristidis K},
  doi          = {10.1093/biomtc/ujaf037},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf037},
  shortjournal = {Biometrics},
  title        = {Vine copula mixed models for meta-analysis of diagnostic accuracy studies without a gold standard},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference on the relative risk following covariate-adaptive randomization. <em>BIOMTC</em>, <em>81</em>(2), ujaf036. (<a href='https://doi.org/10.1093/biomtc/ujaf036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate-adaptive randomization (CAR) is widely adopted in clinical trials to ensure balanced treatment allocations across key baseline covariates. Although much research has focused on analyzing average treatment effects, the inference of relative risk under CAR experiments has been less thoroughly explored. In this study, we examine a covariate-adjusted estimate of relative risk and investigate the properties of its associated hypothesis tests under CAR. We first derive the theoretical properties of the covariate-adjusted relative risk for a broad class of CAR procedures. Our findings indicate that conventional tests for relative risk tend to be conservative, leading to reduced type I error rates. To mitigate this issue, we introduce model-based and model-robust methods that enhance the estimation of standard errors. We demonstrate the validity and usage of model-robust and model-based adjusted tests. Extensive numerical studies have been conducted to demonstrate our theoretical findings and the favorable properties of the proposed adjustment methods.},
  archive      = {J_BIOMTC},
  author       = {Zhao, Fengyu and Liu, Yang and Hu, Feifang},
  doi          = {10.1093/biomtc/ujaf036},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf036},
  shortjournal = {Biometrics},
  title        = {Statistical inference on the relative risk following covariate-adaptive randomization},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference with approximate local false discovery rates. <em>BIOMTC</em>, <em>81</em>(2), ujaf035. (<a href='https://doi.org/10.1093/biomtc/ujaf035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efron’s 2-group model is widely used in large-scale multiple testing. This model assumes that test statistics are drawn independently from a mixture of a null and a non-null distribution. The marginal local false discovery rate (locFDR) is the probability that the hypothesis is null given its test statistic. The procedure that rejects null hypotheses with marginal locFDRs below a fixed threshold maximizes power (the expected number of non-nulls rejected) while controlling the marginal false discovery rate in this model. However, in realistic settings the test statistics are dependent, and taking the dependence into account can boost power. Unfortunately, the resulting calculations are typically exponential in the number of hypotheses, which is impractical. Instead, we propose using |$\textrm {locFDR}_N$|⁠ , which is the probability that the hypothesis is null given the test statistics in its |$N$| -neighborhood. We prove that rejecting for small |$\textrm {locFDR}_N$| is optimal in the restricted class where the decision for each hypothesis is only guided by its |$N$| -neighborhood, and that power increases with |$N$|⁠ . The computational complexity of computing the |$\mathrm{ locFDR}_N$| s increases with |$N$|⁠ , so the analyst should choose the largest |$N$| -neighborhood that is still computationally feasible. We show through extensive simulations that our proposed procedure can be substantially more powerful than alternative practical approaches, even with small |$N$| -neighborhoods. We demonstrate the utility of our method in a genome-wide association study of height.},
  archive      = {J_BIOMTC},
  author       = {Karmakar, Rajesh and Heller, Ruth and Rosset, Saharon},
  doi          = {10.1093/biomtc/ujaf035},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf035},
  shortjournal = {Biometrics},
  title        = {Inference with approximate local false discovery rates},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power-enhanced two-sample mean tests for high-dimensional microbiome compositional data. <em>BIOMTC</em>, <em>81</em>(2), ujaf034. (<a href='https://doi.org/10.1093/biomtc/ujaf034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing differences in mean vectors is a fundamental task in the analysis of high-dimensional microbiome compositional data. Existing methods may suffer from low power if the underlying signal pattern is in a situation that does not favor the deployed test. In this work, we develop 2-sample power-enhanced mean tests for high-dimensional compositional data based on the combination of |$P$| -values, which integrates strengths from 2 popular types of tests: the maximum-type test and the quadratic-type test. We provide rigorous theoretical guarantees on the proposed tests, showing accurate Type-I error rate control and enhanced testing power. Our method boosts the testing power toward a broader alternative space, which yields robust performance across a wide range of signal pattern settings. Our methodology and theory also contribute to the literature on power enhancement and Gaussian approximation for high-dimensional hypothesis testing. We demonstrate the performance of our method on both simulated data and real-world microbiome data, showing that our proposed approach improves the testing power substantially compared to existing methods.},
  archive      = {J_BIOMTC},
  author       = {Li, Danning and Xue, Lingzhou and Yang, Haoyi and Yu, Xiufan},
  doi          = {10.1093/biomtc/ujaf034},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf034},
  shortjournal = {Biometrics},
  title        = {Power-enhanced two-sample mean tests for high-dimensional microbiome compositional data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional data analysis with r by ciprian m. crainiceanu, jeff goldsmith, andrew leroux, and erjia cui, chapman and Hall/CRC, 2024, ISBN: 9781032244716 https://www.routledge.com/Functional-data-analysis-with-R/Crainiceanu-goldsmith-leroux-cui/p/book/9781032244716. <em>BIOMTC</em>, <em>81</em>(2), ujaf030. (<a href='https://doi.org/10.1093/biomtc/ujaf030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Chen, Li-Pang},
  doi          = {10.1093/biomtc/ujaf030},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf030},
  shortjournal = {Biometrics},
  title        = {Functional data analysis with r by ciprian m. crainiceanu, jeff goldsmith, andrew leroux, and erjia cui, chapman and Hall/CRC, 2024, ISBN: 9781032244716 https://www.routledge.com/Functional-data-analysis-with-R/Crainiceanu-goldsmith-leroux-cui/p/book/9781032244716},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A semicompeting risks model with an application to UK biobank data to identify risk factors for diabetes onset and progression. <em>BIOMTC</em>, <em>81</em>(2), ujaf003. (<a href='https://doi.org/10.1093/biomtc/ujaf003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Type 2 diabetes (T2D) is a major health concern worldwide with multiple disease stages, including onset, progression to complications, and death. Understanding the roles of genetic and nongenetic factors at different disease stages is crucial for gaining insights into disease etiology, possible prevention, and treatment strategies. The UK Biobank (UKB) is a valuable resource for studying complex diseases, including T2D, with comprehensive data from half a million volunteer participants. However, the UKB data present some unique challenges due to their semicompeting risks structure, involving 2 nonterminal events (T2D and complications) and one terminal event (death). In this paper, we propose a new shared gamma frailty-based semicompeting risks model within the Bayesian framework to account for subsequent nonterminal and terminal events and enable appropriate analysis. We further propose incorporating prevalent cases, that is, individuals with diabetes at enrollment, to gain more insights into the progression to complications and complications to death. To integrate prevalent cases, we introduce a power prior approach that leads to improved model fit and more efficient estimates. Simulation results demonstrate the efficacy of our modeling framework. We apply our method to identify the impacts of various risk factors at different stages of T2D development.},
  archive      = {J_BIOMTC},
  author       = {Sheikh, Md Tuhin and Zhao, Hongyu},
  doi          = {10.1093/biomtc/ujaf003},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf003},
  shortjournal = {Biometrics},
  title        = {A semicompeting risks model with an application to UK biobank data to identify risk factors for diabetes onset and progression},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data integration methods for micro-randomized trials. <em>BIOMTC</em>, <em>81</em>(2), ujaf002. (<a href='https://doi.org/10.1093/biomtc/ujaf002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing statistical methods for the analysis of micro-randomized trials (MRTs) are designed to estimate causal excursion effects using data from a single MRT. In practice, however, researchers can often find previous MRTs that employ similar interventions. In this paper, we develop data integration methods that capitalize on this additional information, leading to statistical efficiency gains. To further increase efficiency, we demonstrate how to combine these approaches according to a generalization of multivariate precision weighting that allows for correlation between estimates, and we show that the resulting meta-estimator possesses an asymptotic optimality property. We illustrate our methods in simulation and in a case study involving 2 MRTs in the area of smoking cessation, finding that the proposed methods can reduce standard errors by over 30% without sacrificing asymptotic unbiasedness or calibrated statistical inference.},
  archive      = {J_BIOMTC},
  author       = {Huch, E and Nahum-Shani, I and Potter, L and Lam, C and Wetter, D W and Dempsey, W},
  doi          = {10.1093/biomtc/ujaf002},
  journal      = {Biometrics},
  month        = {6},
  number       = {2},
  pages        = {ujaf002},
  shortjournal = {Biometrics},
  title        = {Data integration methods for micro-randomized trials},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regression to the mean for bivariate distributions. <em>BIOMTC</em>, <em>81</em>(1), ujaf033. (<a href='https://doi.org/10.1093/biomtc/ujaf033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression to the mean is said to have occurred when subjects having relatively high or low measurements are remeasured closer to the population mean. This phenomenon can influence the conclusion about the effectiveness of a treatment in a pre-post study design. The mean difference of the pre- and post-variables, conditioned on the initial variable being above or below a cut-point, is the sum of regression to the mean and treatment effects. Expressions for regression to the mean are available for the bivariate normal distribution under restrictive assumptions, and for the bivariate Poisson and binomial distributions, more generally. This article derives expressions for regression to the mean for any bivariate distribution while making fewer restrictive assumptions than previous methods. Maximum likelihood estimators are derived, and the unbiasedness, consistency, and asymptotic normality of these estimators are shown for exponential families, where possible. Data on the cholesterol levels in men aged 35-39 are used for decomposing the conditional mean difference in cholesterol level on pre-post occasions into regression to the mean and treatment effects. In another example, data on diastolic blood pressure for 341 patients are used to demonstrate the fraction of change due to regression to the mean and the treatment effects, respectively.},
  archive      = {J_BIOMTC},
  author       = {Khan, Manzoor and Olivier, Jake},
  doi          = {10.1093/biomtc/ujaf033},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf033},
  shortjournal = {Biometrics},
  title        = {Regression to the mean for bivariate distributions},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial linear models for environmental data by dale l. zimmerman and jay m. ver hoef, chapman and Hall/CRC, 2024, ISBN: 9780367183349 https://www.routledge.com/Spatial-linear-models-for-environmental-Data/Zimmerman-VerHoef/p/book/9780367183349. <em>BIOMTC</em>, <em>81</em>(1), ujaf032. (<a href='https://doi.org/10.1093/biomtc/ujaf032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Hughes, John},
  doi          = {10.1093/biomtc/ujaf032},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf032},
  shortjournal = {Biometrics},
  title        = {Spatial linear models for environmental data by dale l. zimmerman and jay m. ver hoef, chapman and Hall/CRC, 2024, ISBN: 9780367183349 https://www.routledge.com/Spatial-linear-models-for-environmental-Data/Zimmerman-VerHoef/p/book/9780367183349},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial sampling with r by dick j. brus, chapman and Hall/CRC, 2022, ISBN: 9781032193854 https://www.routledge.com/Spatial-sampling-with-R/Brus/p/book/9781032193854. <em>BIOMTC</em>, <em>81</em>(1), ujaf029. (<a href='https://doi.org/10.1093/biomtc/ujaf029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Menezes, Raquel},
  doi          = {10.1093/biomtc/ujaf029},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf029},
  shortjournal = {Biometrics},
  title        = {Spatial sampling with r by dick j. brus, chapman and Hall/CRC, 2022, ISBN: 9781032193854 https://www.routledge.com/Spatial-sampling-with-R/Brus/p/book/9781032193854},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mixed-effects bayesian regression model for multivariate group testing data. <em>BIOMTC</em>, <em>81</em>(1), ujaf028. (<a href='https://doi.org/10.1093/biomtc/ujaf028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Laboratories use group (pooled) testing with multiplex assays to reduce the time and cost associated with screening large populations for infectious diseases. Multiplex assays test for multiple diseases simultaneously, and combining their use with group testing can lead to highly efficient screening protocols. However, these benefits come at the expense of a more complex data structure which can hinder surveillance efforts. To overcome this challenge, we develop a general Bayesian framework to estimate a mixed multivariate probit model with data arising from any group testing protocol that uses multiplex assays. In the formulation of this model, we account for the correlation between true disease statuses and heterogeneity across population subgroups, and we provide for automated variable selection through the adoption of spike and slab priors. To perform model fitting, we develop an attractive posterior sampling algorithm which is straightforward to implement. We illustrate our methodology through numerical studies and analyze chlamydia and gonorrhea group testing data collected by the State Hygienic Laboratory at the University of Iowa.},
  archive      = {J_BIOMTC},
  author       = {McMahan, Christopher S and Joyner, Chase N and Tebbs, Joshua M and Bilder, Christopher R},
  doi          = {10.1093/biomtc/ujaf028},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf028},
  shortjournal = {Biometrics},
  title        = {A mixed-effects bayesian regression model for multivariate group testing data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the effects of high-throughput structural neuroimaging predictors on whole-brain functional connectome outcomes via network-based matrix-on-vector regression. <em>BIOMTC</em>, <em>81</em>(1), ujaf027. (<a href='https://doi.org/10.1093/biomtc/ujaf027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint analysis of multimodal neuroimaging data is vital in brain research, revealing complex interactions between brain structures and functions. Our study is motivated by the analysis of a vast dataset of brain functional connectivity (FC) and multimodal structural imaging (SI) features from the UK Biobank. Specifically, we aim to investigate the effects of SI features, such as white matter microstructure integrity (WMMI) and cortical thickness, on the whole-brain functional connectome network. This analysis is inherently challenging due to the extensive structural-functional associations and the intricate network patterns present in multimodal high-dimensional neuroimaging data. To bridge methodological gaps, we developed a novel multi-level sub-graph extraction method (dense bipartite with nested unipartite graph) within a matrix(network)-on-vector regression model. This method identifies subsets of spatially specific SI features that intensely and systematically influence FC sub-networks, while effectively suppressing false positives in large-scale datasets. Applying our method to a multimodal neuroimaging dataset of 4242 participants ffrom the UK Biobank, we evaluated the effects of whole-brain WMMI and cortical thickness on resting-state FC. Our findings indicate that the WMMI in corticospinal tracts and inferior cerebellar peduncle significantly affect functional connections of sensorimotor, salience, and executive sub-networks, with an average correlation of 0.81 ( ⁠|$p < 0.001$|⁠ ).},
  archive      = {J_BIOMTC},
  author       = {Lu, Tong and Zhang, Yuan and Lyzinski, Vince and Bi, Chuan and Kochunov, Peter and Hong, Elliot and Chen, Shuo},
  doi          = {10.1093/biomtc/ujaf027},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf027},
  shortjournal = {Biometrics},
  title        = {Evaluating the effects of high-throughput structural neuroimaging predictors on whole-brain functional connectome outcomes via network-based matrix-on-vector regression},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal treatment regime estimation in practice: Challenges and choices in a randomized clinical trial for depression. <em>BIOMTC</em>, <em>81</em>(1), ujaf026. (<a href='https://doi.org/10.1093/biomtc/ujaf026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important aspect of precision medicine is the tailoring of treatments to specific patient types. Nowadays, various methods are available to estimate for this purpose so-called optimal treatment regimes, that is, decision rules for treatment assignment that map patterns of pretreatment characteristics to treatment alternatives and that maximize the expected patient benefit. However, the application of these methods to real-life data has been limited and comes with nonstandard statistical issues. In search of best practices, we reanalyzed data from a randomized clinical trial for the treatment of dysthymic disorder. While the original objective of this trial was to detect a marginally best treatment alternative, we wanted to estimate an optimal treatment regime using 2 prominent estimation methods: Q-learning and value search estimation. An important obstacle in the dataset under study was the occurrence of missing values. This was handled with multiple imputation, a thoughtful implementation of which, however, implied several challenges. Other challenges were implied by the concrete implementation of value search estimation. In this paper, all the choices we have made in the analysis to handle the aforementioned issues are detailed together with a motivation and a description of possible alternatives. Accordingly, this paper may serve as a guide to apply optimal treatment regime estimation in data-analytic practice.},
  archive      = {J_BIOMTC},
  author       = {Stijven, Florian and Tran, Trung Dung and Driessen, Ellen and Alonso Abad, Ariel and Molenberghs, Geert and Verbeke, Geert and Van Mechelen, Iven},
  doi          = {10.1093/biomtc/ujaf026},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf026},
  shortjournal = {Biometrics},
  title        = {Optimal treatment regime estimation in practice: Challenges and choices in a randomized clinical trial for depression},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model-free framework for evaluating the reliability of a new device with multiple imperfect reference standards. <em>BIOMTC</em>, <em>81</em>(1), ujaf025. (<a href='https://doi.org/10.1093/biomtc/ujaf025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common practice for establishing the reliability of a new computer-aided diagnostic (CAD) device is to evaluate how well its clinical measurements agree with those of a gold standard test. However, in many clinical studies, a gold standard is unavailable, and one needs to aggregate information from multiple imperfect reference standards for evaluation. A key challenge here is the heterogeneity in diagnostic accuracy across different reference standards, which may lead to biased evaluation of a device if improperly accounted for during the aggregation process. We propose an intuitive and easy-to-use statistical framework for evaluation of a device by assessing agreement between its measurements and the weighted sum of measurements from multiple imperfect reference standards, where weights representing relative reliability of each reference standard are determined by a model-free, unsupervised inductive procedure. Specifically, the inductive procedure recursively assigns higher weights to reference standards whose assessments are more consistent with each other and form a majority opinion, while assigning lower weights to those with greater discrepancies. Unlike existing methods, our approach does not require any modeling assumptions or external data to quantify heterogeneous accuracy levels of reference standards. It only requires specifying an appropriate agreement index used for weight assignment and device evaluation. The framework is applied to evaluate a CAD device for kidney obstruction by comparing its diagnostic ratings with those of multiple nuclear medicine physicians.},
  archive      = {J_BIOMTC},
  author       = {Cui, Ying and Yu, Qi and Manatunga, Amita and Jang, Jeong Hoon},
  doi          = {10.1093/biomtc/ujaf025},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf025},
  shortjournal = {Biometrics},
  title        = {A model-free framework for evaluating the reliability of a new device with multiple imperfect reference standards},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian nonparametric trees for principal causal effects. <em>BIOMTC</em>, <em>81</em>(1), ujaf024. (<a href='https://doi.org/10.1093/biomtc/ujaf024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal stratification analysis evaluates how causal effects of a treatment on a primary outcome vary across strata of units defined by their treatment effect on some intermediate quantity. This endeavor is substantially challenged when the intermediate variable is continuously scaled and there are infinitely many basic principal strata. We employ a Bayesian nonparametric approach to flexibly evaluate treatment effects across flexibly modeled principal strata. The approach uses Bayesian Causal Forests (BCF) to simultaneously specify 2 Bayesian Additive Regression Tree models; one for the principal stratum membership and one for the outcome, conditional on principal strata. We show how the capability of BCF for capturing treatment effect heterogeneity is particularly relevant for assessing how treatment effects vary across the surface defined by continuously scaled principal strata, in addition to other benefits relating to targeted selection and regularization-induced confounding. The capabilities of the proposed approach are illustrated with a simulation study, and the methodology is deployed to investigate how causal effects of power plant emissions control technologies on ambient particulate pollution vary as a function of the technologies’ impact on sulfur dioxide emissions.},
  archive      = {J_BIOMTC},
  author       = {Kim, Chanmin and Zigler, Corwin},
  doi          = {10.1093/biomtc/ujaf024},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf024},
  shortjournal = {Biometrics},
  title        = {Bayesian nonparametric trees for principal causal effects},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian scalar-on-network regression with applications to brain functional connectivity. <em>BIOMTC</em>, <em>81</em>(1), ujaf023. (<a href='https://doi.org/10.1093/biomtc/ujaf023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Bayesian regression model relating scalar outcomes to brain functional connectivity represented as symmetric positive definite (SPD) matrices. Unlike many proposals that simply vectorize the matrix-valued connectivity predictors, thereby ignoring their geometric structure, the method presented here respects the Riemannian geometry of SPD matrices by using a tangent space modeling. Dimension reduction is performed in the tangent space, relating the resulting low-dimensional representations to the responses. The dimension reduction matrix is learned in a supervised manner with a sparsity-inducing prior imposed on a Stiefel manifold to prevent overfitting. Our method yields a parsimonious regression model that allows uncertainty quantification of all model parameters and identification of key brain regions that predict the outcomes. We demonstrate the performance of our approach in simulation settings and through a case study to predict Picture Vocabulary scores using data from the Human Connectome Project.},
  archive      = {J_BIOMTC},
  author       = {Ju, Xiaomeng and Park, Hyung G and Tarpey, Thaddeus},
  doi          = {10.1093/biomtc/ujaf023},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf023},
  shortjournal = {Biometrics},
  title        = {Bayesian scalar-on-network regression with applications to brain functional connectivity},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference on change points in generalized semiparametric segmented models. <em>BIOMTC</em>, <em>81</em>(1), ujaf022. (<a href='https://doi.org/10.1093/biomtc/ujaf022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The segmented model has significant applications in scientific research when the change-point effect exists. In this article, we propose a comprehensive semiparametric framework in segmented models to test the existence and estimate the location of change points in the generalized outcome setting. The proposed framework is based on a semismooth estimating equation for the change-point estimation and an average score-type test for hypothesis testing. The root- n consistency, asymptotic normality, and asymptotic efficiency of estimators for all parameters in the segmented model are rigorously studied. The distribution of the average score-type test statistics under the null hypothesis is rigorously derived. Extensive simulation studies are conducted to assess the numerical performance of the proposed change-point estimation method and the average score-type test. We investigate change-point effects of baseline glomerular filtration rate and body mass index on bleeding after intervention using data from Blue Cross Blue Shield. This application study successfully identifies statistically significant change-point effects, with the estimated values providing clinically meaningful insights.},
  archive      = {J_BIOMTC},
  author       = {Yang, Guangyu and Zhang, Baqun and Zhang, Min},
  doi          = {10.1093/biomtc/ujaf022},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf022},
  shortjournal = {Biometrics},
  title        = {Statistical inference on change points in generalized semiparametric segmented models},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse bernoulli mixture modeling with negative-unlabeled data: An approach to identify and characterize long COVID. <em>BIOMTC</em>, <em>81</em>(1), ujaf021. (<a href='https://doi.org/10.1093/biomtc/ujaf021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SARS-CoV-2–infected individuals have reported a diverse collection of persistent and often debilitating symptoms commonly referred to as long COVID or post-acute sequelae of SARS-CoV-2 (PASC). Identifying PASC and its subphenotypes is challenging because available data are “negative-unlabeled” as uninfected individuals must be PASC negative, but those with prior infection have unknown PASC status. Moreover, feature selection among many potentially informative characteristics can facilitate reaching a concise and easily interpretable PASC definition. Therefore, to characterize PASC and the spectrum of PASC subphenotypes while identifying a minimal set of features, we propose a Bernoulli mixture model with novel parameterization to accommodate negative-unlabeled data and Bayesian priors to induce sparsity. We present an efficient expectation-maximization algorithm for estimation, and a grid search procedure to select the number of clusters and level of sparsity. We evaluate the proposed method with a simulation study and an analysis of data on self-reported symptoms from the ongoing Researching COVID to Enhance Recovery-Adult Cohort study.},
  archive      = {J_BIOMTC},
  author       = {Cao, Tingyi and Reeder, Harrison T and Foulkes, Andrea S},
  doi          = {10.1093/biomtc/ujaf021},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf021},
  shortjournal = {Biometrics},
  title        = {Sparse bernoulli mixture modeling with negative-unlabeled data: An approach to identify and characterize long COVID},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining covariate adjustment with group sequential, information-adaptive designs to improve randomized trial efficiency. <em>BIOMTC</em>, <em>81</em>(1), ujaf020. (<a href='https://doi.org/10.1093/biomtc/ujaf020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group sequential designs (GSDs), which involve preplanned interim analyses that allow for early stopping for efficacy or futility, are commonly used for ethical and efficiency reasons. Covariate adjustment, which involves appropriately adjusting for prespecified prognostic baseline variables, can improve precision and is generally recommended by regulators. Combining these, that is, using adjusted estimators at interim and final analyses of a GSD, has potential for dual benefits. We address 2 challenges involved in combining these methods. First, adjusted estimators may lack the independent increments structure (asymptotically) that is required to directly apply standard stopping boundaries for GSDs. We address this by applying a linear transformation to the sequence of adjusted estimators across analysis times, resulting in a new sequence of consistent, asymptotically normal estimators with the independent increments property that either improves or leaves precision unchanged. This approach generalizes foundational results on GSDs with semiparametric efficient estimators to any sequence of regular, asymptotically linear estimators. Second, we address the practical problem of handling uncertainty about how much (if any) precision gain will result from covariate adjustment. This is important for trial planning, since an incorrect projection of a covariate’s prognostic value risks an over- or underpowered trial. We propose using information-adaptive designs, that is, continuing the trial until the required information level is achieved. This design enables faster, more efficient trials, without sacrificing validity or power.},
  archive      = {J_BIOMTC},
  author       = {Van Lancker, Kelly and Betz, Joshua F and Rosenblum, Michael},
  doi          = {10.1093/biomtc/ujaf020},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf020},
  shortjournal = {Biometrics},
  title        = {Combining covariate adjustment with group sequential, information-adaptive designs to improve randomized trial efficiency},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individualized multi-treatment response curves estimation using RBF-net with shared neurons. <em>BIOMTC</em>, <em>81</em>(1), ujaf019. (<a href='https://doi.org/10.1093/biomtc/ujaf019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous treatment effect estimation is an important problem in precision medicine. Specific interests lie in identifying the differential effect of different treatments based on some external covariates. We propose a novel non-parametric treatment effect estimation method in a multi-treatment setting. Our non-parametric modeling of the response curves relies on radial basis function-nets with shared hidden neurons. Our model thus facilitates modeling commonality among the treatment outcomes. The estimation and inference schemes are developed under a Bayesian framework using thresholded best linear projections and implemented via an efficient Markov chain Monte Carlo algorithm, appropriately accommodating uncertainty in all aspects of the analysis. The numerical performance of the method is demonstrated through simulation experiments. Applying our proposed method to MIMIC data, we obtain several interesting findings related to the impact of different treatment strategies on the length of intensive care unit stay and 12-h Sequential Organ Failure Assessment score for sepsis patients who are home-discharged.},
  archive      = {J_BIOMTC},
  author       = {Chang, Peter and Roy, Arkaprava},
  doi          = {10.1093/biomtc/ujaf019},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf019},
  shortjournal = {Biometrics},
  title        = {Individualized multi-treatment response curves estimation using RBF-net with shared neurons},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly modeling means and variances for nonlinear mixed effects models with measurement errors and outliers. <em>BIOMTC</em>, <em>81</em>(1), ujaf018. (<a href='https://doi.org/10.1093/biomtc/ujaf018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal data analyses, the within-individual repeated measurements often exhibit large variations and these variations appear to change over time. Understanding the nature of the within-individual systematic and random variations allows us to conduct more efficient statistical inferences. Motivated by human immunodeficiency virus (HIV) viral dynamic studies, we considered a nonlinear mixed effects model for modeling the longitudinal means, together with a model for the within-individual variances which also allows us to address outliers in the repeated measurements. Statistical inference was then based on a joint model for the mean and variance, implemented by a computationally efficient approximate method. Extensive simulations evaluated the proposed method. We found that the proposed method produces more efficient estimates than the corresponding method without modeling the variances. Moreover, the proposed method provides robust inference against outliers. The proposed method was applied to a recent HIV-related dataset, with interesting new findings.},
  archive      = {J_BIOMTC},
  author       = {Ye, Qian and Wu, Lang and Lima, Viviane Dias},
  doi          = {10.1093/biomtc/ujaf018},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf018},
  shortjournal = {Biometrics},
  title        = {Jointly modeling means and variances for nonlinear mixed effects models with measurement errors and outliers},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Positive-definite regularized estimation for high-dimensional covariance on scalar regression. <em>BIOMTC</em>, <em>81</em>(1), ujaf017. (<a href='https://doi.org/10.1093/biomtc/ujaf017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariance is an important measure of marginal dependence among variables. However, heterogeneity in subject covariances and regression models for high-dimensional covariance matrices is not well studied. Compared to regression analysis for conditional means, modeling high-dimensional covariances is much more challenging due to the large set of free parameters and the intrinsic positive-definite property that puts constraints on the regression parameters. In this paper, we propose a regularized estimation method for the regression coefficients of covariances under sufficient and necessary constraints for the positive definiteness of the conditional average covariance matrices given covariates. The proposed estimator satisfies the sparsity and positive-definite properties simultaneously. An alternating direction method of multipliers (ADMM) algorithm is proposed to solve the constrained and regularized optimization problem. We show the convergence of the proposed ADMM algorithm and derive the convergence rates of the proposed estimators for the regression coefficients and the heterogeneous covariances. The proposed method is evaluated by simulation studies, and its practical application is demonstrated by a case study on brain connectivity.},
  archive      = {J_BIOMTC},
  author       = {He, Jie and Qiu, Yumou and Zhou, Xiao-Hua},
  doi          = {10.1093/biomtc/ujaf017},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf017},
  shortjournal = {Biometrics},
  title        = {Positive-definite regularized estimation for high-dimensional covariance on scalar regression},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The subtype-free average causal effect for heterogeneous disease etiology. <em>BIOMTC</em>, <em>81</em>(1), ujaf016. (<a href='https://doi.org/10.1093/biomtc/ujaf016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies have shown that the effect an exposure may have on a disease can vary for different subtypes of the same disease. However, existing approaches to estimate and compare these effects largely overlook causality. In this paper, we study the effect smoking may have on having colorectal cancer subtypes defined by a trait known as microsatellite instability (MSI). We use principal stratification to propose an alternative causal estimand, the Subtype-Free Average Causal Effect (SF-ACE). The SF-ACE is the causal effect of the exposure among those who would be free from other disease subtypes under any exposure level. We study non-parametric identification of the SF-ACE and discuss different monotonicity assumptions, which are more nuanced than in the standard setting. As is often the case with principal stratum effects, the assumptions underlying the identification of the SF-ACE from the data are untestable and can be too strong. Therefore, we also develop sensitivity analysis methods that relax these assumptions. We present 3 different estimators, including a doubly robust estimator, for the SF-ACE. We implement our methodology for data from 2 large cohorts to study the heterogeneity in the causal effect of smoking on colorectal cancer with respect to MSI subtypes.},
  archive      = {J_BIOMTC},
  author       = {Sasson, A and Wang, M and Ogino, S and Nevo, D},
  doi          = {10.1093/biomtc/ujaf016},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf016},
  shortjournal = {Biometrics},
  title        = {The subtype-free average causal effect for heterogeneous disease etiology},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiply robust difference-in-differences estimation of causal effect curves for continuous exposures. <em>BIOMTC</em>, <em>81</em>(1), ujaf015. (<a href='https://doi.org/10.1093/biomtc/ujaf015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers commonly use difference-in-differences (DiD) designs to evaluate public policy interventions. While methods exist for estimating effects in the context of binary interventions, policies often result in varied exposures across regions implementing the policy. Yet, existing approaches for incorporating continuous exposures face substantial limitations in addressing confounding variables associated with intervention status, exposure levels, and outcome trends. These limitations significantly constrain policymakers’ ability to fully comprehend policy impacts and design future interventions. In this work, we propose new estimators for causal effect curves within the DiD framework, accounting for multiple sources of confounding. Our approach accommodates misspecification of a subset of intervention, exposure, and outcome models while avoiding any parametric assumptions on the effect curve. We present the statistical properties of the proposed methods and illustrate their application through simulations and a study investigating the heterogeneous effects of a nutritional excise tax under different levels of accessibility to cross-border shopping.},
  archive      = {J_BIOMTC},
  author       = {Hettinger, Gary and Lee, Youjin and Mitra, Nandita},
  doi          = {10.1093/biomtc/ujaf015},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf015},
  shortjournal = {Biometrics},
  title        = {Multiply robust difference-in-differences estimation of causal effect curves for continuous exposures},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general, flexible, and harmonious framework to construct interpretable functions in regression analysis. <em>BIOMTC</em>, <em>81</em>(1), ujaf014. (<a href='https://doi.org/10.1093/biomtc/ujaf014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An interpretable model or method has several appealing features, such as reliability to adversarial examples, transparency of decision-making, and communication facilitator. However, interpretability is a subjective concept, and even its definition can be diverse. The same model may be deemed as interpretable by a study team, but regarded as a black-box algorithm by another squad. Simplicity, accuracy and generalizability are some additional important aspects of evaluating interpretability. In this work, we present a general, flexible and harmonious framework to construct interpretable functions in regression analysis with a focus on continuous outcomes. We formulate a functional skeleton in light of users’ expectations of interpretability. A new measure based on Mallows’s |$C_p$| -statistic is proposed for model selection to balance approximation, generalizability, and interpretability. We apply this approach to derive a sample size formula in adaptive clinical trial designs to demonstrate the general workflow, and to explain operating characteristics in a Bayesian Go/No-Go paradigm to show the potential advantages of using meaningful intermediate variables. Generalization to categorical outcomes is illustrated in an example of hypothesis testing based on Fisher’s exact test. A real data analysis of NHANES (National Health and Nutrition Examination Survey) is conducted to investigate relationships between some important laboratory measurements. We also discuss some extensions of this method.},
  archive      = {J_BIOMTC},
  author       = {Zhan, Tianyu and Kang, Jian},
  doi          = {10.1093/biomtc/ujaf014},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf014},
  shortjournal = {Biometrics},
  title        = {A general, flexible, and harmonious framework to construct interpretable functions in regression analysis},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing selection bias in cluster randomized experiments via weighting. <em>BIOMTC</em>, <em>81</em>(1), ujaf013. (<a href='https://doi.org/10.1093/biomtc/ujaf013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cluster randomized experiments, individuals are often recruited after the cluster treatment assignment, and data are typically only available for the recruited sample. Post-randomization recruitment can lead to selection bias, inducing systematic differences between the overall and the recruited populations and between the recruited intervention and control arms. In this setting, we define causal estimands for the overall and the recruited populations. We prove, under the assumption of ignorable recruitment, that the average treatment effect on the recruited population can be consistently estimated from the recruited sample using inverse probability weighting. Generally, we cannot identify the average treatment effect on the overall population. Nonetheless, we show, via a principal stratification formulation, that one can use weighting of the recruited sample to identify treatment effects on two meaningful subpopulations of the overall population: Individuals who would be recruited into the study regardless of the assignment, and individuals who would be recruited into the study under treatment but not under control. We develop an estimation strategy and a sensitivity analysis approach for checking the ignorable recruitment assumption, which we implement in the publicly available CRTrecruit R package. The proposed methods are applied to the ARTEMIS cluster randomized trial, where removing co-payment barriers increases the persistence of P2Y |$_{12}$| inhibitor among the always-recruited population.},
  archive      = {J_BIOMTC},
  author       = {Papadogeorgou, Georgia and Liu, Bo and Li, Fan},
  doi          = {10.1093/biomtc/ujaf013},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf013},
  shortjournal = {Biometrics},
  title        = {Addressing selection bias in cluster randomized experiments via weighting},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Potential outcome simulation for efficient head-to-head comparison of adaptive dose-finding designs. <em>BIOMTC</em>, <em>81</em>(1), ujaf012. (<a href='https://doi.org/10.1093/biomtc/ujaf012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dose-finding trials are a key component of the drug development process and rely on a statistical design to help inform dosing decisions. Triallists wishing to choose a design require knowledge of operating characteristics of competing methods. This is often assessed using a large-scale simulation study with multiple designs and configurations investigated, which can be time-consuming and therefore limits the scope of the simulation. We introduce a new approach to the design of simulation studies of dose-finding trials. The approach simulates all potential outcomes that individuals could experience at each dose level in the trial. Datasets are simulated in advance and then applied to each of the competing methods to enable a more efficient head-to-head comparison. Furthermore, individual trial datasets can be interrogated to understand when designs deviate in their decision making. In three case-studies, we show sizeable reductions in Monte Carlo error for comparing a performance metric between two competing designs. Efficiency gains depend on the similarity of the designs. Comparing two Phase I/II design variants, with high correlation of recommending the same optimal biologic dose, we show that the new approach requires a simulation study that is approximately 48 times smaller than the conventional approach. Furthermore, advance-simulated trial datasets can be reused to assess the performance of designs across multiple configurations. We recommend researchers consider this more efficient simulation approach in their dose-finding studies and we have updated the R package escalation to help facilitate implementation.},
  archive      = {J_BIOMTC},
  author       = {Sweeting, Michael and Slade, Daniel and Jackson, Dan and Brock, Kristian},
  doi          = {10.1093/biomtc/ujaf012},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf012},
  shortjournal = {Biometrics},
  title        = {Potential outcome simulation for efficient head-to-head comparison of adaptive dose-finding designs},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple and powerful method for large-scale composite null hypothesis testing with applications in mediation analysis. <em>BIOMTC</em>, <em>81</em>(1), ujaf011. (<a href='https://doi.org/10.1093/biomtc/ujaf011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale mediation analysis has received increasing interest in recent years, especially in genome-wide epigenetic studies. The statistical problem in large-scale mediation analysis concerns testing composite null hypotheses in the context of large-scale multiple testing. The classical Sobel’s and joint significance tests are overly conservative and therefore are underpowered in practice. In this work, we propose a testing method for large-scale composite null hypothesis testing to properly control the type I error and hence improve the testing power. Our method is simple and essentially only requires counting the number of observed test statistics in a certain region. Non-asymptotic theories are established under weak assumptions and indicate that the proposed method controls the type I error well and is powerful. Extensive simulation studies confirm our non-asymptotic theories and show that the proposed method controls the type I error in all settings and has strong power. A data analysis on DNA methylation is also presented to illustrate our method.},
  archive      = {J_BIOMTC},
  author       = {Liu, Yaowu},
  doi          = {10.1093/biomtc/ujaf011},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf011},
  shortjournal = {Biometrics},
  title        = {A simple and powerful method for large-scale composite null hypothesis testing with applications in mediation analysis},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instrumental variable estimation of complier casual treatment effects with interval-censored competing risks data. <em>BIOMTC</em>, <em>81</em>(1), ujaf010. (<a href='https://doi.org/10.1093/biomtc/ujaf010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses the assessment of causal treatment effects on a time-to-event outcome, a crucial part of many scientific investigations. Although some methods have been developed for the problem, they are not applicable to situations where there exist both interval censoring and competing risks. We fill in this critical gap under a class of transformation models for cumulative incidence functions by developing an instrumented variable (IV) estimation approach. The IV is a valuable tool commonly used to mitigate the impact of endogenous treatment selection and to determine causal treatment effects in an unbiased manner. The proposed method is flexible as the model includes many commonly used models such as the sub-distributional proportional odds and hazards models (ie, the Fine–Gray model) as special cases. The resulting estimator for the regression parameter is shown to be consistent and asymptotically normal. A simulation study is conducted to evaluate finite sample performance of the proposed approach and suggests that it works well in practice. It is applied to a breast cancer screening study.},
  archive      = {J_BIOMTC},
  author       = {Lou, Yichen and Ma, Yuqing and Sun, Jianguo and Wang, Peijie and Ye, Zhisheng},
  doi          = {10.1093/biomtc/ujaf010},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf010},
  shortjournal = {Biometrics},
  title        = {Instrumental variable estimation of complier casual treatment effects with interval-censored competing risks data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composite likelihood inference for space-time point processes. <em>BIOMTC</em>, <em>81</em>(1), ujaf009. (<a href='https://doi.org/10.1093/biomtc/ujaf009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamics of a rain forest is extremely complex involving births, deaths, and growth of trees with complex interactions between trees, animals, climate, and environment. We consider the patterns of recruits (new trees) and dead trees between rain forest censuses. For a current census, we specify regression models for the conditional intensity of recruits and the conditional probabilities of death given the current trees and spatial covariates. We estimate regression parameters using conditional composite likelihood functions that only involve the conditional first order properties of the data. When constructing assumption lean estimators of covariance matrices of parameter estimates, we only need mild assumptions of decaying conditional correlations in space, while assumptions regarding correlations over time are avoided by exploiting conditional centering of composite likelihood score functions. Time series of point patterns from rain forest censuses are quite short, while each point pattern covers a fairly big spatial region. To obtain asymptotic results, we therefore use a central limit theorem for the fixed timespan—increasing spatial domain asymptotic setting. This also allows us to handle the challenge of using stochastic covariates constructed from past point patterns. Conveniently, it suffices to impose weak dependence assumptions on the innovations of the space-time process. We investigate the proposed methodology by simulation studies and an application to rain forest data.},
  archive      = {J_BIOMTC},
  author       = {Jalilian, Abdollah and Cuevas-Pacheco, Francisco and Xu, Ganggang and Waagepetersen, Rasmus},
  doi          = {10.1093/biomtc/ujaf009},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf009},
  shortjournal = {Biometrics},
  title        = {Composite likelihood inference for space-time point processes},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining experimental and observational data through a power likelihood. <em>BIOMTC</em>, <em>81</em>(1), ujaf008. (<a href='https://doi.org/10.1093/biomtc/ujaf008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials are the gold standard for causal inference and play a pivotal role in modern evidence-based medicine. However, the sample sizes they use are often too limited to provide adequate power for drawing causal conclusions. In contrast, observational data are becoming increasingly accessible in large volumes but can be subject to bias as a result of hidden confounding. Given these complementary features, we propose a power likelihood approach to augmenting randomized controlled trials with observational data to improve the efficiency of treatment effect estimation. We provide a data-adaptive procedure for maximizing the expected log predictive density (ELPD) to select the learning rate that best regulates the information from the observational data. We validate our method through a simulation study that shows increased power while maintaining an approximate nominal coverage rate. Finally, we apply our method in a real-world data fusion study augmenting the PIONEER 6 clinical trial with a US health claims dataset, demonstrating the effectiveness of our method and providing detailed guidance on how to address practical considerations in its application.},
  archive      = {J_BIOMTC},
  author       = {Lin, Xi and Tarp, Jens Magelund and Evans, Robin J},
  doi          = {10.1093/biomtc/ujaf008},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf008},
  shortjournal = {Biometrics},
  title        = {Combining experimental and observational data through a power likelihood},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature screening for metric space-valued responses based on fréchet regression with its applications. <em>BIOMTC</em>, <em>81</em>(1), ujaf007. (<a href='https://doi.org/10.1093/biomtc/ujaf007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various applications, we need to handle more general types of responses, such as distributional data and matrix-valued data, rather than a scalar variable. When the dimension of predictors is ultrahigh, it is necessarily important to identify the relevant predictors for such complex types of responses. For example, in our Alzheimer’s disease neuroimaging study, we need to select the relevant single nucleotide polymorphisms out of 582 591 candidates for the distribution of voxel-level intensities in each of 42 brain regions. To this end, we propose a new sure independence screening (SIS) procedure for general metric space-valued responses based on global Fréchet regression, termed as Fréchet-SIS. The marginal general residual sum of squares is utilized to serve as a marginal utility for evaluating the importance of predictors, where only a distance between data objects is needed. We theoretically show that the proposed Fréchet-SIS procedure enjoys the sure screening property under mild regularity conditions. Monte Carlo simulations are conducted to demonstrate its excellent finite-sample performance. In Alzheimer’s disease neuroimaging study, we identify important genes that correlate with brain activity across different stages of the disease and brain regions. In addition, we also include an economic case study to illustrate our proposal.},
  archive      = {J_BIOMTC},
  author       = {Tian, Bing and Kang, Jian and Zhong, Wei},
  doi          = {10.1093/biomtc/ujaf007},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf007},
  shortjournal = {Biometrics},
  title        = {Feature screening for metric space-valued responses based on fréchet regression with its applications},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudo-observations for bivariate survival data. <em>BIOMTC</em>, <em>81</em>(1), ujaf006. (<a href='https://doi.org/10.1093/biomtc/ujaf006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pseudo-observations approach has been gaining popularity as a method to estimate covariate effects on censored survival data. It is used regularly to estimate covariate effects on quantities such as survival probabilities, restricted mean life, cumulative incidence, and others. In this work, we propose to generalize the pseudo-observations approach to situations where a bivariate failure-time variable is observed, subject to right censoring. The idea is to first estimate the joint survival function of both failure times and then use it to define the relevant pseudo-observations. Once the pseudo-observations are calculated, they are used as the response in a generalized linear model. We consider 2 common nonparametric estimators of the joint survival function: the estimator of Lin and Ying (1993) and the Dabrowska estimator (Dabrowska, 1988). For both estimators, we show that our bivariate pseudo-observations approach produces regression estimates that are consistent and asymptotically normal. Our proposed method enables estimation of covariate effects on quantities such as the joint survival probability at a fixed bivariate time point or simultaneously at several time points and, consequentially, can estimate covariate-adjusted conditional survival probabilities. We demonstrate the method using simulations and an analysis of 2 real-world datasets.},
  archive      = {J_BIOMTC},
  author       = {Travis-Lumer, Yael and Mandel, Micha and Betensky, Rebecca A},
  doi          = {10.1093/biomtc/ujaf006},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf006},
  shortjournal = {Biometrics},
  title        = {Pseudo-observations for bivariate survival data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A regularized bayesian dirichlet-multinomial regression model for integrating single-cell-level omics and patient-level clinical study data. <em>BIOMTC</em>, <em>81</em>(1), ujaf005. (<a href='https://doi.org/10.1093/biomtc/ujaf005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The abundance of various cell types can vary significantly among patients with varying phenotypes and even those with the same phenotype. Recent scientific advancements provide mounting evidence that other clinical variables, such as age, gender, and lifestyle habits, can also influence the abundance of certain cell types. However, current methods for integrating single-cell-level omics data with clinical variables are inadequate. In this study, we propose a regularized Bayesian Dirichlet-multinomial regression framework to investigate the relationship between single-cell RNA sequencing data and patient-level clinical data. Additionally, the model employs a novel hierarchical tree structure to identify such relationships at different cell-type levels. Our model successfully uncovers significant associations between specific cell types and clinical variables across three distinct diseases: pulmonary fibrosis, COVID-19, and non-small cell lung cancer. This integrative analysis provides biological insights and could potentially inform clinical interventions for various diseases.},
  archive      = {J_BIOMTC},
  author       = {Guo, Yanghong and Yu, Lei and Guo, Lei and Xu, Lin and Li, Qiwei},
  doi          = {10.1093/biomtc/ujaf005},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf005},
  shortjournal = {Biometrics},
  title        = {A regularized bayesian dirichlet-multinomial regression model for integrating single-cell-level omics and patient-level clinical study data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Report of the editors—2024. <em>BIOMTC</em>, <em>81</em>(1), ujaf004. (<a href='https://doi.org/10.1093/biomtc/ujaf004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1093/biomtc/ujaf004},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf004},
  shortjournal = {Biometrics},
  title        = {Report of the editors—2024},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified combination framework for dependent tests with applications to microbiome association studies. <em>BIOMTC</em>, <em>81</em>(1), ujaf001. (<a href='https://doi.org/10.1093/biomtc/ujaf001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel meta-analysis framework to combine dependent tests under a general setting, and utilize it to synthesize various microbiome association tests that are calculated from the same dataset. Our development builds upon the classical meta-analysis methods of aggregating P -values and also a more recent general method of combining confidence distributions, but makes generalizations to handle dependent tests. The proposed framework ensures rigorous statistical guarantees, and we provide a comprehensive study and compare it with various existing dependent combination methods. Notably, we demonstrate that the widely used Cauchy combination method for dependent tests, referred to as the vanilla Cauchy combination in this article, can be viewed as a special case within our framework. Moreover, the proposed framework provides a way to address the problem when the distributional assumptions underlying the vanilla Cauchy combination are violated. Our numerical results demonstrate that ignoring the dependence among the to-be-combined components may lead to a severe size distortion phenomenon. Compared to the existing P -value combination methods, including the vanilla Cauchy combination method and other methods, the proposed combination framework is flexible and can be adapted to handle the dependence accurately and utilizes the information efficiently to construct tests with accurate size and enhanced power. The development is applied to the microbiome association studies, where we aggregate information from multiple existing tests using the same dataset. The combined tests harness the strengths of each individual test across a wide range of alternative spaces, enabling more efficient and meaningful discoveries of vital microbiome associations.},
  archive      = {J_BIOMTC},
  author       = {Yu, Xiufan and Zhang, Linjun and Srinivasan, Arun and Xie, Min-ge and Xue, Lingzhou},
  doi          = {10.1093/biomtc/ujaf001},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujaf001},
  shortjournal = {Biometrics},
  title        = {A unified combination framework for dependent tests with applications to microbiome association studies},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Change surface regression for nonlinear subgroup identification with application to warfarin pharmacogenomics data. <em>BIOMTC</em>, <em>81</em>(1), ujae169. (<a href='https://doi.org/10.1093/biomtc/ujae169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pharmacogenomics stands as a pivotal driver toward personalized medicine, aiming to optimize drug efficacy while minimizing adverse effects by uncovering the impact of genetic variations on inter-individual outcome variability. Despite its promise, the intricate landscape of drug metabolism introduces complexity, where the correlation between drug response and genes can be shaped by numerous nongenetic factors, often exhibiting heterogeneity across diverse subpopulations. This challenge is particularly pronounced in datasets such as the International Warfarin Pharmacogenetic Consortium (IWPC), which encompasses diverse patient information from multiple nations. To capture the between-patient heterogeneity in dosing requirement, we formulate a novel change surface model as a model-based approach for multiple subgroup identification in complex datasets. A key feature of our approach is its ability to accommodate nonlinear subgroup divisions, providing a clearer understanding of dynamic drug-gene associations. Furthermore, our model effectively handles high-dimensional data through a doubly penalized approach, ensuring both interpretability and adaptability. We propose an iterative 2-stage method that combines a change point detection technique in the first stage with a smoothed local adaptive majorize-minimization algorithm for surface regression in the second stage. Performance of the proposed methods is evaluated through extensive numerical studies. Application of our method to the IWPC dataset leads to significant new findings, where 3 subgroups subject to different pharmacogenomic relationships are identified, contributing valuable insights into the complex dynamics of drug-gene associations in patients.},
  archive      = {J_BIOMTC},
  author       = {Liu, Pan and Li, Yaguang and Li, Jialiang},
  doi          = {10.1093/biomtc/ujae169},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae169},
  shortjournal = {Biometrics},
  title        = {Change surface regression for nonlinear subgroup identification with application to warfarin pharmacogenomics data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving estimation efficiency for survival data analysis by integrating a coarsened time-to-event outcome from an external study. <em>BIOMTC</em>, <em>81</em>(1), ujae168. (<a href='https://doi.org/10.1093/biomtc/ujae168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of big data, increasing availability of data makes combining different data sources to obtain more accurate estimations a popular topic. However, the development of data integration is often hindered by the heterogeneity in data forms across studies. In this paper, we focus on a case in survival analysis where we have primary study data with a continuous time-to-event outcome and complete covariate measurements, while the data from an external study contain an outcome observed at regular intervals, and only a subset of covariates is measured. To incorporate external information while accounting for the different data forms, we posit working models and obtain informative weights by empirical likelihood, which will be used to construct a weighted estimator in the main analysis. We have established the theory demonstrating that the new estimator has higher estimation efficiency compared to the conventional ones, and this advantage is robust to working model misspecification, as confirmed in our simulation studies. To assess its utility, we apply our method to accommodate data from the National Alzheimer’s Coordinating Center to improve the analysis of the Alzheimer’s Disease Neuroimaging Initiative Phase 1 study.},
  archive      = {J_BIOMTC},
  author       = {Deng, Daxuan and Zhang, Lijun and Feng, Hao and Chinchilli, Vernon M and Chen, Chixiang and Wang, Ming},
  doi          = {10.1093/biomtc/ujae168},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae168},
  shortjournal = {Biometrics},
  title        = {Improving estimation efficiency for survival data analysis by integrating a coarsened time-to-event outcome from an external study},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating hypothetical estimands with causal inference and missing data estimators in a diabetes trial case study. <em>BIOMTC</em>, <em>81</em>(1), ujae167. (<a href='https://doi.org/10.1093/biomtc/ujae167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ICH E9 addendum on estimands in clinical trials provides a framework for precisely defining the treatment effect that is to be estimated, but says little about estimation methods. Here, we report analyses of a clinical trial in type 2 diabetes, targeting the effects of randomized treatment, handling rescue treatment and discontinuation of randomized treatment using the so-called hypothetical strategy. We show how this can be estimated using mixed models for repeated measures, multiple imputation, inverse probability of treatment weighting, G-formula, and G-estimation. We describe their assumptions and practical details of their implementation using packages in R. We report the results of these analyses, broadly finding similar estimates and standard errors across the estimators. We discuss various considerations relevant when choosing an estimation approach, including computational time, how to handle missing data, whether to include post intercurrent event data in the analysis, whether and how to adjust for additional time-varying confounders, and whether and how to model different types of intercurrent event data separately.},
  archive      = {J_BIOMTC},
  author       = {Olarte Parra, Camila and Daniel, Rhian M and Wright, David and Bartlett, Jonathan W},
  doi          = {10.1093/biomtc/ujae167},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae167},
  shortjournal = {Biometrics},
  title        = {Estimating hypothetical estimands with causal inference and missing data estimators in a diabetes trial case study},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed lag models for retrospective cohort data with application to a study of built environment and body weight. <em>BIOMTC</em>, <em>81</em>(1), ujae166. (<a href='https://doi.org/10.1093/biomtc/ujae166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed lag models (DLMs) estimate the health effects of exposure over multiple time lags prior to the outcome and are widely used in time series studies. Applying DLMs to retrospective cohort studies is challenging due to inconsistent lengths of exposure history across participants, which is common when using electronic health record databases. A standard approach is to define subcohorts of individuals with some minimum exposure history, but this limits power and may amplify selection bias. We propose alternative full-cohort methods that use all available data while simultaneously enabling examination of the longest time lag estimable in the cohort. Through simulation studies, we find that restricting to a subcohort can lead to biased estimates of exposure effects due to confounding by correlated exposures at more distant lags. By contrast, full-cohort methods that incorporate multiple imputation of complete exposure histories can avoid this bias to efficiently estimate lagged and cumulative effects. Applying full-cohort DLMs to a study examining the association between residential density (a proxy for walkability) over 12 years and body weight, we find evidence of an immediate effect in the prior 1-2 years. We also observed an association at the maximal lag considered (12 years prior), which we posit reflects an earlier ( ⁠|$\ge$| 12 years) or incrementally increasing prior effect over time. DLMs can be efficiently incorporated within retrospective cohort studies to identify critical windows of exposure.},
  archive      = {J_BIOMTC},
  author       = {Bobb, Jennifer F and Mooney, Stephen J and Cruz, Maricela and Vernez Moudon, Anne and Drewnowski, Adam and Arterburn, David and Cook, Andrea J},
  doi          = {10.1093/biomtc/ujae166},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae166},
  shortjournal = {Biometrics},
  title        = {Distributed lag models for retrospective cohort data with application to a study of built environment and body weight},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized G-estimation for effect modifier selection in a structural nested mean model for repeated outcomes. <em>BIOMTC</em>, <em>81</em>(1), ujae165. (<a href='https://doi.org/10.1093/biomtc/ujae165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effect modification occurs when the impact of the treatment on an outcome varies based on the levels of other covariates known as effect modifiers. Modeling these effect differences is important for etiological goals and for purposes of optimizing treatment. Structural nested mean models (SNMMs) are useful causal models for estimating the potentially heterogeneous effect of a time-varying exposure on the mean of an outcome in the presence of time-varying confounding. A data-adaptive selection approach is necessary if the effect modifiers are unknown a priori and need to be identified. Although variable selection techniques are available for estimating the conditional average treatment effects using marginal structural models or for developing optimal dynamic treatment regimens, all of these methods consider a single end-of-follow-up outcome. In the context of an SNMM for repeated outcomes, we propose a doubly robust penalized G-estimator for the causal effect of a time-varying exposure with a simultaneous selection of effect modifiers and prove the oracle property of our estimator. We conduct a simulation study for the evaluation of its performance in finite samples and verification of its double-robustness property. Our work is motivated by the study of hemodiafiltration for treating patients with end-stage renal disease at the Centre Hospitalier de l’Université de Montréal. We apply the proposed method to investigate the effect heterogeneity of dialysis facility on the repeated session-specific hemodiafiltration outcomes.},
  archive      = {J_BIOMTC},
  author       = {Jaman, Ajmery and Wang, Guanbo and Ertefaie, Ashkan and Bally, Michèle and Lévesque, Renée and Platt, Robert W and Schnitzer, Mireille E},
  doi          = {10.1093/biomtc/ujae165},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae165},
  shortjournal = {Biometrics},
  title        = {Penalized G-estimation for effect modifier selection in a structural nested mean model for repeated outcomes},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional partially linear functional cox models. <em>BIOMTC</em>, <em>81</em>(1), ujae164. (<a href='https://doi.org/10.1093/biomtc/ujae164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a commonly employed method for analyzing time-to-event data involving functional predictors, the functional Cox model assumes a linear relationship between the functional principal component (FPC) scores of the functional predictors and the hazard rates. However, in practical scenarios, such as our study on the survival time of kidney transplant recipients, this assumption often fails to hold. To address this limitation, we introduce a class of high-dimensional partially linear functional Cox models, which accommodates the non-linear effects of functional predictors on the response and allows for diverging numbers of scalar predictors and FPCs as the sample size increases. We employ the group smoothly clipped absolute deviation method to select relevant scalar predictors and FPCs, and use B-splines to obtain a smoothed estimate of the non-linear effect. The finite sample performance of the estimates is evaluated through simulation studies. The model is also applied to a kidney transplant dataset, allowing us to make inferences about the non-linear effects of functional predictors on patients’ hazard rates, as well as to identify significant scalar predictors for long-term survival time.},
  archive      = {J_BIOMTC},
  author       = {Chen, Xin and Liu, Hua and Men, Jiaqi and You, Jinhong},
  doi          = {10.1093/biomtc/ujae164},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae164},
  shortjournal = {Biometrics},
  title        = {High-dimensional partially linear functional cox models},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal inference with cross-temporal design. <em>BIOMTC</em>, <em>81</em>(1), ujae163. (<a href='https://doi.org/10.1093/biomtc/ujae163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When many participants in a randomized trial do not comply with their assigned intervention, the randomized encouragement design is a possible solution. In this design, the causal effects of the intervention can be estimated among participants who would have experienced the intervention if encouraged. For many policy interventions, encouragements cannot be randomized and investigators need to rely on observational data. To address this, we propose a cross-temporal design, which uses time to mimic a randomized encouragement experiment. However, time may be confounded with temporal trends that influence the outcomes. To disentangle these trends from the intervention effects, we replace the commonly used exclusion restrictions with temporal assumptions. We develop Bayesian procedures to estimate the causal effects and compare it to instrumental variables and matching approaches in simulations. The Bayesian approach outperforms the other 2 approaches in terms of estimation accuracy, and it is relatively robust to various violations of the common trends assumption. Taking advantage of the expansion of the Medicare Advantage (MA) program between 2011 and 2017, we implement the proposed method to estimate the effects of MA enrollment on the risk of skilled nursing facility residents being re-hospitalized within 30 days after discharge from the hospital.},
  archive      = {J_BIOMTC},
  author       = {Cao, Yi and Gozalo, Pedro L and Gutman, Roee},
  doi          = {10.1093/biomtc/ujae163},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae163},
  shortjournal = {Biometrics},
  title        = {Causal inference with cross-temporal design},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted Q-learning for optimal dynamic treatment regimes with nonignorable missing covariates. <em>BIOMTC</em>, <em>81</em>(1), ujae161. (<a href='https://doi.org/10.1093/biomtc/ujae161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes (DTRs) formalize medical decision-making as a sequence of rules for different stages, mapping patient-level information to recommended treatments. In practice, estimating an optimal DTR using observational data from electronic medical record (EMR) databases can be complicated by nonignorable missing covariates resulting from informative monitoring of patients. Since complete case analysis can provide consistent estimation of outcome model parameters under the assumption of outcome-independent missingness, Q-learning is a natural approach to accommodating nonignorable missing covariates. However, the backward induction algorithm used in Q-learning can introduce challenges, as nonignorable missing covariates at later stages can result in nonignorable missing pseudo-outcomes at earlier stages, leading to suboptimal DTRs, even if the longitudinal outcome variables are fully observed. To address this unique missing data problem in DTR settings, we propose 2 weighted Q-learning approaches where inverse probability weights for missingness of the pseudo-outcomes are obtained through estimating equations with valid nonresponse instrumental variables or sensitivity analysis. The asymptotic properties of the weighted Q-learning estimators are derived, and the finite-sample performance of the proposed methods is evaluated and compared with alternative methods through extensive simulation studies. Using EMR data from the Medical Information Mart for Intensive Care database, we apply the proposed methods to investigate the optimal fluid strategy for sepsis patients in intensive care units.},
  archive      = {J_BIOMTC},
  author       = {Sun, Jian and Fu, Bo and Su, Li},
  doi          = {10.1093/biomtc/ujae161},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae161},
  shortjournal = {Biometrics},
  title        = {Weighted Q-learning for optimal dynamic treatment regimes with nonignorable missing covariates},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust bayesian graphical regression models for assessing tumor heterogeneity in proteomic networks. <em>BIOMTC</em>, <em>81</em>(1), ujae160. (<a href='https://doi.org/10.1093/biomtc/ujae160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical models are powerful tools to investigate complex dependency structures in high-throughput datasets. However, most existing graphical models make one of two canonical assumptions: (i) a homogeneous graph with a common network for all subjects or (ii) an assumption of normality, especially in the context of Gaussian graphical models. Both assumptions are restrictive and can fail to hold in certain applications such as proteomic networks in cancer. To this end, we propose an approach termed robust Bayesian graphical regression (rBGR) to estimate heterogeneous graphs for non-normally distributed data. rBGR is a flexible framework that accommodates non-normality through random marginal transformations and constructs covariate-dependent graphs to accommodate heterogeneity through graphical regression techniques. We formulate a new characterization of edge dependencies in such models called conditional sign independence with covariates, along with an efficient posterior sampling algorithm. In simulation studies, we demonstrate that rBGR outperforms existing graphical regression models for data generated under various levels of non-normality in both edge and covariate selection. We use rBGR to assess proteomic networks in lung and ovarian cancers to systematically investigate the effects of immunogenic heterogeneity within tumors. Our analyses reveal several important protein–protein interactions that are differentially associated with the immune cell abundance; some corroborate existing biological knowledge, whereas others are novel findings.},
  archive      = {J_BIOMTC},
  author       = {Yao, Tsung-Hung and Ni, Yang and Bhadra, Anindya and Kang, Jian and Baladandayuthapani, Veerabhadran},
  doi          = {10.1093/biomtc/ujae160},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae160},
  shortjournal = {Biometrics},
  title        = {Robust bayesian graphical regression models for assessing tumor heterogeneity in proteomic networks},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed model building and recursive integration for big spatial data modeling. <em>BIOMTC</em>, <em>81</em>(1), ujae159. (<a href='https://doi.org/10.1093/biomtc/ujae159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the need for computationally tractable spatial methods in neuroimaging studies, we develop a distributed and integrated framework for estimation and inference of Gaussian process model parameters with ultra-high-dimensional likelihoods. We propose a shift in viewpoint from whole to local data perspectives that is rooted in distributed model building and integrated estimation and inference. The framework’s backbone is a computationally and statistically efficient integration procedure that simultaneously incorporates dependence within and between spatial resolutions in a recursively partitioned spatial domain. Statistical and computational properties of our distributed approach are investigated theoretically and in simulations. The proposed approach is used to extract new insights into autism spectrum disorder from the autism brain imaging data exchange.},
  archive      = {J_BIOMTC},
  author       = {Hector, Emily C and Reich, Brian J and Eloyan, Ani},
  doi          = {10.1093/biomtc/ujae159},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae159},
  shortjournal = {Biometrics},
  title        = {Distributed model building and recursive integration for big spatial data modeling},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian processes for time series with lead–lag effects with applications to biology data. <em>BIOMTC</em>, <em>81</em>(1), ujae156. (<a href='https://doi.org/10.1093/biomtc/ujae156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating the relationship, particularly the lead–lag effect, between time series is a common question across various disciplines, especially when uncovering biological processes. However, analyzing time series presents several challenges. Firstly, due to technical reasons, the time points at which observations are made are not at uniform intervals. Secondly, some lead–lag effects are transient, necessitating time-lag estimation based on a limited number of time points. Thirdly, external factors also impact these time series, requiring a similarity metric to assess the lead–lag relationship. To counter these issues, we introduce a model grounded in the Gaussian process, affording the flexibility to estimate lead–lag effects for irregular time series. In addition, our method outputs dissimilarity scores, thereby broadening its applications to include tasks such as ranking or clustering multiple pairwise time series when considering their strength of lead–lag effects with external factors. Crucially, we offer a series of theoretical proofs to substantiate the validity of our proposed kernels and the identifiability of kernel parameters. Our model demonstrates advances in various simulations and real-world applications, particularly in the study of dynamic chromatin interactions, compared to other leading methods.},
  archive      = {J_BIOMTC},
  author       = {Mu, Wancen and Chen, Jiawen and Davis, Eric S and Reed, Kathleen and Phanstiel, Douglas and Love, Michael I and Li, Didong},
  doi          = {10.1093/biomtc/ujae156},
  journal      = {Biometrics},
  month        = {3},
  number       = {1},
  pages        = {ujae156},
  shortjournal = {Biometrics},
  title        = {Gaussian processes for time series with lead–lag effects with applications to biology data},
  volume       = {81},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
