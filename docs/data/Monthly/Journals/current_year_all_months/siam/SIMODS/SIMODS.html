<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMODS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simods">SIMODS - 67</h2>
<ul>
<li><details>
<summary>
(2025). Deep block proximal linearized minimization algorithm for nonconvex inverse problems. <em>SIMODS</em>, <em>7</em>(4), 1729-1754. (<a href='https://doi.org/10.1137/24M1682464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Image restoration is typically addressed through nonconvex inverse problems, which are often solved using first-order blockwise splitting methods. In this paper, we consider a general type of nonconvex optimization model that captures many inverse image problems and present an inertial block proximal linearized minimization (iBPLM) algorithm. Our new method unifies the Jacobi-type parallel and the Gauss–Seidel-type alternating update rules and extends beyond these approaches. The inertial technique is also incorporated into each blockwise subproblem update, which can accelerate numerical convergence. Furthermore, we extend this framework with a plug-and-play variant (PnP-iBPLM) that integrates deep gradient denoisers, offering a flexible and robust solution for complex imaging tasks. We provide comprehensive theoretical analysis, demonstrating both subsequential and global convergence of the proposed algorithms. To validate our methods, we apply them to multiblock dictionary learning problems in image denoising and deblurring. Experimental results show that both iBPLM and PnP-iBPLM significantly enhance numerical performance and robustness in these applications.},
  archive      = {J_SIMODS},
  author       = {Chaoyan Huang and Zhongming Wu and Yanqi Cheng and Tieyong Zeng and Carola-Bibiane Schönlieb and Angelica I. Aviles-Rivero},
  doi          = {10.1137/24M1682464},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1729-1754},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Deep block proximal linearized minimization algorithm for nonconvex inverse problems},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phase retrieval with semialgebraic and ReLU neural network priors. <em>SIMODS</em>, <em>7</em>(4), 1705-1728. (<a href='https://doi.org/10.1137/23M161731X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The key ingredient to retrieving a signal from its Fourier magnitudes—namely, to solve the phase retrieval problem—is an effective prior on the sought signal. In this paper, we study the phase retrieval problem under the prior that the signal lies in a semialgebraic set. This is a very general prior as semialgebraic sets include linear models, sparse models, and ReLU neural network generative models. The latter is the main motivation of this paper, due to the remarkable success of deep generative models in a variety of imaging tasks, including phase retrieval. We prove that almost all signals in can be determined from their Fourier magnitudes, up to a sign, if they lie in a (generic) semialgebraic set of dimension . The same is true for all signals if the semialgebraic set is of dimension . We also generalize these results to the problem of signal recovery from the second moment in multireference alignment models with multiplicity-free representations of compact groups. This general result is then used to derive improved sample complexity bounds for recovering band-limited functions on the sphere from their noisy copies, each acted upon by a random element of .},
  archive      = {J_SIMODS},
  author       = {Tamir Bendory and Nadav Dym and Dan Edidin and Arun Suresh},
  doi          = {10.1137/23M161731X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1705-1728},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Phase retrieval with semialgebraic and ReLU neural network priors},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Representation theorems for matrix product states. <em>SIMODS</em>, <em>7</em>(4), 1690-1704. (<a href='https://doi.org/10.1137/24M172010X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we investigate the universal representational capacity of matrix product states (MPSs) in the context of Boolean and continuous functions. We show that MPSs can precisely realize arbitrary Boolean functions by presenting a constructive method for designing MPS representations of Boolean gates. Furthermore, we prove that the function space of MPSs, equipped with scale-invariant sigmoidal activation functions, is dense in the space of continuous functions on compact subsets of . By examining the connection between MPSs and neural networks (NNs), we establish that an MPS with scale-invariant sigmoidal activation is equivalent to a one-hidden-layer NN equipped with kernel functions. For specific MPS models, we construct equivalent NNs and show that nonlinear kernels, such as polynomial kernels coupling input components, arise naturally in these representations. Finally, we analyze the realization of Gaussian processes via infinitely wide MPSs, leveraging their equivalence to NNs. Our analysis of MPSs highlights the critical role of the kernel function in MPSs, as it encodes the correlations between different components of the input data. Its careful design directly influences the model’s representational capacity.},
  archive      = {J_SIMODS},
  author       = {Erdong Guo and David Draper},
  doi          = {10.1137/24M172010X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1690-1704},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Representation theorems for matrix product states},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic optimal transport in banach spaces for regularized estimation of multivariate quantiles. <em>SIMODS</em>, <em>7</em>(4), 1664-1689. (<a href='https://doi.org/10.1137/23M1550852'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a new stochastic algorithm for solving entropic optimal transport (EOT) between two absolutely continuous probability measures and . Our work is motivated by the specific setting of Monge–Kantorovich quantiles where the source measure is either the uniform distribution on the unit hypercube or the spherical uniform distribution. Using the knowledge of the source measure, we propose to parametrize a Kantorovich dual potential by its Fourier coefficients. In this way, each iteration of our stochastic algorithm reduces to two Fourier transforms that enables us to make use of the fast Fourier transform in order to implement a fast numerical method to solve EOT. We study the almost sure convergence of our stochastic algorithm that takes its values in an infinite-dimensional Banach space. Then, using numerical experiments, we illustrate the performances of our approach on the computation of regularized Monge–Kantorovich quantiles. In particular, we investigate the potential benefits of entropic regularization for the smooth estimation of multivariate quantiles using data sampled from the target measure .},
  archive      = {J_SIMODS},
  author       = {Bernard Bercu and Jérémie Bigot and Gauthier Thurin},
  doi          = {10.1137/23M1550852},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1664-1689},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Stochastic optimal transport in banach spaces for regularized estimation of multivariate quantiles},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complete and continuous invariants of 1-periodic sequences in polynomial time. <em>SIMODS</em>, <em>7</em>(4), 1643-1663. (<a href='https://doi.org/10.1137/25M1733574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Inevitable noise in real measurements motivates the challenging problem of continuously quantifying the similarity between rigid objects such as periodic time series and 1-dimensional materials considered under isometry maintaining interpoint distances. The past work developed many Hausdorff-like distances, which have slow or approximate algorithms due to minimizations over infinitely many isometries. For all finite and 1-periodic sequences under isometry and rigid motion in any high-dimensional Euclidean space, we introduce complete invariants and Lipschitz continuous metrics whose time complexities are polynomial in both input size and ambient dimension. The key novelty in the periodic case is the Lipschitz continuity under perturbations that discontinuously change a minimum period. The proven continuity is practically important for maintaining scientific integrity by real-time detection of near-duplicate structures in experimental and simulated materials datasets.},
  archive      = {J_SIMODS},
  author       = {Vitaliy A. Kurlin},
  doi          = {10.1137/25M1733574},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1643-1663},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Complete and continuous invariants of 1-periodic sequences in polynomial time},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The pontryagin maximum principle for training convolutional neural networks. <em>SIMODS</em>, <em>7</em>(4), 1616-1642. (<a href='https://doi.org/10.1137/24M1675369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A novel batch sequential quadratic Hamiltonian (bSQH) algorithm for training convolutional neural networks (CNNs) with -based regularization is presented. This methodology is based on a discrete-time Pontryagin maximum principle (PMP). It uses forward and backward sweeps together with the layerwise approximate maximization of an augmented Hamiltonian function, where the augmentation parameter is chosen adaptively. A technique for determining this augmentation parameter is proposed, and the loss-reduction and convergence properties of the bSQH algorithm are analyzed theoretically and validated numerically. Results of numerical experiments in the context of image classification with a sparsity-enforcing, -based regularizer demonstrate the effectiveness of the proposed method in full-batch and mini-batch modes.},
  archive      = {J_SIMODS},
  author       = {S. Hofmann and A. Borzì},
  doi          = {10.1137/24M1675369},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1616-1642},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The pontryagin maximum principle for training convolutional neural networks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear meta-learning can guarantee faster rates. <em>SIMODS</em>, <em>7</em>(4), 1594-1615. (<a href='https://doi.org/10.1137/24M1662977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Many recent theoretical works on meta-learning aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. The main aim of theoretical guarantees on the subject is to establish the extent to which convergence rates—in learning a common representation—may scale with the number of tasks (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks and task-specific regression functions are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonlinearity maps to an infinite dimensional reproducing kernel Hilbert space, we show that additional biases can be mitigated with careful regularization that leverages the smoothness of task-specific regression functions, yielding improved rates that scale with the number of tasks as desired.},
  archive      = {J_SIMODS},
  author       = {Dimitri Meunier and Zhu Li and Arthur Gretton and Samory Kpotufe},
  doi          = {10.1137/24M1662977},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1594-1615},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Nonlinear meta-learning can guarantee faster rates},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On neural network approximation of ideal adversarial attack and convergence of adversarial training. <em>SIMODS</em>, <em>7</em>(4), 1568-1593. (<a href='https://doi.org/10.1137/23M1590512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Adversarial attacks are usually expressed in terms of a gradient-based operation on the input data and model; this results in heavy computations every time an attack is generated. Recent empirical works exhibit attacks that can be approximated by neural networks. This work provides a general theoretical framework to represent adversarial attacks as a trainable function without further gradient computation. We first motivate that the theoretical best attacks, under proper conditions, can be represented as smooth piecewise functions (piecewise Hölder functions). Then we obtain an approximation result of such functions by a neural network. Subsequently, we emulate the ideal attack process by a neural network and reduce the adversarial training to a mathematical game between an attack network and a training model (a defense network). We also obtain convergence rates of adversarial loss in terms of the sample size for adversarial training in such a setting.},
  archive      = {J_SIMODS},
  author       = {Rajdeep Haldar and Qifan Song},
  doi          = {10.1137/23M1590512},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1568-1593},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On neural network approximation of ideal adversarial attack and convergence of adversarial training},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative modeling of lévy area for high order SDE simulation. <em>SIMODS</em>, <em>7</em>(4), 1541-1567. (<a href='https://doi.org/10.1137/23M161077X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. It is well known that when numerically simulating solutions to stochastic differential equations (SDEs), achieving a strong convergence rate better than (where is the step-size) usually requires the use of certain iterated integrals of Brownian motion, commonly referred to as its “Lévy areas,” However, these stochastic integrals are difficult to simulate due to their non-Gaussian nature. and for a -dimensional Brownian motion with , no fast almost-exact sampling algorithm is known. In this paper, we propose LévyGAN, a deep-learning-based model for generating approximate samples of Lévy area conditional on a Brownian increment. Due to our “bridge-flipping” operation, the output samples match all joint and conditional odd moments exactly. Our generator employs a tailored graph neural network (GNN)-inspired architecture, which enforces the correct dependency structure between the output distribution and the conditioning variable. Furthermore, we incorporate a mathematically principled characteristic-function-based discriminator. Lastly, we introduce a novel training mechanism, termed “Chen-training,” which circumvents the need for expensive-to-generate training data-sets. This new training procedure is underpinned by our two main theoretical results. For four-dimensional Brownian motion, we show that LévyGAN exhibits state-of-the-art performance across several metrics which measure both the joint and marginal distributions. We conclude with a numerical experiment on the log-Heston model, a popular SDE in mathematical finance, demonstrating that a high-quality synthetic Lévy area can lead to high order weak convergence and variance reduction when using multilevel Monte Carlo (MLMC).},
  archive      = {J_SIMODS},
  author       = {Andraž Jelinčič and Jiajie Tao and William F. Turner and Thomas Cass and James Foster and Hao Ni},
  doi          = {10.1137/23M161077X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1541-1567},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Generative modeling of lévy area for high order SDE simulation},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enforcing katz and PageRank centrality measures in complex networks. <em>SIMODS</em>, <em>7</em>(3), 1514-1539. (<a href='https://doi.org/10.1137/24M1690849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We investigate the problem of enforcing a desired centrality measure in complex networks, while still keeping the original pattern of the network. Specifically, by representing the network as a graph with suitable nodes and weighted edges, we focus on computing the smallest perturbation on the weights required to obtain a prescribed PageRank or Katz centrality index for the nodes. Our approach relies on optimization procedures that scale with the number of modified edges, enabling the exploration of different scenarios and altering network structure and dynamics.},
  archive      = {J_SIMODS},
  author       = {Stefano Cipolla and Fabio Durastante and Beatrice Meini},
  doi          = {10.1137/24M1690849},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1514-1539},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Enforcing katz and PageRank centrality measures in complex networks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariance alignment: From maximum likelihood estimation to Gromov–Wasserstein. <em>SIMODS</em>, <em>7</em>(3), 1491-1513. (<a href='https://doi.org/10.1137/24M1682841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Feature alignment methods are used in many scientific disciplines for data pooling, annotation, and comparison. As an instance of a permutation learning problem, feature alignment presents significant statistical and computational challenges. In this work, we propose the covariance alignment model to study and compare various alignment methods and establish a minimax lower bound for covariance alignment that has a nonstandard dimension scaling because of the presence of a nuisance parameter. This lower bound is in fact minimax optimal and is achieved by a natural quasi maximum likelihood estimator. However, this estimator involves a search over all permutations which is computationally infeasible even when the problem has moderate size. To overcome this limitation, we show that the celebrated Gromov–Wasserstein algorithm from optimal transport, which is more amenable to fast implementation even on large-scale problems, is also minimax optimal. These results give the first statistical justification for the deployment of the Gromov–Wasserstein algorithm in practice.},
  archive      = {J_SIMODS},
  author       = {Yanjun Han and Philippe Rigollet and George Stepaniants},
  doi          = {10.1137/24M1682841},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1491-1513},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Covariance alignment: From maximum likelihood estimation to Gromov–Wasserstein},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Log-concave density estimation with independent components. <em>SIMODS</em>, <em>7</em>(3), 1465-1490. (<a href='https://doi.org/10.1137/24M1646947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a method for estimating a log-concave density on from samples, under the assumption that there exists an orthogonal transformation that makes the components of the random vector independent. While log-concave density estimation is hard both computationally and statistically, the independent components assumption alleviates both issues while still maintaining a large nonparametric class. We prove that under mild conditions, at most samples (suppressing constants and log factors) suffice for our proposed estimator to be within of the original density in squared Hellinger distance. On the computational front, while the usual log-concave maximum likelihood estimate can be obtained via a finite-dimensional convex program, it is slow to compute—especially in higher dimensions. We demonstrate through numerical experiments that our estimator can be computed efficiently, making it more practical to use.},
  archive      = {J_SIMODS},
  author       = {Sharvaj Kubal and Christian Campbell and Elina Robeva},
  doi          = {10.1137/24M1646947},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1465-1490},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Log-concave density estimation with independent components},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the quality of randomized approximations of tukey’s depth. <em>SIMODS</em>, <em>7</em>(3), 1441-1464. (<a href='https://doi.org/10.1137/24M1654919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Tukey’s depth (or halfspace depth) is a widely used measure of centrality for multivariate data. However, exact computation of Tukey’s depth is known to be a hard problem in high dimensions. As a remedy, randomized approximations of Tukey’s depth have been proposed. In this paper we explore when such randomized algorithms return a good approximation of Tukey’s depth. We study the case when the data are sampled from a log-concave isotropic distribution. We prove that if one requires that the algorithm runs in polynomial time in the dimension, the randomized algorithm correctly approximates the maximal depth and depths close to zero. On the other hand, for any point of intermediate depth, any good approximation requires exponential complexity.},
  archive      = {J_SIMODS},
  author       = {Simon Briend and Gábor Lugosi and Roberto Imbuzeiro Oliveira},
  doi          = {10.1137/24M1654919},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1441-1464},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On the quality of randomized approximations of tukey’s depth},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Theoretical foundations of ordinal multidimensional scaling, including internal unfolding and external unfolding. <em>SIMODS</em>, <em>7</em>(3), 1422-1440. (<a href='https://doi.org/10.1137/24M1691624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We provide a comprehensive theory of multiple variants of ordinal multidimensional scaling, including internal unfolding and external unfolding. We first follow Shepard [J. Math. Psychol., 3 (1966), pp. 287–315] and work in a continuum model to gain insight. We then follow Kleindessner and von Luxburg [Uniqueness of ordinal embedding, in Conference on Learning Theory, 2014, pp. 40–67] and work in an asymptotic discrete setting.},
  archive      = {J_SIMODS},
  author       = {Ery Arias-Castro and Clément Berenfeld and Daniel Kane},
  doi          = {10.1137/24M1691624},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1422-1440},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Theoretical foundations of ordinal multidimensional scaling, including internal unfolding and external unfolding},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum a posteriori inference for factor graphs via benders’ decomposition. <em>SIMODS</em>, <em>7</em>(3), 1394-1421. (<a href='https://doi.org/10.1137/23M1624981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Many Bayesian statistical inference problems come down to computing a maximum a posteriori (MAP) assignment of latent variables. Yet, standard methods for estimating the MAP assignment do not have a finite time guarantee that the algorithm has converged to a fixed point. Previous research has found that MAP inference can be represented in dual form as a linear programming problem with a nonpolynomial number of constraints. A Lagrangian relaxation of the dual yields a statistical inference algorithm as a linear programming problem. However, the decision as to which constraints to remove in the relaxation is often heuristic. We present a method for maximum a posteriori inference in general Bayesian factor models that sequentially adds constraints to the fully relaxed dual problem using Benders’ decomposition. Our method enables the incorporation of expressive integer and logical constraints in clustering problems such as must-link, cannot-link, and a minimum number of whole samples allocated to each cluster. Using this approach, we derive MAP estimation algorithms for the Bayesian Gaussian mixture model and latent Dirichlet allocation. Empirical results show that our method produces a higher optimal posterior value compared to Gibbs sampling and variational Bayes methods for standard data sets and provides a certificate of convergence.},
  archive      = {J_SIMODS},
  author       = {Harsh Vardhan Dubey and Ji Ah Lee and Patrick Flaherty},
  doi          = {10.1137/23M1624981},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1394-1421},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Maximum a posteriori inference for factor graphs via benders’ decomposition},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clustering in pure-attention hardmax transformers and its role in sentiment analysis. <em>SIMODS</em>, <em>7</em>(3), 1367-1393. (<a href='https://doi.org/10.1137/24M167086X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Transformers are extremely successful machine learning models whose mathematical properties remain poorly understood. Here, we rigorously characterize the behavior of transformers with hardmax self-attention and normalization sublayers as the number of layers tends to infinity. By viewing such transformers as discrete-time dynamical systems describing the evolution of points in a Euclidean space, and thanks to a geometric interpretation of the self-attention mechanism based on hyperplane separation, we show that the transformer inputs asymptotically converge to a clustered equilibrium determined by special points called leaders. We then leverage this theoretical understanding to solve sentiment analysis problems from language processing using a fully interpretable transformer model, which effectively captures “context” by clustering meaningless words around leader words carrying the most meaning. Finally, we outline remaining challenges to bridge the gap between the mathematical analysis of transformers and their real-life implementation.},
  archive      = {J_SIMODS},
  author       = {Albert Alcalde and Giovanni Fantuzzi and Enrique Zuazua},
  doi          = {10.1137/24M167086X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1367-1393},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Clustering in pure-attention hardmax transformers and its role in sentiment analysis},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel diffusion: Infinite dimensional score-based diffusion models for image generation. <em>SIMODS</em>, <em>7</em>(3), 1337-1366. (<a href='https://doi.org/10.1137/23M1614092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Score-based diffusion models (SBDMs) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of finite size. This paper develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. In addition to the quest for generating images at ever-higher resolutions, our primary motivation is to create a well-posed infinite-dimensional learning problem that we can discretize consistently on multiple resolution levels. We thereby intend to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process using trace class operators to ensure that the latent distribution is well-defined in the infinite-dimensional setting and derive the reverse processes for finite-dimensional approximations. Second, we illustrate that approximating the score function with an operator network is beneficial for multilevel training. After deriving the convergence of the discretization and the approximation of multilevel training, we demonstrate some practical benefits of our infinite-dimensional SBDM approach on a synthetic Gaussian mixture example, the MNIST dataset, and a dataset generated from a nonlinear 2D reaction-diffusion equation.},
  archive      = {J_SIMODS},
  author       = {Paul Hagemann and Sophie Mildenberger and Lars Ruthotto and Gabriele Steidl and Nicole Tianjiao Yang},
  doi          = {10.1137/23M1614092},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1337-1366},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Multilevel diffusion: Infinite dimensional score-based diffusion models for image generation},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plug-in estimation of schrödinger bridges. <em>SIMODS</em>, <em>7</em>(3), 1315-1336. (<a href='https://doi.org/10.1137/24M1687340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a procedure for estimating the Schrödinger bridge between two probability distributions. Unlike existing approaches, our method does not require iteratively simulating forward and backward diffusions or training neural networks to fit unknown drifts. Instead, we show that the potentials obtained from solving the static entropic optimal transport problem between the source and target samples can be modified to yield a natural plug-in estimator of the time-dependent drift that defines the bridge between two measures. Under minimal assumptions, we show that our proposal, which we call the Sinkhorn bridge, provably estimates the Schrödinger bridge with a rate of convergence that depends on the intrinsic dimensionality of the target measure. Our approach combines results from the areas of sampling, and theoretical and statistical entropic optimal transport.},
  archive      = {J_SIMODS},
  author       = {Aram-Alexandre Pooladian and Jonathan Niles-Weed},
  doi          = {10.1137/24M1687340},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1315-1336},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Plug-in estimation of schrödinger bridges},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Block majorization minimization with extrapolation and application to \({\beta }\)-NMF. <em>SIMODS</em>, <em>7</em>(3), 1292-1314. (<a href='https://doi.org/10.1137/24M1660188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a Block Majorization Minimization method with Extrapolation (BMMe) for solving a class of multiconvex optimization problems. The extrapolation parameters of BMMe are updated using a novel adaptive update rule. By showing that block majorization minimization can be reformulated as a block mirror descent method, with the Bregman divergence adaptively updated at each iteration, we establish subsequential convergence for BMMe. We use this method to design efficient algorithms to tackle nonnegative matrix factorization problems with -divergences (-NMF) for . These algorithms, which are multiplicative updates with extrapolation, benefit from our novel results, which offer convergence guarantees. We also empirically illustrate the significant acceleration of BMMe for -NMF through extensive experiments.},
  archive      = {J_SIMODS},
  author       = {Le Thi Khanh Hien and Valentin Leplat and Nicolas Gillis},
  doi          = {10.1137/24M1660188},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1292-1314},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Block majorization minimization with extrapolation and application to \({\beta }\)-NMF},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive smooth nonstationary bandits. <em>SIMODS</em>, <em>7</em>(3), 1265-1291. (<a href='https://doi.org/10.1137/24M167651X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study a -armed nonstationary bandit model where rewards change smoothly, as captured by Hölder class assumptions on rewards as functions of time. Such smooth changes are parametrized by a Hölder exponent and coefficient . While various subcases of this general model have been studied in isolation, we first establish the minimax dynamic regret rate generally for all . Next, we show that this optimal dynamic regret can be attained adaptively, without knowledge of . To contrast, even with parameter knowledge, upper bounds were only previously known for limited regimes and [A. Slivkins, J. Mach. Learn. Res., 15 (2014), pp. 2533–2568], [R. Krishnamurthy and A. Gopalan, On Slowly-Varying Non-Stationary Bandits, preprint, arXiv:2110.12916, 2021], [A. G. Manegueu, A. Carpentier, and Y. Yu, Generalized, Non-stationary Bandits, preprint, arXiv:2102.00725, 2021], [S. Jia, Q. Xie, N. Kallus, and P. Frazier, Smooth non-stationary bandits, in International Conference on Machine Learning, JMLR.org, 2023, pp. 14930–14944]. Thus, our work resolves open questions raised by disparate threads of the literature. We also study the problem of attaining faster gap-dependent regret rates in nonstationary bandits. While such rates are long known to be impossible in general [A. Garivier and E. Moulines, On upper-confidence bound policies for switching bandit problems, in Proceedings of the 22nd International Conference on Algorithmic Learning Theory, Springer, 2011, pp. 174–188], we show that environments admitting a safe arm [J. Suk and S. Kpotufe, Tracking most significant arm switches in bandits, in Conference on Learning Theory, 2022] allow for much faster rates than the worst-case scaling with . While previous works in this direction focused on attaining the usual logarithmic regret bounds, as summed over stationary periods, our new gap-dependent rates reveal new optimistic regimes of nonstationarity where even the logarithmic bounds are pessimistic. We show that our new gap-dependent rate is tight and that its achievability (i.e., as made possible by a safe arm) has a surprisingly simple and clean characterization within the smooth Hölder class model.},
  archive      = {J_SIMODS},
  author       = {Joe Suk},
  doi          = {10.1137/24M167651X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1265-1291},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Adaptive smooth nonstationary bandits},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient estimation of the central mean subspace via smoothed gradient outer products. <em>SIMODS</em>, <em>7</em>(3), 1241-1264. (<a href='https://doi.org/10.1137/23M1626700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of sufficient dimension reduction for multi-index models. The estimators of the central mean subspace in prior works either have slow (nonparametric) convergence rates or rely on stringent distributional conditions (e.g., elliptical symmetric covariate distribution ). In this paper, we show that a fast parametric convergence rate of form is achievable via estimating the expected smoothed gradient outer product for a general class of distribution that admits Gaussian or heavier distributions. When the link function is a polynomial with a degree of at most and is the standard Gaussian, we show that the prefactor depends on the ambient dimension as .},
  archive      = {J_SIMODS},
  author       = {Gan Yuan and Mingyue Xu and Samory Kpotufe and Daniel Hsu},
  doi          = {10.1137/23M1626700},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1241-1264},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Efficient estimation of the central mean subspace via smoothed gradient outer products},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian deep learning with multilevel trace-class neural networks. <em>SIMODS</em>, <em>7</em>(3), 1210-1240. (<a href='https://doi.org/10.1137/23M1544738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this article we consider Bayesian inference associated to deep neural networks (DNNs) and in particular, trace-class neural network (TNN) priors [T. Sell and S. S. Singh, Dimension-robust Function Space Mcmc with Neural Network Priors, 2020] which can be preferable to traditional DNNs because they (a) are identifiable and (b) possess desirable convergence properties. TNN priors are defined on functions with infinitely many hidden units, and have strongly convergent Karhunen–Loeve-type approximations with finitely many hidden units. A practical hurdle is that the Bayesian solution is computationally demanding, requiring simulation methods, so approaches to drive down the complexity are needed. In this paper, we leverage the strong convergence of TNN in order to apply multilevel Monte Carlo (MLMC) to these models. In particular, an MLMC method that was introduced by Beskos et al. [SIAM/ASA J. Uncertain. Quantif., 6 (2018), pp. 762–786] is used to approximate posterior expectations of Bayesian TNN models with optimal computational complexity, and this is mathematically proved. The results are verified with several numerical experiments on model problems arising in machine learning, including some toy regression and classification models, MNIST image classification, and a challenging reinforcement learning problem. Furthermore, we illustrate the practical utility of the method on MNIST as well as IMDb sentiment classification.},
  archive      = {J_SIMODS},
  author       = {Neil K. Chada and Ajay Jasra and Kody J. H. Law and Sumeetpal S. Singh},
  doi          = {10.1137/23M1544738},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1210-1240},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Bayesian deep learning with multilevel trace-class neural networks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monotone generative modeling via a Gromov–Monge embedding. <em>SIMODS</em>, <em>7</em>(3), 1184-1209. (<a href='https://doi.org/10.1137/24M1673772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Generative adversarial networks are popular for generative tasks; however, they often require careful architecture selection and extensive empirical tuning, and they are prone to mode collapse. To overcome these challenges, we propose a novel model that identifies the low-dimensional structure of the underlying data distribution, maps it into a low-dimensional latent space while preserving the underlying geometry, and then optimally transports a reference measure to the embedded distribution. We prove three key properties of our method: (1) the encoder preserves the geometry of the underlying data; (2) the generator is -cyclically monotone, where is an intrinsic embedding cost employed by the encoder; and (3) the discriminator’s modulus of continuity improves with the geometric preservation of the data. Numerical experiments demonstrate the effectiveness of our approach in generating high-quality images and exhibiting robustness to both mode collapse and training instability.},
  archive      = {J_SIMODS},
  author       = {Wonjun Lee and Yifei Yang and Dongmian Zou and Gilad Lerman},
  doi          = {10.1137/24M1673772},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1184-1209},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Monotone generative modeling via a Gromov–Monge embedding},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral complexity of deep neural networks. <em>SIMODS</em>, <em>7</em>(3), 1154-1183. (<a href='https://doi.org/10.1137/24M1675746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. It is well known that randomly initialized, push-forward, fully connected neural networks weakly converge to isotropic Gaussian processes in the limit where the width of all layers goes to infinity. In this paper, we propose to use the angular power spectrum of the limiting fields to characterize the complexity of the network architecture. In particular, we define sequences of random variables associated with the angular power spectrum and provide a full characterization of the network complexity in terms of the asymptotic distribution of these sequences as the depth diverges. On this basis, we classify neural networks as low-disorder, sparse, or high-disorder; we show how this classification highlights a number of distinct features for standard activation functions and, in particular, sparsity properties of ReLU networks. Our theoretical results are also validated by numerical simulations.},
  archive      = {J_SIMODS},
  author       = {Simmaco Di Lillo and Domenico Marinucci and Michele Salvi and Stefano Vigogna},
  doi          = {10.1137/24M1675746},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1154-1183},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Spectral complexity of deep neural networks},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random multitype spanning forests for synchronization on sparse graphs. <em>SIMODS</em>, <em>7</em>(3), 1123-1153. (<a href='https://doi.org/10.1137/24M1649563'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Random diffusions are a popular tool in Monte Carlo estimations, with well-established algorithms such as walk-on-spheres (WoS) going back several decades. In this work, we introduce diffusion estimators for the problems of angular synchronization and smoothing on graphs, in the presence of a rotation associated to each edge. Unlike classical WoS algorithms that are pointwise estimators, our diffusion estimators allow for global estimations by propagating along the branches of random spanning subgraphs called multitype spanning forests. Building upon efficient samplers based on variants of Wilson’s algorithm, we show that our estimators outperform standard numerical-linear-algebra solvers in challenging instances, depending on the topology and density of the graph.},
  archive      = {J_SIMODS},
  author       = {Hugo Jaquard and Pierre-Olivier Amblard and Simon Barthelmé and Nicolas Tremblay},
  doi          = {10.1137/24M1649563},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1123-1153},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Random multitype spanning forests for synchronization on sparse graphs},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When big data actually are low-rank, or entrywise approximation of certain function-generated matrices. <em>SIMODS</em>, <em>7</em>(3), 1098-1122. (<a href='https://doi.org/10.1137/24M1687133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The article concerns low-rank approximation of matrices generated by sampling a smooth function of two -dimensional variables. We identify several misconceptions surrounding a claim that, for a specific class of analytic functions, such matrices admit accurate entrywise approximation of rank that is independent of and grows as —colloquially known as “big-data matrices are approximately low-rank.” We provide a theoretical explanation of the numerical results presented in support of this claim, describing three narrower classes of functions for which function-generated matrices can be approximated within an entrywise error of order with rank that is independent of the dimension : (i) functions of the inner product of the two variables, (ii) functions of the Euclidean distance between the variables, and (iii) shift-invariant positive-definite kernels. We extend our argument to tensor-train approximation of tensors generated with functions of the “higher-order inner product” of their multiple variables. We discuss our results in the context of low-rank approximation of (i) growing datasets and (ii) attention in transformer neural networks.},
  archive      = {J_SIMODS},
  author       = {Stanislav Budzinskiy},
  doi          = {10.1137/24M1687133},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1098-1122},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {When big data actually are low-rank, or entrywise approximation of certain function-generated matrices},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability of sequential lateration and of stress minimization in the presence of noise. <em>SIMODS</em>, <em>7</em>(3), 1077-1097. (<a href='https://doi.org/10.1137/24M1661790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sequential lateration is a class of methods for multidimensional scaling where a suitable subset of nodes is first embedded by some method, e.g., a clique embedded by classical scaling, and then the remaining nodes are recursively embedded by lateration. A graph is a lateration graph when it can be embedded by such a procedure. We provide a stability result for a particular variant of sequential lateration. We do so in a setting where the dissimilarities represent noisy Euclidean distances between nodes in a geometric lateration graph. We then deduce, as a corollary, a perturbation bound for stress minimization. To argue that our setting applies broadly, we show that a (large) random geometric graph is a lateration graph with high probability under mild conditions, extending a previous result of Aspnes et al. [IEEE Trans. Mobile Comput., 5 (2006), pp. 1663–1678].},
  archive      = {J_SIMODS},
  author       = {Ery Arias-Castro and Siddharth Vishwanath},
  doi          = {10.1137/24M1661790},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1077-1097},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Stability of sequential lateration and of stress minimization in the presence of noise},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral barron space for deep neural network approximation. <em>SIMODS</em>, <em>7</em>(3), 1053-1076. (<a href='https://doi.org/10.1137/23M1598738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We prove the sharp embedding between the spectral Barron space and the Besov space with embedding constants independent of the input dimension. Given the spectral Barron space as the target function space, we prove a dimension-free convergence result that if the neural network contains hidden layers with units per layer, then the upper and lower bounds of the -approximation error are with , where is the smoothness index of the spectral Barron space.},
  archive      = {J_SIMODS},
  author       = {Yulei Liao and Pingbing Ming},
  doi          = {10.1137/23M1598738},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1053-1076},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Spectral barron space for deep neural network approximation},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReLU neural networks with linear layers are biased towards single- and multi-index models. <em>SIMODS</em>, <em>7</em>(3), 1021-1052. (<a href='https://doi.org/10.1137/24M1672158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Neural networks often operate in the overparameterized regime, in which there are far more parameters than training samples, allowing the training data to fit perfectly. That is, training the network effectively learns an interpolating function, and properties of the interpolant affect predictions the network will make on new samples. This manuscript explores how properties of such functions learned by neural networks of depth greater than two layers. Our framework considers a family of networks of varying depths that all have the same capacity but different representation costs. The representation cost of a function induced by a neural network architecture is the minimum sum of squared weights needed for the network to represent the function; it reflects the function space bias associated with the architecture. Our results show that adding additional linear layers to the input side of a shallow ReLU network yields a representation cost favoring functions with low mixed variation–-that is, it has limited variation in directions orthogonal to a low-dimensional subspace and can be well approximated by a single- or multi-index model. This bias occurs because minimizing the sum of squared weights of the linear layers is equivalent to minimizing a low-rank promoting Schatten quasi-norm of a single “virtual” weight matrix. Our experiments confirm this behavior in standard network training regimes. They additionally show that linear layers can improve generalization and the learned network is well-aligned with the true latent low-dimensional linear subspace when data is generated using a multi-index model.},
  archive      = {J_SIMODS},
  author       = {Suzanna Parkinson and Greg Ongie and Rebecca Willett},
  doi          = {10.1137/24M1672158},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {1021-1052},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {ReLU neural networks with linear layers are biased towards single- and multi-index models},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resolving the mixing time of the langevin algorithm to its stationary distribution for log-concave sampling. <em>SIMODS</em>, <em>7</em>(3), 993-1020. (<a href='https://doi.org/10.1137/24M1638689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sampling from a high-dimensional distribution is a fundamental task in statistics, engineering, and the sciences. A canonical approach is the Langevin algorithm, i.e., the Markov chain for the discretized Langevin diffusion. This is the sampling analog of gradient descent. Despite being studied for several decades in multiple communities, tight mixing bounds for this algorithm remain unresolved even in the seemingly simple setting of log-concave distributions over a bounded domain. This paper characterizes the mixing time of the Langevin algorithm to its stationary distribution in this setting (and others). This mixing result can be combined with any bound on the discretization bias in order to sample from the stationary distribution of the continuous Langevin diffusion. In this way, we disentangle the study of the mixing and bias of the Langevin algorithm. Our key insight is to introduce a technique from the differential privacy literature to the sampling literature. This technique, called privacy amplification by iteration, uses as a potential a variant of Rényi divergence that is made geometrically aware via optimal transport smoothing. This gives a short, simple proof of optimal mixing bounds and has several additional appealing properties. First, our approach removes all unnecessary assumptions required by other sampling analyses. Second, our approach unifies many settings: it extends unchanged if the Langevin algorithm uses projections, stochastic minibatch gradients, or strongly convex potentials (whereby our mixing time improves exponentially). Third, our approach exploits convexity only through the contractivity of a gradient step—reminiscent of how convexity is used in textbook proofs of gradient descent. In this way, we offer a new approach towards further unifying the analyses of optimization and sampling algorithms.},
  archive      = {J_SIMODS},
  author       = {Jason M. Altschuler and Kunal Talwar},
  doi          = {10.1137/24M1638689},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {993-1020},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Resolving the mixing time of the langevin algorithm to its stationary distribution for log-concave sampling},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Select without fear: Almost all minibatch schedules generalize optimally. <em>SIMODS</em>, <em>7</em>(3), 965-992. (<a href='https://doi.org/10.1137/23M1617096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We establish matching upper and lower generalization error bounds for minibatch gradient descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules, including stochastic GD (SGD) with random reshuffling, SGD with single shuffling, and incremental gradient methods. We consider smooth Lipschitz-convex/nonconvex/strongly convex loss functions and show that classical upper bounds for SGD also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Last, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones.},
  archive      = {J_SIMODS},
  author       = {Konstantinos E. Nikolakakis and Amin Karbasi and Dionysis Kalogerias},
  doi          = {10.1137/23M1617096},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {965-992},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Select without fear: Almost all minibatch schedules generalize optimally},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffeomorphic measure matching with kernels for generative modeling. <em>SIMODS</em>, <em>7</em>(3), 937-964. (<a href='https://doi.org/10.1137/24M1642202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This article presents a general framework for the transport of probability measures toward minimum divergence generative modeling and sampling using ODEs and reproducing kernel Hilbert spaces, inspired by ideas from diffeomorphic matching and image registration. A theoretical analysis of the proposed method is presented, giving a priori error bounds in terms of the complexity of the model, the number of samples in the training set, and model misspecification. An extensive suite of numerical experiments further highlights the properties, strengths, and weaknesses of the method and extends its applicability to other tasks, such as conditional simulation and inference.},
  archive      = {J_SIMODS},
  author       = {Biraj Pandey and Bamdad Hosseini and Pau Batlle and Houman Owhadi},
  doi          = {10.1137/24M1642202},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {937-964},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Diffeomorphic measure matching with kernels for generative modeling},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptively inexact first-order method for bilevel optimization with application to hyperparameter learning. <em>SIMODS</em>, <em>7</em>(3), 906-936. (<a href='https://doi.org/10.1137/24M1653513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Various tasks in data science are modeled utilizing the variational regularization approach, where manually selecting regularization parameters presents a challenge. The difficulty gets exacerbated when employing regularizers involving a large number of hyperparameters. To overcome this challenge, bilevel learning can be employed to learn such parameters from data. However, neither exact function values nor exact gradients with respect to the hyperparameters are attainable, necessitating methods that only rely on inexact evaluation of such quantities. State-of-the-art inexact gradient-based methods a priori select a sequence of the required accuracies and cannot identify an appropriate step size since the Lipschitz constant of the hypergradient is unknown. In this work, we propose an algorithm with backtracking line search that only relies on inexact function evaluations and hypergradients and show convergence to a stationary point. Furthermore, the proposed algorithm determines the required accuracy dynamically rather than manually selected before running it. Our numerical experiments demonstrate the efficiency and feasibility of our approach for hyperparameter estimation on a range of relevant problems in imaging and data science such as total variation and field of experts denoising and multinomial logistic regression. Particularly, the results show that the algorithm is robust to its own hyperparameters such as the initial accuracies and step size.},
  archive      = {J_SIMODS},
  author       = {Mohammad Sadegh Salehi and Subhadip Mukherjee and Lindon Roberts and Matthias J. Ehrhardt},
  doi          = {10.1137/24M1653513},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {906-936},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {An adaptively inexact first-order method for bilevel optimization with application to hyperparameter learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online machine teaching under learner uncertainty: Gradient descent learners of a quadratic loss. <em>SIMODS</em>, <em>7</em>(3), 884-905. (<a href='https://doi.org/10.1137/24M1657997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We revisit the framework of online machine teaching, a special case of active learning in which a teacher with full knowledge of a model attempts to train a learner by adaptively presenting examples. While online machine teaching example selection strategies are typically designed assuming omniscience, i.e., the teacher has absolute knowledge of the learner state, we show that efficient machine teaching is possible even when the teacher is uncertain about the learner initialization. Specifically, we consider the case of learners that perform gradient descent of a quadratic loss to learn a linear classifier, and we propose an online machine teaching algorithm in which the teacher simultaneously learns the learner state while teaching the learner. We theoretically show that the learner’s mean square error decreases exponentially with the number of examples, thus achieving a performance similar to the omniscient case and outperforming two stage strategies that first attempt to make the teacher omniscient before teaching. We empirically illustrate our approach in the context of a cross-lingual sentiment analysis problem.},
  archive      = {J_SIMODS},
  author       = {Belen Martin-Urcelay and Christopher J. Rozell and Matthieu R. Bloch},
  doi          = {10.1137/24M1657997},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {884-905},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Online machine teaching under learner uncertainty: Gradient descent learners of a quadratic loss},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simple alternating minimization provably solves complete dictionary learning. <em>SIMODS</em>, <em>7</em>(3), 855-883. (<a href='https://doi.org/10.1137/23M1568120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper focuses on the noiseless complete dictionary learning problem, where the goal is to represent a set of given signals as linear combinations of a small number of atoms from a learned dictionary. There are two main challenges faced by theoretical and practical studies of dictionary learning: the lack of theoretical guarantees for practically used heuristic algorithms and their poor scalability when dealing with huge-scale datasets. Towards addressing these issues, we propose a simple and efficient algorithm that provably recovers the ground truth when applied to the nonconvex and discrete formulation of the problem in the noiseless setting. We also extend our proposed method to mini-batch and online settings where the data is huge-scale or arrives continuously over time. At the core of our proposed method lies an efficient preconditioning technique that transforms the unknown dictionary to a near-orthonormal one, for which we prove a simple alternating minimization technique converges linearly to the ground truth under minimal conditions. Our numerical experiments on synthetic and real datasets showcase the superiority of our method compared with the existing techniques.},
  archive      = {J_SIMODS},
  author       = {Geyu Liang and Gavin Zhang and Salar Fattahi and Richard Y. Zhang},
  doi          = {10.1137/23M1568120},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {855-883},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Simple alternating minimization provably solves complete dictionary learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence of gradient descent for recurrent neural networks: A nonasymptotic analysis. <em>SIMODS</em>, <em>7</em>(2), 826-854. (<a href='https://doi.org/10.1137/24M1642500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We analyze recurrent neural networks with diagonal hidden-to-hidden weight matrices, trained with gradient descent in the supervised learning setting, and prove that gradient descent can achieve optimality without massive overparameterization. Our in-depth nonasymptotic analysis (i) provides improved bounds on the network size in terms of the sequence length , sample size , and ambient dimension , and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function. Remarkably, this analysis reveals that an appropriately initialized recurrent neural network trained with samples can achieve optimality with a network size that scales only logarithmically with . This sharply contrasts with the prior works that require high-order polynomial dependency of on to establish strong regularity conditions. Our results are based on an explicit characterization of the class of dynamical systems that can be approximated and learned by recurrent neural networks via norm-constrained transportation mappings, and establishing local smoothness properties of the hidden state with respect to the learnable parameters.},
  archive      = {J_SIMODS},
  author       = {Semih Cayci and Atilla Eryilmaz},
  doi          = {10.1137/24M1642500},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {826-854},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Convergence of gradient descent for recurrent neural networks: A nonasymptotic analysis},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalization bounds for message passing networks on mixture of graphons. <em>SIMODS</em>, <em>7</em>(2), 802-825. (<a href='https://doi.org/10.1137/24M1651526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the generalization capabilities of message passing neural networks (MPNNs), a prevalent class of graph neural networks. We derive generalization bounds specifically for MPNNs with normalized sum aggregation and mean aggregation. Our analysis is based on a data generation model incorporating a finite set of template graphons. Each graph within this framework is generated by sampling from one of the graphons with a certain degree of perturbation. In particular, we extend previous MPNN generalization results to a more realistic setting, which includes the following modifications: (1) We analyze simple random graphs with Bernoulli-distributed edges instead of weighted graphs, (2) we sample both graphs and graph signals from perturbed graphons instead of clean graphons, and (3) we analyze sparse graphs instead of dense graphs. In this more realistic and challenging scenario, we provide a generalization bound that decreases as the average number of nodes in the graphs increases. Our results imply that MPNNs with higher complexity than the size of the training set can still generalize effectively as long as the graphs are sufficiently large.},
  archive      = {J_SIMODS},
  author       = {Sohir Maskey and Gitta Kutyniok and Ron Levie},
  doi          = {10.1137/24M1651526},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {802-825},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Generalization bounds for message passing networks on mixture of graphons},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Insights into kernel PCA with application to multivariate extremes. <em>SIMODS</em>, <em>7</em>(2), 777-801. (<a href='https://doi.org/10.1137/24M1678635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The principal goal of this paper is twofold: (1) provide general insights into kernel PCA showing that it can effectively identify clusters of preimages when the data consists of a discrete signal with added noise, and (2) apply kernel PCA for describing the dependence structure of multivariate extremes. Kernel PCA has been motivated as a tool for denoising and clustering of the approximate preimages. The idea is that such a structure should be captured by the first principal components in the corresponding function space. We provide some simple insights that naturally lead to clustered preimages when the underlying data comes from a discrete signal corrupted by noise. Specifically, we use the Davis–Kahan theory to give a perturbation bound on the performance of preimages that quantifies the impact of noise in clustering a discrete signal. We then propose kernel PCA as a method for analyzing the dependence structure of multivariate extremes and demonstrate that it can be a powerful tool for clustering and dimension reduction. In this case, kernel PCA is applied only to the extremal part of the sample, i.e., the angular part of random vectors for which the radius exceeds a large threshold. More specifically, we focus on the asymptotic dependence of multivariate extremes characterized by the angular or spectral measure in extreme value theory and provide a careful analysis in the case where the extremes are generated from a linear factor model. We give theoretical analysis of the ingredients in the Davis–Kahan perturbation bounds by leveraging their asymptotic distribution. Our theoretical findings are complemented with numerical experiments illustrating the finite sample performance of our methods.},
  archive      = {J_SIMODS},
  author       = {Marco Avella Medina and Richard A. Davis and Gennady Samorodnitsky},
  doi          = {10.1137/24M1678635},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {777-801},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Insights into kernel PCA with application to multivariate extremes},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invariant kernels on riemannian symmetric spaces: A harmonic-analytic approach. <em>SIMODS</em>, <em>7</em>(2), 752-776. (<a href='https://doi.org/10.1137/23M1613566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This work aims to prove that the classical Gaussian kernel, when defined on a non-Euclidean symmetric space, is never positive-definite for any choice of parameter. To achieve this goal, the paper develops new geometric and analytical arguments. These provide a rigorous characterization of the positive-definiteness of the Gaussian kernel, which is complete but for a limited number of scenarios in low dimensions that are treated by numerical computations. Chief among these results are the L-Godement theorems (where ), which provide verifiable necessary and sufficient conditions for a kernel defined on a symmetric space of noncompact type to be positive-definite. A celebrated theorem, sometimes called the Bochner–Godement theorem, already gives such conditions and is far more general in its scope but is especially hard to apply. Beyond the connection with the Gaussian kernel, the new results in this work lay out a blueprint for the study of invariant kernels on symmetric spaces, bringing forth specific harmonic analysis tools that suggest many future applications.},
  archive      = {J_SIMODS},
  author       = {Nathaël Da Costa and Cyrus Mostajeran and Juan-Pablo Ortega and Salem Said},
  doi          = {10.1137/23M1613566},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {752-776},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Invariant kernels on riemannian symmetric spaces: A harmonic-analytic approach},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A priori estimates for deep residual network in continuous-time reinforcement learning. <em>SIMODS</em>, <em>7</em>(2), 723-751. (<a href='https://doi.org/10.1137/24M1640811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignore the unique characteristics of continuous-time control problems, are unable to directly estimate the generalization error of the Bellman optimal loss, and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semigroup and Lipschitz properties. Under this method, we can directly analyze the a priori generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an a priori generalization error without the curse of dimensionality.},
  archive      = {J_SIMODS},
  author       = {Shuyu Yin and Qixuan Zhou and Fei Wen and Tao Luo},
  doi          = {10.1137/24M1640811},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {723-751},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A priori estimates for deep residual network in continuous-time reinforcement learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear tomographic reconstruction via nonsmooth optimization. <em>SIMODS</em>, <em>7</em>(2), 699-722. (<a href='https://doi.org/10.1137/24M1678982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study iterative signal reconstruction in computed tomography, wherein measurements are produced by a linear transformation of the unknown signal followed by an exponential nonlinear map. Approaches based on preprocessing the data with a log transform and then solving the resulting linear inverse problem are tempting since they are amenable to convex optimization methods; however, such methods perform poorly when the underlying image has high dynamic range, as in X-ray imaging of tissue with embedded metal. We show that a suitably initialized subgradient method applied to a natural nonsmooth, nonconvex loss function produces iterates that converge to the unknown signal of interest at a geometric rate under the statistical model proposed by Fridovich-Keil et al. [Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction, 2023]. Our recovery program enjoys improved conditioning compared to the formulation proposed by the latter work, enabling faster iterative reconstruction from substantially fewer samples.},
  archive      = {J_SIMODS},
  author       = {Vasileios Charisopoulos and Rebecca Willett},
  doi          = {10.1137/24M1678982},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {699-722},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Nonlinear tomographic reconstruction via nonsmooth optimization},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized nyström approximation of non-negative self-adjoint operators. <em>SIMODS</em>, <em>7</em>(2), 670-698. (<a href='https://doi.org/10.1137/24M165082X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The randomized singular value decomposition (SVD) has become a popular approach to computing cheap, yet accurate, low-rank approximations to matrices due to its efficiency and strong theoretical guarantees. Recent work by Boullé and Townsend [Found. Comput. Math., 23 (2023), pp. 709–739] presents an infinite-dimensional analogue of the randomized SVD to approximate Hilbert–Schmidt operators. However, many applications involve computing low-rank approximations to symmetric positive semi-definite matrices. In this setting, it is well established that the randomized Nyström approximation is usually preferred over the randomized SVD. This paper explores an infinite-dimensional analogue of the Nyström approximation to compute low-rank approximations to non-negative self-adjoint trace-class operators. We present an analysis of the method and, along the way, improve the existing infinite-dimensional bounds for the randomized SVD. Our analysis yields bounds on the expected value and tail bounds for the Nyström approximation error in the operator, trace, and Hilbert–Schmidt norms. Numerical experiments on integral operators arising from Gaussian process sampling and Bayesian inverse problems are used to validate the proposed infinite-dimensional Nyström algorithm.},
  archive      = {J_SIMODS},
  author       = {David Persson and Nicolas Boullé and Daniel Kressner},
  doi          = {10.1137/24M165082X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {670-698},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Randomized nyström approximation of non-negative self-adjoint operators},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding planted cliques using gradient descent. <em>SIMODS</em>, <em>7</em>(2), 643-669. (<a href='https://doi.org/10.1137/24M1680489'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The planted clique problem is a paradigmatic model of statistical-to-computational gaps: the planted clique is information-theoretically detectable if its size , but polynomial-time algorithms only exist for the recovery task when . By now, there are many algorithms that succeed as soon as . Glaringly, however, no black-box optimization method, e.g., gradient descent or the Metropolis process, has been shown to work. In fact, Chen, Mossel, and Zadik recently showed that any Metropolis process whose state space is the set of cliques fails to find any sublinear-sized planted clique in polynomial time if initialized naturally from the empty set. We show that using the method of Lagrange multipliers, namely optimizing the Hamiltonian given by the sum of the objective function and the clique constraint over the space of all subgraphs, succeeds. In particular, we prove that Markov chains which minimize this Hamiltonian (gradient descent and a low-temperature relaxation of it) succeed at recovering planted cliques of size if initialized from the full graph. Importantly, initialized from the empty set, the relaxation still does not help the gradient descent find sublinear planted cliques. We also demonstrate robustness of these Markov chain approaches under a natural contamination model.},
  archive      = {J_SIMODS},
  author       = {Reza Gheissari and Aukosh Jagannath and Yiming Xu},
  doi          = {10.1137/24M1680489},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {643-669},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Finding planted cliques using gradient descent},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Landmark alternating diffusion. <em>SIMODS</em>, <em>7</em>(2), 621-642. (<a href='https://doi.org/10.1137/24M1656682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Alternating Diffusion (AD) is a commonly used diffusion-based sensor fusion algorithm. While it has been successfully applied to various problems, its computational burden remains a limitation. Inspired by the use of landmarks considered in the Robust and Scalable Embedding via Landmark Diffusion (ROSELAND), we propose a variation of AD, called Landmark AD (LAD), which captures the essence of AD while offering superior computational efficiency. We provide a series of theoretical analyses of LAD under the manifold setup and apply it to the automatic sleep stage annotation problem with two electroencephalogram channels to demonstrate its application.},
  archive      = {J_SIMODS},
  author       = {Sing-Yuan Yeh and Hau-Tieng Wu and Ronen Talmon and Mao-Pei Tsui},
  doi          = {10.1137/24M1656682},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {621-642},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Landmark alternating diffusion},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the nonconvexity of push-forward constraints and its consequences in machine learning. <em>SIMODS</em>, <em>7</em>(2), 597-620. (<a href='https://doi.org/10.1137/24M1645036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The push-forward operation enables one to redistribute a probability measure through a deterministic map. It plays a key role in statistics and optimization: many learning problems (notably from optimal transport, generative modeling, and algorithmic fairness) include constraints or penalties framed as push-forward conditions on the model. However, the literature lacks general theoretical insights on the (non)convexity of such constraints and its consequences on the associated learning problems. This paper aims at filling this gap. In the first part, we provide a range of sufficient and necessary conditions for the (non)convexity of two sets of functions: the maps transporting one probability measure to another and the maps inducing equal output distributions across distinct probability measures. This highlights that for most probability measures, these push-forward constraints are not convex. In the second part, we show how this result implies critical limitations on the design of convex optimization problems for learning generative models or groupwise fair predictors. This work will hopefully help researchers and practitioners have a better understanding of the critical impact of push-forward conditions onto convexity.},
  archive      = {J_SIMODS},
  author       = {Lucas De Lara and Mathis Deronzier and Alberto González-Sanz and Virgile Foy},
  doi          = {10.1137/24M1645036},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {597-620},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On the nonconvexity of push-forward constraints and its consequences in machine learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). G-mapper: Learning a cover in the mapper construction. <em>SIMODS</em>, <em>7</em>(2), 572-596. (<a href='https://doi.org/10.1137/24M1641312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. However, the Mapper algorithm requires tuning several parameters in order to generate a “nice” Mapper graph. This paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on G-means clustering, which searches for the optimal number of clusters in -means by iteratively applying the Anderson–Darling test. Our splitting procedure employs a Gaussian mixture model to carefully choose the cover according to the distribution of the given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets, while also running significantly faster than a previous iterative method.},
  archive      = {J_SIMODS},
  author       = {Enrique Alvarado and Robin Belton and Emily Fischer and Kang-Ju Lee and Sourabh Palande and Sarah Percival and Emilie Purvine},
  doi          = {10.1137/24M1641312},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {572-596},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {G-mapper: Learning a cover in the mapper construction},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral properties of elementwise-transformed spiked matrices. <em>SIMODS</em>, <em>7</em>(2), 542-571. (<a href='https://doi.org/10.1137/23M1627985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This work concerns elementwise transformations of spiked matrices: . Here, is a function applied elementwise, is a low-rank signal matrix, and is white noise. We find that principal component analysis (PCA) is capable of recovering signal under highly nonlinear or discontinuous transformations. Specifically, in the high-dimensional setting where is of size with and , we uncover a phase transition: for signal-to-noise ratios above a precise threshold—depending on , the distribution of elements of , and the limiting aspect ratio —the principal components of (partially) recover those of . Below this threshold, the principal components of are asymptotically orthogonal to the signal. In contrast, in the standard setting where PCA is applied to directly, the analogous phase transition depends only on . Similar phenomena occur with square and symmetric and a generalized Wigner matrix. This model accommodates diverse data types not covered by prior spiked-matrix theory, including forms of discrete data, preprocessed data, and data with missing values. Our results provide theoretical justification for applying PCA to such data, helping to elucidate PCA’s empirical success.},
  archive      = {J_SIMODS},
  author       = {Michael J. Feldman},
  doi          = {10.1137/23M1627985},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {542-571},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Spectral properties of elementwise-transformed spiked matrices},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic gradient descent for streaming linear and rectified linear systems with adversarial corruptions. <em>SIMODS</em>, <em>7</em>(2), 516-541. (<a href='https://doi.org/10.1137/24M1652167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose SGD-exp, a stochastic gradient descent approach for linear and ReLU regressions under Massart noise (adversarial semi-random corruption model) for the fully streaming setting. We show novel nearly linear convergence guarantees of SGD-exp to the true parameter with up to Massart corruption rate, and with any corruption rate in the case of symmetric oblivious corruptions. This is the first convergence guarantee result for robust ReLU regression in the streaming setting, and it shows the improved convergence rate over previous robust methods for linear regression due to a choice of an exponentially decaying step size, known for its efficiency in practice. Our analysis is based on the drift analysis of a discrete stochastic process, which could also be interesting on its own.},
  archive      = {J_SIMODS},
  author       = {Halyun Jeong and Deanna Needell and Elizaveta Rebrova},
  doi          = {10.1137/24M1652167},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {516-541},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Stochastic gradient descent for streaming linear and rectified linear systems with adversarial corruptions},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The positivity of the neural tangent kernel. <em>SIMODS</em>, <em>7</em>(2), 495-515. (<a href='https://doi.org/10.1137/24M1659534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Neural tangent kernel (NTK) has emerged as a fundamental concept in the study of wide neural networks. In particular, it is known that the positivity of the NTK is directly related to the memorization capacity of sufficiently wide networks, i.e., to the possibility of reaching zero loss in training via gradient descent. Here we will improve on previous works and obtain a sharp result concerning the positivity of the NTK of feedforward networks of any depth. More precisely, we will show that, for any nonpolynomial activation function, the NTK is strictly positive definite. Our results are based on a novel characterization of polynomial functions, which is of independent interest.},
  archive      = {J_SIMODS},
  author       = {Luís Carvalho and João L. Costa and José Mourão and Gonçalo Oliveira},
  doi          = {10.1137/24M1659534},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {495-515},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The positivity of the neural tangent kernel},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient algorithms for regularized nonnegative scale-invariant low-rank approximation models. <em>SIMODS</em>, <em>7</em>(2), 468-494. (<a href='https://doi.org/10.1137/24M1657948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Regularized nonnegative low-rank approximations, such as sparse nonnegative matrix factorization or sparse nonnegative Tucker decomposition, form an important branch of dimensionality reduction models known for their enhanced interpretability. From a practical perspective, however, selecting appropriate regularizers and regularization coefficients, as well as designing efficient algorithms, remains challenging due to the multifactor nature of these models and the limited theoretical guidance available. This paper addresses these challenges by studying a more general model, the homogeneous regularized scale-invariant model. We prove that the scaleinvariance inherent to low-rank approximation models induces an implicit regularization effect that balances solutions. This insight provides a deeper understanding of the role of regularization functions in low-rank approximation models, informs the selection of regularization hyperparameters, and enables the design of balancing strategies to accelerate the empirical convergence of optimization algorithms. Additionally, we propose a generic majorization-minimization algorithm capable of handling -regularized nonnegative low-rank approximations with non-Euclidean loss functions, with convergence guarantees. Our contributions are demonstrated on sparse nonnegative matrix factorization, ridge-regularized nonnegative canonical polyadic decomposition, and sparse nonnegative Tucker decomposition.},
  archive      = {J_SIMODS},
  author       = {Jeremy E. Cohen and Valentin Leplat},
  doi          = {10.1137/24M1657948},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {468-494},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Efficient algorithms for regularized nonnegative scale-invariant low-rank approximation models},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble linear interpolators: The role of ensembling. <em>SIMODS</em>, <em>7</em>(2), 438-467. (<a href='https://doi.org/10.1137/24M1642548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Interpolators are unstable. For example, the minimum norm least squares interpolator exhibits unbounded test errors when dealing with noisy data. In this paper, we study how ensemble stabilizes and thus improves the generalization performance, measured by the out-of-sample prediction risk, of an individual interpolator. We focus on bagged linear interpolators, as bagging is a popular randomization-based ensemble method that can be implemented in parallel. We introduce the multiplier-bootstrap-based bagged least squares estimator, which can then be formulated as an average of the sketched least squares estimators. The proposed multiplier bootstrap encompasses the classical bootstrap with replacement as a special case, along with a more intriguing variant which we call the Bernoulli bootstrap. Focusing on the proportional regime where the sample size scales proportionally with the feature dimensionality, we investigate the out-of-sample prediction risks of the sketched and bagged least squares estimators in both underparametrized and overparameterized regimes. Our results reveal the statistical roles of sketching and bagging. In particular, sketching modifies the aspect ratio and shifts the interpolation threshold of the minimum norm estimator. However, the risk of the sketched estimator continues to be unbounded around the interpolation threshold due to excessive variance. In stark contrast, bagging effectively mitigates this variance, leading to a bounded limiting out-of-sample prediction risk. To further understand this stability improvement property, we establish that bagging acts as a form of implicit regularization, substantiated by the equivalence of the bagged estimator with its explicitly regularized counterpart. We also discuss several extensions.},
  archive      = {J_SIMODS},
  author       = {Mingqi Wu and Qiang Sun},
  doi          = {10.1137/24M1642548},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {438-467},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Ensemble linear interpolators: The role of ensembling},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). \({O({k})}\)-equivariant dimensionality reduction on stiefel manifolds. <em>SIMODS</em>, <em>7</em>(2), 410-437. (<a href='https://doi.org/10.1137/23M1603443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Many real-world data sets live on high-dimensional Stiefel and Grassmannian manifolds, and , respectively, and benefit from projection onto lower-dimensional Stiefel (respectively, Grassmannian) manifolds. In this work, we propose an algorithm called principal Stiefel coordinates to reduce data dimensionality from to in an -equivariant manner . We begin by observing that each element defines an isometric embedding of into . Next, we optimize for such an embedding map that minimizes data fit error by warm-starting with the output of principal component analysis (PCA) and applying gradient descent. Then we define a continuous and -equivariant map that acts as a “closest point operator” to project the data onto the image of in under the embedding determined by , while minimizing distortion. Because this dimensionality reduction is -equivariant, these results extend to Grassmannian manifolds as well. Lastly, we show that the PCA output globally minimizes projection error in a noiseless setting, but that our algorithm achieves a meaningfully different and improved outcome when the data does not lie exactly on the image of a linearly embedded lower-dimensional Stiefel manifold as above. Multiple numerical experiments using synthetic and real-world data are performed.},
  archive      = {J_SIMODS},
  author       = {Andrew Lee and Harlin Lee and Jose A. Perea and Nikolas Schonsheck and Madeleine Weinstein},
  doi          = {10.1137/23M1603443},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {410-437},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {\({O({k})}\)-equivariant dimensionality reduction on stiefel manifolds},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random projection neural networks of best approximation: Convergence theory and practical applications. <em>SIMODS</em>, <em>7</em>(2), 385-409. (<a href='https://doi.org/10.1137/24M1639890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We investigate the concept of best approximation for feedforward neural networks (FNNs) and explore their convergence properties through the lens of random projection neural networks (RPNNs). RPNNs have predetermined and permanently fixed internal weights and biases, offering computational efficiency. We demonstrate that there exists a choice of external weights, for any family of such RPNNs, with nonpolynomial infinitely differentiable activation functions, that exhibit an exponential convergence rate when approximating any infinitely differentiable function. For illustration purposes, we test the proposed RPNN-based function approximation, with parsimoniously chosen basis functions, across five benchmark function approximation problems. Results show that RPNNs achieve performance comparable to established methods, such as Legendre polynomials, highlighting their potential for efficient and accurate function approximation.},
  archive      = {J_SIMODS},
  author       = {Gianluca Fabiani},
  doi          = {10.1137/24M1639890},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {385-409},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Random projection neural networks of best approximation: Convergence theory and practical applications},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CA-PCA: Manifold dimension estimation, adapted for curvature. <em>SIMODS</em>, <em>7</em>(1), 355-383. (<a href='https://doi.org/10.1137/23M1575135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The success of algorithms in the analysis of high-dimensional data is often attributed to the manifold hypothesis, which supposes that this data lie on or near a manifold of much lower dimension. It is often useful to determine or estimate the dimension of this manifold before performing dimension reduction, for instance. Existing methods for dimension estimation are calibrated using a flat unit ball. In this paper, we develop CA-PCA, a version of local PCA based instead on a calibration of a quadratic embedding, acknowledging the curvature of the underlying manifold. Numerous careful experiments show that this adaptation improves the estimator in a wide range of settings.},
  archive      = {J_SIMODS},
  author       = {Anna C. Gilbert and Kevin O’Neill},
  doi          = {10.1137/23M1575135},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {355-383},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {CA-PCA: Manifold dimension estimation, adapted for curvature},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random fourier signature features. <em>SIMODS</em>, <em>7</em>(1), 329-354. (<a href='https://doi.org/10.1137/23M1620478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Tensor algebras give rise to one of the most powerful measures of similarity for sequences of arbitrary length called the signature kernel accompanied with attractive theoretical guarantees from stochastic analysis. Previous algorithms to compute the signature kernel scale quadratically in terms of the length and number of the sequences. To mitigate this severe computational bottleneck, we develop a random Fourier feature-based acceleration of the signature kernel acting on the inherently non-Euclidean domain of sequences. We show uniform approximation guarantees for the proposed unbiased estimator of the signature kernel, while keeping its computation linear in the sequence length and number. In addition, combined with recent advances on tensor projections, we derive two even more scalable time series features with favorable concentration properties and computational complexity both in time and memory. Our empirical results show that the reduction in computational cost comes at a negligible price in terms of accuracy on moderate size datasets, and it enables one to scale to large datasets up to a million time series. We release the code publicly available at https://github.com/tgcsaba/ksig.},
  archive      = {J_SIMODS},
  author       = {Csaba Tóth and Harald Oberhauser and Zoltán Szabó},
  doi          = {10.1137/23M1620478},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {329-354},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Random fourier signature features},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised Gromov–Wasserstein optimal transport with metric-preserving constraints. <em>SIMODS</em>, <em>7</em>(1), 301-328. (<a href='https://doi.org/10.1137/24M1630499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce the supervised Gromov–Wasserstein (sGW) optimal transport, an extension of Gromov–Wasserstein that incorporates potential infinity entries in the cost tensor. These infinity entries enable sGW to enforce application-induced constraints on preserving pairwise distance to a certain extent. A numerical solver is proposed for the sGW problem and the effectiveness is demonstrated in various numerical experiments. The high-order constraints in sGW are transferred to constraints on the coupling matrix by solving a minimal vertex cover problem. The transformed problem is solved by the mirror-C descent iteration coupled with the supervised optimal transport solver. In the numerical experiments, we first validate the proposed framework by applying it to matching synthetic datasets and investigating the impact of the model parameters. Additionally, we apply sGW to aligning single-cell RNA sequencing data where the datasets are partially overlapping and only intra-dataset metrics are used. Through comparisons with other Gromov–Wasserstein variants, we demonstrate that sGW offers an additional utility of controlling distance preservation, leading to automatic estimation of overlapping portions of datasets, which brings improved stability and flexibility in data-driven applications. The codes for sGW and for reproducing the results are available on Github [https://github.com/zcang/supervisedGW].},
  archive      = {J_SIMODS},
  author       = {Zixuan Cang and Yaqi Wu and Yanxiang Zhao},
  doi          = {10.1137/24M1630499},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {301-328},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Supervised Gromov–Wasserstein optimal transport with metric-preserving constraints},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). First-order conditions for optimization in the wasserstein space. <em>SIMODS</em>, <em>7</em>(1), 274-300. (<a href='https://doi.org/10.1137/23M156687X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study first-order optimality conditions for constrained optimization in the Wasserstein space, whereby one seeks to minimize a real-valued function over the space of probability measures endowed with the Wasserstein distance. Our analysis combines recent insights on the geometry and the differential structure of the Wasserstein space with more classical calculus of variations. We show that simple rationales such as “setting the derivative to zero” and “gradients are aligned at optimality” carry over to the Wasserstein space. We deploy our tools to study and solve optimization problems in the setting of distributionally robust optimization and statistical inference. The generality of our methodology allows us to naturally deal with functionals, such as mean-variance, Kullback–Leibler divergence, and Wasserstein distance, which are traditionally difficult to study in a unified framework.},
  archive      = {J_SIMODS},
  author       = {Nicolas Lanzetti and Saverio Bolognani and Florian Dörfler},
  doi          = {10.1137/23M156687X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {274-300},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {First-order conditions for optimization in the wasserstein space},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel-based regularized learning with random projections: Beyond least squares. <em>SIMODS</em>, <em>7</em>(1), 253-273. (<a href='https://doi.org/10.1137/23M1602954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider kernel-based regularized learning with the help of the random projection technique to ease its computational burden. The random projection approaches we study here include the randomized sketching method and the Nyström approximation method. Current works under the least squares loss demonstrated an optimal learning rate under an appropriate source condition and capacity condition. However, beyond this simplest loss, it seems challenging to appropriately incorporate both conditions due to the unavailability of a closed-form solution. In this work, we consider a sufficiently general class of convex losses which include logistic loss, quantile loss, and hinge loss, for example, and establish the same optimal learning rate as in the least squares case under mild regularity assumptions. Our result also covers the unattainable case where the true function is not in the reproducing kernel Hilbert space. To incorporate the source condition, Young’s inequality for operators is used, while to characterize the capacity, Rademacher complexity is adopted. We illustrate the performances of random projection with some numerical examples.},
  archive      = {J_SIMODS},
  author       = {Jiamin Liu and Junzhuo Gao and Heng Lian},
  doi          = {10.1137/23M1602954},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {253-273},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Kernel-based regularized learning with random projections: Beyond least squares},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating gaussian mixtures using sparse polynomial moment systems. <em>SIMODS</em>, <em>7</em>(1), 224-252. (<a href='https://doi.org/10.1137/23M1610082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The method of moments is a classical statistical technique for density estimation that solves a system of moment equations to estimate the parameters of an unknown distribution. A fundamental question critical to understanding identifiability asks how many moment equations are needed to get finitely many solutions and how many solutions there are. We answer this question for classes of Gaussian mixture models using the tools of polyhedral geometry. In addition, we show that a generic Gaussian -mixture model is identifiable from its first moments. Using these results, we present a homotopy algorithm that performs parameter recovery for high-dimensional Gaussian mixture models where the number of paths tracked scales linearly in the dimension.},
  archive      = {J_SIMODS},
  author       = {Julia Lindberg and Carlos Améndola and Jose Israel Rodriguez},
  doi          = {10.1137/23M1610082},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {224-252},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Estimating gaussian mixtures using sparse polynomial moment systems},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multifidelity covariance estimation via regression on the manifold of symmetric positive definite matrices. <em>SIMODS</em>, <em>7</em>(1), 189-223. (<a href='https://doi.org/10.1137/23M159247X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties enabling practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that the MRMF estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.},
  archive      = {J_SIMODS},
  author       = {Aimee Maurais and Terrence Alsup and Benjamin Peherstorfer and Youssef M. Marzouk},
  doi          = {10.1137/23M159247X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {189-223},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Multifidelity covariance estimation via regression on the manifold of symmetric positive definite matrices},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric finite mixture models with possible shape constraints: A cubic newton approach. <em>SIMODS</em>, <em>7</em>(1), 163-188. (<a href='https://doi.org/10.1137/21M1430972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We explore computational aspects of maximum likelihood estimation of the mixture proportions in a nonparametric finite mixture model—a convex optimization problem with old roots in statistics. Motivated by problems in shape constrained inference, we also consider structured variants of this problem with additional convex polyhedral constraints. We propose a new cubic regularized Newton method for this problem and present novel worst-case and local computational guarantees for our algorithm. We extend earlier work by Nesterov and Polyak to the case of a self-concordant objective with polyhedral constraints, such as the ones considered herein. We propose a Frank–Wolfe method to solve the cubic regularized Newton subproblem and derive efficient solutions for the linear optimization oracles that may be of independent interest. In the particular case of Gaussian mixtures without shape constraints, we derive bounds on how well the finite mixture problem approximates the infinite-dimensional Kiefer–Wolfowitz maximum likelihood estimator. Experiments on synthetic and real datasets suggest that our proposed algorithms exhibit improved runtimes and scalability features over prior benchmarks.},
  archive      = {J_SIMODS},
  author       = {Haoyue Wang and Shibal Ibrahim and Rahul Mazumder},
  doi          = {10.1137/21M1430972},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {163-188},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Nonparametric finite mixture models with possible shape constraints: A cubic newton approach},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized low-rank approximations beyond gaussian random matrices. <em>SIMODS</em>, <em>7</em>(1), 136-162. (<a href='https://doi.org/10.1137/23M1593255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper expands the analysis of randomized low-rank approximation beyond the Gaussian distribution to four classes of random matrices: (1) independent sub-Gaussian entries, (2) independent sub-Gaussian columns, (3) independent bounded columns, and (4) independent columns with bounded second moment. Using a novel interpretation of the low-rank approximation error involving sample covariance matrices, we provide insight into the requirements of a good random matrix for randomized low-rank approximations. Although our bounds involve unspecified absolute constants (a consequence of underlying nonasymptotic theory of random matrices), they allow for qualitative comparisons across distributions. The analysis offers some details on the minimal number of samples (the number of columns of the random matrix ) and the error in the resulting low-rank approximation. We illustrate our analysis in the context of the randomized subspace iteration method as a representative algorithm for low-rank approximation; however, all the results are broadly applicable to other low-rank approximation techniques. We conclude our discussion with numerical examples using both synthetic and real-world test matrices.},
  archive      = {J_SIMODS},
  author       = {Arvind K. Saibaba and Agnieszka Międlar},
  doi          = {10.1137/23M1593255},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {136-162},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Randomized low-rank approximations beyond gaussian random matrices},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Function-space optimality of neural architectures with multivariate nonlinearities. <em>SIMODS</em>, <em>7</em>(1), 110-135. (<a href='https://doi.org/10.1137/23M1620971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We investigate the function-space optimality (specifically, the Banach-space optimality) of a large class of shallow neural architectures with multivariate nonlinearities/activation functions. To that end, we construct a new family of Banach spaces defined via a regularization operator, the -plane transform, and a sparsity-promoting norm. We prove a representer theorem that states that the solution sets to learning problems posed over these Banach spaces are completely characterized by neural architectures with multivariate nonlinearities. These optimal architectures have skip connections and are tightly connected to orthogonal weight normalization and multi-index models, both of which have received recent interest in the neural network community. Our framework is compatible with a number of classical nonlinearities including the rectified linear unit activation function, the norm activation function, and the radial basis functions found in the theory of thin-plate/polyharmonic splines. We also show that the underlying spaces are special instances of reproducing kernel Banach spaces and variation spaces. Our results shed light on the regularity of functions learned by neural networks trained on data, particularly with multivariate nonlinearities, and provide new theoretical motivation for several architectural choices found in practice.},
  archive      = {J_SIMODS},
  author       = {Rahul Parhi and Michael Unser},
  doi          = {10.1137/23M1620971},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {110-135},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Function-space optimality of neural architectures with multivariate nonlinearities},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KL convergence guarantees for score diffusion models under minimal data assumptions. <em>SIMODS</em>, <em>7</em>(1), 86-109. (<a href='https://doi.org/10.1137/23M1613670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Diffusion models are a new class of generative models that revolve around the estimation of the score function associated with an SDE. Subsequent to its acquisition, the approximated score function is then harnessed to simulate the corresponding time-reversal process, ultimately enabling the generation of approximate data samples. Despite their evident practical significance these models carry, a notable challenge persists in the form of a lack of comprehensive quantitative results, especially in scenarios involving nonregular scores and estimators. In almost all reported bounds in Kullback–Leibler (KL) divergence, it is assumed that either the score function or its approximation is Lipschitz uniformly in time. However, this condition is very restrictive in practice or appears to be difficult to establish. To circumvent this issue, previous works mainly focused on establishing convergence bounds in KL for an early stopped version of the diffusion model and a smoothed version of the data distribution or assuming that the data distribution is supported on a compact manifold. These explorations have led to interesting bounds in either Wasserstein or Fortet–Mourier metrics. However, the question remains about the relevance of such an early stopping procedure or compactness conditions, in particular, if there exists a natural and mild condition ensuring explicit and sharp convergence bounds in KL. In this article, we tackle the aforementioned limitations by focusing on score diffusion models with fixed step size stemming from the Ornstein–Uhlenbeck semigroup and its kinetic counterpart. Our study provides a rigorous analysis, yielding simple, improved, and sharp convergence bounds in KL applicable to any data distribution with finite Fisher information with respect to the standard Gaussian distribution.},
  archive      = {J_SIMODS},
  author       = {Giovanni Conforti and Alain Durmus and Marta Gentiloni Silveri},
  doi          = {10.1137/23M1613670},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {86-109},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {KL convergence guarantees for score diffusion models under minimal data assumptions},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse evolution layers: Physics-informed regularizers for image segmentation. <em>SIMODS</em>, <em>7</em>(1), 55-85. (<a href='https://doi.org/10.1137/24M1633662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Traditional image processing methods employing PDEs offer a multitude of meaningful regularizers along with valuable theoretical foundations for a wide range of image-related tasks. This makes their integration into neural networks a promising avenue. In this paper, we introduce a novel regularization approach inspired by the reverse process of PDE-based evolution models. Specifically, we propose inverse evolution layers (IELs), which serve as bad property amplifiers to penalize neural networks of which outputs have undesired characteristics. Using IELs, one can achieve specific regularization objectives and endow neural network outputs with corresponding properties of the PDE models. Our experiments, focusing on semantic segmentation tasks using heat-diffusion IELs, demonstrate their effectiveness in mitigating noisy label effects. Additionally, we develop curve-motion IELs to enforce convex shape regularization in neural network–based segmentation models for preventing the generation of concave outputs. Our results indicate that IELs may offer a potential regularization mechanism for addressing challenges related to noisy labels.},
  archive      = {J_SIMODS},
  author       = {Chaoyu Liu and Zhonghua Qiao and Chao Li and Carola-Bibiane Schönlieb},
  doi          = {10.1137/24M1633662},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {55-85},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Inverse evolution layers: Physics-informed regularizers for image segmentation},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive joint distribution learning. <em>SIMODS</em>, <em>7</em>(1), 28-54. (<a href='https://doi.org/10.1137/24M1629900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We develop a new framework for estimating joint probability distributions using tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized, and positive model of a Radon–Nikodym derivative, which we estimate from sample sizes of up to several millions, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. Our proposal is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.},
  archive      = {J_SIMODS},
  author       = {Damir Filipović and Michael D. Multerer and Paul Schneider},
  doi          = {10.1137/24M1629900},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {28-54},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Adaptive joint distribution learning},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Principles for initialization and architecture selection in graph neural networks with ReLU activations. <em>SIMODS</em>, <em>7</em>(1), 1-27. (<a href='https://doi.org/10.1137/23M1600621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This article derives and validates three principles for initialization and architecture selection in finite width graph neural networks (GNNs) with ReLU activations. First, we theoretically derive what is essentially the unique generalization to ReLU GNNs of the well-known He-initialization. Our initialization scheme guarantees that the average scale of network outputs and gradients remains order one at initialization. Second, we prove in finite width vanilla ReLU GNNs that oversmoothing is unavoidable at large depth when using fixed aggregation operators, regardless of initialization. We then prove that using residual aggregation operators, obtained by interpolating a fixed aggregation operator with the identity, provably alleviates oversmoothing at initialization. Finally, we show that the common practice of using residual connections with a fixup-type initialization provably avoids correlation collapse in final layer features at initialization. Through ablation studies, we find that using the correct initialization, residual aggregation operators, and residual connections in the forward pass significantly and reliably speeds up early training dynamics in deep ReLU GNNs on a variety of tasks.},
  archive      = {J_SIMODS},
  author       = {Gage DeZoort and Boris Hanin},
  doi          = {10.1137/23M1600621},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Principles for initialization and architecture selection in graph neural networks with ReLU activations},
  volume       = {7},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
