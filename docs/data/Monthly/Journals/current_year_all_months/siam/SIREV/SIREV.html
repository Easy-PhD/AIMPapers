<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIREV</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sirev">SIREV - 65</h2>
<ul>
<li><details>
<summary>
(2025). Book review: Algorithmic mathematics in machine learning. <em>SIREV</em>, <em>67</em>(4), 917--918. (<a href='https://doi.org/10.1137/25M1741121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current academic landscape, nearly every mathematician will at some point be called upon to contribute—be it through teaching or research—to the burgeoning fields of data science and machine learning. Acquiring the necessary fundamentals in these areas ought to be straightforward. However, for many mathematicians, a significant language barrier arises when encountering the more computer science oriented literature. Bohn, Garcke, and Griebel tackle this challenge from a thoroughly mathematical perspective. Their notation is impeccable, consistently clarifying whether the subject at hand is a scalar, vector, matrix, or function. Concepts are introduced with unwavering rigor, distinguishing between well-posed and ill-posed problems, as well as between algorithms backed by convergence results and those that remain heuristic in nature.},
  archive      = {J_SIREV},
  author       = {Volker H. Schulz},
  doi          = {10.1137/25M1741121},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {917--918},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Algorithmic mathematics in machine learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: A new Lotka–Volterra model of competition with strategic aggression: Civil wars when strategy comes into play. <em>SIREV</em>, <em>67</em>(4), 915--917. (<a href='https://doi.org/10.1137/25M1740838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This book offers a fresh and innovative approach to competitive system modeling by introducing strategic aggression as a central factor in population dynamics. Through rigorous mathematical analysis, the authors provide valuable insights for researchers and academics in applied mathematics, economics, and social sciences. Moreover, the model’s relevance to real-world phenomena such as the increasing frequency and duration of civil conflicts over recent decades further enhances the book’s significance, making it a valuable resource for those seeking to understand conflict dynamics through a mathematical lens. We confirm that we have no affiliations with the book’s authors or editors. However, we recognize that this book aligns well with one of the courses in our research group, the Industrial and Financial Mathematics Research Group, specifically in the study of dynamic systems, where we also explore extensions of the Lotka–Volterra model by incorporating aggressive strategy considerations.},
  archive      = {J_SIREV},
  author       = {Rikha Rahim and Ahmad F. Sihombing and Ika W. Palupi and Nona T. Sapulette},
  doi          = {10.1137/25M1740838},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {915--917},
  shortjournal = {SIAM Rev.},
  title        = {Book review: A new Lotka–Volterra model of competition with strategic aggression: Civil wars when strategy comes into play},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Classical numerical analysis: A comprehensive course. <em>SIREV</em>, <em>67</em>(4), 914--915. (<a href='https://doi.org/10.1137/24M1700983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This textbook on classical numerical analysis is a true gem for students, educators, and practitioners in applied mathematics. With its broad scope and meticulous organization, it serves as a cornerstone reference for a wide range of topics from numerical linear algebra to numerical differential equations, optimization, and approximation theory. Whether you are teaching or attending an entry-level graduate course, this textbook offers all the essential tools to build a solid foundation in numerical analysis.},
  archive      = {J_SIREV},
  author       = {Guosheng Fu},
  doi          = {10.1137/24M1700983},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {914--915},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Classical numerical analysis: A comprehensive course},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Mathematical analysis: A very short introduction. <em>SIREV</em>, <em>67</em>(4), 913. (<a href='https://doi.org/10.1137/24M1676211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is the second book I have reviewed in the Oxford University Press A Very Short Introduction series. The first one was Eric Lauga’s Fluid Mechanics: A Very Short Introduction, reviewed in this journal a year ago. These A Very Short Introduction books are pocket-sized and written by expert authors, and (judging by the book list published by the Oxford University Press) they present all kinds of interesting and challenging topics in a readable way. Earl’s book is no exception—its author has succeeded in making a few highly technical topics accessible.},
  archive      = {J_SIREV},
  author       = {Anita T. Layton},
  doi          = {10.1137/24M1676211},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {913},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Mathematical analysis: A very short introduction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Modeling social behavior: Mathematical and agent-based models of social dynamics and cultural evolution. <em>SIREV</em>, <em>67</em>(4), 909--912. (<a href='https://doi.org/10.1137/24M1700922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paul Smaldino’s take on math modeling is unique. Many math modeling textbooks lean into engineering and physics applications, including topics like mechanics, optimization, and operations research. Modeling textbooks in the life or social sciences typically focus on biology or economics. The text Modeling Social Behavior uses simple agent-based, discrete, network, and probabilistic models of social animals (especially humans) to explore phenomena as varied as flocking, segregation, contagion, opinion dynamics, and cultural evolution. The book is an eclectic survey of applications and basic methods in math modeling of social dynamics.},
  archive      = {J_SIREV},
  author       = {Sara Clifton},
  doi          = {10.1137/24M1700922},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {909--912},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Modeling social behavior: Mathematical and agent-based models of social dynamics and cultural evolution},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Featured review: The sequential quadratic hamiltonian method: Solving optimal control problems. <em>SIREV</em>, <em>67</em>(4), 905--909. (<a href='https://doi.org/10.1137/24M1700958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal control theory has long been a cornerstone of mathematical modeling and decision-making across disciplines such as engineering, economics, and the physical sciences. Yet, as the complexity of control systems continues to grow, so does the demand for more robust and efficient computational techniques to solve these problems. Alfio Borzì’s The Sequential Quadratic Hamiltonian Method: Solving Optimal Control Problems addresses this challenge head-on, introducing a groundbreaking numerical optimization procedure, the sequential quadratic Hamiltonian (SQH) method. This book not only builds upon the theoretical framework established by the Pontryagin maximum principle (PMP), but also offers a practical computational tool that is both versatile and robust. With applications ranging from differential Nash games to deep learning via residual neural networks, the book is as much a testament to the SQH method’s adaptability as it is to its computational power. In this review, we describe the book’s structure, its significant contributions to the field of applied and computational mathematics, and its interdisciplinary relevance. We explore how the SQH method redefines the landscape of optimal control, offering new pathways for both theoretical investigation and practical implementation.},
  archive      = {J_SIREV},
  author       = {Souvik Roy},
  doi          = {10.1137/24M1700958},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {905--909},
  shortjournal = {SIAM Rev.},
  title        = {Featured review: The sequential quadratic hamiltonian method: Solving optimal control problems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book reviews. <em>SIREV</em>, <em>67</em>(4), 903--904. (<a href='https://doi.org/10.1137/25M176653X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Anita T. Layton},
  doi          = {10.1137/25M176653X},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {903--904},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DUE: A deep learning framework and library for modeling unknown equations. <em>SIREV</em>, <em>67</em>(4), 873--902. (<a href='https://doi.org/10.1137/24M1671827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Equations, particularly differential equations, are fundamental for understanding natural phenomena and predicting complex dynamics across various scientific and engineering disciplines. However, the governing equations for many complex systems remain unknown due to intricate underlying mechanisms. Recent advancements in machine learning and data science offer a new paradigm for modeling unknown equations from measurement or simulation data. This paradigm shift, known as data-driven discovery or modeling, stands at the forefront of artificial intelligence for science (AI4Science), with significant progress made in recent years. In this paper, we introduce a systematic educational framework for data-driven modeling of unknown equations using deep learning. This versatile framework is capable of learning unknown ordinary differential equations (ODEs), partial differential equations (PDEs), differential-algebraic equations (DAEs), integro-differential equations (IDEs), stochastic differential equations (SDEs), reduced or partially observed systems, and nonautonomous differential equations. Based on this framework, we have developed Deep Unknown Equations (DUE), an open-source software package designed to facilitate the data-driven modeling of unknown equations using modern deep learning techniques. DUE serves as an educational tool for classroom instruction, enabling students and newcomers to gain hands-on experience with differential equations, data-driven modeling, and contemporary deep learning approaches such as fully connected neural networks (FNNs), residual neural networks (ResNet), generalized ResNet (gResNet), operator semigroup networks (OSG-Net), and transformers from large language models (LLMs). Additionally, DUE is a versatile and accessible toolkit for researchers across various scientific and engineering fields. It is applicable not only for learning unknown equations from data, but also for surrogate modeling of known, yet complex equations that are costly to solve using traditional numerical methods. We provide detailed descriptions of DUE and demonstrate its capabilities through diverse examples which serve as templates that can be easily adapted for other applications. The source code for DUE is available at https://github.com/AI4Equations/due.},
  archive      = {J_SIREV},
  author       = {Junfeng Chen and Kailiang Wu and Dongbin Xiu},
  doi          = {10.1137/24M1671827},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {873--902},
  shortjournal = {SIAM Rev.},
  title        = {DUE: A deep learning framework and library for modeling unknown equations},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Least squares and the not-normal equations. <em>SIREV</em>, <em>67</em>(4), 865--872. (<a href='https://doi.org/10.1137/23M161851X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. For many of the classic problems of linear algebra, effective and efficient numerical algorithms exist, particularly for situations where dimensions are not too large. The linear least squares problem is one such example: excellent algorithms exist when factorization is feasible. However, for large-dimensional (often sparse) linear least squares problems there currently exist good solution algorithms only for well-conditioned problems or for problems where there are lots of data but only a few variables in the solution. Such approaches ubiquitously employ normal equations and so have to contend with conditioning issues. We explore some alternative approaches that we characterize as not-normal equations where conditioning may not be such an issue.},
  archive      = {J_SIREV},
  author       = {Andrew J. Wathen},
  doi          = {10.1137/23M161851X},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {865--872},
  shortjournal = {SIAM Rev.},
  title        = {Least squares and the not-normal equations},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Education. <em>SIREV</em>, <em>67</em>(4), 863. (<a href='https://doi.org/10.1137/25M1766528'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Hélène Frankowska},
  doi          = {10.1137/25M1766528},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {863},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Turning big data into tiny data: Coresets for unsupervised learning problems. <em>SIREV</em>, <em>67</em>(4), 801--861. (<a href='https://doi.org/10.1137/25M1799684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We develop and analyze a method to reduce the size of a very large set of data points in a high-dimensional Euclidean space to a small set of weighted points such that the result of a predetermined data analysis task on the reduced set is approximately the same as that for the original point set. For example, computing the first principal components of the reduced set will return approximately the first principal components of the original set, or computing the centers of a -means clustering on the reduced set will return an approximation for the original set. Such a reduced set is also known as a coreset. The main new features of our construction are that the cardinality of the reduced set is independent of the dimension of the input space and that the sets are mergeable [P. K. Agarwal et al., Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI Symposium on Principals of Database Systems, 2012, pp. 23–34]. The latter property means that the union of two reduced sets is a reduced set for the union of the two original sets. It allows us to turn our methods into streaming or distributed algorithms using standard approaches. For problems such as -means and subspace approximation the coreset sizes are also independent of the number of input points. Our method is based on data-dependently projecting the points on a low-dimensional subspace and reducing the cardinality of the points inside this subspace using known methods. The proposed approach works for a wide range of data analysis techniques including -means clustering, principal component analysis, and subspace clustering. The main conceptual contribution is a new coreset definition that allows charging for the costs that appear for every solution to an additive constant.},
  archive      = {J_SIREV},
  author       = {Dan Feldman and Melanie Schmidt and Christian Sohler},
  doi          = {10.1137/25M1799684},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {801--861},
  shortjournal = {SIAM Rev.},
  title        = {Turning big data into tiny data: Coresets for unsupervised learning problems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIGEST. <em>SIREV</em>, <em>67</em>(4), 799. (<a href='https://doi.org/10.1137/25M1766541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/25M1766541},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {799},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The fundamental subspaces of ensemble kalman inversion. <em>SIREV</em>, <em>67</em>(4), 771--798. (<a href='https://doi.org/10.1137/24M1693143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Ensemble Kalman inversion (EKI) methods are a family of iterative methods for solving weighted least squares problems, especially those arising in scientific and engineering inverse problems in which unknown parameters or states are estimated from observed data by minimizing the weighted square norm of the data misfit. Implementation of EKI only requires the evaluation of the forward model mapping the unknown to the data, and does not require derivatives or adjoints of the forward model. The methods therefore offer an attractive alternative to gradient-based optimization approaches in inverse problem settings where evaluating derivatives or adjoints of the forward model is computationally intractable. This work presents a new analysis of the behavior of both deterministic and stochastic versions of basic EKI for linear observation operators, resulting in a natural interpretation of EKI’s convergence properties in terms of “fundamental subspaces” analogous to Strang’s fundamental subspaces of linear algebra. Our analysis directly examines the discrete EKI iterations instead of their continuous-time limits considered in previous analyses, and it provides spectral decompositions that define six fundamental subspaces of EKI spanning both observation and state spaces. This approach verifies convergence rates previously derived for continuous-time limits, and yields new results describing both deterministic and stochastic EKI convergence behavior with respect to the standard minimum-norm weighted least squares solution in terms of the fundamental subspaces. Numerical experiments illustrate our theoretical results.},
  archive      = {J_SIREV},
  author       = {Elizabeth Qian and Christopher Beattie},
  doi          = {10.1137/24M1693143},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {771--798},
  shortjournal = {SIAM Rev.},
  title        = {The fundamental subspaces of ensemble kalman inversion},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the loewner framework, the kolmogorov superposition theorem, and the curse of dimensionality. <em>SIREV</em>, <em>67</em>(4), 737--770. (<a href='https://doi.org/10.1137/24M1656657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Loewner framework is an interpolatory approach for the approximation of linear and nonlinear systems. The purpose here is to extend this framework to linear parametric systems with an arbitrary number of parameters. To achieve this, a new generalized multivariate rational function realization is proposed. We then introduce the -dimensional multivariate Loewner matrices and show that they can be computed by solving a set of coupled Sylvester equations. The null space of these Loewner matrices allows the construction of multivariate rational functions in barycentric form. The principal result of this work is to show how the null space of -dimensional Loewner matrices can be computed using a sequence of one-dimensional Loewner matrices. Thus, a decoupling of the variables is achieved, which leads to a drastic reduction of the computational burden. Equally importantly, this burden is alleviated by avoiding the explicit construction of large-scale -dimensional Loewner matrices of size . The proposed methodology achieves the decoupling of variables, leading (i) to a reduction in complexity from to below when and (ii) to memory storage bounded by the largest variable dimension rather than their product, thus taming the curse of dimensionality and making the solution scalable to very large data sets. This decoupling of the variables leads to a result similar to the Kolmogorov superposition theorem for rational functions. Thus, making use of barycentric representations, every multivariate rational function can be computed using the composition and superposition of single-variable functions. Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation. Numerical examples highlight the effectiveness and scalability of the method.},
  archive      = {J_SIREV},
  author       = {Athanasios C. Antoulas and Ion Victor Gosea and Charles Poussot-Vassal},
  doi          = {10.1137/24M1656657},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {737--770},
  shortjournal = {SIAM Rev.},
  title        = {On the loewner framework, the kolmogorov superposition theorem, and the curse of dimensionality},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research spotlights. <em>SIREV</em>, <em>67</em>(4), 735. (<a href='https://doi.org/10.1137/25M1766516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Stefan M. Wild},
  doi          = {10.1137/25M1766516},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {735},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical review of mathieu function programs for integer orders and real parameters. <em>SIREV</em>, <em>67</em>(4), 661--733. (<a href='https://doi.org/10.1137/23M1572726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Mathieu function is a special function satisfying the Mathieu differential equation. Since its inception in 1868, numerous algorithms and programs have been published to calculate it, and so it is about time to review the performance of available software. First, the fundamentals of Mathieu functions are summarized such as definition, normalization, nomenclature, and methods of solution. Then, we review several programs for Mathieu functions of integer orders with real parameters and compare the results numerically by running individual software; in addition, Bessel function routines are also compared. Finally, a straightforward algorithm is recommended with codes written in MATLAB and GNU Octave.},
  archive      = {J_SIREV},
  author       = {Ho-Chul Shin},
  doi          = {10.1137/23M1572726},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {661--733},
  shortjournal = {SIAM Rev.},
  title        = {Numerical review of mathieu function programs for integer orders and real parameters},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey and review. <em>SIREV</em>, <em>67</em>(4), 659. (<a href='https://doi.org/10.1137/25M1766504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Marlis Hochbruck},
  doi          = {10.1137/25M1766504},
  journal      = {SIAM Review},
  month        = {12},
  number       = {4},
  pages        = {659},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Statistical foundations of actuarial learning and its applications. <em>SIREV</em>, <em>67</em>(3), 656--658. (<a href='https://doi.org/10.1137/24M1651575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In insurance mathematics and actuarial sciences, modeling the dynamics of insured events is a pivotal challenge that demands advanced and sophisticated techniques due to the growing complexity of insurance markets. This complexity, coupled with the exponential growth in data availability in recent years, has acted as a catalyst for the adoption of datacentric approaches in forecasting random phenomena.},
  archive      = {J_SIREV},
  author       = {Olivier Menoukeu-Pamen},
  doi          = {10.1137/24M1651575},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {656--658},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Statistical foundations of actuarial learning and its applications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Dissipative lattice dynamical systems. <em>SIREV</em>, <em>67</em>(3), 655--656. (<a href='https://doi.org/10.1137/24M1675606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lattice dynamical systems (LDS) are infinite-dimensional systems of ordinary differential equations (ODEs). They can be formulated as ODEs on a Banach space of bi-infinite sequences. They may arise in various ways: some are obtained as discretizations of partial differential equations or integral equations, and others are infinite-dimensional counterparts of finite-dimensional ODE models such as the Hopfield neural network model. This book studies various kinds of LDS that might be of autonomous, nonautonomous, or random nature. It focuses on first showing that the underlying LDS induces an autonomous, nonautonomous, or random semidynamical system, then providing sufficient criteria for the existence of a global, pullback, or random attractor.},
  archive      = {J_SIREV},
  author       = {Ábel Garab},
  doi          = {10.1137/24M1675606},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {655--656},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Dissipative lattice dynamical systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Differential equations: Solving ordinary and partial differential equations with mathematica. <em>SIREV</em>, <em>67</em>(3), 654--655. (<a href='https://doi.org/10.1137/24M170096X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some books on differential equations or computational methods for differential equations present the mathematical theories or numerical algorithms in detail, but include only a few illustrative codes. In contrast, the book under review places the emphasis on Mathematica codes. In other words, this book is a collection of Mathematica codes for the solutions of various types of differential equations.},
  archive      = {J_SIREV},
  author       = {Hao Chen},
  doi          = {10.1137/24M170096X},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {654--655},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Differential equations: Solving ordinary and partial differential equations with mathematica},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Discrete variational problems with interfaces. <em>SIREV</em>, <em>67</em>(3), 651--654. (<a href='https://doi.org/10.1137/24M1677964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this book the authors present the variational analysis via -convergence of functionals defined on functions , where is a small parameter finally tending to zero, is a so-called lattice (typically ), and is a finite state-space. The functionals (often called energies due to applications in physics) are of many different types, but share the common feature that when the lattice spacing tends to zero, functions with bounded energy (or suitable transformations) give rise to a finite partition.},
  archive      = {J_SIREV},
  author       = {Matthias Ruf},
  doi          = {10.1137/24M1677964},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {651--654},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Discrete variational problems with interfaces},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Inverse optimal control and inverse noncooperative dynamic game theory: A minimum-principle approach. <em>SIREV</em>, <em>67</em>(3), 650--651. (<a href='https://doi.org/10.1137/24M1635120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse Optimal Control and Inverse Noncooperative Dynamic Game Theory is a detailed exploration of inverse problems in control theory, particularly suited to researchers and practitioners working on multiagent systems, robotics, and economics. The book systematically builds up the theory and techniques necessary for recovering cost functions, which can explain observed behavior in both individual agents and systems of agents operating with competing objectives. The authors provide a deep dive into the mathematical foundations, addressing both discrete and continuous systems in optimal control and dynamic game theory.},
  archive      = {J_SIREV},
  author       = {Sebastián Zamorano Aliaga},
  doi          = {10.1137/24M1635120},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {650--651},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Inverse optimal control and inverse noncooperative dynamic game theory: A minimum-principle approach},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Featured review: Making democracy count: How mathematics improves voting, electoral maps, and representation. <em>SIREV</em>, <em>67</em>(3), 645--650. (<a href='https://doi.org/10.1137/24M1675655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Democracy: What’s math got to do with it? If you were to ask someone who has not previously studied this topic, you’d likely receive the obvious and simple answer: We count votes, and whoever gets the most votes wins. Okay. If you find someone who follows politics, you might get another answer: We need statistics to take good polls and make predictions. A little more satisfying, and true, but you could say that this type of analysis is more commentary on who is winning and losing in the political process rather than an analysis of the process itself. In the recent book Making Democracy Count: How Mathematics Improves Voting, Electoral Maps, and Representation, Ismar Volić gives another answer: Math has everything to do with democracy.},
  archive      = {J_SIREV},
  author       = {Beth Malmskog},
  doi          = {10.1137/24M1675655},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {645--650},
  shortjournal = {SIAM Rev.},
  title        = {Featured review: Making democracy count: How mathematics improves voting, electoral maps, and representation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book reviews. <em>SIREV</em>, <em>67</em>(3), 643. (<a href='https://doi.org/10.1137/25M1741480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Anita T. Layton},
  doi          = {10.1137/25M1741480},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {643},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling still matters: A surprising instance of catastrophic floating point errors in mathematical biology and numerical methods for ODEs. <em>SIREV</em>, <em>67</em>(3), 624--641. (<a href='https://doi.org/10.1137/23M1563967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We guide the reader on a journey through mathematical modeling and numerical analysis, emphasizing the crucial interplay of both disciplines. Targeting undergraduate students with basic knowledge of dynamical systems and numerical methods for ordinary differential equations, we explore a model from mathematical biology where numerical methods fail badly due to catastrophic floating point errors. We analyze the reasons for this behavior by studying the steady states of the model and use the theory of invariants to develop an alternative model suited for numerical simulations. Our story is intended to motivate the combining of analytical knowledge and numerical knowledge, even in those cases where the world looks fine at first sight. We have set up an online repository containing an interactive notebook with all the numerical experiments in this article to make this study fully reproducible and useful for classroom teaching.},
  archive      = {J_SIREV},
  author       = {Cordula Reisch and Hendrik Ranocha},
  doi          = {10.1137/23M1563967},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {624--641},
  shortjournal = {SIAM Rev.},
  title        = {Modeling still matters: A surprising instance of catastrophic floating point errors in mathematical biology and numerical methods for ODEs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion models for generative artificial intelligence: An introduction for applied mathematicians. <em>SIREV</em>, <em>67</em>(3), 607--623. (<a href='https://doi.org/10.1137/23M1626232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Generative artificial intelligence (GAI) refers to algorithms that create synthetic but realistic output. Diffusion models currently offer state-of-the-art performance in GAI for images. They also form a key component in more general tools, including text-to-image generators and large language models. Diffusion models work by adding noise to the available training data and then learning how to reverse the process. The reverse operation may then be applied to new random data in order to produce new outputs. We provide a brief introduction to diffusion models for applied mathematicians and statisticians. Our key aims are to (a) present illustrative computational examples, (b) give a careful derivation of the underlying mathematical formulas involved, and (c) draw a connection with partial differential equation (PDE) diffusion models. We provide code for the computational experiments. We hope that this topic will be of interest to advanced undergraduate and postgraduate students. Portions of the material may also provide useful motivational examples for those who teach courses in stochastic processes, inference, machine learning, PDEs, or scientific computing.},
  archive      = {J_SIREV},
  author       = {Catherine Higham and Desmond J. Higham and Peter Grindrod},
  doi          = {10.1137/23M1626232},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {607--623},
  shortjournal = {SIAM Rev.},
  title        = {Diffusion models for generative artificial intelligence: An introduction for applied mathematicians},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Education. <em>SIREV</em>, <em>67</em>(3), 605. (<a href='https://doi.org/10.1137/25M1741479'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Hélène Frankowska},
  doi          = {10.1137/25M1741479},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {605},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal survival strategies for diffusive flows: A schrödinger bridge approach to unbalanced transport. <em>SIREV</em>, <em>67</em>(3), 579--604. (<a href='https://doi.org/10.1137/25M176581X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Diffusive flows, and their discrete counterparts, are ubiquitous in the physical and engineering sciences. In many important examples, the total mass is not preserved and therefore standard probabilistic models are not suitable. Examples include electrons which may be absorbed by the medium in which they travel. In population genetics, some individuals may “disappear” due to their genotype. In traffic flows over a network, some vehicles might simply exit the circulation and park. In this more general situation, where some of the mass may be lost, it is of particular interest to reconcile the observed initial and final marginal distributions with a given prior. In the case when the two marginals are probability distributions, and thus of equal mass, this problem was posed and, to a considerable extent, solved by E. Schrödinger in 1931/32. It is now known as the Schrödinger Bridge Problem (SBP). It turns out that Schrödinger’s problem can be viewed as both a modeling and a control problem. Due to the fundamental significance of this problem, interest in the SBP and in its deterministic (zero-noise limit) counterpart of optimal mass transport (OMT) has in recent years enticed scientists from a broad spectrum of disciplines, including physics, stochastic control, computer science, probability theory, and geometry. Yet, while the mathematics and applications of SBP/OMT have been developing at a considerable pace, accounting for marginals of unequal mass has received scant attention. The problem of interpolating between “unbalanced” marginals has been approached by introducing source/sink terms into the transport equations in an ad hoc manner, chiefly driven by applications in image registration. Nevertheless, as hinted at above, losses are inherent in many physical processes and, thereby, models that account for lossy transport may also need to be reconciled with observed marginals following Schrödinger’s quest, that is, to adjust the probability of trajectories of particles, including those that do not make it to the terminal observation point, so that the updated evolution represents the most likely way that particles may have been transported, or vanished, at some intermediate point. Thus, the purpose of this work is to develop such a natural generalization of the SBP for diffusive evolution with losses, whereupon particles are “killed” (jump into a coffin/extinction state) according to a probabilistic law, and thereby mass is gradually lost along their stochastically driven flow. Through a suitable embedding, which appears to be novel, we turn the problem into an SBP for stochastic processes that combine diffusive and jump characteristics. Then, following a large-deviations formalism in the style of E. Schrödinger, given a prior law that allows for losses, we ask for the most probable evolution of particles along with the most likely killing rate as the particles transition between the specified marginals. Our approach differs sharply from previous work involving a Feynman–Kac multiplicative reweighing of the reference measure. The latter, as we argue, is far from Schrödinger’s quest. An iterative scheme, generalizing the celebrated Fortet–IPF–Sinkhorn algorithm, permits the computation of the new drift and the new killing rate of the path-space solution measure. We also formulate and solve a related fluid-dynamic control problem for the flow of one-time marginals where both the drift and the new killing rate play the role of control variable. A numerical example illustrating the new theoretical results is also presented.},
  archive      = {J_SIREV},
  author       = {Yongxin Chen and Tryphon T. Georgiou and Michele Pavon},
  doi          = {10.1137/25M176581X},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {579--604},
  shortjournal = {SIAM Rev.},
  title        = {Optimal survival strategies for diffusive flows: A schrödinger bridge approach to unbalanced transport},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIGEST. <em>SIREV</em>, <em>67</em>(3), 577. (<a href='https://doi.org/10.1137/25M1741492'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/25M1741492},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {577},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical bayesian inverse problems: A high-dimensional statistics viewpoint. <em>SIREV</em>, <em>67</em>(3), 543--575. (<a href='https://doi.org/10.1137/24M1629328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper analyzes hierarchical Bayesian inverse problems using techniques from high-dimensional statistics. Our analysis leverages a property of hierarchical Bayesian regularizers that we call approximate decomposability to obtain nonasymptotic bounds on the reconstruction error attained by maximum a posteriori estimators. The new theory explains how hierarchical Bayesian models that exploit sparsity, group sparsity, and sparse representations of the unknown parameter can achieve accurate reconstructions in high-dimensional settings.},
  archive      = {J_SIREV},
  author       = {Daniel Sanz-Alonso and Nathan Waniorek},
  doi          = {10.1137/24M1629328},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {543--575},
  shortjournal = {SIAM Rev.},
  title        = {Hierarchical bayesian inverse problems: A high-dimensional statistics viewpoint},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research spotlights. <em>SIREV</em>, <em>67</em>(3), 541. (<a href='https://doi.org/10.1137/25M1741467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Stefan M. Wild},
  doi          = {10.1137/25M1741467},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {541},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic dual dynamic programming and its variants: A review. <em>SIREV</em>, <em>67</em>(3), 415--539. (<a href='https://doi.org/10.1137/23M1575093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We provide a tutorial-style review of stochastic dual dynamic programming (SDDP), one of the state-of-the-art solution methods for large-scale multistage stochastic programs. Since it was introduced about 30 years ago for solving large-scale multistage stochastic linear programming problems in energy planning, SDDP has been applied to practical problems from several fields and has been enriched by various improvements and enhancements to address broader problem classes. We begin with a detailed introduction to SDDP, with special focus on its motivation, complexity, and required assumptions. Then, we present and discuss in depth the existing enhancements as well as current research trends that allow for the alleviation of those assumptions.},
  archive      = {J_SIREV},
  author       = {Christian Füllner and Steffen Rebennack},
  doi          = {10.1137/23M1575093},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {415--539},
  shortjournal = {SIAM Rev.},
  title        = {Stochastic dual dynamic programming and its variants: A review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey and review. <em>SIREV</em>, <em>67</em>(3), 413. (<a href='https://doi.org/10.1137/25M1741455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Marlis Hochbruck},
  doi          = {10.1137/25M1741455},
  journal      = {SIAM Review},
  month        = {8},
  number       = {3},
  pages        = {413},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Stochastic integral and differential equations in mathematical modelling. <em>SIREV</em>, <em>67</em>(2), 411. (<a href='https://doi.org/10.1137/24M163548X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A short discussion on stochastic calculus is given under the assumption that the fundamentals of probability theory are known to readers. Some related basic details on probability theory should have been included to make the book more self-contained. Further, analytic solutions of some stochastic differential equations (SDEs), which are used in modeling real-life events, are given. However, author should have included well-posedness under the general assumptions and then should have either discussed these SDEs as a special case or provided an explanation for the necessity of dealing with such equations separately.},
  archive      = {J_SIREV},
  author       = {Chaman Kumar},
  doi          = {10.1137/24M163548X},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {411},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Stochastic integral and differential equations in mathematical modelling},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Optimal mass transport on euclidean spaces. <em>SIREV</em>, <em>67</em>(2), 408--411. (<a href='https://doi.org/10.1137/24M1637854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal transport was originally invented by Gaspard Monge [“Mémoire sur la théorie des déblais et des remblais,” Mem. Math. Phys. Acad. Royale Sci., (1781), pp. 666–704] to model the problem of optimally mapping one distribution of mass onto another. This was later reformulated by Leonid Kantorovich as a well-posed linear program using the notion of transport plans instead of maps in [“On the translocation of masses,” Dokl. Akad. Nauk. USSR (N.S.), 37 (1942), pp. 199–201], which earned him the Nobel Memorial Prize in Economic Sciences. In the past four decades the field of optimal transport has grown far beyond its original purpose and has evolved into a driving force for applications both within mathematics and in other sciences. This book review deals with the new monograph Optimal Mass Transport on Euclidean Spaces by Francesco Maggi.},
  archive      = {J_SIREV},
  author       = {Leon Bungert},
  doi          = {10.1137/24M1637854},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {408--411},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Optimal mass transport on euclidean spaces},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Algorithmic mathematics in machine learning. <em>SIREV</em>, <em>67</em>(2), 406--408. (<a href='https://doi.org/10.1137/24M1702611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2024 Nobel Prize in Physics was awarded to John Hopfield and Geoffrey Hinton for their work on artificial intelligence and machine learning. The award has been somewhat controversial in the physics community and prompted some heated debates, since the only apparent use of physics is the Boltzmann distribution in the sampling function of the Boltzmann machine [D. H. Ackley, G. E. Hinton, and T. J. Sejnowski, Cog. Sci., 9 (1985), pp. 147–169]. If we leave aside this debate for the time being, it is undeniable that artificial intelligence and machine learning have had a transformative effect on various areas of science and technology.},
  archive      = {J_SIREV},
  author       = {Hollis Williams and Azza M. Algatheem},
  doi          = {10.1137/24M1702611},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {406--408},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Algorithmic mathematics in machine learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Big data analytics for smart transport and healthcare systems. <em>SIREV</em>, <em>67</em>(2), 405--406. (<a href='https://doi.org/10.1137/24M1637581'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big Data Analytics for Smart Transport and Healthcare Systems explores the praxis of data analysis for urban, human-focused datasets. Through a series of timely case studies, the authors demonstrate the need for interdisciplinary approaches to studying big data. This text, which covers topics ranging from flight status to the COVID-19 pandemic, introduces crucial tools for effective and responsible data science and will prove useful for data scientists across a variety of fields.},
  archive      = {J_SIREV},
  author       = {Esha Datta},
  doi          = {10.1137/24M1637581},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {405--406},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Big data analytics for smart transport and healthcare systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Math in drag. <em>SIREV</em>, <em>67</em>(2), 404--405. (<a href='https://doi.org/10.1137/24M1668767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Math is like a drag queen: marvelous, whimsical, at times even controversial, but never boring!” That it how the preface of Math in Drag begins. It is also an excellent description of the book. Math in Drag was authored by Kyne Santos, who often goes by Kyne. Kyne studied mathematics at the University of Waterloo and went viral teaching math on TikTok. Indeed, over a million people have flocked to Kyne’s @onlinekyne account for camp explanations of quadratic equations and square roots. Kyne is also a drag queen and competed in the first season of Canada’s Drag Race.},
  archive      = {J_SIREV},
  author       = {Laura W. Layton},
  doi          = {10.1137/24M1668767},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {404--405},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Math in drag},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Featured review: How data happened: A history from the age of reason to the age of algorithms. <em>SIREV</em>, <em>67</em>(2), 401--403. (<a href='https://doi.org/10.1137/24M1635521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It’s 7.30 am when my alarm wakes me up and I am greeted by my notifications. While eating breakfast, I watch videos YouTube recommends to me: sometimes news stories, sometimes my guilty pleasure of a new “Say Yes to the Dress” clip. On my way to campus, I play my daylist, a curated playlist from Spotify based on what I normally listen to on a given weekday and time. Apparently, as I write this, “Nostalgia 2010s Tuesday Afternoon” is waiting for me. In the classroom, I teach students how to load in data, visualize it, and run a regression.},
  archive      = {J_SIREV},
  author       = {Rachel Roca},
  doi          = {10.1137/24M1635521},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {401--403},
  shortjournal = {SIAM Rev.},
  title        = {Featured review: How data happened: A history from the age of reason to the age of algorithms},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book reviews. <em>SIREV</em>, <em>67</em>(2), 399. (<a href='https://doi.org/10.1137/24M1717920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Anita T. Layton},
  doi          = {10.1137/24M1717920},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {399},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty analysis of a simple river quality model using differential inequalities. <em>SIREV</em>, <em>67</em>(2), 375--398. (<a href='https://doi.org/10.1137/23M1616406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present and discuss the Streeter–Phelps equations, which were the first river quality model. If the parameters are constants, then the model in its linear formulation can be solved explicitly. This reveals, however, that depending on parameters and initial data, the model might predict negative oxygen concentrations, which marks a breakdown of the model. To address this shortcoming, we introduce a nonlinear modification which, in the case of constant parameters, we can study in the phase plane. In real-world applications, parameters are never constant and are usually known not exactly, but instead with some uncertainty. We show how we can use the solutions for the constant parameter case to obtain estimates for the unknown solutions from estimates of the model parameters, using differential inequalities.},
  archive      = {J_SIREV},
  author       = {Grace D’Agostino and Hermann J. Eberl},
  doi          = {10.1137/23M1616406},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {375--398},
  shortjournal = {SIAM Rev.},
  title        = {Uncertainty analysis of a simple river quality model using differential inequalities},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Education. <em>SIREV</em>, <em>67</em>(2), 373. (<a href='https://doi.org/10.1137/24M1717919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Hélène Frankowska},
  doi          = {10.1137/24M1717919},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {373},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A nonlocal-to-local approach to aggregation-diffusion equations. <em>SIREV</em>, <em>67</em>(2), 353--372. (<a href='https://doi.org/10.1137/25M1726248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Over the past few decades, nonlocal models have been widely used to describe aggregation phenomena in biology, physics, engineering, and the social sciences. These are often derived as mean-field limits of attraction-repulsion agent-based models and consist of systems of nonlocal partial differential equations. Using differential adhesion between cells as a biological case study, we introduce a novel local model of aggregation-diffusion phenomena. This system of local aggregation-diffusion equations is fourth-order, resembling thin-film or Cahn–Hilliard type equations. In this framework, cell sorting phenomena are explained through relative surface tensions between distinct cell types. The local model emerges as a limiting case of short-range interactions, providing a significant simplification of earlier nonlocal models while preserving the same phenomenology. This simplification makes the model easier to implement numerically and more amenable to calibration to quantitative data. In addition, we discuss recent analytical results based on the gradient flow structure of the model, along with open problems and future research directions.},
  archive      = {J_SIREV},
  author       = {C. Falcó and R. E. Baker and J. A. Carrillo},
  doi          = {10.1137/25M1726248},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {353--372},
  shortjournal = {SIAM Rev.},
  title        = {A nonlocal-to-local approach to aggregation-diffusion equations},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIGEST. <em>SIREV</em>, <em>67</em>(2), 351. (<a href='https://doi.org/10.1137/24M1717932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/24M1717932},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {351},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computerized tomography and reproducing kernels. <em>SIREV</em>, <em>67</em>(2), 321--350. (<a href='https://doi.org/10.1137/23M1616716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The X-ray transform is one of the most fundamental integral operators in image processing and reconstruction. In this paper, we revisit the formalism of the X-ray transform by considering it as an operator between reproducing kernel Hilbert spaces (RKHSs). Within this framework, the X-ray transform can be viewed as a natural analogue of Euclidean projection. The RKHS framework considerably simplifies projection image interpolation, and it leads to an analogue of the celebrated representer theorem for the problem of tomographic reconstruction. It leads to methodology that is dimension-free and stands apart from conventional filtered backprojection techniques, as it does not hinge on the Fourier transform. It also allows us to establish sharp stability results at a genuinely functional level (i.e., without recourse to discretization), but in the realistic setting where the data are discrete and noisy. The RKHS framework is versatile, accommodating any reproducing kernel on a unit ball, affording a high level of generality. When the kernel is chosen to be rotation-invariant, explicit spectral representations can be obtained, elucidating the regularity structure of the associated Hilbert spaces. Moreover, the reconstruction problem can be solved at the same computational cost as filtered backprojection.},
  archive      = {J_SIREV},
  author       = {Ho Yun and Victor M. Panaretos},
  doi          = {10.1137/23M1616716},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {321--350},
  shortjournal = {SIAM Rev.},
  title        = {Computerized tomography and reproducing kernels},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research spotlights. <em>SIREV</em>, <em>67</em>(2), 319. (<a href='https://doi.org/10.1137/24M1717907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Stefan M. Wild},
  doi          = {10.1137/24M1717907},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {319},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The Gross–Pitaevskii equation and eigenvector nonlinearities: Numerical methods and algorithms. <em>SIREV</em>, <em>67</em>(2), 256--317. (<a href='https://doi.org/10.1137/22M1516324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this review paper, we provide an overview of numerical methods used in the study of the Gross–Pitaevskii eigenvalue problem (GPEVP). The GPEVP is an important nonlinear Schrödinger equation that is used in quantum physics to describe the ground states of ultracold bosonic gases. The discretization of the GPEVP leads to a nonlinear eigenvalue problem with eigenvector nonlinearities. The rich variety of numerical techniques in the literature for tackling the GPEVP has ingredients from linear algebra, partial differential equations, and numerical optimization as well as gradient flows on Riemannian manifolds. We review this heterogeneous body of literature with a focus on a unified treatment of seemingly different approaches, algorithms, and method properties, and we point to open problems and future challenges in the field.},
  archive      = {J_SIREV},
  author       = {Patrick Henning and Elias Jarlebring},
  doi          = {10.1137/22M1516324},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {256--317},
  shortjournal = {SIAM Rev.},
  title        = {The Gross–Pitaevskii equation and eigenvector nonlinearities: Numerical methods and algorithms},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective optimization using the r2 utility. <em>SIREV</em>, <em>67</em>(2), 213--255. (<a href='https://doi.org/10.1137/23M1578371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The goal of multiobjective optimization is to identify a collection of points which describe the best possible trade-offs among the multiple objectives. In order to solve this vector-valued optimization problem, practitioners often appeal to the use of scalarization functions in order to transform the multiobjective problem into a collection of single-objective problems. This set of scalarized problems can then be solved using traditional single-objective optimization techniques. In this paper, we formalize this convention into a general mathematical framework. We show how this strategy effectively recasts the original multiobjective optimization problem into a single-objective optimization problem defined over sets. An appropriate class of objective functions for this new problem is that of the R2 utilities, which are utility functions that are defined as a weighted integral over the scalarized optimization problem. As part of our work, we show that these utilities are monotone and submodular set functions that can be optimized effectively using greedy optimization algorithms. We then analyze the performance of these greedy algorithms both theoretically and empirically. Our analysis largely focuses on Bayesian optimization, which is a popular probabilistic framework for black-box optimization.},
  archive      = {J_SIREV},
  author       = {Ben Tu and Nikolas Kantas and Robert M. Lee and Behrang Shafei},
  doi          = {10.1137/23M1578371},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {213--255},
  shortjournal = {SIAM Rev.},
  title        = {Multiobjective optimization using the r2 utility},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey and review. <em>SIREV</em>, <em>67</em>(2), 211. (<a href='https://doi.org/10.1137/24M1717890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Marlis Hochbruck},
  doi          = {10.1137/24M1717890},
  journal      = {SIAM Review},
  month        = {5},
  number       = {2},
  pages        = {211},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Mathematical pictures at a data science exhibition. <em>SIREV</em>, <em>67</em>(1), 208--209. (<a href='https://doi.org/10.1137/24M1635077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The book Mathematical Pictures at a Data Science Exhibition aims to introduce the reader to the many mathematical ideas that congregate under the ever-expanding umbrella of data science. Given the meteoric rise of this field and the immense speed at which it often moves, this book acts as a welcome road map for graduate students and researchers in the field. Given its focus on theory, the book should be most appreciated by mathematicians as well as theoretical statisticians and computer scientists. While algorithms are the main focus of the book, the exposition is by no means a hands-on tutorial in data science, but rather an introductory text on the theoretical ideas behind data science algorithms and problems.},
  archive      = {J_SIREV},
  author       = {Bamdad Hosseini},
  doi          = {10.1137/24M1635077},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {208--209},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Mathematical pictures at a data science exhibition},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Elegant simulations. from simple oscillators to many-body systems. <em>SIREV</em>, <em>67</em>(1), 207--208. (<a href='https://doi.org/10.1137/24M1690953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elegant Simulations covers various aspects of modeling and simulating mechanical systems described at the elementary level by many-interacting particles. The book presents the topics from an original and fresh point of view. The complex many-body dynamics is reproduced at the elementary level in terms of simple models that are easy to understand and interpret. The principal benefit for the reader is that this approach helps to develop an intuitive picture of the complex many-body dynamics.},
  archive      = {J_SIREV},
  author       = {Omar Morandi},
  doi          = {10.1137/24M1690953},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {207--208},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Elegant simulations. from simple oscillators to many-body systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Essential statistics for data science: A concise crash course. <em>SIREV</em>, <em>67</em>(1), 206--207. (<a href='https://doi.org/10.1137/24M167562X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is a bold book! Professor Zhu wants to provide the basic statistical knowledge needed by data scientists in a super-short volume. It reminds me a bit of Larry Wasserman’s All of Statistics (Springer, 2014), but is aimed at Masters students (often from fields other than statistics) or advanced undergraduates (also often from other fields). As an attendee at far too many faculty meetings, I applaud brevity and focus. As an amateur stylist, I admire strong technical writing. And as an applied statistician who has taught basic statistics to Masters and Ph.D. students from other disciplines, I appreciate the need for a book of this kind. For the right course I would happily use this book, although I would need to supplement it with other material.},
  archive      = {J_SIREV},
  author       = {David Banks},
  doi          = {10.1137/24M167562X},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {206--207},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Essential statistics for data science: A concise crash course},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Probability adventures. <em>SIREV</em>, <em>67</em>(1), 205--206. (<a href='https://doi.org/10.1137/24M1646108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first look at Probability Adventures brought back memories of a conference in Ubatuba, Brazil, in 2001, where as a young Master’s student I worried that true science had to be deadly serious. Fortunately, several inspiring teachers came to the rescue. Andrei Toom’s words resonated deeply with me when he began his lecture by saying, “Every mathematician is a big child.” The esteemed audience beamed with approval. Today, I look at Probability Adventures and applaud Mark Huber for honoring the child in all of us and offering a reading that is both fun and mathematically rigorous.},
  archive      = {J_SIREV},
  author       = {Nevena Marić},
  doi          = {10.1137/24M1646108},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {205--206},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Probability adventures},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book review: Numerical methods in physics with python. second edition. <em>SIREV</em>, <em>67</em>(1), 204--205. (<a href='https://doi.org/10.1137/24M1650466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical Methods in Physics with Python by Alex Gezerlis is an excellent example of a textbook built on long and established teaching experience. The goals are clearly defined in the preface: Gezerlis aims to gently introduce undergraduate physics students to the branch of numerical methods and their concrete implementation in Python. To this end, the author considers a physics-applications-first approach. Every chapter begins with a motivation section on real physics problems (simple but adequate for undergraduate students), ends with a concrete project on a physics application, and is completed by a rich list of exercises often designed with a physics appeal.},
  archive      = {J_SIREV},
  author       = {Gabriele Ciaramella},
  doi          = {10.1137/24M1650466},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {204--205},
  shortjournal = {SIAM Rev.},
  title        = {Book review: Numerical methods in physics with python. second edition},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Featured review: Numerical integration of differential equations. <em>SIREV</em>, <em>67</em>(1), 197--204. (<a href='https://doi.org/10.1137/24M1678684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The book under review was originally published under the auspices of the National Research Council in 1933 (the year John was born), and it was republished as a Dover edition in 1956 (three years before Rob was born). At 108 pages—including title page, preface, table of contents, and index—it’s very short. Even so, it contains a significant amount of information that was of technical importance for its time and is of historical importance now.},
  archive      = {J_SIREV},
  author       = {John C. Butcher and Robert M. Corless},
  doi          = {10.1137/24M1678684},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {197--204},
  shortjournal = {SIAM Rev.},
  title        = {Featured review: Numerical integration of differential equations},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book reviews. <em>SIREV</em>, <em>67</em>(1), 195--196. (<a href='https://doi.org/10.1137/24M169148X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Anita T. Layton},
  doi          = {10.1137/24M169148X},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {195--196},
  shortjournal = {SIAM Rev.},
  title        = {Book reviews},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neighborhood watch in mechanics: Nonlocal models and convolution. <em>SIREV</em>, <em>67</em>(1), 176--193. (<a href='https://doi.org/10.1137/22M1541721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper is intended to serve as a low-hurdle introduction to nonlocality for graduate students and researchers with an engineering mechanics or physics background who did not have a formal introduction to the underlying mathematical basis. We depart from simple examples motivated by structural mechanics to form a physical intuition and demonstrate nonlocality using concepts familiar to most engineers. We then show how concepts of nonlocality are at the core of one of the most active current research fields in applied mechanics, namely, in phase-field modeling of fracture. From a mathematical perspective, these developments rest on the concept of convolution in both its discrete and its continuous forms. The previous mechanical examples may thus serve as an intuitive explanation of what convolution implies from a physical perspective. In the supplementary material we highlight a broader range of applications of the concepts of nonlocality and convolution in other branches of science and engineering by generalizing from the examples explained in detail in the main body of the article.},
  archive      = {J_SIREV},
  author       = {Thomas Nagel and Tymofiy Gerasimov and Jere Remes and Dominik Kern},
  doi          = {10.1137/22M1541721},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {176--193},
  shortjournal = {SIAM Rev.},
  title        = {Neighborhood watch in mechanics: Nonlocal models and convolution},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural networks and applied linear algebra. <em>SIREV</em>, <em>67</em>(1), 141--175. (<a href='https://doi.org/10.1137/23M1609786'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sparse matrix computations are ubiquitous in scientific computing. Given the recent interest in scientific machine learning, it is natural to ask how sparse matrix computations can leverage neural networks (NNs). Unfortunately, multilayer perceptron (MLP) NNs are typically not natural for either graph or sparse matrix computations. The issue lies with the fact that MLPs require fixed-sized inputs, while scientific applications generally generate sparse matrices with arbitrary dimensions and a wide range of different nonzero patterns (or matrix graph vertex interconnections). While convolutional NNs could possibly address matrix graphs where all vertices have the same number of nearest neighbors, a more general approach is needed for arbitrary sparse matrices, e.g., those arising from discretized partial differential equations on unstructured meshes. Graph neural networks (GNNs) are one such approach suitable to sparse matrices. The key idea is to define aggregation functions (e.g., summations) that operate on variable-size input data to produce data of a fixed output size so that MLPs can be applied. The goal of this paper is to provide an introduction to GNNs for a numerical linear algebra audience. Concrete GNN examples are provided to illustrate how many common linear algebra tasks can be accomplished using GNNs. We focus on iterative and multigrid methods that employ computational kernels such as matrix-vector products, interpolation, relaxation methods, and strength-of-connection measures. Our GNN examples include cases where parameters are determined a priori as well as cases where parameters must be learned. The intent of this paper is to help computational scientists understand how GNNs can be used to adapt machine learning concepts to computational tasks associated with sparse matrices. It is hoped that this understanding will further stimulate data-driven extensions of classical sparse linear algebra tasks.},
  archive      = {J_SIREV},
  author       = {Nicholas S. Moore and Eric C. Cyr and Peter Ohm and Christopher M. Siefert and Raymond S. Tuminaro},
  doi          = {10.1137/23M1609786},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {141--175},
  shortjournal = {SIAM Rev.},
  title        = {Graph neural networks and applied linear algebra},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Education. <em>SIREV</em>, <em>67</em>(1), 139--140. (<a href='https://doi.org/10.1137/24M1691478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Hélène Frankowska},
  doi          = {10.1137/24M1691478},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {139--140},
  shortjournal = {SIAM Rev.},
  title        = {Education},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Limits of learning dynamical systems. <em>SIREV</em>, <em>67</em>(1), 107--137. (<a href='https://doi.org/10.1137/24M1696974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A dynamical system is a transformation of a phase space, and the transformation law is the primary means of defining as well as identifying the dynamical system and is the object of focus of many learning techniques. However, there are many secondary aspects of dynamical systems—invariant sets, the Koopman operator, and Markov approximations—that provide alternative objectives for learning techniques. Crucially, while many learning methods are focused on the transformation law, we find that forecast performance can depend on how well these other aspects of the dynamics are approximated. These different facets of a dynamical system correspond to objects in completely different spaces—namely, interpolation spaces, compact Hausdorff sets, unitary operators, and Markov operators, respectively. Thus, learning techniques targeting any of these four facets perform different kinds of approximations. We examine whether an approximation of any one of these aspects of the dynamics could lead to an approximation of another facet. Many connections and obstructions are brought to light in this analysis. Special focus is placed on methods of learning the primary feature—the dynamics law itself. The main question considered is the connection between learning this law and reconstructing the Koopman operator and the invariant set. The answers are tied to the ergodic and topological properties of the dynamics, and they reveal how these properties determine the limits of forecasting techniques.},
  archive      = {J_SIREV},
  author       = {Tyrus Berry and Suddhasattwa Das},
  doi          = {10.1137/24M1696974},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {107--137},
  shortjournal = {SIAM Rev.},
  title        = {Limits of learning dynamical systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIGEST. <em>SIREV</em>, <em>67</em>(1), 105. (<a href='https://doi.org/10.1137/24M1691454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {The Editors},
  doi          = {10.1137/24M1691454},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {105},
  shortjournal = {SIAM Rev.},
  title        = {SIGEST},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The troublesome kernel: On hallucinations, no free lunches, and the accuracy-stability tradeoff in inverse problems. <em>SIREV</em>, <em>67</em>(1), 73--104. (<a href='https://doi.org/10.1137/23M1568739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Methods inspired by artificial intelligence (AI) are starting to fundamentally change computational science and engineering through breakthrough performance on challenging problems. However, the reliability and trustworthiness of such techniques is a major concern. In inverse problems in imaging, the focus of this paper, there is increasing empirical evidence that methods may suffer from hallucinations, i.e., false, but realistic-looking artifacts; instability, i.e., sensitivity to perturbations in the data; and unpredictable generalization, i.e., excellent performance on some images, but significant deterioration on others. This paper provides a theoretical foundation for these phenomena. We give mathematical explanations for how and when such effects arise in arbitrary reconstruction methods, with several of our results taking the form of “no free lunch” theorems. Specifically, we show that (i) methods that overperform on a single image can wrongly transfer details from one image to another, creating a hallucination; (ii) methods that overperform on two or more images can hallucinate or be unstable; (iii) optimizing the accuracy-stability tradeoff is generally difficult; (iv) hallucinations and instabilities, if they occur, are not rare events and may be encouraged by standard training; and (v) it may be impossible to construct optimal reconstruction maps for certain problems. Our results trace these effects to the kernel of the forward operator whenever it is nontrivial, but also apply to the case when the forward operator is ill-conditioned. Based on these insights, our work aims to spur research into new ways to develop robust and reliable AI-based methods for inverse problems in imaging.},
  archive      = {J_SIREV},
  author       = {Nina M. Gottschling and Vegard Antun and Anders C. Hansen and Ben Adcock},
  doi          = {10.1137/23M1568739},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {73--104},
  shortjournal = {SIAM Rev.},
  title        = {The troublesome kernel: On hallucinations, no free lunches, and the accuracy-stability tradeoff in inverse problems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research spotlights. <em>SIREV</em>, <em>67</em>(1), 71. (<a href='https://doi.org/10.1137/24M1691442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Stefan M. Wild},
  doi          = {10.1137/24M1691442},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {71},
  shortjournal = {SIAM Rev.},
  title        = {Research spotlights},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-adaptive approaches to stochastic optimization: A survey. <em>SIREV</em>, <em>67</em>(1), 3--70. (<a href='https://doi.org/10.1137/22M1538946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Uncertainty is prevalent in engineering design and data-driven problems and, more broadly, in decision making. Due to inherent risk-averseness and ambiguity about assumptions, it is common to address uncertainty by formulating and solving conservative optimization models expressed using measures of risk and related concepts. We survey the rapid development of risk measures over the last quarter century. From their beginning in financial engineering, we recount their spread to nearly all areas of engineering and applied mathematics. Solidly rooted in convex analysis, risk measures furnish a general framework for handling uncertainty with significant computational and theoretical advantages. We describe the key facts, list several concrete algorithms, and provide an extensive list of references for further reading. The survey recalls connections with utility theory and distributionally robust optimization, points to emerging applications areas such as fair machine learning, and defines measures of reliability.},
  archive      = {J_SIREV},
  author       = {Johannes O. Royset},
  doi          = {10.1137/22M1538946},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {3--70},
  shortjournal = {SIAM Rev.},
  title        = {Risk-adaptive approaches to stochastic optimization: A survey},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey and review. <em>SIREV</em>, <em>67</em>(1), 1. (<a href='https://doi.org/10.1137/24M1691430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIREV},
  author       = {Marlis Hochbruck},
  doi          = {10.1137/24M1691430},
  journal      = {SIAM Review},
  month        = {3},
  number       = {1},
  pages        = {1},
  shortjournal = {SIAM Rev.},
  title        = {Survey and review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
