<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JUQ</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="juq">JUQ - 65</h2>
<ul>
<li><details>
<summary>
(2025). \({p}\)-multilevel monte carlo for acoustic scattering from large deviation rough random surfaces. <em>JUQ</em>, <em>13</em>(4), 1950--1971. (<a href='https://doi.org/10.1137/23M1618909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study time-harmonic acoustic scattering on large deviation rough random scatterers. Therein, the roughness of the scatterers is caused by a low Sobolev regularity in the covariance function of their deformation field. Concretely, we consider matrix-valued covariance functions whose components are given by Matérn covariance functions with small smoothness index. The low Sobolev regularity is reflected by a slow decay of the deformation field’s Karhunen–Loève expansion. The motivation for this study arises from physical phenomena where small-scale material defects can potentially introduce nonsmooth deviations from a reference domain. The primary challenge in this scenario is that the scattered wave is also random, which makes computational predictions unreliable. Therefore, it is essential to quantify these uncertainties to ensure robust and well-informed design processes. While existing methods for uncertainty quantification typically rely on domain mapping or perturbation approaches, it turns out that large and rough random deviations are not satisfactorily covered. To close this gap, and although counterintuitive at first, we show that the -multilevel Monte Carlo method can provide an efficient tool for uncertainty quantification in this setting. To this end, we discuss the stable implementation of higher-order polynomial approximation of the deformation field by means of barycentric interpolation and provide a cost-to-accuracy analysis. Our considerations are complemented by numerical experiments in three dimensions on an intricate scattering geometry.},
  archive      = {J_JUQ},
  author       = {Jürgen Dölz and Wei Huang and Michael Multerer},
  doi          = {10.1137/23M1618909},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1950--1971},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {\({p}\)-multilevel monte carlo for acoustic scattering from large deviation rough random surfaces},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-explicit shape holomorphy in uncertainty quantification for acoustic scattering. <em>JUQ</em>, <em>13</em>(4), 1904--1949. (<a href='https://doi.org/10.1137/24M1688643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider frequency-domain acoustic scattering at a homogeneous star-shaped penetrable obstacle, whose shape is uncertain and modeled via a radial spectral parameterization with random coefficients. Using recent results on the stability of Helmholtz transmission problems with piecewise constant coefficients from A. Moiola and E. A. Spence, Math. Models Methods Appl. Sci., 29 (2019), pp. 317–354, we obtain frequency-explicit statements on the holomorphic dependence of the scattered field and the far-field pattern on the stochastic shape parameters. This paves the way for applying general results on the efficient construction of high-dimensional surrogate models. We also take into account the effect of domain truncation by means of perfectly matched layers (PMLs). In addition, spatial regularity estimates which are explicit in terms of the wavenumber permit us to quantify the impact of finite-element Galerkin discretization using high-order Lagrangian finite-element spaces.},
  archive      = {J_JUQ},
  author       = {R. Hiptmair and Ch. Schwab and E. A. Spence},
  doi          = {10.1137/24M1688643},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1904--1949},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Frequency-explicit shape holomorphy in uncertainty quantification for acoustic scattering},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence rates for the maximum a posteriori estimator in PDE-regression models with random design. <em>JUQ</em>, <em>13</em>(4), 1862--1903. (<a href='https://doi.org/10.1137/25M1744526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the statistical inverse problem of recovering a parameter from data arising from the Gaussian regression problem with nonlinear forward map , random design point , and Gaussian noise . The estimation strategy is based on a least squares approach given -constraints. We establish the existence of a least squares estimator as a maximizer for a given functional for Lipschitz-type assumptions on the forward map . A general concentration result is shown, which is used to prove consistency and upper bounds for the prediction error. The corresponding rates of convergence reflect not only the smoothness of the parameter of interest but also the ill-posedness of the underlying inverse problem. We apply the general model to the Darcy problem, where the recovery of an unknown coefficient function of a PDE is of interest. For this example, we also provide corresponding rates of convergence for the prediction and estimation errors. Additionally, we briefly discuss the applicability of the general model to other problems.},
  archive      = {J_JUQ},
  author       = {Maximilian Siebel},
  doi          = {10.1137/25M1744526},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1862--1903},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Convergence rates for the maximum a posteriori estimator in PDE-regression models with random design},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model error covariance estimation for weak constraint variational data assimilation. <em>JUQ</em>, <em>13</em>(4), 1828--1861. (<a href='https://doi.org/10.1137/24M1695889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. State estimates from weak constraint four-dimensional variational (4D-Var) data assimilation can vary significantly depending on the data and model error covariances. As a result, the accuracy of these estimates heavily depends on the correct specification of both model and observational data error covariances. In this work, we assume that the data error is known and focus on estimating the model error covariance by framing weak constraint 4D-Var as a regularized inverse problem, where the inverse model error covariance serves as the regularization matrix. We consider both isotropic and nonisotropic forms of the model error covariance, with hyperparameters such as the model error variance, spatial correlation length, and temporal correlation scale. Using the representer method, we reduce the 4D-Var problem from state space to data space, enabling the efficient application of regularization parameter selection techniques to estimate the model covariance hyperparameters. The Representer method also provides an analytic expression for the optimal state estimate, allowing us to derive matrix expressions for the three regularization parameter selection methods i.e., the L-curve, generalized cross-validation (GCV), and method. We validate our approach by assimilating simulated data into a one-dimensional(1D)transport equation modeling wildfire smoke transport under various observational noise and forward model perturbations. In these experiments, the goal is to identify the model error covariance estimates that accurately capture the influence of observational data versus model predictions on assimilated state estimates. The regularization parameter selection methods successfully estimate hyperparameters for both isotropic and nonisotropic model error covariances, which reflect whether the first guess model predictions are more or less reliable than the observational data. The results further indicate that isotropic variances are sufficient when the first guess is more accurate than the data, whereas nonisotropic covariances are preferred when the observational data are more reliable.},
  archive      = {J_JUQ},
  author       = {Sandra R. Babyale and Jodi Mead and Donna Calhoun and Patricia O. Azike},
  doi          = {10.1137/24M1695889},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1828--1861},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Model error covariance estimation for weak constraint variational data assimilation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequentially refined latin hypercube designs with flexibly and adaptively chosen sample sizes. <em>JUQ</em>, <em>13</em>(4), 1812--1827. (<a href='https://doi.org/10.1137/24M1673048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Latin hypercube designs are the most popular type of experimental design for computer experiments. Sequentially refined Latin hypercube designs are useful for computer experiments that are carried out in batches. In this work, we propose the first type of sequentially refined Latin hypercube designs that allow the size of subsequent batches to be flexibly chosen after completing former batches. Such designs are useful when the amount of additional computational resources or the total number of trials needed is unknown when designing the first stage experiments. Numerical results show our proposed designs are uniformly better than preceding types of sequentially refined Latin hypercube designs for the problem of uncertainty quantification.},
  archive      = {J_JUQ},
  author       = {Jin Xu and Junpeng Gong and Xiaojun Duan and Zhengming Wang and Xu He},
  doi          = {10.1137/24M1673048},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1812--1827},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sequentially refined latin hypercube designs with flexibly and adaptively chosen sample sizes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grouped orthogonal arrays for computer experiments with grouped inputs. <em>JUQ</em>, <em>13</em>(4), 1791--1811. (<a href='https://doi.org/10.1137/25M1741960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose methods for constructing a new type of space-filling design, termed a grouped orthogonal array, to accommodate natural input grouping in computer experiments. Such a design maintains space-filling properties across all inputs while exhibiting enhanced space-filling properties within groups compared to across groups. Guided by combinatorial orthogonality as a space-filling criterion, our methods generate grouped orthogonal arrays as special strength-two orthogonal arrays whose columns are partitioned into groups of strength-three orthogonal arrays. The proposed methods are easy to implement and capable of handling a large number of factors. We provide examples to illustrate the methods and compare the constructed designs with other space-filling designs, highlighting the benefits of leveraging the grouping structure. Additionally, simulations demonstrate the effectiveness of the proposed designs in the problem of computer model emulation.},
  archive      = {J_JUQ},
  author       = {Wenlong Li and Jian-Feng Yang and Peter Chien},
  doi          = {10.1137/25M1741960},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1791--1811},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Grouped orthogonal arrays for computer experiments with grouped inputs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive design for contour estimation from computer experiments with quantitative and qualitative inputs. <em>JUQ</em>, <em>13</em>(4), 1766--1790. (<a href='https://doi.org/10.1137/24M1685742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Computer experiments with quantitative and qualitative inputs are widely used to study many scientific and engineering processes. Much of the existing work has focused on design and modeling or process optimization for such experiments. This paper proposes an adaptive design approach for estimating a contour from computer experiments with quantitative and qualitative inputs. A new criterion is introduced to search for the follow-up inputs. The key features of the proposed criterion are (a) the criterion yields adaptive search regions; and (b) it is region-based cooperative in that for each stage of the sequential procedure, the candidate points in the design space is divided into two disjoint groups using confidence bounds, and within each group, an acquisition function is used to select a candidate point. Among the two selected points, a point that is closer to the contour level with the higher uncertainty or that has higher uncertainty when the distance between its prediction and the contour level is within a threshold is chosen. The proposed approach provides empirically more accurate contour estimation than existing approaches as illustrated in numerical examples and a real application. Theoretical justification of the proposed adaptive search region is given.},
  archive      = {J_JUQ},
  author       = {Anita Shahrokhian and Xinwei Deng and C. Devon Lin and Pritam Ranjan and Li Xu},
  doi          = {10.1137/24M1685742},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1766--1790},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Adaptive design for contour estimation from computer experiments with quantitative and qualitative inputs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient-adjusted underdamped langevin dynamics for sampling. <em>JUQ</em>, <em>13</em>(4), 1735--1765. (<a href='https://doi.org/10.1137/24M1702015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sampling from a target distribution is a fundamental problem with wide-ranging applications in scientific computing and machine learning. Traditional Markov chain Monte Carlo (MCMC) algorithms, such as the unadjusted Langevin algorithm (ULA), derived from the overdamped Langevin dynamics, have been extensively studied. From an optimization perspective, the Kolmogorov forward equation of the overdamped Langevin dynamics can be treated as the gradient flow of the relative entropy in the space of probability densities embedded with Wasserstein-2 metrics. Several efforts have also been devoted to including momentum-based methods, such as underdamped Langevin dynamics, for faster convergence of sampling algorithms. Recent advances in optimization have demonstrated the effectiveness of primal-dual damping and Hessian-driven damping dynamics in achieving faster convergence when solving optimization problems. Motivated by these developments, we introduce a class of stochastic differential equations (SDEs) called gradient-adjusted underdamped Langevin dynamics (GAUL), which add stochastic perturbations in primal-dual damping dynamics and Hessian-driven damping dynamics from optimization. We prove that GAUL admits the correct invariant distribution, whose marginal is the target distribution. The proposed method outperforms overdamped and underdamped Langevin dynamics regarding convergence speed in the total variation distance for Gaussian target distributions. Moreover, using the Euler–Maruyama discretization, we show that the mixing time toward a biased target distribution only depends on the square root of the condition number of the target covariance matrix. In addition, we propose another discretization scheme based on the splitting method, which yields a smaller first-order asymptotic bias than the Euler–Maruyama scheme when sampling a Gaussian distribution. Numerical experiments for non-Gaussian target distributions, such as Bayesian regression problems and Bayesian neural networks, further illustrate the advantages of our approach over classical methods based on overdamped or underdamped Langevin dynamics. We also compare with the randomized Hamiltonian Monte Carlo method, showing that it achieves competitive performance.},
  archive      = {J_JUQ},
  author       = {Xinzhe Zuo and Stanley Osher and Wuchen Li},
  doi          = {10.1137/24M1702015},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1735--1765},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Gradient-adjusted underdamped langevin dynamics for sampling},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A gradient-based and determinant-free framework for fully bayesian gaussian process regression. <em>JUQ</em>, <em>13</em>(4), 1709--1734. (<a href='https://doi.org/10.1137/25M1724833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Gaussian Process Regression (GPR) is widely used for inferring functions from noisy data. GPR crucially relies on the choice of a kernel, which might be specified in terms of a collection of hyperparameters that must be chosen or learned. Fully Bayesian GPR seeks to infer these kernel hyperparameters in a Bayesian sense, and the key computational challenge in sampling from their posterior distribution is the need for frequent determinant evaluations of large kernel matrices. This paper introduces a gradient-based, determinant-free approach for fully Bayesian GPR that combines a Gaussian integration trick for avoiding the determinant with Hamiltonian Monte Carlo (HMC) sampling. Our framework permits a matrix-free formulation and reduces the difficulty of dealing with hyperparameter gradients to a simple automatic differentiation. Our implementation is highly flexible and leverages GPU acceleration with linear-scaling memory footprint. Numerical experiments demonstrate the method’s ability to scale gracefully to both high-dimensional hyperparameter spaces and large kernel matrices.},
  archive      = {J_JUQ},
  author       = {P. Michael Kielstra and Michael Lindsey},
  doi          = {10.1137/25M1724833},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1709--1734},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A gradient-based and determinant-free framework for fully bayesian gaussian process regression},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean field games for controlling coherent structures in nonlinear fluid systems. <em>JUQ</em>, <em>13</em>(4), 1681--1708. (<a href='https://doi.org/10.1137/24M1632231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper discusses the control of coherent structures in turbulent flows, which has broad applications among complex systems in science and technology. Mean field games have been proved a powerful tool and are proposed here to control the stochastic Lagrangian particles as players tracking the flow and tracer fields. We derive optimal control solutions for general nonlinear fluid systems using mean-field game models and develop computational algorithms to efficiently solve the resulting coupled forward and backward mean-field system. A precise link is established for the control of passive tracer density and the scalar vorticity field based on the functional Hamilton–Jacobi equations derived from the mean field models. A new iterative numerical strategy is then constructed to compute the optimal solution with fast convergence. We verify the skill of the mean-field control models and illustrate their practical efficiency on a prototype model modified from the viscous Burgers equation under various cost functions in both deterministic and stochastic formulations. The good model performance implies potential effectiveness of the strategy for more general high-dimensional turbulent systems.},
  archive      = {J_JUQ},
  author       = {Yuan Gao and Di Qi},
  doi          = {10.1137/24M1632231},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {12},
  number       = {4},
  pages        = {1681--1708},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Mean field games for controlling coherent structures in nonlinear fluid systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse inverse cholesky factorization of dense kernel matrices by greedy conditional selection. <em>JUQ</em>, <em>13</em>(3), 1649--1679. (<a href='https://doi.org/10.1137/23M1606253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Dense kernel matrices resulting from pairwise evaluations of a kernel function arise naturally in machine learning and statistics. Previous work in constructing sparse approximate inverse Cholesky factors of such matrices by minimizing Kullback–Leibler divergence recovers the Vecchia approximation for Gaussian processes. These methods rely only on the geometry of the evaluation points to construct the sparsity pattern. In this work, we instead construct the sparsity pattern by leveraging a greedy selection algorithm that maximizes mutual information with target points, conditional on all points previously selected. For selecting points out of , the naive time complexity is , but by maintaining a partial Cholesky factor we reduce this to . Furthermore, for multiple targets we achieve a time complexity of , which is maintained in the setting of aggregated Cholesky factorization where a selected point need not condition every target. We apply the selection algorithm to image classification and recovery of sparse Cholesky factors. By minimizing Kullback–Leibler divergence, we apply the algorithm to Cholesky factorization, Gaussian process regression, and preconditioning the conjugate gradient method, improving over -nearest neighbors selection.},
  archive      = {J_JUQ},
  author       = {Stephen Huan and Joseph Guinness and Matthias Katzfuss and Houman Owhadi and Florian Schäofer},
  doi          = {10.1137/23M1606253},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1649--1679},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sparse inverse cholesky factorization of dense kernel matrices by greedy conditional selection},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariate-informed bifidelity bias correction of distributional output. <em>JUQ</em>, <em>13</em>(3), 1616--1648. (<a href='https://doi.org/10.1137/24M1690667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Stochastic computational physics simulators often exist at varying fidelities, with those of higher fidelities being more physically accurate but at an increased computational cost relative to biased, low-fidelity simulators. The underlying stochasticity suggests running ensemble simulations, which may be prohibitively expensive at high fidelities. In this work we propose a functional data distribution-on-distribution bias correction model that predicts high-fidelity distributional output from a corresponding paired low-fidelity distributional output. Our model respects constraints on distribution-based functional data objects (e.g., probability density functions and cumulative distribution functions) through the use of log quantile density (LQD) functions. In particular, we specify a Gaussian process (GP) emulator on a basis decomposition of LQD discrepancies, which allows known covariates from the low-fidelity simulation to inform the bias correction model. GPs have the benefits of fast uncertainty quantification and reasonably fast training time for small to moderate amounts of training data, both desirable for the emulation of computer model output. We examine properties of our model through a comprehensive set of synthetically generated density pairs, considering various classes of discrepancies in densities (location, scale, shape, and modality). We then demonstrate the model on particle transport simulations through bifidelity representations of subsurface fracture networks.},
  archive      = {J_JUQ},
  author       = {Justin D. Strait and Kelly R. Moran and Alexander C. Murph and Jeffrey D. Hyman and Hari S. Viswanathan and Philip H. Stauffer},
  doi          = {10.1137/24M1690667},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1616--1648},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Covariate-informed bifidelity bias correction of distributional output},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable simulation-based inference for implicitly defined models using a metamodel for monte carlo log-likelihood estimator. <em>JUQ</em>, <em>13</em>(3), 1578--1615. (<a href='https://doi.org/10.1137/24M1707079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Models implicitly defined through a random simulator of a process have become widely used in scientific and industrial applications in recent years. However, simulation-based inference methods for such implicit models, like approximate Bayesian computation (ABC), often scale poorly as data size increases. We develop a scalable inference method for implicitly defined models using a metamodel for the Monte Carlo log-likelihood estimator derived from simulations. This metamodel characterizes both statistical and simulation-based randomness in the distribution of the log-likelihood estimator across different parameter values. Our metamodel-based method quantifies uncertainty in parameter estimation in a principled manner, leveraging the local asymptotic normality of the mean function of the log-likelihood estimator. We apply this method to construct accurate confidence intervals for parameters of partially observed Markov process models where the Monte Carlo log-likelihood estimator is obtained using the bootstrap particle filter. We numerically demonstrate that our method enables accurate and highly scalable parameter inference across several examples, including a mechanistic compartment model for infectious diseases.},
  archive      = {J_JUQ},
  author       = {Joonha Park},
  doi          = {10.1137/24M1707079},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1578--1615},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Scalable simulation-based inference for implicitly defined models using a metamodel for monte carlo log-likelihood estimator},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable bayesian physics-informed kolmogorov-arnold networks. <em>JUQ</em>, <em>13</em>(3), 1543--1577. (<a href='https://doi.org/10.1137/25M1729253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Bayesian physics-informed neural networks serve as important scientific machine learning methods for uncertainty quantification. Although multilayer perceptrons are commonly employed as surrogates, they often suffer from overfitting due to their large number of parameters. Kolmogorov–Arnold networks (KANs) offer an alternative solution with fewer parameters. However, gradient-based inference methods, such as Hamiltonian Monte Carlo (HMC), may result in computational inefficiency when applied to KANs, especially for large-scale datasets, due to the high cost of back-propagation. To address these challenges, we propose a novel approach, combining the dropout Tikhonov ensemble Kalman inversion with Chebyshev KANs. This gradient-free scheme effectively mitigates overfitting and enhances numerical stability. Additionally, we incorporate the active subspace method to reduce the parameter-space dimensionality, allowing us to improve the accuracy of predictions and obtain more reliable uncertainty estimates. Extensive experiments demonstrate the efficacy of our approach in various test cases, including scenarios with large datasets and high noise levels. Our results demonstrate that the new method achieves comparable or better accuracy and much higher efficiency as well as stability compared to HMC, in addition to scalability.},
  archive      = {J_JUQ},
  author       = {Zhiwei Gao and George Em Karniadakis},
  doi          = {10.1137/25M1729253},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1543--1577},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Scalable bayesian physics-informed kolmogorov-arnold networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian emulation of grey-box multimodel ensembles exploiting known interior structure. <em>JUQ</em>, <em>13</em>(3), 1501--1542. (<a href='https://doi.org/10.1137/24M1669037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Computer models are widely used to study complex real world physical systems. However, there are major limitations to their direct use including their complex structure; large numbers of inputs and outputs; and long evaluation times. Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs. It is commonly assumed that computer models behave like a “black-box” function with no knowledge of the output prior to its evaluation. This ensures that emulators are generalizable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behavior. We assume a “grey-box” computer model and develop a methodological toolkit for its analysis. This includes multimodel ensemble subsampling to identifying a representative model subset to reduce computational expense; constructing a targeted Bayesian design for optimization or decision support; a “divide-and-conquer” approach to emulating sums of outputs; structured emulators exploiting known constrained and structured behavior of constituent outputs through splitting the parameter space and imposing truncations; emulation of sums of time series outputs; and emulation of multimodel ensemble outputs. Combining these methods establishes a hierarchical emulation framework which achieves greater physical interpretability and more accurate emulator predictions. This research is motivated by and applied to the commercially important TNO OLYMPUS Well Control Optimization Challenge from the petroleum industry which we re-express as a decision support under uncertainty problem. We thus encourage users to examine their “black-box” simulators to achieve superior emulator accuracy.},
  archive      = {J_JUQ},
  author       = {Jonathan Owen and Ian Vernon},
  doi          = {10.1137/24M1669037},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1501--1542},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Bayesian emulation of grey-box multimodel ensembles exploiting known interior structure},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building population-informed priors for bayesian inference using data-consistent stochastic inversion. <em>JUQ</em>, <em>13</em>(3), 1475--1500. (<a href='https://doi.org/10.1137/24M1678234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Bayesian inference provides a powerful tool for leveraging observational data to inform model predictions and uncertainties. However, when such data are limited, Bayesian inference may not adequately constrain uncertainty without the use of highly informative priors. Common approaches for constructing informative priors typically rely on either assumptions or knowledge of the underlying physics, which is not always available. In this work, we consider the scenario where data are available on a population of assets/individuals, which occurs in many problem domains such as biomedical or digital twin applications, and leverage this population-level data to systematically constrain the Bayesian prior and subsequently improve individualized inferences. The approach proposed in this paper is based upon a recently developed technique known as data-consistent inversion (DCI) for constructing a pullback probability measure. Succinctly, we utilize DCI to build population-informed priors for subsequent Bayesian inference on individuals. While the approach is general and applies to nonlinear maps and arbitrary priors, we prove that for linear inverse problems with Gaussian priors, the population-informed prior produces an increase in the information gain as measured by the determinant and trace of the inverse posterior covariance. We also demonstrate that the Kullback–Leibler divergence often improves with high probability. Numerical results, including linear-Gaussian examples and one inspired by digital twins for additively manufactured assets, indicate that there is significant value in using these population-informed priors.},
  archive      = {J_JUQ},
  author       = {Rebekah D. White and John D. Jakeman and Tim Wildey and Troy Butler},
  doi          = {10.1137/24M1678234},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1475--1500},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Building population-informed priors for bayesian inference using data-consistent stochastic inversion},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-dimensional subspace regularization through structured tensor priors. <em>JUQ</em>, <em>13</em>(3), 1452--1474. (<a href='https://doi.org/10.1137/24M1688497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Specifying a prior distribution is an essential part of solving Bayesian inverse problems. The prior encodes a belief on the nature of the solution and this regularizes the problem. In this article we completely characterize a Gaussian prior that encodes the belief that the solution is a structured tensor that lies in a low-dimensional subspace. We define the notion of -constrained tensors and show that they describe a large variety of different structures such as Hankel, circulant, triangular, symmetric, and so on. We prove that the low-dimensional subspace defined by this prior is the right nullspace of the matrix that defines the tensor structure. We completely characterize the Gaussian probability distribution of such tensors by specifying its mean vector and covariance matrix in terms of and . Furthermore, explicit expressions are proved for the covariance matrix of tensors whose entries are invariant under a permutation. These results unlock a whole new class of priors for Bayesian inverse problems. We illustrate how new kernel functions can be designed and efficiently computed and apply our results on two particular Bayesian inverse problems: completing a Hankel matrix from a few noisy measurements and learning an image classifier of handwritten digits. The effectiveness of the proposed priors is demonstrated for both problems. All applications have been implemented as reactive Pluto notebooks in Julia.},
  archive      = {J_JUQ},
  author       = {Kim Batselier},
  doi          = {10.1137/24M1688497},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1452--1474},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Low-dimensional subspace regularization through structured tensor priors},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariance-free bifidelity control variates importance sampling for rare event reliability analysis. <em>JUQ</em>, <em>13</em>(3), 1406--1451. (<a href='https://doi.org/10.1137/24M1658498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Multifidelity modeling has been steadily gaining attention as a tool to address the problem of exorbitant model evaluation costs that makes the estimation of failure probabilities a significant computational challenge for complex real-world problems, particularly when failure is a rare event. To implement multifidelity modeling, estimators that efficiently combine information from multiple models/sources are necessary. In past works, the variance reduction techniques of control variates (CV) and importance sampling (IS) have been leveraged for this task. In this paper, we present the CVIS framework—a creative take on a coupled CV and IS estimator for bifidelity reliability analysis. The framework addresses some of the practical challenges of the CV method by using an estimator for the control variate mean and sidestepping the need to estimate the covariance between the original estimator and the control variate through a clever choice for the tuning constant. The task of selecting an efficient IS distribution is also considered, with a view towards maximally leveraging the bifidelity structure and maintaining expressivity. Additionally, a diagnostic is provided that indicates both the efficiency of the algorithm as well as the relative predictive quality of the models utilized. Finally, the behavior and performance of the framework is explored through analytical and numerical examples.},
  archive      = {J_JUQ},
  author       = {Promit Chakroborty and Somayajulu L. N. Dhulipala and Michael D. Shields},
  doi          = {10.1137/24M1658498},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1406--1451},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Covariance-free bifidelity control variates importance sampling for rare event reliability analysis},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sampling parameters of ordinary differential equations with constrained langevin dynamics. <em>JUQ</em>, <em>13</em>(3), 1374--1405. (<a href='https://doi.org/10.1137/24M1691569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Fitting models to data to obtain distributions of consistent parameter values is important for uncertainty quantification, model comparison, and prediction. Standard Markov chain Monte Carlo (MCMC) approaches for fitting ordinary differential equations (ODEs) to time-series data involve proposing trial parameter sets, numerically integrating the ODEs forward in time, and accepting or rejecting the trial parameter sets. When the model dynamics depend nonlinearly on the parameters, as is generally the case, trial parameter sets are often rejected, and MCMC approaches become prohibitively computationally costly to converge. Here, we build on methods for numerical continuation and trajectory optimization to introduce an approach in which we use Langevin dynamics in the joint space of variables and parameters to sample models that satisfy constraints on the dynamics. We demonstrate the method by sampling Hopf bifurcations and limit cycles of a model of a biochemical oscillator in a Bayesian framework for parameter estimation, and we attain performance that matches or exceeds the performance of leading MCMC approaches that require numerically integrating the ODEs forward in time. We describe numerical experiments that provide insight into the speedup. The method is general and can be used in any framework for parameter estimation and model selection.},
  archive      = {J_JUQ},
  author       = {Chris Chi and Jonathan Weare and Aaron R. Dinner},
  doi          = {10.1137/24M1691569},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1374--1405},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sampling parameters of ordinary differential equations with constrained langevin dynamics},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic convergence analysis of the inverse potential problem. <em>JUQ</em>, <em>13</em>(3), 1334--1373. (<a href='https://doi.org/10.1137/24M1703422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we investigate the inverse problem of recovering a potential coefficient in an elliptic partial differential equation from the observations at deterministic sampling points in the domain subject to random noise. We employ a least squares formulation with an penalty on the potential in order to obtain a numerical reconstruction and the Galerkin finite element method for the spatial discretization. Under mild regularity assumptions on the problem data, we provide a stochastic convergence analysis on the regularized solution and the finite element approximation in a high-probability sense. The obtained error bounds depend explicitly on the regularization parameter , the number of observation points, and the mesh size . These estimates provide a useful guideline for choosing relevant algorithmic parameters. Furthermore, we develop a monotonically convergent adaptive algorithm for determining a suitable regularization parameter in the absence of a priori knowledge. Numerical experiments are also provided to complement the theoretical results.},
  archive      = {J_JUQ},
  author       = {Bangti Jin and Qimeng Quan and Wenlong Zhang},
  doi          = {10.1137/24M1703422},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1334--1373},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Stochastic convergence analysis of the inverse potential problem},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-convergence to global minimizers for adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks. <em>JUQ</em>, <em>13</em>(3), 1294--1333. (<a href='https://doi.org/10.1137/24M1639464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Stochastic gradient descent (SGD) optimization methods such as the plain vanilla SGD method and the popular Adam optimizer are nowadays the method of choice in the training of artificial neural networks (ANNs). Despite the remarkable success of SGD methods in the ANN training in numerical simulations, it remains in essentially all practical relevant scenarios an open problem to rigorously explain why SGD methods seem to succeed to train ANNs. In particular, in most practically relevant supervised learning problems, it seems that SGD methods do with high probability not converge to global minimizers in the optimization landscape of the ANN training problem. Nevertheless, it remains an open problem of research to disprove the convergence of SGD methods to global minimizers. In this work, we solve this research problem in the situation of shallow ANNs with the rectified linear unit (ReLU) and related activations with the standard mean square error loss by disproving in the training of such ANNs that SGD methods (such as the plain vanilla SGD, the momentum SGD, the AdaGrad, the RMSprop, and the Adam optimizer) can find a global minimizer with high probability. Even stronger, we reveal in the training of such ANNs that SGD methods do with high probability fail to converge to global minimizers in the optimization landscape. The findings of this work do, however, not disprove that SGD methods succeed to train ANNs since they do not exclude the possibility that SGD methods find good local minimizers whose risk values are close to the risk values of the global minimizers. In this context, another key contribution of this work is to establish the existence of a hierarchical structure of local minimizers with distinct risk values in the optimization landscape of ANN training problems with ReLU and related activations.},
  archive      = {J_JUQ},
  author       = {Arnulf Jentzen and Adrian Riekert},
  doi          = {10.1137/24M1639464},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1294--1333},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Non-convergence to global minimizers for adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerate langevin sampling with birth-death process and exploration component. <em>JUQ</em>, <em>13</em>(3), 1265--1293. (<a href='https://doi.org/10.1137/23M1577584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sampling a probability distribution with known likelihood is a fundamental task in computational science and engineering. Aiming at multimodality, we propose a new sampling method that takes advantage of both the birth-death process and exploration component. The main idea of this method is look before you leap. We keep two sets of samplers, one at a warmer temperature and one at the original temperature. The former one serves as the pioneer in exploring new modes and passing useful information to the other, while the latter one samples the target distribution after receiving the information. We derive a mean-field limit and show how the exploration component accelerates the sampling process. Moreover, we prove exponential asymptotic convergence under mild assumption. Finally, we test this on experiments from previous literature and compare our methodology to previous ones.},
  archive      = {J_JUQ},
  author       = {Lezhi Tan and Jianfeng Lu},
  doi          = {10.1137/23M1577584},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1265--1293},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Accelerate langevin sampling with birth-death process and exploration component},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quasi–Monte carlo integration for feedback control under uncertainty. <em>JUQ</em>, <em>13</em>(3), 1228--1264. (<a href='https://doi.org/10.1137/24M1695531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A control in feedback form is derived for linear quadratic, time-invariant optimal control problems subject to parabolic partial differential equations with coefficients depending on a countably infinite number of uncertain parameters. It is shown that the Riccati-based feedback operator depends analytically on the parameters provided that the system operator depends analytically on the parameters, as is the case, for instance, in diffusion problems when the diffusion coefficient is parameterized by a Karhunen–Loève expansion. These novel parametric regularity results allow the application of quasi–Monte Carlo (QMC) methods to efficiently compute an a priori chosen feedback law based on the expected value. Moreover, under moderate assumptions on the input random field, QMC methods achieve superior error rates compared to ordinary Monte Carlo methods, independently of the stochastic dimension of the problem. Indeed, our paper for the first time studies Banach space–valued integration by higher-order QMC methods.},
  archive      = {J_JUQ},
  author       = {Philipp A. Guth and Peter Kritzer and Karl Kunisch},
  doi          = {10.1137/24M1695531},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1228--1264},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Quasi–Monte carlo integration for feedback control under uncertainty},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical finite elements via interacting particle langevin dynamics. <em>JUQ</em>, <em>13</em>(3), 1200--1227. (<a href='https://doi.org/10.1137/24M1693593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we develop a class of interacting particle Langevin algorithms to solve inverse problems for partial differential equations (PDEs). In particular, we leverage the statistical finite element method (statFEM) formulation to obtain a finite-dimensional latent variable statistical model where the parameter is that of the (discretized) forward map and the latent variable is the statFEM solution of the PDE which is assumed to be partially observed. We then adapt a recently proposed expectation-maximization–like scheme, interacting particle Langevin algorithm (IPLA), for this problem and obtain a joint estimation procedure for the parameters and the latent variables. We consider three main examples: (i) estimating the forcing for a linear Poisson PDE, (ii) estimating diffusivity for a linear Poisson PDE, and (iii) estimating the forcing for a nonlinear Poisson PDE. We provide computational complexity estimates for forcing estimation in the linear case. We also provide comprehensive numerical experiments and preconditioning strategies that significantly improve the performance, showing that the proposed class of methods can be the choice for parameter inference in PDE models.},
  archive      = {J_JUQ},
  author       = {Alex Glyn-Davies and Connor Duffin and Ieva Kazlauskaite and Mark Girolami and Ö. Deniz Akyildiz},
  doi          = {10.1137/24M1693593},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1200--1227},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Statistical finite elements via interacting particle langevin dynamics},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased markov chain quasi-monte carlo for gibbs samplers. <em>JUQ</em>, <em>13</em>(3), 1174--1199. (<a href='https://doi.org/10.1137/24M1660747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In statistical analysis, Monte Carlo (MC) is a classical numerical integration method. When faced with challenging sampling problems, Markov chain Monte Carlo (MCMC) is a commonly employed method. However, the MCMC estimator is biased after a fixed number of iterations. Unbiased MCMC, an advancement achieved through coupling techniques, addresses this bias issue in MCMC and allows us to run many short chains in parallel. Quasi-Monte Carlo (QMC), known for its high order of convergence, is an alternative to MC. By incorporating the idea of QMC into MCMC, Markov chain quasi-Monte Carlo (MCQMC) effectively reduces the variance of MCMC, especially in Gibbs samplers. This work presents a novel approach that integrates unbiased MCMC with MCQMC, referred to as an unbiased MCQMC method. This method renders unbiased estimators while improving the rate of convergence significantly. Numerical experiments demonstrate that for Gibbs sampling, unbiased MCQMC with a sample size of can yield a faster root mean square error (RMSE) rate than the rate of unbiased MCMC, approaching an RMSE rate of for low-dimensional problems. Surprisingly, in a challenging problem of a 1049-dimensional Pólya Gamma Gibbs sampler, the RMSE can still be reduced by several times for moderate sample sizes.},
  archive      = {J_JUQ},
  author       = {Jiarui Du and Zhijian He},
  doi          = {10.1137/24M1660747},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1174--1199},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Unbiased markov chain quasi-monte carlo for gibbs samplers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error bounds for a kernel-based constrained optimal smoothing approximation. <em>JUQ</em>, <em>13</em>(3), 1145--1173. (<a href='https://doi.org/10.1137/24M1676491'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper establishes error bounds for the convergence of a piecewise linear approximation of the constrained optimal smoothing problem posed in a reproducing kernel Hilbert space (RKHS). This problem can be reformulated as a Bayesian estimation problem involving a Gaussian process related to the kernel of the RKHS. Consequently, error bounds can be interpreted as a quantification of the maximum a posteriori (MAP) accuracy. To the best of our knowledge, no error bounds have been proposed for this type of problem so far. The convergence results are provided as a function of the grid size, the regularity of the kernel, and the distance from the kernel interpolant of the approximation to the set of constraints. Inspired by the MaxMod algorithm from recent literature, which sequentially allocates knots for the piecewise linear approximation, we conduct our analysis for nonequispaced knots. These knots are even allowed to be nondense, which impacts the definition of the optimal smoothing solution and our error bound quantifiers. Finally, we illustrate our theorems through several numerical experiments involving constraints such as boundedness and monotonicity.},
  archive      = {J_JUQ},
  author       = {Laurence Grammont and François Bachoc and Andrés F. López-Lopera},
  doi          = {10.1137/24M1676491},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1145--1173},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Error bounds for a kernel-based constrained optimal smoothing approximation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistency of bayesian inference for a subdiffusion equation. <em>JUQ</em>, <em>13</em>(3), 1116--1144. (<a href='https://doi.org/10.1137/24M1707419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we consider the inverse problem of determining an unknown potential in a subdiffusion equation from its solution using a nonparametric Bayesian approach. Our aim is to establish the consistency of the posterior distribution with Gaussian priors. To do so, we need some key estimates of the forward problem. For the forward problem, we have to overcome the fact that the solution of the subdiffusion equation is less regular than that of the classical heat equation. The main ingredient is the maximum principle for the subdiffusion equation. We show that the posterior contracts to the ground truth at a polynomial rate.},
  archive      = {J_JUQ},
  author       = {Pu-Zhao Kow and Jenn-Nan Wang},
  doi          = {10.1137/24M1707419},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1116--1144},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Consistency of bayesian inference for a subdiffusion equation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precision and cholesky factor estimation for gaussian processes. <em>JUQ</em>, <em>13</em>(3), 1085--1115. (<a href='https://doi.org/10.1137/24M1717282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper studies the estimation of large precision matrices and Cholesky factors obtained by observing a Gaussian process at many locations. Under general assumptions on the precision and the observations, we show that the sample complexity scales poly-logarithmically with the size of the precision matrix and its Cholesky factor. The key challenge in these estimation tasks is the polynomial growth of the condition number of the target matrices with their size. For precision estimation, our theory hinges on an intuitive local regression technique on the lattice graph which exploits the approximate sparsity implied by the screening effect. For Cholesky factor estimation, we leverage a block-Cholesky decomposition recently used to establish complexity bounds for sparse Cholesky factorization.},
  archive      = {J_JUQ},
  author       = {Jiaheng Chen and Daniel Sanz-Alonso},
  doi          = {10.1137/24M1717282},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1085--1115},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Precision and cholesky factor estimation for gaussian processes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing gaussian process surrogates for optimization and posterior approximation via random exploration. <em>JUQ</em>, <em>13</em>(3), 1054--1084. (<a href='https://doi.org/10.1137/24M1677009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper proposes novel noise-free Bayesian optimization strategies that rely on a random exploration step to enhance the accuracy of Gaussian process surrogate models. The new algorithms retain the ease of implementation of the classical GP-UCB algorithm, but the additional random exploration step accelerates their convergence, nearly achieving the optimal convergence rate. Furthermore, to facilitate Bayesian inference with intractable likelihoods, we propose to utilize optimization iterates for maximum a posteriori estimation to build a Gaussian process surrogate model for the unnormalized log-posterior density. We provide bounds for the Hellinger distance between the true and the approximate posterior distributions in terms of the number of design points. We demonstrate the effectiveness of our Bayesian optimization algorithms in nonconvex benchmark objective functions, in a machine learning hyperparameter tuning problem, and in a black-box engineering design problem. The effectiveness of our posterior approximation approach is demonstrated in two Bayesian inference problems for parameters of dynamical systems.},
  archive      = {J_JUQ},
  author       = {Hwanwoo Kim and Daniel Sanz-Alonso},
  doi          = {10.1137/24M1677009},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1054--1084},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Enhancing gaussian process surrogates for optimization and posterior approximation via random exploration},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal control under uncertainty with joint chance state constraints: Almost-everywhere bounds, variance reduction, and application to (Bi)linear elliptic PDEs. <em>JUQ</em>, <em>13</em>(3), 1028--1053. (<a href='https://doi.org/10.1137/24M171557X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study optimal control of partial differential equations (PDEs) under uncertainty with the state variable subject to joint chance constraints. The controls are deterministic, but the states are probabilistic due to random variables in the governing equation. Joint chance constraints ensure that the random state variable meets pointwise bounds with high probability. For linear governing PDEs and elliptically distributed random parameters, we prove existence and uniqueness results for almost-everywhere state bounds. Using the spherical-radial decomposition (SRD) of the uncertain variable, we prove that when the probability is very large or small, the resulting Monte Carlo estimator for the chance constraint probability exhibits substantially reduced variance compared to the standard Monte Carlo estimator. We further illustrate how the SRD can be leveraged to efficiently compute derivatives of the probability function, and we discuss different expansions of the uncertain variable in the governing equation. Numerical examples for linear and bilinear PDEs compare the performance of Monte Carlo and quasi-Monte Carlo sampling methods, examining probability estimation convergence as the number of samples increases. We also study how the accuracy of the probabilities depends on the truncation of the random variable expansion, and we numerically illustrate the variance reduction of the SRD.},
  archive      = {J_JUQ},
  author       = {René Henrion and Georg Stadler and Florian Wechsung},
  doi          = {10.1137/24M171557X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {1028--1053},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Optimal control under uncertainty with joint chance state constraints: Almost-everywhere bounds, variance reduction, and application to (Bi)linear elliptic PDEs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel monte carlo metamodeling for variance function estimation. <em>JUQ</em>, <em>13</em>(3), 980--1027. (<a href='https://doi.org/10.1137/24M1689740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This work introduces a novel multilevel Monte Carlo (MLMC) metamodeling approach for variance function estimation. Although devising an efficient experimental design for simulation metamodeling can be elusive, the MLMC-based approach addresses this challenge by dynamically adjusting the number of design points and budget allocation at each level, thereby automatically creating an efficient design. Theoretical analyses show that, under mild conditions, the proposed MLMC metamodeling approach for variance function estimation can achieve superior computational efficiency compared to standard Monte Carlo metamodeling while achieving the desired level of accuracy. Additionally, this work establishes the asymptotic normality of the MLMC metamodeling estimator under certain sufficient conditions, providing valuable insights for uncertainty quantification. Finally, two MLMC metamodeling procedures are proposed for variance function estimation: one to achieve a target accuracy level and another to efficiently utilize a fixed computational budget. Numerical evaluations support the theoretical results and demonstrate the potential of the proposed approach in facilitating global sensitivity analysis.},
  archive      = {J_JUQ},
  author       = {Jingtao Zhang and Xi Chen},
  doi          = {10.1137/24M1689740},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {980--1027},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multilevel monte carlo metamodeling for variance function estimation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularization for the approximation of functions by mollified discretization methods. <em>JUQ</em>, <em>13</em>(3), 957--979. (<a href='https://doi.org/10.1137/24M1678210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Some prominent discretization methods such as finite elements provide a way to approximate a function of variables from values it takes on the nodes of the corresponding mesh. The accuracy is in -norm, where is the order of the underlying method. When the data are measured or computed with systematical experimental noise, some statistical regularization might be desirable, with a smoothing method of order (like the number of vanishing moments of a kernel). This idea is behind the use of some regularized discretization methods, whose approximation properties are the subject of this paper. We decipher the interplay of and for reconstructing a smooth function on regular bounded domains from measurements with noise of order . We establish that for certain regimes with small noise depending on , when , statistical smoothing is not necessarily the best option and no regularization is more beneficial than statistical regularization. We precisely quantify this phenomenon and show that the gain can achieve a multiplicative order . We illustrate our estimates by numerical experiments conducted in dimension with and finite elements.},
  archive      = {J_JUQ},
  author       = {Marc Hoffmann and Camille Pouchol},
  doi          = {10.1137/24M1678210},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {957--979},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Regularization for the approximation of functions by mollified discretization methods},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shape optimization under constraints on the probability of a quadratic functional to exceed a given threshold. <em>JUQ</em>, <em>13</em>(3), 931--956. (<a href='https://doi.org/10.1137/23M1605107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This article is dedicated to shape optimization of elastic materials under random loadings where the particular focus is on the minimization of failure probabilities. Our approach relies on the fact that the area of integration is an ellipsoid in the high-dimensional parameter space when the shape functional of interest is quadratic. We derive the respective expressions for the shape functional and the related shape gradient. As showcase for the numerical implementation, we assume that the random loading of the state equation under consideration is a Gaussian random field. By exploiting the specialities of this setting, we derive an efficient shape optimization algorithm. Numerical results in three spatial dimensions validate the feasibility of our approach.},
  archive      = {J_JUQ},
  author       = {Marc Dambrine and Giulio Gargantini and Helmut Harbrecht and Jérôme Maynadier},
  doi          = {10.1137/23M1605107},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {931--956},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Shape optimization under constraints on the probability of a quadratic functional to exceed a given threshold},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the mean field theory of ensemble kalman filters for SPDEs. <em>JUQ</em>, <em>13</em>(3), 891--930. (<a href='https://doi.org/10.1137/24M1658954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper is concerned with the mathematical analysis of continuous time ensemble Kalman filters (EnKBFs) and their mean field limit in an infinite dimensional setting. The signal is determined by a nonlinear stochastic partial differential equation (SPDE), which is posed in the standard variational setting. Assuming global one-sided Lipschitz conditions and finite dimensional observations, we first prove the well posedness of both the EnKBF and its corresponding mean field version. We then investigate the quantitative convergence of the EnKBF towards its mean field limit, recovering the rates suggested by the law of large numbers for bounded observation functions. The main tool hereby are exponential moment estimates for the empirical covariance of the EnKBF, which may be of independent interest. In the appendix of the paper we investigate the connection of the mean field EnKBF and the stochastic filtering problem. In particular we derive the feedback particle filter for infinite dimensional signals and show that the mean field EnKBF can viewed as its constant gain approximation.},
  archive      = {J_JUQ},
  author       = {Sebastian W. Ertel},
  doi          = {10.1137/24M1658954},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {9},
  number       = {3},
  pages        = {891--930},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On the mean field theory of ensemble kalman filters for SPDEs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical guarantees of group-invariant GANs. <em>JUQ</em>, <em>13</em>(2), 862--890. (<a href='https://doi.org/10.1137/24M1666628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This work presents the first statistical performance guarantees for group-invariant generative models. Many real-world datasets, such as images and molecules, are invariant to certain group symmetries, which can be taken advantage of to learn more efficiently, as we rigorously demonstrate in this work. Here we specifically study generative adversarial networks (GANs) and quantify the gains when incorporating symmetries into the model. Group-invariant GANs are a type in which the generators and discriminators are hardwired with group symmetries. Empirical studies have shown that these networks are capable of learning group-invariant distributions with significantly improved data efficiency. In this study, we aim to rigorously quantify this improvement by analyzing the reduction in sample complexity and in the discriminator approximation error for group-invariant GANs. Our findings indicate that when learning group-invariant distributions, the number of samples required for group-invariant GANs decreases proportionally by a factor of the group size, and the discriminator approximation error has a reduced lower bound. An important point is that the overall error reduction cannot be achieved merely through data augmentation on the training data. Numerical results substantiate our theory and highlight the stark contrast between learning with group-invariant GANs and using data augmentation. This work also sheds light on the study of other generative models with group symmetries, such as score-based generative models.},
  archive      = {J_JUQ},
  author       = {Ziyu Chen and Markos A. Katsoulakis and Luc Rey-Bellet and Wei Zhu},
  doi          = {10.1137/24M1666628},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {862--890},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Statistical guarantees of group-invariant GANs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex-valued signal recovery using a generalized bayesian LASSO. <em>JUQ</em>, <em>13</em>(2), 831--861. (<a href='https://doi.org/10.1137/24M1644778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recovering complex-valued images from noisy indirect data is important in applications such as ultrasound imaging and synthetic aperture radar. While there are many effective algorithms to recover point estimates of the magnitude, fewer are designed to recover the phase. Quantifying uncertainty in the estimate can also provide valuable information for real-time decision making. This investigation therefore proposes a new Bayesian inference method that recovers point estimates while also quantifying the uncertainty for complex-valued signals or images given noisy and indirect observation data. Our method is motivated by the Bayesian LASSO approach for real-valued sparse signals, and here we demonstrate that the Bayesian LASSO can be effectively adapted to recover complex-valued images whose magnitude is sparse in some (e.g., the gradient) domain. Numerical examples demonstrate our algorithm’s robustness to noise as well as its computational efficiency.},
  archive      = {J_JUQ},
  author       = {Dylan Green and Jonathan Lindbloom and Anne Gelb},
  doi          = {10.1137/24M1644778},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {831--861},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Complex-valued signal recovery using a generalized bayesian LASSO},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Antithetic multilevel methods for elliptic and hypoelliptic diffusions with applications. <em>JUQ</em>, <em>13</em>(2), 805--830. (<a href='https://doi.org/10.1137/24M1695178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a new antithetic multilevel Monte Carlo (MLMC) method for the estimation of expectations with respect to laws of diffusion processes that can be elliptic or hypoelliptic. In particular, we consider the case where one has to resort to time discretization of the diffusion and numerical simulation of such schemes. Inspired by recent works, we introduce a new MLMC estimator of expectations, which does not require any Lévy area simulation and has a strong error of order 2 and a weak error of order 2. We then show how this approach can be used in the context of the filtering problem associated with partially observed diffusions with discrete time observations. We illustrate that in numerical simulations our new approaches provide efficiency gains for several problems, particularly when the diffusion process is hypoelliptic, relative to some existing methods.},
  archive      = {J_JUQ},
  author       = {Yuga Iguchi and Ajay Jasra and Mohamed Maama and Alexandros Beskos},
  doi          = {10.1137/24M1695178},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {805--830},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Antithetic multilevel methods for elliptic and hypoelliptic diffusions with applications},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor train based sampling algorithms for approximating regularized wasserstein proximal operators. <em>JUQ</em>, <em>13</em>(2), 775--804. (<a href='https://doi.org/10.1137/24M1633765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a tensor train (TT) based algorithm designed for sampling from a target distribution and employ TT approximation to capture the high-dimensional probability density evolution of overdamped Langevin dynamics. This involves utilizing the regularized Wasserstein proximal operator, which exhibits a simple kernel integration formulation, i.e., the softmax formula of the traditional proximal operator. The integration, performed in , poses a challenge in practical scenarios, making the algorithm practically implementable only with the aid of TT approximation. In the specific context of Gaussian distributions, we rigorously establish the unbiasedness and linear convergence of our sampling algorithm towards the target distribution. To assess the effectiveness of our proposed methods, we apply them to various scenarios, including Gaussian families, Gaussian mixtures, bimodal distributions, and Bayesian inverse problems in numerical examples. The sampling algorithm exhibits superior accuracy and faster convergence when compared to classical Langevin dynamics type sampling algorithms.},
  archive      = {J_JUQ},
  author       = {Fuqun Han and Stanley Osher and Wuchen Li},
  doi          = {10.1137/24M1633765},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {775--804},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Tensor train based sampling algorithms for approximating regularized wasserstein proximal operators},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust A-optimal experimental design for sensor placement in bayesian linear inverse problems. <em>JUQ</em>, <em>13</em>(2), 744--774. (<a href='https://doi.org/10.1137/24M1667543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Optimal design of experiments for Bayesian inverse problems has recently gained wide popularity and attracted much attention, especially in the computational science and Bayesian inversion communities. An optimal design maximizes a predefined utility function that is formulated in terms of the elements of an inverse problem, an example being optimal sensor placement for parameter identification. The state-of-the-art algorithmic approaches following this simple formulation generally overlook misspecification of the elements of the inverse problem, such as the prior or the measurement uncertainties. This work presents an efficient algorithmic approach for designing optimal experimental design schemes for Bayesian linear inverse problems such that the optimal design is robust to misspecification of elements of the inverse problem. Specifically, we consider a worst-case scenario approach for the uncertain or misspecified parameters, formulate robust objectives, and propose an algorithmic approach for optimizing such objectives. Both relaxation and stochastic solution approaches are discussed with detailed analysis and insight into the interpretation of the problem and the proposed algorithmic approach. Extensive numerical experiments to validate and analyze the proposed approach are carried out for sensor placement in a parameter identification problem.},
  archive      = {J_JUQ},
  author       = {Ahmed Attia and Sven Leyffer and Todd S. Munson},
  doi          = {10.1137/24M1667543},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {744--774},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Robust A-optimal experimental design for sensor placement in bayesian linear inverse problems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for model correction of dynamical systems with data scarcity. <em>JUQ</em>, <em>13</em>(2), 718--743. (<a href='https://doi.org/10.1137/24M1704324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a deep learning framework for correcting existing dynamical system models utilizing only a scarce high-fidelity data set. In many practical situations, one has a low-fidelity model that can capture the dynamics reasonably well but lacks high resolution, due to the inherent limitation of the model and the complexity of the underlying physics. When high resolution data become available, it is natural to seek model correction to improve the resolution of the model predictions. We focus on the case when the amount of high-fidelity data is so small that most of the existing data driven modeling methods cannot be applied. In this paper, we address these challenges with a model-correction method which only requires a scarce high-fidelity data set. Our method first seeks a deep neural network (DNN) model to approximate the existing low-fidelity model. By using the scarce high-fidelity data, the method then corrects the DNN model via transfer learning (TL). After TL, an improved DNN model with high prediction accuracy to the underlying dynamics is obtained. One distinct feature of the proposed method is that it does not assume a specific form of the model correction terms. Instead, it offers an inherent correction to the low-fidelity model via TL. A set of numerical examples is presented to demonstrate the effectiveness of the proposed method.},
  archive      = {J_JUQ},
  author       = {Caroline Tatsuoka and Dongbin Xiu},
  doi          = {10.1137/24M1704324},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {718--743},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Deep learning for model correction of dynamical systems with data scarcity},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing scale parameter estimators for gaussian process interpolation with the brownian motion prior: Leave-one-out cross validation and maximum likelihood. <em>JUQ</em>, <em>13</em>(2), 679--717. (<a href='https://doi.org/10.1137/23M1586884'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Gaussian process (GP) regression is a Bayesian nonparametric method for regression and interpolation that offers a principled way of quantifying the uncertainties of predicted function values. For the quantified uncertainties to be well-calibrated, however, the kernel of the GP prior has to be carefully selected. In this paper, we theoretically compare two methods for choosing the kernel in GP regression: cross-validation and maximum likelihood estimation. Focusing on scale parameter estimation of a Brownian motion kernel in the noiseless setting, we prove that cross-validation can yield asymptotically well-calibrated credible intervals for a broader class of ground-truth functions than maximum likelihood estimation, suggesting an advantage of the former over the latter. Finally, motivated by the findings, we propose interior cross-validation, a procedure that adapts to an even broader class of ground-truth functions.},
  archive      = {J_JUQ},
  author       = {Masha Naslidnyk and Motonobu Kanagawa and Toni Karvonen and Maren Mahsereci},
  doi          = {10.1137/23M1586884},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {679--717},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Comparing scale parameter estimators for gaussian process interpolation with the brownian motion prior: Leave-one-out cross validation and maximum likelihood},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A convergent interacting particle method for computing KPP front speeds in random flows. <em>JUQ</em>, <em>13</em>(2), 639--678. (<a href='https://doi.org/10.1137/23M1604564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper aims to efficiently compute the spreading speeds of reaction-diffusion-advection fronts in divergence-free random flows under the Kolmogorov–Petrovsky–Piskunov (KPP) nonlinearity. We develop a stochastic interacting particle method (IPM) for the reduced principal eigenvalue (Lyapunov exponent) problem of an associated linear advection-diffusion operator with spatially random coefficients. The Fourier representation of the random advection field and the Feynman–Kac formula of the principal eigenvalue (Lyapunov exponent) form the foundation of our method, which is implemented as a genetic evolution algorithm. The particles undergo advection-diffusion and mutation/selection through a fitness function that originates in the Feynman–Kac semigroup. We analyze the convergence of the algorithm based on operator splitting and present numerical results on representative flows, such as 2D cellular flow and 3D Arnold–Beltrami–Childress (ABC) flow under random perturbations. The 2D examples serve as a consistency check with semi-Lagrangian computation. The 3D results demonstrate that IPM, being mesh-free and self-adaptive, is easy to implement and efficient for computing front spreading speeds in the advection-dominated regime for high-dimensional random flows on unbounded domains where no truncation is needed.},
  archive      = {J_JUQ},
  author       = {Tan Zhang and Zhongjian Wang and Jack Xin and Zhiwen Zhang},
  doi          = {10.1137/23M1604564},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {639--678},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A convergent interacting particle method for computing KPP front speeds in random flows},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Goal-oriented error estimation and adaptivity for stochastic collocation FEM. <em>JUQ</em>, <em>13</em>(2), 613--638. (<a href='https://doi.org/10.1137/24M1673280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose and analyze a general goal-oriented adaptive strategy for approximating quantities of interest (QoIs) associated with solutions to linear elliptic PDEs with random inputs. The QoIs are represented by bounded linear or continuously Gâteaux differentiable nonlinear goal functionals, and the approximations are computed using the sparse grid stochastic collocation finite element method (SC-FEM). The proposed adaptive strategy relies on novel reliable a posteriori estimates of the errors in approximating QoIs. One of the key features of our error estimation approach is the introduction of a correction term into the approximation of QoIs in order to compensate for the lack of (global) Galerkin orthogonality in the SC-FEM setting. Computational results generated using the proposed adaptive algorithm are presented in the paper for representative elliptic problems with affine and nonaffine parametric coefficient dependence and for a range of linear and nonlinear goal functionals.},
  archive      = {J_JUQ},
  author       = {Alex Bespalov and Dirk Praetorius and Thomas Round and Andrey Savinov},
  doi          = {10.1137/24M1673280},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {613--638},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Goal-oriented error estimation and adaptivity for stochastic collocation FEM},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature calibration for computer models. <em>JUQ</em>, <em>13</em>(2), 591--612. (<a href='https://doi.org/10.1137/24M163253X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Computer model calibration involves using partial and imperfect observations of the real world to learn which values of a model’s input parameters lead to outputs that are consistent with real-world observations. When trying to calibrate to high-dimensional output (e.g., a spatial field), what is important to the credibility of the model is that key emergent physical phenomena are represented, even if not faithfully or in the right place. Commonly used approaches, which represent the output as a linear combination of a small set of basis vectors, often fail to appropriately compare model output and data when the position of key emergent phenomena shifts, consequently leading to poor model calibration. To overcome this, we present kernel-based history matching (KHM), generalizing the meaning of the technique sufficiently to be able to project model outputs and observations into a higher-dimensional feature space, where patterns can be compared without their location necessarily being fixed. We develop the technical methodology, present an expert-driven kernel selection algorithm, and then apply the techniques to the calibration of boundary layer clouds for the French climate model IPSL-CM.},
  archive      = {J_JUQ},
  author       = {Wenzhe Xu and Daniel B. Williamson and Frederic Hourdin and Romain Roehrig},
  doi          = {10.1137/24M163253X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {591--612},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Feature calibration for computer models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate-based global sensitivity analysis with statistical guarantees via floodgate. <em>JUQ</em>, <em>13</em>(2), 563--590. (<a href='https://doi.org/10.1137/24M1718330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Computational models are utilized in many scientific domains to simulate complex systems. Sensitivity analysis is an important practice to aid our understanding of the mechanics of these models, but performing a sufficient number of model evaluations to obtain accurate sensitivity estimates can often be prohibitively expensive. A common solution to reduce the computational burden is to use a surrogate model that approximates the original model reasonably well at a fraction of the cost. However, in exchange for the computational benefits of surrogate-based sensitivity analysis, this approach suffers a loss in accuracy arising from the difference between the surrogate and the original model. To address this issue, we adapt the floodgate method of Zhang and Janson [Floodgate: Inference for Model-Free Variable Importance, arXiv:2007.01283, 2020] to provide valid surrogate-based confidence intervals rather than a point estimate, allowing for the computational benefit of using a surrogate that is especially pronounced for high-dimensional models, while still retaining rigorous and accurate bounds on the global sensitivity with respect to the original (nonsurrogate) model. Our confidence interval is asymptotically valid with almost no conditions on the computational model or the surrogate. Additionally, the accuracy (width) of our confidence interval shrinks as the surrogate’s accuracy increases, so when an accurate surrogate is used, the confidence interval we report will correspondingly be quite narrow, instilling appropriately high confidence in its estimate. We demonstrate the properties of our method through numerical simulations on the small Hymod hydrological model and an application to the more complex CBM-Z meteorological model with a recent neural-network-based surrogate.},
  archive      = {J_JUQ},
  author       = {Massimo Aufiero and Lucas Janson},
  doi          = {10.1137/24M1718330},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {563--590},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Surrogate-based global sensitivity analysis with statistical guarantees via floodgate},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning inducing points and uncertainty on molecular data by scalable variational gaussian processes. <em>JUQ</em>, <em>13</em>(2), 543--562. (<a href='https://doi.org/10.1137/23M1549584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Uncertainty control and scalability to large datasets are two main issues in the deployment of Gaussian process (GP) models within the autonomous machine learning–based prediction pipelines in material science and chemistry. One way to address both of these issues is by introducing the latent inducing point variables and choosing the right approximation for the marginal log-likelihood objective. Here, we empirically show that variational learning of the inducing points in a molecular descriptor space improves the prediction of energies and atomic forces on two molecular dynamics datasets. First, we show that variational GPs can learn to represent the configurations of the molecules of different types that were not present within the initialization set of configurations. We provide a comparison of alternative log-likelihood training objectives and variational distributions. Among several evaluated approximate marginal log-likelihood objectives, we show that predictive log-likelihood provides excellent uncertainty estimates and good predictive quality. Furthermore, we extend our study to fit the atomic forces for a large molecular crystal system, showing that variational GP models perform well for predicting atomic forces by efficiently learning a sparse representation of the dataset.},
  archive      = {J_JUQ},
  author       = {Mikhail Tsitsvero and Mingoo Jin and Andrey Lyalin},
  doi          = {10.1137/23M1549584},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {543--562},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Learning inducing points and uncertainty on molecular data by scalable variational gaussian processes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An inverse source problem for the stochastic multiterm time-fractional diffusion-wave equation. <em>JUQ</em>, <em>13</em>(2), 518--542. (<a href='https://doi.org/10.1137/23M1628334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper concerns both the direct and inverse source problems for the stochastic multiterm time-fractional diffusion-wave equation driven by a fractional Brownian motion. Regarding the direct problem, the well-posedness is established and the regularity of the solution is characterized for the equation. In the inverse problem, the uniqueness and instability are investigated on the determination of the diffusion coefficient in the random source. Furthermore, a reconstruction formula is provided for the phaseless Fourier modes of the diffusion coefficient in the random source, based on the variance of the boundary data. To reconstruct the time-dependent source function from its phaseless Fourier modes, the PhaseLift algorithm, combined with a spectral cut-off regularization technique, is employed to tackle the phase retrieval problem. The effectiveness of the proposed method is demonstrated through numerical experiments.},
  archive      = {J_JUQ},
  author       = {Xiaoli Feng and Qiang Yao and Peijun Li and Xu Wang},
  doi          = {10.1137/23M1628334},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {518--542},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An inverse source problem for the stochastic multiterm time-fractional diffusion-wave equation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse grid approximation of nonlinear SPDEs: The Landau–Lifshitz–Gilbert equation. <em>JUQ</em>, <em>13</em>(2), 472--517. (<a href='https://doi.org/10.1137/24M1646054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We show convergence rates for a sparse grid approximation of the distribution of solutions of the stochastic Landau–Lifshitz–Gilbert (LLG) equation. Beyond being a frequently studied equation in engineering and physics, the stochastic LLG equation poses many interesting challenges that do not appear simultaneously in previous works on uncertainty quantification: The equation is strongly nonlinear and time-dependent and has a nonconvex side constraint. Moreover, the parametrization of the stochastic noise features countably many unbounded parameters and low regularity compared to other elliptic and parabolic problems studied in uncertainty quantification. We use a novel technique to establish uniform holomorphic regularity of the parameter-to-solution map based on a Gronwall-type estimate and the implicit function theorem. This method is very general and is based on a set of abstract assumptions. Thus, it can be applied beyond the LLG equation as well. We demonstrate numerically the feasibility of approximating with sparse grid and show a clear advantage of a multilevel sparse grid scheme.},
  archive      = {J_JUQ},
  author       = {Xin An and Josef Dick and Michael Feischl and Andrea Scaglioni and Thanh Tran},
  doi          = {10.1137/24M1646054},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {472--517},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sparse grid approximation of nonlinear SPDEs: The Landau–Lifshitz–Gilbert equation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing inverse scattering with surrogate modeling and bayesian inference for functional inputs. <em>JUQ</em>, <em>13</em>(2), 449--471. (<a href='https://doi.org/10.1137/24M1637295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Inverse scattering aims to infer information about a hidden object by using the received scattered waves and training data collected from forward mathematical models. Recent advances in computing have led to increasing attention towards functional inverse inference, which can reveal more detailed properties of a hidden object. However, rigorous studies on the functional inverse, including the reconstruction of the functional input and quantification of uncertainty, remain scarce. Motivated by an inverse scattering problem where the objective is to infer the functional input representing the refractive index of a bounded scatterer, a new Bayesian framework is proposed. It contains a surrogate model that takes into account the functional inputs directly through kernel functions, and a Bayesian procedure that infers functional inputs through the posterior distribution. Furthermore, the proposed Bayesian framework is extended to reconstruct the functional inverse by integrating multi-fidelity simulations, including a high-fidelity simulator solved by finite element methods and a low-fidelity simulator called the Born approximation. When compared with existing alternatives developed by finite basis expansion, the proposed method provides more accurate functional recoveries with smaller prediction variations.},
  archive      = {J_JUQ},
  author       = {Chih-Li Sung and Yao Song and Ying Hung},
  doi          = {10.1137/24M1637295},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {449--471},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Advancing inverse scattering with surrogate modeling and bayesian inference for functional inputs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for conservation law McKean–Vlasov SDEs via deep neural networks. <em>JUQ</em>, <em>13</em>(2), 425--448. (<a href='https://doi.org/10.1137/24M1668949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This work addresses statistical inference for a flux function based on observations from a nonlinear McKean–Vlasov process governed by a conservation law. This process is nonlinear, as the drift term of the corresponding SDE depends on the process’s law through its distribution function. We propose a novel maximum likelihood estimation (MLE)–type approach for estimating the invariant density and the flux function of this nonlinear process using deep neural networks. We derive convergence rates for the relative entropy distance between the estimated invariant density and its true value, which imply rates for the flux function estimate. Technically, our paper is among the first in the literature to address MLE of noncompact densities using deep neural networks. We also consider observations from the corresponding particle system and present some numerical results.},
  archive      = {J_JUQ},
  author       = {Denis Belomestny and Tatiana Orlova},
  doi          = {10.1137/24M1668949},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {425--448},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Statistical inference for conservation law McKean–Vlasov SDEs via deep neural networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An energy-based model approach to rare event probability estimation. <em>JUQ</em>, <em>13</em>(2), 400--424. (<a href='https://doi.org/10.1137/23M1605065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The estimation of rare event probabilities plays a pivotal role in diverse fields. Our aim is to determine the probability of a hazard or system failure occurring when a quantity of interest exceeds a critical value. In our approach, the distribution of the quantity of interest is represented by an energy density, characterized by a free energy function. To efficiently estimate the free energy, a bias potential is introduced. Using concepts from energy-based models (EBMs), this bias potential is optimized such that the corresponding probability density function approximates a predefined distribution targeting the failure region of interest. Given the optimal bias potential, the free energy function and the rare event probability of interest can be determined. The approach is applicable not just in traditional rare event settings, where the variable upon which the quantity of interest relies has a known distribution, but also in inversion settings, where the variable follows a posterior distribution. By combining the EBM approach with a Stein discrepancy–based stopping criterion, we aim for a balanced accuracy-efficiency trade-off. Furthermore, we explore both parametric and nonparametric approaches for the bias potential, with the latter eliminating the need for choosing a particular parameterization but depending strongly on the accuracy of the kernel density estimate used in the optimization process. Through three illustrative test cases encompassing both traditional and inversion settings, we show that the proposed EBM approach, when properly configured, (i) allows stable and efficient estimation of rare event probabilities and (ii) compares favorably against subset sampling approaches.},
  archive      = {J_JUQ},
  author       = {Lea Friedli and David Ginsbourger and Arnaud Doucet and Niklas Linde},
  doi          = {10.1137/23M1605065},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {400--424},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An energy-based model approach to rare event probability estimation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel markov chain monte carlo with likelihood scaling for bayesian inversion with high-resolution observations. <em>JUQ</em>, <em>13</em>(2), 375--399. (<a href='https://doi.org/10.1137/24M1634333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a multilevel Markov chain Monte Carlo (MCMC) method for the Bayesian inference of random field parameters in PDEs using high-resolution data. Compared to existing multilevel MCMC methods, we additionally consider level-dependent data resolution and introduce a suitable likelihood scaling to enable consistent cross-level comparisons. We theoretically show that this approach attains the same convergence rates as when using level-independent treatment of data, but at significantly reduced computational cost. The convergence analysis focuses on Lipschitz continuous transformations of Gaussian random fields with Matérn covariance structure. These results are illustrated using numerical experiments for a two-dimensional plane stress problem, where Young’s modulus is estimated from discretizations of the displacement field.},
  archive      = {J_JUQ},
  author       = {P. Vanmechelen and G. Lombaert and G. Samaey},
  doi          = {10.1137/24M1634333},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {375--399},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multilevel markov chain monte carlo with likelihood scaling for bayesian inversion with high-resolution observations},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive uncertainty quantification for stochastic hyperbolic conservation laws. <em>JUQ</em>, <em>13</em>(2), 339--374. (<a href='https://doi.org/10.1137/23M1624750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a predictor-corrector adaptive method for the study of hyperbolic partial differential equations (PDEs) under uncertainty. Constructed around the framework of stochastic finite volume (SFV) methods, our approach circumvents sampling schemes or simulation ensembles while also preserving fundamental properties, in particular hyperbolicity of the resulting systems and conservation of the discrete solutions. Furthermore, we augment the existing SFV theory with a priori convergence results for statistical quantities, in particular push-forward densities, which we demonstrate through numerical experiments. By linking refinement indicators to regions of the physical and stochastic spaces, we drive anisotropic refinements of the discretizations, introducing new degrees of freedom where deemed profitable. To illustrate our proposed method, we consider a series of numerical examples for nonlinear hyperbolic PDEs based on Burgers’ and Euler’s equations.},
  archive      = {J_JUQ},
  author       = {Jake J. Harmon and Svetlana Tokareva and Anatoly Zlotnik and Pieter J. Swart},
  doi          = {10.1137/23M1624750},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {6},
  number       = {2},
  pages        = {339--374},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Adaptive uncertainty quantification for stochastic hyperbolic conservation laws},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional optimal transport on function spaces. <em>JUQ</em>, <em>13</em>(1), 304--338. (<a href='https://doi.org/10.1137/23M1618922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a systematic study of conditional triangular transport maps in function spaces from the perspective of optimal transportation and with a view towards amortized Bayesian inference. More specifically, we develop a theory of constrained optimal transport problems that describe block-triangular Monge maps that characterize conditional measures along with their Kantorovich relaxations. This work generalizes the theory of optimal triangular transport to separable infinite dimensional function spaces with general cost functions. We further tailor our results to the case of Bayesian inference problems and obtain regularity estimates on the conditioning maps from the prior to the posterior. Finally, we present numerical experiments that demonstrate the computational applicability of our theoretical results for amortized and likelihood-free inference of functional parameters.},
  archive      = {J_JUQ},
  author       = {Bamdad Hosseini and Alexander W. Hsu and Amirhossein Taghvaei},
  doi          = {10.1137/23M1618922},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {304--338},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Conditional optimal transport on function spaces},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMC and underdamped langevin united in the unadjusted convex smooth case. <em>JUQ</em>, <em>13</em>(1), 278--303. (<a href='https://doi.org/10.1137/23M1608963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider a family of unadjusted generalized HMC samplers, which includes standard position HMC samplers and discretizations of the underdamped Langevin process. A detailed analysis and optimization of the parameters is conducted in the Gaussian case, which shows an improvement from to for the convergence rate in terms of the condition number by using partial velocity refreshment, with respect to classical full refreshments. A similar effect is observed empirically for two related algorithms, namely Metropolis-adjusted gHMC and kinetic piecewise-deterministic Markov processes. Then, a stochastic gradient version of the samplers is considered, for which dimension-free convergence rates are established for log-concave smooth targets over a large range of parameters, gathering in a unified framework previous results on position HMC and underdamped Langevin and extending them to HMC with inertia.},
  archive      = {J_JUQ},
  author       = {Nicolaï Gouraud and Pierre Le Bris and Adrien Majka and Pierre Monmarché},
  doi          = {10.1137/23M1608963},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {278--303},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {HMC and underdamped langevin united in the unadjusted convex smooth case},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entropy-based burn-in time analysis and ranking for (A)MCMC algorithms in high dimension. <em>JUQ</em>, <em>13</em>(1), 251--277. (<a href='https://doi.org/10.1137/23M1611932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Many recent and often (adaptive) Markov chain Monte Carlo (A)MCMC methods are associated in practice to unknown rates of convergence. We propose a simulation-based methodology to estimate and compare MCMC’s performance in terms of shortest burn-in time, using a Kullback divergence criterion requiring an estimate of the entropy of the algorithm densities at each iteration, computed from i.i.d. simulated chains. In previous works, we proved some consistency results in an MCMC setup for an entropy estimate based on Monte Carlo integration of a kernel density estimate proposed in [L. Györfi and E. C. Van Der Meulen, An entropy estimate based on a kernel density estimation, in Limit Theorems in Probability and Statistics, North-Holland, Amsterdam, 1989, pp. 229–240], and we investigate an alternative nearest neighbor (NN) entropy estimate from [L. Kozachenko and N. N. Leonenko, Problemy Peredachi Informatsii, 23 (1987), pp. 9–16]. This estimate has been used mostly in univariate situations until recently, when entropy estimation in higher dimensions has been considered in other fields like neuroscience or system biology. Unfortunately, in higher dimensions, both estimators converge slowly with a noticeable bias. The present work goes several steps further, with bias reduction and automatic (A)MCMC burn-in time analysis in mind. First, for bias reduction, we apply in our situation a “crossed NN-type” nonparametric estimate of the Kullback divergence between two densities, based on i.i.d. samples from each, introduced by [Q. Wang, S. R. Kulkarni, and S. Verdú, A nearest-neighbor approach to estimating divergence between continuous random vectors, in IEEE International Symposium on Information Theory, Seattle, WA, 2006, pp. 242–246], [Q. Wang, S. R. Kulkarni, and S. Verdú, IEEE Trans. Inform. Theory, 55 (2009), pp. 2392–2405]. We prove the consistency of these entropy estimates under recent uniform control conditions, for the successive densities of a generic class of MCMC algorithm to which most of the methods proposed in the recent literature belong. Second, we propose an original solution based on a PCA for reducing relevant dimension and bias in even higher dimensions whenever PCA is efficient. Our algorithms for MCMC simulation and entropy estimation are progressively added to the R package EntropyMCMC, taking advantage of recent advances in high-performance (parallel) computing.},
  archive      = {J_JUQ},
  author       = {Didier Chauveau and Pierre Vandekerkhove},
  doi          = {10.1137/23M1611932},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {251--277},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Entropy-based burn-in time analysis and ranking for (A)MCMC algorithms in high dimension},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective optimization using expected quantile improvement for decision making in disease outbreaks. <em>JUQ</em>, <em>13</em>(1), 228--250. (<a href='https://doi.org/10.1137/24M1633625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Optimization under uncertainty is important in many applications, particularly to inform policy and decision making in areas such as public health. A key source of uncertainty arises from the incorporation of environmental variables as inputs into computational models or simulators. Such variables represent uncontrollable features of the optimization problem, and reliable decision making must account for the uncertainty they propagate to the simulator outputs. Often, multiple, competing objectives are defined from these outputs such that the final optimal decision is a compromise between different goals. Here, we present emulation-based optimization methodology for such problems that extends expected quantile improvement (EQI) to address multiobjective optimization. Focusing on the practically important case of two objectives, we use a sequential design strategy to identify the Pareto front of optimal solutions. Uncertainty from the environmental variables is integrated out using Monte Carlo samples from the simulator. Interrogation of the expected output from the simulator is facilitated by use of (Gaussian process) emulators. The methodology is demonstrated on an optimization problem from public health involving the dispersion of anthrax spores across a spatial terrain. Environmental variables include meteorological features that impact the dispersion, and the methodology identifies the Pareto front even when there is considerable input uncertainty.},
  archive      = {J_JUQ},
  author       = {Daria Semochkina and Alexander I. J. Forrester and David C. Woods},
  doi          = {10.1137/24M1633625},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {228--250},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multiobjective optimization using expected quantile improvement for decision making in disease outbreaks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elastic bayesian model calibration. <em>JUQ</em>, <em>13</em>(1), 195--227. (<a href='https://doi.org/10.1137/24M1644092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Functional data are ubiquitous in scientific modeling. For instance, quantities of interest are modeled as functions of time, space, energy, density, etc. Uncertainty quantification methods for computer models with functional response have resulted in tools for emulation, sensitivity analysis, and calibration that are widely used. However, many of these tools do not perform well when the computer model’s parameters control both the amplitude variation of the functional output and its alignment (or phase variation). This paper introduces a framework for Bayesian model calibration when the model responses are misaligned functional data. The approach generates two types of data out of the misaligned functional responses: (1) aligned functions so that the amplitude variation is isolated and (2) warping functions that isolate the phase variation. These two types of data are created for the computer simulation data (both of which may be emulated) and the experimental data. The calibration approach uses both types so that it seeks to match both the amplitude and phase of the experimental data. The framework is careful to respect constraints that arise, especially when modeling phase variation, and is framed in a way that it can be done with readily available calibration software. We demonstrate the techniques on two simulated data examples and on two dynamic material science problems: a strength model calibration using flyer plate experiments and an equation of state model calibration using experiments performed on the Sandia National Laboratories’ Z-machine.},
  archive      = {J_JUQ},
  author       = {Devin Francom and J. Derek Tucker and Gabriel Huerta and Kurtis Shuler and Daniel Ries},
  doi          = {10.1137/24M1644092},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {195--227},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Elastic bayesian model calibration},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Worst-case learning under a multifidelity model. <em>JUQ</em>, <em>13</em>(1), 171--194. (<a href='https://doi.org/10.1137/24M1671025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Inspired by multifidelity methods in computer simulations, this article introduces procedures for designing surrogates for the input/output relationship of a high-fidelity code. These surrogates should be learned from runs of both the high-fidelity and low-fidelity codes and be accompanied by error guarantees that are deterministic rather than stochastic. For this purpose, the article advocates a framework tied to a theory focusing on worst-case guarantees, namely optimal recovery. The multifidelity considerations triggered new theoretical results in three scenarios: the globally optimal estimation of linear functionals, the globally optimal approximation of arbitrary quantities of interest in Hilbert spaces, and their locally optimal approximation, still within Hilbert spaces. The latter scenario boils down to the determination of the Chebyshev center for the intersection of two hyperellipsoids. It is worth noting that the mathematical framework presented here, together with its possible extension, seems to be relevant in several other contexts briefly discussed.},
  archive      = {J_JUQ},
  author       = {Simon Foucart and Nicolas Hengartner},
  doi          = {10.1137/24M1671025},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {171--194},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Worst-case learning under a multifidelity model},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive hierarchical ensemble kalman filter with reduced basis models. <em>JUQ</em>, <em>13</em>(1), 140--170. (<a href='https://doi.org/10.1137/24M1653690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The use of reduced order modeling techniques in combination with ensemble-based methods for estimating the state of systems described by nonlinear partial differential equations has been of great interest in recent years in the data assimilation community. Methods such as the multifidelity ensemble Kalman filter and the multilevel ensemble Kalman filter are recognized as state-of-the-art techniques. However, in many cases, the construction of low-fidelity models in an offline stage, before solving the data assimilation problem, prevents them from being both accurate and computationally efficient. In our work, we investigate the use of adaptive reduced basis techniques in which the approximation space is modified online by combining information extracted from a limited number of full order solutions and information extracted from reduced models trained at previous time steps. This allows us to simultaneously ensure good accuracy and low cost for the employed models and thus improve the performance of the multifidelity and multilevel methods.},
  archive      = {J_JUQ},
  author       = {Francesco A. B. Silva and Cecilia Pagliantini and Karen Veroy},
  doi          = {10.1137/24M1653690},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {140--170},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An adaptive hierarchical ensemble kalman filter with reduced basis models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable method for bayesian experimental design without integrating over posterior distribution. <em>JUQ</em>, <em>13</em>(1), 114--139. (<a href='https://doi.org/10.1137/23M1603364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We address the computational efficiency of finding the A-optimal Bayesian experimental design, where the observation map is based on partial differential equations and thus computationally expensive to evaluate. A-optimality is a widely used and easily interpreted criterion, that seeks the optimal experimental design by minimizing the expected conditional variance. Our study presents a novel likelihood-free approach to the A-optimal experimental design that does not require sampling or integration over the Bayesian posterior distribution. In our proposed approach, we estimate the expected conditional variance via the variance of the conditional expectation and approximate the conditional expectation using its orthogonal projection property. We derive an asymptotic error estimate for the proposed estimator of the expected conditional variance and verify it with numerical experiments. Furthermore, we extend our approach to the case where the domain of the experimental design parameters is continuous. Specifically, we propose a nonlocal approximation of the conditional expectation using an artificial neural network and apply transfer learning and data augmentation to reduce the number of evaluations of the measurement model. Through numerical experiments, we demonstrate that our method greatly reduces the number of measurement model evaluations compared with widely used importance sampling-based approaches. Code is available at https://github.com/vinh-tr-hoang/DOEviaPACE.},
  archive      = {J_JUQ},
  author       = {Vinh Hoang and Luis Espath and Sebastian Krumscheid and Raúl Tempone},
  doi          = {10.1137/23M1603364},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {114--139},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Scalable method for bayesian experimental design without integrating over posterior distribution},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification in machine learning based segmentation: A post-hoc approach for left ventricle volume estimation in MRI. <em>JUQ</em>, <em>13</em>(1), 90--113. (<a href='https://doi.org/10.1137/23M161433X'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Recent studies have confirmed cardiovascular diseases remain responsible for the highest mortality rate among noncommunicable diseases. The accurate left ventricular (LV) volume estimation is critical for valid diagnosis and management of various cardiovascular conditions, but poses a significant challenge due to inherent uncertainties associated with the segmentation algorithms in magnetic resonance imaging. Recent machine learning advancements, particularly U-Net-like convolutional networks, have facilitated automated segmentation for medical images, but struggles under certain pathologies and/or different scanner vendors and imaging protocols. This study proposes a novel methodology for post-hoc uncertainty estimation in the LV volume prediction using Itô stochastic differential equations to model pathwise behavior for the prediction error. The model describes the area of the left ventricle along the heart’s long axis. The method is agnostic to the underlying segmentation algorithm, facilitating its use with various existing and future segmentation technologies. The proposed approach provides a mechanism for quantifying uncertainty, enabling medical professionals to intervene for unreliable predictions. This is of utmost importance in critical applications such as medical diagnosis, where prediction accuracy and reliability can directly impact patient outcomes. The method is also robust to dataset changes, enabling application for medical centers with limited access to labeled data. Our findings highlight the proposed uncertainty estimation methodology’s potential to enhance automated segmentation robustness and generalizability, paving the way for more reliable and accurate LV volume estimation in clinical settings as well as opening new avenues for uncertainty quantification in biomedical image segmentation, providing promising directions for future research.},
  archive      = {J_JUQ},
  author       = {Felix Terhag and Philipp Knechtges and Achim Basermann and Raúl Tempone},
  doi          = {10.1137/23M161433X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {90--113},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification in machine learning based segmentation: A post-hoc approach for left ventricle volume estimation in MRI},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equispaced fourier representations for efficient gaussian process regression from a billion data points. <em>JUQ</em>, <em>13</em>(1), 63--89. (<a href='https://doi.org/10.1137/23M1565310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a Fourier-based fast algorithm for Gaussian process regression in low dimensions. It approximates a translationally invariant covariance kernel by complex exponentials on an equispaced Cartesian frequency grid of nodes. This results in a weight-space system matrix with Toeplitz structure, which can thus be applied to a vector in operations via the fast Fourier transform (FFT), independent of the number of data points . The linear system can be set up in operations using nonuniform FFTs. This enables efficient massive-scale regression via an iterative solver, even for kernels with fat-tailed spectral densities (large ). We provide bounds on both kernel approximation and posterior mean errors. Numerical experiments for squared-exponential and Matérn kernels in one, two, and three dimensions often show 1–2 orders of magnitude acceleration over state-of-the-art rank-structured solvers at comparable accuracy. Our method allows two-dimensional Matérn- regression from data points to be performed in two minutes on a standard desktop, with posterior mean accuracy . This opens up spatial statistics applications 100 times larger than previously possible.},
  archive      = {J_JUQ},
  author       = {Philip Greengard and Manas Rachh and Alex H. Barnett},
  doi          = {10.1137/23M1565310},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {63--89},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Equispaced fourier representations for efficient gaussian process regression from a billion data points},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sampling low-fidelity outputs for estimation of high-fidelity density and its tails. <em>JUQ</em>, <em>13</em>(1), 30--62. (<a href='https://doi.org/10.1137/24M1639142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In a multifidelity setting, data are available under the same conditions from two (or more) sources, e.g., computer codes, one being lower-fidelity but computationally cheaper, and the other higher-fidelity and more expensive. This paper studies for which low-fidelity outputs one should obtain high-fidelity outputs if the goal is to estimate the probability density function of the latter, especially when it comes to the distribution tails and extremes. It is suggested to approach this problem from the perspective of the importance sampling of low-fidelity outputs according to some proposal distribution, combined with special considerations for the distribution tails based on extreme value theory. The notion of an optimal proposal distribution is introduced and investigated, in both theory and simulations. The approach is motivated and illustrated with an application to estimate the probability density function of record extremes of ship motions, obtained through two computer codes of different fidelities.},
  archive      = {J_JUQ},
  author       = {Minji Kim and Kevin O’Connor and Vladas Pipiras and Themistoklis Sapsis},
  doi          = {10.1137/24M1639142},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {30--62},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sampling low-fidelity outputs for estimation of high-fidelity density and its tails},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polynomial chaos surrogate construction for random fields with parametric uncertainty. <em>JUQ</em>, <em>13</em>(1), 1--29. (<a href='https://doi.org/10.1137/23M1613505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Engineering and applied science rely on computational experiments to rigorously study physical systems. The mathematical models used to probe these systems are highly complex, and sampling-intensive studies often require prohibitively many simulations for acceptable accuracy. Surrogate models provide a means of circumventing the high computational expense of sampling such complex models. In particular, polynomial chaos expansions (PCEs) have been successfully used for uncertainty quantification studies of deterministic models where the dominant source of uncertainty is parametric. We discuss an extension to conventional PCE surrogate modeling to enable surrogate construction for stochastic computational models that have intrinsic noise in addition to parametric uncertainty. We develop a PCE surrogate on a joint space of intrinsic and parametric uncertainty, enabled by Rosenblatt transformations, which are evaluated via kernel density estimation of the associated conditional cumulative distributions. Furthermore, we extend the construction to random field data via the Karhunen–Loève expansion. We then take advantage of closed-form solutions for computing PCE Sobol indices to perform a global sensitivity analysis of the model which quantifies the intrinsic noise contribution to the overall model output variance. Additionally, the resulting joint PCE is generative in the sense that it allows generating random realizations at any input parameter setting that are statistically approximately equivalent to realizations from the underlying stochastic model. The method is demonstrated on a chemical catalysis example model and a synthetic example controlled by a parameter that enables a switch from unimodal to bimodal response distributions.},
  archive      = {J_JUQ},
  author       = {Joy N. Mueller and Khachik Sargsyan and Craig J. Daniels and Habib N. Najm},
  doi          = {10.1137/23M1613505},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  month        = {3},
  number       = {1},
  pages        = {1--29},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Polynomial chaos surrogate construction for random fields with parametric uncertainty},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
