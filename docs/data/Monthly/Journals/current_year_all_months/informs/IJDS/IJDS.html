<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijds">IJDS - 18</h2>
<ul>
<li><details>
<summary>
(2025). Call for Papers—INFORMS journal on data science virtual special issue on the dual edge of AI: Catalyzing and challenging the future of energy systems. <em>IJDS</em>, <em>4</em>(3), iii-iv. (<a href='https://doi.org/10.1287/ijds.2026.cfp.v05.n1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2026.cfp.v05.n1},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {iii-iv},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Call for Papers—INFORMS journal on data science virtual special issue on the dual edge of AI: Catalyzing and challenging the future of energy systems},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based feature selection method under budget constraint for multiclass classification problems. <em>IJDS</em>, <em>4</em>(3), 265-282. (<a href='https://doi.org/10.1287/ijds.2024.0050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel graph-based method for budget-constrained feature selection (GB-BC-FS) in multiclass classification problems. The method identifies a subset of features that complement each other’s ability to distinguish between different classes, thereby utilizing the entire feature space while maintaining the model’s predictive performance and adhering to budget constraints on feature costs. This is achieved through an intuitive heuristic based on a scoring function, allowing users to calibrate the solution provided by GB-BC-FS. The calibration prioritizes selecting features with complementary qualities while minimizing the costs associated with feature collection, under constraint compliance. The approach is designed to handle practical limitations, making it suitable for applications where resources like cost and time are constrained. This not only improves computational efficiency but also aligns with broader implications related to optimizing resource utilization and ensuring practical applicability in data-driven industries. The effectiveness of GB-BC-FS was validated through extensive experimental analysis, including two comprehensive experiments with a real case study. These experiments demonstrated that GB-BC-FS significantly outperforms existing state-of-the-art approaches, achieving an average accuracy improvement of 10.4% and saving an average of 85.17% in run time compared with finding the optimal set of features, all while adhering to budget limits. Our code is fully documented and available online at https://github.com/davidlevinwork/gbfs/ . Funding: This work was supported by the Israeli Ministry of Innovation, Science and Technology [Grant 0004323]. Data Ethics & Reproducibility Note: The code capsule is available at https://github.com/davidlevinwork/gbfs/ and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2024.0050 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2024.0050},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {265-282},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Graph-based feature selection method under budget constraint for multiclass classification problems},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic neighbourhood components analysis. <em>IJDS</em>, <em>4</em>(3), 248-264. (<a href='https://doi.org/10.1287/ijds.2023.0018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance metric learning is a fundamental task in data mining and is known to enhance the performance of various distance-based algorithms. In this paper, we consider stochastic training data in which repeated feature vectors can belong to different classes, a scenario in which existing methods of metric learning are known to struggle. This type of data is common in stochastic simulations, where multidimensional, recurrent system states are subject to inherent randomness. Classification models on such high-resolution simulation-generated data play a critical role in real-time decision making across diverse applications. This paper presents and implements a stochastic version of the popular neighbourhood components analysis. We demonstrate its behaviour on stochastic data using simulation models and reveal its advantages when used for nearest neighbour classification. Meanwhile, the assumptions of stochastic labelling and repeated feature vectors extend to data from various domains, suggesting that the method can attain broad impact. For example, beyond its applications to system control and decision making with digital twin simulation, it may enhance the analysis of data from sensor networks, recommender systems, and crowdsourced platforms, where stochasticity and recurring feature patterns are typical. Funding: This work was supported by the Engineering and Physical Sciences Research Council–funded STOR-i Centre for Doctoral Training at Lancaster University [Grant EP/L015692/1]. In addition, Barry L. Nelson’s work was supported by the U.S. National Science Foundation [Grant DMS-1854562]. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.0189724.v5 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0018 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0018},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {248-264},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Stochastic neighbourhood components analysis},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating hidden epidemic: A bayesian spatiotemporal compartmental modeling approach. <em>IJDS</em>, <em>4</em>(3), 230-247. (<a href='https://doi.org/10.1287/ijds.2023.0020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efforts to mitigate public health crises have been complicated by unreported cases and the ever-changing trends of those monitored health events across geographic regions and socioeconomic cultures. To resolve both challenges, we propose a Bayesian spatiotemporal susceptible-exposed-infected-recovered-removed (BayST-SEIRD) framework that builds the hidden effects of neighboring communities, local features, and the reporting rates into its transmission mechanism. To alleviate the computational burdens embedded in a fully Bayesian algorithm, we propose an alternating approach that learns the compartmental structure and the spatial effects separately. With a simulation study, we show that this algorithm can accurately retrieve our designed system. Then, we apply BayST-SEIRD to model the coronavirus disease 2019 (COVID-19) dynamics in the metropolitan Atlanta area. We observe that most counties’ reporting rates were below 10% of the projected total infected population and that age and educational level are negatively correlated with the exposing rate, suggesting the needs for stronger incentives for COVID-19 testing and quarantine among the younger population. Importantly, BayST-SEIRD facilitates the reconstruction of actual case counts of the monitored subject among neighboring communities, which is critical to designing impactful public health policy interventions. Funding: This research was supported by the National Center for Advancing Translational Sciences of the National Institutes of Health under Award Number UL1TR002378. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The work of K. Paynabar was partially supported by the Fouts Family Chair. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6447675/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0020 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0020},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {230-247},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Estimating hidden epidemic: A bayesian spatiotemporal compartmental modeling approach},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Observational vs. experimental data when making automated decisions using machine learning. <em>IJDS</em>, <em>4</em>(3), 197-229. (<a href='https://doi.org/10.1287/ijds.2023.0012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decisions supported by machine learning often aim to improve outcomes through interventions, such as influencing purchasing behavior with ads or increasing customer retention with special offers. However, using observational data to estimate these effects can introduce confounding bias. Although experimental data can mitigate confounding, it is not always feasible to obtain and can be costly when it is. This paper presents theoretical results focusing on the impact of confounding on decision making, emphasizing that optimizing decisions often involves determining whether a causal effect exceeds a threshold rather than minimizing bias in the estimate. Consequently, models built with readily available but confounded data can sometimes yield decisions as good as or better than those based on costly, unconfounded data. This can occur when larger effects are more likely to be overestimated or when the benefits of larger, cheaper data sets outweigh the drawbacks of confounding. We validate the theoretical findings using benchmark data from the 2016 Atlantic Causal Inference Conference causal modeling competition, encompassing 77 scenarios and 7,700 data sets. We then introduce theoretical conditions, weaker than ignorability, that characterize when confounding preserves effect rankings. These conditions allow for empirical heuristic tests to assess whether observational data aligns with this structure. Finally, we apply our findings in a large-scale case study using advertising data, demonstrating how these insights can guide decision making in practice. Funding: This research, including Yanfang Hou’s contributions, was supported by the Research Grants Council [Grant 26500822]. The authors thank Ira Rennert and the New York University/Stern Fubon Center for support. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.6587526.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0012 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0012},
  journal      = {INFORMS Journal on Data Science},
  month        = {7-9},
  number       = {3},
  pages        = {197-229},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Observational vs. experimental data when making automated decisions using machine learning},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Call for Papers—INFORMS journal on data science virtual special issue on data science for digital twin technologies. <em>IJDS</em>, <em>4</em>(2), v-vi. (<a href='https://doi.org/10.1287/ijds.2025.cfp.v04.n2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2025.cfp.v04.n2},
  journal      = {INFORMS Journal on Data Science},
  month        = {4-6},
  number       = {2},
  pages        = {v-vi},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Call for Papers—INFORMS journal on data science virtual special issue on data science for digital twin technologies},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Call for Papers—INFORMS journal on data science virtual special issue on generative AI, foundation models, and deep learning with applications to business analytics. <em>IJDS</em>, <em>4</em>(2), iii-iv. (<a href='https://doi.org/10.1287/ijds.2025.cfp.v03.n3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2025.cfp.v03.n3},
  journal      = {INFORMS Journal on Data Science},
  month        = {4-6},
  number       = {2},
  pages        = {iii-iv},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Call for Papers—INFORMS journal on data science virtual special issue on generative AI, foundation models, and deep learning with applications to business analytics},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solar radiation ramping events modeling using spatio-temporal point processes. <em>IJDS</em>, <em>4</em>(2), 173-196. (<a href='https://doi.org/10.1287/ijds.2023.0006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate modeling and prediction of solar ramping events are critical for enhancing the situational awareness of solar power generation systems. The impact of weather conditions, including temperature, humidity, and cloud density, on the emergence and position of solar ramping events is well acknowledged. In addition, abnormal ramping events are typically strongly correlated in space and time, posing a challenge for modeling these events with complex spatio-temporal correlations. To address this challenge, we propose a novel spatio-temporal categorical point process model that effectively addresses the correlation and interaction among ramping events. Through extensive real-data experiments, we demonstrate the interpretability and predictive power of our model. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: C. Xu, M. Zhang, and Y. Xie were partially supported by the National Science Foundation (NSF) [Grants CCF-1650913, DMS-1938106, DMS-1830210, and CMMI-2015787]. Additional support from the NSF [Grants DMS-2134037 and CMMI-2112533] is gratefully acknowledged. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/7597817/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0006 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0006},
  journal      = {INFORMS Journal on Data Science},
  month        = {4-6},
  number       = {2},
  pages        = {173-196},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Solar radiation ramping events modeling using spatio-temporal point processes},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clustering and representative selection for high-dimensional data with human-in-the-loop. <em>IJDS</em>, <em>4</em>(2), 154-172. (<a href='https://doi.org/10.1287/ijds.2022.9014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel decision-making procedure called human-in-the-loop clustering and representative selection (HITL-CARS) that involves users’ domain knowledge for analyzing high-dimensional data sets. The proposed method simultaneously clusters strongly correlated variables and estimates a linear regression model with only a few selected variables from cluster representatives and independent variables. In this work, we model the CARS procedure as a mixed-integer programming problem on the basis of penalized likelihood and partition around medoids clustering. After users obtain analysis results from CARS and provide their advice based on their domain knowledge, HITL-CARS refines analyses for accounting users’ inputs. Simulation studies show that the one-stage CARS performs better than the two-stage group Lasso and clustering representative Lasso in metrics such as true-positive, false-positive, exchangeable representative selection, and so on. Additionally, sensitivity and parameter misspecification studies present the robustness of the CARS to different preset parameters and provide guidance on how to start and adjust the HILT-CARS procedure. A real-life example of brain mapping data shows that HITL-CARS could aid in discovering important brain regions associated with depression symptoms and provide predictive analytics on cluster representatives. History: Olivia Sheng served as the senior editor for this article. Funding: S.-T. Yang and J.-C. Lu were partially supported by Lu’s 2023-24 Jim Pope Fellowship through The James G. and Dee H. Pope Faculty Fellows Endowment Fund at Georgia Institute of Technology. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.0310071.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.9014 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.9014},
  journal      = {INFORMS Journal on Data Science},
  month        = {4-6},
  number       = {2},
  pages        = {154-172},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Clustering and representative selection for high-dimensional data with human-in-the-loop},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting multiple changepoints by exploiting their spatiotemporal correlations: A bayesian hierarchical approach. <em>IJDS</em>, <em>4</em>(2), 133-153. (<a href='https://doi.org/10.1287/ijds.2024.0030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing the nonstationarity of spatiotemporal data over time via changepoints has received increasing attention in various research fields. Although extensive studies have been conducted to investigate changepoint detection with spatiotemporal data, research on detecting multiple clusters of spatiotemporally correlated changepoints has remained unexplored. In this paper, we propose a multilayer Bayesian hierarchical model: The first layer uncovers the spatiotemporal correlations of changepoints based on multiple propagation binary variables, which describe the occurrences of change propagations. The second and third layers compose nonhomogeneous hidden Markov models to capture time series data and their state sequences, in which changes of states signify changepoints. We perform Bayesian inference for changepoints and change propagations via a forward-backward algorithm that combines recursion and Gibbs sampling. Based on the experiments with simulated data, we show that our method significantly improves the detection accuracy toward spatiotemporally correlated changepoints. A real-world application to bike-sharing data also demonstrates the effectiveness of our method. This research has significant relevance to companies operating systems across geographical regions, as it enables a more robust understanding of emerging trends and shifts in spatiotemporal data. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: Financial support from the National Natural Science Foundation of China [Grants 12271287, 72361137005, and 72401177] is gratefully acknowledged. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/5810483/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2024.0030 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2024.0030},
  journal      = {INFORMS Journal on Data Science},
  month        = {4-6},
  number       = {2},
  pages        = {133-153},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Detecting multiple changepoints by exploiting their spatiotemporal correlations: A bayesian hierarchical approach},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonstationary and sparsely-correlated multioutput gaussian process with spike-and-slab prior. <em>IJDS</em>, <em>4</em>(2), 114-132. (<a href='https://doi.org/10.1287/ijds.2023.0022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multioutput Gaussian process (MGP) is commonly used as a transfer learning method to leverage information among multiple outputs. A key advantage of MGP is providing uncertainty quantification for prediction, which is highly important for subsequent decision-making tasks. However, traditional MGP may not be sufficiently flexible to handle multivariate data with dynamic characteristics, particularly when dealing with complex temporal correlations. Additionally, because some outputs may lack correlation, transferring information among them may lead to negative transfer. To address these issues, this study proposes a nonstationary MGP model that can capture both the dynamic and sparse correlation among outputs. Specifically, the covariance functions of MGP are constructed using convolutions of time-varying kernel functions. Then a dynamic spike-and-slab prior is placed on correlation parameters to automatically decide which sources are informative to the target output in the training process. An expectation-maximization (EM) algorithm is proposed for efficient model fitting. Both numerical studies and real cases demonstrate its efficacy in capturing dynamic and sparse correlation structure and mitigating negative transfer for high-dimensional time-series data modeling. A mountain-car reinforcement learning case highlights that transferring knowledge from source expertise can enhance the efficiency of the target decision-making process. Our method holds promise for application in more complex multitask decision-making challenges within nonstationary environments in the future. History: Rema Padman served as the senior editor for this article. Funding: This work was supported by NSFC [Grants NSFC-72171003, NSFC-71932006, and NSFC-72101147]. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.4010696.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0022 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0022},
  journal      = {INFORMS Journal on Data Science},
  month        = {4-6},
  number       = {2},
  pages        = {114-132},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Nonstationary and sparsely-correlated multioutput gaussian process with spike-and-slab prior},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-aware calibration of classifiers. <em>IJDS</em>, <em>4</em>(2), 101-113. (<a href='https://doi.org/10.1287/ijds.2024.0038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most classification techniques in machine learning are able to produce probability predictions in addition to class predictions. However, these predicted probabilities are often not well calibrated in that they deviate from the actual outcome rates (i.e., the proportion of data instances that actually belong to a certain class). A lack of calibration can jeopardize downstream decision tasks that rely on accurate probability predictions. Although several post hoc calibration methods have been proposed, they generally do not consider the potentially asymmetric costs associated with overprediction versus underprediction. In this research, we formally define the problem of cost-aware calibration and propose a metric to quantify the cost of miscalibration for a given classifier. Next, we propose three approaches to achieve cost-aware calibration, two of which are cost-aware adaptations of existing calibration algorithms; the third one (named MetaCal ) is a Bayes optimal learning algorithm inspired by prior work on cost-aware classification. We carry out systematic empirical evaluations on multiple public data sets to demonstrate the effectiveness of the proposed approaches in reducing the cost of miscalibration. Finally, we generalize the definition and metric as well as solution algorithms of cost-aware calibration to account for nonlinear cost structures that may arise in real-world decision tasks. History: David Martens served as the senior editor for this article. Data Ethics & Reproducibility Note: There are no data ethics considerations. The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.8552538.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2024.0038 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2024.0038},
  journal      = {INFORMS Journal on Data Science},
  month        = {4-6},
  number       = {2},
  pages        = {101-113},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Cost-aware calibration of classifiers},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reduced modeling approach for making predictions with incomplete data having blockwise missing patterns. <em>IJDS</em>, <em>4</em>(1), 85-99. (<a href='https://doi.org/10.1287/ijds.2022.9016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete data with blockwise missing patterns are commonly encountered in analytics, and solutions typically entail listwise deletion or imputation. However, as the proportion of missing values in input features increases, listwise or columnwise deletion leads to information loss, whereas imputation diminishes the integrity of the training data set. We present the blockwise reduced modeling (BRM) method for analyzing blockwise missing patterns, which adapts and improves on the notion of reduced modeling proposed by Friedman, Kohavi, and Yun in 1996 as lazy decision trees. In contrast to the original idea of reduced modeling of delaying model induction until a prediction is required, our method is significantly faster because it exploits the blockwise missing patterns to pretrain ensemble models that require minimum imputation of data. Models are pretrained over the overlapping subsets of an incomplete data set that contain only populated values. During prediction, each test instance is mapped to one of these models based on its feature-missing pattern. BRM can be applied to any supervised learning model for tabular data. We benchmark the predictive performance of BRM using simulations of blockwise missing patterns on three complete data sets from public repositories. Thereafter, we evaluate its utility on three data sets with actual blockwise missing patterns. We demonstrate that BRM is superior to most existing benchmarks in terms of predictive performance for linear and nonlinear models. It also scales well and is more reliable than existing benchmarks for making predictions with blockwise missing pattern data. History: Maytal Saar-Tsechansky served as the senior editor for this article. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/0274716/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.9016 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.9016},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {85-99},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {A reduced modeling approach for making predictions with incomplete data having blockwise missing patterns},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair collaborative learning (FairCL): A method to improve fairness amid personalization. <em>IJDS</em>, <em>4</em>(1), 67-84. (<a href='https://doi.org/10.1287/ijds.2024.0029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model personalization has attracted widespread attention in recent years. In an ideal situation, if individuals’ data are sufficient, model personalization can be realized by building models separately for different individuals using their own data. But, in reality, individuals often have data sets of varying sizes and qualities. To overcome this disparity, collaborative learning has emerged as a generic strategy for model personalization, but there is no mechanism to ensure fairness in this framework. In this paper, we develop fair collaborative learning (FairCL) that could potentially integrate a variety of fairness concepts. We further focus on two specific fairness metrics, the bounded individual loss and individual fairness, and develop a self-adaptive algorithm for FairCL and conduct both simulated and real-world case studies. Our study reveals that model fairness and accuracy could be improved simultaneously in the context of model personalization. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: This work was supported by the Breakthrough T1D Award [Grant 2-SRA-2022-1259-S-B]. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/1331847/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2024.0029 ). The real-world data, including the transportation demand management and surgical site infection data sets, are proprietary and not publicly available. Other results are available at https://github.com/ryanlif/FairCL .},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2024.0029},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {67-84},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Fair collaborative learning (FairCL): A method to improve fairness amid personalization},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multilabel classification for fine-level event extraction from aviation accident reports. <em>IJDS</em>, <em>4</em>(1), 51-66. (<a href='https://doi.org/10.1287/ijds.2022.0032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large numbers of accident reports are recorded in the aviation domain, which greatly values improving aviation safety. To better use those reports, we must understand the most important events or impact factors according to the accident reports. However, the increasing number of accident reports requires large efforts from domain experts to label those reports. To make the labeling process more efficient, many researchers have started developing algorithms to automatically identify the underlying events from accident reports. This article argues that we can identify the events more accurately by leveraging the event taxonomy. More specifically, we consider the problem to be a hierarchical classification task, where we first identify the coarse-level information and then predict the fine-level information. We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into the bidirectional encoder representations from transformers model. To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels. The effectiveness of our framework is evaluated using data collected by the National Transportation Safety Board. It has been shown that fine-level prediction accuracy is highly improved and that the regularization term can be beneficial to the rare event identification problem. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: The research reported in this paper was supported by funds from NASA University Leadership Initiative program (Contract No. NNX17AJ86A, Project Officer: Dr. Anupa Bajwa, Principal Investigator: Dr. Yongming Liu) and NSF DMS 1830363. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/9128124/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0032 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0032},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {51-66},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Hierarchical multilabel classification for fine-level event extraction from aviation accident reports},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank robust subspace tensor clustering for metro passenger flow modeling. <em>IJDS</em>, <em>4</em>(1), 33-50. (<a href='https://doi.org/10.1287/ijds.2022.0028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor clustering has become an important topic, specifically in spatiotemporal modeling, because of its ability to cluster spatial modes (e.g., stations or road segments) and temporal modes (e.g., time of day or day of the week). Our motivating example is from subway passenger flow modeling, where similarities between stations are commonly found. However, the challenges lie in the innate high-dimensionality of tensors and also the potential existence of anomalies. This is because the three tasks, that is, dimension reduction, clustering, and anomaly decomposition, are intercorrelated with each other, and treating them in a separate manner will render a suboptimal performance. Thus, in this work, we design a tensor-based subspace clustering and anomaly decomposition technique for simultaneous outlier-robust dimension reduction and clustering for high-dimensional tensors. To achieve this, a novel low-rank robust subspace clustering decomposition model is proposed by combining Tucker decomposition, sparse anomaly decomposition, and subspace clustering. An effective algorithm based on Block Coordinate Descent is proposed to update the parameters. Prudent experiments prove the effectiveness of the proposed framework via the simulation study, with a gain of +25% clustering accuracy over benchmark methods in a hard case. The interrelations of the three tasks are also analyzed via ablation studies, validating the interrelation assumption. Moreover, a case study in station clustering based on real passenger flow data is conducted, with quite valuable insights discovered. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: H. Yan is partially funded by DOE [DE-EE0009354] and NSF [CMMI 2316654]. F. Tsung is partially funded with the RGC [GRF 16201718 and 16216119]. The authors appreciate the help from the Hong Kong MTR Co. research, marketing, and customer service teams. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6536164/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0028 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0028},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {33-50},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Low-rank robust subspace tensor clustering for metro passenger flow modeling},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal time series forecasting using an iterative kernel-based regression. <em>IJDS</em>, <em>4</em>(1), 20-32. (<a href='https://doi.org/10.1287/ijds.2023.0019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal time series analysis is a growing area of research that includes different types of tasks, such as forecasting, prediction, clustering, and visualization. In many domains, like epidemiology or economics, time series data are collected to describe the observed phenomenon in particular locations over a predefined time slot and predict future behavior. Regression methods provide a simple mechanism for evaluating empirical functions over scattered data points. In particular, kernel-based regressions are suitable for cases in which the relationship between the data points and the function is not linear. In this work, we propose a kernel-based iterative regression model, which fuses data from several spatial locations for improving the forecasting accuracy of a given time series. In more detail, the proposed method approximates and extends a function based on two or more spatial input modalities coded by a series of multiscale kernels, which are averaged as a convex combination. The proposed spatio-temporal regression resembles ideas that are present in deep learning architectures, such as passing information between scales. Nevertheless, the construction is easy to implement, and it is also suitable for modeling data sets of limited size. Experimental results demonstrate the proposed model for solar energy prediction, forecasting epidemiology infections, and future number of fire events. The method is compared with well-known regression techniques and highlights the benefits of the proposed model in terms of accuracy and flexibility. The reliable outcome of the proposed model and its nonparametric nature yield a robust tool to be integrated as a forecasting component in wide range of decision support systems that analyze time series data. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: This research was supported by the Israel Science Foundation [Grant 1144/20] and partly supported by the Ministry of Science and Technology, Israel [Grant 5614]. Data Ethics & Reproducibility Note: The code capsule is available on Code Ocean at https://codeocean.com/capsule/6417440/tree and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0019 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0019},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {20-32},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Spatio-temporal time series forecasting using an iterative kernel-based regression},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking cost-sensitive classification in deep learning via adversarial data augmentation. <em>IJDS</em>, <em>4</em>(1), 1-19. (<a href='https://doi.org/10.1287/ijds.2022.0033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-sensitive classification is critical in applications where misclassification errors widely vary in cost. However, overparameterization poses fundamental challenges to the cost-sensitive modeling of deep neural networks (DNNs). The ability of a DNN to fully interpolate a training data set can render a DNN, evaluated purely on the training set, ineffective in distinguishing a cost-sensitive solution from its overall accuracy maximization counterpart. This necessitates rethinking cost-sensitive classification in DNNs. To address this challenge, this paper proposes a cost-sensitive adversarial data augmentation (CSADA) framework to make overparameterized models cost sensitive. The overarching idea is to generate targeted adversarial examples that push the decision boundary in cost-aware directions. These targeted adversarial samples are generated by maximizing the probability of critical misclassifications and used to train a model with more conservative decisions on costly pairs. Experiments on well-known data sets and a pharmacy medication image (PMI) data set, made publicly available, show that our method can effectively minimize the overall cost and reduce critical errors while achieving comparable overall accuracy. History: Nick Street served as the senior editor for this article. Funding: Research reported in this publication was supported by the National Library of medicine of the National Institutes of Health in the United States under award number R01LM013624. Data Ethics & Reproducibility Note: This paper abides by data ethics requirements. Data used are publicly available online at https://deepblue.lib.umich.edu/data/concern/data_sets/6d56zw997 . Codes to replicate the results of this paper are available on Code Ocean at https://doi.org/10.24433/CO.2139841.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0033 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0033},
  journal      = {INFORMS Journal on Data Science},
  month        = {1-3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Rethinking cost-sensitive classification in deep learning via adversarial data augmentation},
  volume       = {4},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
