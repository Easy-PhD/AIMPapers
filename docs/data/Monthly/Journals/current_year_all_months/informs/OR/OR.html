<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="or">OR - 156</h2>
<ul>
<li><details>
<summary>
(2025). A robust optimization approach to network control using local information exchange. <em>OR</em>, <em>73</em>(5), 2849-2866. (<a href='https://doi.org/10.1287/opre.2020.0217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing policies for a network of agents is typically done by formulating an optimization problem where each agent has access to state measurements of all the other agents in the network. Such policy designs with centralized information exchange result in optimization problems that are typically hard to solve, require establishing substantial communication links, and do not promote privacy since all information is shared among the agents. Designing policies based on arbitrary communication structures can lead to nonconvex optimization problems that are typically NP-hard. In this work, we propose an optimization framework for decentralized policy designs. In contrast to the centralized information exchange, our approach requires only local communication exchange among the neighboring agents matching the physical coupling of the network. Thus, each agent only requires information from its direct neighbors, minimizing the need for excessive communication and promoting privacy amongst the agents. Using robust optimization techniques, we formulate a convex optimization problem with a loosely coupled structure that can be solved efficiently. We numerically demonstrate the efficacy of the proposed approach in energy management and supply chain applications. We show that the proposed approach leads to solutions that closely approximate those obtained by the centralized formulation only at a fraction of the computational effort. Funding: This research was supported by the Swiss National Science Foundation [Grant 51NF40_180545 under the National Centres of Competence in Research (NCCR) Automation and Grant P2ELP2_195149, Early Postdoc Mobility Fellowship]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2020.0217 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0217},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2849-2866},
  shortjournal = {Oper. Res.},
  title        = {A robust optimization approach to network control using local information exchange},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regular variable returns to scale production frontier and efficiency measurement. <em>OR</em>, <em>73</em>(5), 2830-2848. (<a href='https://doi.org/10.1287/opre.2021.0470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most frequently used empirical production frontier in data envelopment analysis, the variable returns to scale frontier, has a convex technology set and displays a special structure in economics, called the regular variable returns to scale in this paper; the production technology exhibits increasing returns to scale at the beginning of the production process followed by constant returns to scale and decreasing returns to scale. When the assumption of convexity is relaxed, modeling regular variable returns to scale becomes difficult, and currently, no satisfactory solution is available in multioutput production. Overcoming these difficulties, this paper adopts a suggestion in literature to incorporate regular variable returns to scale into the free disposal hull frontier under multiple outputs. We establish a framework for analyzing regular variable returns to scale and recommend an empirical production frontier for measuring technical efficiency with such pattern and multiple outputs. In the presence of regular variable returns to scale without convexity, the value of the technical efficiency measure computed from this new frontier is closer to the “true” value than that from the free disposal hull frontier, and the conventional variable returns to scale frontier may cause misleading implications. Funding: This research was partially supported by the National Natural Science Foundation of China [Grants 72001061 and 72243002], Hong Kong Shue Yan University [University Research Grant URG/20/01], and the Research Grants Council of the Hong Kong Special Administrative Region [Faculty Development Scheme/UGC/FDS15/E02/21]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0470 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0470},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2830-2848},
  shortjournal = {Oper. Res.},
  title        = {Regular variable returns to scale production frontier and efficiency measurement},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustifying conditional portfolio decisions via optimal transport. <em>OR</em>, <em>73</em>(5), 2801-2829. (<a href='https://doi.org/10.1287/opre.2021.0243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a data-driven portfolio selection model that integrates side information, conditional estimation, and robustness using the framework of distributionally robust optimization. Conditioning on the observed side information, the portfolio manager solves an allocation problem that minimizes the worst-case conditional risk-return tradeoff, subject to all possible perturbations of the covariate-return probability distribution in an optimal transport ambiguity set. Despite the nonlinearity of the objective function in the probability measure, we show that the distributionally robust portfolio allocation with a side information problem can be reformulated as a finite-dimensional optimization problem. If portfolio decisions are made based on either the mean-variance or the mean-conditional value-at-risk criterion, the reformulation can be further simplified to second-order or semidefinite cone programs. Empirical studies in the U.S. equity market demonstrate the advantage of our integrative framework against other benchmarks. Funding: The material in this paper is based on work supported by the Air Force Office of Scientific Research [Award FA9550-20-1-0397]. Additional support is gratefully acknowledged from the National Science Foundation [Grants 1915967, 1820942, and 1838676], the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2016-05208], and the China Merchant Bank. V. A. Nguyen gratefully acknowledges the generous support from the Chinese University of Hong Kong [Improvement on Competitiveness in Hiring New Faculties Funding Scheme] and the Chinese University of Hong Kong [Direct Grant 4055191]. S. Wang is partially supported by the National Natural Science Foundation of China [Grant 72371022]. Finally, this research was enabled in part by support provided by Compute Canada. Supplemental Material: The computer code and data that support the findings of this study and the online appendix are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0243 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0243},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2801-2829},
  shortjournal = {Oper. Res.},
  title        = {Robustifying conditional portfolio decisions via optimal transport},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical approach to robust stability of multiclass queueing networks. <em>OR</em>, <em>73</em>(5), 2782-2800. (<a href='https://doi.org/10.1287/opre.2023.0147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the global—relative to control policies—stability of multiclass queueing networks. In these, as is known, it is generally insufficient that the nominal utilization at each server is below 100%. Certain policies, although work conserving, may destabilize a network that satisfies the nominal-load conditions; additional conditions on the primitives are needed for global stability (stability under any work-conserving policy). The global-stability region was fully characterized for two-station networks in Dai and Vande Vate [Dai JG, Vande Vate JH (1996) Global stability of two-station queueing networks. Stochastic Networks (Springer, New York), 1–26.] but a general framework for networks with more than two stations remains elusive. In this paper, we offer progress on this front by considering a subset of nonidling control policies, namely, queue-ratio (QR) policies. These include as special cases all static-priority policies. With this restriction, we are able to introduce a complete framework that applies to networks of any size. Our framework breaks the analysis of robust QR stability (stability under any QR policy) into (i) robust state-space collapse, and (ii) robust stability of the Skorohod problem (SP) representing the fluid workload. Sufficient conditions for both are specified in terms of simple optimization problems. We use these optimization problems to prove that the family of QR policies satisfies a weak form of convexity relative to policies. A direct implication of this convexity is that if the SP is stable for all static-priority policies (the “extreme” QR policies), then it is also stable under any QR policy. Whereas robust QR stability is weaker than global stability, our framework recovers necessary and sufficient conditions for global stability in specific networks. Funding: This work was supported by the National Science Foundation [Grant CMMI-1856511]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0147 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0147},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2782-2800},
  shortjournal = {Oper. Res.},
  title        = {A hierarchical approach to robust stability of multiclass queueing networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolution bounds on quantile aggregation. <em>OR</em>, <em>73</em>(5), 2761-2781. (<a href='https://doi.org/10.1287/opre.2021.0765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile aggregation with dependence uncertainty has a long history in probability theory, with wide applications in finance, risk management, statistics, and operations research. Using a recent result on inf-convolution of quantile-based risk measures, we establish new analytical bounds for quantile aggregation, which we call convolution bounds. Convolution bounds both unify every analytical result available in quantile aggregation and enlighten our understanding of these methods. These bounds are the best available in general. Moreover, convolution bounds are easy to compute, and we show that they are sharp in many relevant cases. They also allow for interpretability on the extremal dependence structure. The results directly lead to bounds on the distribution of the sum of random variables with arbitrary dependence. We discuss relevant applications in risk management and economics. Funding: This work was supported by the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [Grant 2023B1212010001]; The Chinese University of Hong Kong (Shenzhen) research startup fund [Grant UDF01003336]; Natural Sciences and Engineering Research Council of Canada [Grants CRC-2022-00141 and RGPIN-2024-03728]; Shenzhen Science and Technology Program [Grant RCBS20231211090814028]; National Science Foundation [Grants 1915967, 2118199, 2229011, CAREER CMMI-1834710, and IIS-1849280]; Air Force Office of Scientific Research [Grant FA9550-20-1-0397]; and National Natural Science Foundation of China [Grant 12401624]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2021.0765 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0765},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2761-2781},
  shortjournal = {Oper. Res.},
  title        = {Convolution bounds on quantile aggregation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Near-optimal adaptive policies for serving stochastically departing customers. <em>OR</em>, <em>73</em>(5), 2744-2760. (<a href='https://doi.org/10.1287/opre.2022.0548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a multistage stochastic optimization problem, studying how a single server should prioritize stochastically departing customers. In this setting, our objective is to determine an adaptive service policy that maximizes the expected total reward collected along a discrete planning horizon, in the presence of customers who are independently departing between one stage and the next with known stationary probabilities. Despite its deceiving structural simplicity, we are unaware of nontrivial results regarding the rigorous design of optimal or truly near-optimal policies at present time. Our main contribution resides in proposing a quasi-polynomial-time approximation scheme for serving impatient customers. Specifically, letting n be the number of underlying customers, our algorithm identifies in O ( n O ϵ ( log 2 n ) ) time a service policy whose expected reward is within factor 1 − ϵ of the optimal adaptive reward. Our method for deriving this approximation scheme synthesizes various stochastic analyses in order to investigate how the adaptive optimum is affected by alterations to several instance parameters, including the reward values, the departure probabilities, and the collection of customers itself. Funding: This work was supported by the Israel Science Foundation [1407/20]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0548 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0548},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2744-2760},
  shortjournal = {Oper. Res.},
  title        = {Near-optimal adaptive policies for serving stochastically departing customers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximating the set of nash equilibria for convex games. <em>OR</em>, <em>73</em>(5), 2729-2743. (<a href='https://doi.org/10.1287/opre.2023.0541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Feinstein and Rudloff (2024) , it was shown that the set of Nash equilibria for any noncooperative N player game coincides with the set of Pareto optimal points of a certain vector optimization problem with nonconvex ordering cone. To avoid dealing with a nonconvex ordering cone, an equivalent characterization of the set of Nash equilibria as the intersection of the Pareto optimal points of N multi-objective problems (i.e., with the natural ordering cone) is proven. So far, algorithms to compute the exact set of Pareto optimal points of a multi-objective problem exist only for the class of linear problems, which reduces the possibility of finding the true set of Nash equilibria by those algorithms to linear games only. In this paper, we will consider the larger class of convex games. Because typically only approximate solutions can be computed for convex vector optimization problems, we first show, in total analogy to the result above, that the set of ϵ -approximate Nash equilibria can be characterized by the intersection of ϵ -approximate Pareto optimal points for N convex multi-objective problems. Then, we propose an algorithm based on results from vector optimization and convex projections that allows for the computation of a set that, on one hand, contains the set of all true Nash equilibria and is, on the other hand, contained in the set of ϵ -approximate Nash equilibria. In addition to the joint convexity of the cost function for each player, this algorithm works provided the players are restricted by either shared polyhedral constraints or independent convex constraints. Funding: This work was supported by Austrian Science Funds (FWF) [W1260-N35]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0541 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0541},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2729-2743},
  shortjournal = {Oper. Res.},
  title        = {Approximating the set of nash equilibria for convex games},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The analytics of robust satisficing: Predict, optimize, satisfice, then fortify. <em>OR</em>, <em>73</em>(5), 2708-2728. (<a href='https://doi.org/10.1287/opre.2023.0199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel approach to prescriptive analytics that leverages robust satisficing techniques to determine optimal decisions in situations of distribution ambiguity and parameter estimation uncertainty. Our decision model relies on a reward function that incorporates uncertain parameters, which can be predicted using available side information. However, the accuracy of the linear prediction model depends on the quality of regression coefficient estimates derived from the available data. To achieve a desired level of fragility under distribution ambiguity, we begin by solving a residual-based robust satisficing model in which the residuals from the regression are used to construct an estimated empirical distribution and a target is established relative to the predict-then-optimize objective value. In the face of estimation uncertainty, we then solve an estimation-fortified robust satisficing model that minimizes the influence of estimation uncertainty while ensuring that the solution would maintain at most the same level of fragility in achieving a less ambitious guarding target. Our approach is supported by statistical justifications, and we propose tractable models for various scenarios, such as saddle functions, two-stage linear optimization problems, and decision-dependent predictions. We demonstrate the effectiveness of our approach through case studies involving a wine portfolio investment problem and a multiproduct pricing problem using real-world data. Our numerical studies show that our approach outperforms the predict-then-optimize approach in achieving higher expected rewards and at lower risks when evaluated on the actual distribution. Notably, we observe significant improvements over the benchmarks, particularly in cases of limited data availability. Funding: The research of M. Sim was supported by the Ministry of Education, Singapore under its 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010]. The research of Q. Tang was supported by Nanyang Technological University [Start-Up Grant 020022-00001] and the Ministry of Education, Singapore [Tier 1 Grant 25010057]. The research of M. Zhou was supported by the National Natural Science Foundation of China [Grants 72301075 and 72293564/72293560]. The research of T. Zhu was supported by the National Natural Science Foundation of China [Grant 72401058]. Any opinions, findings, conclusions, or recommendations expressed in the material are those of the authors and do not necessarily reflect the views of the Ministry of Education, Singapore. Supplementary Material: The online appendix and computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0199 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0199},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2708-2728},
  shortjournal = {Oper. Res.},
  title        = {The analytics of robust satisficing: Predict, optimize, satisfice, then fortify},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ranking and contextual selection. <em>OR</em>, <em>73</em>(5), 2695-2707. (<a href='https://doi.org/10.1287/opre.2023.0378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new ranking-and-selection procedure, called ranking and contextual selection, in which covariates provide context for data-driven decisions. Our procedure optimizes over a set of covariate design points off-line and then, given an actual observation of the covariate, makes an online decision based on classification—a distinctly new approach. We prove the existence of an experimental design that yields a pointwise probability of good selection guarantee and derive a postexperiment assessment of our procedure that provides an optimality gap upper bound with guaranteed coverage for decisions with respect to future covariates. We illustrate ranking and contextual selection with an application to assortment optimization using data available from Yahoo!. Funding: This work was supported by the National Science Foundation [Grant CMMI-2206973]. Supplemental Material: This article includes an online appendix and computer code and data supporting the study’s findings at https://doi.org/10.1287/opre.2023.0378 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0378},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2695-2707},
  shortjournal = {Oper. Res.},
  title        = {Ranking and contextual selection},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally constrained black-box stochastic gradient estimation and optimization. <em>OR</em>, <em>73</em>(5), 2680-2694. (<a href='https://doi.org/10.1287/opre.2021.0307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider stochastic gradient estimation using only black-box function evaluations, where the function argument lies within a probability simplex. This problem is motivated from gradient-descent optimization procedures in multiple applications in distributionally robust analysis and inverse model calibration involving decision variables that are probability distributions. We are especially interested in obtaining gradient estimators where one or few sample observations or simulation runs apply simultaneously to all directions. Conventional zeroth-order gradient schemes such as simultaneous perturbation face challenges as the required moment conditions that allow the “canceling” of higher-order biases cannot be satisfied without violating the simplex constraints. We investigate a new set of required conditions on the random perturbation generator, which leads us to a class of implementable gradient estimators using Dirichlet mixtures. We study the statistical properties of these estimators and their utility in constrained stochastic approximation. We demonstrate the effectiveness of our procedures and compare with benchmarks via several numerical examples. Funding: The authors gratefully acknowledge support from the National Science Foundation [Grants CAREER CMMI-1834710 and IIS-1849280]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0307 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0307},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2680-2694},
  shortjournal = {Oper. Res.},
  title        = {Distributionally constrained black-box stochastic gradient estimation and optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in inverse optimization: Incenter cost, augmented suboptimality loss, and algorithms. <em>OR</em>, <em>73</em>(5), 2661-2679. (<a href='https://doi.org/10.1287/opre.2023.0254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In inverse optimization (IO), an expert agent solves an optimization problem parametric in an exogenous signal. From a learning perspective, the goal is to learn the expert’s cost function given a data set of signals and corresponding optimal actions. Motivated by the geometry of the IO set of consistent cost vectors, we introduce the “incenter” concept, a new notion akin to the recently proposed circumcenter concept. Discussing the geometric and robustness interpretation of the incenter cost vector, we develop corresponding tractable convex reformulations that are in contrast with the circumcenter, which we show is equivalent to an intractable optimization program. We further propose a novel loss function called augmented suboptimality loss (ASL), a relaxation of the incenter concept for problems with inconsistent data. Exploiting the structure of the ASL, we propose a novel first-order algorithm, which we name stochastic approximate mirror descent . This algorithm combines stochastic and approximate subgradient evaluations, together with mirror descent update steps, which are provably efficient for the IO problems with discrete feasible sets with high cardinality. We implement the IO approaches developed in this paper as a Python package called InvOpt. Our numerical experiments are reproducible, and the underlying source code is available as examples in the InvOpt package. Funding: This work was partially supported by the European Research Council [Grant TRUST-949796]. Supplemental Review: The empirical results in this paper were replicated. The code, data, and files required to reproduce the results were reviewed and are available at https://doi.org/10.1287/opre.2023.0254.cd . Supplemental Material: This article includes an online appendix, computer code and data supporting the study’s findings, and replication files. All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2023.0254.cd .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0254},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2661-2679},
  shortjournal = {Oper. Res.},
  title        = {Learning in inverse optimization: Incenter cost, augmented suboptimality loss, and algorithms},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven clustering and feature-based retail electricity pricing with smart meters. <em>OR</em>, <em>73</em>(5), 2636-2660. (<a href='https://doi.org/10.1287/opre.2022.0112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an electric utility company that serves retail electricity customers over a discrete-time horizon. In each period, the company observes the customers’ consumption and high-dimensional features on customer characteristics and exogenous factors. A distinctive element of our work is that these features exhibit three types of heterogeneity—over time, customers, or both. Based on the consumption and feature observations, the company can dynamically adjust the retail electricity price at the customer level. The consumption depends on the features: there is an underlying structure of clusters in the feature space, and the relationship between consumption and features is different in each cluster. Initially, the company knows neither the underlying cluster structure nor the corresponding consumption models. We design a data-driven policy of joint spectral clustering and feature-based pricing and show that our policy achieves near-optimal performance; that is, its average regret converges to zero at the fastest achievable rate. This work is the first to theoretically analyze joint clustering and feature-based pricing with different types of feature heterogeneity. Our case study based on real-life smart meter data from Texas illustrates that our policy increases company profits by more than 100% over a three-month period relative to the company policy and is robust to various forms of model misspecification. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0112 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0112},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2636-2660},
  shortjournal = {Oper. Res.},
  title        = {Data-driven clustering and feature-based retail electricity pricing with smart meters},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note–Dynamic duopolistic competition with sticky prices. <em>OR</em>, <em>73</em>(5), 2627-2635. (<a href='https://doi.org/10.1287/opre.2023.0473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A paradoxical conclusion arises in a series of game-theoretic models: the limit equilibria retain frictional qualities even as frictions seemingly vanish. This originates in textbook models, such as the differential game by Fershtman and Kamien [Fershtman C, Kamien MI (1987) Dynamic duopolistic competition with sticky prices. Econometrica 55(5):1151–1164] on duopolistic competition with sticky prices. We show that this paradox is an artifact of the type of limit restricted by continuous-time modeling. Fershtman and Kamien find that the closed-loop equilibrium remains surprisingly distinct from the static Cournot equilibrium as price adjustment becomes infinitely fast. We formulate and solve a discrete-time analog that nests their continuous-time model. Contrary to their conclusion, we show that the frictionless closed-loop equilibrium converges to the static Cournot equilibrium. Price stickiness persists instantaneously in the continuous-time setting because this approach cannot control the extent of price adjustment per period. Because of this subtle limitation, limit results differ between continuous- and discrete-time formulations of the model.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0473},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2627-2635},
  shortjournal = {Oper. Res.},
  title        = {Technical Note–Dynamic duopolistic competition with sticky prices},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reciprocity between tree ensemble optimization and multilinear optimization. <em>OR</em>, <em>73</em>(5), 2610-2626. (<a href='https://doi.org/10.1287/opre.2022.0150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we establish a low-degree polynomially-sized reduction between tree ensemble optimization and optimization of multilinear functions over a Cartesian product of simplices. We use this insight to derive new formulations for tree ensemble optimization problems and to obtain new convex hull results for multilinear polytopes. A computational experiment on multicommodity transportation problems with costs modeled using tree ensembles shows the practical advantage of our formulation relative to existing formulations of tree ensembles and other piecewise-linear modeling techniques. Funding: This work was supported by Division of Civil, Mechanical and Manufacturing Innovation [Grants 1727989, 1917323]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0150 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0150},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2610-2626},
  shortjournal = {Oper. Res.},
  title        = {A reciprocity between tree ensemble optimization and multilinear optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deepest cuts for benders decomposition. <em>OR</em>, <em>73</em>(5), 2591-2609. (<a href='https://doi.org/10.1287/opre.2021.0503'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its inception, Benders decomposition (BD) has been successfully applied to a wide range of large-scale mixed-integer (linear) problems. The key element of BD is the derivation of Benders cuts, which are often not unique. In this paper, we introduce a novel unifying Benders cut selection technique based on a geometric interpretation of cut depth, produce deepest Benders cuts based on ℓ p -norms, and study their properties. Specifically, we show that deepest cuts resolve infeasibility through minimal deviation (in a distance sense) from the incumbent point, are relatively sparse, and may produce optimality cuts even when classic Benders would require a feasibility cut. Leveraging the duality between separation and projection, we develop a guided projections algorithm for producing deepest cuts, exploiting the combinatorial structure and decomposability of problem instances. We then propose a generalization of our Benders separation problem, which not only brings several well-known cut selection strategies under one umbrella, but also, when endowed with a homogeneous function, enjoys several properties of geometric separation problems. We show that, when the homogeneous function is linear, the separation problem takes the form of the minimal infeasible subsystems (MIS) problem. As such, we provide systematic ways of selecting the normalization coefficients of the MIS method and introduce a directed depth-maximizing algorithm for deriving these cuts. Inspired by the geometric interpretation of distance-based cuts and the repetitive nature of two-stage stochastic programs, we introduce a tailored algorithm to further facilitate deriving these cuts. Our computational experiments on various benchmark problems illustrate effectiveness of deepest cuts in reducing both computation time and number of Benders iterations and producing high-quality bounds at early iterations. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0503 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0503},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2591-2609},
  shortjournal = {Oper. Res.},
  title        = {Deepest cuts for benders decomposition},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Search in the dark: The case with recall and gaussian learning. <em>OR</em>, <em>73</em>(5), 2572-2590. (<a href='https://doi.org/10.1287/opre.2023.0150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classic sequential search problem rewards the decision maker with the highest sampled value minus a cost per sample. If the sampling distribution is unknown, then a Bayesian decision maker faces a complex balance between exploration and exploitation. We solve the stopping problem of sampling from a normal distribution with unknown mean and variance and a conjugate prior, a longstanding open problem. The optimal stopping region may be empty (it may be optimal to continue the search regardless of the offer one receives, especially at the early stages), or it may consist of one or two bounded intervals. Whereas a single reservation price cannot describe the optimal rule, we do find an optimal index policy taking the form of a standardized reservation rule: stop if and only if the standardized value of the current best exceeds a threshold that depends on the standardized search cost. We also provide an algorithm to compute the index function, producing a practical way to implement the optimal stopping rule for any given prior, sampling history, and sampling horizon. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0150 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0150},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2572-2590},
  shortjournal = {Oper. Res.},
  title        = {Search in the dark: The case with recall and gaussian learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note–Stability of a queue fed by scheduled traffic at critical loading. <em>OR</em>, <em>73</em>(5), 2567-2571. (<a href='https://doi.org/10.1287/opre.2023.0039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the workload process for a single server queue with deterministic service times in which customers arrive according to a scheduled traffic process. A scheduled arrival sequence is one in which customers are scheduled to arrive at constant interarrival times, but each customer’s actual arrival time is perturbed from her scheduled arrival time by a random perturbation. In this paper, we consider a critically loaded queue in which the service rate equals the arrival rate. Unlike a queue fed by renewal traffic, this queue can be stable even in the presence of critical loading. We show that for finite mean perturbations, a necessary and sufficient condition for stability is when the positive part of the perturbation has bounded support, with no requirement on the negative part of the perturbation. Perhaps surprisingly, this criterion is not reversible, in the sense that such a queue can be stable for a scheduled traffic process in forward time, but unstable for the time-reversal of the same traffic process.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0039},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2567-2571},
  shortjournal = {Oper. Res.},
  title        = {Technical Note–Stability of a queue fed by scheduled traffic at critical loading},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Markdown policies for demand learning with forward-looking customers. <em>OR</em>, <em>73</em>(5), 2550-2566. (<a href='https://doi.org/10.1287/opre.2019.0402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the markdown pricing problem of a firm that sells a product to a mixture of myopic and forward-looking customers. The firm faces uncertainty about the customers’ forward-looking behavior, arrival pattern, and valuations for the product, which we collectively refer to as the demand model. Over a multiperiod selling season, the firm sequentially marks down the product’s price and makes demand observations to learn about the underlying demand model. Because forward-looking customers create an intertemporal dependency, we identify that the keys to achieving good profit performance are (i) judiciously accumulating information on the demand model and (ii) preserving the market size in early sales periods. Based on these, we construct and analyze markdown policies that exhibit near-optimal performance under a wide variety of forward-looking customer behaviors. Funding: Financial support from Duke University Fuqua School of Business; the University of Chicago Booth School of Business; and CUHK Business School, the Chinese University of Hong Kong is gratefully acknowledged. H. (K.) Chen thanks the Hong Kong Research Grants Council [Grant GRF14506622]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2019.0402 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.0402},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2550-2566},
  shortjournal = {Oper. Res.},
  title        = {Markdown policies for demand learning with forward-looking customers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On consistency of signature using lasso. <em>OR</em>, <em>73</em>(5), 2530-2549. (<a href='https://doi.org/10.1287/opre.2024.1133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signatures are iterated path integrals of continuous and discrete-time processes, and their universal nonlinearity linearizes the problem of feature selection in time series data analysis. This paper studies the consistency of signature using Lasso regression, both theoretically and numerically. We establish conditions under which the Lasso regression is consistent both asymptotically and in finite sample. Furthermore, we show that the Lasso regression is more consistent with the Itô signature for time series and processes that are closer to the Brownian motion and with weaker interdimensional correlations, whereas it is more consistent with the Stratonovich signature for mean-reverting time series and processes. We demonstrate that signature can be applied to learn nonlinear functions and option prices with high accuracy, and the performance depends on properties of the underlying process and the choice of the signature. Funding: R. Zhang’s research was supported by the National Key Research and Development Program of China [Grant 2022YFA1007900], the National Natural Science Foundation of China [Grant 72342004 and Grant 12271013], the Fundamental Research Funds for the Central Universities (Peking University), and Yinhua Education Foundation. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2024.1133 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1133},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2530-2549},
  shortjournal = {Oper. Res.},
  title        = {On consistency of signature using lasso},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Policy learning with competing agents. <em>OR</em>, <em>73</em>(5), 2515-2529. (<a href='https://doi.org/10.1287/opre.2022.0687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision makers often aim to learn a treatment assignment policy under a capacity constraint on the number of agents that they can treat. When agents can respond strategically to such policies, competition arises, complicating estimation of the optimal policy. In this paper, we study capacity-constrained treatment assignments in the presence of such interference. We consider a dynamic model in which the decision maker allocates treatments at each time step and heterogeneous agents myopically best respond to the previous treatment assignment policy. When the number of agents is large but finite, we show that the threshold for receiving treatment under a given policy converges to the policy’s mean-field equilibrium threshold. Based on this result, we develop a consistent estimator for the policy gradient. In a semisynthetic experiment with data from the National Education Longitudinal Study of 1988, we demonstrate that this estimator can be used for learning capacity-constrained policies in the presence of strategic behavior. Funding: This work was supported by National Science Foundation (NSF) [Grant SES-2242876]. R. Sahoo is supported by NSF Graduate Research Fellowship Program [Grant DGE-1656518], a Stanford University Data Science Fellowship, and a Stanford University Ethics in Society Fellowship. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2022.0687 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0687},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2515-2529},
  shortjournal = {Oper. Res.},
  title        = {Policy learning with competing agents},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind network revenue management and bandits with knapsacks under limited switches. <em>OR</em>, <em>73</em>(5), 2496-2514. (<a href='https://doi.org/10.1287/opre.2020.0753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the impact of limited switches on resource-constrained dynamic pricing with demand learning. We focus on the classical price-based blind network revenue management problem and extend our results to the bandits with knapsacks problem. In both settings, a decision maker faces stochastic and distributionally unknown demand, and must allocate finite initial inventory across multiple resources over time. In addition to standard resource constraints, we impose a switching constraint that limits the number of allowable action changes over the time horizon. We establish matching upper and lower bounds on the optimal regret and develop computationally efficient limited-switch algorithms that achieve it. We show that the optimal regret rate is fully characterized by a piecewise-constant function of the switching budget, which further depends on the number of resource constraints. Our results highlight the fundamental role of resource constraints in shaping the statistical complexity of online learning under limited switches. Extensive simulations demonstrate that our algorithms maintain strong cumulative reward performance while significantly reducing the number of switches. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2020.0753 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0753},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2496-2514},
  shortjournal = {Oper. Res.},
  title        = {Blind network revenue management and bandits with knapsacks under limited switches},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instrumenting while experimenting: An empirical method for competitive pricing at scale. <em>OR</em>, <em>73</em>(5), 2477-2495. (<a href='https://doi.org/10.1287/opre.2022.0157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate operational decisions require precise knowledge of the causal effects of such decisions on outcomes, a task that becomes increasingly complex in dynamic business environments. We propose an idea of “instrumenting while experimenting,” whereby researchers can create their own instruments by “injecting” small, random variations directly into the decision-making process and then use such variations to obtain causal estimates of the impact of varying business decisions at scale without disrupting everyday operations. To illustrate the effectiveness of this idea, we partner with a leading U.S. e-commerce retailer and develop a competitive pricing method in the context of increasing competition in online retailing. Our method allows retailers to respond more accurately to competitors’ price changes at scale. Operationally, we first construct a parsimonious demand model to capture the key trade-offs in competitive pricing. This model accounts for potential shifts in customer behaviors based on whether the focal retailer holds a price advantage relative to its competitors. Next, we design and implement a large-scale randomized price experiment on over 10,000 products. Leveraging the experiment as well as the control function approach, we are able to obtain unbiased estimates of key pricing components in the demand model, in particular, price elasticities of customers in both price advantage and disadvantage regions as well as the sales lift when undercutting competitors in price. Lastly, we recommend price responses by solving a constrained optimization problem that uses the estimated demand model as an input. We test this pricing method through another large-scale controlled field experiment on over 10,000 products and demonstrate significant improvements—increasing revenue by over 15% and increasing profit by over 10%. Simulation analyses reveal that these improvements are attributable to the joint implementation of demand modeling (contributing 17% of the total improvement), price optimization (36%), and our proposed estimation method (48%). Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2022.0157 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0157},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2477-2495},
  shortjournal = {Oper. Res.},
  title        = {Instrumenting while experimenting: An empirical method for competitive pricing at scale},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online learning with sample selection bias. <em>OR</em>, <em>73</em>(5), 2458-2476. (<a href='https://doi.org/10.1287/opre.2023.0223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of personalized recommendations on online platforms, where user preferences are unknown, and users interact with the platform through a series of sequential decisions (such as clicking to watch on video platforms or clicking to donate on donation platforms). The platform aims to maximize the final outcome (e.g., viewing duration on video platforms or donations on donation platforms). However, the platform only observes the final outcome for users who complete the first stage (clicking on the recommendation). The final outcome for users who do not complete the first stage (not clicking on the recommendation) remains unobserved (also referred to as funneling ). This censoring of outcomes creates a selection bias issue, as the observed outcomes at different stages are often correlated. We demonstrate that failing to account for this selection bias results in biased estimates and suboptimal recommendations. In fact, well-performing personalized learning algorithms perform poorly and incur linear regret in this setting. Therefore, we propose the sample selection bandit (SSB) algorithm, which combines Heckman’s two-step estimator with the “optimism under uncertainty” principle to address the sample selection bias issue. We show that the SSB algorithm achieves a rate-optimal regret rate (up to logarithmic terms) of O ˜ ( T ) . Furthermore, we conduct extensive numerical experiments on both synthetic data and real donation data collected from GoFundMe (a crowdfunding platform), demonstrating significant improvements over benchmark state-of-the-art learning algorithms in this setting. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2023.0223 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0223},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2458-2476},
  shortjournal = {Oper. Res.},
  title        = {Online learning with sample selection bias},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Better regularization for sequential decision spaces: Fast convergence rates for nash, correlated, and team equilibria. <em>OR</em>, <em>73</em>(5), 2430-2457. (<a href='https://doi.org/10.1287/opre.2021.0633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the application of iterative first-order methods to the problem of computing equilibria of large-scale extensive-form games. First-order methods must typically be instantiated with a regularizer that serves as a distance-generating function (DGF) for the decision sets of the players. In this paper, we introduce a new weighted entropy-based distance-generating function. We show that this function is equivalent to a particular set of new weights for the dilated entropy distance–generating function on a treeplex while retaining the simpler structure of the regular entropy function for the unit cube. This function achieves significantly better strong-convexity properties than existing weight schemes for the dilated entropy while maintaining the same easily implemented closed-form proximal mapping as the prior state of the art. Extensive numerical simulations show that these superior theoretical properties translate into better numerical performance as well. We then generalize our new entropy distance function, as well as general dilated distance functions, to the scaled extension operator. The scaled extension operator is a way to recursively construct convex sets, which generalizes the decision polytope of extensive-form games as well as the convex polytopes corresponding to correlated and team equilibria. Correspondingly, we give the first efficiently computable distance-generating function for all those strategy polytopes. By instantiating first-order methods with our regularizers, we achieve several new results, such as the first method for computing ex ante correlated team equilibria with a guaranteed 1 / T rate of convergence and efficient proximal updates. Similarly, we show that our regularizers can be used to speed up the computation of correlated solution concepts. Funding: G. Farina was supported by the National Science Foundations [Grant CCF-2443068] and by T. Sandholm’s grants listed below and a Facebook fellowship. C. Kroer was supported by the Office of Naval Research [Grants N00014-22-1-2530 and N00014-23-1-2374] and the National Science Foundation [Grants IIS-2147361 and IIS-2238960]. T. Sandholm was supported by the Vannevar Bush Faculty Fellowship, Office of Naval Research [Grant ONR N00014-23-1-2876], the National Science Foundation Division of Information and Intelligent Systems [Grants RI-1718457, RI-2312342, RI-1901403, and CCF-1733556], the Army Research Office [Grants W911NF2010081 and W911NF2210266], and the National Institutes of Health [Grant A240108S001]. This work was further supported by the National Science Foundation Division of Information and Intelligent Systems [Grant 1617590] and the Army Research Office [Grant W911NF-17-1-0082]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0633 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0633},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2430-2457},
  shortjournal = {Oper. Res.},
  title        = {Better regularization for sequential decision spaces: Fast convergence rates for nash, correlated, and team equilibria},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Change-point detection in dynamic networks with missing links. <em>OR</em>, <em>73</em>(5), 2417-2429. (<a href='https://doi.org/10.1287/opre.2021.0413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural changes occur in dynamic networks quite frequently and their detection is an important question in many situations, such as fraud detection or cybersecurity. Real-life networks are often incompletely observed because of individual nonresponse or network size. In the present paper, we consider the problem of change-point detection at a temporal sequence of partially observed networks. The goal is to test whether there is a change in the network parameters. Our approach is based on the matrix cumulative sum test statistic and allows growing the size of networks. We show that the proposed test is minimax optimal and robust to missing links. We also demonstrate the good behavior of our approach in practice through simulation study and a real-data application. Funding: The work of O. Klopp was funded by the CY Initiative [Grant Investissements d’Avenir Agence Nationale de Recherche-16-Initiatives d’Excellence-0008] and Labex MME-DII [Grant ANR11-LBX-0023-01]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0413 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0413},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2417-2429},
  shortjournal = {Oper. Res.},
  title        = {Change-point detection in dynamic networks with missing links},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal abort policy for mission-critical systems under imperfect condition monitoring. <em>OR</em>, <em>73</em>(5), 2396-2416. (<a href='https://doi.org/10.1287/opre.2022.0643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although most on-demand mission-critical systems are engineered to be reliable to support critical tasks, occasional failures may still occur during missions. To increase system survivability, a common practice is to abort the mission before an imminent failure. We consider optimal mission abort for a system whose deterioration follows a general three-state (normal, defective, failed) semi-Markov chain. The failure is assumed self-revealed, whereas the healthy and defective states have to be inferred from imperfect condition-monitoring data. Because of the non-Markovian process dynamics, optimal mission abort for this partially observable system is an intractable stopping problem. For a tractable solution, we introduce a novel tool of Erlang mixtures to approximate nonexponential sojourn times in the semi-Markov chain. This allows us to approximate the original process by a surrogate continuous-time Markov chain whose optimal control policy can be solved through a partially observable Markov decision process (POMDP). We show that the POMDP optimal policies converge almost surely to the optimal abort decision rules when the Erlang rate parameter diverges. This implies that the expected cost by adopting the POMDP solution converges to the optimal expected cost. Next, we provide comprehensive structural results on the optimal policy of the surrogate POMDP. Based on the results, we develop a modified point-based value iteration algorithm to numerically solve the surrogate POMDP. We further consider mission abort in a multitask setting where a system executes several tasks consecutively before a thorough inspection. Through a case study on an unmanned aerial vehicle, we demonstrate the capability of real-time implementation of our model, even when the condition-monitoring signals are generated with high frequency. Funding: This work was supported in part by the National Science Foundation of China [Grants 72171037, 72471144, 72371161, and 72071071], Singapore MOE AcRF Tier 2 grants [Grants A-8001052-00-00 and A-8002472-00-00], and the Future Resilient Systems project supported by the National Research Foundation Singapore under its CREATE program. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2022.0643 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0643},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2396-2416},
  shortjournal = {Oper. Res.},
  title        = {Optimal abort policy for mission-critical systems under imperfect condition monitoring},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D-optimal orienteering for post-earthquake reconnaissance planning. <em>OR</em>, <em>73</em>(5), 2375-2395. (<a href='https://doi.org/10.1287/opre.2023.0470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immediately following a major earthquake, reconnaissance surveys seek to assess structural damage throughout the region with the help of a limited number of on-ground inspections. The goal is to collect informative and representative data that will guide subsequent relief efforts. We formulate a new type of vehicle routing problem, in which vehicles are tasked with data collection, and the objective function measures data quality using a nonlinear, nonseparable experimental design criterion. We create novel exact methods for this problem and demonstrate their practical potential in a realistic case study using a state-of-the-art earthquake simulator. Funding: J. Wang, I. O. Ryzhov, N. Marković, and G. Ou acknowledge the support of the National Science Foundation (NSF) Division of Civil, Mechanical and Manufacturing Innovation [Grant 2112828]. W. Xie acknowledges the support of the NSF Division of Computing and Communication Foundations [Grant 2246417]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2023.0470 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0470},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2375-2395},
  shortjournal = {Oper. Res.},
  title        = {D-optimal orienteering for post-earthquake reconnaissance planning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stationary mean-field equilibrium model of irreversible investment in a two-regime economy. <em>OR</em>, <em>73</em>(5), 2351-2374. (<a href='https://doi.org/10.1287/opre.2023.0250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a mean-field model of firms competing à la Cournot on a commodity market, where the commodity price is given in terms of a power inverse demand function of the industry-aggregate production. Investment is irreversible and production capacity depreciates at a constant rate. Production is subject to Gaussian productivity shocks, whereas large nonanticipated macroeconomic events driven by a two-state continuous-time Markov chain can change the volatility of the shocks, as well as the price function. Firms wish to maximize expected discounted revenues of production, net of investment, and operational costs. Investment decisions are based on the long-run stationary price of the commodity. We prove existence, uniqueness, and characterization of the stationary mean-field equilibrium of the model. The equilibrium investment strategy is of barrier type, and it is triggered by a couple of endogenously determined investment thresholds, one per state of the economy. We provide a quasi-closed form expression of the stationary density of the state, and we show that our model can produce Pareto distribution of firms’ size. This is a feature that is consistent both with observations at the aggregate level of industries and at the level of a particular industry. We provide evidence that persistent periods of economic downturn increase market concentration. We demonstrate that firms with slowly depreciating production capacities fare better in a stable, average economy, whereas firms with quickly depreciating assets can benefit from sequences of boom and bust. Funding: This work was supported by the Agence Nationale de la Recherche [Grants ANR-19-CE05-0042 and MIRTE ANR-23-EXMA-0011] and the Deutsche Forschungsgemeinschaft [Grant SFB 1283/22021-317210226].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0250},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2351-2374},
  shortjournal = {Oper. Res.},
  title        = {A stationary mean-field equilibrium model of irreversible investment in a two-regime economy},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Political districting to optimize the polsby-popper compactness score with application to voting rights. <em>OR</em>, <em>73</em>(5), 2330-2350. (<a href='https://doi.org/10.1287/opre.2024.1078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the academic literature and in expert testimony, the Polsby-Popper score is the most popular way to measure the compactness of a political district. Given a district with area A and perimeter P , its Polsby-Popper score is given by ( 4 π A ) / P 2 . This score takes values between zero and one, with circular districts achieving a perfect score of one. In this paper, we propose the first mathematical optimization models to draw districts (or districting plans) with optimum Polsby-Popper score. Specifically, we propose new mixed-integer second-order cone programs (MISOCPs), which can be solved with existing optimization software. Experiments show that they can identify the most compact single districts at the precinct level and the most compact plans at the county level. Then, we turn to the problem of drawing compact plans with a large number of majority-minority districts. This is the task faced by plaintiffs in Voting Rights Act cases who must show that an alternative plan exists in which the minority group could achieve better representation, a legal hurdle known as the first Gingles precondition. For this task, we propose new MISOCP-based heuristics that often outperform enacted maps on standard criteria, sometimes by substantial margins. They also perform well against state-of-the-art heuristics like short bursts and can be used to polish maps with hundreds of thousands of census blocks. Our techniques could assist plaintiffs when seeking to overturn maps that dilute the voting strength of minority groups. Our code is available on GitHub. Funding: This work was supported by the National Science Foundation [Grant 1942065].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1078},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2330-2350},
  shortjournal = {Oper. Res.},
  title        = {Political districting to optimize the polsby-popper compactness score with application to voting rights},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the almost threshold policy for multisourcing under uncertain supplies. <em>OR</em>, <em>73</em>(5), 2319-2329. (<a href='https://doi.org/10.1287/opre.2024.1193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With extended supply chains and increased global sourcing, the uncertainty in supply streams has become a major consideration in formulating procurement strategies. Many studies in the existing literature characterize the optimal procurement policies under specific assumptions of the supply and demand distributions. In several special cases, a threshold policy or an almost threshold policy is shown to be optimal. A recent study by Feng and Shathikumar [Feng Q, Shanthikumar JG (2018) Supply and demand functions in inventory models. Oper. Res. 66(1):77–91] generalizes the previous results by establishing the optimality of an almost threshold policy under any demand distribution and a set of conditions on the stochastic supply functions, including stochastic linearity in midpoint, stochastic increasing in the dispersive order, and vanishing of the no-delivery probability. In this note, we significantly enhance this result and for the first time, fully characterize the optimal multisourcing policy for general stochastic supply functions that are stochastically linear in midpoint, the weakest known condition to ensure the concavity of the associated dynamic program. Using an iterative constructive approach, we prove that an almost threshold policy is optimal. This result is extended to price-dependent demands and positively dependent supplies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1193},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2319-2329},
  shortjournal = {Oper. Res.},
  title        = {On the almost threshold policy for multisourcing under uncertain supplies},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Near-optimal mixed (s,S) policy for a multiwarehouse, multistore inventory system with lost sales and fixed cost. <em>OR</em>, <em>73</em>(5), 2306-2318. (<a href='https://doi.org/10.1287/opre.2024.0717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a firm managing a multiperiod, multiwarehouse, multistore (MWMS) inventory problem with fixed ordering cost at each store over a finite time horizon. The warehouses are endowed with initial inventories at the start of the horizon, and the stores are periodically replenished from the warehouses. The decisions are the order quantities from each store at each period. The optimal policy for this problem is complex and computationally intractable. We construct a mixed ( s , S ) policy based on the optimal solutions of a Lagrangian relaxation. Under this policy, each store makes use of at most two ( s , S ) policies; one is applied during the first phase of the selling horizon, and the second is applied in the remaining periods. We prove that this policy is near optimal as the length of the time horizon grows. In contrast to the existing works on the MWMS problem without fixed cost for which near-optimal policies can be developed using an optimal Lagrangian solution, with fixed cost, it is crucial to adopt a mixture of Lagrangian solutions, and simply applying a pure optimal Lagrangian solution can be highly suboptimal. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2024.0717 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.0717},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2306-2318},
  shortjournal = {Oper. Res.},
  title        = {Near-optimal mixed (s,S) policy for a multiwarehouse, multistore inventory system with lost sales and fixed cost},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-item online order fulfillment in a two-layer network. <em>OR</em>, <em>73</em>(5), 2297-2305. (<a href='https://doi.org/10.1287/opre.2022.0100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global e-commerce boom has driven rapid expansion of fulfillment infrastructure, with e-retailers building more warehouses to offer faster deliveries. However, fulfillment costs have surged over the past decade. This paper addresses the problem of minimizing these costs, where an e-retailer must decide in real time which warehouse(s) will fulfill each order, considering inventory constraints. Orders can be split among warehouses at an additional cost. We focus on a regional distribution center (RDC)–front distribution center (FDC) distribution network used by major e-retailers, which consists of larger RDCs and smaller FDCs. We analyze the performance of a simple myopic policy that selects the least expensive fulfillment option for each order without considering future impacts. We provide theoretical bounds on the performance ratio of the myopic policy compared with an optimal clairvoyant policy and demonstrate the strengths of the myopic policy within this two-layer network. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2022.0100 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0100},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2297-2305},
  shortjournal = {Oper. Res.},
  title        = {Multi-item online order fulfillment in a two-layer network},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acknowledgment to referees (2024). <em>OR</em>, <em>73</em>(4), iii. (<a href='https://doi.org/10.1287/opre.2025.apprec.v73.n4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OR},
  doi          = {10.1287/opre.2025.apprec.v73.n4},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {iii},
  shortjournal = {Oper. Res.},
  title        = {Acknowledgment to referees (2024)},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Off-line estimation of controlled markov chains: Minimaxity and sample complexity. <em>OR</em>, <em>73</em>(4), 2281-2295. (<a href='https://doi.org/10.1287/opre.2023.0046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study a natural nonparametric estimator of the transition probability matrices of a finite controlled Markov chain. We consider an off-line setting with a fixed data set of size m , collected using a so-called logging policy. We develop sample complexity bounds for the estimator and establish conditions for minimaxity. Our statistical bounds depend on the logging policy through its mixing properties. We show that achieving a particular statistical risk bound involves a subtle and interesting trade-off between the strength of the mixing properties and the number of samples. We demonstrate the validity of our results under various examples, such as ergodic Markov chains; weakly ergodic inhomogeneous Markov chains; and controlled Markov chains with nonstationary Markov, episodic, and greedy controls. Lastly, we use these sample complexity bounds to establish concomitant ones for off-line evaluation of stationary Markov control policies. Funding: I. Banerjee was supported in part by the Ross-Lynn fellowship and McLean scholarship at Purdue University. H. Honnappa was partly supported by the National Science Foundation [Grants CAREER/2143752, DMS/1812197 and DMS/2153915]. V. Rao was supported by the National Science Foundation [Grants RI/1816499 and DMS/1812197]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0046 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0046},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2281-2295},
  shortjournal = {Oper. Res.},
  title        = {Off-line estimation of controlled markov chains: Minimaxity and sample complexity},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convex chance-constrained programs with wasserstein ambiguity. <em>OR</em>, <em>73</em>(4), 2264-2280. (<a href='https://doi.org/10.1287/opre.2021.0709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chance constraints yield nonconvex feasible regions in general. In particular, when the uncertain parameters are modeled by a Wasserstein ball, existing studies showed that the distributionally robust (pessimistic) chance constraint admits a mixed-integer conic representation. This paper identifies sufficient conditions that lead to convex feasible regions of chance constraints with Wasserstein ambiguity. First, when uncertainty arises from the right-hand side of a pessimistic joint chance constraint, we show that the ensuing feasible region is convex if the Wasserstein ball is centered around a log-concave distribution (or, more generally, an α -concave distribution with α ≥ − 1 ). In addition, we propose a block coordinate ascent algorithm and prove its convergence to global optimum, as well as the rate of convergence. Second, when uncertainty arises from the left-hand side of a pessimistic two-sided chance constraint, we show the convexity if the Wasserstein ball is centered around an elliptical and star unimodal distribution. In addition, we propose a family of second-order conic inner approximations, and we bound their approximation error and prove their asymptotic exactness. Furthermore, we extend the convexity results to optimistic chance constraints. Funding: This work was supported by the National Science Foundation [Grants ECCS-1845980, OIA-2119691, and OIA-1946391] and the Air Force Office of Scientific Research [Grant FA9550-23-1-0323]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0709 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0709},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2264-2280},
  shortjournal = {Oper. Res.},
  title        = {Convex chance-constrained programs with wasserstein ambiguity},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic control of service systems with returns: Application to design of postdischarge hospital readmission prevention programs. <em>OR</em>, <em>73</em>(4), 2242-2263. (<a href='https://doi.org/10.1287/opre.2022.0066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a control problem for queueing systems in which customers may return for additional episodes of service after their initial service completion. At each service completion epoch, the decision maker can choose to reduce the probability of return for the departing customer but at a cost that is convex increasing in the amount of reduction in the return probability. Other costs are incurred as customers wait in the queue and every time they return for service. Our primary motivation comes from postdischarge quality improvement interventions (e.g., follow-up phone calls, outpatient appointments) frequently used in a variety of healthcare settings to reduce unplanned hospital readmissions. Our objective is to understand how the cost of interventions should be balanced with the reductions in congestion and service costs. To this end, we consider a fluid approximation of the queueing system and characterize the structure of optimal long-run average and bias-optimal transient control policies for the fluid model. Our structural results motivate the design of intuitive surge protocols whereby different intensities of interventions (corresponding to different levels of reduction in the return probability) are provided based on the congestion in the system. Through extensive simulation experiments, we study the performance of the fluid policy for the stochastic system and identify parameter regimes in which it leads to significant cost savings compared with a fixed long-run average optimal policy that ignores holding costs and a simple policy that uses the highest level of intervention whenever the queue is nonempty. In particular, we find that, in a parameter regime relevant to our motivating application, dynamically adjusting the intensity of interventions could result in up to 25.4% reduction in long-run average cost and 33.7% in finite-horizon costs compared with the simple aggressive policy. Funding: V. Sarhangian was supported by the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2018-04518]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0066 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0066},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2242-2263},
  shortjournal = {Oper. Res.},
  title        = {Dynamic control of service systems with returns: Application to design of postdischarge hospital readmission prevention programs},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strong optimal classification trees. <em>OR</em>, <em>73</em>(4), 2223-2241. (<a href='https://doi.org/10.1287/opre.2021.0034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are among the most popular machine learning models and are used routinely in applications ranging from revenue management and medicine to bioinformatics. In this paper, we consider the problem of learning optimal binary classification trees with univariate splits. Literature on the topic has burgeoned in recent years, motivated both by the empirical suboptimality of heuristic approaches and the tremendous improvements in mixed-integer optimization (MIO) technology. Yet, existing MIO-based approaches from the literature do not leverage the power of MIO to its full extent: they rely on weak formulations, resulting in slow convergence and large optimality gaps. To fill this gap in the literature, we propose an intuitive flow-based MIO formulation for learning optimal binary classification trees. Our formulation can accommodate side constraints to enable the design of interpretable and fair decision trees. Moreover, we show that our formulation has a stronger linear optimization relaxation than existing methods in the case of binary data. We exploit the decomposable structure of our formulation and max-flow/min-cut duality to derive a Benders’ decomposition method to speed-up computation. We propose a tailored procedure for solving each decomposed subproblem that provably generates facets of the feasible set of the MIO as constraints to add to the main problem. We conduct extensive computational experiments on standard benchmark data sets on which we show that our proposed approaches are 29 times faster than state-of-the-art MIO-based techniques and improve out-of-sample performance by up to 8%. Funding: P. Vayanos and S. Aghaei gratefully acknowledge support from the Hilton C. Foundation, the Homeless Policy Research Institute, the Home for Good foundation under the “C.E.S. Triage Tool Research & Refinement” grant. P. Vayanos is funded in part by the National Science Foundation, under CAREER [Grant 2046230]. She is grateful for this support. A. Gómez is funded in part by the National Science Foundation under [Grants 1930582 and 2006762]. Supplemental Material: The online appendix and code files are available at https://doi.org/10.1287/opre.2021.0034 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0034},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2223-2241},
  shortjournal = {Oper. Res.},
  title        = {Strong optimal classification trees},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonadaptive stochastic score classification and explainable half-space evaluation. <em>OR</em>, <em>73</em>(4), 2204-2222. (<a href='https://doi.org/10.1287/opre.2023.0431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential testing problems involve a complex system with several components, each of which is “working” with some independent probability. The outcome of each component can be determined by performing a test, which incurs some cost. The overall system status is given by a function f of the outcomes of its components. The goal is to evaluate this function f by performing tests at the minimum expected cost. Although there has been extensive prior work on this topic, provable approximation bounds are mainly limited to simple functions, like “ k -out-of- n ” and half-spaces. We consider significantly more general “score classification” functions, and we provide the first constant-factor approximation algorithm (improving over a previous logarithmic approximation ratio). Moreover, our policy is nonadaptive; it just involves performing tests in an a priori fixed order. We also consider the related half-space evaluation problem, where we want to evaluate some function on d half-spaces (e.g., the intersection of half-spaces). We show that our approach provides an O ( d 2 log d ) -approximation algorithm for this problem. Our algorithms also extend to the setting of “batched” tests, where multiple tests can be performed simultaneously while incurring an extra setup cost. Finally, we perform computational experiments that demonstrate the practical performance of our algorithm for score classification. We observe that, for most instances, the cost of our algorithm is within a factor of 1.5 of an information-theoretic lower bound on the optimal value. Funding: This work was supported by the Division of Computing and Communication Foundations [Grants CCF-2006778 and CCF-2006953] and the Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-1940766].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0431},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2204-2222},
  shortjournal = {Oper. Res.},
  title        = {Nonadaptive stochastic score classification and explainable half-space evaluation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logarithmic regret in multisecretary and online linear programs with continuous valuations. <em>OR</em>, <em>73</em>(4), 2188-2203. (<a href='https://doi.org/10.1287/opre.2022.0036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I use empirical processes to study how the shadow prices of a linear program that allocates an endowment of n β ∈ R m resources to n customers behave as n → ∞ . I show the shadow prices (i) adhere to a concentration of measure, (ii) converge to a multivariate normal under central-limit-theorem scaling, and (iii) have a variance that decreases like Θ ( 1 / n ) . I use these results to prove that the expected regret in an online linear program is Θ ( log n ) , both when the customer variable distribution is known upfront and must be learned on the fly. This result tightens the sharpest known upper bound from O ( log n log log n ) to O ( log n ) , and it extends the Ω ( log n ) lower bound known for single-dimensional problems to the multidimensional setting. I illustrate my new techniques with a simple analysis of a multisecretary problem. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0036 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0036},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2188-2203},
  shortjournal = {Oper. Res.},
  title        = {Logarithmic regret in multisecretary and online linear programs with continuous valuations},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardness of pricing routes for two-stage stochastic vehicle routing problems with scenarios. <em>OR</em>, <em>73</em>(4), 2177-2187. (<a href='https://doi.org/10.1287/opre.2023.0569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem with stochastic demands (VRPSD) generalizes the classic vehicle routing problem by considering customer demands as random variables. Similar to other vehicle routing variants, state-of-the-art algorithms for the VRPSD are often based on set-partitioning formulations, which require efficient routines for the associated pricing problems. However, all of these set partitioning–based approaches have strong assumptions on the correlation between the demands of random variables (e.g., no correlation), a simplification that diverges from real-world settings where correlations frequently exist. In contrast, there is a significant effort in the stochastic programming community to solve problems where the uncertainty is modeled with a finite set of scenarios. This approach can approximate more diverse distributions via sampling and is particularly appealing in data-driven contexts where historical data are readily available. To fill this gap, we focus on the VRPSD with demands given by scenarios. We show that for any route relaxation (where repeated visits are allowed in a route) and any approximation of the recourse cost that satisfies some mild assumptions, the VRPSD pricing problem is still strongly N P -hard. This provides a very strong argument for the difficulty of developing efficient column generation–based algorithms for the VRPSD with demands following an empirical probability distribution of scenarios. Funding: This work was supported by Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2020-04030].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0569},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2177-2187},
  shortjournal = {Oper. Res.},
  title        = {Hardness of pricing routes for two-stage stochastic vehicle routing problems with scenarios},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving blockchain consistency bound by assigning weights to random blocks. <em>OR</em>, <em>73</em>(4), 2156-2176. (<a href='https://doi.org/10.1287/opre.2022.0463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchains based on the celebrated Nakamoto consensus protocol have shown promise in several applications, including cryptocurrencies. However, these blockchains have inherent scalability limits caused by the protocol’s consensus properties. In particular, the consistency property demonstrates a tight trade-off between block production speed and the system’s security in terms of resisting adversarial attacks. As such, this paper proposes a novel method called Ironclad, which improves the blockchain consistency bound by assigning a different weight to randomly selected blocks. We apply our method to the original Nakamoto protocol and rigorously prove that such a combination can significantly improve the consistency bound by analyzing the fundamental consensus properties. This kind of improvement enables a much faster block production rate than the original Nakamoto protocol but with the same security guarantee. Funding: This work was supported in part by the WeBank-Hong Kong University of Science and Technology Joint Lab [Project WEB19EG01-M]. The research of J. Zhang was supported in part by the Hong Kong Research Grants Council [Grants 16208120 and 16214121]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0463 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0463},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2156-2176},
  shortjournal = {Oper. Res.},
  title        = {Improving blockchain consistency bound by assigning weights to random blocks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A short and general duality proof for wasserstein distributionally robust optimization. <em>OR</em>, <em>73</em>(4), 2146-2155. (<a href='https://doi.org/10.1287/opre.2023.0135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general duality result for Wasserstein distributionally robust optimization that holds for any Kantorovich transport cost, measurable loss function, and nominal probability distribution. Assuming an interchangeability principle inherent in existing duality results, our proof only uses one-dimensional convex analysis. Furthermore, we demonstrate that the interchangeability principle holds if and only if certain measurable projection and weak measurable selection conditions are satisfied. To illustrate the broader applicability of our approach, we provide a rigorous treatment of duality results in distributionally robust Markov decision processes and distributionally robust multistage stochastic programming. Additionally, we extend our analysis to other problems such as infinity-Wasserstein distributionally robust optimization, risk-averse optimization, and globalized distributionally robust counterpart. Funding: L. Zhang acknowledges the support of Xunyu Zhou and the Nie Center for Intelligent Asset Management at Columbia University. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0135 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0135},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2146-2155},
  shortjournal = {Oper. Res.},
  title        = {A short and general duality proof for wasserstein distributionally robust optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-adaptive local decision rules. <em>OR</em>, <em>73</em>(4), 2125-2145. (<a href='https://doi.org/10.1287/opre.2023.0564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For parameterized mixed-binary optimization problems, we construct local decision rules that prescribe near-optimal courses of action across a set of parameter values. The decision rules stem from solving risk-adaptive training problems over classes of continuous, possibly nonlinear mappings. In asymptotic and nonasymptotic analysis, we establish that the decision rules prescribe near-optimal decisions locally for the actual problems without relying on linearity, convexity, or smoothness. The development also accounts for practically important aspects such as inexact function evaluations, solution tolerances in training problems, regularization, and reformulations to solver-friendly models. The decision rules also furnish a means to carry out sensitivity and stability analysis for broad classes of parameterized optimization problems. We develop a decomposition algorithm for solving the resulting training problems and demonstrate its ability to generate quality decision rules on a nonlinear binary optimization model from search theory. Funding: J. O. Royset is supported in part by the Office of Naval Research (ONR) [Grant N000142412277]. M. A. Lejeune was supported by the National Science Foundation [Grants DMS-2318519 and ECCS-2114100], and the Office of Naval Research [Grant N00014-22-1-2649]. Supplemental Material: The computer code and data that support the findings of this study and the online appendix are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0564 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0564},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2125-2145},
  shortjournal = {Oper. Res.},
  title        = {Risk-adaptive local decision rules},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic allocation of reusable resources: Logarithmic regret in overloaded networks. <em>OR</em>, <em>73</em>(4), 2097-2124. (<a href='https://doi.org/10.1287/opre.2022.0429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of dynamically allocating reusable resources to customers of n types. There are d pools of resources and a finite number of units from each resource. If a customer request is accepted, the decision maker collects a type-dependent reward, and the customer occupies, for a random service time, one unit from each resource in a set of these. Upon service completion, these resource units become available for future allocation. This is a loss network: requests that are not accepted leave immediately. The decision maker’s objective is to maximize the long-run average reward subject to the resource capacity constraint. A natural linear programming (LP) relaxation of the problem serves as an upper bound on the performance of any policy. We identify a condition that generalizes the notion of overload in single-resource networks (i.e., when d = 1 ). The LP guides our construction of a threshold policy. In this policy, the number of thresholds equals the number of resource types (hence, does not depend on the number of customer types). These thresholds are applied to a “corrected” headcount process. In the case of a single resource, the corrected head count is the number of resource units that are occupied. We prove that, in overloaded networks, the additive loss (or regret) of this policy benchmarked against the LP upper bound is logarithmic in the total arrival volume in the high-customer-volume, many-resource-units, asymptotic regime. No policy can achieve sublogarithmic regret. Simulations showcase the performance of the proposed policy. Funding: X. Xie and I. Gurvich were supported by an Amazon Research Award and DRAGONS – Dynamic Resource Allocation Gains for Operational Networked Sharing, Department of Defense (Army) [Grant STTR A18B-T007]. S. Küçükyavuz was supported by an Office of Naval Research [Grant N00014-22-1-2602]. Supplemental Material: Data and code files are available at https://doi.org/10.1287/opre.2022.0429 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0429},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2097-2124},
  shortjournal = {Oper. Res.},
  title        = {Dynamic allocation of reusable resources: Logarithmic regret in overloaded networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal regularized online allocation by adaptive re-solving. <em>OR</em>, <em>73</em>(4), 2079-2096. (<a href='https://doi.org/10.1287/opre.2022.0486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a dual-based algorithm framework for solving the regularized online resource allocation problems, which have potentially nonconcave cumulative rewards, hard resource constraints, and a nonseparable regularizer. Under a strategy of adaptively updating the resource constraints, the proposed framework only requests approximate solutions to the empirical dual problems up to a certain accuracy and yet delivers an optimal logarithmic regret under a locally second-order growth condition. Surprisingly, a delicate analysis of the dual objective function enables us to eliminate the notorious log-log factor in regret bound. The flexible framework renders renowned and computationally fast algorithms immediately applicable, for example, dual stochastic gradient descent. Additionally, an infrequent re-solving scheme is proposed, which significantly reduces computational demands without compromising the optimal regret performance. A worst-case square-root regret lower bound is established if the resource constraints are not adaptively updated during dual optimization, which underscores the critical role of adaptive dual variable update. Comprehensive numerical experiments demonstrate the merits of the proposed algorithm framework. Funding: This work was supported by the National Foreign Expert Project [G2022030026], the Research Grants Council, and the University Grants Committee [Grants GRF 16211220, GRF 16300121, and GRF 16301622]. Supplemental Material: The online appendices and code files are available at https://doi.org/10.1287/opre.2022.0486 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0486},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2079-2096},
  shortjournal = {Oper. Res.},
  title        = {Optimal regularized online allocation by adaptive re-solving},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically optimal clearing control of backlogs in multiclass processing systems. <em>OR</em>, <em>73</em>(4), 2061-2078. (<a href='https://doi.org/10.1287/opre.2022.0570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a dynamic scheduling problem for a processing system facing the problem of optimally clearing a large backlog of unsatisfied demand from several classes of customers (or jobs). We formulate the problem as a multiclass queueing model with a large initial queue and arrival rates that approximately equal the system’s processing capacity. The goal is to find a scheduling policy that minimizes a holding-and-abandonment cost during the transient period in which the system is considered congested. Because computing an exact solution to the optimal-control problem is infeasible, we develop a unified asymptotic approximation that covers, in particular, the conventional and the many-server heavy-traffic regimes. In addition to the generality and flexibility of our unified asymptotic framework, we also prove a strong form of asymptotic optimality, under which the costs converge in expectation and in probability. In particular, for the special two-class case, we prove that a static priority policy, which follows a discounted c μ / θ rule, is asymptotically optimal. When there are more than two classes of customers, we show that any admissible control that follows the best-effort rule, which gives the lowest priority to one of the classes according to the discounted c μ / θ ordering, becomes asymptotically optimal after some relatively short time period. Finally, using heuristic arguments and insights from our analyses, we propose scheduling policies that build on the best-effort rule. An extensive numerical study shows that those proposed policies are effective and provides guidance as to when to use either policy in practice. Funding: O. Perry and L. Yu were partially supported by NSF [Grants CMMI 1763100 and CMMI 2006350]. L. Yu is currently supported by NSFC [Grants 72201153, 72242106, and 72394361]. Supplemental Material: The online appendix and code are available at https://doi.org/10.1287/opre.2022.0570 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0570},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2061-2078},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically optimal clearing control of backlogs in multiclass processing systems},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal dynamic mechanism under customer search. <em>OR</em>, <em>73</em>(4), 2045-2060. (<a href='https://doi.org/10.1287/opre.2022.0136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the seller’s revenue-maximizing mechanism in the face of a customer who searches for outside alternatives over a finite horizon. The customer’s utility from searches is modeled as a general function—referred to as the recall function—of the past search outcomes. Without observing the customer’s valuation of the product or any realization of search outcomes, the seller can propose and commit to a contract with the customer before the search process begins. Under a general recall function, we show that the optimal strategy for the seller is to offer a menu of American options consisting of deposits and strike prices. In the case in which the customer can only recall a few recent outside alternatives, we further establish that, under the optimal mechanism, customers with low valuation search for outside alternatives without engaging with the seller, whereas high-valuation customers exercise the option immediately, effectively turning the option into an exploding offer. Customers with intermediate valuation only exercise the option, if ever, at the end of the search horizon. Whereas a longer search horizon or smaller search cost both increase the customer’s utility from searches, they have different impacts on the seller’s revenue. More search opportunities lead to an exponential decrease in the seller’s revenue, and in the limit, the optimal mechanism converges to a posted price mechanism. In contrast, as the search cost increases, the seller’s revenue may initially decrease and then increase. In the extreme case in which the search cost exceeds the average value of outside alternatives, the customer’s sequential search problem reduces to strategically timing purchases of the seller’s product. Our optimal mechanism, in this case, reduces to making a single exploding offer with a monopoly price. Funding: This work is funded in part by the Ministry of Education, Singapore, under its 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0136 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0136},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2045-2060},
  shortjournal = {Oper. Res.},
  title        = {Optimal dynamic mechanism under customer search},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boundary effects in the diffusion of new products on cartesian networks. <em>OR</em>, <em>73</em>(4), 2026-2044. (<a href='https://doi.org/10.1287/opre.2022.0004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the effect of boundaries in the discrete Bass model on D -dimensional Cartesian networks. In two dimensions, this model describes the diffusion of new products that spread primarily by spatial peer effects, such as residential photovoltaic solar systems. We show analytically that nodes (residential units) that are located near the boundary are less likely to adopt than centrally located ones. This boundary effect is local and decays exponentially with the distance from the boundary. At the aggregate level, boundary effects reduce the overall adoption level. The magnitude of this reduction scales as 1 M 1 / D , where M is the number of nodes. Our analysis is supported by empirical evidence on the effect of boundaries on the adoption of solar. Funding: This material is based upon work supported by the Department of Energy (DOE) [Grant DE-EE0007657]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0004 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0004},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2026-2044},
  shortjournal = {Oper. Res.},
  title        = {Boundary effects in the diffusion of new products on cartesian networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic relocations in car-sharing networks. <em>OR</em>, <em>73</em>(4), 2010-2025. (<a href='https://doi.org/10.1287/opre.2021.0062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel dynamic car relocation policy for a car-sharing network with centralized control and uncertain, unbalanced demand. The policy is derived from a reformulation of the linear programming fluid model approximation of the dynamic problem. We project the full-dimensional fluid approximation onto the lower-dimensional space of relocation decisions only. This projection results in a characterization of the problem as n + 1 linear programs, where n is the number of nodes in the network. The reformulation uncovers structural properties that are interpretable using absorbing Markov chain concepts and allows us to write the gradient with respect to the relocation decisions in closed form. Our policy exploits these gradients to make dynamic car relocation decisions. We provide extensive numerical results on hundreds of random networks where our dynamic car relocation policy consistently outperforms the standard static policy. Our policy reduces the optimality gap in steady state by more than 23% on average. Also, in a short-term, time-varying setting, the lookahead version of our dynamic policy outperforms the static lookahead policy slightly more than in the time-homogeneous tests. Funding: This work was supported by the Natural Sciences and Engineering Research Council of Canada [Grants RGPGP-2015-00050 and RGPIN-2018-04561]. Supplemental Material: The computer code, data, and e-companion that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0062 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0062},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2010-2025},
  shortjournal = {Oper. Res.},
  title        = {Dynamic relocations in car-sharing networks},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Establishing convergence of infinite-server queues with batch arrivals to shot-noise processes. <em>OR</em>, <em>73</em>(4), 2002-2009. (<a href='https://doi.org/10.1287/opre.2023.0353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Across domains as diverse as communication channels, computing systems, and public health management, a myriad of real-world queueing systems receive batch arrivals of jobs or customers. In this work, we show that under a natural scaling regime, both the queue-length process and the workload process associated with a properly scaled sequence of infinite-server queueing systems with batch arrivals converge almost surely, uniformly on compact sets, to shot-noise processes. Given the applicability of these models, our relatively direct and accessible methodology may also be of independent interest, where we invoke the Glivenko–Cantelli theorem when the Strong Law of Large Numbers fails to hold for the queue-length batch scaling yet then, exploit the continuity of stationary excess distributions and the classic strong law when the Glivenko–Cantelli theorem fails to hold in the workload batch scaling. These results strengthen a convergence result recently established in the work of de Graaf et al. [de Graaf WF, Scheinhardt WR, Boucherie RJ (2017) Shot-noise fluid queues and infinite-server systems with batch arrivals. Performance Evaluation 116:143–155] in multiple ways, and furthermore, they provide new insight into how the queue-length and workload limits differ from one another.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0353},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {2002-2009},
  shortjournal = {Oper. Res.},
  title        = {Establishing convergence of infinite-server queues with batch arrivals to shot-noise processes},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partition and prosper: Design and pricing of single bundle. <em>OR</em>, <em>73</em>(4), 1983-2001. (<a href='https://doi.org/10.1287/opre.2022.0465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product bundling is a widely used selling strategy among multiproduct firms, yet designing and pricing bundles optimally remain a complex challenge. This paper addresses this fundamental issue by exploring the selection and pricing of a single bundle from a range of products. For instance, in the single bundle with the rest (SBR) framework, the bundle is optimally chosen and priced, whereas the remaining products are offered individually, collectively maximizing profit. We show that the SBR optimization problem under multivariate normal valuations is polynomial-time solvable, provided that the associated covariance matrix can be decomposed into a positive diagonal matrix minus a positive semidefinite matrix of (small) fixed rank. Interestingly, we also show that the subproblem of SBR optimization, where individual product prices are predetermined, is N P -hard, even if customer valuations are independent. Building on these results, we use a Bayesian optimization (BO) algorithm combined with a conic integer programming reformulation to solve the general SBR optimization problem under correlated valuations. We further show that SBR is a constant approximation to more complex mechanisms in terms of profit performance. Extensive numerical results demonstrate that our BO algorithm has superior performance over baseline heuristics, and SBR achieves significantly higher profit than separate selling and grand bundling. Interestingly, simulation studies reveal that allowing customers the additional option to purchase products either as part of a bundle or individually enhances social welfare (i.e., increases both profit and customer surplus) compared with SBR, separate selling, and grand bundling. These findings highlight the potential benefits of bundle pricing strategies in achieving improved outcomes for both firms and customers. Funding : H. Sun is supported by the National Natural Science Foundation of China [Grant 72301168] and the Shanghai Pujiang Programme [Grant 23PJC062]. X. Li is supported by the Singapore Ministry of Education [Tier 1 Grant 23-0619-P0001 and Tier 1 Grant 24-0500-A0001] and the National Natural Science Foundation of China [Grant 72171156]. C.-P. Teo is supported by the National Research Foundation Singapore [Grant I2001E0059] and the Singapore Ministry of Education [Grant MOE-2019-T3-1-010]. C.-P. Teo is also supported by the Natural Science Foundation of Chongqing, China [Grant CSTB2022NSCQ-MSX1667]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2022.0465 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0465},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1983-2001},
  shortjournal = {Oper. Res.},
  title        = {Partition and prosper: Design and pricing of single bundle},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian and randomized clock auctions. <em>OR</em>, <em>73</em>(4), 1965-1982. (<a href='https://doi.org/10.1287/opre.2022.0421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a single-parameter mechanism design problem, a provider is looking to sell some service to a group of potential buyers. Each buyer i has a private value v i for receiving this service, but a feasibility constraint restricts which buyers can be simultaneously served. Recent work in economics introduced (deferred-acceptance) clock auctions as a superior class of auctions for this problem due to their transparency, simplicity, and strong incentive guarantees. Subsequent work focused on evaluating these auctions in terms of worst-case social welfare approximation, leading to strong impossibility results: Without prior information regarding buyers’ values, deterministic clock auctions cannot achieve bounded approximations, even for feasibility constraints comprising two maximal feasible sets. We demonstrate how to circumvent these negative results by leveraging prior information or randomization . In particular, we provide clock auctions that give an O ( log log k ) -approximation for arbitrary downward-closed feasibility constraints with k maximal feasible sets for three different information regimes. The more prior information we have access to, the simpler the proposed auctions. In addition, we propose a parametrization of the complexity of clock auctions, paving the way for exciting future research. Funding: This work was supported by the Simons Foundation [Grant 820931], the Science and Technology Innovation 2030 [Grant 2018AAA0100903], the National Natural Science Foundation of China [Grant 62150610500], the Central University Basic Research Fund of China, the National Science Foundation [Grants CCF-1755955 and CCF-2008280], the H2020 European Research Council [Grant 866132], and the Israel Science Foundation [Grant 317/17]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0421 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0421},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1965-1982},
  shortjournal = {Oper. Res.},
  title        = {Bayesian and randomized clock auctions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian mechanism design for blockchain transaction fee allocation. <em>OR</em>, <em>73</em>(4), 1944-1964. (<a href='https://doi.org/10.1287/opre.2024.0865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In blockchain systems, the design of transaction fee mechanisms (TFMs) is essential for stability and satisfaction for both miners and users. A recent work has proven the impossibility of collusion-proof mechanisms that achieve both nonzero miner revenue and Dominant Strategy Incentive Compatibility (DSIC) for users. However, a positive miner revenue is important in practice to motivate miners. To address this challenge, we consider a Bayesian game setting and relax the DSIC requirement for users to Bayesian Nash Incentive Compatibility (BNIC). In particular, we propose an auxiliary mechanism method that makes connections between BNIC and DSIC mechanisms. With the auxiliary mechanism method, we design a TFM based on the multinomial logit (MNL) choice model, and prove that the TFM has both BNIC and collusion-proof properties with an asymptotic constant-factor approximation of optimal miner revenue for i.i.d. bounded valuations. Our result breaks the zero-revenue barrier while preserving truthfulness and collusion-proof properties. Funding: X. Chen thanks the NSF [Grant IIS-1845444] for support. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2024.0865 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.0865},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1944-1964},
  shortjournal = {Oper. Res.},
  title        = {Bayesian mechanism design for blockchain transaction fee allocation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—On dynamic pricing with covariates. <em>OR</em>, <em>73</em>(4), 1932-1943. (<a href='https://doi.org/10.1287/opre.2021.0802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider dynamic pricing with covariates under a generalized linear demand model: A seller can dynamically adjust the price of a product over a horizon of T time periods, and at each time period t , the demand of the product is jointly determined by the price and an observable covariate vector x t ∈ R d through a generalized linear model with unknown coefficients. Most of the existing literature assumes the covariate vectors x t s are independently and identically distributed (i.i.d.); the few papers that relax this assumption either sacrifice model generality or yield suboptimal regret bounds. In this paper, we show that Upper Confidence Bound and Thompson sampling-based pricing algorithms can achieve an O ( d T log T ) regret upper bound without assuming any statistical structure on the covariates x t . Our upper bound on the regret matches the lower bound up to logarithmic factors. We thus show that (i) the i.i.d. assumption is not necessary for obtaining low regret, and (ii) the regret bound can be independent of the (inverse) minimum eigenvalue of the covariance matrix of the x t s, a quantity present in previous bounds. Moreover, we consider a constrained setting of the dynamic pricing problem where there is a limited and unreplenishable inventory, and we develop theoretical results that relate the best achievable algorithm performance to a variation measure with respect to the temporal distribution shift of the covariates. We also demonstrate the proposed algorithms’ performance with numerical experiments. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0802 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0802},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1932-1943},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On dynamic pricing with covariates},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Community-engaged school district design: A stream-based approach. <em>OR</em>, <em>73</em>(4), 1916-1931. (<a href='https://doi.org/10.1287/opre.2022.0621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehensive community engagement in public school district design is essential to create equitable and effective enrollment policies reflective of community needs and values. We revisit the school district design problem with a focus on codesigning with community partners. We introduce a new compact formulation that incorporates multiple decisions simultaneously by assigning students in each geographic unit to a set of schools (e.g., elementary, middle, and high schools, and schools with specialized programming) with a single composite variable, referred to as a “stream.” This formulation is computationally efficient and easily reconfigurable for evolving problem specifications that are endemic to community codesign. These features were essential in the district redesign process described in this paper, allowing the community to iteratively develop proposals to address inequities in access to education and improve the student assignment process. Funding: This work was supported by the National Science Foundation [Grant CMMI-1727744] and Northwestern University’s McCormick Catalyst Fund.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0621},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1916-1931},
  shortjournal = {Oper. Res.},
  title        = {Community-engaged school district design: A stream-based approach},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically optimal competitive ratio for online allocation of reusable resources. <em>OR</em>, <em>73</em>(4), 1897-1915. (<a href='https://doi.org/10.1287/opre.2021.0695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of online allocation (matching, budgeted allocations, and assortments) of reusable resources for which an adversarial sequence of resource requests is revealed over time and any allocated resource is used/rented for a stochastic duration drawn independently from a resource-dependent usage distribution. Previously, it was known that a greedy algorithm is 0.5-competitive against the clairvoyant benchmark that knows the entire sequence of requests in advance. We give a novel algorithm that is ( 1 − 1 / e ) -competitive for arbitrary usage distributions when the starting capacity of each resource is large and the usage distributions are known. This is the best achievable competitive ratio guarantee for the problem; that is, no online algorithm can have a better competitive ratio. We also give a distribution-oblivious online algorithm and show that it is ( 1 − 1 / e ) -competitive in special cases. At the heart of our algorithms is a new quantity that factors in the potential of reusability for each resource by (computationally) creating an asymmetry between identical units of the resource. We establish the performance guarantee for our algorithms by constructing a feasible solution to a novel system of inequalities that allows direct comparison with the clairvoyant benchmark instead of a linear programming relaxation of the benchmark. Our technique generalizes the primal-dual analysis framework for online resource allocation and may be of broader interest. Funding: This work was supported by Google (Google Research Scholar Program) and the Division of Civil, Mechanical and Manufacturing Innovation [Grants 1351838, 1636046, and 2340306]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0695 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0695},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1897-1915},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically optimal competitive ratio for online allocation of reusable resources},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic fair division with partial information. <em>OR</em>, <em>73</em>(4), 1876-1896. (<a href='https://doi.org/10.1287/opre.2023.0608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the fundamental problem of fairly and efficiently allocating T indivisible items among n agents with additive preferences. Items become available over a sequence of rounds, and every item must be allocated immediately and irrevocably before the next one arrives. Previous work shows that when the agents’ valuations for the items are drawn from known distributions, it is possible (under mild assumptions) to find allocations that are envy-free with high probability and Pareto efficient ex post. However, this requires that agents accurately report their values to the algorithm, which rarely happens in practice. We study a partial-information setting, where true item values are hidden from the algorithm and it is only possible to elicit ordinal information in the form of a ranking or pairwise comparison relative to prior items. When values are drawn from i.i.d. distributions, or correlated distributions consisting of a shared common value for each item with i.i.d. noise, we give an algorithm that is envy-free and ( 1 − ϵ ) -welfare-maximizing with high probability. We provide similar guarantees (envy-freeness and a constant approximation to welfare with high probability) even with minimally expressive queries that ask for a comparison with a single previous item. For independent but nonidentical agents, we obtain envy-freeness and a constant approximation to Pareto efficiency with high probability. Our results are asymptotically tight. A computational study shows that envy-freeness and efficiency can be achieved on practical time-horizons. Funding: D. Halpern is supported by the National Science Foundation Graduate Research Fellowship Program [Grant DGE1745303]. A. Psomas is supported in part by an NSF CAREER award (Division of Computing and Communication Foundations) [Grant CCF-2144208], a Google AI for Social Good award, and research awards from Google and Supra. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0608 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0608},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1876-1896},
  shortjournal = {Oper. Res.},
  title        = {Dynamic fair division with partial information},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On simple mechanisms for dependent items. <em>OR</em>, <em>73</em>(4), 1849-1875. (<a href='https://doi.org/10.1287/opre.2022.0552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of selling n heterogeneous items to a single buyer, whose values for different items are dependent. Under arbitrary dependence, others show that no simple mechanism can achieve a nonnegligible fraction of the optimal revenue even with only two items. We consider the setting where the buyer’s type is drawn from a correlated distribution that can be captured by a Markov random field (MRF), one of the most prominent frameworks for modeling high-dimensional distributions with structure. We show how the performance of simple mechanisms depends on some natural parameters of the MRF for several fundamental classes of the buyer’s valuations. Our results are based on the duality framework by of others and a new concentration inequality for XOR-of-OR-of-Singletons functions over dependent random variables. Funding: This research was supported by a Sloan Foundation Research Fellowship and the National Science Foundation [Award CCF-1942583 (CAREER)].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0552},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1849-1875},
  shortjournal = {Oper. Res.},
  title        = {On simple mechanisms for dependent items},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simultaneous column-and-row generation solution method for liner shipping network design. <em>OR</em>, <em>73</em>(4), 1825-1848. (<a href='https://doi.org/10.1287/opre.2020.0458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The liner shipping network design (LSND) problem involves creating regular ship rotations to transport containerized cargo between seaports. The objective is to maximize carrier profit by balancing revenue from satisfied demand against operating and transshipment costs. Finding an optimal solution is challenging because of complex rotation structures and joint decisions on fleet deployment, cargo routing, and rotation design. This work introduces a set partitioning–like formulation for LSND with transshipment costs, featuring an exponential number of variables and constraints. The formulation captures key service components, such as ship type, sailing speed, and frequency. Addressing transshipment costs requires numerous rotation-dependent variables and constraints, making even linear programming relaxation difficult to solve. To tackle this, we propose a simultaneous column-and-row generation (SCRG) solution method with novel speedup techniques. Integrating SCRG into a branch-and-price algorithm, we develop an exact method for LSND and test it on two variants with different rotation configurations. Extensive computational experiments demonstrate the method’s effectiveness and efficiency. In addition to advancing solution methods for LSND, this work enhances the SCRG-based method and expands its practical applications. Funding: This research was supported by the National Natural Science Foundation of China [Grants 72171147, 72031006] and the Research Grants Council of Hong Kong SAR, China [Grant 15221619]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2020.0458 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0458},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1825-1848},
  shortjournal = {Oper. Res.},
  title        = {A simultaneous column-and-row generation solution method for liner shipping network design},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint order fulfillment and inventory management in assemble-to-order generalized w systems. <em>OR</em>, <em>73</em>(4), 1805-1824. (<a href='https://doi.org/10.1287/opre.2021.0281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper characterizes joint order fulfillment and inventory policies for assemble-to-order generalized W systems, in which k products are assembled from a common component and k product-specific (dedicated) components. We consider a periodic-review system and focus on nested fulfillment policies, in which orders are fulfilled in decreasing order of profit margins. We prove that the optimal fulfillment policy of a two-product W system is nested. For systems with more than two products, although nested policies may not be optimal in general, we identify a sufficient condition for the optimal policy to be nested, and furthermore, we show that a nested policy is asymptotically optimal in a high-demand regime. Based on these results, we develop an asymptotically optimal strategy for the joint fulfillment and inventory decision under which the policy for demand fulfillment is static nested and that for inventory procurement is of the newsvendor type. In addition, we derive insights regarding the interactions between the component inventories and find that although the inventories of the common component and a particular dedicated component are complementary, those of different dedicated components may not be substitutable. Funding: The work of M. Yu was partially supported by the Hong Kong Research Grant Council [Grants 16502018 and 647312]. The work of S. Zheng was partially supported by the Hong Kong Research Grant Council [Grant 16502621]. The work of J. Chen was partially supported by the National Natural Science Foundation of China [Grants 72171202 and 72232007]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0281 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0281},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1805-1824},
  shortjournal = {Oper. Res.},
  title        = {Joint order fulfillment and inventory management in assemble-to-order generalized w systems},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Split liver transplantation: An analytical decision support model. <em>OR</em>, <em>73</em>(4), 1785-1804. (<a href='https://doi.org/10.1287/opre.2022.0131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Split liver transplantation (SLT) is a procedure that potentially saves two lives using one liver, increasing the total benefit derived from the limited number of donated livers available. SLT may also improve equity by giving transplant candidates who are physically smaller (including children) increased access to liver transplants. However, SLT is rarely used in the United States. To help quantify the benefits of increased SLT utilization and provide decision support tools, we introduce a deceased-donor liver allocation model with both efficiency and fairness objectives. We formulate our model as a multiqueue fluid system, incorporating the specifics of donor-recipient size matching and patients’ dynamically changing health conditions. Leveraging a novel decomposition result, we find the exact optimal matching procedure, enabling us to benchmark the performance of different allocation policies against the theoretical optimal. Numerical results, utilizing data from the Organ Procurement and Transplantation Network, show that increased utilization of SLT can significantly reduce patient deaths, increase total quality-adjusted life years, and improve fairness among different patient groups. Funding: This work was supported by Carnegie Mellon University Tepper’s Health Care Initiative Funding from 2022 to 2023. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0131 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0131},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1785-1804},
  shortjournal = {Oper. Res.},
  title        = {Split liver transplantation: An analytical decision support model},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard and soft defense against a sequence of aerial threats. <em>OR</em>, <em>73</em>(4), 1767-1784. (<a href='https://doi.org/10.1287/opre.2024.1025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of missiles and drones (hereafter referred to as threats) in attacks by both state and nonstate actors highlights the critical need for a robust defense system to counter these threats. We develop a combat model for the engagement between a Blue defender who is subject to repeated attacks by Red threats. The defender employs two types of defenses: hard interceptors, such as antiballistic missiles, and soft measures, such as directed-energy weapons and jamming. Employing strategies for these two types of defensive options are evaluated by a two-dimensional measure of effectiveness: expected number of leaking Red threats and the expected expenditure of hard interceptors. We define efficient frontiers on this two-dimensional space and identify defense strategies that compose these frontiers. Funding: This research is supported by funding from the Naval Postgraduate School, Naval Research Program [PE 0605853N/2098]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2024.1025 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1025},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1767-1784},
  shortjournal = {Oper. Res.},
  title        = {Hard and soft defense against a sequence of aerial threats},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unit commitment without commitment: A dynamic programming approach for managing an integrated energy system under uncertainty. <em>OR</em>, <em>73</em>(4), 1744-1766. (<a href='https://doi.org/10.1287/opre.2023.0546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though variability and uncertainty have always posed challenges for power systems, the increasing use of renewable energy sources has exacerbated these issues. At a vertically integrated utility, the system operator manages many generation units—renewable and otherwise—and storage units to ensure that the total energy produced matches contemporaneous demand. Current industry practice at these utilities involves solving “unit commitment” and “economic dispatch” optimization problems to choose production plans. These models, though complex, do not explicitly incorporate uncertainty. In this paper, we develop a dynamic programming approach to help system operators manage production under uncertainty. We formulate the problem as a stochastic dynamic program and use Lagrangian methods to decompose the system across units. The Lagrangian model relaxes the demand-matching constraint and introduces stochastic Lagrange multipliers that can be interpreted as prices representing the varying marginal value of energy production; each unit is then operated to maximize its own expected “profit” given these uncertain prices. These unit-specific value functions are then used to incorporate longer-term effects in dispatch decisions. The unit-specific value functions also provide a way to value generation and storage units in an uncertain environment. We develop relevant theory and demonstrate this dynamic approach using data from the Duke Energy Carolinas and Progress systems. Our numerical experiments demonstrate that this dynamic approach is computationally feasible at an industrial scale and can improve current practice. Specifically, our results suggest that this dynamic approach can reduce operational costs by about 2% on average in the present Duke Energy system and, in a “future” system with increased solar and storage capacity, can reduce operational costs by 4%–5% on average. Perhaps more strikingly, this dynamic approach, on average, performs within 0.2%–0.3% of production plans based on perfect foresight about future net demands. Funding: This work was supported by the Department of Energy Advanced Research Projects Agency - Energy [Grant DE-AR0001283, “A Grid that’s Risk-Aware for Clean Electricity (GRACE)”] and Fuqua and Tuck. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2023.0546 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0546},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1744-1766},
  shortjournal = {Oper. Res.},
  title        = {Unit commitment without commitment: A dynamic programming approach for managing an integrated energy system under uncertainty},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tailored base-surge policies in dual-sourcing inventory systems with demand learning. <em>OR</em>, <em>73</em>(4), 1723-1743. (<a href='https://doi.org/10.1287/opre.2022.0624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a periodic-review dual-sourcing inventory system in which the expedited supplier is faster and more costly, whereas the regular supplier is slower and cheaper. Under full demand distributional information, it is well known that the optimal policy is extremely complex but the celebrated Tailored Base-Surge (TBS) policy performs near optimally. Under such a policy, a constant order is placed at the regular source in each period, whereas the order placed at the expedited source follows a simple order-up-to rule. In this paper, we assume that the firm does not know the demand distribution a priori and makes adaptive inventory ordering decisions in each period based only on the past sales (a.k.a. censored demand) data. The standard performance measure is regret, which is the cost difference between a feasible learning algorithm and the clairvoyant (full-information) benchmark. When the benchmark is chosen to be the (full-information) best Tailored Base-Surge policy, we develop the first nonparametric learning algorithm that admits a regret bound of O ( T ( log T ) 3 log log T ) , which is provably tight up to a logarithmic factor. Leveraging the structure of this problem, our approach combines the power of bisection search and stochastic gradient descent and also involves a delicate high-probability coupling argument between our and the clairvoyant optimal system dynamics. Funding: The research of C. Shi is partially supported by an Amazon research award. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0624 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0624},
  journal      = {Operations Research},
  month        = {7-8},
  number       = {4},
  pages        = {1723-1743},
  shortjournal = {Oper. Res.},
  title        = {Tailored base-surge policies in dual-sourcing inventory systems with demand learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2024 operations research reviewer meritorious service awards. <em>OR</em>, <em>73</em>(3), iii. (<a href='https://doi.org/10.1287/opre.2025.ack.v73.n3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reviewers are an integral part of the journal. Without their efforts, the journal could not operate. The journal recognizes a select number of reviewers that have gone above and beyond normal expectations with “Distinguished Service” and “Meritorious Service” awards. On behalf of OPRE , editor-in-chief Amy Ward expresses her deepest gratitude to all those who served as reviewers for the journal in 2024. The process for determining these awards in a given year is as follows:},
  archive      = {J_OR},
  doi          = {10.1287/opre.2025.ack.v73.n3},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {iii},
  shortjournal = {Oper. Res.},
  title        = {2024 operations research reviewer meritorious service awards},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tight guarantees for multiunit prophet inequalities and online stochastic knapsack. <em>OR</em>, <em>73</em>(3), 1703-1721. (<a href='https://doi.org/10.1287/opre.2022.0309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prophet inequalities are a useful tool for designing online allocation procedures and comparing their performance to the optimal offline allocation. In the basic setting of k -unit prophet inequalities, a well-known procedure with its celebrated performance guarantee of 1 − 1 k + 3 has found widespread adoption in mechanism design and general online allocation problems in online advertising, healthcare scheduling, and revenue management. Despite being commonly used to derive approximately optimal algorithms for multiresource allocation problems, the tightness of the 1 − 1 / k + 3 guarantee has remained unknown. In this paper, we characterize the tight guarantee for multiunit prophet inequalities, which we show is in fact strictly greater than 1 − 1 k + 1 for all k > 1 . This improvement is achieved using duality for a new linear programming (LP) that is based on online contention resolution schemes (OCRS), and as a by-product, we also show the Magician’s policy (but not guarantee) to be instance optimal. We also consider the more general online stochastic knapsack problem where each individual allocation can consume an arbitrary fraction of the initial capacity. Here we introduce a new “best-fit” procedure with a performance guarantee of 1 3 + e − 2 ≈ 0.319 , which we also show is tight with respect to the standard LP relaxation. This improves the previously best-known guarantee of 0.2 for online knapsack. Our analysis differs from existing ones by eschewing the need to split items into “large” or “small” based on capacity consumption, using instead an invariant for the overall utilization on different sample paths. Finally, we refine our technique for the unit-density special case of knapsack, and improve the guarantee from 0.321 to 0.3557 in the multiresource appointment scheduling application of Stein et al. (2020) . Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0309 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0309},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1703-1721},
  shortjournal = {Oper. Res.},
  title        = {Tight guarantees for multiunit prophet inequalities and online stochastic knapsack},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive, doubly optimal no-regret learning in strongly monotone and exp-concave games with gradient feedback. <em>OR</em>, <em>73</em>(3), 1675-1702. (<a href='https://doi.org/10.1287/opre.2022.0446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online gradient descent (OGD) is well-known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of Θ ( log T ) for strongly convex cost functions, and (2) in the multiagent setting of strongly monotone games with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of Θ ( 1 T ) . Whereas these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, AdaOGD , that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves O ( log 2 ( T ) ) regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs AdaOGD in strongly monotone games, the joint action converges in a last-iterate sense to a unique Nash equilibrium at a rate of O ( log 3 T T ) , again optimal up to log factors. We illustrate our algorithms in a learning version of the classic newsvendor problem, in which, because of lost sales, only (noisy) gradient feedback can be observed. Our results immediately yield the first feasible and near-optimal algorithm for both the single-retailer and multiretailer settings. We also extend our results to the more general setting of exp-concave cost functions and games, using the online Newton step algorithm. Funding: This work is generously supported by the National Science Foundation [Grant CCF-2106508]. This work is also partially funded by a grant from the European Research Council under the Synergy program and the 2024 New York University Center for Global Economy and Business grant.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0446},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1675-1702},
  shortjournal = {Oper. Res.},
  title        = {Adaptive, doubly optimal no-regret learning in strongly monotone and exp-concave games with gradient feedback},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the robustness of second-price auctions in prior-independent mechanism design. <em>OR</em>, <em>73</em>(3), 1659-1674. (<a href='https://doi.org/10.1287/opre.2022.0428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical Bayesian mechanism design relies on the common prior assumption, but the common prior is often not available in practice. We study the design of prior-independent mechanisms that relax this assumption: The seller is selling an indivisible item to n buyers such that the buyers’ valuations are drawn from a joint distribution that is unknown to both the buyers and the seller, buyers do not need to form beliefs about competitors, and the seller assumes the distribution is adversarially chosen from a specified class. We measure performance through the worst-case regret , or the difference between the expected revenue achievable with perfect knowledge of buyers’ valuations and the actual mechanism revenue. We study a broad set of classes of valuation distributions that capture a wide spectrum of possible dependencies: independent and identically distributed (i.i.d.) distributions, mixtures of i.i.d. distributions, affiliated and exchangeable distributions, exchangeable distributions, and all joint distributions. We derive in quasi closed form the minimax values and the associated optimal mechanism. In particular, we show that the first three classes admit the same minimax regret value, which is decreasing with the number of competitors, whereas the last two have the same minimax regret equal to that of the case n = 1. Furthermore, we show that the minimax optimal mechanisms have a simple form across all settings: a second-price auction with random reserve prices , which shows its robustness in prior-independent mechanism design. En route to our results, we also develop a principled methodology to determine the form of the optimal mechanism and worst-case distribution via first-order conditions that should be of independent interest in other minimax problems. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0428 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0428},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1659-1674},
  shortjournal = {Oper. Res.},
  title        = {On the robustness of second-price auctions in prior-independent mechanism design},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matching impatient and heterogeneous demand and supply. <em>OR</em>, <em>73</em>(3), 1637-1658. (<a href='https://doi.org/10.1287/opre.2022.0005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service platforms must determine rules for matching heterogeneous demand (customers) and supply (workers) that arrive randomly over time and may be lost if forced to wait too long for a match. Our objective is to maximize the cumulative value of matches, minus costs incurred when demand and supply wait. We develop a fluid model, that approximates the evolution of the stochastic model and captures explicitly the nonlinear dependence between the amount of demand and supply waiting and the distribution of their patience times, also known as reneging or abandonment times in the literature. The fluid model-invariant states approximate the steady-state mean queue lengths in the stochastic system and, therefore, can be used to develop an optimization problem whose optimal solution provides matching rates between demand and supply types that are asymptotically optimal (on fluid scale as demand and supply rates grow large). We propose a discrete review matching policy that asymptotically achieves the optimal matching rates. We further show that, when the aforementioned matching optimization problem has an optimal extreme point solution, which occurs when the patience time distributions have increasing hazard rate functions, a state-independent priority policy that ranks the edges on the bipartite graph connecting demand and supply is asymptotically optimal. A key insight from this analysis is that the ranking critically depends on the patience time distributions and may be different for different distributions even if they have the same mean, demonstrating that models assuming, for example, exponential patience times for tractability, may lack robustness. Finally, we observe that, when holding costs are zero, a discrete review policy that does not require knowledge of interarrival and patience time distributions is asymptotically optimal. Funding: This work is supported by the Charles M. Harper Faculty Fellowship at the University of Chicago Booth School of Business. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0005 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0005},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1637-1658},
  shortjournal = {Oper. Res.},
  title        = {Matching impatient and heterogeneous demand and supply},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive lagrangian policies for a multiwarehouse, multistore inventory system with lost sales. <em>OR</em>, <em>73</em>(3), 1615-1636. (<a href='https://doi.org/10.1287/opre.2022.0668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the inventory control problem of a multiwarehouse, multistore system over a time horizon when the warehouses receive no external replenishment. This problem is prevalent in retail settings, and it is referred to in the work of [ Jackson PL (1988) Stock allocation in a two-echelon distribution system or “what to do until your ship comes in.” Management Sci. 34(7):880–895] as the problem of “what to do until your (external) shipment comes in.” The warehouses are stocked with initial inventories, and the stores are dynamically replenished from the warehouses in each period of the planning horizon. Excess demand in each period at a store is lost. The optimal policy for this problem is complex and state dependent, and because of the curse of dimensionality, computing the optimal policy using standard dynamic programming is numerically intractable. Static Lagrangian base-stock (LaBS) policies have been developed for this problem [Miao S, Jasin S, Chao X (2022) Asymptotically optimal Lagrangian policies for one-warehouse multi-store system with lost sales. Oper. Res. 70(1):141–159] and shown to be asymptotically optimal. In this paper, we develop adaptive policies that dynamically adjust the control parameters of a vanilla static LaBS policy using realized historical demands. We show, both theoretically and numerically, that adaptive policies significantly improve the performance of the LaBS policy, with the magnitude of improvement characterized by the number of policy adjustments. In particular, when the number of adjustments is a logarithm of the length of time horizon, the policy is rate optimal in the sense that the rate of the loss (in terms of the dependency on the length of the time horizon) matches that of the theoretical lower bound. Among other insights, our results also highlight the benefit of incorporating the “pooling effect” in designing a dynamic adjustment scheme. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0668 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0668},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1615-1636},
  shortjournal = {Oper. Res.},
  title        = {Adaptive lagrangian policies for a multiwarehouse, multistore inventory system with lost sales},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assigning and scheduling generalized malleable jobs under subadditive or submodular processing speeds. <em>OR</em>, <em>73</em>(3), 1598-1614. (<a href='https://doi.org/10.1287/opre.2022.0168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malleable scheduling is a model that captures the possibility of parallelization to expedite the completion of time-critical tasks. A malleable job can be allocated and processed simultaneously on multiple machines, occupying the same time interval on all these machines. We study a general version of this setting, in which the functions determining the joint processing speed of machines for a given job follow different discrete concavity assumptions (subadditivity, fractional subadditivity, submodularity, and matroid ranks). We show that under these assumptions, the problem of scheduling malleable jobs at minimum makespan can be approximated by a considerably simpler assignment problem. Moreover, we provide efficient approximation algorithms for both the scheduling and the assignment problem, with increasingly stronger guarantees for increasingly stronger concavity assumptions, including a logarithmic approximation factor for the case of submodular processing speeds and a constant approximation factor when processing speeds are determined by matroid rank functions. Computational experiments indicate that our algorithms outperform the theoretical worst-case guarantees. Funding: D. Fotakis received financial support from the Hellenic Foundation for Research and Innovation (H.F.R.I.) [“First Call for H.F.R.I. Research Projects to Support Faculty Members and Researchers and the Procurement of High-Cost Research Equipment Grant,” Project BALSAM, HFRI-FM17-1424]. J. Matuschke received financial support from the Fonds Wetenschappelijk Onderzoek-Vlanderen [Research Project G072520N “Optimization and Analytics for Stochastic and Robust Project Scheduling”]. O. Papadigenopoulos received financial support from the National Science Foundation Institute for Machine Learning [Award 2019844]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0168 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0168},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1598-1614},
  shortjournal = {Oper. Res.},
  title        = {Assigning and scheduling generalized malleable jobs under subadditive or submodular processing speeds},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Slowly varying regression under sparsity. <em>OR</em>, <em>73</em>(3), 1581-1597. (<a href='https://doi.org/10.1287/opre.2022.0330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the framework of slowly varying regression under sparsity, which allows sparse regression models to vary slowly and sparsely. We formulate the problem of parameter estimation as a mixed-integer optimization problem and demonstrate that it can be reformulated exactly as a binary convex optimization problem through a novel relaxation. The relaxation utilizes a new equality on Moore-Penrose inverses that convexifies the nonconvex objective function while coinciding with the original objective on all feasible binary points. This allows us to solve the problem significantly more efficiently and to provable optimality using a cutting plane–type algorithm. We develop a highly optimized implementation of such algorithm, which substantially improves upon the asymptotic computational complexity of a straightforward implementation. We further develop a fast heuristic method that is guaranteed to produce a feasible solution and, as we empirically illustrate, generates high-quality warm-start solutions for the binary optimization problem. To tune the framework’s hyperparameters, we propose a practical procedure relying on binary search that, under certain assumptions, is guaranteed to recover the true model parameters. We show, on both synthetic and real-world data sets, that the resulting algorithm outperforms competing formulations in comparable times across a variety of metrics, including estimation accuracy, predictive power, and computational time, and is highly scalable, enabling us to train models with tens of thousands of parameters. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0330 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0330},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1581-1597},
  shortjournal = {Oper. Res.},
  title        = {Slowly varying regression under sparsity},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Branch-and-price for prescriptive contagion analytics. <em>OR</em>, <em>73</em>(3), 1558-1580. (<a href='https://doi.org/10.1287/opre.2023.0308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contagion models are ubiquitous in epidemiology, social sciences, engineering, and management. This paper formulates a prescriptive contagion analytics model where a decision maker allocates shared resources across multiple segments of a population, each governed by continuous-time contagion dynamics. These problems feature a large-scale mixed-integer nonconvex optimization structure with constraints governed by ordinary differential equations. This paper develops a branch-and-price methodology for this class of problems based on (i) a set partitioning reformulation; (ii) a column generation decomposition; (iii) a state-clustering algorithm for discrete-decision continuous-state dynamic programming; and (iv) a tripartite branching scheme to circumvent nonlinearities. We apply the methodology to four real-world cases: vaccine distribution, vaccination centers deployment, content promotion, and congestion mitigation. Extensive experiments show that the algorithm scales to large and otherwise-intractable instances, outperforming state-of-the-art benchmarks. Our methodology provides practical benefits in contagion systems—In particular, we show that it can increase the effectiveness of a vaccination campaign in a setting replicating the rollout of COVID-19 vaccines in 2021. We provide an open-source implementation of the methodology to enable replication. Funding: K. Wang’s research was supported by the National Natural Science Foundation of China [Grants 72322002, 72331001, and 72361137001] Supplemental Material: The computer code, data, and e-companion that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0308 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0308},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1558-1580},
  shortjournal = {Oper. Res.},
  title        = {Branch-and-price for prescriptive contagion analytics},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile optimization via multiple-timescale local search for black-box functions. <em>OR</em>, <em>73</em>(3), 1535-1557. (<a href='https://doi.org/10.1287/opre.2022.0534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider quantile optimization of black-box functions that are estimated with noise. We propose two new iterative three-timescale local search algorithms. The first algorithm uses an appropriately modified finite-difference-based gradient estimator that requires 2 d + 1 samples of the black-box function per iteration of the algorithm, where d is the number of decision variables (dimension of the input vector). For higher-dimensional problems, this algorithm may not be practical if the black-box function estimates are expensive. The second algorithm employs a simultaneous-perturbation-based gradient estimator that uses only three samples for each iteration regardless of problem dimension. Under appropriate conditions, we show the almost sure convergence of both algorithms. In addition, for the class of strongly convex functions, we further establish their (finite-time) convergence rate through a novel fixed-point argument. Simulation experiments indicate that the algorithms work well on a variety of test problems and compare well with recently proposed alternative methods. Funding: This work was supported by the Air Force Office of Scientific Research [Grant FA95502010211] and the National Science Foundation [Grants CMMI-2027527, IIS-2123684]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0534 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0534},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1535-1557},
  shortjournal = {Oper. Res.},
  title        = {Quantile optimization via multiple-timescale local search for black-box functions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing service menus for bipartite queueing systems. <em>OR</em>, <em>73</em>(3), 1496-1534. (<a href='https://doi.org/10.1287/opre.2022.0179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a multiclass, multiserver queueing system, in which customers of different types have heterogeneous preferences over the many servers available. The goal of the service provider is to design a menu of service classes that balances two competing objectives: (1) maximize customers’ average matching reward and (2) minimize customers’ average waiting time. A service class corresponds to a single queue served by a subset of servers under a first come, first served–assign longest idle server service discipline. Customers act as rational self-interested utility-maximizing agents when choosing which service class to join. In particular, they join the class that maximizes their expected ex ante net utility, which is given by the difference between the server-dependent service reward they receive and a disutility based on the mean steady-state waiting time of the service class they join. We study the problem under (conventional) heavy-traffic conditions: that is, in the limit as the traffic intensity of the system approaches one from below. For the case of two servers, we provide a complete characterization of the possible menus and their delay-reward trade-offs. For a general number of servers, we prove that if the service provider only cares about minimizing average delay or maximizing total matching reward, then very simple menus are optimal. Finally, we provide mixed-integer linear programming formulations for optimizing the delay-reward trade-off within fairly rich and practically relevant families of menus, which we term partitioned and tailored . Funding: R. Caldentey thanks the University of Chicago Booth School of Business for financial support.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0179},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1496-1534},
  shortjournal = {Oper. Res.},
  title        = {Designing service menus for bipartite queueing systems},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal diagonal preconditioning. <em>OR</em>, <em>73</em>(3), 1479-1495. (<a href='https://doi.org/10.1287/opre.2022.0592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preconditioning has long been a staple technique in optimization, often applied to reduce the condition number of a matrix and speed up the convergence of algorithms. Although there are many popular preconditioning techniques in practice, most lack guarantees on reductions in condition number, and the degree to which we can improve over existing heuristic preconditioners remains an important question. In this paper, we study the problem of optimal diagonal preconditioning that achieves maximal reduction in the condition number of any full-rank matrix by scaling its rows and/or columns with positive numbers. We first reformulate the problem as a quasiconvex optimization problem and provide a simple algorithm based on bisection. Then, we develop an interior point algorithm with O ( log ( 1 / ϵ ) ) iteration complexity, where each iteration consists of a Newton update based on the Nesterov-Todd direction. Next, we specialize in one-sided optimal diagonal preconditioning problems and demonstrate that they can be formulated as standard dual semidefinite program (SDP) problems. We then develop efficient customized solvers for the SDP approach and study the empirical performance of our optimal diagonal preconditioning procedures through extensive experiments. Our findings suggest that optimal diagonal preconditioners can significantly improve upon existing heuristics-based diagonal preconditioners at reducing condition numbers, and our SDP approach can find such optimal preconditioners efficiently. We also extend our SDP approach to compute optimal mixtures of heuristic preconditioners, which further improves its scalability and applicability. Funding: Z. Qu was supported by a Stanford Interdisciplinary Graduate Fellowship; W. Gao was supported in part by the National Natural Science Foundation of China (NSFC) [Grants NSFC-72150001, NSFC-72225009, NSFC-72394360, and NSFC72394365]; O. Hinder was supported by Pitt momentum funds and a Mascaro center for sustainable innovation faculty fellowship; Z. Zhou was supported by NSF Grant CCF [Grant 2312205] and the New York University Research Catalyst Prize. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0592 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0592},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1479-1495},
  shortjournal = {Oper. Res.},
  title        = {Optimal diagonal preconditioning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust CARA optimization. <em>OR</em>, <em>73</em>(3), 1459-1478. (<a href='https://doi.org/10.1287/opre.2021.0654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose robust optimization models and their tractable approximations that cater for ambiguity-averse decision makers whose underlying risk preferences are consistent with constant absolute risk aversion (CARA). Specifically, we focus on maximizing the worst-case expected exponential utility where the underlying uncertainty is generated from a set of stochastically independent factors with ambiguous marginals. To obtain computationally tractable formulations, we propose a hierarchy of approximations, starting from formulating the objective function as tractable concave functions in affinely perturbed cases, developing approximations in concave piecewise affinely perturbed cases, and proposing new multideflected linear decision rules for adaptive optimization models. We also extend the framework to address a multiperiod consumption model. The resultant models would take the form of an exponential conic optimization problem (ECOP), which can be practicably solved using current off-the-shelf solvers. We present numerical examples including project management and multiperiod inventory management with financing to illustrate how our approach can be applied to obtain high-quality solutions that could outperform current stochastic optimization approaches, especially in situations with high risk aversion levels. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2021.0654 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0654},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1459-1478},
  shortjournal = {Oper. Res.},
  title        = {Robust CARA optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic optimization approaches for an operating room and anesthesiologist scheduling problem. <em>OR</em>, <em>73</em>(3), 1430-1458. (<a href='https://doi.org/10.1287/opre.2022.0258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose combined allocation, assignment, sequencing, and scheduling problems under uncertainty involving multiple operation rooms (ORs), anesthesiologists, and surgeries as well as methodologies for solving such problems. Specifically, given sets of ORs, regular anesthesiologists, on-call anesthesiologists, and surgeries, our methodologies solve the following decision-making problems simultaneously: (1) an allocation problem that decides which ORs to open and which on-call anesthesiologists to call in, (2) an assignment problem that assigns an OR and an anesthesiologist to each surgery, and (3) a sequencing and scheduling problem that determines the order of surgeries and their scheduled start times in each OR. To address the uncertainty of each surgery’s duration, we propose and analyze stochastic programming (SP) and distributionally robust optimization (DRO) models with both risk-neutral and risk-averse objectives. We obtain near-optimal solutions of our SP models using sample average approximation and propose a computationally efficient column-and-constraint generation method to solve our DRO models. In addition, we derive symmetry-breaking constraints that improve the models’ solvability. Using real-world, publicly available surgery data and a case study from a health system in New York, we conduct extensive computational experiments comparing the proposed methodologies empirically and theoretically, demonstrating where significant performance improvements can be gained. Additionally, we derive several managerial insights relevant to practice. Supplemental Material: The e-companion and the computer code and data that supports the findings of this study is available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0258 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0258},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1430-1458},
  shortjournal = {Oper. Res.},
  title        = {Stochastic optimization approaches for an operating room and anesthesiologist scheduling problem},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven compositional optimization in misspecified regimes. <em>OR</em>, <em>73</em>(3), 1395-1411. (<a href='https://doi.org/10.1287/opre.2021.0295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As systems grow in size, scale, and intricacy, the challenges of misspecification become even more pronounced. In this paper, we focus on parametric misspecification in regimes complicated by risk and nonconvexity. When this misspecification may be resolved via a parallel learning process, we develop data-driven schemes for resolving a broad class of misspecified stochastic compositional optimization problems. Notably, this rather broad class of compositional problems can contend with challenges posed by diverse forms of risk, dynamics, and nonconvexity, significantly extending the reach of such avenues. Specifically, we consider the minimization of a stochastic compositional function over a closed and convex set X in a regime, where certain parameters are unknown or misspecified. Existing algorithms can accommodate settings where the parameters are correctly specified, but efficient first-order schemes are hitherto unavailable for the imperfect information compositional counterparts. Via a data-driven compositional optimization approach, we develop asymptotic and rate guarantees for unaccelerated and accelerated schemes for convex, strongly convex, and nonconvex problems in a two-level regime. Additionally, we extend the accelerated schemes to the general T -level setting. Notably, the nonasymptotic rate guarantees in all instances show no degradation from the rate statements obtained in a correctly specified regime. Further, under mild assumptions, our schemes achieve optimal (or near-optimal) sample complexities for general T -level strongly convex and nonconvex compositional problems, providing a marked improvement over prior work. Our numerical experiments support the theoretical findings based on the resolution of a misspecified three-level compositional risk-averse optimization problem. Funding: E. X. Fang is partially supported by the National Science Foundation [Grants DMS-2230795 and DMS-2230797]. U. V. Shanbhag is partially supported by the Office of Naval Research [Grant N00014-22-1-2589] and the Department of Energy [Grant DE-SC0023303]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0295 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0295},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1395-1411},
  shortjournal = {Oper. Res.},
  title        = {Data-driven compositional optimization in misspecified regimes},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An exact solution to wordle. <em>OR</em>, <em>73</em>(3), 1384-1394. (<a href='https://doi.org/10.1287/opre.2022.0434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose and scale a framework based on exact dynamic programming to solve the game of Wordle, which has withstood many attempts to be solved by a variety of methods ranging from reinforcement learning to information theory. First, we derive a mathematical model of the game, present the resultant Bellman equation, and outline a series of optimizations to make this approach tractable. We then outline how to extend the framework to solve variants of the game—such as Wordle Hard Mode, optimizing for the worst case, and optimizing under nonuniform word probabilities—and present results for all game variants considered. We show that the best starting guess is SALET, that the algorithm finds all hidden words in at most five guesses, and that the average number of guesses starting with SALET is 3.421. We conclude by presenting experiments that illuminate why some approximate methods have struggled to solve the game, which our framework successfully circumvents because of its exact nature. We have also implemented our algorithm at wordleopt.com , so that the reader may interact with the optimal policy.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0434},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1384-1394},
  shortjournal = {Oper. Res.},
  title        = {An exact solution to wordle},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Column-randomized linear programs: Performance guarantees and applications. <em>OR</em>, <em>73</em>(3), 1366-1383. (<a href='https://doi.org/10.1287/opre.2020.0494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a randomized method for solving linear programs with a large number of columns but a relatively small number of constraints. Because enumerating all the columns is usually unrealistic, such linear programs are commonly solved by column generation, which is often still computationally challenging because of the intractability of the subproblem in many applications. Instead of iteratively introducing one column at a time as in column generation, our proposed method involves sampling a collection of columns according to a user-specified randomization scheme and solving the linear program consisting of the sampled columns. Although similar methods for solving large-scale linear programs by sampling columns (or, equivalently, sampling constraints in the dual) have been proposed in the literature, in this paper we derive an upper bound on the optimality gap that holds with high probability. This bound converges to the optimality gap of a linear program related to the sampling distribution at a rate inversely proportional to the square root of the number of sampled columns. We analyze the gap of this latter linear program, which we dub the distributional counterpart, and derive conditions under which this gap will be small. Finally, we numerically demonstrate the effectiveness of the proposed method in the cutting-stock problem and in nonparametric choice model estimation. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2020.0494 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0494},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1366-1383},
  shortjournal = {Oper. Res.},
  title        = {Column-randomized linear programs: Performance guarantees and applications},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven minimax optimization with expectation constraints. <em>OR</em>, <em>73</em>(3), 1345-1365. (<a href='https://doi.org/10.1287/opre.2022.0110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention to data-driven optimization approaches, including the well-known stochastic gradient descent method, has grown significantly over recent decades, but data-driven constraints have rarely been studied because of the computational challenges of projections onto the feasible set defined by these hard constraints. In this paper, we focus on the nonsmooth convex-concave stochastic minimax regime and formulate the data-driven constraints as expectation constraints. The minimax expectation constrained problem subsumes a broad class of real-world applications, including data-driven robust optimization, optimization with misspecification, and area under the receiver operating characteristic curve (AUC) maximization with fairness constraints. We propose a class of efficient primal-dual algorithms to tackle the minimax expectation constrained problem and show that our algorithms converge at the optimal rate of O ( 1 / N ) , where N is the number of iterations. We demonstrate the practical efficiency of our algorithms by conducting numerical experiments on large-scale real-world applications. Funding: This work was supported by the National Key R&D Program of China [Grants 2020YFA0711900 and 2020YFA0711901]; the National Natural Science Foundation of China [Grants 12271107 and 62141407]; the Shanghai Science and Technology Program [21JC1400600]; and the National Science Foundation [Grant DMS-1953199]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0110 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0110},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1345-1365},
  shortjournal = {Oper. Res.},
  title        = {Data-driven minimax optimization with expectation constraints},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—An unexpected stochastic dominance: Pareto distributions, dependence, and diversification. <em>OR</em>, <em>73</em>(3), 1336-1344. (<a href='https://doi.org/10.1287/opre.2022.0505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We find the perhaps surprising inequality that the weighted average of independent and identically distributed Pareto random variables with infinite mean is larger than one such random variable in the sense of first-order stochastic dominance. This result holds for more general models including super-Pareto distributions, negative dependence, and triggering events and yields superadditivity of the risk measure value-at-risk for these models. Funding: This work was supported by the Canadian Network for Research and Innovation in Machining Technology, Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2018-03823].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0505},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1336-1344},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—An unexpected stochastic dominance: Pareto distributions, dependence, and diversification},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly modeling and clustering tensors in high dimensions. <em>OR</em>, <em>73</em>(3), 1320-1335. (<a href='https://doi.org/10.1287/opre.2021.0635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of jointly modeling and clustering populations of tensors by introducing a high-dimensional tensor mixture model with heterogeneous covariances. To effectively tackle the high dimensionality of tensor objects, we employ plausible dimension reduction assumptions that exploit the intrinsic structures of tensors, such as low rankness in the mean and separability in the covariance. In estimation, we develop an efficient high-dimensional expectation conditional maximization ( HECM ) algorithm that breaks the intractable optimization in the M step into a sequence of much simpler conditional optimization problems, each of which is convex, admits regularization, and has closed-form updating formulas. Our theoretical analysis is challenged by both the nonconvexity in the expectation maximization-type estimation and having access to only the solutions of conditional maximizations in the M step, leading to the notion of dual nonconvexity. We demonstrate that the proposed HECM algorithm, with an appropriate initialization, converges geometrically to a neighborhood that is within statistical precision of the true parameter. The efficacy of our proposed method is demonstrated through comparative numerical experiments and an application to a medical study, where our proposal achieves an improved clustering accuracy over existing benchmarking methods. Funding: The research of J. Zhang was supported by the National Science Foundation [Grants DMS-2326893 and DMS-2329296]. The research of W. W. Sun was partially supported by the National Science Foundation [Grant SES-2217440]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2021.0635 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0635},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1320-1335},
  shortjournal = {Oper. Res.},
  title        = {Jointly modeling and clustering tensors in high dimensions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revenue maximization under unknown private values with nonobligatory inspection. <em>OR</em>, <em>73</em>(3), 1307-1319. (<a href='https://doi.org/10.1287/opre.2022.0024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of selling k units of an item to n unit-demand buyers to maximize revenue, where the buyers’ values are independently distributed (not necessarily identical) according to publicly known distributions but unknown to the buyers themselves, with the option of allowing buyers to inspect the item at a cost. This problem can be interpreted as a revenue-maximizing variant of Weitzman’s Pandora’s problem with a nonobligatory inspection. We first fully characterize the optimal mechanism in selling to a single buyer subject to an upper bound on the allocation probability. Using this characterization, we then present an approximation mechanism that achieves 1 − 1 / k + 3 of the optimal revenue in expectation. Our mechanism is sequential and has a simple implementation that works in an online setting where buyers arrive in an arbitrary unknown order, yet achieving the aforementioned approximation with respect to the optimal offline mechanism. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2022.0024 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0024},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1307-1319},
  shortjournal = {Oper. Res.},
  title        = {Revenue maximization under unknown private values with nonobligatory inspection},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized pricing and assortment optimization under consumer choice models with local network effects. <em>OR</em>, <em>73</em>(3), 1289-1306. (<a href='https://doi.org/10.1287/opre.2021.0645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a consumer choice model where each consumer’s utility is affected by their neighbors’ purchase probabilities in a network. We first characterize the choice probabilities in this model and then consider the associated personalized assortment optimization problem. Although this problem is NP-hard, we show that for star networks, the optimal assortment to the central consumer and peripheral consumers cannot be strictly larger than that without network effects, and the optimal assortment to each peripheral consumer must be a revenue-ordered assortment. Then, because each node in a network can represent a group of consumers, we propose a novel idea where the sellers are allowed to offer “randomized assortments” to each node in the network. We show that allowing for randomized assortments may further increase the revenue, and for general network setting, under certain conditions, the optimal assortment for each consumer must be a combination of two adjacent revenue-ordered assortments. Such a result gives important insights and allows us to convert the problem into a (nonconvex) continuous optimization problem. Finally, we consider the optimal pricing problem under this model. We develop some structural properties as well as efficient algorithms for the optimal pricing problem. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72394361 and 72425013] and the Guangdong Key Laboratory of Mathematical Foundations for Artificial Intelligence. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study are available at https://doi.org/10.1287/opre.2021.0645 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0645},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1289-1306},
  shortjournal = {Oper. Res.},
  title        = {Personalized pricing and assortment optimization under consumer choice models with local network effects},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic resource allocation: Algorithmic design principles and spectrum of achievable performances. <em>OR</em>, <em>73</em>(3), 1273-1288. (<a href='https://doi.org/10.1287/opre.2022.0504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic resource allocation problems are ubiquitous, arising in inventory management, order fulfillment, online advertising, and other applications. We initially focus on one of the simplest models of online resource allocation: the multisecretary problem. In the multisecretary problem, a decision maker sequentially hires up to B out of T candidates, and candidate ability values are independently and identically distributed from a distribution F on [ 0 , 1 ] . First, we investigate fundamental limits on performance as a function of the value distribution under consideration. We quantify performance in terms of regret , defined as the additive loss relative to the best performance achievable in hindsight. We present a novel fundamental regret lower bound scaling of Ω ( T 1 2 − 1 2 ( 1 + β ) ) for distributions with gaps in their support, with β quantifying the mass accumulation of types (values) around these gaps. This lower bound contrasts with the constant and logarithmic regret guarantees shown to be achievable in prior work, under specific assumptions on the value distribution. Second, we introduce a novel algorithmic principle, conservativeness with respect to gaps (CwG), which yields near-optimal performance with regret scaling of O ˜ ( T 1 2 − 1 2 ( 1 + β ) ) for any distribution in a class parameterized by the mass accumulation parameter β . We then turn to operationalizing the CwG principle across dynamic resource allocation problems. We study a general and practical algorithm, repeatedly act using multiple simulations, which simulates possible futures to estimate a hindsight-based approximation of the value-to-go function. We establish that this algorithm inherits theoretical performance guarantees of algorithms tailored to the distribution of resource requests, including our CwG-based algorithm, and find that it outperforms them in numerical experiments. Funding: This work was supported by the National Science Foundation [Grant 1653477]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0504 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0504},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1273-1288},
  shortjournal = {Oper. Res.},
  title        = {Dynamic resource allocation: Algorithmic design principles and spectrum of achievable performances},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—Revenue management with calendar-aware and dependent demands: Asymptotically tight fluid approximations. <em>OR</em>, <em>73</em>(3), 1260-1272. (<a href='https://doi.org/10.1287/opre.2023.0442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When modeling the demand in revenue management systems, a natural approach is to focus on a canonical interval of time, such as a week, so that we forecast the demand over each week in the selling horizon. Ideally, we would like to use random variables with general distributions to model the demand over each week. The current demand can give a signal for the future demand, so we also would like to capture the dependence between the demands over different weeks. Prevalent demand models in the literature, which are based on a discrete-time approximation to a Poisson process, are not compatible with these needs. In this paper, we focus on revenue management models that are compatible with a natural approach for forecasting the demand. Building such models through dynamic programming is not difficult. We divide the selling horizon into multiple stages, each stage being a canonical interval of time on the calendar. We have a random number of customer arrivals in each stage whose distribution is arbitrary and depends on the number of arrivals in the previous stage. The question that we seek to answer is the form of the corresponding fluid approximation. We give the correct fluid approximation in the sense that it yields asymptotically optimal policies. The form of our fluid approximation is surprising as its constraints use expected capacity consumption of a resource up to a certain time period conditional on the demand in the stage just before the time period in question. Letting K be the number of stages in the selling horizon, c min be the smallest resource capacity, and ϵ be a lower bound on the mass function of the demand in a stage, we use the fluid approximation to give a policy with a performance guarantee of 1 − O ( ( c min + K / ϵ 6 ) log c min c min ) . As the resource capacities and the number of stages increase with the same rate, the performance guarantee converges to one. To our knowledge, this result gives the first asymptotically optimal policy under dependent demands with arbitrary distributions. When the demands in different stages are independent, letting σ 2 be the variance proxy for the demand in each stage, a similar performance guarantee holds by replacing 1 ϵ 6 with σ 2 . Our computational work indicates that using the right fluid approximation can make a dramatic impact in practice. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2023.0442 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0442},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1260-1272},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Revenue management with calendar-aware and dependent demands: Asymptotically tight fluid approximations},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—Leveraging the degree of dynamic substitution in assortment and inventory planning. <em>OR</em>, <em>73</em>(3), 1248-1259. (<a href='https://doi.org/10.1287/opre.2023.0181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the joint assortment and inventory planning problem with stockout-based substitution. In this problem, we pick the number of units to stock for the products at the beginning of the selling horizon. Each arriving customer makes a choice among the set of products with remaining on-hand inventories. Our goal is to pick the stocking quantities to maximize the total expected revenue from the sales net of the stocking cost. We develop a rounding scheme that uses the solution to a fluid approximation to generate stocking quantities with performance guarantees that improve earlier results. Letting T be the number of time periods in the selling horizon and n be the number of products, when customers choose under a general choice model, we show that we can round the solution to the fluid approximation to obtain stocking quantities with an optimality gap of O ( n T ) , improving earlier optimality gaps by a logarithmic factor. More importantly, when customers choose under the multinomial logit model, by leveraging the degree of substitution, we show that our rounded fluid solution is within an optimality gap of O ( log T T log T ) . The optimality gap that we give under the multinomial logit model is the first one that does not depend on the number of products. Such an optimality gap has important practical implications. Earlier results cannot guarantee that the stocking quantities generated by the fluid approximation perform well when both the demand volume and the number of products are large, which is a regime becoming more relevant for online retail applications with large product variety. In contrast, we can guarantee that stocking quantities generated by our rounding scheme perform well when both the demand volume and the number of products are large. Funding: This research was partially funded by a grant from Amazon.com Inc., which was awarded through collaboration with the Columbia Center of AI Technology (CAIT). J. Zhang is partly supported by the National Natural Science Foundation of China (NSFC) [Grant NSFC-72394361] and the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [2023B1212010001]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2023.0181 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0181},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1248-1259},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Leveraging the degree of dynamic substitution in assortment and inventory planning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trading with concave price impact and impact Decay—Theory and evidence. <em>OR</em>, <em>73</em>(3), 1230-1247. (<a href='https://doi.org/10.1287/opre.2023.0620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study statistical arbitrage problems accounting for the nonlinear and transient price impact of metaorders observed empirically. We show that simple explicit trading rules can be derived even for general nonparametric alpha and liquidity signals and also discuss extensions to several impact decay timescales. These results are illustrated using a proprietary data set of Capital Fund Management metaorders, which allows us to calibrate the levels, concavity, and decay parameters of the price impact model and analyze their effects on optimal trading. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/https://doi.org/10.1287/opre.2023.0620 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0620},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1230-1247},
  shortjournal = {Oper. Res.},
  title        = {Trading with concave price impact and impact Decay—Theory and evidence},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk budgeting allocation for dynamic risk measures. <em>OR</em>, <em>73</em>(3), 1208-1229. (<a href='https://doi.org/10.1287/opre.2023.0299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define and develop an approach for risk budgeting allocation—a risk diversification portfolio strategy—where risk is measured using a dynamic time-consistent risk measure. For this, we introduce a notion of dynamic risk contributions that generalize the classical Euler contributions, which allows us to obtain dynamic risk contributions in a recursive manner. We prove that for the class of coherent dynamic distortion risk measures, the risk allocation problem may be recast as a sequence of strictly convex optimization problems. Moreover, we show that self-financing dynamic risk budgeting strategies with initial wealth of one are scaled versions of the solution of the sequence of convex optimization problems. Furthermore, we develop an actor-critic approach, leveraging the elicitability of dynamic risk measures, to solve for risk budgeting strategies using deep learning. Funding: S. M. Pesenti and S. Jaimungal acknowledge support from the Natural Sciences and Engineering Research Council of Canada [Grants DGECR-2020-00333, RGPIN-2020-04289, RGPIN-2024-04317, RGPIN-2018-05705, and RGPAS-2018-522715]. Y. F. Saporito acknowledges support from the Conselho Nacional de Desenvolvimento Científico e Tecnológico [Grant 306695/2021-9] and the Fundação Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro [Grant E-26/201.375/2022 272760]. R. S. Targinoc acknowledges support from the Conselho Nacional de Desenvolvimento Científico e Tecnológico [Grant 200293/2022-2] and the Fundação Carlos Chagas Filho de Amparo à Pesquisa do Estado do Rio de Janeiro [Grants E-26/201.350, E-26/211.426, and E-26/211.578].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0299},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1208-1229},
  shortjournal = {Oper. Res.},
  title        = {Risk budgeting allocation for dynamic risk measures},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeLuxing: Deep lagrangian underestimate fixing for column-generation-based exact methods. <em>OR</em>, <em>73</em>(3), 1184-1207. (<a href='https://doi.org/10.1287/opre.2023.0398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an innovative variable fixing strategy called de ep L agrangian u nderestimate fi xing (DeLuxing). It is a highly effective approach for removing unnecessary variables in column-generation (CG)-based exact methods used to solve challenging discrete optimization problems commonly encountered in various industries, including vehicle routing problems (VRPs). DeLuxing employs a novel linear programming (LP) formulation with only a small subset of the enumerated variables, which is theoretically guaranteed to yield qualified dual solutions for computing Lagrangian underestimates (LUs). Because of their small sizes, DeLuxing can efficiently solve a sequence of similar LPs to generate multiple high-quality LUs, and thus can, in most cases, remove over 75% of the variables from the enumerated pool. We extend the fundamental concept underpinning the new formulation to contexts beyond variable fixing, namely, variable type relaxation and cutting plane addition. We demonstrate the effectiveness of the proposed method in accelerating CG-based exact methods via the capacitated multitrip vehicle routing problem with time windows (CMTVRPTW), its two important variants with loading times or release dates, and the classic capacitated VRP (CVRP) and VRPTW. Enhanced by DeLuxing and the extensions, one of the best exact methods for solving the CMTVRPTW doubles the size of instances solved optimally for the first time while being more than 7 times on average and up to over 20 times as fast as top-performing exact methods reported in the literature. It further accelerates RouteOpt, one of the world’s fastest VRP solvers, by 45% and 71%, respectively, for solving the 200-node CVRP and 300-node VRPTW instances. Although DeLuxing is less effective for longer routes, it still contributes to an effective primal heuristic. Funding: This research was supported by the National Science Foundation, Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-2309667], and Alibaba Group US, DAMO Academy [Grant AGR00022274]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0398 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0398},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1184-1207},
  shortjournal = {Oper. Res.},
  title        = {DeLuxing: Deep lagrangian underestimate fixing for column-generation-based exact methods},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing the path towards plastic-free oceans. <em>OR</em>, <em>73</em>(3), 1165-1183. (<a href='https://doi.org/10.1287/opre.2023.0515'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing ocean plastic pollution is irreversibly harming ecosystems and human economic activities. We partner with a nonprofit organization and use optimization to help clean up oceans from plastic faster. Specifically, we optimize the route of their plastic collection system in the ocean to maximize the quantity of plastic collected over time. We formulate the problem as a longest path problem in a well-structured graph. However, because collection directly impacts future plastic density, the corresponding edge lengths are nonlinear polynomials. After analyzing the structural properties of the edge lengths, we propose a search-and-bound method, which leverages a relaxation of the problem solvable via dynamic programming and clustering, to efficiently find high-quality solutions (within 6% optimal in practice) and develop a tailored branch-and-bound strategy to solve it to provable optimality. On one year of ocean data, our optimization-based routing approach increases the quantity of plastic collected by more than 60% compared with the current routing strategy, hence speeding up the progress toward plastic-free oceans. Funding: J. Pauphilet and B. Song wish to acknowledge the financial support of the Research and Materials Development Fund (RAMD_Pauphilet_J_22/23_8789) at London Business School. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, are available at https://doi.org/10.1287/opre.2023.0515 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0515},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1165-1183},
  shortjournal = {Oper. Res.},
  title        = {Optimizing the path towards plastic-free oceans},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heuristic (S, t) solutions via an FPTAS for a one-warehouse multiretailer problem. <em>OR</em>, <em>73</em>(3), 1151-1164. (<a href='https://doi.org/10.1287/opre.2024.1177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difficulty of analyzing and optimizing the stochastic one-warehouse multiretailer problem under the ( S , T ) policy motivates the need to consider approximate but high-fidelity systems that are easier to scrutinize. We consider one such model in the setting in which retailers face independent normally distributed demand with given (nonidentical) means and variances. Safety stock is computed via a type-I service-level formula that ignores allocation issues, and the cost function is computed based on a power-of-two (POT) periodic ordering policy. A critical component of finding good solutions for this problem is solving the continuous relaxation of the optimization model involved. In doing so, the linking square root term introduced by the warehouse safety stock complicates this task as the cost does not decouple by individual retailers for a fixed warehouse replenishment interval. We alleviate this issue by substituting the linking term with an accurate piecewise linear estimator. We show how this helps us design a fully polynomial-time approximation scheme for the problem. From this, we can obtain a POT policy that has a guarantee of 1.061 ( 1 + ϵ ) for small ϵ and for any base planning period and 1.021 ( 1 + ϵ ) when the base planning period is optimally chosen, which is essentially the best possible guarantee up to the deterministic result. We show via an empirical study that the algorithm proposed returns heuristic ( S , T ) solutions that exhibit minimal suboptimality relative to the exact solution. Further, the time needed to compute these solutions is milliseconds for a handful of retailers (in contrast to the multiple hours required by the best existing methods) and just a few minutes for a system of one million retailers. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2024.1177 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2024.1177},
  journal      = {Operations Research},
  month        = {5-6},
  number       = {3},
  pages        = {1151-1164},
  shortjournal = {Oper. Res.},
  title        = {Heuristic (S, t) solutions via an FPTAS for a one-warehouse multiretailer problem},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—On the convergence rate of stochastic approximation for gradient-based stochastic optimization. <em>OR</em>, <em>73</em>(2), 1143-1150. (<a href='https://doi.org/10.1287/opre.2023.0055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider stochastic optimization via gradient-based search. Under a stochastic approximation framework, we apply a recently developed convergence rate analysis to provide a new finite-time error bound for a class of problems with convex differentiable structures. For noisy black-box functions, our main result allows us to derive finite-time bounds in the setting where the gradients are estimated via finite-difference estimators, including those based on randomized directions such as the simultaneous perturbation stochastic approximation algorithm. In particular, the convergence rate analysis sheds light on when it may be advantageous to use such randomized gradient estimates in terms of problem dimension and noise levels. Funding: This work was supported by the Air Force Office of Scientific Research [Grant FA95502010211] and the National Science Foundation [Grants IIS-2123684 and CMMI-2027527].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0055},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1143-1150},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On the convergence rate of stochastic approximation for gradient-based stochastic optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recovering Dantzig–Wolfe bounds by cutting planes. <em>OR</em>, <em>73</em>(2), 1128-1142. (<a href='https://doi.org/10.1287/opre.2023.0048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dantzig–Wolfe (DW) decomposition is a well-known technique in mixed-integer programming (MIP) for decomposing and convexifying constraints to obtain potentially strong dual bounds. We investigate cutting planes that can be derived using the DW decomposition algorithm and show that these cuts can provide the same dual bounds as DW decomposition. More precisely, we generate one cut for each DW block, and when combined with the constraints in the original formulation, these cuts imply the objective function cut one can simply write using the DW bound. This approach typically leads to a formulation with lower dual degeneracy that consequently has a better computational performance when solved by standard MIP solvers in the original space. We also discuss how to strengthen these cuts to improve the computational performance further. We test our approach on the multiple knapsack assignment problem and the temporal knapsack problem, and we show that the proposed cuts are helpful in accelerating the solution time without the need to implement branch and price. Funding: This work was supported by the Office of Naval Research [Grant N00014-21-1-2575]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.0048 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0048},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1128-1142},
  shortjournal = {Oper. Res.},
  title        = {Recovering Dantzig–Wolfe bounds by cutting planes},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing Bayes–Nash equilibrium strategies in auction games via simultaneous online dual averaging. <em>OR</em>, <em>73</em>(2), 1102-1127. (<a href='https://doi.org/10.1287/opre.2022.0287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auctions are modeled as Bayesian games with continuous type and action spaces. Determining equilibria in auction games is computationally hard in general, and no exact solution theory is known. We introduce an algorithmic framework in which we discretize type and action space and then learn distributional strategies via online optimization algorithms. One advantage of distributional strategies is that we do not have to make any assumptions on the shape of the bid function. Besides, the expected utility of agents is linear in the strategies. It follows that, if our optimization algorithms converge to a pure strategy, then they converge to an approximate equilibrium of the discretized game with high precision. Importantly, we show that the equilibrium of the discretized game approximates an equilibrium in the continuous game. In a wide variety of auction games, we provide empirical evidence that the approach approximates the analytical (pure) Bayes–Nash equilibrium closely. This speed and precision are remarkable because, in many finite games, learning dynamics do not converge or are even chaotic. In standard models in which agents are symmetric, we find equilibrium in seconds. Whereas we focus on dual averaging, we show that the overall approach converges independent of the regularizer, and alternative online convex optimization methods achieve similar results even though the discretized game satisfies neither monotonicity nor variational stability globally. The method allows for interdependent valuations and different types of utility functions, and it can be used to find equilibrium in auction markets and beyond. Funding: M. Bichler was supported by the Deutsche Forschungsgemeinschaft (DFG) (German Research Foundation) [Grant BI 1057/9]. M. Fichtl and M. Oberlechner were funded by the DFG [Grant GRK 2201/2 - Projektnummer 277991500].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0287},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1102-1127},
  shortjournal = {Oper. Res.},
  title        = {Computing Bayes–Nash equilibrium strategies in auction games via simultaneous online dual averaging},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear classifiers under infinite imbalance. <em>OR</em>, <em>73</em>(2), 1075-1101. (<a href='https://doi.org/10.1287/opre.2021.0376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the behavior of linear discriminant functions for binary classification in the infinite-imbalance limit, where the sample size of one class grows without bound while the sample size of the other remains fixed. The coefficients of the classifier minimize an empirical loss specified through a weight function. We show that for a broad class of weight functions, the intercept diverges but the rest of the coefficient vector has a finite almost sure limit under infinite imbalance, extending prior work on logistic regression. The limit depends on the left-tail growth rate of the weight function, for which we distinguish two cases: subexponential and exponential. The limiting coefficient vectors reflect robustness or conservatism properties in the sense that they optimize against certain worst-case alternatives. In the subexponential case, the limit is equivalent to an implicit choice of upsampling distribution for the minority class. We apply these ideas in a credit risk setting, with particular emphasis on performance in the high-sensitivity and high-specificity regions. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://pubsonline.informs.org/doi/suppl/10.1287/opre.2021.0376 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0376},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1075-1101},
  shortjournal = {Oper. Res.},
  title        = {Linear classifiers under infinite imbalance},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse optimization: Theory and applications. <em>OR</em>, <em>73</em>(2), 1046-1074. (<a href='https://doi.org/10.1287/opre.2022.0382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse optimization describes a process that is the “reverse” of traditional mathematical optimization. Unlike traditional optimization, which seeks to compute optimal decisions given an objective and constraints, inverse optimization takes decisions as input and determines objective and/or constraint parameters that render these decisions approximately or exactly optimal. In recent years, there has been an explosion of interest in the mathematics and applications of inverse optimization. This paper provides a comprehensive review of both the methodological and application-oriented literature in this field. Funding: T. C. Y. Chan received funding support from the Natural Science and Engineering Research Council of Canada. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0382 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0382},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1046-1074},
  shortjournal = {Oper. Res.},
  title        = {Inverse optimization: Theory and applications},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fluid policies, reoptimization, and performance guarantees in dynamic resource allocation. <em>OR</em>, <em>73</em>(2), 1029-1045. (<a href='https://doi.org/10.1287/opre.2022.0601'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many sequential decision problems involve deciding how to allocate shared resources across a set of independent systems at each point in time. A classic example is the restless bandit problem, in which a budget constraint limits the selection of arms. Fluid relaxations provide a natural approximation technique for this broad class of problems. A recent stream of research has established strong performance guarantees for feasible policies based on fluid relaxations. In this paper, we generalize and improve these recent performance results. First, we provide easy-to-implement feasible fluid policies that achieve performance within O ( N ) of optimal, where N is the number of subproblems. This result holds for a general class of dynamic resource allocation problems with heterogeneous subproblems and multiple shared resource constraints. Second, we show using a novel proof technique that a feasible fluid policy that chooses actions using a reoptimized fluid value function achieves performance within O ( N ) of optimal as well. To the best of our knowledge, this performance guarantee is the first one for reoptimization for the general dynamic resource allocation problems that we consider. The scaling of the constants with respect to time in these results implies similar results in the infinite horizon setting. Finally, we develop and analyze a class of feasible fluid-budget balancing policies that stay “close” to actions selected by an optimal fluid policy while simultaneously using as much of the shared resources as possible. We show that this policy achieves performance within O (1) of optimal under particular nondegeneracy assumptions. This result generalizes recent advances for restless bandit problems by considering (a) any finite number of actions for each subproblem and (b) heterogeneous subproblems with a fixed number of types. We demonstrate the use of these techniques on dynamic multiwarehouse inventory problems and find empirically that these fluid-based policies achieve excellent performance, as our theory suggests. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0601 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0601},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1029-1045},
  shortjournal = {Oper. Res.},
  title        = {Fluid policies, reoptimization, and performance guarantees in dynamic resource allocation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed-integer optimization with constraint learning. <em>OR</em>, <em>73</em>(2), 1011-1028. (<a href='https://doi.org/10.1287/opre.2021.0707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish a broad methodological foundation for mixed-integer optimization with learned constraints. We propose an end-to-end pipeline for data-driven decision making in which constraints and objectives are directly learned from data using machine learning, and the trained models are embedded in an optimization formulation. We exploit the mixed-integer optimization representability of many machine learning methods, including linear models, decision trees, ensembles, and multilayer perceptrons, which allows us to capture various underlying relationships between decisions, contextual variables, and outcomes. We also introduce two approaches for handling the inherent uncertainty of learning from data. First, we characterize a decision trust region using the convex hull of the observations to ensure credible recommendations and avoid extrapolation. We efficiently incorporate this representation using column generation and propose a more flexible formulation to deal with low-density regions and high-dimensional data sets. Then, we propose an ensemble learning approach that enforces constraint satisfaction over multiple bootstrapped estimators or multiple algorithms. In combination with domain-driven components, the embedded models and trust region define a mixed-integer optimization problem for prescription generation. We implement this framework as a Python package ( OptiCL ) for practitioners. We demonstrate the method in both World Food Programme planning and chemotherapy optimization. The case studies illustrate the framework’s ability to generate high-quality prescriptions and the value added by the trust region, the use of ensembles to control model robustness, the consideration of multiple machine learning methods, and the inclusion of multiple learned constraints. Funding: This work was supported by the Dutch Scientific Council [Grant OCENW.GROOT.2019.015] and the National Science Foundation [Grant 174530]. Additionally, H. Wiberg was supported by the National Science Foundation Graduate Research Fellowship [Grant 174530]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0707 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0707},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {1011-1028},
  shortjournal = {Oper. Res.},
  title        = {Mixed-integer optimization with constraint learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online matching frameworks under stochastic rewards, product ranking, and unknown patience. <em>OR</em>, <em>73</em>(2), 995-1010. (<a href='https://doi.org/10.1287/opre.2021.0371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study generalizations of online bipartite matching in which each arriving vertex (customer) views a ranked list of offline vertices (products) and matches to (purchases) the first one they deem acceptable. The number of products that the customer has patience to view can be stochastic and dependent on the products seen. We develop a framework that views the interaction with each customer as an abstract resource consumption process and derive new results for these online matching problems under the adversarial, nonstationary, and independent and identically-distributed arrival models, assuming we can (approximately) solve the product ranking problem for each single customer. To that end, we show new results for product ranking under two cascade-click models: an optimal algorithm when each item has its own hazard rate for making the customer depart and a 1/2-approximate algorithm when the customer has a general item-independent patience distribution. We also present a constant-factor 0.027-approximate algorithm in a new model where items are not initially available and arrive over time. We complement these positive results by presenting three additional negative results relating to these problems. Funding: N. Grammel was supported in part by NSF award [CCF-1918749] and by research awards from Amazon and Google. A. Srinivasan was supported in part by NSF awards [CCF-1422569, CCF-1749864, and CCF-1918749], as well as research awards from Adobe, Amazon, and Google. W. Ma was supported in part by a research award from Amazon. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0371 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0371},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {995-1010},
  shortjournal = {Oper. Res.},
  title        = {Online matching frameworks under stochastic rewards, product ranking, and unknown patience},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—Improved sample-complexity bounds in stochastic optimization. <em>OR</em>, <em>73</em>(2), 986-994. (<a href='https://doi.org/10.1287/opre.2018.0340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world network-optimization problems often involve uncertain parameters during the optimization phase. Stochastic optimization is a key approach introduced in the 1950s to address such uncertainty. This paper presents improved upper bounds on the number of samples required for the sample-average approximation method in stochastic optimization. It enhances the sample complexity of existing approaches in this setting, providing faster approximation algorithms for any method that employs this framework. This work is particularly relevant for solving problems like the stochastic Steiner tree problem. Funding: The research of A. Baveja is partially supported by the United States Department of Transportation (via the University Transportation Research Center Program) [Grant 49198-25-26] and the British Council [the UKIERI Research Program]. The work of A. Chavan was done while he was a graduate student at the University of Maryland. The research of A. Srinivasan is partially supported by the National Science Foundation [Awards CNS 1010789, CCF-1422569, CCF-1749864, and CCF-1918749]; Adobe, Inc. [research awards]; Amazon, Inc.; and Google, Inc. The research of P. Xu was supported in part by the National Science Foundation [Awards CNS 1010789 and CCF-1422569 (when he was a graduate student)] and is partially funded by the National Science Foundation [CRII Award IIS-1948157]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2018.0340 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2018.0340},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {986-994},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Improved sample-complexity bounds in stochastic optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally robust optimization under distorted expectations. <em>OR</em>, <em>73</em>(2), 969-985. (<a href='https://doi.org/10.1287/opre.2020.0685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributionally robust optimization (DRO) has arisen as an important paradigm for addressing the issue of distributional ambiguity in decision optimization. In the case in which a decision maker is not risk neutral, the most common scheme applied in DRO for capturing the risk attitude is to employ an expected utility functional. In this paper, we propose to address a decision maker’s risk attitude in DRO by following an alternative scheme known as dual expected utility. In this scheme, a distortion function is applied to convert physical probabilities into subjective probabilities so that the resulting expectation, called a distorted expectation, captures the decision maker’s risk attitude. Unlike an expected utility functional, which is linear in probability, in the dual scheme, the distorted expectation is generally nonlinear in probability. We distinguish DRO based on distorted expectations by coining the term “distributionally robust distortion risk optimization” (DRDRO) and show that DRDRO problems can be equally, if not more, tractable to solve as DRO problems based on utility functionals. Our tractability results hold for any distortion function, and hence, our scheme provides more flexibility in capturing more realistic forms of risk attitudes. These include, as an important example, the inverse S-shaped distortion functionals in cumulative prospect theory. We demonstrate through a numerical example that a production manager who overly weights “very good” and “very bad” outcomes may act as if the manager is risk averse when distributional ambiguity is considered. Funding: J. Cai gratefully acknowledges financial support from the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2022-03354]. J. Y.-M. Li gratefully acknowledges financial support from the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2014-05602]. T. Mao gratefully acknowledges financial support from the National Natural Science Foundation of China [Grants 12371476, 71671176, and 71921001]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2020.0685 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0685},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {969-985},
  shortjournal = {Oper. Res.},
  title        = {Distributionally robust optimization under distorted expectations},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customer scheduling in large service systems under model uncertainty. <em>OR</em>, <em>73</em>(2), 949-968. (<a href='https://doi.org/10.1287/opre.2022.0144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling in the context of many-server queues has received considerable attention. When there are multiple customer classes and many servers, it is common to make simplifying assumptions that result in a “low-fidelity” model, potentially leading to model misspecification. However, empirical evidence suggests that these assumptions may not accurately reflect real-world scenarios. Although relaxing these assumptions can yield a more accurate “high-fidelity” model, it often becomes complex and challenging, if not impossible, to solve. In this paper, we introduce a novel approach for decision makers to generate high-quality scheduling policies for large service systems based on a simple and tractable low-fidelity model instead of its complex and intractable high-fidelity counterpart. At the core of our approach is a robust control formulation, wherein optimization is conducted against an imaginary adversary. This adversary optimally exploits the potential weaknesses of a scheduling rule within prescribed limits defined by an uncertainty set by dynamically perturbing the low-fidelity model. This process assists decision-makers in assessing the vulnerability of a given scheduling policy to model errors stemming from the low-fidelity model. Moreover, our proposed robust control framework is complemented by practical data-driven schemes for uncertainty set selection. Extensive numerical experiments, including a case study based on a U.S. call center data set, substantiate the effectiveness of our framework by revealing scheduling policies that can significantly reduce the system’s costs in comparison with established benchmarks in the literature. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0144 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0144},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {949-968},
  shortjournal = {Oper. Res.},
  title        = {Customer scheduling in large service systems under model uncertainty},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic localization methods for convex discrete optimization via simulation. <em>OR</em>, <em>73</em>(2), 927-948. (<a href='https://doi.org/10.1287/opre.2022.0030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and analyze a set of new sequential simulation-optimization algorithms for large-scale multidimensional discrete optimization via simulation problems with a convexity structure. The “large-scale” notion refers to that the discrete decision variable has a large number of values from which to choose on each dimension of the decision variable. The proposed algorithms are targeted to identify a solution that is close to the optimal solution given any precision level with any given probability. To achieve this target, utilizing the convexity structure, our algorithm design does not need to scan all the choices of the decision variable, but instead sequentially draws a subset of choices of the decision variable and uses them to “localize” potentially near-optimal solutions to an adaptively shrinking region. To show the power of the proposed methods based on the localization idea, we first consider one-dimensional large-scale problems. We develop the shrinking uniform sampling algorithm, which is proved to achieve the target with an optimal expected simulation cost under an asymptotic criterion. For multidimensional problems, we combine the idea of localization with subgradient information and propose a framework to design stochastic cutting-plane methods, whose expected simulation costs have a low dependence on the scale and the dimension of the problems. In addition, utilizing the discrete nature of the problems, we propose a stochastic dimension-reduction algorithm, which does not require prior information about the Lipschitz constant of the objective function, and its simulation costs are upper bounded by a value that is independent of the Lipschitz constant. We implement the proposed algorithms on synthetic problems and queueing simulation-optimization problems and demonstrate better performances compared with benchmark methods especially for large-scale examples. Funding: H. Zhang and J. Lavaei were funded by grants from Army Research Office (ARO), Office of Naval Research (ONR), Air Force Office of Scientific Research(AFOSR), and National Science Foundation (NSF). H. Zhang was partially supported by the Two Sigma PhD Fellowship. Z. Zheng was partially supported by the Hellman Fellows Fund and NSF. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0030 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0030},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {927-948},
  shortjournal = {Oper. Res.},
  title        = {Stochastic localization methods for convex discrete optimization via simulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A doubly stochastic simulator with applications in arrivals modeling and simulation. <em>OR</em>, <em>73</em>(2), 910-926. (<a href='https://doi.org/10.1287/opre.2021.0597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework that integrates classic Monte Carlo simulators and Wasserstein generative adversarial networks to model, estimate, and simulate a broad class of arrival processes with general nonstationary and multidimensional random arrival rates. Classic Monte Carlo simulators have advantages in capturing the interpretable “physics” of a stochastic object, whereas neural network–based simulators have advantages in capturing less interpretable complicated dependence within a high-dimensional distribution. We propose a doubly stochastic simulator that integrates a stochastic generative neural network and a classic Monte Carlo Poisson simulator to utilize the advantages of both. Such integration brings challenges to both theoretical reliability and computational tractability for the estimation of the simulator given real data, in which the estimation is done through minimizing the Wasserstein distance between the distribution of the simulation output and of real data. Regarding theoretical properties, we prove consistency and convergence rate for the estimated simulator under a nonparametric smoothness assumption. Regarding computational efficiency and tractability for the estimation procedure, we address a challenge in gradient evaluation that arises from the discontinuity in the Monte Carlo Poisson simulator. Numerical experiments with synthetic and real data sets are implemented to illustrate the performance of the proposed framework. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2021.0597 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0597},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {910-926},
  shortjournal = {Oper. Res.},
  title        = {A doubly stochastic simulator with applications in arrivals modeling and simulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning and optimization with seasonal patterns. <em>OR</em>, <em>73</em>(2), 894-909. (<a href='https://doi.org/10.1287/opre.2023.0017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A standard assumption adopted in the multiarmed bandit (MAB) framework is that the mean rewards are constant over time. This assumption can be restrictive in the business world as decision makers often face an evolving environment in which the mean rewards are time-varying. In this paper, we consider a nonstationary MAB model with K arms whose mean rewards vary over time in a periodic manner. The unknown periods can be different across arms and scale with the length of the horizon T polynomially. We propose a two-stage policy that combines the Fourier analysis with a confidence bound–based learning procedure to learn the periods and minimize the regret. In stage one, the policy correctly estimates the periods of all arms with high probability. In stage two, the policy explores the periodic mean rewards of arms using the periods estimated in stage one and exploits the optimal arm in the long run. We show that our learning policy incurs a regret upper bound O ˜ ( T ∑ k = 1 K T k ) , where T k is the period of arm k . Moreover, we establish a general lower bound Ω ( T max k { T k } ) for any policy. Therefore, our policy is near optimal up to a factor of K . Funding: The research of N. Chen is partly supported by the Natural Sciences and Engineering Research Council of Canada [Discovery Grant RGPIN-2020-04038]. C. Wang acknowledges support from the National Natural Science Foundation of China [Grants 72293561 and 71802115] and the Tsinghua University Initiative Scientific Research Program. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0017 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0017},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {894-909},
  shortjournal = {Oper. Res.},
  title        = {Learning and optimization with seasonal patterns},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate almost stochastic dominance: Transfer characterizations and sufficient conditions under dependence uncertainty. <em>OR</em>, <em>73</em>(2), 879-893. (<a href='https://doi.org/10.1287/opre.2022.0596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most often, important decisions involve several unknown attributes. This produces a double challenge in the sense that both assessing the individual multiattribute preferences and assessing the joint distribution of the attributes can be extremely hard. To handle the first challenge, we suggest multivariate almost stochastic dominance, a relation based on bounding marginal utilities. We provide necessary and sufficient characterizations in terms of simple transfers, which are easily communicated to decision makers and, thus, can be used for preference elicitation. To handle the second challenge, we develop sufficient conditions that do not consider the dependence structure and are based on either marginal distributions of the attributes or just their means and variances. We apply the theoretical results to a case study of comparing the efficiency of photovoltaic plants. Funding: M. Scarsini is a member of Gruppo Nazionale per l’Analisi Matematica, la Probabilità, e le loro Applicazioni-Istituto Nazionale di Alta Matematica Francesco Severi (GNAMPA-INdAM). His work was partially supported by GNAMPA-INdAM (Project CUP_E53C22001930001 “Limiting behavior of stochastic dynamics in the Schelling segregation model”) and the Italian Ministry of Education, Universities, and Research (Research Projects of National Interest 2017 Project ALGADIMAR “Algorithms, Games, and Digital Markets”). Supplemental Material: The online companion is available at https://doi.org/10.1287/opre.2022.0596 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0596},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {879-893},
  shortjournal = {Oper. Res.},
  title        = {Multivariate almost stochastic dominance: Transfer characterizations and sufficient conditions under dependence uncertainty},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified theory of robust and distributionally robust optimization via the primal-worst-equals-dual-best principle. <em>OR</em>, <em>73</em>(2), 862-878. (<a href='https://doi.org/10.1287/opre.2021.0268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust optimization and distributionally robust optimization are modeling paradigms for decision making under uncertainty where the uncertain parameters are only known to reside in an uncertainty set or are governed by any probability distribution from within an ambiguity set, respectively, and a decision is sought that minimizes a cost function under the most adverse outcome of the uncertainty. In this paper, we develop a rigorous and general theory of robust and distributionally robust nonlinear optimization using the language of convex analysis. Our framework is based on a generalized “primal-worst-equals-dual-best” principle that establishes strong duality between a semi-infinite primal worst and a nonconvex dual best formulation, both of which admit finite convex reformulations. This principle offers an alternative formulation for robust optimization problems that obviates the need to mobilize the machinery of abstract semi-infinite duality theory to prove strong duality in distributionally robust optimization. We illustrate the modeling power of our approach through convex reformulations for distributionally robust optimization problems whose ambiguity sets are defined through general optimal transport distances, which generalize earlier results for Wasserstein ambiguity sets. Funding: This research was supported by the Swiss National Science Foundation [NCCR Automation Grant 51NF40_180545] and the Engineering and Physical Sciences Research Council [Grant EP/R045518/1]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0268 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0268},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {862-878},
  shortjournal = {Oper. Res.},
  title        = {A unified theory of robust and distributionally robust optimization via the primal-worst-equals-dual-best principle},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved decision rule approximations for multistage robust optimization via copositive programming. <em>OR</em>, <em>73</em>(2), 842-861. (<a href='https://doi.org/10.1287/opre.2018.0505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study decision rule approximations for generic multistage robust linear optimization problems. We examine linear decision rules for the case when the objective coefficients, the recourse matrices, and the right-hand sides are uncertain, and we explore quadratic decision rules for the case when only the right-hand sides are uncertain. The resulting optimization problems are NP hard but amenable to copositive programming reformulations that give rise to tight, tractable semidefinite programming solution approaches. We further enhance these approximations through new piecewise decision rule schemes. Finally, we prove that our proposed approximations are tighter than the state-of-the-art schemes and demonstrate their superiority through numerical experiments. Funding: G. A. Hanasusanto was supported by the National Science Foundation [Grants 1752125 and 2153606]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2018.0505 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2018.0505},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {842-861},
  shortjournal = {Oper. Res.},
  title        = {Improved decision rule approximations for multistage robust optimization via copositive programming},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—A data-driven approach to beating SAA out of sample. <em>OR</em>, <em>73</em>(2), 829-841. (<a href='https://doi.org/10.1287/opre.2021.0393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whereas solutions of distributionally robust optimization (DRO) problems can sometimes have a higher out-of-sample expected reward than the sample average approximation (SAA), there is no guarantee. In this paper, we introduce a class of distributionally optimistic optimization (DOO) models and show that it is always possible to “beat” SAA out-of-sample if we consider not just worst case (DRO) models but also best case (DOO) ones. We also show, however, that this comes at a cost: optimistic solutions are more sensitive to model error than either worst case or SAA optimizers and, hence, are less robust, and calibrating the worst or best case model to outperform SAA may be difficult when data are limited. Funding: J. Gotoh is supported in part by the Japan Society for the Promotion of Science [Grant 20H00285]. M. J. Kim is supported in part by the Natural Sciences and Engineering Research Council of Canada [Discovery Grant RGPIN-2015-04019]. A. E. B. Lim is supported by the Ministry of Education, Singapore, under its 2021 Academic Research Fund Tier 2 grant call [Grant MOE-T2EP20121-0014]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0393 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0393},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {829-841},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A data-driven approach to beating SAA out of sample},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—On adaptivity in nonstationary stochastic optimization with bandit feedback. <em>OR</em>, <em>73</em>(2), 819-828. (<a href='https://doi.org/10.1287/opre.2022.0576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the nonstationary stochastic optimization problem with bandit feedback and dynamic regret measures. The seminal work of Besbes et al. (2015) shows that, when aggregated function changes are known a priori, a simple restarting algorithm attains the optimal dynamic regret. In this work, we design a stochastic optimization algorithm with fixed step sizes, which, combined with the multiscale sampling framework in existing research, achieves the optimal dynamic regret in nonstationary stochastic optimization without prior knowledge of function changing budget, thereby closing a question that has been open for a while. We also establish an additional result showing that any algorithm achieving good regret against stationary benchmarks with high probability could be automatically converted to an algorithm that achieves good regret against dynamic benchmarks (for problems that admit O ˜ ( T ) regret against stationary benchmarks in fully adversarial settings, a dynamic regret of O ˜ ( V T 1 / 3 T 2 / 3 ) is expected), which is potentially applicable to a wide class of bandit convex optimization and other types of bandit algorithms.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0576},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {819-828},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On adaptivity in nonstationary stochastic optimization with bandit feedback},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic online fisher markets: Static pricing limits and adaptive enhancements. <em>OR</em>, <em>73</em>(2), 798-818. (<a href='https://doi.org/10.1287/opre.2023.0636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisher markets are one of the most fundamental models for resource allocation. However, the problem of computing equilibrium prices in Fisher markets typically relies on complete knowledge of users’ budgets and utility functions and requires transactions to happen in a static market where all users are present simultaneously. Motivated by these practical considerations, we study an online variant of Fisher markets, wherein users with privately known utility and budget parameters, drawn independently and identically (i.i.d.) from a distribution, arrive sequentially. In this setting, we first study the limitations of static pricing algorithms, which set uniform prices for all users, along two performance metrics: (i) regret, that is, the optimality gap in the objective of the Eisenberg-Gale program between an online algorithm and an oracle with complete information, and (ii) capacity violations, that is, the overconsumption of goods relative to their capacities. Given the limitations of static pricing, we design adaptive posted-pricing algorithms, one with knowledge of the distribution of users’ budget and utility parameters and another that adjusts prices solely based on past observations of user consumption, that is, revealed preference feedback, with improved performance guarantees. Finally, we present numerical experiments to compare our revealed preference algorithm’s performance to several benchmarks. Funding: This work was supported by the Stanford Interdisciplinary Graduate Fellowship. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0636 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0636},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {798-818},
  shortjournal = {Oper. Res.},
  title        = {Stochastic online fisher markets: Static pricing limits and adaptive enhancements},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal fairness in learning and earning: Price protection guarantee and phase transitions. <em>OR</em>, <em>73</em>(2), 775-797. (<a href='https://doi.org/10.1287/opre.2022.0629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the prevalence of price protection guarantee which helps to promote temporal fairness in dynamic pricing, we study the impact of such policy on the design of online learning algorithms for data-driven dynamic pricing with initially unknown customer demand. Under the price protection guarantee, a customer who purchased a product in the past can receive a refund from the seller during the so-called price protection period (typically defined as a certain time window after the purchase date) in case the seller decides to lower the price. We consider a setting where a firm sells a product over a horizon of T time steps. For this setting, we characterize how the value of M , the length of the price protection period, can affect the optimal regret of the learning process. We derive the optimal regret by first establishing a fundamental impossible regime with the novel refund-aware regret lower bound analysis. Then, we propose LEAP , a phased exploration type algorithm for Learning and EArning under Price Protection, to match this lower bound up to logarithmic factors or even doubly logarithmic factors (when there are only two prices available to the seller). Our results reveal the surprising phase transitions of the optimal regret with respect to M . Specifically, when M is not too large, the optimal regret has no major difference when compared with that of the classic setting with no price protection guarantee. In addition, there also exists an upper limit on how much the optimal regret can deteriorate when M grows large. Finally, we conduct extensive numerical simulations with both synthetic and real-world data sets to show the benefit of LEAP over other heuristic methods for this problem. The numerical results suggest that under certain realistic assumptions, it is indeed beneficial for the seller to set a longer price protection period. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0629 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0629},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {775-797},
  shortjournal = {Oper. Res.},
  title        = {Temporal fairness in learning and earning: Price protection guarantee and phase transitions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Political districting to minimize county splits. <em>OR</em>, <em>73</em>(2), 752-774. (<a href='https://doi.org/10.1287/opre.2023.0094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When partitioning a state into political districts, a common criterion is that political subdivisions, like counties, should not be split across multiple districts. This criterion is encoded into most state constitutions and is sometimes enforced quite strictly by the courts. However, map drawers, courts, and the public typically do not know what amount of splitting is truly necessary, even to satisfy basic criteria, like contiguity and population balance. In this paper, we provide answers for all congressional, state senate, and state house districts in the United States using 2020 census data. Our approach is based on integer programming. The associated codes and experimental results are available on GitHub. Funding: This material is based upon work supported by the National Science Foundation [Grant 1942065]. Supplementary Material: All supplemental materials, including the computer code that supports the findings of this study, are available at https://doi.org/10.1287/opre.2023.0094 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0094},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {752-774},
  shortjournal = {Oper. Res.},
  title        = {Political districting to minimize county splits},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EFX: A simpler approach and an (Almost) optimal guarantee via rainbow cycle number. <em>OR</em>, <em>73</em>(2), 738-751. (<a href='https://doi.org/10.1287/opre.2023.0433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of envy-freeness up to any good (EFX) allocations is a fundamental open problem in discrete fair division. The goal is to determine the existence of an allocation of a set of indivisible goods among n agents for which no agent envies another, following the removal of any single good from the other agent’s bundle. Because the general problem has been elusive, progress is made on two fronts: (i) proving existence when n is small and (ii) proving the existence of relaxations of EFX. In this paper, we improve and simplify the state-of-the-art results on both fronts with new techniques. For the case of three agents, the existence of EFX was first shown with additive valuations and then extended to nice-cancelable valuations. As our first main result, we simplify and improve this result by showing the existence of EFX allocations when two of the agents have general monotone valuations and one has a maximin share (MMS)–feasible valuation (a strict generalization of nice-cancelable valuation functions). Our approach is significantly simpler than the previous ones, and it also avoids using the standard concepts of envy graph and champion graph and may find use in other fair-division problems. Second, we consider approximate EFX allocations with few unallocated goods (charity). Through a promising new method using a problem in extremal combinatorics called rainbow cycle number (RCN), the existence of ( 1 − ϵ ) -EFX allocation with O ( ( n / ϵ ) 4 5 ) charity was established. This is done by upper bounding the RCN by O ( d 4 ) in d -dimension. They conjecture RCN to be O ( d ) . We almost settle this conjecture by improving the upper bound to O ( d log d ) and thereby get (almost) optimal charity of O ˜ ( ( n / ϵ ) 1 2 ) that is possible through this method. Our technique is much simpler than the previous ones and is based on the probabilistic method. Funding: This work was supported by the Division of Computing and Communication Foundations (B. R. Chaudhury, R. Mehta, J. Garg) [Grants CCF-1750436, CCF-1942321, CCF-2334461], the National Science Foundation (N. Alon) [Grant DMS-2154082], and the United States–Israel Binational Science Foundation (N. Alon) [Grant 2018267].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0433},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {738-751},
  shortjournal = {Oper. Res.},
  title        = {EFX: A simpler approach and an (Almost) optimal guarantee via rainbow cycle number},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural estimation of markov decision processes in high-dimensional state space with finite-time guarantees. <em>OR</em>, <em>73</em>(2), 720-737. (<a href='https://doi.org/10.1287/opre.2022.0511'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the task of estimating a structural model of dynamic decisions by a human agent based on the observable history of implemented actions and visited states. This problem has an inherent nested structure: In the inner problem, an optimal policy for a given reward function is identified, whereas in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper, we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising reward estimation accuracy. In the proposed algorithm, each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show the proposed algorithm converges to a stationary solution with a finite-time guarantee. Further, if the reward is parameterized linearly, the algorithm approximates the maximum likelihood estimator sublinearly. Funding: M. Hong and S. Zeng are supported by the National Science Foundation [Grants EPCN-2311007 and CCF-1910385]. This work is also part of AI-CLIMATE: “AI Institute for Climate-Land Interactions, Mitigation, Adaptation, Tradeoffs and Economy” and is supported by the U.S. Department of Agriculture National Institute of Food and Agriculture and the National Science Foundation National AI Research Institutes [Competitive Award 2023-67021-39829]. A. Garcia is partially supported by the Army Research Office [Grant W911NF-22-1-0213]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0511 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0511},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {720-737},
  shortjournal = {Oper. Res.},
  title        = {Structural estimation of markov decision processes in high-dimensional state space with finite-time guarantees},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient algorithms for a class of stochastic hidden convex optimization and its applications in network revenue management. <em>OR</em>, <em>73</em>(2), 704-719. (<a href='https://doi.org/10.1287/opre.2022.0216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of stochastic nonconvex optimization in the form of min x ∈ X F ( x ) ≔ E ξ [ f ( ϕ ( x , ξ ) ) ] , that is, F is a composition of a convex function f and a random function ϕ . Leveraging an (implicit) convex reformulation via a variable transformation u = E [ ϕ ( x , ξ ) ] , we develop stochastic gradient-based algorithms and establish their sample and gradient complexities for achieving an ϵ -global optimal solution. Interestingly, our proposed Mirror Stochastic Gradient (MSG) method operates only in the original x -space using gradient estimators of the original nonconvex objective F and achieves O ˜ ( ϵ − 2 ) complexities, matching the lower bounds for solving stochastic convex optimization problems. Under booking limits control, we formulate the air-cargo network revenue management (NRM) problem with random two-dimensional capacity, random consumption, and routing flexibility as a special case of the stochastic nonconvex optimization, where the random function ϕ ( x , ξ ) = x ∧ ξ , that is, the random demand ξ truncates the booking limit decision x . Extensive numerical experiments demonstrate the superior performance of our proposed MSG algorithm for booking limit control with higher revenue and lower computation cost than state-of-the-art bid-price-based control policies, especially when the variance of random capacity is large. Funding: This work was partly supported by the National Science Foundation [Grants CMMI-1761699, CRII-1755829], the ZJU-UIUC Institute Research Program, and NCCR Automation in Switzerland. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0216 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0216},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {704-719},
  shortjournal = {Oper. Res.},
  title        = {Efficient algorithms for a class of stochastic hidden convex optimization and its applications in network revenue management},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On (Random-order) online contention resolution schemes for the matching polytope of (Bipartite) graphs. <em>OR</em>, <em>73</em>(2), 689-703. (<a href='https://doi.org/10.1287/opre.2023.0339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Contention Resolution Schemes (OCRSs) represent a modern tool for selecting a subset of elements, subject to resource constraints, when the elements are presented to the algorithm sequentially. OCRSs have led to some of the best-known competitive ratio guarantees for online resource allocation problems, with the added benefit of treating different online decisions—accept/reject, probing, pricing—in a unified manner. This paper analyzes OCRSs for resource constraints defined by matchings in graphs, a fundamental structure in combinatorial optimization. We consider two dimensions of variants: the elements being presented in adversarial or random order; and the graph being bipartite or general. We improve the state of the art for all combinations of variants, both in terms of algorithmic guarantees and impossibility results. Some of our algorithmic guarantees are best-known, even compared with Contention Resolution Schemes that can choose the order of arrival or are offline. All in all, our results for OCRSs directly improve the best-known competitive ratios for online accept/reject, probing, and pricing problems on graphs in a unified manner. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0339 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0339},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {689-703},
  shortjournal = {Oper. Res.},
  title        = {On (Random-order) online contention resolution schemes for the matching polytope of (Bipartite) graphs},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ordinary and prophet planning under uncertainty in bernoulli congestion games. <em>OR</em>, <em>73</em>(2), 672-688. (<a href='https://doi.org/10.1287/opre.2023.0252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an atomic congestion game in which each player i participates in the game with an exogenous and known probability p i ∈ ( 0 , 1 ] , independently of everybody else, or stays out and incurs no cost. We compute the parameterized price of anarchy to characterize the impact of demand uncertainty on the efficiency of selfish behavior, considering two different notions of a social planner. A prophet planner knows the realization of the random participation in the game; the ordinary planner does not. As a consequence, a prophet planner can compute an adaptive social optimum that selects different solutions depending on the players who turn out to be active, whereas an ordinary planner faces the same uncertainty as the players and can only minimize the expected social cost according to the player participation distribution. For both types of planners, we obtain tight bounds for the price of anarchy by solving suitable optimization problems parameterized by the maximum participation probability q = max i p i . In the case of affine costs, we find an analytic expression for the corresponding bounds. Funding: The research of R. Cominetti was supported by Fondo Nacional de Desarrollo Científico y Tecnológico [Grants 1171501 and ANID/PIA/ACT192094]. M. Scarsini gratefully acknowledges the support and hospitality of Fondo Nacional de Desarrollo Científico y Tecnológico [Grant 1130564] and Núcleo Milenio “Información y Coordinación en Redes.” The research of M. Scarsini was supported by the Gruppo Nazionale per l’Analisi Matematica, la Probabilità e le loro Applicazioni [Grant CUP_E53C22001930001], the Ministero dell’Università e della Ricerca [Grants PRIN 2017 ALGADIMAR and PRIN 2022 2022EKNE5K], and the European Union–Next Generation EU, Component M4C2, Investment 1.1 [Grant PRIN PNRR P2022XT8C8]. This work was also supported by European Cooperation in Science and Technology [Grant CA16228 GAMENET]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0252 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0252},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {672-688},
  shortjournal = {Oper. Res.},
  title        = {Ordinary and prophet planning under uncertainty in bernoulli congestion games},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotically tight bounds on the optimal pricing strategy with patient customers. <em>OR</em>, <em>73</em>(2), 664-671. (<a href='https://doi.org/10.1287/opre.2021.0459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers a monopolist seller facing both patient and impatient customers. Given the current price, the impatient customers will either purchase or leave immediately, depending on the relative magnitude between this price and their valuation of the product. In comparison, the patient customers will wait for some periods to see if the price will drop to their valuation, and if that occurs, they will purchase immediately. The monopolist designs the pricing strategy to maximize the long-run average revenue from them. We give tight bounds on both the optimal strategy’s cycle period and the optimal revenue when the patient customers possess a high patience level. This result answers the open question of the optimal cycle period raised by extant work. Later we also extend our theoretical result to the general case with multiple patience levels. Funding: C. Zhou is supported in part by NSFC [Grant 11871364] and IoTeX Foundation Industry [Grant A-8001180-00-00]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2021.0459 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0459},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {664-671},
  shortjournal = {Oper. Res.},
  title        = {Asymptotically tight bounds on the optimal pricing strategy with patient customers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalization guarantees for multi-item profit maximization: Pricing, auctions, and randomized mechanisms. <em>OR</em>, <em>73</em>(2), 648-663. (<a href='https://doi.org/10.1287/opre.2021.0026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study multi-item profit maximization when there is an underlying distribution over buyers’ values. In practice, a full description of the distribution is typically unavailable, so we study the setting where the mechanism designer only has samples from the distribution. If the designer uses the samples to optimize over a complex mechanism class—such as the set of all multi-item, multibuyer mechanisms—a mechanism may have high average profit over the samples, but low expected profit. This raises the central question of this paper: How many samples are sufficient to ensure that a mechanism’s average profit is close to its expected profit? To answer this question, we uncover structure shared by many pricing, auction, and lottery mechanisms: For any set of buyers’ values, profit is piecewise linear in the mechanism’s parameters. Using this structure, we prove new bounds for mechanism classes not yet studied in the sample-based mechanism design literature and match or improve over the best-known guarantees for many classes. Finally, we provide tools for optimizing an important tradeoff: More complex mechanisms typically have higher average profit over the samples than simpler mechanisms, but more samples are required to ensure that average profit nearly matches expected profit. Funding: This material is based on work supported by the National Science Foundation [Grants CCF-1422910, CCF-1535967, CCF-1733556, CCF-1910321, IIS-1617590, IIS-1618714, IIS-1718457, IIS-1901403, RI-2312342, SES-1919453 and a Graduate Research Fellowship]; the Army Research Office [Awards W911NF2010081, W911NF1710082, and W911NF2210266]; Office of Naval Research [Award N00014-23-1-2876]; the Defense Advanced Research Projects Agency [Cooperative Agreement HR00112020003]; Vannevar Bush Faculty Fellowship; an Amazon [Research Award]; a Microsoft Research [Faculty Fellowship]; an Amazon Web Services [Machine Learning Research Award]; a Bloomberg [Data Science research grant]; an International Business Machines Corporation [PhD Fellowship]; and a fellowship from Carnegie Mellon University's Center for Machine Learning and Health. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0026 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0026},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {648-663},
  shortjournal = {Oper. Res.},
  title        = {Generalization guarantees for multi-item profit maximization: Pricing, auctions, and randomized mechanisms},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monetizing positive externalities to mitigate the infrastructure underinvestment problem. <em>OR</em>, <em>73</em>(2), 632-647. (<a href='https://doi.org/10.1287/opre.2023.0075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many cities face challenges in financing their infrastructure. If a decision maker cannot capture all the benefits of its investment, there is a risk of underinvestment. Hong Kong’s transit operator designed a scheme in which it not only receives fare revenues, but also participates in a property management business, exploiting the positive externalities of public transport on nearby property prices. We develop a stochastic Stackelberg game of timing to explore the rationale of this scheme. The underlying problem is nontrivial because the operator faces a two-dimensional optimal stopping problem that cannot be reduced by a change of numéraire. We determine the operator’s optimal investment policy via the intermediation of a “penalized problem” and derive comparative statics. We determine the circumstances under which monetizing positive externalities effectively favors infrastructure investment. Other management problems have similar structures. Funding: This work was supported by the National Science Foundation [Grant NSF-DMS 220 4795]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0075 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0075},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {632-647},
  shortjournal = {Oper. Res.},
  title        = {Monetizing positive externalities to mitigate the infrastructure underinvestment problem},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Budget-driven multiperiod hub location: A robust time-series approach. <em>OR</em>, <em>73</em>(2), 613-631. (<a href='https://doi.org/10.1287/opre.2022.0319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the (un)capacitated multiperiod hub location problem with uncertain periodic demands. With a distributionally robust approach that considers time series, we build a model driven by budgets on periodic costs. In particular, we construct a nested ambiguity set that characterizes uncertain periodic demands via a general multivariate time-series model, and to ensure stable periodic costs, we propose to constrain each expected periodic cost within a budget whereas optimizing the robustness level by maximizing the size of the nested ambiguity set. Statistically, the nested ambiguity set ensures that the model’s solution enjoys finite-sample performance guarantees under certain regularity conditions on the underlying VAR( p ) or VARMA( p , q ) process of the stochastic demand. Operationally, we show that our budget-driven model in the uncapacitated case essentially optimizes a “Sharpe ratio”–type criterion over the worst case among all periods, and we discuss how cost budgets would affect the optimal robustness level. Computationally, the uncapacitated model can be efficiently solved via a bisection search algorithm that solves (in each iteration) a mixed-integer conic program, whereas the capacitated model can be approximated by using decision rules. Finally, numerical experiments demonstrate the attractiveness and competitiveness of our proposed model. Funding: Z. Chen is funded in part by the Hong Kong Research Grants Council General Research Fund [CUHK-11507223] and the National Natural Science Foundation of China [72394395, 72422002]. S. Wang is supported by the National Natural Science Foundation of China [Grants 72471224, 72171221, 71922020, and 71988101], the Fundamental Research Funds for the Central Universities [Grant UCAS-E2ET0808X2], and a grant from MOE Social Science Laboratory of Digital Economic Forecasts and Policy Simulation at UCAS. J. Hu is supported by the National Natural Science Foundation of China [Grants 72192843 and 71872171]. Supplemental Material: All supplemental materials, including the code, data, and files required to reproduce the results, were reviewed and are available at https://doi.org/10.1287/opre.2022.0319 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0319},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {613-631},
  shortjournal = {Oper. Res.},
  title        = {Budget-driven multiperiod hub location: A robust time-series approach},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust workforce management with crowdsourced delivery. <em>OR</em>, <em>73</em>(2), 595-612. (<a href='https://doi.org/10.1287/opre.2023.0125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how crowdsourced delivery platforms with both contracted and ad hoc couriers can effectively manage their workforce to meet delivery demands amidst uncertainties. Our objective is to minimize the hiring costs of contracted couriers and the crowdsourcing costs of ad hoc couriers, while considering the uncertain availability and behavior of the latter. Because of the complication of calibrating these uncertainties through data-driven approaches, we instead introduce a basic reduced information model to estimate the upper bound of the crowdsourcing cost and a generalized reduced information model to obtain a tighter bound. Subsequently, we formulate a robust satisficing model associated with the generalized reduced information model and show that a binary search algorithm can tackle the model exactly by solving a modest number of convex optimization problems. Our numerical tests using Solomon’s data sets show that reduced information models provide decent approximations for practical delivery scenarios. Simulation tests further demonstrate that the robust satisficing model has better out-of-sample performance than the empirical optimization model that minimizes the total cost under historical scenarios. Funding: C. Cheng was supported by the National Natural Science Foundation of China [Grants 72471042, 72101049, and 72232001] and the Fundamental Research Funds for the Central Universities [Grant DUT23RC(3)045]. M. Sim and Y. Zhao were supported by the Ministry of Education, Singapore, under its 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010]. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0125 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0125},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {595-612},
  shortjournal = {Oper. Res.},
  title        = {Robust workforce management with crowdsourced delivery},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Search for an immobile hider on a binary tree with unreliable locational information. <em>OR</em>, <em>73</em>(2), 583-594. (<a href='https://doi.org/10.1287/opre.2023.0128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial search of a network for an immobile Hider (or target) was introduced and solved for rooted trees by Shmuel Gal in 1979. In this zero-sum game, a Hider picks a point to hide on the tree and a Searcher picks a unit speed trajectory starting at the root. The payoff (to the Hider) is the search time. In Gal’s model (and many subsequent investigations), the Searcher receives no additional information after the Hider chooses his location. In reality, the Searcher will often receive such locational information. For homeland security, mobile sensors on vehicles have been used to locate radioactive material stashed in an urban environment. In a military setting, mobile sensors can detect chemical signatures from land mines. In predator-prey search, the predator often has specially attuned senses (hearing for wolves, vision for eagles, smell for dogs, sonar for bats, pressure sensors for sharks) that may help it locate the prey. How can such noisy locational information be used by the Searcher to modify her route? We model such information as signals which indicate which of two branches of a binary tree should be searched first, where all signals have a known accuracy p < 1. Our solution calculates which branch (at every branch node) is favored , meaning it should always be searched first when the signal is in that direction. When the signal is in the other direction, we calculate the probability the signal should be followed. Compared with the optimal Hider strategy in the classic search game of Gal, the Hider’s optimal distribution for this model is more skewed toward leaf nodes that are further from the root. Funding: This work was supported by the Air Force Office of Scientific Research [Grant FA9550-23-1-0556]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0128 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0128},
  journal      = {Operations Research},
  month        = {3-4},
  number       = {2},
  pages        = {583-594},
  shortjournal = {Oper. Res.},
  title        = {Search for an immobile hider on a binary tree with unreliable locational information},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the optimality of greedy policies in dynamic matching. <em>OR</em>, <em>73</em>(1), 560-582. (<a href='https://doi.org/10.1287/opre.2021.0596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study centralized dynamic matching markets with finitely many agent types and heterogeneous match values. A network topology describes the pairs of agent types that can form a match and the value generated from each match. A matching policy is hindsight optimal if the policy can (nearly) maximize the total value simultaneously at all times. We find that suitably designed greedy policies are hindsight optimal in two-way matching networks. This implies that there is essentially no positive externality from having agents waiting to form future matches. We first show that the greedy longest-queue policy with a minor variation is hindsight optimal. Importantly, the policy is greedy relative to a residual network, which includes only nonredundant matches with respect to the static optimal matching rates. Moreover, when the residual network is acyclic (e.g., as in two-sided networks), we prescribe a greedy static priority policy that is also hindsight optimal. The priority order of this policy is robust to arrival rate perturbations that do not alter the residual network. Hindsight optimality is closely related to the lengths of type-specific queues. Queue lengths cannot be smaller (in expectation) than of the order of ϵ − 1 , where ϵ is the general position gap that quantifies the stability in the network. The greedy longest-queue policy achieves this lower bound. Funding: This work was supported by National Science Foundation (CMMI-2010940) and U.S. Department of Defense (STTR A18B-T007).},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0596},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {560-582},
  shortjournal = {Oper. Res.},
  title        = {On the optimality of greedy policies in dynamic matching},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shape-constrained regression using sum of squares polynomials. <em>OR</em>, <em>73</em>(1), 543-559. (<a href='https://doi.org/10.1287/opre.2021.0383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hierarchy of semidefinite programs (SDPs) for the problem of fitting a shape-constrained (multivariate) polynomial to noisy evaluations of an unknown shape-constrained function. These shape constraints include convexity or monotonicity over a box. We show that polynomial functions that are optimal to any fixed level of our hierarchy form a consistent estimator of the underlying shape-constrained function. As a by-product of the proof, we establish that sum of squares-convex polynomials are dense in the set of polynomials that are convex over an arbitrary box. A similar sum-of-squares-type density result is established for monotone polynomials. In addition, we classify the complexity of convex and monotone polynomial regression as a function of the degree of the polynomial regressor. Whereas our results show NP-hardness of these problems for degree three or larger, we can check numerically that our SDP-based regressors often achieve a similar training error at low levels of the hierarchy. Finally, on the computational side, we present an empirical comparison of our SDP-based convex regressors with the convex least squares estimator introduced in Hildreth [ Hildreth C (1954) Point estimates of ordinates of concave functions. J. Amer. Statist. Assoc. 49(267):598–619] and Holloway [ Holloway CA (1979) On the estimation of convex functions. Oper. Res. 27(2):401–407] and show that our regressor is valuable in settings in which the number of data points is large and the dimension is relatively small. We demonstrate the performance of our regressor for the problem of computing optimal transport maps in a color transfer task and that of estimating the optimal value function of a conic program. A real-time application of the latter problem to inventory management contract negotiation is presented. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0383 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0383},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {543-559},
  shortjournal = {Oper. Res.},
  title        = {Shape-constrained regression using sum of squares polynomials},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal routing under demand surges: The value of future arrival rates. <em>OR</em>, <em>73</em>(1), 510-542. (<a href='https://doi.org/10.1287/opre.2022.0282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the growing availability of advanced demand forecast tools, we study how to use future demand information in designing routing strategies in queueing systems under demand surges. We consider a parallel server system operating in a nonstationary environment with general time-varying arrival rates. Servers are cross-trained to help nonprimary customer classes during demand surges. However, such flexibility comes with various operational costs, such as a loss of efficiency and inconvenience in coordination. We characterize how to incorporate the future arrival information into the routing policy to balance the tradeoff between various costs and quantify the benefit of doing so. Based on transient fluid control analysis, we develop a two-stage index-based look-ahead policy that explicitly takes the overflow costs and future arrival rates into account. The policy has an interpretable structure, is easy to implement and is adaptive when the future arrival information is inaccurate. In the special case of the N-model, we prove that this policy is asymptotically optimal even in the presence of certain prediction errors in the demand forecast. We substantiate our theoretical analysis with extensive numerical experiments, showing that our policy achieves superior performance compared with other benchmark policies (i) in complicated parallel server systems and (ii) when the demand forecast is imperfect with various forms of prediction errors. Funding: This work was supported by the National Science Foundation Civil, Mechanical, and Manufacturing Innovation [Grant 1944209]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0282 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0282},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {510-542},
  shortjournal = {Oper. Res.},
  title        = {Optimal routing under demand surges: The value of future arrival rates},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal routing to parallel servers in heavy traffic. <em>OR</em>, <em>73</em>(1), 483-509. (<a href='https://doi.org/10.1287/opre.2022.0055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a system with heterogeneous parallel servers, each with an infinite waiting room. Upon arrival, a job is routed to the queue of one of the servers, possibly depending on the dynamic state information such as the real-time queue lengths, the arrival, and service history of jobs. The objective is to find the routing policy that best uses the available state information to minimize the expected stationary queue length. In this paper, we establish the diffusion limit for the round-robin policy (respectively, arrival-chasing policy, service-chasing policy), and show that with properly chosen parameters, it achieves the optimal performance asymptotically within the class of admissible policies that require no state information (respectively, require arrival history, service history). Like the jointhe-shortest-queue and the balanced routing policies that use real-time queue length information, the optimal service-chasing policy is also asymptotically optimal over all admissible policies. Further analysis of the diffusion limits yields a number of insights into the performance of these routing policies and reveals the value of various state information. We numerically demonstrate the effectiveness of the estimators derived from the diffusion limits for the policies being studied and obtain interesting observations. We also address the problem of interchange of limits under the aforementioned policies, which justifies the stationary performance of the diffusion limit as a valid approximation to that of the original system under respective policies. Methodologically, this study contributes to the application of the BIGSTEP method for constructing control policy to optimize stationary performance and the recipe for justifying the interchange of limits in the heavy traffic analysis of stochastic processing networks. Funding: This work was supported by the Research Grants Council, University Grants Committee [GRF Grant 15501421 and NSFC/RGC Grant N_PolyU590/22]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0055 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0055},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {483-509},
  shortjournal = {Oper. Res.},
  title        = {Optimal routing to parallel servers in heavy traffic},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The when and how of delegated search. <em>OR</em>, <em>73</em>(1), 461-482. (<a href='https://doi.org/10.1287/opre.2019.0498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firms often outsource search processes, such as the acquisition of real estate, new technologies, or talent. To ensure the efficacy of such delegated search, firms need to carefully design incentive contracts to attenuate the ill effects of agency issues. We model this problem using a dynamic principal-agent framework, embedding the standard sequential search model. The optimal contract pays the agent a fixed per-period fee plus a bonus for finding a suitable alternative. The bonus size is defined a priori and decreases over time, whereas the range of values deemed suitable expands over time. If the principal is unable to contract on the value of the delivered alternatives, the optimal contract consists of two parts. Early in the search process, the agent is granted a small bonus for every alternative brought to the principal, irrespective of whether the principal accepts it; late in the search process, the agent is awarded a comparatively larger bonus, which is decreasing in time, but only if the principal accepts the alternative. We also consider situations where the principal chooses between searching in house and outsourcing. This decision is shown to hinge on the principal’s trade-off between speed and quality. The age-old aphorism “if you want it done right, do it yourself” holds, as in-house search is optimal for a principal who prioritizes quality. Yet, in the context of our model, we also establish an addendum: “If you want it done fast, hire someone else to do it.” Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2019.0498 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.0498},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {461-482},
  shortjournal = {Oper. Res.},
  title        = {The when and how of delegated search},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strategic pricing in volatile markets. <em>OR</em>, <em>73</em>(1), 444-460. (<a href='https://doi.org/10.1287/opre.2021.0550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study dynamic entry deterrence through limit pricing in markets subject to persistent demand shocks. An incumbent is privately informed about its costs, high or low, and can deter a Bayesian potential entrant by setting its prices strategically. The entrant can irreversibly enter the market at any time for a fixed cost, earning a payoff that depends on the market conditions and the incumbent’s unobserved type. Market demand evolves as a geometric Brownian motion. When market demand is low, entry becomes a distant threat, so there is little benefit to further deterrence, and, in equilibrium, a weak incumbent becomes tempted to reveal itself by raising its prices. We characterize a unique equilibrium in which the entrant enters when market demand is sufficiently high (relative to the incumbent’s current reputation), and the weak incumbent mixes over revealing itself when market demand is sufficiently low. In this equilibrium, pricing and entry decisions exhibit path dependence, depending not only on the market’s current size, but also its historical minimum. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2021.0550 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0550},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {444-460},
  shortjournal = {Oper. Res.},
  title        = {Strategic pricing in volatile markets},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual inverse optimization: Offline and online learning. <em>OR</em>, <em>73</em>(1), 424-443. (<a href='https://doi.org/10.1287/opre.2021.0369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problems of offline and online contextual optimization with feedback information, where instead of observing the loss, we observe, after the fact, the optimal action an oracle with full knowledge of the objective function would have taken. We aim to minimize regret, which is defined as the difference between our losses and the ones incurred by an all-knowing oracle. In the offline setting, the decision maker has information available from past periods and needs to make one decision, whereas in the online setting, the decision maker optimizes decisions dynamically over time based a new set of feasible actions and contextual functions in each period. For the offline setting, we characterize the optimal minimax policy, establishing the performance that can be achieved as a function of the underlying geometry of the information induced by the data. In the online setting, we leverage this geometric characterization to optimize the cumulative regret. We develop an algorithm that yields the first regret bound for this problem that is logarithmic in the time horizon. Finally, we show via simulation that our proposed algorithms outperform previous methods from the literature. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2021.0369 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0369},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {424-443},
  shortjournal = {Oper. Res.},
  title        = {Contextual inverse optimization: Offline and online learning},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Competitive algorithms for the online minimum peak job scheduling. <em>OR</em>, <em>73</em>(1), 408-423. (<a href='https://doi.org/10.1287/opre.2021.0080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a fundamental online scheduling problem called the minimum peak job scheduling (MPJS) problem. In this problem, there is a sequence of arriving jobs, each with a specified required scheduled time for one unit of a scarce and reusable resource. The goal is to schedule each job upon arrival within a scheduling interval to minimize the resulting peak utilization (i.e., the maximum number of units used simultaneously throughout the entire scheduling interval). The MPJS problem captures many practical settings of real-time appointment scheduling. Its offline version where all jobs are known in advance is equivalent to the well-known bin-packing problem, where jobs correspond to items and the unit resource is a bin. However, the online variant of MPJS allows additional flexibility in that initially, one only commits to the scheduling time, but the allocation to the resources can be done later. In the bin-packing problem, this corresponds to the ability to move items across bins. Some relaxed versions of online bin-packing problems have already been studied, but none fundamentally capture the MPJS model studied in this paper. The paper describes the first competitive online algorithm to the MPJS problem called the harmonic rematching (HR) algorithm. The analysis shows that the HR algorithm has an asymptotic competitive ratio below 1.5. The fact that the current best lower bound on randomized online algorithms for the bin-packing problem is 1.536 highlights the fundamental difference between these two related models. Funding: The work of C. Escribe was partially supported by the Centre for Humane Innovations in Clinical Care [Grant 4000093665]. The work of M. Hu was partially funded by an MGH-MIT Fellowship. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0080 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0080},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {408-423},
  shortjournal = {Oper. Res.},
  title        = {Competitive algorithms for the online minimum peak job scheduling},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian process-based random search for continuous optimization via simulation. <em>OR</em>, <em>73</em>(1), 385-407. (<a href='https://doi.org/10.1287/opre.2021.0303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random search is an important category of algorithms to solve continuous optimization via simulation problems. To design an efficient random search algorithm, the handling of the triple “E” (i.e., exploration, exploitation and estimation) is critical. The first two E’s refer to the design of sampling distribution to balance explorative and exploitative searches, whereas the third E refers to the estimation of objective function values based on noisy simulation observations. In this paper, we propose a class of Gaussian process-based random search (GPRS) algorithms, which provide a new framework to handle the triple “E.” In each iteration, algorithms under the framework build a Gaussian process surrogate model to estimate the objective function based on single observation of each sampled solution and randomly sample solutions from a lower-bounded sampling distribution. Under the assumption of heteroscedastic and known simulation noise, we prove the global convergence of GPRS algorithms. Moreover, for Gaussian processes having continuously differentiable sample paths, we show that the rate of convergence of GPRS algorithms can be no slower than O p ( n − 1 / ( d + 2 ) ) . Then, we introduce a specific GPRS algorithm to show how to design an integrated GPRS algorithm with adaptive sampling distributions and how to implement the algorithm efficiently. Numerical experiments show that the algorithm has good performances, even for problems where the variances of simulation noises are unknown. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72031007, 72091211, 71931007]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0303 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0303},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {385-407},
  shortjournal = {Oper. Res.},
  title        = {Gaussian process-based random search for continuous optimization via simulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expanding service capabilities through an on-demand workforce. <em>OR</em>, <em>73</em>(1), 363-384. (<a href='https://doi.org/10.1287/opre.2021.0651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An on-demand workforce can greatly benefit a traditional call center by allowing it to adjust its service capacity on demand quickly. Despite its conceptual elegance, the operationalization of this process is challenging due to the various sources of randomness involved. The purpose of this paper is to help call centers enhance service levels while keeping operating expenses low by taking advantage of an on-call pool of temporary agents in day-to-day operations. For that purpose, we develop a two-stage decision model in which the first stage seeks the optimal mix of permanent and on-call staff, and the second stage seeks a joint on-demand staffing and call scheduling policy to minimize the associated cost given the base staffing level and the size of the on-call pool. Because the exact analysis of the two-stage decision model seems analytically intractable, we resort to an approximation in a suitable asymptotic regime. In that regime, we characterize the system dynamics of the service operation and derive an optimal joint on-demand staffing and call scheduling policy for the second-stage problem, which in turn is used to find an approximate solution to the first-stage problem. In particular, the derived policy for the second-stage problem involves tapping into the on-call pool to procure a team of on-demand agents when the number of calls to be processed exceeds a certain threshold and dismissing them when it falls below another threshold; additionally, the call scheduling rule shows an unusual pattern due to the interplay between staffing and scheduling decisions. Extensive numerical studies under realistic parameter settings show that the solution approach we propose can achieve significant cost savings. Funding: W. Liu has been supported by the President’s Graduate Fellowship at the National University of Singapore. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0651 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0651},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {363-384},
  shortjournal = {Oper. Res.},
  title        = {Expanding service capabilities through an on-demand workforce},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scores for multivariate distributions and level sets. <em>OR</em>, <em>73</em>(1), 344-362. (<a href='https://doi.org/10.1287/opre.2020.0365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasts of multivariate probability distributions are required for a variety of applications. Scoring rules enable the evaluation of forecast accuracy and comparison between forecasting methods. We propose a theoretical framework for scoring rules for multivariate distributions that encompasses the existing quadratic score and multivariate continuous ranked probability score. We demonstrate how this framework can be used to generate new scoring rules. In some multivariate contexts, it is a forecast of a level set that is needed, such as a density level set for anomaly detection or the level set of the cumulative distribution as a measure of risk. This motivates consideration of scoring functions for such level sets. For univariate distributions, it is well established that the continuous ranked probability score can be expressed as the integral over a quantile score. We show that, in a similar way, scoring rules for multivariate distributions can be decomposed to obtain scoring functions for level sets. Using this, we present scoring functions for different types of level sets, including density level sets and level sets for cumulative distributions. To compute the scores, we propose a simple numerical algorithm. We perform a simulation study to support our proposals, and we use real data to illustrate usefulness for forecast combining and conditional value at risk estimation. Funding: The work of S. Li was supported by the National Natural Science Foundation of China [Grant 12201399] and the Shanghai Frontier Research Institute for Modern Analysis.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0365},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {344-362},
  shortjournal = {Oper. Res.},
  title        = {Scores for multivariate distributions and level sets},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving efficiency in black-box simulation of distribution tails with self-structuring importance samplers. <em>OR</em>, <em>73</em>(1), 325-343. (<a href='https://doi.org/10.1287/opre.2021.0331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel importance sampling (IS) scheme for estimating distribution tails of performance measures modeled with a rich set of tools, such as linear programs, integer linear programs, piecewise linear/quadratic objectives, feature maps specified with deep neural networks, etc. The conventional approach of explicitly identifying efficient changes of measure suffers from feasibility and scalability concerns beyond highly stylized models because of their need to be tailored intricately to the objective and the underlying probability distribution. This bottleneck is overcome in the proposed scheme with an elementary transformation that is capable of implicitly inducing an effective IS distribution in a variety of models by replicating the concentration properties observed in less rare samples. This novel approach is guided by developing a large deviations principle that brings out the phenomenon of self-similarity of optimal IS distributions. The proposed sampler is the first to attain asymptotically optimal variance reduction across a spectrum of multivariate distributions despite being oblivious to the specifics of the underlying model. Its applicability is illustrated with contextual shortest-path and portfolio credit risk models informed by neural networks. Funding: This work was supported by the Singapore Ministry of Education Academic Research Fund [Grant MOE2019-T2-2-163]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0331 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0331},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {325-343},
  shortjournal = {Oper. Res.},
  title        = {Achieving efficiency in black-box simulation of distribution tails with self-structuring importance samplers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Projective hedging algorithms for multistage stochastic programming, supporting distributed and asynchronous implementation. <em>OR</em>, <em>73</em>(1), 311-324. (<a href='https://doi.org/10.1287/opre.2022.0228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a decomposition algorithm for multistage stochastic programming that resembles the progressive hedging method of Rockafellar and Wets but is provably capable of several forms of asynchronous operation. We derive the method from a class of projective operator splitting methods fairly recently proposed by Combettes and Eckstein, significantly expanding the known applications of those methods. Our derivation assures convergence for convex problems whose feasible set is compact, subject to some standard regularity conditions and a mild “fairness” condition on subproblem selection. The method’s convergence guarantees are deterministic and do not require randomization, in contrast to other proposed asynchronous variations of progressive hedging. Computational experiments described in an online appendix show the method to outperform progressive hedging on large-scale problems in a highly parallel computing environment. Funding: This work was supported by the National Science Foundation Directorate of Computer and Information Science and Engineering [Grant CCF-1617617] and U.S. Department of Energy [Office of Electricity’s Advanced Grid Modeling program]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0228 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0228},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {311-324},
  shortjournal = {Oper. Res.},
  title        = {Projective hedging algorithms for multistage stochastic programming, supporting distributed and asynchronous implementation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavior-aware queueing: The finite-buffer setting with many strategic servers. <em>OR</em>, <em>73</em>(1), 290-310. (<a href='https://doi.org/10.1287/opre.2023.2487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service system design is often informed by queueing theory. Traditional queueing theory assumes that servers work at constant speeds. That is reasonable in computer science and manufacturing contexts. However, servers in service systems are people, and in contrast to machines, the incentives created by design decisions influence their work speeds. We study how server work speed is affected by managerial decisions concerning (i) how many servers to staff and how much to pay them and (ii) whether and when to turn away customers in the context of many-server queues with finite or infinite buffers ( M / M / N / k with k ∈ Z + ∪ { ∞ } ) in which the work speeds emerge as the solution to a noncooperative game. We show that a symmetric equilibrium always exists in a loss system ( N = k ) and provide conditions for equilibrium existence in a single-server system ( N = 1). For the general M / M / N / k system, we provide a sufficient condition for the existence of a solution to the first-order condition and bounds on such a solution; however, showing that it is an equilibrium is challenging because of the existence of multiple local maxima in the utility function. Nevertheless, in an asymptotic regime in which demand becomes large, the utility function becomes concave, allowing us to characterize underloaded, critically loaded, and overloaded equilibria. Funding: This work was supported in part by funding from the Social Sciences and Humanities Research Council of Canada [Grant 430-2020-00334] and the Charles M. Harper Faculty Fellowship at the University of Chicago Booth School of Business. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2487 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2487},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {290-310},
  shortjournal = {Oper. Res.},
  title        = {Behavior-aware queueing: The finite-buffer setting with many strategic servers},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—Online matching with bayesian rewards. <em>OR</em>, <em>73</em>(1), 278-289. (<a href='https://doi.org/10.1287/opre.2021.0499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study in this paper an online matching problem where a central platform needs to match a number of limited resources to different groups of users that arrive sequentially over time. The reward of each matching option depends on both the type of resource and the time period the user arrives. The matching rewards are assumed to be unknown but drawn from probability distributions that are known a priori. The platform then needs to learn the true rewards online based on real-time observations of the matching results. The goal of the central platform is to maximize the total reward from all of the matchings without violating the resource capacity constraints. We formulate this matching problem with Bayesian rewards as a Markovian multiarmed bandit problem with budget constraints, where each arm corresponds to a pair of a resources and a time period. We devise our algorithm by first finding policies for each single arm separately via a relaxed linear program and then “assembling” these policies together through judicious selection criteria and well-designed pulling orders. We prove that the expected reward of our algorithm is at least 1 2 ( 2 − 1 ) of the expected reward of an optimal algorithm. Funding: The authors thank the Massachusetts Institute of Technology (MIT)-IBM partnership in Artificial Intelligence and the MIT Data Science Laboratory for support. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0499 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0499},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {278-289},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Online matching with bayesian rewards},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—A stationary infinite-horizon supply contract under asymmetric inventory information. <em>OR</em>, <em>73</em>(1), 270-277. (<a href='https://doi.org/10.1287/opre.2020.0495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a decentralized supply chain in which a supplier sells goods to a retailer facing general random demand over an infinite horizon. The retailer satisfies the demand to the extent of the inventory on hand. The retailer has private information about the retailer’s stock in each period, and the supplier offers the retailer a supply contract menu to account for the information asymmetry. We obtain a necessary condition for optimizing a long-term stationary truth-telling contract under general demand and belief distributions. We apply it to a batch-order contract, which replenishes a prespecified inventory quantity for a fixed payment in each period only when the retailer’s beginning inventory becomes zero. Methodologically, we formulate the supplier’s contract design as a calculus of variations problem and apply the concept of Gâteaux derivative to obtain these results. This methodology can potentially be applied to other dynamic contracting problems. Funding: A. Bensoussan acknowledges support from the National Science Foundation [Grant NSF-DMS 220 47 95]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2020.0495 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0495},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {270-277},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A stationary infinite-horizon supply contract under asymmetric inventory information},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—Conic mixed-binary sets: Convex hull characterizations and applications. <em>OR</em>, <em>73</em>(1), 251-269. (<a href='https://doi.org/10.1287/opre.2020.0827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a general conic mixed-binary set where each homogeneous conic constraint j involves an affine function of independent continuous variables and an epigraph variable associated with a nonnegative function, f j , of common binary variables. Sets of this form naturally arise as substructures in a number of applications, including mean-risk optimization, chance-constrained problems, portfolio optimization, lot sizing and scheduling, fractional programming, variants of the best subset selection problem, a class of sparse semidefinite programs, and distributionally robust chance-constrained programs. We give a convex hull description of this set that relies on simultaneous characterization of the epigraphs of f j ’s, which is easy to do when all functions f j ’s are submodular. Our result unifies and generalizes an existing result in two important directions. First, it considers multiple general convex cone constraints instead of a single second-order cone type constraint. Second, it takes arbitrary nonnegative functions instead of a specific submodular function obtained from the square root of an affine function. We close by demonstrating the applicability of our results in the context of a number of problem classes. Funding: The research is supported, in part, by ONR [Grants N00014-19-1-2321 and N00014-22-1-2602], AFOSR [Grant FA9550-22-1-0365], the Institute for Basic Science [IBS-R029-C1, Y2], the FOUR Brain Korea 21 Program [NRF-5199990113928], the National Research Foundation of Korea [NRF-2022M3J6A1063021], the KAIST Starting Fund [KAIST-G04220016], NSF [Grant CMMI 1454548], and Early Postdoc Mobility Fellowship SNSF [Grant P2ELP2_195149].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0827},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {251-269},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Conic mixed-binary sets: Convex hull characterizations and applications},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technical Note—A new rate-optimal sampling allocation for linear belief models. <em>OR</em>, <em>73</em>(1), 239-250. (<a href='https://doi.org/10.1287/opre.2022.2337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a new optimal sampling budget allocation for belief models based on linear regression with continuous covariates, where the expected response is interpreted as the value of the covariate vector, and an “error” occurs if a lower-valued vector is falsely identified as being better than a higher-valued one. Our allocation optimizes the rate at which the probability of error converges to zero using a large deviations theoretic characterization. This is the first large deviations-based optimal allocation for continuous decision spaces, and it turns out to be considerably simpler and easier to implement than allocations that use discretization. We give a practicable sequential implementation and illustrate its empirical potential. Funding: This work was supported by the National Science Foundation [Grant CMMI-2112828].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2337},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {239-250},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A new rate-optimal sampling allocation for linear belief models},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal no-regret learning in repeated first-price auctions. <em>OR</em>, <em>73</em>(1), 209-238. (<a href='https://doi.org/10.1287/opre.2020.0282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study online learning in repeated first-price auctions where a bidder, only observing the winning bid at the end of each auction, learns to adaptively bid to maximize the cumulative payoff. To achieve this goal, the bidder faces censored feedback: If the bidder wins the bid, then the bidder is not able to observe the highest bid of the other bidders, which we assume is i.i.d. drawn from an unknown distribution. In this paper, we develop the first learning algorithm that achieves a near-optimal O ˜ ( T ) regret bound, by exploiting two structural properties of first-price auctions, that is, the specific feedback structure and payoff function. We first formulate the feedback structure in first-price auctions as partially ordered contextual bandits, a combination of the graph feedback across actions (bids), the cross-learning across contexts (private values), and a partial order over the contexts. We establish both strengths and weaknesses of this framework by showing a curious separation that a regret nearly independent of the action/context sizes is possible under stochastic contexts but is impossible under adversarial contexts. In particular, this framework leads to an O ( T log 2.5 T ) regret for first-price auctions when the bidder’s private values are independent and identically distributed. Despite the limitation of this framework, we further exploit the special payoff function of first-price auctions to develop a sample-efficient algorithm even in the presence of adversarially generated private values. We establish an O ( T log 3 T ) regret bound for this algorithm, hence providing a complete characterization of optimal learning guarantees for first-price auctions. Funding: This project was supported in part by the National Science Foundation [Awards CCF-2106467 and CCF-2106508]. Y. Han and T. Weissman were partially supported by the Yahoo Faculty Research and Engagement Program.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0282},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {209-238},
  shortjournal = {Oper. Res.},
  title        = {Optimal no-regret learning in repeated first-price auctions},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to persuade on the fly: Robustness against ignorance. <em>OR</em>, <em>73</em>(1), 194-208. (<a href='https://doi.org/10.1287/opre.2021.0529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by information sharing in online platforms, we study repeated persuasion between a sender and a stream of receivers, where, at each time, the sender observes a payoff-relevant state drawn independently and identically from an unknown distribution and shares state information with the receivers, who each choose an action. The sender seeks to persuade the receivers into taking actions aligned with the sender’s preference by selectively sharing state information. However, in contrast to the standard models, neither the sender nor the receivers know the distribution, and the sender has to persuade while learning the distribution on the fly. We study the sender’s learning problem of making persuasive action recommendations to achieve low regret against the optimal persuasion mechanism with the knowledge of the distribution. To do this, we first propose and motivate a persuasiveness criterion for the unknown distribution setting that centers robustness as a requirement in the face of uncertainty. Our main result is an algorithm that, with high probability, is robustly persuasive and achieves O ( T log T ) regret, where T is the horizon length. Intuitively, at each time, our algorithm maintains a set of candidate distribution and chooses a signaling mechanism that is simultaneously persuasive for all of them. Core to our proof is a tight analysis about the cost of robust persuasion, which may be of independent interest. We further prove that this regret order is optimal (up to logarithmic terms) by showing that no algorithm can achieve regret better than Ω ( T ) . Funding: Y. Zu and K. Iyer gratefully acknowledge partial support from the National Science Foundation (NSF) Division of Civil, Mechanical, and Manufacturing Innovation [Grant CMMI-2002156]. H. Xu is supported by the NSF Division of Computing and Communication Foundations [Award CCF-2303372], the Army Research Office [Award W911NF-23-1-0030], and a Google Faculty Research Award. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0529 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0529},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {194-208},
  shortjournal = {Oper. Res.},
  title        = {Learning to persuade on the fly: Robustness against ignorance},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pricing optimal outcomes in coupled and non-convex markets: Theory and applications to electricity markets. <em>OR</em>, <em>73</em>(1), 178-193. (<a href='https://doi.org/10.1287/opre.2023.0401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world markets, participants have non-convex preferences, and the allocation problem needs to consider complex constraints. Electricity markets are a prime example, but similar problems appear in many markets, which has led to a growing literature on market design. Competitive equilibrium does not generally exist in such markets. Today, power markets use heuristic pricing rules based on the dual of a relaxed allocation problem. With increasing levels of renewables, these rules have come under scrutiny as they lead to high out-of-market side payments to some participants and inadequate congestion signals. We show that existing pricing heuristics optimize specific design goals that can be conflicting. The tradeoffs can be substantial, and we establish that the design of pricing rules is fundamentally a multiobjective optimization problem addressing different incentives. In addition to traditional multiobjective optimization techniques that involve weighting individual objectives, we introduce a novel parameter-free pricing rule that minimizes incentives for market participants to deviate locally. Our theoretical and experimental findings show how the new pricing rule capitalizes on the upsides of existing pricing rules under scrutiny today. It leads to prices that incur low make-whole payments while providing adequate congestion signals and low lost opportunity costs. Our suggested pricing rule does not require weighing objectives, it is computationally scalable, and balances tradeoffs in a principled manner, addressing a critical policy issue in electricity markets. Funding: The financial support from the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) [Grant BI 1057/9-1] is gratefully acknowledged. Supplemental Material: The computer code and data that supports the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0401 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0401},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {178-193},
  shortjournal = {Oper. Res.},
  title        = {Pricing optimal outcomes in coupled and non-convex markets: Theory and applications to electricity markets},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Price interpretability of prediction markets: A convergence analysis. <em>OR</em>, <em>73</em>(1), 157-177. (<a href='https://doi.org/10.1287/opre.2022.0417'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction markets are long known for prediction accuracy. This study systematically explores the fundamental properties of prediction markets, addressing questions about their information aggregation process and the factors contributing to their remarkable efficacy. We propose a novel multivariate utility–based mechanism that unifies several existing automated market-making schemes. Using this mechanism, we establish the convergence results for markets comprised of risk-averse traders who have heterogeneous beliefs and repeatedly interact with the market maker. We demonstrate that the resulting limiting wealth distribution aligns with the Pareto efficient frontier defined by the utilities of all market participants. With the help of this result, we establish analytical and numerical results for the limiting price in different market models. Specifically, we show that the limiting price converges to the geometric mean of agent beliefs in exponential utility-based markets. In risk measure-based markets, we construct a family of risk measures that satisfy the convergence criteria and prove that the price converges to a unique level represented by the weighted power mean of agent beliefs. In broader markets with constant relative risk aversion utilities, we reveal that the limiting price can be characterized by systems of equations that encapsulate agent beliefs, risk parameters, and wealth. Despite the impact of traders’ trading sequences on the limiting price, we establish a price invariance result for markets with a large trader population. Using this result, we propose an efficient approximation scheme for the limiting price. Numerical experiments demonstrate that the accuracy of this approximation scheme outperforms existing approximation methods across various scenarios. Our findings serve to aid market designers in better tailoring and adjusting the market-making mechanism for more effective opinion elicitation. Funding: This work was supported by the National Natural Science Foundation of China [Grants 71671045, 71971132, 72150002, 72201067, and 72394361], the InnoHK initiative of the Government of the HKSAR, Laboratory for AI-Powered Financial Technologies, the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence [Grant 2023B1212010001], the Shanghai Research Center for Data Science and Decision Technology, and the Key Laboratory of Interdisciplinary Research of Computation and Economics, Ministry of Education, Shanghai University of Finance and Economics. Supplemental Material: The computer code and data that supports the findings of this study is available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0417 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0417},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {157-177},
  shortjournal = {Oper. Res.},
  title        = {Price interpretability of prediction markets: A convergence analysis},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of lookahead and approximate policy evaluation in reinforcement learning with linear value function approximation. <em>OR</em>, <em>73</em>(1), 139-156. (<a href='https://doi.org/10.1287/opre.2022.0357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function approximation is widely used in reinforcement learning to handle the computational difficulties associated with very large state spaces. However, function approximation introduces errors that may lead to instabilities when using approximate dynamic programming techniques to obtain the optimal policy. Therefore, techniques such as lookahead for policy improvement and m -step rollout for policy evaluation are used in practice to improve the performance of approximate dynamic programming with function approximation. We quantitatively characterize the impact of lookahead and m -step rollout on the performance of approximate dynamic programming (DP) with function approximation. (i) Without a sufficient combination of lookahead and m -step rollout, approximate DP may not converge. (ii) Both lookahead and m -step rollout improve the convergence rate of approximate DP. (iii) Lookahead helps mitigate the effect of function approximation and the discount factor on the asymptotic performance of the algorithm. Our results are presented for two approximate DP methods: one that uses least-squares regression to perform function approximation and another that performs several steps of gradient descent of the least-squares objective in each iteration. Funding: The research presented here was supported in part by a grant from Sandia National Labs and the NSF [Grants CCF 1934986, CCF 2207547, CNS 2106801], ONR [Grant N00014-19-1-2566], and ARO [Grant W911NF-19-1-0379].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0357},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {139-156},
  shortjournal = {Oper. Res.},
  title        = {The role of lookahead and approximate policy evaluation in reinforcement learning with linear value function approximation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online learning for constrained assortment optimization under markov chain choice model. <em>OR</em>, <em>73</em>(1), 109-138. (<a href='https://doi.org/10.1287/opre.2022.0693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a dynamic assortment selection problem where arriving customers make purchase decisions among offered products from a universe of products under a Markov chain choice (MCC) model. The retailer only observes the assortment and the customer’s single choice per period. Given limited display capacity, resource constraints, and no a priori knowledge of problem parameters, the retailer’s objective is to sequentially learn the choice model and optimize cumulative revenues over a finite selling horizon. We develop a fast linear system based explore-then-commit (FastLinETC for short) learning algorithm that balances the tradeoff between exploration and exploitation. The algorithm can simultaneously estimate the arrival and transition probabilities in the MCC model by solving a linear system of equations and determining the near-optimal assortment based on these estimates. Furthermore, our consistent estimators offer superior computational times compared with existing heuristic estimation methods, which often suffer from inconsistency or a significant computational burden. Funding: The research of Q. Luo is partially supported by the National Science Foundation [Grant CMMI-2308750]. The research of Z. Huang is partially supported by the Shanghai Sailing Program [Grant 22YF1451100 and the Fundamental Research Funds for the Central Universities]. The research of C. Shi is partially supported by Amazon [Research Award].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0693},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {109-138},
  shortjournal = {Oper. Res.},
  title        = {Online learning for constrained assortment optimization under markov chain choice model},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drone-delivery network for opioid overdose: Nonlinear integer queueing-optimization models and methods. <em>OR</em>, <em>73</em>(1), 86-108. (<a href='https://doi.org/10.1287/opre.2022.0489'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new stochastic emergency network design model that uses a fleet of drones to quickly deliver naloxone in response to opioid overdoses. The network is represented as a collection of M / G / K queueing systems in which the capacity K of each system is a decision variable, and the service time is modeled as a decision-dependent random variable. The model is a queuing-based optimization problem which locates fixed (drone bases) and mobile (drones) servers and determines the drone dispatching decisions and takes the form of a nonlinear integer problem intractable in its original form. We develop an efficient reformulation and algorithmic framework. Our approach reformulates the multiple nonlinearities (fractional, polynomial, exponential, factorial terms) to give a mixed-integer linear programming (MILP) formulation. We demonstrate its generalizability and show that the problem of minimizing the average response time of a collection of M / G / K queueing systems with unknown capacity K is always MILP-representable. We design an outer approximation branch-and-cut algorithmic framework that is computationally efficient and scales well. The analysis based on real-life data reveals that drones can in Virginia Beach: (1) decrease the response time by 82%, (2) increase the survival chance by more than 273%, (3) save up to 33 additional lives per year, and (4) provide annually up to 279 additional quality-adjusted life years. Funding: M. A. Lejeune acknowledges the support of the National Science Foundation [Grant ECCS-2114100] and the Office of Naval Research [Grant N00014-22-1-2649]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0489 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0489},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {86-108},
  shortjournal = {Oper. Res.},
  title        = {Drone-delivery network for opioid overdose: Nonlinear integer queueing-optimization models and methods},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the security of united states elections with robust optimization. <em>OR</em>, <em>73</em>(1), 61-85. (<a href='https://doi.org/10.1287/opre.2023.0422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For more than a century, election officials across the United States have inspected voting machines before elections using a procedure called logic and accuracy testing (LAT). This procedure consists of election officials casting a test deck of ballots into each voting machine and confirming the machine produces the expected vote total for each candidate. We bring a scientific perspective to LAT by introducing the first formal approach to designing test decks with rigorous security guarantees. Specifically, our approach employs robust optimization to find test decks that are guaranteed to detect any voting machine misconfiguration that would cause votes to be swapped across candidates. Of all the test decks with this security guarantee, our robust optimization problem yields the test deck with the minimum number of ballots, thereby minimizing implementation costs for election officials. To facilitate deployment at scale, we develop a practically efficient exact algorithm for solving our robust optimization problems based on the cutting plane method. In partnership with the Michigan Bureau of Elections, we retrospectively applied our approach to all 6,928 ballot styles from Michigan’s November 2022 general election; this retrospective study reveals that the test decks with rigorous security guarantees obtained by our approach require, on average, only 1.2% more ballots than current practice. Our approach has since been piloted in real-world elections by the Michigan Bureau of Elections as a low-cost way to improve election security and increase public trust in democratic institutions. Funding: This research was supported by the U.S. National Science Foundation [Grant CNS-1518888]. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study are available at https://doi.org/10.1287/opre.2023.0422 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0422},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {61-85},
  shortjournal = {Oper. Res.},
  title        = {Improving the security of united states elections with robust optimization},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preventing price-mediated contagion due to fire sales externalities: Strategic foundations of macroprudential regulation. <em>OR</em>, <em>73</em>(1), 40-60. (<a href='https://doi.org/10.1287/opre.2023.0237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We offer a stress test framework in which interaction between regulated banks occurs through the impact they may have on asset prices when they deleverage. Because banks are constrained to maintain their risk-based capital ratio higher than a threshold, the deleveraging problem yields a generalized game in which the solvency constraint of each bank depends on the decisions of the others. We analyze the game under microprudential but also under macroprudential regulation. Microprudential regulation corresponds to the standard situation in which each bank considers its own solvency constraint, whereas macroprudential regulation is defined as the situation in which each bank faces a systemic constraint in that it must consider the solvency constraints of all the banks. When bankruptcies can be avoided, we show that a Nash equilibrium generically exists under macroprudential regulation, contagion of failures due to fire sales externalities is prevented, whereas it may not exist under microprudential regulation. We eventually analyze the deleveraging problem when bankruptcies cannot be avoided and present additional results. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0237 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0237},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {40-60},
  shortjournal = {Oper. Res.},
  title        = {Preventing price-mediated contagion due to fire sales externalities: Strategic foundations of macroprudential regulation},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application-driven learning: A closed-loop prediction and optimization approach applied to dynamic reserves and demand forecasting. <em>OR</em>, <em>73</em>(1), 22-39. (<a href='https://doi.org/10.1287/opre.2023.0565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision making is generally modeled as sequential forecast-decision steps with no feedback, following an open-loop approach. For instance, in the electricity sector, system operators use the forecast-decision approach followed by ad hoc rules to determine reserve requirements and biased net load forecasts to guard the system against renewable generation and demand uncertainty. Such procedures lack technical formalism to minimize operating and reliability costs. We present a new closed-loop framework, named application-driven learning, in which the best forecasting model is defined according to a given application cost function. We consider applications in which the decision-making process is driven by two-stage optimization schemes fed by multivariate point forecasts. We present our estimation method as a bilevel optimization problem and prove convergence to the best estimator regarding the expected application cost. We propose two solution methods: an exact method based on the KKT conditions of the second-level problems and a scalable heuristic suitable for decomposition. Thus, we offer an alternative scientifically grounded approach to current ad hoc procedures implemented in industry practices. We test the proposed methodology with real data and large-scale systems with thousands of buses. Results show that the proposed methodology is scalable and consistently performs better than the standard open-loop approach. Funding: J. Dias Garcia was partially supported by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) [Finance Code 001]. A. Street was partially supported by Fundação de Amparo à Pesquisa do Estado do Rio de Janeiro (FAPERJ) and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq). T. Homem-de-Mello acknowledges the support of Grant FONDECYT 1221770 from ANID, Chile. Supplemental Material: All supplemental materials, including the computer code and data that support the findings of this study, are available at https://doi.org/10.1287/opre.2023.0565 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0565},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {22-39},
  shortjournal = {Oper. Res.},
  title        = {Application-driven learning: A closed-loop prediction and optimization approach applied to dynamic reserves and demand forecasting},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error propagation in asymptotic analysis of the data-driven (s, s) inventory policy. <em>OR</em>, <em>73</em>(1), 1-21. (<a href='https://doi.org/10.1287/opre.2020.0568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study periodic review stochastic inventory control in the data-driven setting where the retailer makes ordering decisions based only on historical demand observations without any knowledge of the probability distribution of the demand. Because an ( s , S )-policy is optimal when the demand distribution is known, we investigate the statistical properties of the data-driven ( s , S )-policy obtained by recursively computing the empirical cost-to-go functions. This policy is inherently challenging to analyze because the recursion induces propagation of the estimation error backward in time. In this work, we establish the asymptotic properties of this data-driven policy by fully accounting for the error propagation. In this setting, the empirical cost-to-go functions for the estimated parameters are not i.i.d. sums because of the error propagation. Our main methodological innovation comes from an asymptotic representation for multi-sample U -processes in terms of i.i.d. sums. This representation enables us to apply empirical process theory to derive the influence functions of the estimated parameters and to establish joint asymptotic normality. Based on these results, we also propose an entirely data-driven estimator of the optimal expected cost, and we derive its asymptotic distribution. We demonstrate some useful applications of our asymptotic results, including sample size determination and interval estimation. Funding: This work was supported by Singapore MOE AcRF Tier 2 [A-8001052-00-00] and the National Natural Science Foundation of China [72071138]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2020.0568 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0568},
  journal      = {Operations Research},
  month        = {1-2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Oper. Res.},
  title        = {Error propagation in asymptotic analysis of the data-driven (s, s) inventory policy},
  volume       = {73},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
