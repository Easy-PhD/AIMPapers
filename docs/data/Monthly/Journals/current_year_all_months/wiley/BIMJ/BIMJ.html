<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIMJ</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="bimj">BIMJ - 54</h2>
<ul>
<li><details>
<summary>
(2025). Inference under covariate-adaptive randomization using random center-effect. <em>BIMJ</em>, <em>67</em>(5), e70076. (<a href='https://doi.org/10.1002/bimj.70076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimization method is a popular choice for covariate-adaptive randomization in multicenter trials. Existing literature suggests that the type-I error is controlled if minimization variables are included in the statistical analysis. However, in practice, minimization variables with many categories, such as the recruitment center, are often not included in the model. In this paper, we propose including the minimization variable “center” as a random effect and assess its performance using simulations for Gaussian, binary, and Poisson endpoint variables. Our simulation study suggests that the random-effect model controls type-I error and preserves maximum power for all three endpoints under varied clinical trial settings. This approach offers an alternative to the re-randomization test, which regulatory authorities often suggest for sensitivity analysis.},
  archive      = {J_BIMJ},
  author       = {Anjali Pandey and Harsha Shree BS and Andrea Callegaro},
  doi          = {10.1002/bimj.70076},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {5},
  pages        = {e70076},
  shortjournal = {Bio. J.},
  title        = {Inference under covariate-adaptive randomization using random center-effect},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible parametric accelerated failure time models with cure. <em>BIMJ</em>, <em>67</em>(5), e70074. (<a href='https://doi.org/10.1002/bimj.70074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerated failure time (AFT) models offer an attractive alternative to Cox proportional hazards models. AFT models are collapsible and, unlike hazard ratios in proportional hazards models, the acceleration factor—a key effect measure in AFT models—is collapsible, meaning its value remains unchanged when adjusting for additional covariates. In addition, AFT models provide an intuitive interpretation directly on the survival time scale. From the recent development of smooth parametric AFT models, we identify potential issues with their applications and note several desired extensions that have not yet been implemented. To enrich this tool and its application in clinical research, we improve the AFT models within a flexible parametric framework in several ways: we adopt monotone natural splines to constrain the log cumulative hazard to be a monotonic function across its support; allow for time-varying acceleration factors, possibly include cure and accommodating more than one time-varying effect; and implement both mixture and nonmixture cure models. We implement all of these extensions in the rstpm2 package, which is publicly available on CRAN. Simulations highlight a varying success in estimating cure fractions. However, in terms of covariate-effect estimation, flexible AFT models appear to be more robust than the Cox model even when there is a high proportion of cured individuals in the data, regardless of whether cure is reached within the observed data. We also apply some of our extensions of AFT models to real-world survival data.},
  archive      = {J_BIMJ},
  author       = {Birzhan Akynkozhayev and Benjamin Christoffersen and Xingrong Liu and Keith Humphreys and Mark Clements},
  doi          = {10.1002/bimj.70074},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {5},
  pages        = {e70074},
  shortjournal = {Bio. J.},
  title        = {Flexible parametric accelerated failure time models with cure},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimands for early-phase dose optimization trials in oncology. <em>BIMJ</em>, <em>67</em>(5), e70072. (<a href='https://doi.org/10.1002/bimj.70072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase I dose escalation trials in oncology generally aim to find the maximum tolerated dose. However, with the advent of molecular-targeted therapies and antibody drug conjugates, dose-limiting toxicities are less frequently observed, giving rise to the concept of optimal biological dose (OBD), which considers both efficacy and toxicity. The estimand framework presented in the addendum of the ICH E9(R1) guidelines strengthens the dialogue between different stakeholders by bringing in greater clarity in the clinical trial objectives and by providing alignment between the targeted estimand under consideration and the statistical analysis methods. However, there is a lack of clarity in implementing this framework in early-phase dose optimization studies. This paper aims to discuss the estimand framework for dose optimization trials in oncology, considering efficacy and toxicity through utility functions. Such trials should include pharmacokinetics data, toxicity data, and efficacy data. Based on these data, the analysis methods used to identify the optimized dose/s are also described. Focusing on optimizing the utility function to estimate the OBD, the population-level summary measure should reflect only the properties used for estimating this utility function. A detailed strategy recommendation for intercurrent events has been provided using a real-life oncology case study. Key recommendations regarding the estimand attributes include that in a seamless phase I/II dose optimization trial, the treatment attribute should start when the subject receives the first dose. We argue that such a framework brings in additional clarity to dose optimization trial objectives and strengthens the understanding of the drug under consideration, which would enable the correct dose to move to phase II of clinical development.},
  archive      = {J_BIMJ},
  author       = {Ayon Mukherjee and Jonathan L. Moscovici and Zheng Liu},
  doi          = {10.1002/bimj.70072},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {5},
  pages        = {e70072},
  shortjournal = {Bio. J.},
  title        = {Estimands for early-phase dose optimization trials in oncology},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hazards, causality, and practical relevance of collider effects – Comment on beyersmann et al. “Hazards constitute key quantities for analyzing, interpreting and understanding time-to-event data”. <em>BIMJ</em>, <em>67</em>(5), e70071. (<a href='https://doi.org/10.1002/bimj.70071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazards constitute key quantities for analyzing, interpreting, and understanding time-to-event data. Hazards and corresponding effect measures, such as the hazard ratio from the Cox proportional hazards model, have a valid causal interpretation if the hazard function is considered as a function in time rather than hazards at specific time points. In this comment, we would like to add two points: (1) The hazard ratio is also a useful population-level estimand with a valid causal interpretation. (2) Empirical evidence shows that problematic situations, which could occur in theory due to strong heterogeneity, are usually avoided in typical randomized controlled trials.},
  archive      = {J_BIMJ},
  author       = {Ralf Bender and Lars Beckmann},
  doi          = {10.1002/bimj.70071},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {5},
  pages        = {e70071},
  shortjournal = {Bio. J.},
  title        = {Hazards, causality, and practical relevance of collider effects – Comment on beyersmann et al. “Hazards constitute key quantities for analyzing, interpreting and understanding time-to-event data”},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified estimation method for partially linear models with nonmonotone missing at random data. <em>BIMJ</em>, <em>67</em>(5), e70070. (<a href='https://doi.org/10.1002/bimj.70070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially linear models are commonly used in observational studies of the causal effect of treatment and/or exposure when there are observed confounding variables. The models are robust and asymptotically distribution-free for testing the causal null hypothesis. In this research, we investigate methods for estimating the partially linear models with data missing at random in all the variables, including the response, the treatment, and the confounding variables. We develop a general estimation method for inference in partially linear models with nonmonotone missing at random data. It proposes using partially linear working models to improve the estimation efficiency of the standard complete case method. It can be shown that the new estimator is consistent, which does not depend on the correctness of the working models. In addition, we recommend bootstrap estimates for the asymptotic variances and semiparametric models for the missing data probabilities. It is computationally simple and can be directly implemented in standard software. Simulation studies are provided to examine its performance. A real data example with sparsely observed missingness patterns is used to illustrate the method.},
  archive      = {J_BIMJ},
  author       = {Yang Zhao},
  doi          = {10.1002/bimj.70070},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {5},
  pages        = {e70070},
  shortjournal = {Bio. J.},
  title        = {Unified estimation method for partially linear models with nonmonotone missing at random data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian basket trial design using local power prior. <em>BIMJ</em>, <em>67</em>(4), e70069. (<a href='https://doi.org/10.1002/bimj.70069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, basket trials, which allow the evaluation of an experimental therapy across multiple tumor types within a single protocol, have gained prominence in early-phase oncology development. Unlike traditional trials, which evaluate each tumor type separately and often face challenges with limited sample sizes, basket trials offer the advantage of borrowing information across various tumor types to enhance statistical power. However, a key challenge in designing basket trials is determining the appropriate extent of information borrowing while maintaining an acceptable type I error rate control. In this paper, we propose a novel three-component local power prior (local-PP) framework that introduces a dynamic and flexible approach to information borrowing. The framework consists of three components: global borrowing control, pairwise similarity assessments, and a borrowing threshold, allowing for tailored and interpretable borrowing across heterogeneous tumor types. Unlike many existing Bayesian methods that rely on computationally intensive Markov chain Monte Carlo (MCMC) sampling, the proposed approach provides a closed-form solution, significantly reducing computation time in large-scale simulations for evaluating operating characteristics. Extensive simulations demonstrate that the proposed local-PP framework performs comparably to more complex methods while significantly shortening computation time.},
  archive      = {J_BIMJ},
  author       = {Haiming Zhou and Rex Shen and Sutan Wu and Philip He},
  doi          = {10.1002/bimj.70069},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {4},
  pages        = {e70069},
  shortjournal = {Bio. J.},
  title        = {A bayesian basket trial design using local power prior},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using machine learning to improve control for confounding in the dynamic weighted ordinary least squares estimator of optimal adaptive treatment strategies. <em>BIMJ</em>, <em>67</em>(4), e70068. (<a href='https://doi.org/10.1002/bimj.70068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating optimal adaptive treatment strategies (ATSs) can be done in several ways, including dynamic weighted ordinary least squares (dWOLS). This approach is doubly robust as it requires modeling both the treatment and the response, but only one of those models needs to be correctly specified to obtain a consistent estimator. For estimating an average treatment effect, doubly robust methods have been shown to combine better with machine learning methods than alternatives. However, the use of machine learning within dWOLS has not yet been investigated. Using simulation studies, we evaluate and compare the performance of the dWOLS estimator when the treatment probability is estimated either using machine learning algorithms or a logistic regression model. We further investigate the use of an adaptive -out-of- bootstrap method for producing inferences. SuperLearner performed at least as well as logistic regression in terms of bias and variance in scenarios with simple data-generating models and often had improved performance in more complex scenarios. Moreover, the -out-of- bootstrap produced confidence intervals with nominal coverage probabilities for parameters that were estimated with low bias. We also apply our proposed approach to the data from a breast cancer registry in Québec, Canada, to estimate an optimal ATS to personalize the use of hormonal therapy in breast cancer patients. Our method is implemented in the R software and available on GitHub https://github.com/kosstre20/MachineLearningToControlConfoundingPersonalizedMedicine.git . We recommend routine use of machine learning to model treatment within dWOLS, at least as a sensitivity analysis for the point estimates.},
  archive      = {J_BIMJ},
  author       = {Kossi Clément Trenou and Miceline Mésidor and Aida Eslami and Hermann Nabi and Caroline Diorio and Denis Talbot},
  doi          = {10.1002/bimj.70068},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {4},
  pages        = {e70068},
  shortjournal = {Bio. J.},
  title        = {Using machine learning to improve control for confounding in the dynamic weighted ordinary least squares estimator of optimal adaptive treatment strategies},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking probability of success as bayes utility. <em>BIMJ</em>, <em>67</em>(4), e70067. (<a href='https://doi.org/10.1002/bimj.70067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the hybrid frequentist-Bayesian approach, the probability of success (PoS) of a trial is the expected value of the traditional power function of a test with respect to a design prior assigned to the parameter under scrutiny. However, this definition is not univocal and some of the proposals do not lack of potential drawbacks. These problems are related to the fact that such definitions are all based on the probability of rejecting the null hypothesis rather than on the probability of choosing the correct hypothesis, be it the null or the alternative. In this article, we propose a unifying, decision-theoretic approach that yields a new definition of PoS as the expected utility of the trial (u-PoS), that is, as the expected probability of making the correct choice between the two hypotheses. This proposal shows a conceptual advantage over previous definitions of PoS; moreover, it produces smaller optimal sample sizes whenever the design prior assigns positive probability to the null hypothesis.},
  archive      = {J_BIMJ},
  author       = {Fulvio De Santis and Stefania Gubbiotti and Francesco Mariani},
  doi          = {10.1002/bimj.70067},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {4},
  pages        = {e70067},
  shortjournal = {Bio. J.},
  title        = {Rethinking probability of success as bayes utility},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early and late buzzards: Comparing different approaches for quantile-based multiple testing in heavy-tailed wildlife research data. <em>BIMJ</em>, <em>67</em>(4), e70065. (<a href='https://doi.org/10.1002/bimj.70065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical, ecological, and psychological research, there is a need for methods to handle multiple testing, for example, to consider group comparisons with more than two groups. Typical approaches that deal with multiple testing are mean- or variance-based which can be less effective in the context of heavy-tailed and skewed data. Here, the median is the preferred measure of location and the interquartile range (IQR) is an adequate alternative to the variance. Therefore, it may be fruitful to formulate research questions of interest in terms of the median or the IQR. For this reason, we compare different inference approaches for two-sided and noninferiority hypotheses formulated in terms of medians or IQRs in an extensive simulation study. We consider multiple contrast testing procedures combined with a bootstrap method as well as testing procedures with Bonferroni correction. As an example of a multiple testing problem based on heavy-tailed data, we analyze an ecological trait variation in early and late breeding in a medium-sized bird of prey.},
  archive      = {J_BIMJ},
  author       = {Marléne Baumeister and Merle Munko and Kai-Philipp Gladow and Marc Ditzhaus and Nayden Chakarov and Markus Pauly},
  doi          = {10.1002/bimj.70065},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {4},
  pages        = {e70065},
  shortjournal = {Bio. J.},
  title        = {Early and late buzzards: Comparing different approaches for quantile-based multiple testing in heavy-tailed wildlife research data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Validation of a longitudinal marker as a surrogate using mediation analysis and joint modeling: Evolution of the PSA as a surrogate of the disease-free survival. <em>BIMJ</em>, <em>67</em>(4), e70064. (<a href='https://doi.org/10.1002/bimj.70064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal biomarkers constitute a broad class of potential surrogate endpoints in clinical trials. Several approaches have been proposed for surrogate validation but available methods for validating a longitudinal biomarker as a surrogate of a time-to-event endpoint such as death remain limited. In this work, we propose a method for validating a longitudinal outcome as a surrogate of a time-to-event endpoint using a combination of joint modeling and mediation analysis. The proportion of the total treatment effect on the time-to-event endpoint due to its effect on the biomarker is used as a surrogacy measure. This method is developed to integrate meta-analytic data using a joint model with random effects at both the individual and trial levels. From this model, the indirect treatment effect through the surrogate as well as the direct and total treatment effects is derived using a mediation formula. A simulation study was designed to evaluate the performance of this approach. We applied this method to a multicentric study on prostate cancer to investigate the use of prostate-specific antigen level as a surrogate for disease-free survival.},
  archive      = {J_BIMJ},
  author       = {Quentin Le Coent and Catherine Legrand and James J. Dignam and Virginie Rondeau},
  doi          = {10.1002/bimj.70064},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {4},
  pages        = {e70064},
  shortjournal = {Bio. J.},
  title        = {Validation of a longitudinal marker as a surrogate using mediation analysis and joint modeling: Evolution of the PSA as a surrogate of the disease-free survival},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized boosted models to measure racial effects at different quantiles in observational studies. <em>BIMJ</em>, <em>67</em>(3), e70063. (<a href='https://doi.org/10.1002/bimj.70063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the estimation problem of treatment effect at different quantiles in observational studies with longitudinal data. The research motivation is from the NHLBI (National Heart, Lung, and Blood Institute) Growth and Health Study (NGHS), a longitudinal cohort study that aims to discuss the effects of race on cardiovascular risk factors. Because the true propensity score model is unknown, a nonparametric generalized boosted models (GBM) method is adopted to obtain the propensity score estimator. Combining the ideas of quantile regression and inverse probability weighting, a GBM-based quantile weighting estimation method is developed for the quantile treatment effect and applied in NGHS data to measure the racial effects at different quantiles. The results indicate that the racial effect varies with different quantile levels and may not equal to zero. Under various parameter configurations, some simulation studies are conducted to assess the effectiveness and advantages of our proposed estimation method compared with the existing approaches.},
  archive      = {J_BIMJ},
  author       = {Lili Yue and Jiayue Zhang and Ping Yu and Gaorong Li},
  doi          = {10.1002/bimj.70063},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70063},
  shortjournal = {Bio. J.},
  title        = {Generalized boosted models to measure racial effects at different quantiles in observational studies},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bivariate finite mixture random effects model for identifying and accommodating outliers in diagnostic test accuracy meta-analyses. <em>BIMJ</em>, <em>67</em>(3), e70062. (<a href='https://doi.org/10.1002/bimj.70062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlying studies are prevalent in meta-analyses of diagnostic test accuracy studies and may lead to misleading inferences and decision-making unless their negative effect is appropriately dealt with. Statistical methods for detecting and down-weighting the impact of such studies have recently gained the attention of many researchers. However, these methods dichotomize each study in the meta-analysis as outlying or non-outlying and focus on examining the effect of outlying studies on the summary sensitivity and specificity only. We developed and evaluated a robust and flexible random-effects bivariate finite mixture model for meta-analyzing diagnostic test accuracy studies. The proposed model accounts for both the within- and across-study heterogeneity in diagnostic test results, generates the probability that each study in a meta-analysis is outlying instead of dichotomizing the status of the studies, and allows assessing the impact of outlying studies on the pooled sensitivity, pooled specificity, and between-study heterogeneity. Our simulation study and real-life data examples demonstrated that the proposed model was robust to the existence of outlying studies, produced precise point and interval estimates of the pooled sensitivity and specificity, and yielded similar results to the standard models when there were no outliers. Extensive simulations demonstrated relatively better bias and confidence interval width, but comparable root mean squared error and lesser coverage probability of the proposed model. Practitioners can use our proposed model as a stand-alone model to conduct a meta-analysis of diagnostic test accuracy studies or as an alternative sensitivity analysis model when outlying studies are present in a meta-analysis.},
  archive      = {J_BIMJ},
  author       = {Zelalem F. Negeri},
  doi          = {10.1002/bimj.70062},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70062},
  shortjournal = {Bio. J.},
  title        = {A bivariate finite mixture random effects model for identifying and accommodating outliers in diagnostic test accuracy meta-analyses},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outcomes truncated by death in RCTs: A simulation study on the survivor average causal effect. <em>BIMJ</em>, <em>67</em>(3), e70061. (<a href='https://doi.org/10.1002/bimj.70061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous outcome measurements truncated by death present a challenge for the estimation of unbiased treatment effects in randomized controlled trials (RCTs). One way to deal with such situations is to estimate the survivor average causal effect (SACE), but this requires making nontestable assumptions. Motivated by an ongoing RCT in very preterm infants with intraventricular hemorrhage, we performed a simulation study to compare an SACE estimator with complete case analysis (CCA) and analysis after multiple imputation of missing outcomes. We set up nine scenarios combining positive, negative, and no treatment effect on the outcome (cognitive development) and on survival at 2 years of age. Treatment effect estimates from all methods were compared in terms of bias, mean squared error, and coverage with regard to two true treatment effects: the treatment effect on the outcome used in the simulation and the SACE, which was derived by simulation of both potential outcomes per patient. Despite targeting different estimands (principal stratum estimand, hypothetical estimand), the SACE-estimator and multiple imputation gave similar estimates of the treatment effect and efficiently reduced the bias compared to CCA. Also, both methods were relatively robust to omission of one covariate in the analysis, and thus violation of relevant assumptions. Although the SACE is not without controversy, we find it useful if mortality is inherent to the study population. Some degree of violation of the required assumptions is almost certain, but may be acceptable in practice.},
  archive      = {J_BIMJ},
  author       = {Stefanie von Felten and Chiara Vanetta and Christoph M. Rüegger and Sven Wellmann and Leonhard Held},
  doi          = {10.1002/bimj.70061},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70061},
  shortjournal = {Bio. J.},
  title        = {Outcomes truncated by death in RCTs: A simulation study on the survivor average causal effect},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical modeling to adjust for time trends in adaptive platform trials utilizing non-concurrent controls. <em>BIMJ</em>, <em>67</em>(3), e70059. (<a href='https://doi.org/10.1002/bimj.70059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing non-concurrent control (NCC) data in the analysis of late-entering arms in platform trials has recently received considerable attention. While incorporating NCC can lead to increased power and lower sample sizes, it might introduce bias to the effect estimators if temporal drifts are present. Aiming to mitigate this potential bias, we propose various frequentist model-based approaches that leverage the NCC, while adjusting for time. One of the currently available models incorporates time as a categorical fixed effect, separating the trial duration into periods, defined as time intervals bounded by any arm entering or leaving the platform. In this work, we propose two extensions of this model. First, we consider an alternative definition of time by dividing the trial into fixed-length calendar time intervals. Second, we propose alternative model-based time adjustments. Specifically, we investigate adjusting for random effects and employing splines to model time with a polynomial function. We evaluate the performance of the proposed approaches in a simulation study and illustrate their use through a case study. We show that adjusting for time via a spline function controls the type I error in trials with a sufficiently smooth time trend pattern and may lead to power gains compared to the standard fixed effect model. However, the fixed effect model with period adjustment is the most robust model for arbitrary time trends, provided that the trend is equal across all arms. Especially, in trials with sudden changes in the time trend, the period-adjustment model is preferred if NCCs are included.},
  archive      = {J_BIMJ},
  author       = {Pavla Krotka and Martin Posch and Mohamed Gewily and Günter Höglinger and Marta Bofill Roig},
  doi          = {10.1002/bimj.70059},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70059},
  shortjournal = {Bio. J.},
  title        = {Statistical modeling to adjust for time trends in adaptive platform trials utilizing non-concurrent controls},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating the optimal time to perform a positron emission tomography with prostate-specific membrane antigen in prostatectomized patients, based on data from clinical practice. <em>BIMJ</em>, <em>67</em>(3), e70058. (<a href='https://doi.org/10.1002/bimj.70058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prostatectomized patients are at risk of resurgence, and for this reason, during a follow-up period, they are monitored for prostate-specific antigen (PSA) growth, an indicator of tumor progression. The presence of tumors can be evaluated with an expensive exam, called positron emission tomography with prostate-specific membrane antigen (PET-PSMA). To justify the high cost of the PET-PSMA and, at the same time, to contain the risk for the patient, this exam should be recommended only when the evidence of tumor progression is strong. With the aim of estimating the optimal time to recommend the exam based on the patient's history and collected data, we build a hierarchical Bayesian model that describes, jointly, the PSA growth curve and the probability of a positive PET-PSMA. With our proposal, we process all past and present information about the patients PSA measurement and PET-PSMA results, in order to give an informed estimate of the optimal time, improving current practice.},
  archive      = {J_BIMJ},
  author       = {Martina Amongero and Gianluca Mastrantonio and Stefano De Luca and Mauro Gasparini},
  doi          = {10.1002/bimj.70058},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70058},
  shortjournal = {Bio. J.},
  title        = {Estimating the optimal time to perform a positron emission tomography with prostate-specific membrane antigen in prostatectomized patients, based on data from clinical practice},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hazards constitute key quantities for analyzing, interpreting and understanding time-to-event data. <em>BIMJ</em>, <em>67</em>(3), e70057. (<a href='https://doi.org/10.1002/bimj.70057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censoring makes time-to-event data special and requires customized statistical techniques. Survival and event history analysis therefore builds on hazards as the identifiable quantities in the presence of rather general censoring schemes. The reason is that hazards are conditional quantities, given previous survival, which enables estimation based on the current risk set—those still alive and under observation. But it is precisely their conditional nature that has made hazards subject of critique from a causal perspective: A beneficial treatment will help patients survive longer than had they remained untreated. Hence, in a randomized trial, randomization is broken in later risk sets, which, however, are the basis for statistical inference. We survey this dilemma—after all, mapping analyses of hazards onto probabilities in randomized trials is viewed as still having a causal interpretation—and argue that a causal interpretation is possible taking a functional point of view. We illustrate matters with examples from benefit–risk assessment: Prolonged survival may lead to more adverse events, but this need not imply a worse safety profile of the novel treatment. These examples illustrate that the situation at hand is conveniently parameterized using hazards, that the need to use survival techniques is not always fully appreciated and that censoring not necessarily leads to the question of “what, if no censoring?” The discussion should concentrate on how to correctly interpret causal hazard contrasts and analyses of hazards should routinely be translated onto probabilities.},
  archive      = {J_BIMJ},
  author       = {Jan Beyersmann and Claudia Schmoor and Martin Schumacher},
  doi          = {10.1002/bimj.70057},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70057},
  shortjournal = {Bio. J.},
  title        = {Hazards constitute key quantities for analyzing, interpreting and understanding time-to-event data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new inverse probability of selection weighted cox model to deal with outcome-dependent sampling in survival analysis. <em>BIMJ</em>, <em>67</em>(3), e70056. (<a href='https://doi.org/10.1002/bimj.70056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the study of genetic effect modifiers of cancer, we examined weighting approaches to correct for ascertainment bias in survival analysis. Outcome-dependent sampling is common in genetic epidemiology leading to study samples with too many events in comparison to the population and an overrepresentation of young, affected subjects. A usual approach to correct for ascertainment bias in this setting is to use an inverse probability-weighted Cox model, using weights based on external available population-based age-specific incidence rates of the type of cancer under investigation. However, the current approach is not general enough leading to invalid weights in relevant practical settings if oversampling of cases is not observed in all age groups. Based on the same principle of weighting observations by their inverse probability of selection, we propose a new, more general approach, called the generalized weighted approach. We show the advantage of the new generalized weighted cohort method using simulations and two real data sets. In both applications, the goal is to assess the association between common susceptibility loci identified in genome-wide association studies (GWAS) and cancer (colorectal and breast) using data collected through genetic testing in clinical genetics centers.},
  archive      = {J_BIMJ},
  author       = {Vera H. Arntzen and Marta Fiocco and Inge M. M. Lakeman and Maartje Nielsen and Mar Rodríguez-Girondo},
  doi          = {10.1002/bimj.70056},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70056},
  shortjournal = {Bio. J.},
  title        = {A new inverse probability of selection weighted cox model to deal with outcome-dependent sampling in survival analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian inference of phenotypic plasticity of cancer cells based on dynamic model for temporal cell proportion data. <em>BIMJ</em>, <em>67</em>(3), e70055. (<a href='https://doi.org/10.1002/bimj.70055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mounting evidence underscores the prevalent hierarchical organization of cancer tissues. At the foundation of this hierarchy reside cancer stem cells, a subset of cells endowed with the pivotal role of engendering the entire cancer tissue through cell differentiation. In recent times, substantial attention has been directed toward the phenomenon of cancer cell plasticity, where the dynamic interconversion between cancer stem cells and nonstem cancer cells has garnered significant interest. Since the task of detecting cancer cell plasticity from empirical data remains a formidable challenge, we propose a Bayesian statistical framework designed to infer phenotypic plasticity within cancer cells, utilizing temporal data on cancer stem cell proportions. Our approach is grounded in a stochastic model, adept at capturing the dynamic behaviors of cells. Leveraging Bayesian analysis, we scrutinize the moment equation governing cancer stem cell proportions, derived from the Kolmogorov forward equation of our stochastic model. Our methodology introduces an improved Euler method for parameter estimation within nonlinear ordinary differential equation models, also extending insights to compositional data. Extensive simulations robustly validate the efficacy of our proposed method. To further corroborate our findings, we apply our approach to analyze published data from SW620 colon cancer cell lines. Our results harmonize with in situ experiments, thereby reinforcing the utility of our method in discerning and quantifying phenotypic plasticity within cancer cells.},
  archive      = {J_BIMJ},
  author       = {Shuli Chen and Yuman Wang and Da Zhou and Jie Hu},
  doi          = {10.1002/bimj.70055},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70055},
  shortjournal = {Bio. J.},
  title        = {Bayesian inference of phenotypic plasticity of cancer cells based on dynamic model for temporal cell proportion data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A regularized MANOVA test for semicontinuous high-dimensional data. <em>BIMJ</em>, <em>67</em>(3), e70054. (<a href='https://doi.org/10.1002/bimj.70054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a MANOVA test for semicontinuous data that is applicable also when the dimension exceeds the sample size. The test statistic is obtained as a likelihood ratio, where the numerator and denominator are computed at the maxima of penalized likelihood functions under each hypothesis. Closed form solutions for the regularized estimators allow us to avoid computational overheads. We derive the null distribution using a permutation scheme. The power and level of the resulting test are evaluated in a simulation study. We illustrate the new methodology with two original data analyses, one regarding microRNA expression in human blastocyst cultures, and another regarding alien plant species invasion in the island of Socotra (Yemen).},
  archive      = {J_BIMJ},
  author       = {Elena Sabbioni and Claudio Agostinelli and Alessio Farcomeni},
  doi          = {10.1002/bimj.70054},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70054},
  shortjournal = {Bio. J.},
  title        = {A regularized MANOVA test for semicontinuous high-dimensional data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of methodological assumptions and covariates on the cutoff estimation in ROC analysis. <em>BIMJ</em>, <em>67</em>(3), e70053. (<a href='https://doi.org/10.1002/bimj.70053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The receiver operating characteristic (ROC) curve stands as a cornerstone in assessing the efficacy of biomarkers for disease diagnosis. Beyond merely evaluating performance, it provides with an optimal cutoff for biomarker values, crucial for disease categorization. While diverse methodologies exist for cutoff estimation, less attention has been paid to integrating covariate impact into this process. Covariates can strongly impact diagnostic summaries, leading to variations across different covariate levels. Therefore, a tailored covariate-based framework is imperative for outlining covariate-specific optimal cutoffs. Moreover, recent investigations into cutoff estimators have overlooked the influence of ROC curve estimation methodologies. This study endeavors to bridge this gap by addressing the research void. Extensive simulation studies are conducted to scrutinize the performance of ROC curve estimation models in estimating different cutoffs in varying scenarios, encompassing diverse data-generating mechanisms and covariate effects. In addition, leveraging the Alzheimer's Disease Neuroimaging Initiative (ADNI) data set, the research assesses the performance of different biomarkers in diagnosing Alzheimer's disease and determines the suitable optimal cutoffs.},
  archive      = {J_BIMJ},
  author       = {Soutik Ghosal},
  doi          = {10.1002/bimj.70053},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70053},
  shortjournal = {Bio. J.},
  title        = {Impact of methodological assumptions and covariates on the cutoff estimation in ROC analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How should parallel cluster randomized trials with a baseline period be Analyzed?—A survey of estimands and common estimators. <em>BIMJ</em>, <em>67</em>(3), e70052. (<a href='https://doi.org/10.1002/bimj.70052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parallel cluster randomized trial with baseline (PB-CRT) is a common variant of the standard parallel cluster randomized trial (P-CRT). We define two natural estimands in the context of PB-CRTs with informative cluster sizes, the individual-average treatment effect (iATE) and cluster-average treatment effect (cATE), to address individual and cluster-level hypotheses. In this work, we theoretically derive the convergence of the unweighted and inverse cluster-period size weighted (i) independence estimating equation (IEE), (ii) fixed-effects (FE) model, (iii) exchangeable mixed-effects (EME) model, and (iv) nested-exchangeable mixed-effects (NEME) model treatment effect estimators in a PB-CRT with informative cluster sizes and continuous outcomes. Overall, we theoretically show that the unweighted and weighted IEE and FE models yield consistent estimators for the iATE and cATE estimands. Although mixed-effects models yield inconsistent estimators to these two natural estimands under informative cluster sizes, we empirically demonstrate that the EME model is surprisingly robust to bias. This is in sharp contrast to the corresponding analyses in P-CRTs and the NEME model in PB-CRTs when informative cluster sizes are present, carrying implications for practice. We report a simulation study and conclude with a re-analysis of a PB-CRT examining the effects of community youth teams on improving mental health among adolescent girls in rural eastern India.},
  archive      = {J_BIMJ},
  author       = {Kenneth Menglin Lee and Fan Li},
  doi          = {10.1002/bimj.70052},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70052},
  shortjournal = {Bio. J.},
  title        = {How should parallel cluster randomized trials with a baseline period be Analyzed?—A survey of estimands and common estimators},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-markov multistate modeling approaches for multicohort event history data. <em>BIMJ</em>, <em>67</em>(3), e70051. (<a href='https://doi.org/10.1002/bimj.70051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two Cox-based multistate modeling approaches are compared for modeling a complex multicohort event history process. The first approach incorporates cohort information as a fixed covariate, thereby providing a direct estimation of the cohort-specific effects. The second approach includes the cohort as a stratum variable, which offers an extra flexibility in estimating the transition probabilities. Additionally, both approaches may include possible interaction terms between the cohort and a given prognostic predictor. Furthermore, the Markov property conditional on observed prognostic covariates is assessed using a global score test. Whenever departures from the Markovian assumption are revealed for a given transition, the time of entry into the current state is incorporated as a fixed covariate, yielding a semi-Markov process. The two proposed methods are applied to a three-wave dataset of COVID-19-hospitalized adults in the southern Barcelona metropolitan area (Spain), and the corresponding performance is discussed. While both semi-Markovian approaches are shown to be useful, the preferred one will depend on the focus of the inference. To summarize, the cohort–covariate approach enables an insightful discussion on the behavior of the cohort effects, whereas the stratum–cohort approach provides flexibility to estimate transition-specific underlying risks according to the different cohorts.},
  archive      = {J_BIMJ},
  author       = {Xavier Piulachs and Klaus Langohr and Mireia Besalú and Natàlia Pallarès and Jordi Carratalà and Cristian Tebé and Guadalupe Gómez Melis},
  doi          = {10.1002/bimj.70051},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {3},
  pages        = {e70051},
  shortjournal = {Bio. J.},
  title        = {Semi-markov multistate modeling approaches for multicohort event history data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward power analysis for partial least squares-based methods. <em>BIMJ</em>, <em>67</em>(2), e70050. (<a href='https://doi.org/10.1002/bimj.70050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, power analysis has become widely used in applied sciences, with the increasing importance of the replicability issue. When distribution-free methods, such as partial least squares (PLS)-based approaches, are considered, formulating power analysis is challenging. In this study, we introduce the methodological framework of a new procedure for performing power analysis when PLS-based methods are used. Data are simulated by the Monte Carlo method, assuming the null hypothesis of no effect is false and exploiting the latent structure estimated by PLS in the pilot data. In this way, the complex correlation data structure is explicitly considered in power analysis and sample size estimation. The paper offers insights into selecting test statistics for the power analysis procedure, comparing accuracy-based tests and those based on continuous parameters estimated by PLS. Simulated and real data sets are investigated to show how the method works in practice.},
  archive      = {J_BIMJ},
  author       = {Angela Andreella and Livio Finos and Bruno Scarpa and Matteo Stocchero},
  doi          = {10.1002/bimj.70050},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70050},
  shortjournal = {Bio. J.},
  title        = {Toward power analysis for partial least squares-based methods},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new frequentist implementation of the daniels and hughes bivariate meta-analysis model for surrogate endpoint evaluation. <em>BIMJ</em>, <em>67</em>(2), e70048. (<a href='https://doi.org/10.1002/bimj.70048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate endpoints are used when the primary outcome is difficult to measure accurately. Determining if a measure is suitable to use as a surrogate endpoint is a challenging task and a variety of meta-analysis models have been proposed for this purpose. The Daniels and Hughes bivariate model for trial-level surrogate endpoint evaluation is gaining traction but presents difficulties for frequentist estimation and hitherto only Bayesian solutions have been available. This is because the marginal model is not a conventional linear model and the number of unknown parameters increases at the same rate as the number of studies. This second property raises immediate concerns that the maximum likelihood estimator of the model's unknown variance component may be downwardly biased. We derive maximum likelihood estimating equations to motivate a bias adjusted estimator of this parameter. The bias correction terms in our proposed estimating equation are easily computed and have an intuitively appealing algebraic form. A simulation study is performed to illustrate how this estimator overcomes the difficulties associated with maximum likelihood estimation. We illustrate our methods using two contrasting examples from oncology.},
  archive      = {J_BIMJ},
  author       = {Dan Jackson and Michael Sweeting and Robbie C. M. van Aert and Sylwia Bujkiewicz and Keith R. Abrams and Wolfgang Viechtbauer},
  doi          = {10.1002/bimj.70048},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70048},
  shortjournal = {Bio. J.},
  title        = {A new frequentist implementation of the daniels and hughes bivariate meta-analysis model for surrogate endpoint evaluation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On sample size determination for augmented tests based on restricted mean survival time in randomized clinical trials. <em>BIMJ</em>, <em>67</em>(2), e70046. (<a href='https://doi.org/10.1002/bimj.70046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restricted mean survival time (RMST) is gaining attention as a measure to quantify the treatment effect on survival outcomes in randomized clinical trials. Several methods to determine sample size based on the RMST-based tests have been proposed. However, to the best of our knowledge, there is no discussion about the power and sample size regarding the augmented version of RMST-based tests, which utilize baseline covariates for a gain in estimation efficiency and in power for testing no treatment effect. The conventional event-driven study design based on the logrank test allows us to calculate the power for a given hazard ratio without specifying the survival functions. In contrast, the existing sample size determination methods for the RMST-based tests relies on the adequacy of the assumptions of the entire survival curves of two groups. Furthermore, to handle the augmented test, the correlation between the baseline covariates and the martingale residuals must be handled. To address these issues, we propose an approximated sample size formula for the augmented version of the RMST-based test, which does not require specifying the entire survival curve in the treatment group, and also a sample size recalculation approach to update the correlations between the baseline covariates and the martingale residuals with the blinded data. The proposed procedure will enable the studies to have the target power for a given RMST difference even when correct survival functions cannot be specified at the design stage.},
  archive      = {J_BIMJ},
  author       = {Satoshi Hattori and Hajime Uno},
  doi          = {10.1002/bimj.70046},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70046},
  shortjournal = {Bio. J.},
  title        = {On sample size determination for augmented tests based on restricted mean survival time in randomized clinical trials},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-arm crossover randomized controlled trial versus meta-analysis of N-of-1 studies: Comparison of statistical efficiency in determining an intervention effect. <em>BIMJ</em>, <em>67</em>(2), e70045. (<a href='https://doi.org/10.1002/bimj.70045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {N-of-1 trials are currently receiving broader attention in healthcare research when assessing the effectiveness of interventions. In contrast to the most commonly applied two-arm randomized controlled trial (RCT), in an N-of-1 design, the individual acts as their own control condition in the sense of a multiple crossover trial. N-of-1 trials can lead to a higher quality of patient by examining the effectiveness of an intervention at an individual level. Moreover, when a series of N-of-1 trials are properly aggregated, it becomes possible to detect an intervention effect at a population level. This work investigates whether a meta-analysis of summary data of a series of N-of-1 trials allows us to detect a statistically significant intervention effect with fewer participants than in a traditional, prospectively powered two-arm RCT and crossover design when evaluating a digital health intervention in cardiovascular care. After introducing these different analysis approaches, we compared the empirical properties in a simulation study both under the null hypothesis and with respect to power with different between-subject heterogeneity settings and in the presence of a carry-over effect. We further investigate the performance of a sequential aggregation procedure. In terms of simulated power, the threshold of 80% was achieved earlier for the aggregating procedure, requiring fewer participants.},
  archive      = {J_BIMJ},
  author       = {Anna Eleonora Carrozzo and Georg Zimmermann and Arne C. Bathke and Daniel Neunhaeuserer and Josef Niebauer and Stefan T. Kulnik},
  doi          = {10.1002/bimj.70045},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70045},
  shortjournal = {Bio. J.},
  title        = {Two-arm crossover randomized controlled trial versus meta-analysis of N-of-1 studies: Comparison of statistical efficiency in determining an intervention effect},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The shared weighted lindley frailty model for clustered failure time data. <em>BIMJ</em>, <em>67</em>(2), e70044. (<a href='https://doi.org/10.1002/bimj.70044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary goal of this paper is to introduce a novel frailty model based on the weighted Lindley (WL) distribution for modeling clustered survival data. We study the statistical properties of the proposed model. In particular, the amount of unobserved heterogeneity is directly parameterized by the variance of the frailty distribution such as gamma and inverse Gaussian frailty models. Parametric and semiparametric versions of the WL frailty model are studied. A simple expectation–maximization (EM) algorithm is proposed for parameter estimation. Simulation studies are conducted to evaluate its finite sample performance. Finally, we apply the proposed model to a real data set to analyze times after surgery in patients diagnosed with infiltrating ductal carcinoma and compare our results with classical frailty models carried out in this application, which shows the superiority of the proposed model. We implement an R package that includes estimation for fitting the proposed model based on the EM algorithm.},
  archive      = {J_BIMJ},
  author       = {Diego I. Gallardo and Marcelo Bourguignon and John L. Santibáñez},
  doi          = {10.1002/bimj.70044},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70044},
  shortjournal = {Bio. J.},
  title        = {The shared weighted lindley frailty model for clustered failure time data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wavelet-mixed landmark survival models for the effect of short-term changes of potassium in heart failure patients. <em>BIMJ</em>, <em>67</em>(2), e70043. (<a href='https://doi.org/10.1002/bimj.70043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methods to study the association between a longitudinal biomarker and the risk of death are very relevant for the long-term care of subjects affected by chronic illnesses, such as potassium in heart failure patients. Particularly in the presence of comorbidities or pharmacological treatments, sudden crises can cause potassium to undergo very abrupt yet transient changes. In the context of the monitoring of potassium, there is a need for a dynamic model that can be used in clinical practice to assess the risk of death related to an observed patient's potassium trajectory. We considered different landmark survival approaches, starting from the simple approach considering the most recent measurement. We then propose a novel method based on wavelet filtering and landmarking to retrieve the prognostic role of past short-term potassium shifts. We argue that while taking into account the smooth changes in the biomarker, short-term changes cannot be overlooked. State-of-the-art dynamic survival models are prone to give more importance to the smooth component of the potassium profiles. However, our findings suggest that it is essential to also take into account recent potassium instability to capture all the relevant prognostic information. The data used comes from over 2000 subjects, with a total of over 80,000 repeated potassium measurements collected through administrative health records. The proposed wavelet landmark method revealed the prognostic role of past short-term changes in potassium. We also performed a simulation study to assess how and when to apply the proposed wavelet-mixed landmark model.},
  archive      = {J_BIMJ},
  author       = {Caterina Gregorio and Giulia Barbati and Arjuna Scagnetto and Andrea di Lenarda and Francesca Ieva},
  doi          = {10.1002/bimj.70043},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70043},
  shortjournal = {Bio. J.},
  title        = {Wavelet-mixed landmark survival models for the effect of short-term changes of potassium in heart failure patients},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stacking model-based classifiers for dealing with multiple sets of noisy labels. <em>BIMJ</em>, <em>67</em>(2), e70042. (<a href='https://doi.org/10.1002/bimj.70042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised learning in presence of multiple sets of noisy labels is a challenging task that is receiving increasing interest in the ever-evolving landscape of healthcare analytics. Such an issue arises when multiple annotators are tasked to manually label the same training samples, potentially giving rise to discrepancies in class assignments among the supplied labels with respect to the ground truth. Commonly, the labeling process is entrusted to a small group of domain experts, and different level of experience and subjectivity may result in noisy training labels. To solve the classification task leveraging on the availability of multiple data annotators, we introduce a novel ensemble methodology constructed combining model-based classifiers separately trained on single sets of noisy labels. Eigenvalue Decomposition Discriminant Analysis is employed for the definition of the base learners, and six distinct averaging strategies are proposed to combine them. Two solutions necessitate a priori information, such as the partial knowledge of the ground truth labels or the annotators' level of expertise. Differently, the remaining four approaches are entirely data-driven. A simulation study and an application on real data showcase the improved predictive performance of our proposal, while also demonstrating the ability of automatically inferring annotators' expertise level as a by-product of the learning process.},
  archive      = {J_BIMJ},
  author       = {Giulia Montani and Andrea Cappozzo},
  doi          = {10.1002/bimj.70042},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70042},
  shortjournal = {Bio. J.},
  title        = {Stacking model-based classifiers for dealing with multiple sets of noisy labels},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survivor average causal effects for continuous time: A principal stratification approach to causal inference with semicompeting risks. <em>BIMJ</em>, <em>67</em>(2), e70041. (<a href='https://doi.org/10.1002/bimj.70041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semicompeting risks problems, nonterminal time-to-event outcomes, such as time to hospital readmission, are subject to truncation by death. These settings are often modeled with illness-death models for the hazards of the terminal and nonterminal events, but evaluating causal treatment effects with hazard models is problematic due to conditioning on survival—a posttreatment outcome—that is embedded in the definition of a hazard. Extending an existing survivor average causal effect (SACE) estimand, we frame the evaluation of treatment effects in the context of semicompeting risks with principal stratification and introduce two new causal estimands: the time-varying survivor average causal effect (TV-SACE) and the restricted mean survivor average causal effect (RM-SACE). These principal causal effects are defined among units that would survive regardless of assigned treatment. We adopt a Bayesian estimation procedure that parameterizes illness-death models for both treatment arms. We outline a frailty specification that can accommodate within-person correlation between nonterminal and terminal event times, and we discuss potential avenues for adding model flexibility. The method is demonstrated in the context of hospital readmission among late-stage pancreatic cancer patients.},
  archive      = {J_BIMJ},
  author       = {Leah Comment and Fabrizia Mealli and Sebastien Haneuse and Corwin M. Zigler},
  doi          = {10.1002/bimj.70041},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70041},
  shortjournal = {Bio. J.},
  title        = {Survivor average causal effects for continuous time: A principal stratification approach to causal inference with semicompeting risks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatially informed nonnegative matrix trifactorization for coclustering mass spectrometry data. <em>BIMJ</em>, <em>67</em>(2), e70031. (<a href='https://doi.org/10.1002/bimj.70031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mass spectrometry imaging techniques measure molecular abundance in a tissue sample at a cellular resolution, all while preserving the spatial structure of the tissue. This kind of technology offers a detailed understanding of the role of several molecular factors in biological systems. For this reason, the development of fast and efficient computational methods that can extract relevant signals from massive experiments has become necessary. A key goal in mass spectrometry data analysis is the identification of molecules with similar functions in the analyzed biological system. This result can be achieved by studying the spatial distribution of the molecules' abundance patterns. To do so, one can perform coclustering, that is, dividing the molecules into groups according to their expression patterns over the tissue and segmenting the tissue according to the molecules' abundance levels. We present TRIFASE, a semi-nonnegative matrix trifactorization technique that performs coclustering while accounting for the spatial correlation of the data. We propose an estimation algorithm that solves the proposed matrix trifactorization problem. Moreover, to improve scalability, we also propose two heuristic approximations of the most expensive steps, which help the algorithm converge while significantly streamlining the computational cost. We validated our method on a series of simulation experiments, comparing the different estimating strategies discussed in the article. Last, we analyzed a mouse brain tissue sample processed with MALDI-MSI technology, showing how TRIFASE extracts specific expression patterns of molecule abundance in localized tissue areas and discovers blocks of proteins whose activation is directly linked to specific biological mechanisms.},
  archive      = {J_BIMJ},
  author       = {Andrea Sottosanti and Francesco Denti and Stefania Galimberti and Davide Risso and Giulia Capitoli},
  doi          = {10.1002/bimj.70031},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70031},
  shortjournal = {Bio. J.},
  title        = {Spatially informed nonnegative matrix trifactorization for coclustering mass spectrometry data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified conditional borrowing-by-part power prior for dynamic and parameter-specific information borrowing of the gaussian endpoint. <em>BIMJ</em>, <em>67</em>(2), e70029. (<a href='https://doi.org/10.1002/bimj.70029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Borrowing external controls to augment the concurrent control arm is a popular topic in clinical trials. Bayesian dynamic borrowing methods adaptively discount external controls according to prior-data conflict. For the Gaussian endpoint, parameter-specific information borrowing enables differential discounting between the population mean and variance. The borrowing-by-part power prior employs two power parameters to separately downweight external likelihoods concerning the sample mean and variance. However, within the fully Bayesian framework, the posterior inference of the average treatment effect (ATE) defined as the population mean difference is significantly affected by the variance-specific prior-data conflict that reflects the heterogeneity of population variance. Here, we propose the modified conditional borrowing-by-part power prior (MCBPP) that separately discounts the external sample mean and variance according to parameter-specific prior-data conflicts, resulting in a more stable posterior estimation of ATE than its competitors under the same degree of mean-specific prior-data conflict. By fully discounting the external sample variance, the robust MCBPP (rMCBPP) can yield robust posterior inference of ATE against the variance-specific prior-data conflict. Although the population variance is considered a nuisance parameter, its homogeneity is equally important to justify information borrowing. We recommend the rMCBPP for borrowing external controls with a similar sample variance to concurrent controls because it exhibits better control of bias and Type I error rate than the modified power prior (MPP) assuming unknown variance in the absence of population variance heterogeneity. However, when faced with a significant sample variance discrepancy, the MPP assuming unknown variance is preferred given its better performance under severe population variance heterogeneity.},
  archive      = {J_BIMJ},
  author       = {Kai Wang and Han Cao and Chen Yao},
  doi          = {10.1002/bimj.70029},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70029},
  shortjournal = {Bio. J.},
  title        = {Modified conditional borrowing-by-part power prior for dynamic and parameter-specific information borrowing of the gaussian endpoint},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric estimation of a biometric function using ranked set sampling with ties information. <em>BIMJ</em>, <em>67</em>(2), e70007. (<a href='https://doi.org/10.1002/bimj.70007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean residual life (MRL) function plays an important role in the summary and analysis of survival data. The main advantage of this function is that it summarizes the information in units of time instead of a probability scale, which requires careful interpretation. Ranked set sampling (RSS) is a sampling technique designed for situations, where obtaining precise measurements of sample units is expensive or difficult, but ranking them without referring to their accurate values is cost-effective or easy. However, the practical application of RSS is hindered because each sample unit is required to assign a unique rank. To alleviate this difficulty, Frey developed a novel variation of RSS, called RSS-t, that records and utilizes the tie structure in the ranking process. In this paper, we propose several different nonparametric estimators for the MRL function based on RSS-t. Then, we compare the proposed estimators with their counterparts in simple random sampling (SRS) and RSS, where tie information is not utilized. We also implemented our proposed estimators on a real data set related to patient waiting times for liver transplantation, to show their applicability and efficiency in practice. Our results show that using ties information leads to an improved statistical inference for the MRL function, and therefore a smaller sample size is needed to reach a predetermined precision.},
  archive      = {J_BIMJ},
  author       = {Leila Jabari Koopaei and Ehsan Zamanzade and Afshin Parvardeh and Xinlei Wang},
  doi          = {10.1002/bimj.70007},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {2},
  pages        = {e70007},
  shortjournal = {Bio. J.},
  title        = {Nonparametric estimation of a biometric function using ranked set sampling with ties information},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unscaled indices for assessing agreement of functional data. <em>BIMJ</em>, <em>67</em>(1), e70039. (<a href='https://doi.org/10.1002/bimj.70039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A decision to adopt a new medical device requires a rigorous assessment of the reliability and reproducibility of its clinical measurements. In this paper, with the goal of establishing the validity and acceptability of modern high-tech medical devices that generate functional data, we focus on the problem of assessing agreement of multiple functional data that are measured on the same subjects by different methods/technologies/raters. Specifically, we introduce a series of unscaled indices, total deviation index (TDI) and coverage probability (CP), that themselves are functions of time and can delineate the trends of intramethod, intermethod, and total (intra+inter) agreement of functional data across time in terms of the original measurement scale. We also develop scalar-valued TDI and CP indices that summarize the degree of agreement over the entire domain based on the weighted average idea. We advocate an experimental design under which each of the two methods generates replicated functional data measurements for each subject, and express each index using a mean function and variance components of a bivariate multilevel functional linear mixed effects model. Such a formulation allows us to smoothly estimate the indices based on our bivariate multilevel functional principal component analysis approach that only requires eigenanalyses of univariate covariance functions for better efficiency and scalability. Comprehensive simulation studies are conducted to examine the finite-sample properties of the estimators. The proposed method is applied to assess the reliability and reproducibility of renogram curves generated by diuresis renography, a high-tech medical imaging device widely used to detect kidney obstruction.},
  archive      = {J_BIMJ},
  author       = {Kaeum Choi and Jeong Hoon Jang},
  doi          = {10.1002/bimj.70039},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70039},
  shortjournal = {Bio. J.},
  title        = {Unscaled indices for assessing agreement of functional data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parametric estimation of the mean number of events in the presence of competing risks. <em>BIMJ</em>, <em>67</em>(1), e70038. (<a href='https://doi.org/10.1002/bimj.70038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events, for example, hospitalizations or drug prescriptions, are common in time-to-event research. One useful summary measure of the recurrent event process is the mean number of events. Methods for estimating the mean number of events exist and are readily implemented for situations in which the recurrent event is the only possible outcome. However, estimation gets more challenging in the competing risk setting, in which methods are so far limited to nonparametric approaches. To this end, we propose a postestimation command for estimating the mean number of events in the presence of competing risks by jointly modeling the intensity function of the recurrent event and the survival function for the competing events. The proposed method is implemented in the R-package JointFPM which is available on CRAN. Simulations demonstrate low bias and good coverage in scenarios where the intensity of the recurrent event does not depend on the number of previous events. We illustrate our method using data on readmissions after colorectal cancer surgery included in the frailtypack package for R. Estimates of the mean number of events can be used to augment time-to-event analyses when both recurrent and competing events exist. The proposed parametric approach offers estimation of a smooth function across time as well as easy estimation of different contrasts which is not available using a nonparametric approach.},
  archive      = {J_BIMJ},
  author       = {Joshua P. Entrop and Lasse H. Jakobsen and Michael J. Crowther and Mark Clements and Sandra Eloranta and Caroline E. Dietrich},
  doi          = {10.1002/bimj.70038},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70038},
  shortjournal = {Bio. J.},
  title        = {Parametric estimation of the mean number of events in the presence of competing risks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network meta-analysis of time-to-event endpoints with individual participant data using restricted mean survival time regression. <em>BIMJ</em>, <em>67</em>(1), e70037. (<a href='https://doi.org/10.1002/bimj.70037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) extends pairwise meta-analysis to compare multiple treatments simultaneously by combining “direct” and “indirect” comparisons of treatments. The availability of individual participant data (IPD) makes it possible to evaluate treatment effect moderation and to draw inferences about treatment effects by taking the full utilization of individual covariates from multiple clinical trials. In IPD-NMA, restricted mean survival time (RMST) models have gained popularity when analyzing time-to-event outcomes because RMST models offer more straightforward interpretations of treatment effects with fewer assumptions than hazard ratios commonly estimated from Cox models. Existing approaches estimate RMST within each study and then combine by using aggregate-level NMA methods. However, these methods cannot incorporate individual covariates to evaluate the effect moderation. In this paper, we propose advanced RMST NMA models when IPD are available. Our models allow us to study treatment effect moderation and provide a comprehensive understanding about comparative effectiveness of treatments and subgroup effects. The methods are evaluated by an extensive simulation study and illustrated using a real NMA example about treatments for patients with atrial fibrillation.},
  archive      = {J_BIMJ},
  author       = {Kaiyuan Hua and Xiaofei Wang and Hwanhee Hong},
  doi          = {10.1002/bimj.70037},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70037},
  shortjournal = {Bio. J.},
  title        = {Network meta-analysis of time-to-event endpoints with individual participant data using restricted mean survival time regression},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional variable selection with competing events using cooperative penalized regression. <em>BIMJ</em>, <em>67</em>(1), e70036. (<a href='https://doi.org/10.1002/bimj.70036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection is an important step in the analysis of high-dimensional data, yet there are limited options for survival outcomes in the presence of competing risks. Commonly employed penalized Cox regression considers each event type separately through cause-specific models, neglecting possibly shared information between them. We adapt the feature-weighted elastic net (fwelnet), an elastic net generalization, to survival outcomes and competing risks. For two causes, our proposed algorithm fits two alternating cause-specific models, where each model receives the coefficient vector of the complementary model as prior information. We dub this “cooperative penalized regression,” as it enables the modeling of competing risk data with cause-specific models while accounting for shared effects between causes. Coefficients that are shrunken toward zero in the model for the first cause will receive larger penalization weights in the model for the second cause and vice versa. Through multiple iterations, this process ensures stronger penalization of uninformative predictors in both models. We demonstrate our method's variable selection capabilities on simulated genomics data and apply it to bladder cancer microarray data. We evaluate selection performance using the positive predictive value for the correct selection of informative features and the false positive rate for the selection of uninformative variables. The benchmark compares results with cause-specific penalized Cox regression, random survival forests, and likelihood-boosted Cox regression. Results indicate that our approach is more effective at selecting informative features and removing uninformative features. In settings without shared effects, variable selection performance is similar to cause-specific penalized Cox regression.},
  archive      = {J_BIMJ},
  author       = {Lukas Burk and Andreas Bender and Marvin N. Wright},
  doi          = {10.1002/bimj.70036},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70036},
  shortjournal = {Bio. J.},
  title        = {High-dimensional variable selection with competing events using cooperative penalized regression},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mediation analysis with Exposure–Mediator interaction and covariate measurement error under the additive hazards model. <em>BIMJ</em>, <em>67</em>(1), e70035. (<a href='https://doi.org/10.1002/bimj.70035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis is a useful tool to examine how an exposure variable causally affects an outcome variable through an intermediate variable. In recent years, there is increasing research interest in mediation analysis with survival data. The existing literature usually requires accurate measurements of the mediator and the confounders, which is infeasible in many biomedical and social science studies. Ignoring measurement errors may lead to misleading inference results. Furthermore, the current identification results of causal effects under the additive hazards model are limited to the scenario with no exposure–mediator interaction, which can be unappealing in mediation analysis. In this paper, we derive the identification results of direct and indirect effects under the additive hazards model in the presence of exposure–mediator interaction. Furthermore, we propose a corrected approach to adjust for the impact of measurement error in the mediator and the confounders and obtain consistent estimations of the direct and indirect effects. The performance of the proposed method is studied in simulation studies and a real data study.},
  archive      = {J_BIMJ},
  author       = {Ying Yan and Lingzhu Shen},
  doi          = {10.1002/bimj.70035},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70035},
  shortjournal = {Bio. J.},
  title        = {Mediation analysis with Exposure–Mediator interaction and covariate measurement error under the additive hazards model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bias-corrected bayesian nonparametric model for combining studies with varying quality in meta-analysis. <em>BIMJ</em>, <em>67</em>(1), e70034. (<a href='https://doi.org/10.1002/bimj.70034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian nonparametric (BNP) approaches for meta-analysis have been developed to relax distributional assumptions and handle the heterogeneity of random effects distributions. These models account for possible clustering and multimodality of the random effects distribution. However, when we combine studies of varying quality, the resulting posterior is not only a combination of the results of interest but also factors threatening the integrity of the studies' results. We refer to these factors as the studies' internal validity biases (e.g., reporting bias, data quality, and patient selection bias). In this paper, we introduce a new meta-analysis model called the bias-corrected Bayesian nonparametric (BC-BNP) model, which aims to automatically correct for internal validity bias in meta-analysis by only using the reported effects and their standard errors. The BC-BNP model is based on a mixture of a parametric random effects distribution, which represents the model of interest, and a BNP model for the bias component. This model relaxes the parametric assumptions of the bias distribution of the model introduced by Verde. Using simulated data sets, we evaluate the BC-BNP model and illustrate its applications with two real case studies. Our results show several potential advantages of the BC-BNP model: (1) It can detect bias when present while producing results similar to a simple normal–normal random effects model when bias is absent. (2) Relaxing the parametric assumptions of the bias component does not affect the model of interest and yields consistent results with the model of Verde. (3) In some applications, a BNP model of bias offers a better understanding of the studies' biases by clustering studies with similar biases. We implemented the BC-BNP model in the R package jarbes, facilitating its practical application.},
  archive      = {J_BIMJ},
  author       = {Pablo Emilio Verde and Gary L. Rosner},
  doi          = {10.1002/bimj.70034},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70034},
  shortjournal = {Bio. J.},
  title        = {A bias-corrected bayesian nonparametric model for combining studies with varying quality in meta-analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analysis for effects of multiple exposures in the presence of unmeasured confounding. <em>BIMJ</em>, <em>67</em>(1), e70033. (<a href='https://doi.org/10.1002/bimj.70033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiological research aims to investigate how multiple exposures affect health outcomes of interest, but observational studies often suffer from biases caused by unmeasured confounders. In this study, we develop a novel sensitivity model to investigate the effect of correlated multiple exposures on the continuous health outcomes of interest. The proposed sensitivity analysis is model-agnostic and can be applied to any machine learning algorithm. The interval of single- or joint-exposure effects is efficiently obtained by solving a linear programming problem with a quadratic constraint. Some strategies for reducing the input burden in the sensitivity analysis are discussed. We demonstrate the usefulness of sensitivity analysis via numerical studies and real data application.},
  archive      = {J_BIMJ},
  author       = {Boram Jeong and Seungjae Lee and Shinhee Ye and Donghwan Lee and Woojoo Lee},
  doi          = {10.1002/bimj.70033},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70033},
  shortjournal = {Bio. J.},
  title        = {Sensitivity analysis for effects of multiple exposures in the presence of unmeasured confounding},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantification of difference in nonselectivity between in vitro diagnostic medical devices. <em>BIMJ</em>, <em>67</em>(1), e70032. (<a href='https://doi.org/10.1002/bimj.70032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correct measurement results from in vitro diagnostic (IVD) medical devices (MD) are crucial for optimal patient care. The performance of IVD-MDs is often assessed through method comparison studies. Such studies can be compromised by the influence of various factors. The effect of these factors must be examined in every method comparison study, for example, nonselectivity differences between compared IVD-MDs are examined. Historically, selectivity or nonselectivity has been defined as a qualitative term. However, a quantification of nonselectivity differences between IVD-MDs is needed. This paper fills this need by introducing a novel measure for quantifying differences in nonselectivity (DINS) between a pair of IVD-MDs. Assuming one of the IVD-MDs involved in the comparison exhibits high selectivity for the analyte, it becomes feasible to quantify nonselectivity in the other IVD-MD by employing this DINS measure. Our approach leverages elements from univariate ordinary least squares regression and incorporates repeatability IVD-MD variances, resulting in a normalized measure. We also introduce a plug-in estimator for this measure, which is notably linked to the average relative increase in prediction interval widths attributable to DINS. This connection is exploited to establish a criterion for identifying excessive DINS utilizing a proof-of-hazard approach. Utilizing Monte Carlo simulations, we investigate how the estimator relates to population characteristics like DINS and heteroskedasticity. We find that DINS impacts the mean, variance, and 99th percentile of the estimator, while heteroskedasticity affects only the latter two, and to a considerably smaller extent compared to DINS. Importantly, the size of the study design modulates these effects. We also confirm, when using clinical data, that DINS between pairs of IVD-MDs influence the estimator correspondingly to those of simulated data. Thus, the proposed estimator serves as an effective metric for quantifying DINS between IVD-MDs and helping to determine the quality of a method comparison study.},
  archive      = {J_BIMJ},
  author       = {Pernille Kjeilen Fauskanger and Sverre Sandberg and Jesper Johansen and Thomas Keller and Jeffrey Budd and W. Greg Miller and Anne Stavelin and Vincent Delatour and Mauro Panteghini and Bård Støve},
  doi          = {10.1002/bimj.70032},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70032},
  shortjournal = {Bio. J.},
  title        = {Quantification of difference in nonselectivity between in vitro diagnostic medical devices},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing and comparing four families of bayesian network autocorrelation models for binary outcomes: Estimating peer effects involving adoption of medical technologies. <em>BIMJ</em>, <em>67</em>(1), e70030. (<a href='https://doi.org/10.1002/bimj.70030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the extensive use of network autocorrelation models in social network analysis, network autocorrelation models for binary dependent variables have received surprisingly scant attention. In this paper, we develop four network autocorrelation models for a binary random variable defined by whether the peer effect (also termed social influence or contagion) acts on latent continuous outcomes leading to an indirect effect under a normal or a logistic distribution or on the probability of the observed outcome itself under a probit or a logit link function defining a direct effect to account for interdependence between outcomes. For all models, we use a Bayesian approach for model estimation under a uniform prior on a transformed peer effect parameter ( ) designed to enhance model computation and compare results to those under the uniform prior for . We use simulation to assess the performance of Bayesian point and interval estimators for each of the four models when the model that generated the data is used for estimation (precision assessment) and when each of the other three models instead generated the data (robustness assessment). We construct a United States New England region patient-sharing hospital network and apply the four network autocorrelation models to study the adoption of robotic surgery, a new medical technology, among hospitals using a cohort of United States Medicare beneficiaries in 2016 and 2017. Finally, we develop a deviance information criterion for each of the four models to compare their fit to the observed data and use posterior predictive p -values to assess the models' ability to recover specified features of the data. The results find that although the indirect peer effect of the propensity of peer hospital adoption on that of the focal hospital is positive under both latent response autocorrelation models, the direct peer effect of the peer hospital's probability of adopting robotic surgery on the probability of the focal hospital adopting robotic surgery decreases under both mean autocorrelation data models. However, neither of these associations is statistically significant.},
  archive      = {J_BIMJ},
  author       = {Guanqing Chen and A. James O'Malley},
  doi          = {10.1002/bimj.70030},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70030},
  shortjournal = {Bio. J.},
  title        = {Developing and comparing four families of bayesian network autocorrelation models for binary outcomes: Estimating peer effects involving adoption of medical technologies},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The progression-free-survival ratio in molecularly aided tumor trials: A critical examination of current practice and suggestions for alternative methods. <em>BIMJ</em>, <em>67</em>(1), e70028. (<a href='https://doi.org/10.1002/bimj.70028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progression-free-survival ratio is a popular endpoint in oncology trials, which is frequently applied to evaluate the efficacy of molecularly targeted treatments in late-stage patients. Using elementary calculations and simulations, numerous shortcomings of the current methodology are pointed out. As a remedy to these shortcomings, an alternative methodology is proposed, using a marginal Cox model or a marginal accelerated failure time model for clustered time-to-event data. Using comprehensive simulations, it is shown that this methodology outperforms existing methods in settings where the intrapatient correlation is low to moderate. The performance of the model is further demonstrated in a real data example from a molecularly aided tumor trial. Sample size considerations are discussed.},
  archive      = {J_BIMJ},
  author       = {Dominic Edelmann and Tobias Terzer and Peter Horak and Richard Schlenk and Axel Benner},
  doi          = {10.1002/bimj.70028},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70028},
  shortjournal = {Bio. J.},
  title        = {The progression-free-survival ratio in molecularly aided tumor trials: A critical examination of current practice and suggestions for alternative methods},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A principled approach to adjust for unmeasured time-stable confounding of supervised treatment. <em>BIMJ</em>, <em>67</em>(1), e70026. (<a href='https://doi.org/10.1002/bimj.70026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to adjust for unmeasured time-stable confounding when the time between consecutive treatment administrations is fixed. We achieve this by focusing on a new-user cohort. Furthermore, we envisage that all time-stable confounding goes through the potential time on treatment as dictated by the disease condition at the initiation of treatment. Following this logic, we may eliminate all unmeasured time-stable confounding by adjusting for the potential time on treatment. A challenge with this approach is that right censoring of the potential time on treatment occurs when treatment is terminated at the time of the event of interest, for example, if the event of interest is death. We show how this challenge may be solved by means of the expectation-maximization algorithm without imposing any further assumptions on the distribution of the potential time on treatment. The usefulness of the methodology is illustrated in a simulation study. We also apply the methodology to investigate the effect of depression/anxiety drugs on subsequent poisoning by other medications in the Danish population by means of national registries. We find a protective effect of treatment with selective serotonin reuptake inhibitors on the risk of poisoning by various medications (1- year risk difference of approximately ) and a standard Cox model analysis shows a harming effect (1-year risk difference of approximately ), which is consistent with what we would expect due to confounding by indication. Unmeasured time-stable confounding can be entirely adjusted for when the time between consecutive treatment administrations is fixed.},
  archive      = {J_BIMJ},
  author       = {Jeppe Ekstrand Halkjær Madsen and Thomas Delvin and Thomas Scheike and Christian Pipper},
  doi          = {10.1002/bimj.70026},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70026},
  shortjournal = {Bio. J.},
  title        = {A principled approach to adjust for unmeasured time-stable confounding of supervised treatment},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A preplanned multi-stage platform trial for discovering multiple superior treatments with control of FWER and power. <em>BIMJ</em>, <em>67</em>(1), e70025. (<a href='https://doi.org/10.1002/bimj.70025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in the implementation of platform trials, which provide the flexibility to incorporate new treatment arms during the trial and the ability to halt treatments early based on lack of benefit or observed superiority. In such trials, it can be important to ensure that error rates are controlled. This paper introduces a multi-stage design that enables the addition of new treatment arms, at any point, in a preplanned manner within a platform trial, while still maintaining control over the family-wise error rate. This paper focuses on finding the required sample size to achieve a desired level of statistical power when treatments are continued to be tested even after a superior treatment has already been found. This may be of interest if there are treatments from different sponsors which are also superior to the current control or multiple doses being tested. The calculations to determine the expected sample size is given. A motivating trial is presented in which the sample size of different configurations is studied. In addition, the approach is compared to running multiple separate trials and it is shown that in many scenarios if family-wise error rate control is needed there may not be benefit in using a platform trial when comparing the sample size of the trial.},
  archive      = {J_BIMJ},
  author       = {Peter Greenstreet and Thomas Jaki and Alun Bedding and Pavel Mozgunov},
  doi          = {10.1002/bimj.70025},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70025},
  shortjournal = {Bio. J.},
  title        = {A preplanned multi-stage platform trial for discovering multiple superior treatments with control of FWER and power},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing balance of baseline time-dependent covariates via the fréchet distance. <em>BIMJ</em>, <em>67</em>(1), e70024. (<a href='https://doi.org/10.1002/bimj.70024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessment of covariate balance is a key step when performing comparisons between groups particularly in real-world data. We generally evaluate it on baseline covariates, but rarely on longitudinal ones prior to a management decision. We could use pointwise standardized mean differences, standardized differences of slopes, or weights from the model for such purpose. Pointwise differences could be cumbersome for densely sampled longitudinal markers and/or measured at different points. Slopes are suitable for linear or transformable models but not for more complex curves. Weights do not identify the specific covariate(s) responsible for imbalances. This work presents the Fréchet distance as a viable alternative to assess balance of time-dependent covariates. A set of linear and nonlinear curves for which their standardized difference or differences in functional parameters were within 10% sought to identify the Fréchet distance equivalent to this threshold. This threshold is dependent on the level of noise present and thus within group heterogeneity and error variance are needed for its interpretation. Applied to a set of real curves representing the monthly trajectory of hemoglobin A1c from diabetic patients showed that the curves in the two groups were not balanced at the 10% mark. A Beta distribution represents the Fréchet distance distribution reasonably well in most scenarios. This assessment of covariate balance provides the following advantages: It can handle curves of different lengths, shapes, and arbitrary time points. Future work includes examining the utility of this measure under within-series missingness, within-group heterogeneity, its comparison with other approaches, and asymptotics.},
  archive      = {J_BIMJ},
  author       = {Mireya Díaz},
  doi          = {10.1002/bimj.70024},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70024},
  shortjournal = {Bio. J.},
  title        = {Assessing balance of baseline time-dependent covariates via the fréchet distance},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating a domain adaptation approach for integrating different measurement instruments in a longitudinal clinical registry. <em>BIMJ</em>, <em>67</em>(1), e70023. (<a href='https://doi.org/10.1002/bimj.70023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a longitudinal clinical registry, different measurement instruments might have been used for assessing individuals at different time points. To combine them, we investigate deep learning techniques for obtaining a joint latent representation, to which the items of different measurement instruments are mapped. This corresponds to domain adaptation, an established concept in computer science for image data. Using the proposed approach as an example, we evaluate the potential of domain adaptation in a longitudinal cohort setting with a rather small number of time points, motivated by an application with different motor function measurement instruments in a registry of spinal muscular atrophy (SMA) patients. There, we model trajectories in the latent representation by ordinary differential equations (ODEs), where person-specific ODE parameters are inferred from baseline characteristics. The goodness of fit and complexity of the ODE solutions then allow to judge the measurement instrument mappings. We subsequently explore how alignment can be improved by incorporating corresponding penalty terms into model fitting. To systematically investigate the effect of differences between measurement instruments, we consider several scenarios based on modified SMA data, including scenarios where a mapping should be feasible in principle and scenarios where no perfect mapping is available. While misalignment increases in more complex scenarios, some structure is still recovered, even if the availability of measurement instruments depends on patient state. A reasonable mapping is feasible also in the more complex real SMA data set. These results indicate that domain adaptation might be more generally useful in statistical modeling for longitudinal registry data.},
  archive      = {J_BIMJ},
  author       = {Maren Hackenberg and Michelle Pfaffenlehner and Max Behrens and Astrid Pechmann and Janbernd Kirschner and Harald Binder},
  doi          = {10.1002/bimj.70023},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70023},
  shortjournal = {Bio. J.},
  title        = {Investigating a domain adaptation approach for integrating different measurement instruments in a longitudinal clinical registry},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Goodness-of-fit testing for a regression model with a doubly truncated response. <em>BIMJ</em>, <em>67</em>(1), e70022. (<a href='https://doi.org/10.1002/bimj.70022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis and epidemiology, among other fields, interval sampling is often employed. With interval sampling, the individuals undergoing the event of interest within a calendar time interval are recruited. This results in doubly truncated event times. Double truncation, which may appear with other sampling designs too, induces a selection bias, so ordinary statistical methods are generally inconsistent. In this paper, we introduce goodness-of-fit procedures for a regression model when the response variable is doubly truncated. With this purpose, a marked empirical process based on weighted residuals is constructed and its weak convergence is established. Kolmogorov–Smirnov– and Cramér–von Mises–type tests are consequently derived from such core process, and a bootstrap approximation for their practical implementation is given. The performance of the proposed tests is investigated through simulations. An application to model selection for AIDS incubation time as depending on age at infection is provided.},
  archive      = {J_BIMJ},
  author       = {Jacobo de Uña-Álvarez},
  doi          = {10.1002/bimj.70022},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70022},
  shortjournal = {Bio. J.},
  title        = {Goodness-of-fit testing for a regression model with a doubly truncated response},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test statistics and statistical inference for data with informative cluster sizes. <em>BIMJ</em>, <em>67</em>(1), e70021. (<a href='https://doi.org/10.1002/bimj.70021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, investigators often encounter clustered data. The cluster sizes are said to be informative if the outcome depends on the cluster size. Ignoring informative cluster sizes in the analysis leads to biased parameter estimation in marginal and mixed-effect regression models. Several methods to analyze data with informative cluster sizes have been proposed; however, methods to test the informativeness of the cluster sizes are limited, particularly for the marginal model. In this paper, we propose a score test and a Wald test to examine the informativeness of the cluster sizes for a generalized linear model, a Cox model, and a proportional subdistribution hazards model. Statistical inference can be conducted through weighted estimating equations. The simulation results show that both tests control Type I error rates well, but the score test has higher power than the Wald test for right-censored data while the power of the Wald test is generally higher than the score test for the binary outcome. We apply the Wald and score tests to hematopoietic cell transplant data and compare regression analysis results with/without adjusting for informative cluster sizes.},
  archive      = {J_BIMJ},
  author       = {Soyoung Kim and Michael J. Martens and Kwang Woo Ahn},
  doi          = {10.1002/bimj.70021},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70021},
  shortjournal = {Bio. J.},
  title        = {Test statistics and statistical inference for data with informative cluster sizes},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjusted inference for multiple testing procedure in group-sequential designs. <em>BIMJ</em>, <em>67</em>(1), e70020. (<a href='https://doi.org/10.1002/bimj.70020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adjustment of statistical significance levels for repeated analysis in group-sequential trials has been understood for some time. Adjustment accounting for testing multiple hypotheses is also well understood. There is limited research on simultaneously adjusting for both multiple hypothesis testing and repeated analyses of one or more hypotheses. We address this gap by proposing adjusted-sequential p-values that reject when they are less than or equal to the family-wise Type I error rate (FWER). We also propose sequential p $p$ -values for intersection hypotheses to compute adjusted-sequential p $p$ -values for elementary hypotheses. We demonstrate the application using weighted Bonferroni tests and weighted parametric tests for inference on each elementary hypothesis tested.},
  archive      = {J_BIMJ},
  author       = {Yujie Zhao and Qi Liu and Linda Z. Sun and Keaven M. Anderson},
  doi          = {10.1002/bimj.70020},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70020},
  shortjournal = {Bio. J.},
  title        = {Adjusted inference for multiple testing procedure in group-sequential designs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple contrast tests in the presence of partial heteroskedasticity. <em>BIMJ</em>, <em>67</em>(1), e70019. (<a href='https://doi.org/10.1002/bimj.70019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a general approach for handling multiple contrast tests for normally distributed data in the presence of partial heteroskedasticity. In contrast to the usual case of complete heteroskedasticity, the treatments belong to subgroups according to their variances. Treatments within these subgroups are homoskedastic, whereas treatments of different subgroups are heteroskedastic. New candidate as well as already existing approaches are described and compared by α $\alpha$ -simulations. Power simulations show that a gain in power is achieved when the partial heteroskedasticity is taken into account compared to procedures which wrongly assume complete heteroskedasticity. The new approaches will be applied to a phytopathological experiment.},
  archive      = {J_BIMJ},
  author       = {Mario Hasler and Tim Birr and Ludwig A. Hothorn},
  doi          = {10.1002/bimj.70019},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70019},
  shortjournal = {Bio. J.},
  title        = {Multiple contrast tests in the presence of partial heteroskedasticity},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oncology clinical trial design planning based on a multistate model that jointly models progression-free and overall survival endpoints. <em>BIMJ</em>, <em>67</em>(1), e70017. (<a href='https://doi.org/10.1002/bimj.70017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When planning an oncology clinical trial, the usual approach is to assume proportional hazards and even an exponential distribution for time-to-event endpoints. Often, besides the gold-standard endpoint overall survival (OS), progression-free survival (PFS) is considered as a second confirmatory endpoint. We use a survival multistate model to jointly model these two endpoints and find that neither exponential distribution nor proportional hazards will typically hold for both endpoints simultaneously. The multistate model provides a stochastic process approach to model the dependency of such endpoints neither requiring latent failure times nor explicit dependency modeling such as copulae. We use the multistate model framework to simulate clinical trials with endpoints OS and PFS and show how design planning questions can be answered using this approach. In particular, nonproportional hazards for at least one of the endpoints are a consequence of OS and PFS being dependent and are naturally modeled to improve planning. We then illustrate how clinical trial design can be based on simulations from a multistate model. Key applications are coprimary endpoints and group-sequential designs. Simulations for these applications show that the standard simplifying approach may very well lead to underpowered or overpowered clinical trials. Our approach is quite general and can be extended to more complex trial designs, further endpoints, and other therapeutic areas. An R package is available on CRAN.},
  archive      = {J_BIMJ},
  author       = {Alexandra Erdmann and Jan Beyersmann and Kaspar Rufibach},
  doi          = {10.1002/bimj.70017},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70017},
  shortjournal = {Bio. J.},
  title        = {Oncology clinical trial design planning based on a multistate model that jointly models progression-free and overall survival endpoints},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). To tweak or not to tweak. how exploiting flexibilities in gene set analysis leads to overoptimism. <em>BIMJ</em>, <em>67</em>(1), e70016. (<a href='https://doi.org/10.1002/bimj.70016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene set analysis, a popular approach for analyzing high-throughput gene expression data, aims to identify sets of genes that show enriched expression patterns between two conditions. In addition to the multitude of methods available for this task, users are typically left with many options when creating the required input and specifying the internal parameters of the chosen method. This flexibility can lead to uncertainty about the “right” choice, further reinforced by a lack of evidence-based guidance. Especially when their statistical experience is scarce, this uncertainty might entice users to produce preferable results using a “trial-and-error” approach. While it may seem unproblematic at first glance, this practice can be viewed as a form of “cherry-picking” and cause an optimistic bias, rendering the results nonreplicable on independent data. After this problem has attracted a lot of attention in the context of classical hypothesis testing, we now aim to raise awareness of such overoptimism in the different and more complex context of gene set analyses. We mimic a hypothetical researcher who systematically selects the analysis variants yielding their preferred results, thereby considering three distinct goals they might pursue. Using a selection of popular gene set analysis methods, we tweak the results in this way for two frequently used benchmark gene expression data sets. Our study indicates that the potential for overoptimism is particularly high for a group of methods frequently used despite being commonly criticized. We conclude by providing practical recommendations to counter overoptimism in research findings in gene set analysis and beyond.},
  archive      = {J_BIMJ},
  author       = {Milena Wünsch and Christina Sauer and Moritz Herrmann and Ludwig Christian Hinske and Anne-Laure Boulesteix},
  doi          = {10.1002/bimj.70016},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70016},
  shortjournal = {Bio. J.},
  title        = {To tweak or not to tweak. how exploiting flexibilities in gene set analysis leads to overoptimism},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Best subset solution path for linear dimension reduction models using continuous optimization. <em>BIMJ</em>, <em>67</em>(1), e70015. (<a href='https://doi.org/10.1002/bimj.70015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of best variables is a challenging problem in supervised and unsupervised learning, especially in high-dimensional contexts where the number of variables is usually much larger than the number of observations. In this paper, we focus on two multivariate statistical methods: principal components analysis and partial least squares. Both approaches are popular linear dimension-reduction methods with numerous applications in several fields including in genomics, biology, environmental science, and engineering. In particular, these approaches build principal components, new variables that are combinations of all the original variables. A main drawback of principal components is the difficulty to interpret them when the number of variables is large. To define principal components from the most relevant variables, we propose to cast the best subset solution path method into principal component analysis and partial least square frameworks. We offer a new alternative by exploiting a continuous optimization algorithm for best subset solution path. Empirical studies show the efficacy of our approach for providing the best subset solution path. The usage of our algorithm is further exposed through the analysis of two real data sets. The first data set is analyzed using the principle component analysis while the analysis of the second data set is based on partial least square framework.},
  archive      = {J_BIMJ},
  author       = {Benoit Liquet and Sarat Moka and Samuel Muller},
  doi          = {10.1002/bimj.70015},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {1},
  pages        = {e70015},
  shortjournal = {Bio. J.},
  title        = {Best subset solution path for linear dimension reduction models using continuous optimization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
