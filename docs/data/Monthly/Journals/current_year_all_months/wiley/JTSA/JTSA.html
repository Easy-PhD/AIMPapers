<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JTSA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jtsa">JTSA - 57</h2>
<ul>
<li><details>
<summary>
(2025). Decoupling interday and intraday volatility dynamics with price durations. <em>JTSA</em>, <em>46</em>(6), 1224-1250. (<a href='https://doi.org/10.1111/jtsa.12849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel framework for volatility estimation based on price durations with an adaptive price change threshold. This innovation allows us to disentangle daily and intraday volatility dynamics from price durations, which greatly simplifies the parametric modelling of price durations and hence leads to more accurate volatility estimators. Simulation results demonstrate superior finite-sample performance of our duration-based estimators for both spot and integrated volatility compared with some established methods. An empirical application based on intraday data for the SPDR S&P 500 ETF highlights the improved forecasting accuracy of our integrated volatility estimator within a standard daily volatility forecasting framework. Furthermore, an intraday analysis of spot volatility estimation shows that our method can capture the immediate and substantial impact of FOMC news announcements on market volatility.},
  archive      = {J_JTSA},
  author       = {Yifan Li and Ingmar Nolte and Sandra Nolte and Shifan Yu},
  doi          = {10.1111/jtsa.12849},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1224-1250},
  shortjournal = {J. Time Series Anal.},
  title        = {Decoupling interday and intraday volatility dynamics with price durations},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards identification of shocks in linear state-space models: Application to stochastic volatility model. <em>JTSA</em>, <em>46</em>(6), 1205-1223. (<a href='https://doi.org/10.1111/jtsa.70012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-space models are widely used by statisticians because they allow a useful interpretation of some components of interest. Their efficient estimation, as well as the computation of forecasts or nonlinear functions of the observables, depends crucially on the correct specification of the error terms. Gaussianity is a common assumption explicitly used in the Kalman filter recursion, but departing from Gaussianity is of particular interest in fields such as finance, where there is a need for leptokurtic and/or asymmetric distributions to capture some features of the data. We introduce an approach based on the characteristic function or Laplace transform of the observed process and show that, for a large class of state-space models (with finite second-order moments and non-zero higher-order cumulants), it is possible to recover the cumulants of the structural shocks and the measurement errors from the cumulants and cross-cumulants of the observed process and the first-order parameters. This allows the statistician to design specification tests related to the properties of the structural shocks or measurement errors, separately or jointly, or of the data generating process (DGP) of the observed time series. In a non-Gaussian framework, we design a test for the property that the DGP of the observed time series is a state-space model with different shocks versus an ARMA DGP with only a single innovation process, but with the same second-order properties. We illustrate the size and power properties of this test applied to a simple stochastic volatility model.},
  archive      = {J_JTSA},
  author       = {Stéphane Gregoir and Nour Meddahi},
  doi          = {10.1111/jtsa.70012},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1205-1223},
  shortjournal = {J. Time Series Anal.},
  title        = {Towards identification of shocks in linear state-space models: Application to stochastic volatility model},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing in GARCH-X models: Boundary, correlations and bootstrap theory. <em>JTSA</em>, <em>46</em>(6), 1175-1204. (<a href='https://doi.org/10.1111/jtsa.12767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the so-called Fixed Shrinkage (FS) bootstrap for the class of GARCH models with explanatory variables (GARCH-X). Under the assumption of stationary covariates, the proposed FS bootstrap does not require modeling the covariates, as these are kept fixed in the bootstrap generating process. Our main focus is on testing whether one or more of the covariates can be excluded in the GARCH-X model. As is well-known the limiting distribution of the likelihood-ratio (LR) statistic in this setting is non-standard and depends in particular on whether nuisance parameters are on the boundary or in the interior of the parameter space. In particular, and as detailed here, the non-standard limiting distribution depends on correlations, or dependence, between the explanatory variables. The FS bootstrap takes the presence of nuisance parameters into account by implementing shrinking as proposed in Cavaliere et al . (2022) for pure ARCH models. We establish asymptotic validity of the FS bootstrap for GARCH-X models, and demonstrate by simulations that the bootstrap-based test performs extremely well even when nuisance parameters lie on the boundary of the parameter space. The empirical illustration amplifies that the presence of nuisance parameters (especially whether or not on the boundary) are vital for interpreting the dynamics of conditional volatility in financial stock market indices.},
  archive      = {J_JTSA},
  author       = {Heino Bohn Nielsen and Rasmus Søndergaard Pedersen and Anders Rahbek and Sigurd Nellemann Thorsen},
  doi          = {10.1111/jtsa.12767},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1175-1204},
  shortjournal = {J. Time Series Anal.},
  title        = {Testing in GARCH-X models: Boundary, correlations and bootstrap theory},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fractional gaussian noise: Spectral density and estimation methods. <em>JTSA</em>, <em>46</em>(6), 1146-1174. (<a href='https://doi.org/10.1111/jtsa.12750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fractional Brownian motion (fBm) process, governed by a fractional parameter H ∈ ( 0 , 1 ) $$ H\in \left(0,1\right) $$ , is a continuous-time Gaussian process with its increment being the fractional Gaussian noise (fGn). This article first provides a computationally feasible expression for the spectral density of fGn. This expression enables us to assess the accuracy of a range of approximation methods, including the truncation method, Paxson's approximation, and the Taylor series expansion at the near-zero frequency. Next, we conduct an extensive Monte Carlo study comparing the finite sample performance and computational cost of alternative estimation methods for H $$ H $$ under the fGn specification. These methods include two semi-parametric methods (based on the Taylor series expansion), two versions of the Whittle method (utilising either the computationally feasible expression or Paxson's approximation of the spectral density), a time-domain maximum likelihood (ML) method (employing a recursive approach for its likelihood calculation), and a change-of-frequency method. Special attention is paid to highly anti-persistent processes with H $$ H $$ close to zero, which are of empirical relevance to financial volatility modelling. Considering the trade-off between statistical and computational efficiency, we recommend using either the Whittle ML method based on Paxson's approximation or the time-domain ML method. We model the log realized volatility dynamics of 40 financial assets in the US market from 2012 to 2019 with fBm. Although all estimation methods suggest rough volatility, the implied degree of roughness varies substantially with the estimation methods, highlighting the importance of understanding the finite sample performance of various estimation methods.},
  archive      = {J_JTSA},
  author       = {Shuping Shi and Jun Yu and Chen Zhang},
  doi          = {10.1111/jtsa.12750},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1146-1174},
  shortjournal = {J. Time Series Anal.},
  title        = {Fractional gaussian noise: Spectral density and estimation methods},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding multi-horizon forecasts: Identification, estimation and testing. <em>JTSA</em>, <em>46</em>(6), 1125-1145. (<a href='https://doi.org/10.1111/jtsa.12797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The properties of multi-horizon forecasts are investigated by specifying and estimating a model nesting rational forecasts, as well as deviations from rationality resulting from horizon-specific biases and idiosyncratic errors. Identification of the model's parameters is achieved by combining information on forecasts for a given point in time with different information sets, and forecasts at different points in time based on the same information set. Both sets of forecasts are used to estimate the model's parameters by generalised method of moments. A test of rationality is proposed with the twin advantages of circumventing parameter identification and boundary issues. The finite sample properties of the GMM estimator and the rationality test are also investigated. Applying the approach to US GDP growth forecasts of the Survey of Professional Forecasters, the empirical results show that forecasts deviate from rationality over all forecast horizons, with the strength of the deviations from rationality increasing with the forecast horizon.},
  archive      = {J_JTSA},
  author       = {Stan Hurn and Vance Martin and Jing Tian and Lina Xu},
  doi          = {10.1111/jtsa.12797},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1125-1145},
  shortjournal = {J. Time Series Anal.},
  title        = {Understanding multi-horizon forecasts: Identification, estimation and testing},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymmetric stable stochastic volatility models: Estimation, filtering, and forecasting. <em>JTSA</em>, <em>46</em>(6), 1098-1124. (<a href='https://doi.org/10.1111/jtsa.12780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers a stochastic volatility model featuring an asymmetric stable error distribution and a novel way of accounting for the leverage effect. We adopt simulation-based methods to address key challenges in parameter estimation, the filtering of time-varying volatility, and volatility forecasting. More specifically, we make use of the indirect inference method for estimating the static parameters, while the latent volatility is extracted using the extremum Monte Carlo method. Both parameter estimation and volatility extraction are easily adapted to other model specifications, such as those based on other error distributions or on other dynamic processes for volatility. Illustrations are presented for a simulated dataset and for an empirical dataset of daily Bitcoin returns.},
  archive      = {J_JTSA},
  author       = {Francisco Blasques and Siem Jan Koopman and Karim Moussa},
  doi          = {10.1111/jtsa.12780},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1098-1124},
  shortjournal = {J. Time Series Anal.},
  title        = {Asymmetric stable stochastic volatility models: Estimation, filtering, and forecasting},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-day options. <em>JTSA</em>, <em>46</em>(6), 1085-1097. (<a href='https://doi.org/10.1111/jtsa.12819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers new options on Treasury futures than expire each Wednesday and Friday. I examine the variances implied by these options as of the night before expiration, and compare the variances just before FOMC days and employment report days with the variances on other Tuesdays or Thursdays, respectively. This can be used to measure the risk-neutral interest rate uncertainty associated with FOMC announcements and employment reports. I can also compare the average physical and risk-neutral uncertainty. Lastly, I construct options-implied densities on the eve of FOMC and employment report days.},
  archive      = {J_JTSA},
  author       = {Jonathan H. Wright},
  doi          = {10.1111/jtsa.12819},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1085-1097},
  shortjournal = {J. Time Series Anal.},
  title        = {Event-day options},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and inference for higher-order stochastic volatility models with leverage. <em>JTSA</em>, <em>46</em>(6), 1064-1084. (<a href='https://doi.org/10.1111/jtsa.12851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical inference—estimation and testing—for stochastic volatility models is challenging and computationally expensive. This problem is compounded when leverage effects are allowed. We propose efficient, simple estimators for higher-order stochastic volatility models with leverage [SVL ( p ) $$ (p) $$ ], based on a small number of moment equations derived from ARMA representations associated with SVL models, along with the possibility of using “winsorization” to improve stability and efficiency (W-ARMA estimators). The asymptotic distributional theory of the estimators is derived. The computationally simple estimators proposed allow one to easily perform simulation-based (possibly exact) tests, such as Monte Carlo tests (MCTS) or bootstrap procedures. In simulation experiments, we show that: (1) the proposed W-ARMA estimators dominate alternative estimators (including Bayesian estimators) in terms of bias, root-mean-square error, and computation time; (2) local and maximized Monte Carlo tests based on W-ARMA estimators yield good control of the size and power of LR-type tests; (3) taking into account leverage improves volatility forecasting. The methods developed are applied to daily returns for three major stock indices (S&P 500, Dow Jones, Nasdaq), confirming the superiority of SVL ( p ) $$ (p) $$ models over competing conditional volatility models in terms of forecast accuracy.},
  archive      = {J_JTSA},
  author       = {Md. Nazmul Ahsan and Jean-Marie Dufour and Gabriel Rodriguez-Rondon},
  doi          = {10.1111/jtsa.12851},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1064-1084},
  shortjournal = {J. Time Series Anal.},
  title        = {Estimation and inference for higher-order stochastic volatility models with leverage},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S&P 500 microstructure noise components: Empirical inferences from futures and ETF prices. <em>JTSA</em>, <em>46</em>(6), 1032-1063. (<a href='https://doi.org/10.1111/jtsa.12786'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By studying the differences between futures prices and exchange-traded fund prices for the S&P 500 index, original results are obtained about the distribution and persistence of the microstructure noise component created by positive bid-ask spreads and discrete price scales. The bivariate density of this component for futures and exchange-traded fund prices is estimated from high-frequency prices, to provide estimates of the marginal noise densities and measures of noise dependence across the markets studied. Properties of the residual microstructure noise, created by factors other than discrete prices, are also estimated. The residual component has more variation and less persistence than the discrete-price component during the period examined, from January 2010 to December 2012.},
  archive      = {J_JTSA},
  author       = {Stephen J. Taylor},
  doi          = {10.1111/jtsa.12786},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1032-1063},
  shortjournal = {J. Time Series Anal.},
  title        = {S&P 500 microstructure noise components: Empirical inferences from futures and ETF prices},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue in honour of stephen j. taylor: Guest editors' introduction. <em>JTSA</em>, <em>46</em>(6), 1029-1031. (<a href='https://doi.org/10.1111/jtsa.70014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JTSA},
  author       = {Torben G. Andersen and Kim Christensen and Ingmar Nolte},
  doi          = {10.1111/jtsa.70014},
  journal      = {Journal of Time Series Analysis},
  month        = {11},
  number       = {6},
  pages        = {1029-1031},
  shortjournal = {J. Time Series Anal.},
  title        = {Special issue in honour of stephen j. taylor: Guest editors' introduction},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local powers of least-squares-based test for panel fractional Ornstein–Uhlenbeck process. <em>JTSA</em>, <em>46</em>(5), 997-1023. (<a href='https://doi.org/10.1111/jtsa.12777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant advancements have been made in the field of identifying financial asset price bubbles, particularly through the development of time-series unit-root tests featuring fractionally integrated errors and panel unit-root tests. This study introduces an innovative approach for assessing the sign of the persistence parameter ( α ) within a panel fractional Ornstein-Uhlenbeck process, based on the least squares estimator of α . This method incorporates three distinct test statistics based on the Hurst parameter ( H ), which can take values in the range of ( 1 / 2 , 1 ) , be equal to 1 / 2 , or fall within the interval of ( 0 , 1 / 2 ) . The null hypothesis corresponds to α = 0 . Based on a panel of continuous records of observations, the null asymptotic distributions are obtained when the time span ( T ) is fixed and the number of cross sections ( N ) goes to infinity. The power function of the tests is obtained under the local alternative where α is close to zero in the order of 1 / ( T ⁢ N ) . This alternative covers the departure from the unit root hypothesis from the explosive side, enabling the calculation of lower power in bubble tests. The hypothesis testing problem and the local power function are also considered when a panel of discrete-sampled observations is available under a sequential limit, that is, the sampling interval shrinks to zero followed by the N goes to infinity.},
  archive      = {J_JTSA},
  author       = {Katsuto Tanaka and Weilin Xiao and Jun Yu},
  doi          = {10.1111/jtsa.12777},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {997-1023},
  shortjournal = {J. Time Series Anal.},
  title        = {Local powers of least-squares-based test for panel fractional Ornstein–Uhlenbeck process},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential monitoring for changes in GARCH(1,1) models without assuming stationarity. <em>JTSA</em>, <em>46</em>(5), 981-996. (<a href='https://doi.org/10.1111/jtsa.12824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop two families of sequential monitoring procedure to (timely) detect changes in the parameters of a GARCH(1,1) model. Our statistics can be applied irrespective of whether the historical sample is stationary or not, and indeed without previous knowledge of the regime of the observations before and after the break. In particular, we construct our detectors as the CUSUM process of the quasi-Fisher scores of the log likelihood function. To ensure timely detection, we then construct our boundary function (exceeding which would indicate a break) by including a weighting sequence which is designed to shorten the detection delay in the presence of a changepoint. We consider two types of weights: a lighter set of weights, which ensures timely detection in the presence of changes occurring “early, but not too early” after the end of the historical sample; and a heavier set of weights, called “Rényi weights” which is designed to ensure timely detection in the presence of changepoints occurring very early in the monitoring horizon. In both cases, we derive the limiting distribution of the detection delays, indicating the expected delay for each set of weights. Our methodologies can be applied for a general analysis of changepoints in GARCH(1,1) sequences; however, they can also be applied to detect changes from stationarity to explosivity or vice versa, thus allowing to check for “volatility bubbles”, upon applying tests for stationarity before and after the identified break. Our theoretical results are validated via a comprehensive set of simulations, and an empirical application to daily returns of individual stocks.},
  archive      = {J_JTSA},
  author       = {Lajos Horváth and Lorenzo Trapani and Shixuan Wang},
  doi          = {10.1111/jtsa.12824},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {981-996},
  shortjournal = {J. Time Series Anal.},
  title        = {Sequential monitoring for changes in GARCH(1,1) models without assuming stationarity},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel test for the presence of local explosive dynamics. <em>JTSA</em>, <em>46</em>(5), 966-980. (<a href='https://doi.org/10.1111/jtsa.70001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In economics and finance, speculative bubbles take the form of locally explosive dynamics that eventually collapse. We propose a test for the presence of speculative bubbles in the context of mixed causal-noncausal autoregressive processes. The test exploits the fact that bubbles are anticipative, that is, they are generated by an extreme shock in the forward-looking dynamics. In particular, the test uses both path-level deviations and growth rates to assess the presence of a bubble of a given duration and size, at any moment in time. We show that the distribution of the test statistic can be either analytically determined or numerically approximated, depending on the error distribution. Size and power properties of the test are analyzed in controlled Monte Carlo experiments. An empirical application is presented for a monthly oil price index. It demonstrates the ability of the test to detect bubbles and to provide valuable insights in terms of risk assessments in the spirit of Value-at-Risk.},
  archive      = {J_JTSA},
  author       = {F. Blasques and S. J. Koopman and G. Mingoli and S. Telg},
  doi          = {10.1111/jtsa.70001},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {966-980},
  shortjournal = {J. Time Series Anal.},
  title        = {A novel test for the presence of local explosive dynamics},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for a bubble with a stochastically varying explosive coefficient. <em>JTSA</em>, <em>46</em>(5), 945-965. (<a href='https://doi.org/10.1111/jtsa.12768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we test for a bubble in a model with a random explosive autoregressive coefficient. We consider two local alternatives and find that versions of recursive stochastic unit root tests are more powerful when facing a randomly explosive process than the recursive right-tailed ADF tests, whereas the latter performs better in a model with a non-stochastic coefficient. We then propose the union of rejections strategy using the recursive right-tailed ADF and stochastic unit root tests. We examine the finite sample properties of the proposed tests using Monte Carlo simulations and observe that the test based on the union of rejections strategy is the second-best, and its power is close to the best one in most cases.},
  archive      = {J_JTSA},
  author       = {Eiji Kurozumi and Mikihito Nishi},
  doi          = {10.1111/jtsa.12768},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {945-965},
  shortjournal = {J. Time Series Anal.},
  title        = {Testing for a bubble with a stochastically varying explosive coefficient},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stochastic tree for bubble asset modelling and pricing. <em>JTSA</em>, <em>46</em>(5), 932-944. (<a href='https://doi.org/10.1111/jtsa.12801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new stochastic tree representation of a strictly stationary submartingale process for modelling, forecasting, and pricing speculative bubbles on commodity and cryptocurrency markets. The model is compared to other trees proposed in the literature on bubble asset modelling and stochastic volatility approximation. We show that the proposed model is an extension of the well-known Blanchard-Watson bubble. The model provides (quasi) closed-form pricing formulas for European options, which are derived and illustrated.},
  archive      = {J_JTSA},
  author       = {Christian Gourieroux and Joann Jasiak},
  doi          = {10.1111/jtsa.12801},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {932-944},
  shortjournal = {J. Time Series Anal.},
  title        = {A stochastic tree for bubble asset modelling and pricing},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile analysis for financial bubble detection and surveillance. <em>JTSA</em>, <em>46</em>(5), 908-931. (<a href='https://doi.org/10.1111/jtsa.12791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and monitoring financial bubbles is critical, as they can lead to market instability, asset price crashes, and economic downturns with widespread consequences. This article explores the usefulness of quantile regression (QR) technique in detecting and surveilling financial bubbles, encompassing both global testing and real-time monitoring. We demonstrate that the QR-based quantile unit root test, coupled with an optimal quantile selection technique, serves as an effective tool for a global bubble test without necessitating additional recursive techniques. Moreover, we propose two QR-based bubble monitoring techniques. We show that the monitoring statistics follow a random variate under the null hypothesis of no bubbles but diverge to positive infinity in the presence of a mildly explosive bubble, and hence consistently date the origination of a bubble. Monte Carlo simulations suggest that compared with their LS counterparts, in the presence of skewed distributions, the QR-based global test delivers substantially greater power, while the QR-based monitoring procedures offer higher bubble detection rate and more accurate dating of the bubble origination. As an illustration, we conduct a pseudo real-time monitoring exercise with the S&P 500 composite index.},
  archive      = {J_JTSA},
  author       = {Ruike Wu and Shuping Shi and Jilin Wu},
  doi          = {10.1111/jtsa.12791},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {908-931},
  shortjournal = {J. Time Series Anal.},
  title        = {Quantile analysis for financial bubble detection and surveillance},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bubbles and crashes: A tale of quantiles. <em>JTSA</em>, <em>46</em>(5), 884-907. (<a href='https://doi.org/10.1111/jtsa.12794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periodically collapsing bubbles, if they exist, induce asymmetric dynamics in asset prices. In this article, I show that unit root quantile autoregressive models can approximate such dynamics by allowing the largest autoregressive root to take values below unity at low quantiles, which correspond to price crashes, and above unity at upper quantiles, that correspond to bubble expansions. On this basis, I employ two unit root tests based on quantile autoregressions to detect bubbles. Monte Carlo simulations suggest that the two tests have good size and power properties, and can outperform recursive least-squares-based tests. The merits of the two tests are further illustrated in three empirical applications that examine Bitcoin, US equity and US housing markets. In the empirical applications, special attention is given to the issue of controlling for economic fundamentals. The estimation results indicate the presence of asymmetric dynamics that closely match those of the simulated bubble processes.},
  archive      = {J_JTSA},
  author       = {Efthymios G. Pavlidis},
  doi          = {10.1111/jtsa.12794},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {884-907},
  shortjournal = {J. Time Series Anal.},
  title        = {Bubbles and crashes: A tale of quantiles},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved procedure for retrospectively dating the emergence and collapse of bubbles. <em>JTSA</em>, <em>46</em>(5), 867-883. (<a href='https://doi.org/10.1111/jtsa.12810'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new ordinary least squares (OLS)-based procedure for retrospectively dating the emergence and collapse of bubbles. We first consider a data generating process that entails a switch from a unit root regime to an explosive regime followed by a collapse and subsequent return to unit root behavior. We demonstrate analytically that the standard OLS estimates are inconsistent and date both the origination and implosion points with a delay in large samples. A simple modification that involves omitting the residual corresponding to the implosion date is shown to yield consistent estimates. We also develop an efficient dating algorithm that can accommodate a framework with multiple bubbles. The algorithm exploits the explicit form of the unit root restrictions to directly embed them into the recursive optimization problem which obviates the need to rely on an iterative scheme that requires initial values. Extensive simulation experiments indicate that our proposed procedure typically delivers estimates with lower bias and root mean squared error relative to competing alternatives. An empirical illustration is included.},
  archive      = {J_JTSA},
  author       = {Mohitosh Kejriwal and Linh Nguyen and Pierre Perron},
  doi          = {10.1111/jtsa.12810},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {867-883},
  shortjournal = {J. Time Series Anal.},
  title        = {An improved procedure for retrospectively dating the emergence and collapse of bubbles},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new heteroskedasticity-robust test for explosive bubbles. <em>JTSA</em>, <em>46</em>(5), 846-866. (<a href='https://doi.org/10.1111/jtsa.12784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new class of modified regression-based tests for detecting asset price bubbles designed to be robust to the presence of general forms of both conditional and unconditional heteroskedasticity in the price series. This modification, based on the approach developed in Beare (2018) in the context of conventional unit root testing, is achieved by purging the impact of unconditional heteroskedasticity from the data using a kernel estimate of volatility before the application of the bubble detection methods proposed in Phillips, Shi and Yu (2015) (PSY). The modified statistic is shown to achieve the same limiting null distribution as the corresponding (heteroskedasticity-uncorrected) statistic from PSY would obtain under homoskedasticity, such that the usual critical values provided in PSY may still be used. Versions of the test based on regressions including either no intercept or a (redundant) intercept are considered. Representations for asymptotic local power against a single bubble model are also derived. Monte Carlo simulation results highlight that neither one of these tests dominates the other across different bubble locations and magnitudes, and across different models of time-varying volatility. Accordingly, we also propose a test based on a union of rejections between the with- and without-intercept variants of the modified PSY test. The union procedure is shown to perform almost as well as the better of the constituent tests for a given DGP, and also performs very well compared to existing heteroskedasticity-robust tests across a large range of simulation DGPs.},
  archive      = {J_JTSA},
  author       = {David I. Harvey and Stephen J. Leybourne and A. M. Robert Taylor and Yang Zu},
  doi          = {10.1111/jtsa.12784},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {846-866},
  shortjournal = {J. Time Series Anal.},
  title        = {A new heteroskedasticity-robust test for explosive bubbles},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential detector statistics for speculative bubbles. <em>JTSA</em>, <em>46</em>(5), 829-845. (<a href='https://doi.org/10.1111/jtsa.12845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a heteroskedasticity-robust locally best invariant (LBI) statistic to test the hypothesis of a unit root against the alternative of an explosive root associated with speculative bubbles. Compared to existing alternatives such as Dickey-Fuller type tests, the LBI statistic has a standard limiting distribution and greater power, particularly in the empirically relevant scenario of a moderately explosive root. Further refinements, such as the point-optimal linear test, approach the power envelope remarkably closely. To detect bubbles with an unknown starting date, we consider sequential (CUSUM) schemes based on constant and time-varying boundary functions, where the exponentially weighted CUSUM detector with a constant boundary function turns out to be most powerful. We also propose a simple method for date-stamping the start of the bubble consistently. Finally, we illustrate our methods using two empirical examples.},
  archive      = {J_JTSA},
  author       = {Jörg Breitung and Max Diegel},
  doi          = {10.1111/jtsa.12845},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {829-845},
  shortjournal = {J. Time Series Anal.},
  title        = {Sequential detector statistics for speculative bubbles},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speculative bubbles in the recent AI boom: Nasdaq and the magnificent seven. <em>JTSA</em>, <em>46</em>(5), 814-828. (<a href='https://doi.org/10.1111/jtsa.12835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent artificial intelligence (AI) boom covers a period of rapid innovation and wide adoption of AI intelligence technologies across diverse industries. These developments have fueled an unprecedented frenzy in the Nasdaq, with AI-focused companies experiencing soaring stock prices that raise concerns about speculative bubbles and real-economy consequences. Against this background, this study investigates the formation of speculative bubbles in the Nasdaq stock market with a specific focus on the so-called Magnificent Seven (Mag-7) individual stocks during the AI boom, spanning the period from January 2017 to January 2025. We apply the real-time PSY bubble detection methodology of Phillips et al. (2015a, 2015b) while controlling for market and industry factors for individual stocks. Confidence intervals to assess the degree of speculative behavior in asset price dynamics are calculated using the near-unit root approach of Phillips (2023). The findings reveal the presence of speculative bubbles in the Nasdaq stock market and across all Mag-7 stocks. Nvidia and Microsoft experience the longest speculative periods over January 2017–December 2021, while Nvidia and Tesla show the fastest rates of explosive behavior. Speculative bubbles persist in the market and in six of the seven stocks (excluding Apple) from December 2022 to January 2025. Near-unit-root inference indicates mildly explosive dynamics for Nvidia and Tesla (2017–2021) and local-to-unity near explosive behavior for all assets in both periods.},
  archive      = {J_JTSA},
  author       = {Rerotlhe B. Basele and Peter C. B. Phillips and Shuping Shi},
  doi          = {10.1111/jtsa.12835},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {814-828},
  shortjournal = {J. Time Series Anal.},
  title        = {Speculative bubbles in the recent AI boom: Nasdaq and the magnificent seven},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent developments in time-series methods for detecting bubbles and crashes: Guest editors' introduction. <em>JTSA</em>, <em>46</em>(5), 811-813. (<a href='https://doi.org/10.1111/jtsa.70003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JTSA},
  author       = {David I. Harvey and Stephen J. Leybourne},
  doi          = {10.1111/jtsa.70003},
  journal      = {Journal of Time Series Analysis},
  month        = {9},
  number       = {5},
  pages        = {811-813},
  shortjournal = {J. Time Series Anal.},
  title        = {Recent developments in time-series methods for detecting bubbles and crashes: Guest editors' introduction},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A note on johansen's rank conditions and the jordan form of a matrix. <em>JTSA</em>, <em>46</em>(4), 796-805. (<a href='https://doi.org/10.1111/jtsa.12789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note presents insights on the Jordan structure of a matrix which are derived from an extension of the I ⁡ ( 1 ) and I ⁡ ( 2 ) conditions in Johansen (1996). It is first observed that these conditions not only characterize, as it is well known, the size (1 or 2) of the largest Jordan block in the Jordan form of the companion matrix but more generally the geometric multiplicities, the algebraic multiplicities and the whole Jordan structure for eigenvalues of index 1 or 2. In the context of the Granger representation theorem, this means that the Johansen rank conditions do more than determine the order of integration of the process. It is then shown that an extension of these conditions leads to the characterization of the Jordan structure of any matrix.},
  archive      = {J_JTSA},
  author       = {Massimo Franchi},
  doi          = {10.1111/jtsa.12789},
  journal      = {Journal of Time Series Analysis},
  month        = {7},
  number       = {4},
  pages        = {796-805},
  shortjournal = {J. Time Series Anal.},
  title        = {A note on johansen's rank conditions and the jordan form of a matrix},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact likelihood for inverse gamma stochastic volatility models. <em>JTSA</em>, <em>46</em>(4), 774-795. (<a href='https://doi.org/10.1111/jtsa.12795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We obtain a novel analytic expression of the likelihood for a stationary inverse gamma stochastic volatility (SV) model. This allows us to obtain the maximum likelihood estimator for this nonlinear non-Gaussian state space model. Further, we obtain both the filtering and smoothing distributions for the inverse volatilities as mixtures of gammas, and therefore, we can provide the smoothed estimates of the volatility. We show that by integrating out the volatilities the model that we obtain has the resemblance of a GARCH in the sense that the formulas are similar, which simplifies computations significantly. The model allows for fat tails in the observed data. We provide empirical applications using exchange rates data for seven currencies and quarterly inflation data for four countries. We find that the empirical fit of our proposed model is overall better than alternative models for four countries currency data and for two countries inflation data.},
  archive      = {J_JTSA},
  author       = {Roberto Leon-Gonzalez and Blessings Majoni},
  doi          = {10.1111/jtsa.12795},
  journal      = {Journal of Time Series Analysis},
  month        = {7},
  number       = {4},
  pages        = {774-795},
  shortjournal = {J. Time Series Anal.},
  title        = {Exact likelihood for inverse gamma stochastic volatility models},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modal volatility function. <em>JTSA</em>, <em>46</em>(4), 748-773. (<a href='https://doi.org/10.1111/jtsa.12790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We in this article propose a novel non-parametric estimator for the volatility function within a broad context that encompasses nonlinear time series models as a special case. The new estimator, built on the mode value, is designed to complement existing mean volatility measures to reveal distinct data features. We demonstrate that the suggested modal volatility estimator can be obtained asymptotically as well as if the conditional mean regression function were known, assuming observations are from a strictly stationary and absolutely regular process. Under mild regularity conditions, we establish that the asymptotic distributions of the resulting estimator align with those derived from independent observations, albeit with a slower convergence rate compared to non-parametric mean regression. The theory and practice of bandwidth selection are discussed. Moreover, we put forward a variance reduction technique for the modal volatility estimator to attain asymptotic relative efficiency while maintaining the asymptotic bias unchanged. We numerically solve the modal regression model with the use of a modified modal-expectation-maximization algorithm. Monte Carlo simulations are conducted to assess the finite sample performance of the developed estimation procedure. Two real data analyses are presented to further illustrate the newly proposed model in practical applications. To potentially enhance the accuracy of the bias term, we in the end discuss the extension of the method to local exponential modal estimation. We showcase that the suggested exponential modal volatility estimator shares the same asymptotic variance as the non-parametric modal volatility estimator but may exhibit a smaller bias.},
  archive      = {J_JTSA},
  author       = {Aman Ullah and Tao Wang},
  doi          = {10.1111/jtsa.12790},
  journal      = {Journal of Time Series Analysis},
  month        = {7},
  number       = {4},
  pages        = {748-773},
  shortjournal = {J. Time Series Anal.},
  title        = {Modal volatility function},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation for conditional moment models based on martingale difference divergence. <em>JTSA</em>, <em>46</em>(4), 727-747. (<a href='https://doi.org/10.1111/jtsa.12788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a new estimation method for conditional moment models via the martingale difference divergence (MDD). Our MDD-based estimation method is formed in the framework of a continuum of unconditional moment restrictions. Unlike the existing estimation methods in this framework, the MDD-based estimation method adopts a non-integrable weighting function, which could capture more information from unconditional moment restrictions than the integrable weighting function to enhance the estimation efficiency. Due to the nature of shift-invariance in MDD, our MDD-based estimation method can not identify the intercept parameters. To overcome this identification issue, we further provide a two-step estimation procedure for the model with intercept parameters. Under regularity conditions, we establish the asymptotics of the proposed estimators, which are not only easy-to-implement with expectation-based asymptotic variances, but also applicable to time series data with an unspecified form of conditional heteroskedasticity. Finally, we illustrate the usefulness of the proposed estimators by simulations and two real examples.},
  archive      = {J_JTSA},
  author       = {Kunyang Song and Feiyu Jiang and Ke Zhu},
  doi          = {10.1111/jtsa.12788},
  journal      = {Journal of Time Series Analysis},
  month        = {7},
  number       = {4},
  pages        = {727-747},
  shortjournal = {J. Time Series Anal.},
  title        = {Estimation for conditional moment models based on martingale difference divergence},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed orthogonality graphs for continuous-time state space models and orthogonal projections. <em>JTSA</em>, <em>46</em>(4), 692-726. (<a href='https://doi.org/10.1111/jtsa.12787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we derive (local) orthogonality graphs for the popular continuous-time state space models, including in particular multivariate continuous-time ARMA (MCARMA) processes. In these (local) orthogonality graphs, vertices represent the components of the process, directed edges between the vertices indicate causal influences and undirected edges indicate contemporaneous correlations between the component processes. We present sufficient criteria for state space models to satisfy the assumptions of Fasen-Hartmann and Schenk (2024a) so that the (local) orthogonality graphs are well-defined and various Markov properties hold. Both directed and undirected edges in these graphs are characterised by orthogonal projections on well-defined linear spaces. To compute these orthogonal projections, we use the unique controller canonical form of a state space model, which exists under mild assumptions, to recover the input process from the output process. We are then able to derive some alternative representations of the output process and its highest derivative. Finally, we apply these representations to calculate the necessary orthogonal projections, which culminate in the characterisations of the edges in the (local) orthogonality graph. These characterisations are given by the parameters of the controller canonical form and the covariance matrix of the driving Lévy process.},
  archive      = {J_JTSA},
  author       = {Vicky Fasen-Hartmann and Lea Schenk},
  doi          = {10.1111/jtsa.12787},
  journal      = {Journal of Time Series Analysis},
  month        = {7},
  number       = {4},
  pages        = {692-726},
  shortjournal = {J. Time Series Anal.},
  title        = {Mixed orthogonality graphs for continuous-time state space models and orthogonal projections},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local quadratic spectral and covariance matrix estimation. <em>JTSA</em>, <em>46</em>(4), 674-691. (<a href='https://doi.org/10.1111/jtsa.12783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of estimating the spectral density matrix f ⁡ ( w ) of a multi-variate time series is revisited with special focus on the frequencies w = 0 and w = π . Recognizing that the entries of the spectral density matrix at these two boundary points are real-valued, we propose a new estimator constructed from a local polynomial regression of the real portion of the multi-variate periodogram. The case w = 0 is of particular importance, since f ⁡ ( 0 ) is associated with the large-sample covariance matrix of the sample mean; hence, estimating f ⁡ ( 0 ) is crucial to conduct any sort of statistical inference on the mean. We explore the properties of the local polynomial estimator through theory and simulations, and discuss an application to inflation and unemployment.},
  archive      = {J_JTSA},
  author       = {Tucker McElroy and Dimitris N. Politis},
  doi          = {10.1111/jtsa.12783},
  journal      = {Journal of Time Series Analysis},
  month        = {7},
  number       = {4},
  pages        = {674-691},
  shortjournal = {J. Time Series Anal.},
  title        = {Local quadratic spectral and covariance matrix estimation},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local whittle estimation in time-varying long memory series. <em>JTSA</em>, <em>46</em>(4), 647-673. (<a href='https://doi.org/10.1111/jtsa.12782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The memory parameter is usually assumed to be constant in traditional long memory time series. We relax this restriction by considering the memory a time-varying function that depends on a finite number of parameters. A time-varying Local Whittle estimator of these parameters, and hence of the memory function, is proposed. Its consistency and asymptotic normality are shown for locally stationary and locally non-stationary long memory processes, where the spectral behaviour is restricted only at frequencies close to the origin. Its good finite sample performance is shown in a Monte Carlo exercise and in two empirical applications, highlighting its benefits over the fully parametric Whittle estimator proposed by Palma and Olea (2010). Standard inference techniques for the constancy of the memory are also proposed based on this estimator.},
  archive      = {J_JTSA},
  author       = {Josu Arteche and Luis F. Martins},
  doi          = {10.1111/jtsa.12782},
  journal      = {Journal of Time Series Analysis},
  month        = {7},
  number       = {4},
  pages        = {647-673},
  shortjournal = {J. Time Series Anal.},
  title        = {Local whittle estimation in time-varying long memory series},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous inference of a partially linear model in time series. <em>JTSA</em>, <em>46</em>(4), 623-646. (<a href='https://doi.org/10.1111/jtsa.12781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new methodology to conduct simultaneous inference of the non-parametric component in partially linear time series regression models where the non-parametric part is a multi-variate unknown function. In particular, we construct a simultaneous confidence region (SCR) for the multi-variate function by extending the high-dimensional Gaussian approximation to dependent processes with continuous index sets. Our results allow for a more general dependence structure compared to previous works and are widely applicable to a variety of linear and non-linear autoregressive processes. We demonstrate the validity of our proposed methodology by examining the finite-sample performance in the simulation study. Finally, an application in time series, the forward premium regression, is presented, where we construct the SCR for the foreign exchange risk premium from the exchange rate and macroeconomic data.},
  archive      = {J_JTSA},
  author       = {Jiaqi Li and Likai Chen and Kun Ho Kim and Tianwei Zhou},
  doi          = {10.1111/jtsa.12781},
  journal      = {Journal of Time Series Analysis},
  month        = {7},
  number       = {4},
  pages        = {623-646},
  shortjournal = {J. Time Series Anal.},
  title        = {Simultaneous inference of a partially linear model in time series},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On buffered moving average models. <em>JTSA</em>, <em>46</em>(4), 599-622. (<a href='https://doi.org/10.1111/jtsa.12778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been growing interest in extending the popular threshold time series models to include a buffer zone for regime transition. However, almost all attention has been on buffered autoregressive models. Note that the classical moving average (MA) model plays an equally important role as the autoregressive model in classical time series analysis. It is therefore natural to extend our investigation to the buffered MA (BMA) model. We focus on the first-order BMA model while extending to more general MA model should be direct in principle. The proposed model shares the piecewise linear structure of the threshold model, but has a more flexible regime switching mechanism. Its probabilistic structure is studied to some extent. A nonlinear least squares estimation procedure is proposed. Under some standard regularity conditions, the estimator is strongly consistent and the estimator of the coefficients is asymptotically normal when the parameter of the boundary of the buffer zone is known. A portmanteau goodness-of-fit test is derived. Simulation results and empirical examples are carried out and lend further support to the usefulness of the BMA model and the asymptotic results.},
  archive      = {J_JTSA},
  author       = {Yipeng Zhuang and Dong Li and Philip L. H. Yu and Wai Keung Li},
  doi          = {10.1111/jtsa.12778},
  journal      = {Journal of Time Series Analysis},
  month        = {7},
  number       = {4},
  pages        = {599-622},
  shortjournal = {J. Time Series Anal.},
  title        = {On buffered moving average models},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating lagged (cross-)covariance operators of lp-m-approximable processes in cartesian product hilbert spaces. <em>JTSA</em>, <em>46</em>(3), 582-595. (<a href='https://doi.org/10.1111/jtsa.12772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating parameters of functional ARMA, GARCH and invertible processes requires estimating lagged covariance and cross-covariance operators of Cartesian product Hilbert space-valued processes. Asymptotic results have been derived in recent years, either less generally or under a strict condition. This article derives upper bounds of the estimation errors for such operators based on the mild condition - -approximability for each lag, Cartesian power(s) and sample size, where the two processes can take values in different spaces in the context of lagged cross-covariance operators. Implications of our results on eigen elements and parameters in functional AR(MA) models are also discussed.},
  archive      = {J_JTSA},
  author       = {Sebastian Kühnert},
  doi          = {10.1111/jtsa.12772},
  journal      = {Journal of Time Series Analysis},
  month        = {5},
  number       = {3},
  pages        = {582-595},
  shortjournal = {J. Time Series Anal.},
  title        = {Estimating lagged (cross-)covariance operators of lp-m-approximable processes in cartesian product hilbert spaces},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixing properties of non-stationary multi-variate count processes. <em>JTSA</em>, <em>46</em>(3), 552-581. (<a href='https://doi.org/10.1111/jtsa.12775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider multi-variate versions of two popular classes of integer-valued processes. While the transition mechanism is time-homogeneous, a possible non-stationarity is introduced by an exogeneous covariate process. We prove absolute regularity ( β $$ \beta $$ -mixing) for the count process with exponentially decaying mixing coefficients. The proof of this result makes use of some sort of contraction in the transition mechanism which allows a coupling of two versions of the count process such that they eventually coalesce. We show how this result can be used to prove asymptotic normality of a least squares estimator of an involved model parameter.},
  archive      = {J_JTSA},
  author       = {Zinsou Max Debaly and Michael H. Neumann and Lionel Truquet},
  doi          = {10.1111/jtsa.12775},
  journal      = {Journal of Time Series Analysis},
  month        = {5},
  number       = {3},
  pages        = {552-581},
  shortjournal = {J. Time Series Anal.},
  title        = {Mixing properties of non-stationary multi-variate count processes},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean-preserving rounding integer-valued ARMA models. <em>JTSA</em>, <em>46</em>(3), 530-551. (<a href='https://doi.org/10.1111/jtsa.12774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past four decades, research on count time series has made significant progress, but research on ℤ $$ \mathbb{Z} $$ -valued time series is relatively rare. Existing ℤ $$ \mathbb{Z} $$ -valued models are mainly of autoregressive structure, where the use of the rounding operator is very natural. Because of the discontinuity of the rounding operator, the formulation of the corresponding model identifiability conditions and the computation of parameter estimators need special attention. It is also difficult to derive closed-form formulae for crucial stochastic properties. We rediscover a stochastic rounding operator, referred to as mean-preserving rounding, which overcomes the above drawbacks. Then, a novel class of -valued ARMA models based on the new operator is proposed, and the existence of stationary solutions of the models is established. Stochastic properties including closed-form formulae for (conditional) moments, autocorrelation function, and conditional distributions are obtained. The advantages of our novel model class compared to existing ones are demonstrated. In particular, our model construction avoids identifiability issues such that maximum likelihood estimation is possible. A simulation study is provided, and the appealing performance of the new models is shown by several real-world data sets.},
  archive      = {J_JTSA},
  author       = {Christian H. Weiß and Fukang Zhu},
  doi          = {10.1111/jtsa.12774},
  journal      = {Journal of Time Series Analysis},
  month        = {5},
  number       = {3},
  pages        = {530-551},
  shortjournal = {J. Time Series Anal.},
  title        = {Mean-preserving rounding integer-valued ARMA models},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted discrete ARMA models for categorical time series. <em>JTSA</em>, <em>46</em>(3), 505-529. (<a href='https://doi.org/10.1111/jtsa.12773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new and flexible class of ARMA-like (autoregressive moving average) models for nominal or ordinal time series is proposed, which are characterized by using so-called weighting operators and are, thus, referred to as weighted discrete ARMA (WDARMA) models. By choosing an appropriate type of weighting operator, one can model, for example, nominal time series with negative serial dependencies, or ordinal time series where transitions to neighboring states are more likely than sudden large jumps. Essential stochastic properties of WDARMA models are derived, such as the existence of a stationary, ergodic, and -mixing solution as well as closed-form formulae for marginal and bivariate probabilities. Numerical illustrations as well as simulation experiments regarding the finite-sample performance of maximum likelihood estimation are presented. The possible benefits of using an appropriate weighting scheme within the WDARMA class are demonstrated by a real-world data application.},
  archive      = {J_JTSA},
  author       = {Christian H. Weiß and Osama Swidan},
  doi          = {10.1111/jtsa.12773},
  journal      = {Journal of Time Series Analysis},
  month        = {5},
  number       = {3},
  pages        = {505-529},
  shortjournal = {J. Time Series Anal.},
  title        = {Weighted discrete ARMA models for categorical time series},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-normalization inference for linear trends in cointegrating regressions. <em>JTSA</em>, <em>46</em>(3), 491-504. (<a href='https://doi.org/10.1111/jtsa.12771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, statistical tests concerning the trend coefficient in cointegrating regressions are addressed for the case when the stochastic regressors have deterministic linear trends. The self-normalization (SN) approach is adopted for developing inferential methods in the integrated and modified ordinary least squares (IMOLS) estimation framework. Two different self-normalizers are used to construct the SN test statistics: a functional of the recursive IMOLS estimators and a functional of the IMOLS residuals. These two self-normalizers produce two SN tests, denoted by and respectively. Neither test requires studentization with a heteroskedasticity and autocorrelation consistent (HAC) estimator. A trimming parameter must be chosen to implement the test, whereas the test does not require any tuning parameter. In the simulation, the test exhibits the smallest size distortion among the inferential methods examined in this article. However, this may come with some loss of power, particularly in small samples.},
  archive      = {J_JTSA},
  author       = {Cheol-Keun Cho},
  doi          = {10.1111/jtsa.12771},
  journal      = {Journal of Time Series Analysis},
  month        = {5},
  number       = {3},
  pages        = {491-504},
  shortjournal = {J. Time Series Anal.},
  title        = {Self-normalization inference for linear trends in cointegrating regressions},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved estimation of dynamic models of conditional means and variances. <em>JTSA</em>, <em>46</em>(3), 458-490. (<a href='https://doi.org/10.1111/jtsa.12770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using ‘working’ assumptions on conditional third and fourth moments of errors, we propose a method of moments estimator that can have improved efficiency over the popular Gaussian quasi-maximum likelihood estimator (GQMLE). Higher-order moment assumptions are not needed for consistency – we only require the first two conditional moments to be correctly specified – but the optimal instruments are derived under these assumptions. The working assumptions allow both asymmetry in the distribution of the standardized errors as well as fourth moments that can be smaller or larger than that of the Gaussian distribution. The approach is related to the generalized estimation equations (GEE) approach – which seeks the improvement of estimators of the conditional mean parameters by making working assumptions on the conditional second moments. We derive the asymptotic distribution of the new estimator and show that it does not depend on the estimators of the third and fourth moments. A simulation study shows that the efficiency gains over the GQMLE can be non-trivial.},
  archive      = {J_JTSA},
  author       = {Weining Wang and Jeffrey M. Wooldridge and Mengshan Xu},
  doi          = {10.1111/jtsa.12770},
  journal      = {Journal of Time Series Analysis},
  month        = {5},
  number       = {3},
  pages        = {458-490},
  shortjournal = {J. Time Series Anal.},
  title        = {Improved estimation of dynamic models of conditional means and variances},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The Granger–Johansen representation theorem for integrated time series on banach space. <em>JTSA</em>, <em>46</em>(3), 432-457. (<a href='https://doi.org/10.1111/jtsa.12766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove an extended Granger–Johansen representation theorem (GJRT) for finite- or infinite-order integrated autoregressive time series on Banach space. We assume only that the resolvent of the autoregressive polynomial for the series is analytic on and inside the unit circle except for an isolated singularity at unity. If the singularity is a pole of finite order the time series is integrated of the same order. If the singularity is an essential singularity the time series is integrated of order infinity. When there is no deterministic forcing the value of the series at each time is the sum of an almost surely convergent stochastic trend, a deterministic term depending on the initial conditions and a finite sum of embedded white noise terms in the prior observations. This is the extended GJRT. In each case the original series is the sum of two separate autoregressive time series on complementary subspaces – a singular component which is integrated of the same order as the original series and a regular component which is not integrated. The extended GJRT applies to all integrated autoregressive processes irrespective of the spatial dimension, the number of stochastic trends and cointegrating relations in the system and the order of integration.},
  archive      = {J_JTSA},
  author       = {Phil Howlett and Brendan K. Beare and Massimo Franchi and John Boland and Konstantin Avrachenkov},
  doi          = {10.1111/jtsa.12766},
  journal      = {Journal of Time Series Analysis},
  month        = {5},
  number       = {3},
  pages        = {432-457},
  shortjournal = {J. Time Series Anal.},
  title        = {The Granger–Johansen representation theorem for integrated time series on banach space},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dependence properties of stochastic volatility models. <em>JTSA</em>, <em>46</em>(3), 421-431. (<a href='https://doi.org/10.1111/jtsa.12765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concepts of physical dependence and approximability have been extensively used over the past two decades to quantify nonlinear dependence in time series. We show that most stochastic volatility models satisfy both dependence conditions, even if their realizations take values in abstract Hilbert spaces, thus covering univariate, multi-variate and functional models. Our results can be used to apply to general stochastic volatility models a multitude of inferential procedures established for Bernoulli shifts.},
  archive      = {J_JTSA},
  author       = {Piotr Kokoszka and Neda Mohammadi and Haonan Wang},
  doi          = {10.1111/jtsa.12765},
  journal      = {Journal of Time Series Analysis},
  month        = {5},
  number       = {3},
  pages        = {421-431},
  shortjournal = {J. Time Series Anal.},
  title        = {Dependence properties of stochastic volatility models},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing covariance separability for continuous functional data. <em>JTSA</em>, <em>46</em>(3), 402-420. (<a href='https://doi.org/10.1111/jtsa.12764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the covariance structure of data is a fundamental task of statistics. While this task is simple for low-dimensional observations, it becomes challenging for more intricate objects, such as multi-variate functions. Here, the covariance can be so complex that just saving a non-parametric estimate is impractical and structural assumptions are necessary to tame the model. One popular assumption for space-time data is separability of the covariance into purely spatial and temporal factors. In this article, we present a new test for separability in the context of dependent functional time series. While most of the related work studies functional data in a Hilbert space of square integrable functions, we model the observations as objects in the space of continuous functions equipped with the supremum norm. We argue that this (mathematically challenging) setup enhances interpretability for users and is more in line with practical preprocessing. Our test statistic measures the maximal deviation between the estimated covariance kernel and a separable approximation. Critical values are obtained by a non-standard multiplier bootstrap for dependent data. We prove the statistical validity of our approach and demonstrate its practicability in a simulation study and a data example.},
  archive      = {J_JTSA},
  author       = {Holger Dette and Gauthier Dierickx and Tim Kutta},
  doi          = {10.1111/jtsa.12764},
  journal      = {Journal of Time Series Analysis},
  month        = {5},
  number       = {3},
  pages        = {402-420},
  shortjournal = {J. Time Series Anal.},
  title        = {Testing covariance separability for continuous functional data},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fractional stochastic volatility model. <em>JTSA</em>, <em>46</em>(2), 378-397. (<a href='https://doi.org/10.1111/jtsa.12749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a discrete-time fractional stochastic volatility model (FSV) based on fractional Gaussian noise. The new model includes the standard stochastic volatility model as a special case and has the same limit as the fractional integrated stochastic volatility (FISV) model, which is the continuous-time fractional Ornstein–Uhlenbeck process. A simulated maximum likelihood method, which maximizes the time-domain log-likelihood function calculated by the importance sampling technique, and a frequency-domain quasi maximum likelihood method (or quasi Whittle) are employed to estimate the model parameters. Simulation studies suggest that, while both estimation methods can accurately estimate the model, the simulated maximum likelihood method outperforms the quasi Whittle method. As an illustration, we fit the FSV and FISV models with the proposed estimation techniques to the S&P 500 composite index over a sample period spanning 45 years.},
  archive      = {J_JTSA},
  author       = {Shuping Shi and Xiaobin Liu and Jun Yu},
  doi          = {10.1111/jtsa.12749},
  journal      = {Journal of Time Series Analysis},
  month        = {3},
  number       = {2},
  pages        = {378-397},
  shortjournal = {J. Time Series Anal.},
  title        = {Fractional stochastic volatility model},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk parity portfolio optimization under heavy-tailed returns and dynamic correlations. <em>JTSA</em>, <em>46</em>(2), 353-377. (<a href='https://doi.org/10.1111/jtsa.12792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk parity portfolio optimization, using expected shortfall as the risk measure, is investigated when asset returns are fat-tailed and heteroscedastic with regime switching dynamic correlations. The conditional return distribution is modeled by an elliptical multi-variate generalized hyperbolic distribution, allowing for fast parameter estimation via an expectation-maximization algorithm, and a semi-closed form of the risk contributions. A new method for efficient computation of non-Gaussian risk parity weights sidesteps the need for numerical simulations or Cornish–Fisher-type approximations. Accounting for fat-tailed returns, the risk parity allocation is less sensitive to volatility shocks, thereby generating lower portfolio turnover, in particular during market turmoils such as the global financial crisis or the COVID shock. While risk parity portfolios are rather robust to the misuse of the Gaussian distribution, a sophisticated time series model can improve risk-adjusted returns, strongly reduces drawdowns during periods of market stress and enables to use a holistic risk model for portfolio and risk management.},
  archive      = {J_JTSA},
  author       = {Marc S. Paolella and Paweł Polak and Patrick S. Walker},
  doi          = {10.1111/jtsa.12792},
  journal      = {Journal of Time Series Analysis},
  month        = {3},
  number       = {2},
  pages        = {353-377},
  shortjournal = {J. Time Series Anal.},
  title        = {Risk parity portfolio optimization under heavy-tailed returns and dynamic correlations},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-causal and non-invertible ARMA models: Identification, estimation and application in equity portfolios. <em>JTSA</em>, <em>46</em>(2), 325-352. (<a href='https://doi.org/10.1111/jtsa.12776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mixed causal-non-causal invertible-non-invertible autoregressive moving-average (MARMA) models have the advantage of incorporating roots inside the unit circle, thus adjusting the dynamics of financial returns that depend on future expectations. This article introduces new techniques for estimating, identifying and simulating MARMA models. Although the estimation of the parameters is done using second-order moments, the identification relies on the existence of high-order dynamics, captured in the high-order spectral densities and the correlation of the squared residuals. A comprehensive Monte Carlo study demonstrated the robust performance of our estimation and identification methods. We propose an empirical application to 24 portfolios from emerging markets based on the factors: size, book-to-market, profitability, investment and momentum. All portfolios exhibited forward-looking behavior, showing significant non-causal and non-invertible dynamics. Moreover, we found the residuals to be uncorrelated and independent, with no trace of conditional volatility.},
  archive      = {J_JTSA},
  author       = {Alain Hecq and Daniel Velasquez-Gaviria},
  doi          = {10.1111/jtsa.12776},
  journal      = {Journal of Time Series Analysis},
  month        = {3},
  number       = {2},
  pages        = {325-352},
  shortjournal = {J. Time Series Anal.},
  title        = {Non-causal and non-invertible ARMA models: Identification, estimation and application in equity portfolios},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized covariance-based inference for models set-identified from independence restrictions. <em>JTSA</em>, <em>46</em>(2), 300-324. (<a href='https://doi.org/10.1111/jtsa.12779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops statistical inference methods for a class of set-identified models, where the errors are known functions of observations and the parameters satisfy either serial or/and cross-sectional independence conditions. This class of models includes the independent component analysis (ICA), Structural Vector Autoregressive (SVAR), and multi-variate mixed causal–non-causal models. We use the Generalized Covariance (GCov) estimator to compute the residual-based portmanteau statistic for testing the error independence hypothesis. Next, we build the confidence sets for the identified sets of parameters by inverting the test statistic. We also discuss the choice (design) of these statistics. The approach is illustrated by simulations examining the under-identification condition in an ICA model and an application to financial return series.},
  archive      = {J_JTSA},
  author       = {Christian Gourieroux and Joann Jasiak},
  doi          = {10.1111/jtsa.12779},
  journal      = {Journal of Time Series Analysis},
  month        = {3},
  number       = {2},
  pages        = {300-324},
  shortjournal = {J. Time Series Anal.},
  title        = {Generalized covariance-based inference for models set-identified from independence restrictions},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The liquidity uncertainty premium puzzle. <em>JTSA</em>, <em>46</em>(2), 286-299. (<a href='https://doi.org/10.1111/jtsa.12802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The puzzling negative relation between liquidity uncertainty and asset returns, originally put forward by Chordia, Subrahmanyam, and Anshuman (2001) and confirmed by the subsequent empirical literature up to date, is neither robust to the aggregation period, nor to the observation frequency used to compute the volatility of trading volume. We demonstrate that their procedure involves an estimation bias due to the persistence and skewness of volumes. When using an alternative approach based on high-frequency data to measure liquidity uncertainty, the relationship turns out to be positive. However, portfolio strategies based on liquidity uncertainty do not appear to be profitable.},
  archive      = {J_JTSA},
  author       = {Maria Flora and Ilaria Gianstefani and Roberto Renò},
  doi          = {10.1111/jtsa.12802},
  journal      = {Journal of Time Series Analysis},
  month        = {3},
  number       = {2},
  pages        = {286-299},
  shortjournal = {J. Time Series Anal.},
  title        = {The liquidity uncertainty premium puzzle},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting the yield curve: The role of additional and time-varying decay parameters, conditional heteroscedasticity, and macro-economic factors. <em>JTSA</em>, <em>46</em>(2), 258-285. (<a href='https://doi.org/10.1111/jtsa.12769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we analyse the forecasting performance of several parametric extensions of the popular Dynamic Nelson–Siegel (DNS) model for the yield curve. Our focus is on the role of additional and time-varying decay parameters, conditional heteroscedasticity, and macroeconomic variables. We also consider the role of several popular restrictions on the dynamics of the factors. Using a novel dataset of end-of-month continuously compounded Treasury yields on US zero-coupon bonds and frequentist estimation based on the extended Kalman filter, we show that a second decay parameter does not contribute to better forecasts. In concordance with the preferred habitat theory, we also show that the best forecasting model depends on the maturity. For short maturities, the best performance is obtained in a heteroscedastic model with a time-varying decay parameter. However, for long maturities, neither the time-varying decay nor the heteroscedasticity plays any role, and the best forecasts are obtained in the basic DNS model with the shape of the yield curve depending on macroeconomic activity. Finally, we find that assuming non-stationary factors is helpful in forecasting at long horizons.},
  archive      = {J_JTSA},
  author       = {João F. Caldeira and Werley C. Cordeiro and Esther Ruiz and André A.P. Santos},
  doi          = {10.1111/jtsa.12769},
  journal      = {Journal of Time Series Analysis},
  month        = {3},
  number       = {2},
  pages        = {258-285},
  shortjournal = {J. Time Series Anal.},
  title        = {Forecasting the yield curve: The role of additional and time-varying decay parameters, conditional heteroscedasticity, and macro-economic factors},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ridge regularized estimation of VAR models for inference. <em>JTSA</em>, <em>46</em>(2), 235-257. (<a href='https://doi.org/10.1111/jtsa.12737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge regression is a popular method for dense least squares regularization. In this article, ridge regression is studied in the context of VAR model estimation and inference. The implications of anisotropic penalization are discussed, and a comparison is made with Bayesian ridge-type estimators. The asymptotic distribution and the properties of cross-validation techniques are analyzed. Finally, the estimation of impulse response functions is evaluated with Monte Carlo simulations and ridge regression is compared with a number of similar and competing methods.},
  archive      = {J_JTSA},
  author       = {Giovanni Ballarin},
  doi          = {10.1111/jtsa.12737},
  journal      = {Journal of Time Series Analysis},
  month        = {3},
  number       = {2},
  pages        = {235-257},
  shortjournal = {J. Time Series Anal.},
  title        = {Ridge regularized estimation of VAR models for inference},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-frequency instruments and identification-robust inference for stochastic volatility models. <em>JTSA</em>, <em>46</em>(2), 216-234. (<a href='https://doi.org/10.1111/jtsa.12812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel class of stochastic volatility models, which can utilize and relate many high-frequency realized volatility (RV) measures to latent volatility. Instrumental variable methods provide a unified framework for estimation and testing. We study parameter inference problems in the proposed framework with nonstationary stochastic volatility and exogenous predictors in the latent volatility process. Identification-robust methods are developed for a joint hypothesis involving the volatility persistence parameter and the autocorrelation parameter of the composite error (or the noise ratio). For inference about the volatility persistence parameter, projection techniques are applied. The proposed tests include Anderson-Rubin-type tests and their point-optimal versions. For distributional theory, we provide finite-sample tests and confidence sets for Gaussian errors, establish exact Monte Carlo test procedures for non-Gaussian errors (possibly heavy-tailed), and show asymptotic validity under weaker assumptions. Simulation results show that the proposed tests outperform the asymptotic test regarding size and exhibit excellent power in empirically realistic settings. The proposed inference methods are applied to IBM's price and option data (2009–2013). We consider 175 different instruments (IVs) spanning 22 classes and analyze their ability to describe the low-frequency volatility. IVs are compared based on the average length of the proposed identification-robust confidence intervals. The superior instrument set mostly comprises 5-min HF realized measures, and these IVs produce confidence sets which show that the volatility process is nearly unit-root. In addition, we find RVs with higher frequency yield wider confidence intervals than RVs with slightly lower frequency, indicating that these confidence intervals adjust to absorb market microstructure noise. Furthermore, when we consider irrelevant or weak IVs (jumps and signed jumps), the proposed tests produce unbounded confidence intervals. We also find that both RV and BV measures produce almost identical confidence intervals across all 14 subclasses, confirming that our methodology is robust in the presence of jumps. Finally, although jumps contain little information regarding the low-frequency volatility, we find evidence that there may be a nonlinear relationship between jumps and low-frequency volatility.},
  archive      = {J_JTSA},
  author       = {Md. Nazmul Ahsan and Jean-Marie Dufour},
  doi          = {10.1111/jtsa.12812},
  journal      = {Journal of Time Series Analysis},
  month        = {3},
  number       = {2},
  pages        = {216-234},
  shortjournal = {J. Time Series Anal.},
  title        = {High-frequency instruments and identification-robust inference for stochastic volatility models},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time series for QFFE: Special issue of the journal of time series analysis. <em>JTSA</em>, <em>46</em>(2), 214-215. (<a href='https://doi.org/10.1111/jtsa.12814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JTSA},
  author       = {Christian Francq and Christophe Hurlin and Sébastien Laurent and Jean-Michel Zakoian},
  doi          = {10.1111/jtsa.12814},
  journal      = {Journal of Time Series Analysis},
  month        = {3},
  number       = {2},
  pages        = {214-215},
  shortjournal = {J. Time Series Anal.},
  title        = {Time series for QFFE: Special issue of the journal of time series analysis},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating a common break point in means for long-range dependent panel data. <em>JTSA</em>, <em>46</em>(1), 181-209. (<a href='https://doi.org/10.1111/jtsa.12763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study a common break point in means for panel data with cross-sectional dependence through unobservable common factors, in which the observations are long-range dependent over time and are heteroscedastic and may have different degrees of dependence across panels. First, we adopt the least squares method without taking the data features into account to estimate the common break point and to see how the data features affect the asymptotic behaviors of the estimator. Then, an iterative least squares estimator of the common break point which accounts for the common factors in the estimation procedure is examined. Our theoretical results reveal that: (1) There is a trade-off between the overall break magnitude of the panel data and the long-range dependence for both estimators. (2) The second estimation procedure can eliminate the effects of common factors from the asymptotic behaviors of the estimator successfully, but it cannot improve the rate of convergence of the estimator in most cases. Moreover, Monte Carlo simulations are given to illustrate the theoretical results on finite-sample performance.},
  archive      = {J_JTSA},
  author       = {Daiqing Xi and Cheng-Der Fuh and Tianxiao Pang},
  doi          = {10.1111/jtsa.12763},
  journal      = {Journal of Time Series Analysis},
  month        = {1},
  number       = {1},
  pages        = {181-209},
  shortjournal = {J. Time Series Anal.},
  title        = {Estimating a common break point in means for long-range dependent panel data},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A trinomial difference autoregressive process for the bounded ℤ-valued time series. <em>JTSA</em>, <em>46</em>(1), 152-180. (<a href='https://doi.org/10.1111/jtsa.12762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article tackles the modeling challenge of bounded ℤ -valued time series by proposing a novel trinomial difference autoregressive process. This process not only maintains the autocorrelation structure presenting in the classical binomial GARCH model, but also facilitates the analysis of bounded ℤ -valued time series with negative or positive correlation. We verify the stationarity and ergodicity of the couple process (comprising both the observed process and its conditional mean process) while also presenting several stochastic properties. We further discuss the conditional maximum likelihood estimation and establish their asymptotic properties. The effectiveness of these estimators is assessed through simulation studies, followed by the application of the proposed models to two real datasets.},
  archive      = {J_JTSA},
  author       = {Huaping Chen and Zifei Han and Fukang Zhu},
  doi          = {10.1111/jtsa.12762},
  journal      = {Journal of Time Series Analysis},
  month        = {1},
  number       = {1},
  pages        = {152-180},
  shortjournal = {J. Time Series Anal.},
  title        = {A trinomial difference autoregressive process for the bounded ℤ-valued time series},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). General estimation results for tdVARMA array models. <em>JTSA</em>, <em>46</em>(1), 137-151. (<a href='https://doi.org/10.1111/jtsa.12761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article will focus on vector autoregressive-moving average (VARMA) models with time-dependent coefficients (td) to represent general nonstationary time series, not necessarily Gaussian. The coefficients depend on time, possibly on the length of the series n , hence the name tdVARMA ( n ) for the models, but not necessarily on the rescaled time t / n . As a consequence of the dependency on n of the model, we need to consider array processes instead of stochastic processes. Under appropriate assumptions, it is shown that a Gaussian quasi-maximum likelihood estimator is consistent in probability and asymptotically normal. The theoretical results are illustrated using three examples of bivariate processes, the first two with marginal heteroscedasticity. The first example is a tdVAR ( n ) (1) process while the second example is a tdVMA ( n ) (1) process. In these two cases, the finite-sample behavior is checked via a Monte Carlo simulation study. The results are compatible with the asymptotic properties even for small . A third example shows the application of the tdVARMA models for a real time series.},
  archive      = {J_JTSA},
  author       = {Abdelkamel Alj and Rajae Azrak and Guy Mélard},
  doi          = {10.1111/jtsa.12761},
  journal      = {Journal of Time Series Analysis},
  month        = {1},
  number       = {1},
  pages        = {137-151},
  shortjournal = {J. Time Series Anal.},
  title        = {General estimation results for tdVARMA array models},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selecting the number of factors in multi-variate time series. <em>JTSA</em>, <em>46</em>(1), 113-136. (<a href='https://doi.org/10.1111/jtsa.12760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How many factors are there? It is a critical question that researchers and practitioners deal with when estimating factor models. We proposed a new eigenvalue ratio criterion for the number of factors in static approximate factor models. It considers a pooled squared correlation matrix which is defined as a weighted combination of the main observed squared correlation matrices. Theoretical results are given to justify the expected good properties of the criterion, and a Monte Carlo study shows its good finite sample performance in different scenarios, depending on the idiosyncratic error structure and factor strength. We conclude comparing different criteria in a forecasting exercise with macroeconomic data.},
  archive      = {J_JTSA},
  author       = {Angela Caro and Daniel Peña},
  doi          = {10.1111/jtsa.12760},
  journal      = {Journal of Time Series Analysis},
  month        = {1},
  number       = {1},
  pages        = {113-136},
  shortjournal = {J. Time Series Anal.},
  title        = {Selecting the number of factors in multi-variate time series},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bootstrapping non-stationary and irregular time series using singular spectral analysis. <em>JTSA</em>, <em>46</em>(1), 81-112. (<a href='https://doi.org/10.1111/jtsa.12759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the consequences of using Singular Spectral Analysis (SSA) to construct a time series bootstrap. The bootstrap replications are obtained via a SSA decomposition obtained using rescaled trajectories (RT-SSA), a procedure that is particularly useful in the analysis of time series that exhibit nonlinear, non-stationary and intermittent or transient behaviour. The theoretical validity of the RT-SSA bootstrap when used to approximate the sampling properties of a general class of statistics is established under regularity conditions that encompass a very broad range of data generating processes. A smeared and a boosted version of the RT-SSA bootstrap are also presented. Practical implementation of the bootstrap is considered and the results are illustrated using stationary, non-stationary and irregular time series examples.},
  archive      = {J_JTSA},
  author       = {Don S. Poskitt},
  doi          = {10.1111/jtsa.12759},
  journal      = {Journal of Time Series Analysis},
  month        = {1},
  number       = {1},
  pages        = {81-112},
  shortjournal = {J. Time Series Anal.},
  title        = {Bootstrapping non-stationary and irregular time series using singular spectral analysis},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of non-smooth non-parametric estimating equations models with dependent data. <em>JTSA</em>, <em>46</em>(1), 59-80. (<a href='https://doi.org/10.1111/jtsa.12758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers estimation of non-smooth possibly overidentified non-parametric estimating equations models with weakly dependent data. The estimators are based on a kernel smoothed version of the generalized empirical likelihood and the generalized method of moments approaches. The article derives the asymptotic normality of both estimators and shows that the proposed local generalized empirical likelihood estimator is more efficient than the local generalized moment estimator unless a two-step procedure is used. The article also proposes novel tests for the correct specification of the considered model that are shown to have power against local alternatives and are consistent against fixed alternatives. Monte Carlo simulations and an empirical application illustrate the finite sample properties and applicability of the proposed estimators and test statistics.},
  archive      = {J_JTSA},
  author       = {Francesco Bravo},
  doi          = {10.1111/jtsa.12758},
  journal      = {Journal of Time Series Analysis},
  month        = {1},
  number       = {1},
  pages        = {59-80},
  shortjournal = {J. Time Series Anal.},
  title        = {Estimation of non-smooth non-parametric estimating equations models with dependent data},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for the extent of instability in nearly unstable processes. <em>JTSA</em>, <em>46</em>(1), 33-58. (<a href='https://doi.org/10.1111/jtsa.12751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with unit root issues in time series analysis. It has been known for a long time that unit root tests may be flawed when a series although stationary has a root close to unity. That motivated recent papers dedicated to autoregressive processes where the bridge between stability and instability is expressed by means of time-varying coefficients. The process we consider has a companion matrix A n with spectral radius ρ ⁡ ( A n ) < 1 satisfying ρ ⁡ ( A n ) → 1 , a situation described as ‘nearly-unstable’. The question we investigate is: given an observed path supposed to come from a nearly unstable process, is it possible to test for the ‘extent of instability’, i.e. to test how close we are to the unit root? In this regard, we develop a strategy to evaluate and to test for : ‘ ’ against : ‘ ’ when lies in an inner -neighborhood of the unity, for some . Empirical evidence is given about the advantages of the flexibility induced by such a procedure compared to the common unit root tests. We also build a symmetric procedure for the usually left out situation where the dominant root lies around .},
  archive      = {J_JTSA},
  author       = {Marie Badreau and Frédéric Proïa},
  doi          = {10.1111/jtsa.12751},
  journal      = {Journal of Time Series Analysis},
  month        = {1},
  number       = {1},
  pages        = {33-58},
  shortjournal = {J. Time Series Anal.},
  title        = {Testing for the extent of instability in nearly unstable processes},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a matrix-valued autoregressive model. <em>JTSA</em>, <em>46</em>(1), 3-32. (<a href='https://doi.org/10.1111/jtsa.12748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data sets in biology, medicine, and other biostatistical areas deal with matrix-valued time series. The case of a single univariate time series is very well developed in the literature; and single multi-variate series (i.e., vector time series) though less well studied have also been developed. A class of matrix time series models is introduced for dealing with situations where there are multiple sets of multi-variate time series data. Explicit expressions for a matrix autoregressive model along with its cross-autocorrelation functions are derived. Stationarity conditions are also provided. Least squares estimators and maximum likelihood estimators of the model parameters and their asymptotic properties are derived. Results are illustrated through simulation studies and a real data application.},
  archive      = {J_JTSA},
  author       = {S. Yaser Samadi and Lynne Billard},
  doi          = {10.1111/jtsa.12748},
  journal      = {Journal of Time Series Analysis},
  month        = {1},
  number       = {1},
  pages        = {3-32},
  shortjournal = {J. Time Series Anal.},
  title        = {On a matrix-valued autoregressive model},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
