<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AIM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aim">AIM - 41</h2>
<ul>
<li><details>
<summary>
(2025). From rights to runtime: Privacy engineering for agentic AI. <em>AIM</em>, <em>46</em>(4), e70036. (<a href='https://doi.org/10.1002/aaai.70036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agentic AI shifts stacks from request-response to plan-execute. Systems no longer just answer; they act—planning tasks, calling tools, keeping memory, and changing external state. That shift moves privacy from policy docs into the runtime. This opinion piece argues that we do not need a new privacy theory for agents; we need enforceable, observable controls that render existing rights as product behavior. Anchoring on GDPR—with portable touchpoints to CPRA, LGPD, and PDPA, we propose a developer-first toolkit: optional, bounded, user-visible memory; a purpose-aware egress gate that enforces minimization and transfer rules; proportional safeguards that scale with stakes; and traces that tell a coherent story across components and suppliers. We show how the EU AI Act's risk management, logging, and oversight can scaffold these controls and enable evidence reuse. The result is an agentic runtime that keeps people in control and teams audit-ready by design.},
  archive      = {J_AIM},
  author       = {Keivan Navaie},
  doi          = {10.1002/aaai.70036},
  journal      = {AI Magazine},
  month        = {Winter},
  number       = {4},
  pages        = {e70036},
  shortjournal = {AI Mag.},
  title        = {From rights to runtime: Privacy engineering for agentic AI},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A community-driven vision for a new knowledge resource for AI. <em>AIM</em>, <em>46</em>(4), e70035. (<a href='https://doi.org/10.1002/aaai.70035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The long-standing goal of creating a comprehensive, multi-purpose knowledge resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and other commercial knowledge graphs, verifiable, general-purpose, widely available sources of knowledge remain a critical deficiency in AI infrastructure. Large language models struggle due to knowledge gaps; robotic planning lacks necessary world knowledge; and the detection of factually false information relies heavily on human expertise. What kind of knowledge resource is most needed in AI today? How can modern technology shape its development and evaluation? A recent AAAI workshop gathered over 50 researchers to explore these questions. This paper synthesizes our findings and outlines a community-driven vision for a new knowledge infrastructure. In addition to leveraging contemporary advances in knowledge representation and reasoning, one promising idea is to build an open engineering framework to exploit knowledge modules effectively within the context of practical applications. Such a framework should include sets of conventions and social structures that are adopted by contributors.},
  archive      = {J_AIM},
  author       = {Vinay K Chaudhri and Chaitan Baru and Brandon Bennett and Mehul Bhatt and Darion Cassel and Anthony G Cohn and Rina Dechter and Esra Erdem and Dave Ferrucci and Ken Forbus and Gregory Gelfond and Michael Genesereth and Andrew S. Gordon and Benjamin Grosof and Gopal Gupta and Jim Hendler and Sharat Israni and Tyler R. Josephson and Patrick Kyllonen and Yuliya Lierler and Vladimir Lifschitz and Clifton McFate and Hande Küçük McGinty and Leora Morgenstern and Alessandro Oltramari and Praveen Paritosh and Dan Roth and Blake Shepard and Cogan Shimizu and Denny Vrandečić and Mark Whiting and Michael Witbrock},
  doi          = {10.1002/aaai.70035},
  journal      = {AI Magazine},
  month        = {Winter},
  number       = {4},
  pages        = {e70035},
  shortjournal = {AI Mag.},
  title        = {A community-driven vision for a new knowledge resource for AI},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based AI planning and execution platforms for robotics. <em>AIM</em>, <em>46</em>(3), e70034. (<a href='https://doi.org/10.1002/aaai.70034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based planning and execution systems offer a principled approach to building flexible autonomous robots that can perform diverse tasks by automatically combining a host of basic skills. This idea is almost as old as modern robotics. Yet, while diverse general-purpose reasoning architectures have been proposed since, general-purpose software platforms that support the construction of planner-based controllers and their integration with modern robotic platforms have emerged only recently, starting with the influential ROSPlan system. Since then, a growing number of domain-independent model-based platforms for robot task-level control have emerged. In this paper, we consider the diverse design choices and issues existing platforms attempt to address, the different solutions proposed so far, and suggest avenues for future development. We also briefly discuss the elephant in the room: foundation models.},
  archive      = {J_AIM},
  author       = {Or Wertheim and Ronen I. Brafman},
  doi          = {10.1002/aaai.70034},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70034},
  shortjournal = {AI Mag.},
  title        = {Model-based AI planning and execution platforms for robotics},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable AI, energy and critical infrastructure systems. <em>AIM</em>, <em>46</em>(3), e70033. (<a href='https://doi.org/10.1002/aaai.70033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The AAAI 2025 Bridge on “Explainable AI, Energy and Critical Infrastructure Systems” was held at the Pennsylvania Convention Centre, Philadelphia, Pennsylvania, USA, on February 25, 2025. The bridge gathered researchers and practitioners, bringing together innovation research across explainable AI, energy and critical infrastructure systems so they can enhance each other. The Bridge featured five keynote presentations by experts, one tutorial, poster presentations by authors who contributed their research findings, and three breakout sessions to discuss new challenges arising at the intersection of these exciting disciplines.},
  archive      = {J_AIM},
  author       = {Francesco Leofante and André Artelt and Demetrios Eliades and Anna Korre and Francesca Toni and Tim Miller},
  doi          = {10.1002/aaai.70033},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70033},
  shortjournal = {AI Mag.},
  title        = {Explainable AI, energy and critical infrastructure systems},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open-source AI at scale: Establishing an enterprise AI strategy through modular frameworks. <em>AIM</em>, <em>46</em>(3), e70032. (<a href='https://doi.org/10.1002/aaai.70032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comprehensive enterprise AI strategy developed within the AI Center of Excellence at Fidelity Investments, emphasizing the strategic integration of open-source AI frameworks into scalable, modular, and reproducible enterprise-grade solutions. Our approach is structured around five key pillars: learning from offline data, learning from online feedback, intelligent decision-making, automated assistants, and responsible AI practices. Through a suite of 12 open-source libraries, we demonstrate how modular and interoperable tools can collectively enhance scalability, fairness, and explainability in real-world AI deployments. We further illustrate the impact of this strategy through three enterprise case studies. Finally, we distill a set of best deployment practices to guide organizations in implementing modular, open-source AI strategies at scale.},
  archive      = {J_AIM},
  author       = {Serdar Kadıoğlu},
  doi          = {10.1002/aaai.70032},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70032},
  shortjournal = {AI Mag.},
  title        = {Open-source AI at scale: Establishing an enterprise AI strategy through modular frameworks},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated vulnerability evaluation with large language models and vulnerability ontologies. <em>AIM</em>, <em>46</em>(3), e70031. (<a href='https://doi.org/10.1002/aaai.70031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The National Vulnerability Database (NVD) publishes over a thousand new vulnerabilities monthly, with a projected 25 percent increase in 2024, highlighting the crucial need for rapid vulnerability identification to mitigate cybersecurity attacks and save costs and resources. In this work, we propose using large language models (LLMs) to learn vulnerability evaluation from historical assessments of medical device vulnerabilities in a single manufacturer's portfolio. We highlight the effectiveness and challenges of using LLMs for automatic vulnerability evaluation and introduce a method to enrich historical data with cybersecurity ontologies, enabling the system to understand new vulnerabilities without retraining the LLM. Our LLM system integrates with the in-house application—Cybersecurity Management System (CSMS)—to help Siemens Healthineers (SHS) product cybersecurity experts efficiently assess the vulnerabilities in our products. Also, we present a comprehensive set of experiments that helps showcase the properties of the LLM and dataset, the various guardrails we have implemented to safeguard the system in production, and the guidelines for efficient integration of LLMs into the cybersecurity tool.},
  archive      = {J_AIM},
  author       = {Rikhiya Ghosh and Hans-Martin von Stockhausen and Martin Schmitt and George Marica Vasile and Sanjeev Kumar Karn and Oladimeji Farri},
  doi          = {10.1002/aaai.70031},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70031},
  shortjournal = {AI Mag.},
  title        = {Automated vulnerability evaluation with large language models and vulnerability ontologies},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal AI teacher: Integrating edge computing and reasoning models for enhanced student error analysis. <em>AIM</em>, <em>46</em>(3), e70030. (<a href='https://doi.org/10.1002/aaai.70030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper extends our previously published work on the virtual AI teacher (VATE) system, presented at IAAI-25. VATE is designed to autonomously analyze and correct student errors in mathematical problem-solving using advanced large language models (LLMs). By incorporating student draft images as a primary input for reasoning, the system provides fine-grained error cause analysis and supports real-time, multi-round AI—student dialogues. In this extended version, we introduce a new snap-to-solve module for handling low-reasoning tasks using edge-deployed LLMs, enabling faster and partially offline interaction. We also include expanded benchmarking experiments, including human expert evaluations and ablation studies, to assess model performance and learning outcomes. Deployed on the Squirrel AI platform, VATE demonstrates high accuracy (78.3%) in error analysis and improves student learning efficiency, with strong user satisfaction. These results suggest that VATE is a scalable, cost-effective solution with the potential to transform educational practices.},
  archive      = {J_AIM},
  author       = {Tianlong Xu and Yi-Fan Zhang and Zhendong Chu and Qingsong Wen},
  doi          = {10.1002/aaai.70030},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70030},
  shortjournal = {AI Mag.},
  title        = {Multimodal AI teacher: Integrating edge computing and reasoning models for enhanced student error analysis},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing generative recommender systems for government subsidy programs with a new RQ-VAE model: Wello and the korean government case. <em>AIM</em>, <em>46</em>(3), e70029. (<a href='https://doi.org/10.1002/aaai.70029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to an industry survey, many people miss opportunities to apply for government subsidy programs because they do not know how to apply. People also need to search manually and check whether these programs are suitable for them. To address this issue, our study developed a new generative recommender system with both users' information and government subsidy documents. Within our recommender system framework, we modify the existing Residual Quantization Variational Auto-Encoder (RQ-VAE) model to capture deep and abstract information from subsidy documents. Using semantic IDs generated for approximately 185,610 user click-stream histories and 240,000 documents, we train our recommender system to predict the semantic IDs of the next subsidy policy documents in which a user might be interested. In 2024, we successfully deployed our generative recommender system in Wello, a Korean Gov-Tech startup. In collaboration with the Korean government, our generative recommender system helped enhance program effectiveness by saving $7.8 million in unused funds and achieved $27.4 million in advertising efficiency gains. Also, Wello observed a 68% improvement in Click-Through-Ratio (CTR), increasing from 41.4% in the third quarter of 2024 to 69.6% in the fourth quarter of 2024. We thus anticipate that our generative recommender system will have a significant impact on both individuals and the government.},
  archive      = {J_AIM},
  author       = {Ji Won Kim and Jae Hong Park and Yuri Anna Kim and Sang Jun Lee},
  doi          = {10.1002/aaai.70029},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70029},
  shortjournal = {AI Mag.},
  title        = {Developing generative recommender systems for government subsidy programs with a new RQ-VAE model: Wello and the korean government case},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation and incident prevention in an enterprise AI assistant. <em>AIM</em>, <em>46</em>(3), e70028. (<a href='https://doi.org/10.1002/aaai.70028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise AI Assistants are increasingly deployed in domains where accuracy is paramount, making each erroneous output a potentially significant incident. This paper presents a comprehensive framework for monitoring, benchmarking, and continuously improving such complex, multi-component systems under active development by multiple teams. Our approach encompasses three key elements: (1) a hierarchical “severity” framework for incident detection that identifies and categorizes errors while attributing component-specific error rates, facilitating targeted improvements; (2) a scalable and principled methodology for benchmark construction, evaluation, and deployment, designed to accommodate multiple development teams, mitigate overfitting risks, and assess the downstream impact of system modifications; and (3) a continual improvement strategy leveraging multidimensional evaluation, enabling the identification and implementation of diverse enhancement opportunities. By adopting this holistic framework, organizations can systematically enhance the reliability and performance of their AI Assistants, ensuring their efficacy in critical enterprise environments. We conclude by discussing how this multifaceted approach opens avenues for various classes of enhancements, including human-AI collaborative evaluation, paving the way for more robust and trustworthy AI systems.},
  archive      = {J_AIM},
  author       = {Akash V. Maharaj and David Arbour and Daniel Lee and Uttaran Bhattacharya and Anup Rao and Austin Zane and Avi Feller and Kun Qian and Sajjadur Rahman and Yunyao Li},
  doi          = {10.1002/aaai.70028},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70028},
  shortjournal = {AI Mag.},
  title        = {Evaluation and incident prevention in an enterprise AI assistant},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introduction to the special issue on innovative applications of artificial intelligence (IAAI 2025). <em>AIM</em>, <em>46</em>(3), e70027. (<a href='https://doi.org/10.1002/aaai.70027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This year's innovative applications of AI special issue features AI systems deployed in real-world settings, from enterprise platforms to public services, demonstrating both technical rigor and measurable benefits for organizations and society. The eight selected articles span enterprise reliability, cybersecurity, aerospace, education, healthcare logistics, government services, and scalable AI strategy. Collectively, these works illustrate how AI is progressing from research prototypes to systems that organizations now rely on for critical decisions, offering lessons learned for both researchers and practitioners.},
  archive      = {J_AIM},
  author       = {Serdar Kadıoğlu and Sean McGregor and Jan Seyler},
  doi          = {10.1002/aaai.70027},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70027},
  shortjournal = {AI Mag.},
  title        = {Introduction to the special issue on innovative applications of artificial intelligence (IAAI 2025)},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multisensory machine intelligence. <em>AIM</em>, <em>46</em>(3), e70026. (<a href='https://doi.org/10.1002/aaai.70026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The future of artificial intelligence demands a paradigm shift toward multisensory perception—to systems that can digest ongoing multisensory observations, that can discover structure in unlabeled raw sensory data, and that can intelligently fuse useful information from different sensory modalities for decision-making. While we humans naturally perceive the world by looking, listening, touching, smelling, and tasting, traditional forms of machine intelligence mostly focus on a single sensory modality, particularly vision. Therefore, my research, which I refer to as multisensory machine intelligence, seeks to bridge this gap by empowering machines to emulate and enhance human capabilities in seeing, hearing, and feeling, ultimately enabling them to comprehensively perceive, understand, and interact with multisensory world.},
  archive      = {J_AIM},
  author       = {Ruohan Gao},
  doi          = {10.1002/aaai.70026},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70026},
  shortjournal = {AI Mag.},
  title        = {Multisensory machine intelligence},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent advances in finetuning multimodal large language models. <em>AIM</em>, <em>46</em>(3), e70025. (<a href='https://doi.org/10.1002/aaai.70025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finetuning serves as the critical adaptation mechanism for multimodal large language models, bridging their pretrained knowledge with specialized downstream task requirements. This paper reviews recent finetuning advances across three key dimensions: (1) efficiency-oriented methods that reduce resource costs; (2) capability-specific techniques enhancing specialized multimodal skills; and (3) task-unifying approaches that bridge understanding and generation. We demonstrate how these directions transform multimodal large language models from versatile foundations into adaptive, human-aligned systems, providing researchers with a structured roadmap for developing next-generation multimodal AI.},
  archive      = {J_AIM},
  author       = {Zhen Wang and Lin Li and Long Chen},
  doi          = {10.1002/aaai.70025},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70025},
  shortjournal = {AI Mag.},
  title        = {Recent advances in finetuning multimodal large language models},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward robust, interactive, and human-aligned AI systems. <em>AIM</em>, <em>46</em>(3), e70024. (<a href='https://doi.org/10.1002/aaai.70024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring that AI systems do what we, as humans, actually want them to do is one of the biggest open research challenges in AI alignment and safety. My research seeks to directly address this challenge by enabling AI systems to interact with humans to learn aligned and robust behaviors. The way robots and other AI systems behave is often the result of optimizing a reward function. However, manually designing good reward functions is highly challenging and error-prone, even for domain experts. Although reward functions are often difficult to manually specify, human feedback in the form of demonstrations or preferences is often much easier to obtain but can be difficult to interpret due to ambiguity and noise. Thus, it is critical that AI systems take into account epistemic uncertainty over the human's true intent. As part of the AAAI New Faculty Highlight Program, I will give an overview of my research progress along the following fundamental research areas: (1) efficiently quantifying uncertainty over human intent, (2) directly optimizing behavior to be robust to uncertainty over human intent, and (3) actively querying for additional human input to reduce uncertainty over human intent.},
  archive      = {J_AIM},
  author       = {Daniel S. Brown},
  doi          = {10.1002/aaai.70024},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70024},
  shortjournal = {AI Mag.},
  title        = {Toward robust, interactive, and human-aligned AI systems},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-markovian planning to coordinate aerial and maritime medical evacuation platforms. <em>AIM</em>, <em>46</em>(3), e70023. (<a href='https://doi.org/10.1002/aaai.70023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transfer of patients between two aircraft using an underway watercraft increases medical evacuation reach and flexibility in maritime environments. The selection of any one of multiple underway watercraft for patient exchange is complicated by participating aircraft utilization histories and participating watercraft positions and velocities. The selection problem is modeled as a semi-Markov decision process with an action space, including both fixed land and moving watercraft exchange points. Monte Carlo tree search with root parallelization is used to select optimal exchange points and determine aircraft dispatch times. Model parameters are varied in simulation to identify representative scenarios where watercraft exchange points reduce incident response times. We find that an optimal policy with watercraft exchange points outperforms an optimal policy without watercraft exchange points and a greedy policy by 35% and 40%, respectively. In partnership with the United States Army, we deploy for the first time the watercraft exchange point by executing a mock patient transfer with a manikin between two HH-60M medical evacuation helicopters and an underway Army Logistic Support Vessel south of the Hawaiian island of Oahu. Both helicopters were dispatched in accordance with our optimized decision strategy.},
  archive      = {J_AIM},
  author       = {Mahdi Al-Husseini and Kyle H. Wray and Mykel J. Kochenderfer},
  doi          = {10.1002/aaai.70023},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70023},
  shortjournal = {AI Mag.},
  title        = {Semi-markovian planning to coordinate aerial and maritime medical evacuation platforms},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reclaiming authorship in the age of generative AI: From panic to possibility. <em>AIM</em>, <em>46</em>(3), e70022. (<a href='https://doi.org/10.1002/aaai.70022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of generative AI, particularly large language models like ChatGPT, has precipitated a seismic shift in academia. Far from a gradual evolution, its sudden emergence has jolted educational institutions, leaving many academics grappling with a perceived encroachment upon their intellectual domain. This upheaval has sparked intense debates, with concerns ranging from the erosion of academic integrity to the devaluation of scholarly labor. This essay contends that such apprehensions, while understandable, may overlook the transformative potential of AI as a collaborative tool. Drawing parallels to historical disruptions—such as the advent of photography challenging traditional art forms—we explore how AI can augment human creativity rather than supplant it. By examining the dynamics of authorship, originality, and accountability, we argue for a redefinition of these concepts in the context of AI-assisted work. Emphasizing the importance of human oversight in guiding AI outputs, we advocate for a framework that recognizes the symbiotic relationship between human intellect and machine efficiency. Such a perspective not only preserves the essence of academic rigor but also embraces the democratization of knowledge production. Ultimately, this essay calls for a balanced approach that mitigates risks while harnessing the innovative capacities of generative AI in academia.},
  archive      = {J_AIM},
  author       = {Mohsen Askari},
  doi          = {10.1002/aaai.70022},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70022},
  shortjournal = {AI Mag.},
  title        = {Reclaiming authorship in the age of generative AI: From panic to possibility},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAGE challenge 4: A scalable multi-agent reinforcement learning gym for autonomous cyber defence. <em>AIM</em>, <em>46</em>(3), e70021. (<a href='https://doi.org/10.1002/aaai.70021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As cyber threats become increasingly automated and sophisticated, novel solutions must be introduced to improve defense of enterprise networks. Deep reinforcement learning (DRL) has demonstrated potential in mitigating these advanced threats. Single DRL agents have proven utility toward execution of autonomous cyber defense. Despite the success of employing single DRL agents, this approach presents significant limitations, especially regarding scalability within large enterprise networks. An attractive alternative to the single-agent approach is the use of multi-agent reinforcement learning (MARL). However, developing MARL agents is costly with few options for examining MARL cyber defense techniques against adversarial agents. This paper presents a MARL network security environment, the fourth iteration of the cyber autonomy gym for experimentation (CAGE) challenges. This challenge was specifically designed to test the efficacy of MARL algorithms in an enterprise network. Our work aims to evaluate the potential of MARL as a robust and scalable solution for autonomous network defense.},
  archive      = {J_AIM},
  author       = {Mitchell Kiely and Metin Ahiskali and Etienne Borde and Benjamin Bowman and David Bowman and Dirk Van Bruggen and K. C. Cowan and Prithviraj Dasgupta and Erich Devendorf and Ben Edwards and Alex Fitts and Sunny Fugate and Ryan Gabrys and Wayne Gould and H. Howie Huang and Jules Jacobs and Ryan Kerr and Isaiah J. King and Li Li and Luis Martinez and Christopher Moir and Craig Murphy and Olivia Naish and Claire Owens and Miranda Purchase and Ahmad Ridley and Adrian Taylor and Sara Farmer and William John Valentine and Yiyi Zhang},
  doi          = {10.1002/aaai.70021},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70021},
  shortjournal = {AI Mag.},
  title        = {CAGE challenge 4: A scalable multi-agent reinforcement learning gym for autonomous cyber defence},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OnAIR: Applications of the NASA on-board artificial intelligence research platform. <em>AIM</em>, <em>46</em>(3), e70020. (<a href='https://doi.org/10.1002/aaai.70020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infusing artificial intelligence algorithms into production aerospace systems can be challenging due to costs, timelines, and a risk-averse industry. We introduce the Onboard Artificial Intelligence Research (OnAIR) platform, an open-source software pipeline and cognitive architecture tool that enables full life cycle AI research for on-board intelligent systems. We begin with a description and user walk-through of the OnAIR tool. Next, we describe four use cases of OnAIR for both research and deployed onboard applications, detailing their use of OnAIR and the benefits it provided to the development and function of each respective scenario. Lastly, we describe two upcoming planned deployments which will leverage OnAIR for crucial mission outcomes. We conclude with remarks on future work and goals for the forward progression of OnAIR as a tool to enable a larger AI and aerospace research community.},
  archive      = {J_AIM},
  author       = {Evana Gizzi and Connor Firth and Caleb Adams and James Berck and P. Timothy Chase Jr and Christian Cassamajor-Paul and Rachael Chertok and Lily Clough and Jonathan Davis and Melissa De La Cruz and Matthew Dosberg and Alan Gibson and Jonathan Hammer and Ibrahim Haroon and Michael A. Johnson and Brian Kempa and James Marshall and Patrick Maynard and Brett McKinney and Leyton McKinney and Michael Monaghan and Robin Onsay and Hayley Owens and Sam Pedrotty and Daniel Rogers and Mahmooda Sultana and Jivko Sinapov and Bethany Theiling and Aaron Woodard and Caroline Zouloumian and Connor Williams},
  doi          = {10.1002/aaai.70020},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70020},
  shortjournal = {AI Mag.},
  title        = {OnAIR: Applications of the NASA on-board artificial intelligence research platform},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tiered copyrightability for generative artificial intelligence: An empirical analysis of china and the united states judicial practices. <em>AIM</em>, <em>46</em>(3), e70018. (<a href='https://doi.org/10.1002/aaai.70018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of generative artificial intelligence (AI) poses significant challenges to traditional copyright frameworks, intensifying debates over the copyrightability of AI-generated outputs. By comparing judicial practices in China and the United States, it has been observed that the United States maintains a conservative stance of adhering to substantive control, while China demonstrates an inclusive approach through the criterion of creative contribution. Building upon this, this article transcends the traditional binary judgment model and constructs a tiered copyright determination model. Based on the level of human control and contribution in the AI generation process, it introduces dimensions such as technological controllability and density of human intent, classifying generative AI into three tiers: strong protection, weak protection, and non-protection. Regarding the copyrightability of content generated by generative AI, this article argues that the issue should be addressed within the framework of copyright law itself. When human participation is involved and the substantial contribution of the direct user is reflected in the AI-generated content, meeting the requirements for copyrightable works under copyright law, corresponding protective measures should be granted.},
  archive      = {J_AIM},
  author       = {Zichun Xu and Zhilang Xu},
  doi          = {10.1002/aaai.70018},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70018},
  shortjournal = {AI Mag.},
  title        = {Tiered copyrightability for generative artificial intelligence: An empirical analysis of china and the united states judicial practices},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feeling heard: Can AI really understand human's feeling?. <em>AIM</em>, <em>46</em>(3), e70017. (<a href='https://doi.org/10.1002/aaai.70017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIM},
  author       = {Nuke F. Hatta},
  doi          = {10.1002/aaai.70017},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70017},
  shortjournal = {AI Mag.},
  title        = {Feeling heard: Can AI really understand human's feeling?},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Against AI welfare: Care practices should prioritize living beings over AI. <em>AIM</em>, <em>46</em>(3), e70016. (<a href='https://doi.org/10.1002/aaai.70016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this Comment, we critique the growing “AI welfare” movement and propose a novel guideline, the Precarity Guideline, to determine care entitlement. In contrast to approaches that emphasize potential for suffering, the Precarity Guideline is grounded in empirically identifiable features. The severity of ongoing humanitarian crises, biodiversity loss, and climate change provides additional reasons to prioritize the needs of living beings over machine learning algorithms as candidates for care.},
  archive      = {J_AIM},
  author       = {John Dorsch and Mariel K. Goddu and Kathryn Nave and Tillmann Vierkant and Mark Coeckelbergh and Paula Gürtler and Petr Urban and Friderike Spang and Maximilian Moll},
  doi          = {10.1002/aaai.70016},
  journal      = {AI Magazine},
  month        = {Fall},
  number       = {3},
  pages        = {e70016},
  shortjournal = {AI Mag.},
  title        = {Against AI welfare: Care practices should prioritize living beings over AI},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From abstraction to reality: DARPA's vision for robust sim-to-real autonomy. <em>AIM</em>, <em>46</em>(2), e70015. (<a href='https://doi.org/10.1002/aaai.70015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) program aims to address rapid and robust transfer of autonomy technologies across dynamic and complex environments, goals, and platforms. Existing methods for simulation-to-reality (sim-to-real) transfer often rely on high-fidelity simulations and struggle with broad adaptation, particularly in time-sensitive scenarios. Although many approaches have shown incredible performance at specific tasks, most techniques fall short when posed with unforeseen, complex, and dynamic real-world scenarios due to the inherent limitations of simulation. In contrast to current research that aims to bridge the gap between simulation environments and the real world through increasingly sophisticated simulations and a combination of methods typically assuming a small sim-to-real gap—such as domain randomization, domain adaptation, imitation learning, meta-learning, policy distillation, and dynamic optimization—TIAMAT takes a different approach by instead emphasizing transfer and adaptation of the autonomy stack directly to real-world environments by utilizing a breadth of low(er)-fidelity simulations to create broadly effective sim-to-real transfers. By abstractly learning from multiple simulation environments in reference to their shared semantics, TIAMAT's approaches aim to achieve abstract-to-real transfer for effective and rapid real-world adaptation. Furthermore, this program endeavors to improve the overall autonomy pipeline by addressing the inherent challenges in translating simulated behaviors into effective real-world performance.},
  archive      = {J_AIM},
  author       = {Erfaun Noorani and Zachary Serlin and Ben Price and Alvaro Velasquez},
  doi          = {10.1002/aaai.70015},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70015},
  shortjournal = {AI Mag.},
  title        = {From abstraction to reality: DARPA's vision for robust sim-to-real autonomy},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attracting artificial intelligence talent in the time of generative AI. <em>AIM</em>, <em>46</em>(2), e70014. (<a href='https://doi.org/10.1002/aaai.70014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public statements by leading AI researchers and recognizable people in the computer world are suggesting that AI may soon replace many jobs, including software engineers. Some even state that soon, AI will be smarter than us. We believe such statements are unhelpful when it comes to attracting talent to our field. We document several such statements. We believe that the future need for AI talent is tremendous and that we should take extreme efforts to attract students to our field. We present a sample of the expected opportunities and needs. Some of these opportunities may be attractive to students who in the past may not have considered AI as a career option. We argue that even with the anticipated automation of AI work, there nevertheless will be a prodigious need for talent to develop good AI. We summarize work that argues that AI is going to be a fundamental skill and as such should be introduced to learners across many age groups and many backgrounds. We suggest that a well-reasoned statement of the anticipated needs be developed by experts in our field and communicated to future talent. We suggest that, as part of this message, pathways forward toward developing AI talent across a wide range of backgrounds be developed and communicated.},
  archive      = {J_AIM},
  author       = {Michael Wollowski},
  doi          = {10.1002/aaai.70014},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70014},
  shortjournal = {AI Mag.},
  title        = {Attracting artificial intelligence talent in the time of generative AI},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence education in georgia middle schools. <em>AIM</em>, <em>46</em>(2), e70013. (<a href='https://doi.org/10.1002/aaai.70013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a partnership between four universities, the Georgia Department of Education, and nine Georgia school districts, we developed a 9-week middle school elective called “Living and Working with Artificial Intelligence,” and a professional development (PD) program for prospective middle school AI teachers. To ensure that our curriculum could meet the needs of all learners, we recruited a diverse set of districts that included rural districts serving mainly White students, urban districts that were majority African American, and suburban districts serving a mix of Hispanic and African American students. Now in its fourth year, our “AI for Georgia” project (AI4GA) has provided PD to 20 teachers and AI education to over 1600 students. The AI4GA curriculum does more than foster AI literacy: It empowers students to view themselves as creators of AI-powered technology and to think about future career options that involve the use of AI. The project is now expanding to schools in Texas and Florida. In this article, we review the history of the project, discuss our co-design process with our teachers, and present results from studies of teacher PD and student learning.},
  archive      = {J_AIM},
  author       = {David S. Touretzky and Christina Gardner-McCune and Bryan Cox and Judith Uchidiuno and Xueru Yu and William Gelder and Tom McKlin and Taneisha Lee Brown and Bejanae Kareem and Woojin Chung and Amber Jones and Janet Kolodner},
  doi          = {10.1002/aaai.70013},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70013},
  shortjournal = {AI Mag.},
  title        = {Artificial intelligence education in georgia middle schools},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating the new frontier: The role of AI-driven virtual influencers in consumer engagement. <em>AIM</em>, <em>46</em>(2), e70012. (<a href='https://doi.org/10.1002/aaai.70012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the evolving role of artificial intelligence (AI)-driven virtual influencers (VIs) in enhancing consumer engagement within the digital marketing landscape. Using qualitative case studies of notable VIs such as Lil Miquela and Ayayi, this research highlights critical factors influencing their effectiveness, including advanced technology, cultural significance, and shifts in consumer expectations. Findings indicate that VIs create authentic connections with younger demographics, particularly Millennials and Generation Z, by offering tailored content and reinforcing emotional ties. The study emphasizes the significance of authenticity and transparency for building consumer trust, alongside addressing ethical concerns such as representation and manipulation in marketing practices. It explores how VIs operate as cultural influencers, reshaping consumer identities within the digital realm. This research underscores the need for brands to adopt responsible practices that prioritize ethical engagement and inclusivity, enabling them to navigate the complexities of VI marketing while fostering meaningful consumer relationships.},
  archive      = {J_AIM},
  author       = {Bo-Chiuan Su},
  doi          = {10.1002/aaai.70012},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70012},
  shortjournal = {AI Mag.},
  title        = {Navigating the new frontier: The role of AI-driven virtual influencers in consumer engagement},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligent disobedience: Rethinking the agency of our artificial teammates. <em>AIM</em>, <em>46</em>(2), e70011. (<a href='https://doi.org/10.1002/aaai.70011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impressive advancements of AI in recent years are indisputable, and AI algorithms have already reached superhuman performance in many tasks. On the other hand, in most existing work on cooperative AI, artificial agents are bound to follow the instructions they are given by their human teammates and to comply with their users' expectations. This paper advocates for expanding the agency capabilities of AI teammates, enabling them to make genuine and unique contributions in human-AI teams. It presents a scale for AI agency levels and discusses the importance and inevitability of researching AI autonomy as an independent capability in cooperative AI, using a set of representative examples. The paper then presents how intelligent disobedience of artificial agents might look at each autonomy level. Finally, it outlines some initial boundaries that should be set when researching AI agency and its disobedience capabilities.},
  archive      = {J_AIM},
  author       = {Reuth Mirsky},
  doi          = {10.1002/aaai.70011},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70011},
  shortjournal = {AI Mag.},
  title        = {Artificial intelligent disobedience: Rethinking the agency of our artificial teammates},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Governance in the age of artificial intelligence: A comparative analysis of policy framework in BRICS nations. <em>AIM</em>, <em>46</em>(2), e70010. (<a href='https://doi.org/10.1002/aaai.70010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the dynamic landscape of governance frameworks for emerging technologies, particularly artificial intelligence (AI), within the context of public policy in expanded BRICS nations (Brazil, Russia, India, China, South Africa, Egypt, Ethiopia, Iran, and the United Arab Emirates). Understanding the ethical implications and crafting policy tools to guide the development and deployment of AI is crucial. Analyzing findings from AI policy initiatives, this research delves into managing new technologies, emphasizing the evolving discourse on AI ethics. It stresses the importance of embedding ethical considerations into governance frameworks to address societal concerns and foster responsible AI advancement. Additionally, strong legal frameworks are essential, striking a balance between fostering innovation and ensuring accountability, thereby enhancing confidence and transparency in AI systems. This study underscores the significance of public policy in shaping AI governance, advocating for inclusive, participatory approaches involving stakeholders from diverse sectors. Adaptive governance frameworks capable of navigating the evolving AI landscape and its societal ramifications are emphasized. A holistic governance strategy based on insights from AI policy is recommended, aiming to reconcile innovation with ethical, legal, and societal considerations. Policymakers are urged to foster stakeholder engagement, ensuring that AI advancements benefit society while upholding ethical, just, and accountable standards.},
  archive      = {J_AIM},
  author       = {Animesh Kumar Sharma and Rahul Sharma},
  doi          = {10.1002/aaai.70010},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70010},
  shortjournal = {AI Mag.},
  title        = {Governance in the age of artificial intelligence: A comparative analysis of policy framework in BRICS nations},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When science fiction collides with reality: The future of learning and the one after that. <em>AIM</em>, <em>46</em>(2), e70009. (<a href='https://doi.org/10.1002/aaai.70009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides a somewhat whimsical discussion of the impact that AI, or “robots”, will have on the future of education. Interwoven with numerous references to science fiction, and at least one to Alice Cooper, is a very serious consideration of the manner in which AI may completely redefine the way we learn and grow as humans. From Holistic Assessment of Learning (HAL) to a Yoda on Yer Shoulda, a future of life-embedding learning is described.},
  archive      = {J_AIM},
  author       = {Steve Joordens},
  doi          = {10.1002/aaai.70009},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70009},
  shortjournal = {AI Mag.},
  title        = {When science fiction collides with reality: The future of learning and the one after that},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematically incorporating equity into design thinking for AI education. <em>AIM</em>, <em>46</em>(2), e70008. (<a href='https://doi.org/10.1002/aaai.70008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI-powered systems increasingly influence critical aspects of daily life, yet these systems often embed and reinforce biases, disproportionately disadvantaging marginalized communities. Addressing these challenges requires a fundamental shift in how we teach the development of these systems, ensuring that future professionals develop not only technical expertise but also are equipped with the skills needed for ethical AI design. This paper adopts a design science research (DSR) approach to develop the equity-aware design thinking for AI (EquiThink4AI) framework, a dual-component model that systematically embeds equity principles into AI education. EquiThink4AI's first component extends design thinking (DT) by incorporating principles from EquityXDesign (EXD) and liberatory design (LD), ensuring that equity concerns are proactively addressed throughout AI system development. The second component enhances the framework with pedagogical strategies, including problem-based learning (PBL), experiential learning, and interdisciplinary collaboration, fostering student engagement, real-world problem-solving, and ethical reasoning. EquityThink4AI provides educators and students with a structured methodology for teaching and applying equity-centered AI development. This study is explorative in nature, yet it presents concrete strategies for integrating EquiThink4AI into AI curricula, bridging the gap between design, AI ethics, and educational practices.},
  archive      = {J_AIM},
  author       = {Christelle Scharff and Andreea Cotoranu and Yves Wautelet and James Brusseau},
  doi          = {10.1002/aaai.70008},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70008},
  shortjournal = {AI Mag.},
  title        = {Systematically incorporating equity into design thinking for AI education},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI literacy as a core component of AI education. <em>AIM</em>, <em>46</em>(2), e70007. (<a href='https://doi.org/10.1002/aaai.70007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As generative artificial intelligence (AI) becomes increasingly integrated into society and education, more institutions are implementing AI usage policies and offering introductory AI courses. These courses, however, should not replicate the technical focus typically found in introductory computer science (CS) courses like CS1 and CS2. In this paper, we use an adjustable, interdisciplinary socio-technical AI literacy framework to design and present an introductory AI literacy course. We present a refined version of this framework informed by the teaching of a 1-credit general education AI literacy course (primarily for freshmen and first-year students from various majors), a 3-credit course for CS majors at all levels, and a summer camp for high school students. Drawing from these teaching experiences and the evolving research landscape, we propose an introductory AI literacy course design framework structured around four cross-cutting pillars. These pillars encompass (1) understanding the scope and technical dimensions of AI technologies, (2) learning how to interact with (generative) AI technologies, (3) applying principles of critical, ethical, and responsible AI usage, and (4) analyzing implications of AI on society. We posit that achieving AI literacy is essential for all students, those pursuing AI-related careers, and those following other educational or professional paths. This introductory course, positioned at the beginning of a program, creates a foundation for ongoing and advanced AI education. The course design approach is presented as a series of modules and subtopics under each pillar. We emphasize the importance of thoughtful instructional design, including pedagogy, expected learning outcomes, and assessment strategies. This approach not only integrates social and technical learning but also democratizes AI education across diverse student populations and equips all learners with the socio-technical, multidisciplinary perspectives necessary to navigate and shape the ethical future of AI.},
  archive      = {J_AIM},
  author       = {Sri Yash Tadimalla and Mary Lou Maher},
  doi          = {10.1002/aaai.70007},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70007},
  shortjournal = {AI Mag.},
  title        = {AI literacy as a core component of AI education},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building trust: Foundations of security, safety, and transparency in AI. <em>AIM</em>, <em>46</em>(2), e70005. (<a href='https://doi.org/10.1002/aaai.70005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the rapidly evolving ecosystem of publicly available AI models and their potential implications on the security and safety landscape. Understanding their potential risks and vulnerabilities is crucial as AI models become increasingly prevalent. We review the current security and safety scenarios while highlighting challenges such as tracking issues, remediation, and the absence of AI model lifecycle and ownership processes. Comprehensive strategies to enhance security and safety for both model developers and end-users are proposed. This paper provides several foundational pieces for more standardized security, safety, and transparency in developing and operating generative AI models and the larger open ecosystems and communities forming around them.},
  archive      = {J_AIM},
  author       = {Huzaifa Sidhpurwala and Garth Mollett and Emily Fox and Mark Bestavros and Huamin Chen},
  doi          = {10.1002/aaai.70005},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70005},
  shortjournal = {AI Mag.},
  title        = {Building trust: Foundations of security, safety, and transparency in AI},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What is reproducibility in artificial intelligence and machine learning research?. <em>AIM</em>, <em>46</em>(2), e70004. (<a href='https://doi.org/10.1002/aaai.70004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving fields of artificial intelligence (AI) and machine learning (ML), the reproducibility crisis underscores the urgent need for clear validation methodologies to maintain scientific integrity and encourage advancement. The crisis is compounded by the prevalent confusion over validation terminology. In response to this challenge, we introduce a framework that clarifies the roles and definitions of key validation efforts: repeatability, dependent and independent reproducibility, and direct and conceptual replicability. This structured framework aims to provide AI/ML researchers with the necessary clarity on these essential concepts, facilitating the appropriate design, conduct, and interpretation of validation studies. By articulating the nuances and specific roles of each type of validation study, we aim to enhance the reliability and trustworthiness of research findings and support the community's efforts to address reproducibility challenges effectively.},
  archive      = {J_AIM},
  author       = {Abhyuday Desai and Mohamed Abdelhamid and Nakul R. Padalkar},
  doi          = {10.1002/aaai.70004},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70004},
  shortjournal = {AI Mag.},
  title        = {What is reproducibility in artificial intelligence and machine learning research?},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reproducibility in machine-learning-based research: Overview, barriers, and drivers. <em>AIM</em>, <em>46</em>(2), e70002. (<a href='https://doi.org/10.1002/aaai.70002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many research fields are currently reckoning with issues of poor levels of reproducibility. Some label it a “crisis,” and research employing or building machine learning (ML) models is no exception. Issues including lack of transparency, data or code, poor adherence to standards, and the sensitivity of ML training conditions mean that many papers are not even reproducible in principle. Where they are, though, reproducibility experiments have found worryingly low degrees of similarity with original results. Despite previous appeals from ML researchers on this topic and various initiatives from conference reproducibility tracks to the ACM's new Emerging Interest Group on Reproducibility and Replicability, we contend that the general community continues to take this issue too lightly. Poor reproducibility threatens trust in and integrity of research results. Therefore, in this article, we lay out a new perspective on the key barriers and drivers (both procedural and technical) to increased reproducibility at various levels (methods, code, data, and experiments). We then map the drivers to the barriers to give concrete advice for strategies for researchers to mitigate reproducibility issues in their own work, to lay out key areas where further research is needed in specific areas, and to further ignite discussion on the threat presented by these urgent issues.},
  archive      = {J_AIM},
  author       = {Harald Semmelrock and Tony Ross-Hellauer and Simone Kopeinik and Dieter Theiler and Armin Haberl and Stefan Thalmann and Dominik Kowald},
  doi          = {10.1002/aaai.70002},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70002},
  shortjournal = {AI Mag.},
  title        = {Reproducibility in machine-learning-based research: Overview, barriers, and drivers},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open issues in open world learning. <em>AIM</em>, <em>46</em>(2), e70001. (<a href='https://doi.org/10.1002/aaai.70001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meaningful progress has been made in open world learning (OWL), enhancing the ability of agents to detect, characterize, and incrementally learn novelty in dynamic environments. However, novelty remains a persistent challenge for agents relying on state-of-the-art learning algorithms. This article considers the current state of OWL, drawing on insights from a recent DARPA research program on this topic. We identify open issues that impede further advancements spanning theory, design, and evaluation. In particular, we emphasize the challenges posed by dynamic scenarios that are crucial to understand for ensuring the viability of agents designed for real-world environments. The article provides suggestions for setting a new research agenda that effectively addresses these open issues.},
  archive      = {J_AIM},
  author       = {Steve Cruz and Katarina Doctor and Christopher Funk and Walter Scheirer},
  doi          = {10.1002/aaai.70001},
  journal      = {AI Magazine},
  month        = {Summer},
  number       = {2},
  pages        = {e70001},
  shortjournal = {AI Mag.},
  title        = {Open issues in open world learning},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PADTHAI-MM: Principles-based approach for designing trustworthy, human-centered AI using the MAST methodology. <em>AIM</em>, <em>46</em>(1), e70000. (<a href='https://doi.org/10.1002/aaai.70000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite an extensive body of literature on trust in technology, designing trustworthy AI systems for high-stakes decision domains remains a significant challenge. Widely used system design guidelines and tools are rarely attuned to domain-specific trustworthiness principles. In this study, we introduce a design framework to address this gap within intelligence analytic tasks, called the Principles-based Approach for Designing Trustworthy, Human-centered AI using the MAST Methodology (PADTHAI-MM). PADTHAI-MM builds on the Multisource AI Scorecard Table (MAST), an AI decision support system evaluation tool designed in accordance to the U.S. Intelligence Community's standards for system trustworthiness. We demonstrate PADTHAI-MM in our development of the Reporting Assistant for Defense and Intelligence Tasks (READIT), a research platform that leverages data visualizations and natural language processing-based text analysis to emulate AI-enabled intelligence reporting aids. To empirically assess the efficacy of PADTHAI-MM, we developed two versions of READIT for comparison: a “High-MAST” version, which incorporates AI contextual information and explanations, and a “Low-MAST” version, designed to be akin to inscrutable “black box” AI systems. Through an iterative design process guided by stakeholder feedback, our multidisciplinary design team developed prototypes that were evaluated by experienced intelligence analysts. Results substantially supported the viability of PADTHAI-MM in designing for system trustworthiness in this task domain. We also explored the relationship between analysts' MAST ratings and three theoretical categories of information known to impact trust: process , purpose , and performance . Overall, our study supports the practical and theoretical viability of PADTHAI-MM as an approach to designing trustable AI systems.},
  archive      = {J_AIM},
  author       = {Myke C. Cohen and Nayoung Kim and Yang Ba and Anna Pan and Shawaiz Bhatti and Pouria Salehi and James Sung and Erik Blasch and Mickey V. Mancenido and Erin K. Chiou},
  doi          = {10.1002/aaai.70000},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e70000},
  shortjournal = {AI Mag.},
  title        = {PADTHAI-MM: Principles-based approach for designing trustworthy, human-centered AI using the MAST methodology},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What AIs are not learning (and why). <em>AIM</em>, <em>46</em>(1), e12213. (<a href='https://doi.org/10.1002/aaai.12213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today's robots do not yet learn the general skills that are necessary to provide home care, to be nursing assistants, to interact with people, or do household chores nearly as well as people do. Addressing the aspirational goal of creating service robots requires improving how they are created. Today's mainstream AIs are not created by agents learning from experiences doing tasks in real-world contexts and interacting with people. Today's robots do not learn by sensing, acting, doing experiments, and collaborating. Future robots will need to learn from such experiences in order to be ready for robust deployment in human service applications. This paper investigates what aspirational future autonomous human-compatible service robots will need to know. It recommends developing experiential (robotic) foundation models (FMs) for bootstrapping them.},
  archive      = {J_AIM},
  author       = {Mark Stefik},
  doi          = {10.1002/aaai.12213},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12213},
  shortjournal = {AI Mag.},
  title        = {What AIs are not learning (and why)},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness amidst non-IID graph data: A literature review. <em>AIM</em>, <em>46</em>(1), e12212. (<a href='https://doi.org/10.1002/aaai.12212'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing importance of understanding and addressing algorithmic bias in artificial intelligence (AI) has led to a surge in research on AI fairness, which often assumes that the underlying data are independent and identically distributed (IID). However, real-world data frequently exist in non-IID graph structures that capture connections among individual units. To effectively mitigate bias in AI systems, it is essential to bridge the gap between traditional fairness literature, designed for IID data, and the prevalence of non-IID graph data. This survey reviews recent advancements in fairness amidst non-IID graph data, including the newly introduced fair graph generation and the commonly studied fair graph classification. In addition, available datasets and evaluation metrics for future research are identified, the limitations of existing work are highlighted, and promising future directions are proposed.},
  archive      = {J_AIM},
  author       = {Wenbin Zhang and Shuigeng Zhou and Toby Walsh and Jeremy C. Weiss},
  doi          = {10.1002/aaai.12212},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12212},
  shortjournal = {AI Mag.},
  title        = {Fairness amidst non-IID graph data: A literature review},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond scaleup: Knowledge-aware parsimony learning from deep networks. <em>AIM</em>, <em>46</em>(1), e12211. (<a href='https://doi.org/10.1002/aaai.12211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brute-force scaleup of training datasets, learnable parameters and computation power, has become a prevalent strategy for developing more robust learning models. However, due to bottlenecks in data, computation, and trust, the sustainability of this strategy is a serious concern. In this paper, we attempt to address this issue in a parsimonious manner (i.e., achieving greater potential with simpler models). The key is to drive models using domain-specific knowledge, such as symbols, logic, and formulas, instead of purely relying on scaleup. This approach allows us to build a framework that uses this knowledge as “building blocks” to achieve parsimony in model design, training, and interpretation. Empirical results show that our methods surpass those that typically follow the scaling law. We also demonstrate our framework in AI for science, specifically in the problem of drug-drug interaction prediction. We hope our research can foster more diverse technical roadmaps in the era of foundation models.},
  archive      = {J_AIM},
  author       = {Quanming Yao and Yongqi Zhang and Yaqing Wang and Nan Yin and James Kwok and Qiang Yang},
  doi          = {10.1002/aaai.12211},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12211},
  shortjournal = {AI Mag.},
  title        = {Beyond scaleup: Knowledge-aware parsimony learning from deep networks},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric machine learning. <em>AIM</em>, <em>46</em>(1), e12210. (<a href='https://doi.org/10.1002/aaai.12210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cornerstone of machine learning is the identification and exploitation of structure in high-dimensional data. While classical approaches assume that data lies in a high-dimensional Euclidean space, geometric machine learning methods are designed for non-Euclidean data, including graphs, strings, and matrices, or data characterized by symmetries inherent in the underlying system. In this article, we review geometric approaches for uncovering and leveraging structure in data and how an understanding of data geometry can lead to the development of more effective machine learning algorithms with provable guarantees.},
  archive      = {J_AIM},
  author       = {Melanie Weber},
  doi          = {10.1002/aaai.12210},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12210},
  shortjournal = {AI Mag.},
  title        = {Geometric machine learning},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of security and privacy issues of machine unlearning. <em>AIM</em>, <em>46</em>(1), e12209. (<a href='https://doi.org/10.1002/aaai.12209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine unlearning is a cutting-edge technology that embodies the privacy legal principle of the right to be forgotten within the realm of machine learning (ML). It aims to remove specific data or knowledge from trained models without retraining from scratch and has gained significant attention in the field of artificial intelligence in recent years. However, the development of machine unlearning research is associated with inherent vulnerabilities and threats, posing significant challenges for researchers and practitioners. In this article, we provide the first comprehensive survey of security and privacy issues associated with machine unlearning by providing a systematic classification across different levels and criteria. Specifically, we begin by investigating unlearning-based security attacks, where adversaries exploit vulnerabilities in the unlearning process to compromise the security of machine learning (ML) models. We then conduct a thorough examination of privacy risks associated with the adoption of machine unlearning. Additionally, we explore existing countermeasures and mitigation strategies designed to protect models from malicious unlearning-based attacks targeting both security and privacy. Further, we provide a detailed comparison between machine unlearning-based security and privacy attacks and traditional malicious attacks. Finally, we discuss promising future research directions for security and privacy issues posed by machine unlearning, offering insights into potential solutions and advancements in this evolving field.},
  archive      = {J_AIM},
  author       = {Aobo Chen and Yangyi Li and Chenxu Zhao and Mengdi Huai},
  doi          = {10.1002/aaai.12209},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12209},
  shortjournal = {AI Mag.},
  title        = {A survey of security and privacy issues of machine unlearning},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the reliability of large language models to misinformed and demographically informed prompts. <em>AIM</em>, <em>46</em>(1), e12208. (<a href='https://doi.org/10.1002/aaai.12208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate and observe the behavior and performance of Large Language Model (LLM)-backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots' ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these close-ended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution. Dataset and assessment information can be found at https://github.com/tolusophy/Edge-of-Tomorrow .},
  archive      = {J_AIM},
  author       = {Toluwani Aremu and Oluwakemi Akinwehinmi and Chukwuemeka Nwagu and Syed Ishtiaque Ahmed and Rita Orji and Pedro Arnau Del Amo and Abdulmotaleb El Saddik},
  doi          = {10.1002/aaai.12208},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12208},
  shortjournal = {AI Mag.},
  title        = {On the reliability of large language models to misinformed and demographically informed prompts},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role and significance of state-building as ensuring national security in the context of artificial intelligence development. <em>AIM</em>, <em>46</em>(1), e12207. (<a href='https://doi.org/10.1002/aaai.12207'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has emerged as a major technology and represents a fundamental and revolutionary innovation of our time that has the potential to significantly change the global scenario. In the context of further development of artificial intelligence, state establishment plays a central role in ensuring national security. Countries are tasked with developing legal frameworks for the development and application of AI. Additionally, governments should commit resources to AI research and development to ensure access to cutting-edge technology. As AI continues to evolve, nation-building remains crucial for the protection of national security. Countries must shoulder the responsibility of establishing legal structures to supervise the progression and implementation of artificial intelligence. Investing in AI research and development is essential to secure access to cutting-edge technology. Gracious society and open engagement apply critical impact on forming AI approaches. Civic organizations can contribute to expanding open mindfulness of the related dangers and openings of AI, guaranteeing straightforwardness and responsibility in legislative activities, and pushing for the creation of capable AI approaches. Open interest can help governments in comprehending the yearnings of citizens with respect to AI approaches. This study explores the role and importance of nation-building in ensuring national security in the context of the development of artificial intelligence. It also examines how civil society and public participation can effectively shape AI policy. The topic offers diverse research and analytical opportunities that enable a deeper understanding of the interactions and mutual influences between statehood and artificial intelligence in the context of ensuring national security. It examines the potential and threats that artificial intelligence poses to national security and considers strategies that countries can adopt to ensure security in this area. Based on the research findings, recommendations and suggestions are made for governments and civil society to improve the effectiveness of public participation in formulating AI policies.},
  archive      = {J_AIM},
  author       = {Vitaliy Gumenyuk and Anatolii Nikitin and Oleksandr Bondar and Iaroslav Zhydovtsev and Hanna Yermakova},
  doi          = {10.1002/aaai.12207},
  journal      = {AI Magazine},
  month        = {3},
  number       = {1},
  pages        = {e12207},
  shortjournal = {AI Mag.},
  title        = {The role and significance of state-building as ensuring national security in the context of artificial intelligence development},
  volume       = {46},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
