<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim">SIM - 346</h2>
<ul>
<li><details>
<summary>
(2025). Ordinal sparse neural networks for modeling gene- and imaging-environment interactions. <em>SIM</em>, <em>44</em>(23-24), e70302. (<a href='https://doi.org/10.1002/sim.70302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, gene-environment (G-E) interactions and imaging-environment (I-E) interactions play an important role in modeling disease outcomes. Substantial investigations have been made; however, there is still a lack of related studies exploring flexible nonparametric statistical methods for modeling ordinal responses, such as the tumor pathological stage. In this paper, we develop a neural network-based method for modeling ordinal responses with interaction analysis. A novel definition of the output function for the neural network is derived to predict ordinal categories. To facilitate variable selection, we employ a sparse layer within the proposed neural networks. The penalized estimation is obtained using the local quadratic approximation (LQA) algorithm. Extensive simulation studies demonstrate that the proposed method achieves competitive performance in both prediction and variable selection. We further apply our method to breast cancer (BRCA) and skin cutaneous melanoma (SKCM) datasets, examining tumor stage prediction based on G-E and I-E interaction analyses, respectively. The proposed method identifies relevant main effects and interactions, providing insights into the underlying biological mechanisms.},
  archive      = {J_SIM},
  author       = {Jiajing Xue and Yaqing Xu and Jingmao Li and Shuangge Ma and Kuangnan Fang},
  doi          = {10.1002/sim.70302},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70302},
  shortjournal = {Stat. Med.},
  title        = {Ordinal sparse neural networks for modeling gene- and imaging-environment interactions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust estimators for heterogeneous treatment effects in heteroskedastic survival data. <em>SIM</em>, <em>44</em>(23-24), e70301. (<a href='https://doi.org/10.1002/sim.70301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the increasing interest focus on personalized medicine, a number of advanced statistical methods have been developed for estimating heterogeneous treatment effects (HTEs). However, methods for estimating HTEs in medical applications are limited, as they often involve potentially censored and heteroskedastic survival outcomes. Ignoring censoring and heteroskedasticity may introduce bias into HTEs. Therefore, in this study, we proposed two doubly robust (DR) methods for estimating HTEs based on nonparametric failure time (NFT) Bayesian additive regression trees (BART). Our contributions are as follows: (1) by using NFT BART as the prediction model, we avoid many restrictive assumptions, such as linearity, proportional hazards, and homoscedasticity; (2) we extend the DR-Learner to survival data, allowing it to handle the common issue of censoring and confounding in observational data; (3) we conduct a comprehensive simulation study of the present HTEs estimation strategies using several data generation processes in which we systematically vary the sample size of the training set, treatment-specific propensity score distribution, censoring rate, unbalanced treatment assignment, complexity of the model and bias function, and heteroskedastic or homoscedastic outcome. Through simulations, we demonstrate the effectiveness and robustness of the two proposed approaches in estimating HTEs. We also conduct a real data application of individualized hypertension management on observational data from the National Health and Nutrition Examination Survey (NHANES). Consequently, the proposed methods could yield robust estimates of HTE in observational survival data.},
  archive      = {J_SIM},
  author       = {Yuhui Yang and Weiwei Hu and Zhenli Liao and Fangyao Chen},
  doi          = {10.1002/sim.70301},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70301},
  shortjournal = {Stat. Med.},
  title        = {Doubly robust estimators for heterogeneous treatment effects in heteroskedastic survival data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covariate balancing with measurement error. <em>SIM</em>, <em>44</em>(23-24), e70300. (<a href='https://doi.org/10.1002/sim.70300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there is a growing body of causal inference literature focusing on covariate balancing methods. These methods eliminate observed confounding by equalizing covariate moments between the treated and control groups. The validity of covariate balancing relies on an implicit assumption that all covariates are accurately measured, which is frequently violated in observational studies. Nevertheless, the impact of measurement error on covariate balancing is unclear, and there is no existing work on balancing mismeasured covariates adequately. In this article, we show that naively ignoring measurement error reversely increases the magnitude of covariate imbalance and induces bias to treatment effect estimation. We then propose a class of measurement error correction strategies for the existing covariate balancing methods. Theoretically, we show that these strategies successfully recover balance for all covariates and eliminate bias of treatment effect estimation. We assess the proposed correction methods in simulation studies and real data analysis.},
  archive      = {J_SIM},
  author       = {Xialing Wen and Ying Yan},
  doi          = {10.1002/sim.70300},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70300},
  shortjournal = {Stat. Med.},
  title        = {Covariate balancing with measurement error},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring agreement in diagnostics: A practical guide for researchers. <em>SIM</em>, <em>44</em>(23-24), e70299. (<a href='https://doi.org/10.1002/sim.70299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare professionals routinely perform clinical examinations and diagnostic assessments. How the findings of these assessments are interpreted can have significant implications for patient care and outcomes. A recent systematic review on reliability and agreement studies in intrapartum fetal heart rate monitoring highlighted three methodological issues: (1) confusion between the concepts of agreement and reliability, (2) lack of clarity on how agreement and reliability measures are calculated when more than two raters are involved, and (3) confidence intervals seldom reported. This paper aims to clarify how agreement measures can be computed and interpreted when the outcome is binary (e.g., normal/abnormal test result). Using a motivating example in which five experienced obstetricians assessed 20 CTGs, we demonstrate how agreement can be defined, computed, and interpreted in various scenarios. The paper further explains the relationship between agreement measures and the concept of reliability, the distinction between intra- and inter-observer studies, and approaches to make statistical inference and sample size calculations. Particular emphasis is placed on the proportion of agreement, the proportion of specific agreement and kappa coefficients. A shiny application has also been developed to support researchers in their agreement studies. This work completes existing tools such as the Guidelines for Reporting Reliability and Agreement Studies (GRRAS), the Quality Appraisal Tool for Studies of Diagnostic Reliability (QAREL) and STARD guidelines for reporting diagnostic accuracy studies. It is intended to help researchers improve the methodological quality of studies that evaluate the agreement of clinical tests.},
  archive      = {J_SIM},
  author       = {Sophie Vanbelle and Christina Hernandez Engelhart and Ellen Blix},
  doi          = {10.1002/sim.70299},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70299},
  shortjournal = {Stat. Med.},
  title        = {Measuring agreement in diagnostics: A practical guide for researchers},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A noise-tolerant inference procedure for quasi-monte carlo likelihood estimation of a joint model for multiple longitudinal markers and competing risks. <em>SIM</em>, <em>44</em>(23-24), e70298. (<a href='https://doi.org/10.1002/sim.70298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite increasingly widespread use, complex joint models for longitudinal and survival data can be difficult to estimate. Notably, this could be due to the computation of the intractable integral over random effects involved in the likelihood and whose dimensionality increases with the number of shared random effects. In this article, we propose approximating the integral over random effects through a Quasi-Monte Carlo (QMC) approach combined with a noise-tolerant Quasi-Newton algorithm to consider the likelihood randomness induced by the QMC framework. From a simulation study, we demonstrate the suitability of the noise-tolerant Quasi-Newton algorithm to estimate the parameters of a shared random-effect joint model for two longitudinal markers in the presence of two competing events. The noise-tolerant Quasi-Newton algorithm is also compared with a Quasi-Newton algorithm with common draws in the QMC approach that showed good performance. Finally, we illustrate the interest of the noise-tolerant Quasi-Newton algorithm on kidney transplantation data. We jointly modeled the evolution of serum creatinine and donor-specific antibody immunization, as well as their associations with the cause-specific risks of graft failure and death with a functioning graft, using data from the French prospective and observational DIVAT cohort of kidney transplant recipients. The proposed noise-tolerant inference procedure for QMC likelihood estimation is shown to be relevant for estimating a joint model with multiple longitudinal markers and competing risks.},
  archive      = {J_SIM},
  author       = {L. Chabeau and P. Rinder and S. Desmée and M. Giral and E. Dantan},
  doi          = {10.1002/sim.70298},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70298},
  shortjournal = {Stat. Med.},
  title        = {A noise-tolerant inference procedure for quasi-monte carlo likelihood estimation of a joint model for multiple longitudinal markers and competing risks},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A practical framework to design immunization studies based on the beta distribution. <em>SIM</em>, <em>44</em>(23-24), e70293. (<a href='https://doi.org/10.1002/sim.70293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An optimally designed experiment reaches results quicker, at a lower cost, or with fewer observations and is therefore crucial in maximizing resource efficiency in research. In immunization studies, the primary goal is often to characterize antibody kinetics—the change in antibody concentration over time. However, nonlinear models for antibody kinetics present substantial challenges for study design, particularly the need to provide information on the parameters of interest. We propose a novel framework to facilitate the design of immunization studies using simple, understandable information. We assume that the mean antibody concentration follows the structural form of the beta density until reaching a plateau. Using the time and height of the maximum and the time and height of the plateau, we can uniquely determine the antibody kinetics curve. Optimal sampling schedules are determined using D-optimality, with D-efficiency used to compare designs. In a robustness analysis across 12 scenarios, we analyzed the framework's sensitivity to misspecification in the initial information. When misspecifying one parameter at a time, the median D-efficiencies exceeded 0.95 and the first quartiles were greater than or equal to 0.9 for all parameters, highlighting the robustness of the framework. Misspecification in the height of the plateau and time of the maximum affected the D-efficiency the most. The great advantage of the framework is that we only need intuitive information from the medical professionals to design an immunization study, in which determining the antibody kinetics is the main goal.},
  archive      = {J_SIM},
  author       = {Stefan Embacher and Andrea Berghold and Kirsten Maertens and Sereina A. Herzog},
  doi          = {10.1002/sim.70293},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70293},
  shortjournal = {Stat. Med.},
  title        = {A practical framework to design immunization studies based on the beta distribution},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). False discovery estimation in record linkage. <em>SIM</em>, <em>44</em>(23-24), e70292. (<a href='https://doi.org/10.1002/sim.70292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating data from multiple sources expands research opportunities at low cost. However, due to different data collection processes and privacy constraints, unique identifiers are unavailable. Record linkage (RL) algorithms address this by probabilistically linking records based on partially identifying variables. Since these variables lack the strength to perfectly combine information, RL procedures yield an imperfect set of linked records. Therefore, assessing the false discovery proportion (FDP) in RL is crucial for ensuring the reliability of subsequent analyses. In this paper, we introduce a novel method for estimating the FDP in RL for two overlapping data sets. We synthesize data from their estimated empirical distribution and use it along with real data in the linkage process. Since synthetic records cannot form links with real entities, they provide a means to estimate the amount of falsely linked pairs. Notably, this method applies to all RL techniques and across diverse settings where links and non-links have similar distributions–typical in complex tasks with poorly discriminative linking variables and multiple records sharing similar information while representing different entities. By identifying the FDP in RL and selecting suitable model parameters, our approach enables to assess and improve the reliability of linked data. We evaluate its performance using established RL algorithms and benchmark data applications before deploying it to link siblings from the Netherlands Perinatal Registry, where the reliability of previous RL applications has never been confirmed. Through this application, we highlight the importance of accounting for linkage errors when studying mother-child dynamics in healthcare records.},
  archive      = {J_SIM},
  author       = {Kayané Robach and Michel H. Hof and Mark A. van de Wiel},
  doi          = {10.1002/sim.70292},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70292},
  shortjournal = {Stat. Med.},
  title        = {False discovery estimation in record linkage},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating risk factors for pathogenic dose accrual from longitudinal data. <em>SIM</em>, <em>44</em>(23-24), e70291. (<a href='https://doi.org/10.1002/sim.70291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating risk factors for the incidence of a disease is crucial for understanding its etiology. For diseases caused by enteric pathogens, off-the-shelf statistical model-based approaches do not consider the biological mechanisms through which infection occurs and thus can only be used to make comparatively weak statements about the association between risk factors and incidence. Building off of established work in quantitative microbiological risk assessment, we propose a new approach to determining the association between risk factors and dose accrual rates. Our more mechanistic approach achieves a higher degree of biological plausibility, incorporates currently ignored sources of variability, and provides regression parameters that are easily interpretable as the dose accrual rate ratio due to changes in the risk factors under study. We also describe a method for leveraging information across multiple pathogens. The proposed methods are available as an R package at https://github.com/dksewell/dare . Our simulation study shows unacceptable coverage rates from generalized linear models, while the proposed approach empirically maintains the nominal rate even when the model is misspecified. Finally, we demonstrated our proposed approach by applying our method to infant data obtained through the PATHOME study ( https://reporter.nih.gov/project-details/10227256 ), discovering the impact of various environmental factors on infant enteric infections.},
  archive      = {J_SIM},
  author       = {Daniel K. Sewell and Kelly K. Baker},
  doi          = {10.1002/sim.70291},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70291},
  shortjournal = {Stat. Med.},
  title        = {Estimating risk factors for pathogenic dose accrual from longitudinal data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What's the weight? estimating controlled outcome differences in complex surveys for health disparities research. <em>SIM</em>, <em>44</em>(23-24), e70289. (<a href='https://doi.org/10.1002/sim.70289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we are motivated by the problem of estimating racial disparities in health outcomes, specifically the average controlled difference (ACD) in telomere length between Black and White individuals, using data from the National Health and Nutrition Examination Survey (NHANES). To do so, we build a propensity for race to properly adjust for other social determinants while characterizing the controlled effect of race on telomere length. Propensity score methods are broadly employed with observational data as a tool to achieve covariate balance, but how to implement them in complex surveys is less studied—in particular, when the survey weights depend on the group variable under comparison (as the NHANES sampling scheme depends on self-reported race). We propose identification formulas to properly estimate the ACD in outcomes between Black and White individuals, with appropriate weighting for both covariate imbalance across the two racial groups and generalizability . Via extensive simulation, we show that our proposed methods outperform traditional analytic approaches in terms of bias, mean squared error, and coverage when estimating the ACD for our setting of interest. In our data, we find that evidence of racial differences in telomere length between Black and White individuals attenuates after accounting for confounding by socioeconomic factors and utilizing appropriate propensity score and survey weighting techniques. Software to implement these methods and code to reproduce our results can be found in the R package svycdiff , available through the Comprehensive R Archive Network (CRAN) at cran.r-project.org/web/packages/svycdiff/ , or in a development version on GitHub at github.com/salernos/svycdiff .},
  archive      = {J_SIM},
  author       = {Stephen Salerno and Emily K. Roberts and Belinda L. Needham and Tyler H. McCormick and Fan Li and Bhramar Mukherjee and Xu Shi},
  doi          = {10.1002/sim.70289},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70289},
  shortjournal = {Stat. Med.},
  title        = {What's the weight? estimating controlled outcome differences in complex surveys for health disparities research},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep mixture of linear mixed models for complex longitudinal data. <em>SIM</em>, <em>44</em>(23-24), e70288. (<a href='https://doi.org/10.1002/sim.70288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixtures of linear mixed models are widely used for modeling longitudinal data for which observation times differ between subjects. In typical applications, temporal trends are described using a basis expansion, with basis coefficients treated as random effects varying by subject. Additional random effects can describe variation between mixture components or other known sources of variation in complex designs. A key advantage of these models is that they provide a natural mechanism for clustering. Current versions of mixtures of linear mixed models are not specifically designed for the case where there are many observations per subject and complex temporal trends, which require a large number of basis functions to capture. In this case, the subject-specific basis coefficients are a high-dimensional random effects vector, for which the covariance matrix is hard to specify and estimate, especially if it varies between mixture components. To address this issue, we consider the use of deep mixture of factor analyzers models as a prior for the random effects. The resulting deep mixture of linear mixed models is well suited for high-dimensional settings, and we describe an efficient variational inference approach to posterior computation. The efficacy of the method is demonstrated in biomedical applications and on simulated data.},
  archive      = {J_SIM},
  author       = {Lucas Kock and Nadja Klein and David J. Nott},
  doi          = {10.1002/sim.70288},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70288},
  shortjournal = {Stat. Med.},
  title        = {Deep mixture of linear mixed models for complex longitudinal data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating and evaluating counterfactual prediction models. <em>SIM</em>, <em>44</em>(23-24), e70287. (<a href='https://doi.org/10.1002/sim.70287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual prediction methods are required when a model will be deployed in a setting where treatment policies differ from the setting where the model was developed, or when a model provides predictions under hypothetical interventions to support decision-making. However, estimating and evaluating counterfactual prediction models is challenging because, unlike traditional (factual) prediction, one does not observe the potential outcomes for all individuals under all treatment strategies of interest. Here, we discuss how to estimate a counterfactual prediction model, how to assess the model's performance, and how to perform model and tuning parameter selection. We provide identification and estimation results for counterfactual prediction models and for multiple measures of counterfactual model performance, including loss-based measures, the area under the receiver operating characteristic curve, and the calibration curve. Importantly, our results allow valid estimates of model performance under counterfactual intervention even if the candidate prediction model is misspecified, permitting a wider array of use cases. We illustrate these methods using simulation and apply them to the task of developing a statin-naïve risk prediction model for cardiovascular disease.},
  archive      = {J_SIM},
  author       = {Christopher B. Boyer and Issa J. Dahabreh and Jon A. Steingrimsson},
  doi          = {10.1002/sim.70287},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70287},
  shortjournal = {Stat. Med.},
  title        = {Estimating and evaluating counterfactual prediction models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation of additive shared-frailty models for recurrent event data with dependent censoring. <em>SIM</em>, <em>44</em>(23-24), e70286. (<a href='https://doi.org/10.1002/sim.70286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event data with dependent censoring frequently arise in medical follow-up studies. In analyzing such data, one main challenge is addressing the complex dependencies among the recurrent events, failure events, and censoring events. In this paper, we focus on additive shared-frailty models for recurrent event processes and failure times, and propose a robust estimation procedure that accommodates censoring times dependent on both recurrent and failure events, even after conditioning on observed covariates. Notably, our method does not require specifying the exact dependence structure between censoring and recurrent/failure times, nor does it assume a particular frailty distribution. We show that the resulting estimates are consistent and asymptotically normal. We further assess the method's finite-sample performance through simulation studies, and illustrate its practical utility with a hospitalization dataset.},
  archive      = {J_SIM},
  author       = {Xin Chen and Jieli Ding and Liuquan Sun},
  doi          = {10.1002/sim.70286},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70286},
  shortjournal = {Stat. Med.},
  title        = {Robust estimation of additive shared-frailty models for recurrent event data with dependent censoring},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic prediction using functional latent trait joint models for multivariate longitudinal outcomes: An application to parkinson's disease. <em>SIM</em>, <em>44</em>(23-24), e70285. (<a href='https://doi.org/10.1002/sim.70285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progressive and multifaceted nature of Parkinson's disease (PD) calls for the integration of diverse data types, including continuous, ordinal, and binary, in longitudinal studies for a comprehensive understanding of symptom progression and disease trajectory. Significant terminal events, such as severe disability or mortality, highlight the need for joint modeling approaches that simultaneously address multivariate outcomes and time-to-event data. We introduce functional latent trait model-joint model (FLTM-JM), a novel joint modeling framework based on the functional latent trait model (FLTM), to jointly analyze multivariate longitudinal data and survival outcomes. The FLTM component leverages a non-parametric, function-on-scalar regression framework, enabling flexible modeling of complex relationships between covariates and patient outcomes over time. This joint modeling approach supports dynamic, subject-specific predictions, offering valuable insights for personalized treatment strategies. Applied to Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS) data from the Parkinson's Progression Markers Initiative (PPMI), our model effectively identifies the influence of key covariates and demonstrates the utility of dynamic predictions in clinical decision-making. Extensive simulation studies validate the accuracy, robustness, and computational efficiency of FLTM-JM, even under model misspecification.},
  archive      = {J_SIM},
  author       = {Mohammad Samsul Alam and Dongrak Choi and Salil Koner and Sheng Luo},
  doi          = {10.1002/sim.70285},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70285},
  shortjournal = {Stat. Med.},
  title        = {Dynamic prediction using functional latent trait joint models for multivariate longitudinal outcomes: An application to parkinson's disease},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new test for assessing the covariate effect in ROC curves. <em>SIM</em>, <em>44</em>(23-24), e70284. (<a href='https://doi.org/10.1002/sim.70284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ROC curve is a statistical tool that analyzes the accuracy of a diagnostic test in which a variable is used to decide whether an individual is healthy or not. Along with that diagnostic variable, it is usual to have information on some other covariates. In some situations, it is advisable to incorporate that information into the study, as the performance of the ROC curves can be affected by them. Using the covariate-adjusted, the covariate-specific, or the pooled ROC curves, we discuss the implications of excluding or including the covariates in the analysis. Motivated by the above, a new test for comparing the covariate-adjusted and the pooled ROC curve is proposed, and the problem is illustrated by analyzing a real database.},
  archive      = {J_SIM},
  author       = {Arís Fanjul-Hevia and Juan Carlos Pardo-Fernández and Wenceslao González-Manteiga},
  doi          = {10.1002/sim.70284},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70284},
  shortjournal = {Stat. Med.},
  title        = {A new test for assessing the covariate effect in ROC curves},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity analyses for missing in repeatedly measured outcome data. <em>SIM</em>, <em>44</em>(23-24), e70282. (<a href='https://doi.org/10.1002/sim.70282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss practical aspects of conducting sensitivity analyses for missing data with a repeatedly measured outcome. Our motivation is a SMART trial with a repeatedly measured outcome subject to missingness. We discuss and describe delta-based controlled imputation approaches to conducting sensitivity analyses for such trials that typically use linear mixed models for their primary analysis. We find that delta-based sensitivity analyses for trials with repeatedly measured outcome variables are enhanced by using MICE for the imputation. Further, including last-observed-before-time covariates is critical for a repeatedly observed outcome. We also develop some novel metrics for judging the adequacy of sensitivity analyses. Trial Registration: Tailoring Mobile Health Technology to Reduce Obesity and Improve Cardiovascular Health in Resource-Limited Neighborhood Environments: NCT03288207.},
  archive      = {J_SIM},
  author       = {James F. Troendle and Aparajita Sur and Eric S. Leifer and Tiffany Powell-Wiley},
  doi          = {10.1002/sim.70282},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70282},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity analyses for missing in repeatedly measured outcome data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accounting for misclassification of cause of death in weighted cumulative incidence functions for causal analyses. <em>SIM</em>, <em>44</em>(23-24), e70281. (<a href='https://doi.org/10.1002/sim.70281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misclassification between causes of death can produce bias in estimated cumulative incidence functions. When estimating causal quantities, such as comparing the cumulative incidence of death due to specific causes under interventions, such bias can lead to suboptimal decision making. Here, a consistent semiparametric estimator of the cumulative incidence function under interventions in settings with misclassification between two event types is presented. The measurement parameters for this estimator can be informed by validation data or expert knowledge. Moreover, a modified bootstrap approach to variance estimation is proposed for confidence interval construction. The proposed estimator was applied to estimate the cumulative incidence of AIDS-related mortality in the Multicenter AIDS Cohort Study under single- versus combination-drug antiretroviral therapy regimens that may be subject to confounding. The proposed estimator is shown to be consistent and performed well in finite samples via a series of simulation experiments.},
  archive      = {J_SIM},
  author       = {Jessie K. Edwards and Bonnie E. Shook-Sa and Giorgos Bakoyannis and Paul N. Zivich and Michael E. Herce and Stephen R. Cole},
  doi          = {10.1002/sim.70281},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70281},
  shortjournal = {Stat. Med.},
  title        = {Accounting for misclassification of cause of death in weighted cumulative incidence functions for causal analyses},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative inference for accelerated failure time model using clinical center-level summary statistics. <em>SIM</em>, <em>44</em>(23-24), e70279. (<a href='https://doi.org/10.1002/sim.70279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-center clinical research offers numerous advantages, including the ability to obtain a larger combined sample size and to reduce center-specific insufficiency and imbalance in event rates, ultimately leading to more robust and generalizable findings. This paper develops a collaborative analytic framework for survival data analysis using summary statistics and the Accelerated Failure Time (AFT) model, a popular alternative to the Cox proportional hazards model for the analysis of time-to-event data. The AFT model directly accounts for the effects of the covariates on times to event, rather than through hazard functions, with no proportionality assumption required compared to the Cox model. Given that it bypasses the construction of partial likelihood, it gives rise to more flexibility in integrative analyses of survival data collected from multiple clinical sites. Our proposed distributed inference method focuses on a class of parametric AFT models with Weibull, log-normal, and log-logistic distributions for time-to-event outcomes, with a distributed likelihood ratio test established under the generalized gamma distribution to assess the goodness-of-fit across different candidate parametric AFT models. We present large-sample properties for the proposed distributed method and illustrate its performance through simulation experiments and a real-world data example on kidney transplantation.},
  archive      = {J_SIM},
  author       = {Mengtong Hu and Xu Shi and Ziyang Gong and Peter X.-K. Song},
  doi          = {10.1002/sim.70279},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70279},
  shortjournal = {Stat. Med.},
  title        = {Collaborative inference for accelerated failure time model using clinical center-level summary statistics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling the role of baseline risk and additional study-level covariates in meta-analysis of treatment effects. <em>SIM</em>, <em>44</em>(23-24), e70278. (<a href='https://doi.org/10.1002/sim.70278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between the treatment effect and the baseline risk is a recognized tool to investigate the heterogeneity of treatment effects in meta-analyses of clinical trials. Since the baseline risk is difficult to measure, a proxy is adopted, which is based on the rate of events for the subject under the control condition. The use of the proxy in terms of aggregated information at the study level implies that the data are affected by measurement errors, a problem that the literature has explored and addressed in recent years. This paper proposes an extension of the classical meta-analysis with baseline risk information, which includes additional study-specific covariates other than the rate of events to explain heterogeneity. Likelihood-based inference is carried out by including measurement error correction techniques necessary to prevent unreliable inference due to the measurement errors affecting the covariates summarized at the study level. Within-study covariances between risk measures and the covariate components are computed using Taylor expansions based on study-level covariate subgroup summary information. When such information is not available and, more generally, in order to reduce computational difficulties, a pseudo-likelihood solution is developed under a working independence assumption between the observed error-prone measures. The performance of the methods is investigated in a series of simulation studies under different specifications for the sample size, the between-study heterogeneity, and the underlying risk distribution. They are applied to a meta-analysis about the association between COVID-19 and schizophrenia.},
  archive      = {J_SIM},
  author       = {Phuc T. Tran and Annamaria Guolo},
  doi          = {10.1002/sim.70278},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70278},
  shortjournal = {Stat. Med.},
  title        = {Modeling the role of baseline risk and additional study-level covariates in meta-analysis of treatment effects},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive biomarker-based design for early phase clinical trials. <em>SIM</em>, <em>44</em>(23-24), e70275. (<a href='https://doi.org/10.1002/sim.70275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying and quantifying predictive biomarkers is a critical issue of Precision Medicine approaches and patient-centric clinical development strategies. Early phase adaptive designs can improve trial efficiency by allowing for adaptations during the course of the trial. In this work, we are interested in adaptations based on interim analysis permitting a refinement of the existing study population according to their predictive biomarkers. At an early stage, the goal is not to precisely define the target population, but to not miss an efficacy signal that might be limited to a biomarker subgroup. In this work, we propose a one-arm two-stage early phase biomarker-guided design in the setting of an oncology trial where at the time of the interim analysis, several decisions can be made regarding stopping the entire trial early or continuing to recruit patients from the full or a selected patient population. Via simulations, we show that, although the sample size is limited, the proposed design leads to better decision-making compared to a classical design that does not consider an enrichment expansion.},
  archive      = {J_SIM},
  author       = {Alessandra Serra and Gaëlle Saint-Hilary and Sandrine Guilleminot and Julia Geronimi and Pavel Mozgunov},
  doi          = {10.1002/sim.70275},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70275},
  shortjournal = {Stat. Med.},
  title        = {Adaptive biomarker-based design for early phase clinical trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Specification of estimands for complex disease processes using multistate models and utility functions. <em>SIM</em>, <em>44</em>(23-24), e70269. (<a href='https://doi.org/10.1002/sim.70269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex diseases, individuals are often at risk of several types of possibly semi-competing events and may experience recurrent symptomatic episodes. This complex disease course makes it challenging to define target estimands for clinical trials. While composite endpoints are routinely adopted, recent innovations involving the win ratio and other methods based on ranking the disease course have received considerable attention. We emphasize the usefulness of multistate models for addressing challenges arising in complex diseases, along with the simplicity and interpretability that come from defining utilities to synthesize evidence of treatment effects on different aspects of the disease process. Robust variance estimation based on the infinitesimal jackknife means that such methods can be used as the basis of primary analyses of clinical trials. We illustrate the use of utilities for the assessment of bleeding outcomes in a trial of cancer patients with thrombocytopenia.},
  archive      = {J_SIM},
  author       = {Alexandra Bühler and Richard J. Cook and Jerald F. Lawless},
  doi          = {10.1002/sim.70269},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70269},
  shortjournal = {Stat. Med.},
  title        = {Specification of estimands for complex disease processes using multistate models and utility functions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChatGPT as a tool for biostatisticians: A tutorial on applications, opportunities, and limitations. <em>SIM</em>, <em>44</em>(23-24), e70263. (<a href='https://doi.org/10.1002/sim.70263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern large language models (LLMs) have reshaped the workflows of people across countless fields—and biostatistics is no exception. These models offer novel support in drafting study plans, generating software code, or writing reports. However, reliance on LLMs carries the risk of inaccuracies due to potential hallucinations that may produce fabricated “facts”, leading to erroneous statistical statements and conclusions. Such errors could compromise the high precision and transparency fundamental to our field. This tutorial aims to illustrate the impact of LLM-based applications on various contemporary biostatistical tasks. We will explore both the risks and opportunities presented by this new era of artificial intelligence. Our ultimate conclusion emphasizes that advanced applications should only be used in combination with sufficient background knowledge. Over time, consistently verifying LLM outputs may lead to an appropriately calibrated trust in these tools among users.},
  archive      = {J_SIM},
  author       = {Dennis Dobler and Harald Binder and Anne-Laure Boulesteix and Jan-Bernd Igelmann and David Köhler and Ulrich Mansmann and Markus Pauly and André Scherag and Matthias Schmid and Amani Al Tawil and Susanne Weber},
  doi          = {10.1002/sim.70263},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70263},
  shortjournal = {Stat. Med.},
  title        = {ChatGPT as a tool for biostatisticians: A tutorial on applications, opportunities, and limitations},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking the handling of method failure in comparison studies. <em>SIM</em>, <em>44</em>(23-24), e70257. (<a href='https://doi.org/10.1002/sim.70257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparison studies in methodological research are intended to compare methods in an evidence-based manner to help data analysts select a suitable method for their application. To provide trustworthy evidence, they must be carefully designed, implemented, and reported, especially given the many decisions made in planning and running. A common challenge in comparison studies is to handle the “failure” of one or more methods to produce a result for some (real or simulated) data sets, such that their performances cannot be measured in those instances. Despite an increasing emphasis on this topic in recent literature (focusing on non-convergence as a common manifestation), there is little guidance on proper handling and interpretation, and reporting of the chosen approach is often neglected. This paper aims to fill this gap and offers practical guidance on handling method failure in comparison studies. After exploring common handlings across various published comparison studies from classical statistics and predictive modeling, we show that the popular approaches of discarding data sets yielding failure (either for all or the failing methods only) and imputing are inappropriate in most cases. We then recommend a different perspective on method failure—viewing it as the result of a complex interplay of several factors rather than just its manifestation. Building on this, we provide recommendations on more adequate handling of method failure derived from realistic considerations. In particular, we propose considering fallback strategies that directly reflect the behavior of real-world users. Finally, we illustrate our recommendations and the dangers of inadequate handling of method failure through two exemplary comparison studies.},
  archive      = {J_SIM},
  author       = {Milena Wünsch and Moritz Herrmann and Elisa Noltenius and Mattia Mohr and Tim P. Morris and Anne-Laure Boulesteix},
  doi          = {10.1002/sim.70257},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70257},
  shortjournal = {Stat. Med.},
  title        = {Rethinking the handling of method failure in comparison studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Determining the optimal sequence of multiple tests. <em>SIM</em>, <em>44</em>(23-24), e70254. (<a href='https://doi.org/10.1002/sim.70254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of multiple tests can improve medical decision making by balancing the benefits of correctly treating ill patients and avoiding unnecessary treatment for healthy individuals against the potential harms of missed diagnoses, inappropriate treatments, and the costs and risks associated with testing. We quantify the incremental net benefit (INB) of single and multiple tests by accounting for a patient's pre-test probability of disease and the associated benefits, harms, and cost of treatment and testing. We decompose the INB into two components: one that captures the value of information provided by the test, independent of the cost and possible harm of testing, and another that accounts for test costs and harm. Next, we examine conjunctive, disjunctive, and majority aggregation functions, demonstrating their application through examples in prostate cancer, colorectal cancer, and stable coronary artery disease diagnostics. Our approach complements traditional threshold and decision-curve analysis by varying both the pre-test probability of disease and the cost-benefit trade-off of treatment to identify the region over which a given test provides the highest INB. Using empirical test and cost data, we compute decision boundaries to determine when conjunctive, disjunctive, majority, or even single tests are optimal, and, for combinations of tests, in what order they should be administered. In all three application examples, we find that the optimal choice and sequence of tests jointly depend on the probability of disease and the cost-benefit trade-off of treatment. An online tool that visualizes the INB for combined tests is available at https://optimal-testing.streamlit.app/ .},
  archive      = {J_SIM},
  author       = {Lucas Böttcher and Stefan Felder},
  doi          = {10.1002/sim.70254},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70254},
  shortjournal = {Stat. Med.},
  title        = {Determining the optimal sequence of multiple tests},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Difference-in-differences for health policy and practice: A review of modern methods. <em>SIM</em>, <em>44</em>(23-24), e70247. (<a href='https://doi.org/10.1002/sim.70247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Difference-in-differences (DiD) is a popular observational causal inference method in health policy, employed to evaluate the real-world impact of policies and programs. To estimate treatment effects, DiD relies on a “parallel trends assumption” that treatment and comparison groups would have had parallel trajectories on average in the absence of an intervention. Recent years have seen both growing use of DiD in health policy and medicine and rapid advancements in DiD methods. To support DiD implementation in these fields, this paper reviews and synthesizes best practices and recent innovations. We provide recommendations to practitioners in four areas: (1) assessing causal assumptions; (2) adjusting for covariates and other approaches to relax causal assumptions; (3) accounting for staggered treatment timing; and (4) conducting robust inference, especially when normal-based clustered standard errors are inappropriate. For each, we explain challenges and common pitfalls in traditional DiD and recommend methods to address these. We explore current treatment of these topics through a focused literature review of medical DiD studies.},
  archive      = {J_SIM},
  author       = {Shuo Feng and Ishani Ganguli and Youjin Lee and John Poe and Andrew Ryan and Alyssa Bilinski},
  doi          = {10.1002/sim.70247},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70247},
  shortjournal = {Stat. Med.},
  title        = {Difference-in-differences for health policy and practice: A review of modern methods},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A corrected score approach for proportional hazards model with error-contaminated covariates subject to detection limits. <em>SIM</em>, <em>44</em>(23-24), e70243. (<a href='https://doi.org/10.1002/sim.70243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis under the proportional hazards model, covariates may be subject to both measurement error and detection limits. Most existing approaches only address one of these two complications and can lead to substantial bias and erroneous inference when dealing with both simultaneously. There is very limited research that addresses both these problems at the same time. These approaches are exclusively based on likelihood and require distribution assumptions on the underlying true covariates, as well as restricted independence assumptions on the censoring time. We propose a novel corrected score approach that relieves such stringent assumptions and is simpler in computation. The estimator is shown to be consistent and asymptotically normal. The finite sample performance of the proposed estimator is assessed through simulation studies and illustrated by application to data from an AIDS clinical trial. The approach can be used in the case of replicate data or instrumental data. It can also be extended to more general models and outcomes.},
  archive      = {J_SIM},
  author       = {Xiao Song and Ching-Yun Wang},
  doi          = {10.1002/sim.70243},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70243},
  shortjournal = {Stat. Med.},
  title        = {A corrected score approach for proportional hazards model with error-contaminated covariates subject to detection limits},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combined P-value functions for compatible effect estimation and hypothesis testing in drug regulation. <em>SIM</em>, <em>44</em>(23-24), e70224. (<a href='https://doi.org/10.1002/sim.70224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-trials rule in drug regulation requires statistically significant results from two pivotal trials to demonstrate efficacy. However, it is unclear how the effect estimates from both trials should be combined to quantify the drug effect. Fixed-effect meta-analysis is commonly used but may yield confidence intervals that exclude the value of no effect even when the two-trials rule is not fulfilled. We systematically address this by recasting the two-trials rule and meta-analysis in a unified framework of combined p -value functions, where they are variants of Wilkinson's and Stouffer's combination methods, respectively. This allows us to obtain compatible combined p -values, effect estimates, and confidence intervals, which we derive in closed-form. Additionally, we provide new results for Edgington's, Fisher's, Pearson's, and Tippett's p -value combination methods. When both trials have the same true effect, all methods can consistently estimate it, although some show bias. When true effects differ, the two-trials rule and Pearson's method are conservative (converging to the less extreme effect), Fisher's and Tippett's methods are anti-conservative (converging to the more extreme effect), and Edgington's method and meta-analysis are balanced (converging to a weighted average). Notably, Edgington's confidence intervals always asymptotically include the individual trial effects, while meta-analytic confidence intervals shrink to a point at the weighted average effect. We conclude that all of these methods may be appropriate depending on the estimand of interest. We implement combined p -value function inference for two trials in the R package twotrials , allowing researchers to easily perform compatible hypothesis testing and effect estimation.},
  archive      = {J_SIM},
  author       = {Samuel Pawel and Małgorzata Roos and Leonhard Held},
  doi          = {10.1002/sim.70224},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70224},
  shortjournal = {Stat. Med.},
  title        = {Combined P-value functions for compatible effect estimation and hypothesis testing in drug regulation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An approach to design adaptive clinical trials with time-to-event outcomes based on a general bayesian posterior distribution. <em>SIM</em>, <em>44</em>(23-24), e70207. (<a href='https://doi.org/10.1002/sim.70207'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are an integral component of medical research. Trials require careful design to, for example, maintain the safety of participants and to use resources efficiently. Adaptive clinical trials are often more efficient and ethical than standard or non-adaptive trials because they can require fewer participants, target more promising treatments, and stop early with sufficient evidence of effectiveness or harm. The design of adaptive trials is usually undertaken via simulation, which requires assumptions about the data-generating process to be specified a priori. Unfortunately, if such assumptions are misspecified, then the resulting trial design may not perform as expected, leading to, for example, reduced statistical power or an increased Type I error. Motivated by a clinical trial of a vaccine to protect against gastroenteritis in infants, we propose an approach to design adaptive clinical trials with time-to-event outcomes without needing to explicitly define the data-generating process. To facilitate this, we consider trial design within a general Bayesian framework where inference about the treatment effect is based on the partial likelihood. As a result, inference is robust to the form of the baseline hazard function, and we exploit this property to undertake trial design when the data-generating process is only implicitly defined. The benefits of this approach are demonstrated via an illustrative example and via redesigning our motivating clinical trial.},
  archive      = {J_SIM},
  author       = {James M. McGree and Antony M. Overstall and Mark Jones and Robert K. Mahar},
  doi          = {10.1002/sim.70207},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23-24},
  pages        = {e70207},
  shortjournal = {Stat. Med.},
  title        = {An approach to design adaptive clinical trials with time-to-event outcomes based on a general bayesian posterior distribution},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logistic mixed-effects model analysis with pseudo-observations for estimating risk ratios in clustered binary data analysis. <em>SIM</em>, <em>44</em>(20-22), e70280. (<a href='https://doi.org/10.1002/sim.70280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logistic mixed-effects model has been a standard multivariate analysis method for analyzing clustered binary outcome data, for example, longitudinal studies, clustered randomized trials, and multicenter/regional studies. However, the resultant odds ratio estimator cannot be directly interpreted as an effect measure, and it is only interpreted as an approximation of the risk ratio estimator when the frequency of events is small. In this article, we propose a new statistical analysis method that enables providing a risk ratio estimator in the multilevel statistical model framework. The valid risk ratio estimation is realized via augmenting pseudo-observations to the original dataset and then analyzing the modified dataset by the logistic mixed-effects model. The resultant estimators of fixed effect coefficients are theoretically shown to be consistent estimators of the risk ratios. Also, the standard errors and confidence intervals of the risk ratios can be calculated by the bootstrap method. All of the computations are simply implementable by using the R package “glmmrr.” We illustrate the effectiveness of the proposed method via applications to a cluster-randomized trial of the maternal and child health handbook and a longitudinal study of respiratory disease. Also, we provide simulation-based evidence for the accuracy and precision of estimation of risk ratios by the proposed method.},
  archive      = {J_SIM},
  author       = {Hisashi Noma and Masahiko Gosho},
  doi          = {10.1002/sim.70280},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70280},
  shortjournal = {Stat. Med.},
  title        = {Logistic mixed-effects model analysis with pseudo-observations for estimating risk ratios in clustered binary data analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian process regression for value-censored functional and longitudinal data. <em>SIM</em>, <em>44</em>(20-22), e70277. (<a href='https://doi.org/10.1002/sim.70277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process (GP) regression is widely used for flexible and non-parametric Bayesian modeling of data arising from underlying smooth functions. This paper introduces a solution to GP regression when the observations are subject to value-based censoring. We derive exact and closed-form expressions for the conditional posterior distributions of the underlying functions in both the single-curve fitting case and in the case of a hierarchical model where multiple functions are modeled simultaneously. Our method can accommodate left, right, and interval censoring, and is directly applicable as an empirical Bayes method or integrated in a Markov–Chain Monte Carlo sampler for full posterior inference. The method is validated through extensive simulations, where it substantially outperforms naive approaches that either exclude censored observations or treat them as fully observed values. We give an application to a real-world dataset of longitudinal HIV-1 RNA measurements, where the observations are subject to left censoring due to a detection limit.},
  archive      = {J_SIM},
  author       = {Adam Gorm Hoffmann and Claus Thorn Ekstrøm and Benjamin Zeymer Christoffersen and Andreas Kryger Jensen},
  doi          = {10.1002/sim.70277},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70277},
  shortjournal = {Stat. Med.},
  title        = {Gaussian process regression for value-censored functional and longitudinal data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double negative control inference with some invalid negative control exposures for continuous outcome. <em>SIM</em>, <em>44</em>(20-22), e70276. (<a href='https://doi.org/10.1002/sim.70276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative controls have been increasingly used for causal inference when unmeasured confounding exist. Valid negative control exposures (NCEs) could not causally affect outcome, and valid negative control outcomes (NCOs) are not to be causally affected by exposure. In most observational studies, it is easy to find a valid NCO but NCEs are harder to verify due to the current limited knowledge. Invalid NCEs associated with outcome result in biased estimate of causal effects. However, previous work considering invalid negative controls is very limited. In this paper, we develop a double negative control framework for continuous outcomes in the presence of some invalid NCEs. First, we prove that it is possible to identify causal effects with a known pre-defined valid NCO and a pre-defined set of NCEs without knowing exactly their validity. Furthermore, as long as more than 50% of NCEs are valid, the average causal effect could be consistently estimated. Then we design an procedure to select valid NCEs. Finally, we give two kinds of double negative control estimators (sisvNCE and naiveNCE-Median) with a guarantee of theoretical estimation performance. Simulation results show that the performance of our method is robust when the number of invalid NCEs does not exceed a certain threshold. Application results indicate that our method has a promising role in public health.},
  archive      = {J_SIM},
  author       = {Qingqing Yang and Jinzhu Jia},
  doi          = {10.1002/sim.70276},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70276},
  shortjournal = {Stat. Med.},
  title        = {Double negative control inference with some invalid negative control exposures for continuous outcome},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Futility analyses for the MCP-mod methodology based on longitudinal models. <em>SIM</em>, <em>44</em>(20-22), e70274. (<a href='https://doi.org/10.1002/sim.70274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses futility analyses for the MCP-Mod methodology. Formulas are derived for calculating predictive and conditional power for MCP-Mod, which also cover the case when longitudinal models are used allowing to utilize incomplete data from patients at interim. A simulation study is conducted to evaluate the repeated sampling properties of the proposed decision rules and to assess the benefit of using a longitudinal versus a completer only model for decision making at interim. The results suggest that the proposed methods perform adequately and a longitudinal analysis outperforms a completer only analysis, particularly when the recruitment speed is higher and the correlation over time is larger. The proposed methodology is illustrated using real data from a dose-finding study for severe uncontrolled asthma.},
  archive      = {J_SIM},
  author       = {Björn Bornkamp and Jie Zhou and Dong Xi and Weihua Cao},
  doi          = {10.1002/sim.70274},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70274},
  shortjournal = {Stat. Med.},
  title        = {Futility analyses for the MCP-mod methodology based on longitudinal models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing stepped wedge cluster randomized trials with a baseline measurement of the outcome. <em>SIM</em>, <em>44</em>(20-22), e70273. (<a href='https://doi.org/10.1002/sim.70273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster randomized trials (SW-CRTs) are a type of uni-directional crossover designs and are increasingly common in prevention and implementation research. Although sample size formulas have been developed to support the planning of SW-CRTs, almost no prior methods incorporated the baseline measurement of the outcome—a common feature in many randomized trials and, increasingly, in cross-sectional SW-CRTs. In this article, we systematically investigate the possibility of addressing a baseline outcome measurement in designing cross-sectional SW-CRTs. We provide three linear mixed modeling approaches to adjust for the baseline outcome and derive the corresponding variance formula of the treatment effect estimator under each. The derived formulas reveal the efficiency implications of including a baseline outcome measurement, and provide a natural vehicle for the efficiency comparisons across adjustment approaches to generate practical recommendations. We validate the power and sample size methods under each baseline adjustment approach using simulations and provide an illustrative sample size calculation with a baseline outcome using the context of a real SW-CRT.},
  archive      = {J_SIM},
  author       = {Kendra Davis-Plourde and Keith Goldfeld and Heather Allore and Monica Taljaard and Fan Li},
  doi          = {10.1002/sim.70273},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70273},
  shortjournal = {Stat. Med.},
  title        = {Designing stepped wedge cluster randomized trials with a baseline measurement of the outcome},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal machine learning methods and use of cross-fitting in settings with high-dimensional confounding. <em>SIM</em>, <em>44</em>(20-22), e70272. (<a href='https://doi.org/10.1002/sim.70272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational epidemiological studies commonly seek to estimate the causal effect of an exposure on an outcome. Adjustment for potential confounding bias in modern studies is challenging due to the presence of high-dimensional confounding, which occurs when there are many confounders relative to sample size or complex relationships between continuous confounders and exposure and outcome. Doubly robust methods such as Augmented Inverse Probability Weighting (AIPW) and Targeted Maximum Likelihood Estimation (TMLE) have the potential to address these challenges, using data-adaptive approaches and cross-fitting, but despite recent advances, limited evaluation and guidance are available on their implementation in realistic settings where high-dimensional confounding is present. Motivated by an early-life cohort study, we conducted an extensive simulation study to compare the relative performance of AIPW and TMLE using data-adaptive approaches for estimating the average causal effect (ACE). We evaluated the benefits of using cross-fitting with a varying number of folds, as well as the impact of using a reduced versus full (larger, more diverse) library in the Super Learner ensemble learning approach used for implementation. We found that AIPW and TMLE performed similarly in most cases for estimating the ACE, but TMLE was more stable. Cross-fitting improved the performance of both methods, but was more important for variance estimation and coverage than for point estimates, with the number of folds a less important consideration. Using a full Super Learner library was important to reduce bias and variance in complex scenarios typical of modern health research studies.},
  archive      = {J_SIM},
  author       = {Susan Ellul and Stijn Vansteelandt and John B. Carlin and Margarita Moreno-Betancur},
  doi          = {10.1002/sim.70272},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70272},
  shortjournal = {Stat. Med.},
  title        = {Causal machine learning methods and use of cross-fitting in settings with high-dimensional confounding},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biostatisticians meet AI: Navigating shifts while preserving principles. <em>SIM</em>, <em>44</em>(20-22), e70271. (<a href='https://doi.org/10.1002/sim.70271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Bin Zhu},
  doi          = {10.1002/sim.70271},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70271},
  shortjournal = {Stat. Med.},
  title        = {Biostatisticians meet AI: Navigating shifts while preserving principles},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-robust standardization in cluster-randomized trials. <em>SIM</em>, <em>44</em>(20-22), e70270. (<a href='https://doi.org/10.1002/sim.70270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cluster-randomized trials, generalized linear mixed models and generalized estimating equations have conventionally been the default analytic methods for estimating the average treatment effect as routine practice. However, recent studies have demonstrated that their treatment effect coefficient estimators may correspond to ambiguous estimands when the models are misspecified or when there exist informative cluster sizes. In this article, we present a unified approach that standardizes output from a given regression model to ensure estimand-aligned inference for the treatment effect parameters in cluster-randomized trials. We introduce estimators for both the cluster-average and the individual-average treatment effects (marginal estimands) that are always consistent regardless of whether the specified working regression models align with the unknown data generating process. We further explore the use of a deletion-based jackknife variance estimator for inference. The development of our approach also motivates a natural test for informative cluster size. Extensive simulation experiments are designed to demonstrate the advantage of the proposed estimators under a variety of scenarios. The proposed model-robust standardization methods are implemented in the MRStdCRT R package.},
  archive      = {J_SIM},
  author       = {Fan Li and Jiaqi Tong and Xi Fang and Chao Cheng and Brennan C. Kahan and Bingkai Wang},
  doi          = {10.1002/sim.70270},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70270},
  shortjournal = {Stat. Med.},
  title        = {Model-robust standardization in cluster-randomized trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-free approach to evaluate a censored intermediate outcome as a surrogate for overall survival. <em>SIM</em>, <em>44</em>(20-22), e70268. (<a href='https://doi.org/10.1002/sim.70268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials or studies oftentimes require long-term and/or costly follow-up of participants to evaluate a novel treatment/drug/vaccine. There has been increasing interest in the past few decades in using short-term surrogate outcomes as a replacement for the primary outcome, that is, in using the surrogate outcome, which can potentially be observed sooner, to make inferences about the treatment effect on the long-term primary outcome. Very few of the available statistical methods to evaluate a surrogate are applicable to settings where both the surrogate and the primary outcome are time-to-event outcomes subject to censoring. Methods that can handle this setting tend to require parametric assumptions or be limited to assessing only the restricted mean survival time. In this paper, we propose a nonparametric approach to evaluate a censored surrogate outcome, such as time to progression, when the primary outcome is also a censored time-to-event outcome, such as time to death, and the treatment effect of interest is the difference in overall survival. Specifically, we define the proportion of the treatment effect on the primary outcome that is explained (PTE) by the censored surrogate outcome in this context, and estimate this proportion by defining and deriving an optimal transformation of the surrogate information. Our approach provides the added advantage of relaxed assumptions to guarantee that the true PTE is within , along with being model-free. The finite sample performance of our estimators is illustrated via extensive simulation studies and a real data application examining progression-free survival as a surrogate for overall survival for patients with metastatic colorectal cancer.},
  archive      = {J_SIM},
  author       = {Xuan Wang and Tianxi Cai and Lu Tian and Layla Parast},
  doi          = {10.1002/sim.70268},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70268},
  shortjournal = {Stat. Med.},
  title        = {Model-free approach to evaluate a censored intermediate outcome as a surrogate for overall survival},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Response to letter to the editor “Comments on ‘Novel non-linear models for clinical trial analysis with longitudinal data: A tutorial using SAS for both frequentist and bayesian methods’”. <em>SIM</em>, <em>44</em>(20-22), e70266. (<a href='https://doi.org/10.1002/sim.70266'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials with longitudinal continuous data, efficacy inference traditionally focuses on the difference in the mean change from baseline at a single study visit [e.g., mixed models for repeated measures (MMRM)]. Proportional MMRM (pMMRM) reparameterizes this difference as a proportional reduction relative to the placebo mean change. This proportional effect is a nonlinear combination of the means, whereas the difference is a linear combination of the means. It can not only lead to greater power at a single visit by yielding a test statistic lower-bounded by that of the difference but also offers a flexible and intuitive way to combine all or multiple visits for efficacy inference, which can further boost power. It is also asymptotically unbiased. pMMRM with visit-specific proportional effects yields identical parameter estimates to MMRM. When only MMRM outputs are used, the proportional effect calculated by the delta method yields greater power than the difference.},
  archive      = {J_SIM},
  author       = {Guoqiao Wang and Guogen Shan and Yan Li and Yijie Liao and Lon Schneider and Gary Cutter},
  doi          = {10.1002/sim.70266},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70266},
  shortjournal = {Stat. Med.},
  title        = {Response to letter to the editor “Comments on ‘Novel non-linear models for clinical trial analysis with longitudinal data: A tutorial using SAS for both frequentist and bayesian methods’”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tutorial on bayesian functional regression using stan. <em>SIM</em>, <em>44</em>(20-22), e70265. (<a href='https://doi.org/10.1002/sim.70265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript provides step-by-step instructions for implementing Bayesian functional regression models using Stan. Extensive simulations indicate that the inferential performance of the methods is comparable to that of state-of-the-art frequentist approaches. However, Bayesian approaches allow for more flexible modeling and provide an alternative when frequentist methods are not available or may require additional development. Methods and software are illustrated using the accelerometry data from the National Health and Nutrition Examination Survey (NHANES).},
  archive      = {J_SIM},
  author       = {Ziren Jiang and Ciprian Crainiceanu and Erjia Cui},
  doi          = {10.1002/sim.70265},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70265},
  shortjournal = {Stat. Med.},
  title        = {Tutorial on bayesian functional regression using stan},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian multi-factorial design and analysis for estimating combined effects of multiple interventions in a pragmatic clinical trial to improve dementia care. <em>SIM</em>, <em>44</em>(20-22), e70264. (<a href='https://doi.org/10.1002/sim.70264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factorial study designs can be important for understanding the effectiveness of interventions when multiple interventions are under investigation. In this design setting, a unit of randomization can be assigned to any combination of interventions. The rationale for taking this kind of approach can vary depending on the specific questions targeted by the research. These questions, in turn, have implications for the way in which the analyses will be conducted. The goal in this paper is to describe how we developed a factorial design along with a Bayesian analytic plan for a large cluster-randomized trial—the Emergency Departments LEading the transformation of Alzheimer's and Dementia care (ED-LEAD) study—focused on improving care for persons living with dementia.},
  archive      = {J_SIM},
  author       = {Keith S. Goldfeld and Corita R. Grudzen and Manish N. Shah and Abraham A. Brody and Joshua Chodosh and Rebecca Anthopolos},
  doi          = {10.1002/sim.70264},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70264},
  shortjournal = {Stat. Med.},
  title        = {A bayesian multi-factorial design and analysis for estimating combined effects of multiple interventions in a pragmatic clinical trial to improve dementia care},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian adaptive enrichment design for continuous biomarkers. <em>SIM</em>, <em>44</em>(20-22), e70262. (<a href='https://doi.org/10.1002/sim.70262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of precision medicine and targeted therapies in cancer, new challenges in the statistical design of clinical trials have naturally emerged. Most randomized clinical trial designs incorporating predictive biomarkers (those associated with treatment efficacy) assume biomarkers are dichotomous, or dichotomize naturally continuous biomarkers upfront, or find cut points mid-way through the trial to classify patients as biomarker-positive or biomarker-negative. However, these practices ignore or discard information about continuous and possible nonlinear or non-monotone prognostic or predictive effects. In this article, we propose a novel adaptive enrichment trial design to handle continuous biomarkers with any effect shape, including Bayesian marker-adaptive randomization. We demonstrate that this design can correctly make marker-specific trial decisions with high efficiency, resulting in improved performance and patient-centered decisions compared to adaptive cut-point selection approaches without adaptive randomization that further ignore or oversimplify true underlying marker relationships.},
  archive      = {J_SIM},
  author       = {Yue Tu and Yusha Liu and Wendy J. Mack and Lindsay A. Renfro},
  doi          = {10.1002/sim.70262},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70262},
  shortjournal = {Stat. Med.},
  title        = {Bayesian adaptive enrichment design for continuous biomarkers},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power and sample size calculation for multivariate longitudinal trials using the longitudinal rank sum test. <em>SIM</em>, <em>44</em>(20-22), e70261. (<a href='https://doi.org/10.1002/sim.70261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative diseases such as Alzheimer's and Parkinson's often exhibit complex, multivariate longitudinal outcomes that require advanced statistical methods to comprehensively evaluate treatment efficacy. The Longitudinal Rank Sum Test (LRST) offers a nonparametric framework to assess global treatment effects across multiple longitudinal endpoints without requiring multiplicity corrections. This study develops a robust methodology for power and sample size estimation specific to the LRST, integrating theoretical derivations, asymptotic properties, and practical estimation techniques under large sample conditions. Validation through numerical simulations demonstrates the accuracy of the proposed methods, while real-world applications to clinical trials in Alzheimer's disease (AD) and Parkinson's disease (PD) highlight their practical significance. This framework facilitates the design of efficient, well-powered trials, advancing the evaluation of treatments for complex diseases with multivariate longitudinal outcomes.},
  archive      = {J_SIM},
  author       = {Dhrubajyoti Ghosh and Xiaoming Xu and Sheng Luo},
  doi          = {10.1002/sim.70261},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70261},
  shortjournal = {Stat. Med.},
  title        = {Power and sample size calculation for multivariate longitudinal trials using the longitudinal rank sum test},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equivalency between the generalized bivariate bernoulli model dependency test and a logistic regression model with interaction effects. <em>SIM</em>, <em>44</em>(20-22), e70260. (<a href='https://doi.org/10.1002/sim.70260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Kazi Md Farhad Mahmud and Yanming Li and Devin C. Koestler},
  doi          = {10.1002/sim.70260},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70260},
  shortjournal = {Stat. Med.},
  title        = {Equivalency between the generalized bivariate bernoulli model dependency test and a logistic regression model with interaction effects},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handling missing outcome data in cluster randomized trials with both individual- and cluster-level dropout. <em>SIM</em>, <em>44</em>(20-22), e70259. (<a href='https://doi.org/10.1002/sim.70259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing outcome data are common in cluster randomized trials (CRTs), which can complicate inference. Further, the missingness can occur due to dropout of individuals, termed sporadically missing data, or dropout of clusters, termed systematically missing data, and these two types of missingness could have potentially different missing data mechanisms. We aimed to develop a well-performing and practical approach to handle inference in CRTs when outcome data may be both sporadically and systematically missing. To this end, we first examined the performance of several multilevel multiple imputation (MI) methods to handle sporadically and systematically missing CRT outcome data via a simulation study. Specifically, we examined performance under a multilevel covariate-dependent missingness assumption. Our findings indicated that several full conditional specification (FCS) methods designed for missingness in linear mixed models performed well under various scenarios, while an FCS approach using a two-stage estimator often performed poorly. We then developed methods for conducting sensitivity analysis to test the robustness of inferences under different missing at random (MAR) and missing not at random (MNAR) assumptions. The methods allow for different MNAR assumptions for cluster dropout and individual dropout to reflect that they may arise from different missing data mechanisms. We used graphical displays to visualize sensitivity analysis results. Our methods are illustrated using a real data application.},
  archive      = {J_SIM},
  author       = {Analissa Avila and Beth A. Glenn and Roshan Bastani and Catherine M. Crespi},
  doi          = {10.1002/sim.70259},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70259},
  shortjournal = {Stat. Med.},
  title        = {Handling missing outcome data in cluster randomized trials with both individual- and cluster-level dropout},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collapsible kernel machine regression for exposomic analyses. <em>SIM</em>, <em>44</em>(20-22), e70258. (<a href='https://doi.org/10.1002/sim.70258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important goal of environmental epidemiology is to quantify the complex health effects posed by a wide array of environmental exposures. In studies of a small number of exposures, flexible models like Bayesian kernel machine regression (BKMR) are appealing because they allow for non-linear and non-additive associations among exposures. However, this flexibility comes at the cost of low power and difficult interpretation, particularly in exposomic analyses when the number of exposures is large. We propose a flexible framework that allows for the separate selection of additive and non-additive effects, unifying additive models and kernel machine regression. The proposed approach yields increased power and simpler interpretation when there is little evidence of interaction. Further, it allows users to specify separate priors for additive and non-additive effect s, and allows for statistical inference on non-additive interactions. We extend the approach to a class of multiple index models, in which the special case of kernel machine-distributed lag models is nested. We apply the method to motivating data from a subcohort of the Human Early Life Exposome (HELIX) study containing 65 mixture components grouped into 13 distinct exposure classes.},
  archive      = {J_SIM},
  author       = {Glen McGee and Brent A. Coull and Ander Wilson},
  doi          = {10.1002/sim.70258},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70258},
  shortjournal = {Stat. Med.},
  title        = {Collapsible kernel machine regression for exposomic analyses},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of stepped-wedge cluster randomized trials when treatment effects vary by exposure time or calendar time. <em>SIM</em>, <em>44</em>(20-22), e70256. (<a href='https://doi.org/10.1002/sim.70256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped-wedge cluster randomized trials (SW-CRTs) are traditionally analyzed with models that assume an immediate and sustained treatment effect. Previous work has shown that making such an assumption in the analysis of SW-CRTs when the true underlying treatment effect varies by exposure time can produce severely misleading estimates. Alternatively, the true underlying treatment effect might vary by calendar time. Comparatively less work has examined treatment effect structure misspecification in this setting. Here, we evaluate the behavior of the linear mixed effects model-based immediate treatment effect, exposure time-averaged treatment effect, and calendar time-averaged treatment effect estimators in different scenarios where these estimators are misspecified for the true underlying treatment effect structure. We show that the immediate treatment effect estimator is relatively robust to bias when estimating a true underlying calendar time-averaged treatment effect estimand. However, when there is a true underlying calendar (exposure) time-varying treatment effect, misspecifying an analysis with an exposure (calendar) time-averaged treatment effect estimator can yield severely misleading estimates which may converge to a value with the opposite sign of the true calendar (exposure) time-averaged treatment effect estimand. In this article, we highlight these two different time scales on which treatment effects can vary in SW-CRTs and clarify potential vulnerabilities that may arise when considering different types of time-varying treatment effects in a SW design. Accordingly, we emphasize the need for researchers to carefully consider whether the treatment effect may vary as a function of exposure time or calendar time in the analysis of SW-CRTs.},
  archive      = {J_SIM},
  author       = {Kenneth M. Lee and Elizabeth L. Turner and Avi Kenny},
  doi          = {10.1002/sim.70256},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70256},
  shortjournal = {Stat. Med.},
  title        = {Analysis of stepped-wedge cluster randomized trials when treatment effects vary by exposure time or calendar time},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for similarity of dose response in multiregional clinical trials. <em>SIM</em>, <em>44</em>(20-22), e70255. (<a href='https://doi.org/10.1002/sim.70255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the problem of determining whether the dose response relationships between subgroups and the full population in a multiregional trial are similar. Similarity is assessed in terms of the maximal deviation between the dose response curves. We consider a parametric framework and develop two powerful bootstrap tests: one for assessing the similarity between the dose response curves of a single subgroup and that of the full population, and another for comparing the dose response curves of multiple subgroups with that of the full population. We prove the validity of these tests, investigate their finite sample properties through a simulation study and illustrate the methodology with a case study.},
  archive      = {J_SIM},
  author       = {Holger Dette and Lukas Koletzko and Frank Bretz},
  doi          = {10.1002/sim.70255},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70255},
  shortjournal = {Stat. Med.},
  title        = {Testing for similarity of dose response in multiregional clinical trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-phenotype approach to joint testing of main genetic and gene-environment interaction effects. <em>SIM</em>, <em>44</em>(20-22), e70253. (<a href='https://doi.org/10.1002/sim.70253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene-environment (GxE) interactions crucially contribute to complex phenotypes. The statistical power of a GxE interaction study is limited mainly due to weak GxE interaction effect sizes. Joint tests of the main genetic and GxE effects for a univariate phenotype were proposed to utilize the individually weak GxE effects to improve the discovery of associated genetic loci. We develop a testing procedure to evaluate combined genetic and GxE effects on multiple related phenotypes to enhance the power by merging pleiotropy in the main genetic and GxE effects. We base the approach on a general linear hypothesis testing framework for multivariate regression of continuous phenotypes. We implement the generalized estimating equations (GEE) technique under the seemingly unrelated regressions (SUR) setup for binary or mixed phenotypes. We use extensive simulations to show that the test for joint multi-phenotype genetic and GxE effects outperforms the univariate joint test of genetic and GxE effects and the test for multi-phenotype GxE effect concerning power when there is pleiotropy. The test produces a higher power than the test for the multi-phenotype marginal genetic effect for a weak genetic and substantial GxE effect. For more prominent genetic effects, the latter performs better with a limited increase in power. Overall, the multi-phenotype joint approach offers robust, high power across diverse simulation scenarios. We apply the methods to lipid phenotypes with sleep duration as an environmental factor in the UK Biobank. The proposed approach identified ten independent associated genetic loci missed by other competing methods. In a multi-phenotype analysis of apolipoproteins, ApoA1, and ApoB, our approach discovered two distinct loci considering sleep duration as the environmental factor.},
  archive      = {J_SIM},
  author       = {Saurabh Mishra and Arunabha Majumdar},
  doi          = {10.1002/sim.70253},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70253},
  shortjournal = {Stat. Med.},
  title        = {A multi-phenotype approach to joint testing of main genetic and gene-environment interaction effects},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new algorithm for sampling parameters in a structured correlation matrix with application to estimating optimal combinations of muscles to quantify progression in duchenne muscular dystrophy. <em>SIM</em>, <em>44</em>(20-22), e70252. (<a href='https://doi.org/10.1002/sim.70252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this paper is to estimate an optimal combination of biomarkers for individuals with Duchenne muscular dystrophy (DMD), which provides the most sensitive combinations of biomarkers to assess disease progression (in this case, optimal with respect to standardized response mean (SRM) for 4 muscle biomarkers). The biomarker data is incomplete (missing and irregular) multivariate longitudinal data. We propose a normal model with structured covariance designed for our setting. To sample from the posterior distribution of parameters, we develop a Markov Chain Monte Carlo (MCMC) algorithm to address the positive definiteness constraint on the structured correlation matrix. In particular, we propose a novel approach to compute the support of the parameters in the structured correlation matrix; we modify the approach from [1] on the set of the largest possible submatrices of the correlation matrix, where the correlation parameter is a unique element. For each posterior sample, we compute the optimal weights of our construct. We conduct data analysis and simulation studies to evaluate the algorithm and the frequentist properties of the posteriors of correlations and weights. We found that the lower extremities are the most responsive muscles at the early and late ambulatory disease stages, and the biceps brachii is the most responsive at the nonambulatory disease stage.},
  archive      = {J_SIM},
  author       = {Michael K. Kim and Michael J. Daniels and William D. Rooney and Rebecca J. Willcocks and Glenn A. Walter and Krista H. Vandenborne},
  doi          = {10.1002/sim.70252},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70252},
  shortjournal = {Stat. Med.},
  title        = {A new algorithm for sampling parameters in a structured correlation matrix with application to estimating optimal combinations of muscles to quantify progression in duchenne muscular dystrophy},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using confidence distributions in final and interim analyses for single-arm studies or platform trials consisting of single-arm studies. <em>SIM</em>, <em>44</em>(20-22), e70251. (<a href='https://doi.org/10.1002/sim.70251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidence distributions are a frequentist alternative to the Bayesian posterior distribution. These confidence distributions have received more attention in the recent past because of their simplicity. In rare diseases, oncology, or in pediatric drug development, single-arm trials, or platform trials consisting of a series of single-arm trials are increasingly being used, both to establish proof-of-concept and to provide pivotal evidence for a marketing application. Often, these single-arm trials are designed as two-stage designs, or they include sequential or continuous monitoring approaches. They are analyzed using standard frequentist, Bayesian, or other methods. In this paper, we describe how to define analysis strategies based on confidence distributions for such single-arm trials or for platform trials that consist of a series of single arm trials. We focus on binary endpoints and show how to define the corresponding decision rules for final and interim analyses and how to derive their operating characteristics exactly, for example, without simulation. Our approach uses predictive probabilities rather than conditional probabilities (as with stochastic curtailment) to define the interim decision rules. It can be applied to platform, basket, and umbrella trials that consist of a series of single-arm trials but also to stand-alone single arm trials.},
  archive      = {J_SIM},
  author       = {Günter Heimann and Peter Jacko and Tom Parke},
  doi          = {10.1002/sim.70251},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70251},
  shortjournal = {Stat. Med.},
  title        = {Using confidence distributions in final and interim analyses for single-arm studies or platform trials consisting of single-arm studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling joint health effects of environmental exposure mixtures with bayesian additive regression trees. <em>SIM</em>, <em>44</em>(20-22), e70250. (<a href='https://doi.org/10.1002/sim.70250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying the association between mixtures of environmental exposures and health outcomes can be challenging due to issues such as correlation among the exposures and non-linearities or interactions in the exposure-response function. For this reason, one common strategy is to fit flexible nonparametric models to capture the true exposure-response surface. However, once such a model is fit, further decisions are required when it comes to summarizing the marginal and joint effects of the mixture on the outcome. In this work, we describe the use of soft Bayesian additive regression trees (BART) to estimate the exposure-risk surface describing the effect of mixtures of chemical air pollutants and temperature on asthma-related emergency department (ED) visits during the warm season in Atlanta, Georgia, from 2011 to 2018. BART is chosen for its ability to handle large datasets and for its flexibility to be incorporated as a single component of a larger model. We then summarize the results using a strategy known as accumulated local effects to extract meaningful insights into the mixture effects on asthma-related morbidity. Notably, we observe negative associations between and asthma ED visits and harmful associations between ozone and asthma ED visits, both of which are particularly strong on lower temperature days.},
  archive      = {J_SIM},
  author       = {Jacob R. Englert and Stefanie T. Ebelt and Howard H. Chang},
  doi          = {10.1002/sim.70250},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70250},
  shortjournal = {Stat. Med.},
  title        = {Modeling joint health effects of environmental exposure mixtures with bayesian additive regression trees},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group sequential trial design using stepwise monte carlo for increased flexibility and robustness. <em>SIM</em>, <em>44</em>(20-22), e70249. (<a href='https://doi.org/10.1002/sim.70249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are becoming increasingly complex, incorporating numerous parameters and degrees of freedom. Optimal analytic approaches for these intricate trial designs are often unavailable, necessitating extensive simulation to control the Type I error rate and power, while reducing sample size and achieving favorable operating characteristics. This paper proposes a general method to reduce the dimension of the design space using group stepwise methods and Monte Carlo simulations, significantly decreasing the number of iterations required to identify near-optimal parameters. The method extends classical Group Sequential Designs but does not rely on normality assumptions and can accommodate complex trial designs. We offer a simulation study comparing the optimality, precision, and efficiency (runtime) of our method to those of existing approaches and conclude that our new method offers an attractive trade-off among these three key summaries.},
  archive      = {J_SIM},
  author       = {Amitay Kamber and Elad Berkman and Tzviel Frostig and Raviv Pryluk and Bradley P. Carlin},
  doi          = {10.1002/sim.70249},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70249},
  shortjournal = {Stat. Med.},
  title        = {Group sequential trial design using stepwise monte carlo for increased flexibility and robustness},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The estimand framework in diagnostic accuracy studies. <em>SIM</em>, <em>44</em>(20-22), e70248. (<a href='https://doi.org/10.1002/sim.70248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic accuracy studies evaluate how well a diagnostic test can detect or rule out a medical condition. Different events can interfere with the conduct of the test, affecting the test result. Before starting a diagnostic test accuracy study, the clinical question of interest should be precisely defined. Based on that, strategies can be chosen for dealing with the interfering event. We introduce six different strategies for how such events could be handled. We introduce the estimand concept for diagnostic accuracy studies, which consists of the attributes population, target condition, index test, accuracy measure, and the strategies for handling interfering events. The estimand determines which effect is estimated based on the study objective. To bridge the gap between the clinical study objective and the method for the estimation, we illustrate the necessary steps using a fictitious computed tomography scan study. The defined estimand improves the structure of the planning phase, enhances the interdisciplinary exchange, and supports the interpretation based on the study objective.},
  archive      = {J_SIM},
  author       = {Alexander Fierenz and Mouna Akacha and Norbert Benda and Mahnaz Badpa and Patrick M M Bossuyt and Nandini Dendukuri and Britta Rackow and Antonia Zapf},
  doi          = {10.1002/sim.70248},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70248},
  shortjournal = {Stat. Med.},
  title        = {The estimand framework in diagnostic accuracy studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical grouped horseshoe priors for subgroup identification and estimation. <em>SIM</em>, <em>44</em>(20-22), e70246. (<a href='https://doi.org/10.1002/sim.70246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common issue in randomized clinical trials (RCTs) is the identification of subgroups and the estimation of their effects. Typically, RCTs are not powered to estimate the effects of subgroups. However, in some circumstances, treatment may work for some groups and not others, and it is of interest to identify these subgroups and estimate their treatment effects. In this paper, we introduce a novel hierarchical grouped horseshoe prior (HGHP) for subgroup identification and estimation. We show via simulation that our proposed approach yields superior positive predictive value and narrower credible intervals compared to other shrinkage priors. We apply our method to a real clinical trial for COVID-19.},
  archive      = {J_SIM},
  author       = {Ethan M. Alt and Anil Anderson and Qing Li and Jia Hua and Amarjot Kaur and Joseph G. Ibrahim},
  doi          = {10.1002/sim.70246},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70246},
  shortjournal = {Stat. Med.},
  title        = {Hierarchical grouped horseshoe priors for subgroup identification and estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of endpoint delay on the efficiency of multi arm multi stage trials. <em>SIM</em>, <em>44</em>(20-22), e70245. (<a href='https://doi.org/10.1002/sim.70245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-arm multi-stage (MAMS) is an efficient class of trial designs that helps to assess multiple treatment strategies at the same time using an adaptive design. These designs can substantially reduce the average number of samples required compared to an equivalent single stage multi-arm trial. However, if patient recruitment is continued while we await treatment outcomes, a long-term primary outcome leads to a number of ‘pipeline’ patients getting recruited in the trial, who do not benefit from the early termination of a futile arm. This study focuses on quantifying the efficiency loss a MAMS design undergoes, in terms of the expected sample size (ESS), because of outcome delay. We first estimate the number of ‘pipeline’ patients (recruited during the interim analysis (IA) while awaiting outcome data) analytically through different recruitment models, given the total recruitment time. We then compute the ESS accounting for delay and assess the Efficiency Loss (EL). The results indicate that more than 50% of the expected efficiency gain is typically lost due to delay when the delay is more than of the total recruitment length. Although the number of stages have little influence on the efficiency loss, the timing of the IA can impact the efficiency of MAMS designs with delayed outcomes; in particular, conducting the IAs earlier than an equally-spaced design can be harmful for the design. Finally, we conclude that, in order to gain maximum benefit of MAMS in terms of a reduced sample size in multi-arm trials, the outcome delay should be less than a third of the total recruitment length.},
  archive      = {J_SIM},
  author       = {Aritra Mukherjee and James M. S. Wason},
  doi          = {10.1002/sim.70245},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70245},
  shortjournal = {Stat. Med.},
  title        = {Impact of endpoint delay on the efficiency of multi arm multi stage trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A location-scale joint model for studying the link between the time-dependent subject-specific variability of blood pressure and competing events. <em>SIM</em>, <em>44</em>(20-22), e70244. (<a href='https://doi.org/10.1002/sim.70244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the high incidence of cardio and cerebrovascular diseases (CVD), and their association with morbidity and mortality, their prevention is a major public health issue. A high level of blood pressure is a well-known risk factor for these events, and an increasing number of studies suggest that blood pressure variability may also be an independent risk factor. However, these studies suffer from significant methodological weaknesses. In this work, we propose a new location-scale joint model for the repeated measures of a marker and competing events. This joint model combines a mixed model including a subject-specific and time-dependent residual variance modeled through random effects, and cause-specific proportional intensity models for the competing events. The risk of events may depend simultaneously on the current value of the variance, as well as, the current value and the current slope of the marker trajectory. The model is estimated by maximizing the likelihood function using the Marquardt–Levenberg algorithm. The estimation procedure is implemented in an R-package and is validated through a simulation study. This model is applied to study the association between blood pressure variability and the risk of CVD and death from other causes. Using data from a large clinical trial on the secondary prevention of stroke, we find that the current individual variability of blood pressure is associated with the risk of CVD and death. Moreover, the comparison with a model without heterogeneous variance shows the importance of taking into account this variability in the goodness-of-fit and for dynamic predictions.},
  archive      = {J_SIM},
  author       = {Léonie Courcoul and Christophe Tzourio and Mark Woodward and Antoine Barbieri and Hélène Jacqmin-Gadda},
  doi          = {10.1002/sim.70244},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70244},
  shortjournal = {Stat. Med.},
  title        = {A location-scale joint model for studying the link between the time-dependent subject-specific variability of blood pressure and competing events},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RISE: Two-stage rank-based identification of high-dimensional surrogate markers applied to vaccinology. <em>SIM</em>, <em>44</em>(20-22), e70241. (<a href='https://doi.org/10.1002/sim.70241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vaccine trials with long-term participant follow-up, it is of great importance to identify surrogate markers that accurately infer long-term immune responses. These markers offer practical advantages such as providing early, indirect evidence of vaccine efficacy, and can accelerate vaccine development while identifying potential biomarkers. High-throughput technologies such as RNA-sequencing have emerged as promising tools for understanding complex biological systems and informing new treatment strategies. However, these data are high-dimensional, presenting unique statistical challenges for existing surrogate marker identification methods. We introduce Rank-based Identification of high-dimensional SurrogatE Markers (RISE), a novel approach designed for small sample, high-dimensional settings typical in modern vaccine experiments. RISE uses a nonparametric univariate test to screen variables for promising candidates, followed by surrogate evaluation on independent data. Our simulation studies demonstrate RISE's desirable properties, including type one error rate control and empirical power under various conditions. Applying RISE to a clinical trial for inactivated influenza vaccination, we sought to identify genes whose expression could serve as a surrogate for the induced immune response. This analysis revealed a signature of genes appearing to function as a reasonable surrogate for the neutralizing antibody response. Pathways related to innate antiviral signaling and interferon stimulation were strongly represented in this derived surrogate, providing a clear immunological interpretation.},
  archive      = {J_SIM},
  author       = {Arthur Hughes and Layla Parast and Rodolphe Thiébaut and Boris P. Hejblum},
  doi          = {10.1002/sim.70241},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70241},
  shortjournal = {Stat. Med.},
  title        = {RISE: Two-stage rank-based identification of high-dimensional surrogate markers applied to vaccinology},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accounting for misclassification of binary outcomes in external control arm studies for unanchored indirect comparisons: Simulations and applied example. <em>SIM</em>, <em>44</em>(20-22), e70236. (<a href='https://doi.org/10.1002/sim.70236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-arm control trials are increasingly proposed as a potential approach for treatment evaluation. However, the limitations of this design restrict its methodological acceptability. Regulatory agencies have raised concerns about this approach, although it is sometimes required in applications based solely on such studies. Consequently, the need for accurate indirect treatment comparisons has become critical, especially when constructing external control arms using routinely collected data as outcome measurements may differ from those recorded in the single-arm trial leading to potential misclassification of outcomes. This study aimed to quantify the bias from ignoring misclassification of a binary outcome within unanchored indirect comparisons, through simulations, and to propose a likelihood-based method to correct this bias (i.e., the outcome-corrected model). Simulations demonstrated that ignoring misclassification results in significant bias and poor coverage probabilities. In contrast, the outcome-corrected model reduced bias, improved 95% confidence interval coverage probability and root mean square error in various scenarios. The methodology was applied to two hepatocellular carcinoma trials illustrating a practical application. The findings underscore the importance of addressing outcome misclassification in indirect comparisons. The proposed correction method may improve reliability in unanchored indirect treatment comparisons.},
  archive      = {J_SIM},
  author       = {Mikail Nourredine and Antoine Gavoille and Côme Lepage and Behrouz Kassai-Koupai and Michel Cucherat and Fabien Subtil},
  doi          = {10.1002/sim.70236},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70236},
  shortjournal = {Stat. Med.},
  title        = {Accounting for misclassification of binary outcomes in external control arm studies for unanchored indirect comparisons: Simulations and applied example},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What is fair? defining fairness in machine learning for health. <em>SIM</em>, <em>44</em>(20-22), e70234. (<a href='https://doi.org/10.1002/sim.70234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring that machine-learning (ML) models are safe, effective, and equitable across all patients is critical for clinical decision-making and for preventing the amplification of existing health disparities. In this work, we examine how fairness is conceptualized in ML for health, including why ML models may lead to unfair decisions and how fairness has been measured in diverse real-world applications. We review commonly used fairness notions within group, individual, and causal-based frameworks. We also discuss the outlook for future research and highlight opportunities and challenges in operationalizing fairness in health-focused applications.},
  archive      = {J_SIM},
  author       = {Jianhui Gao and Benson Chou and Zachary R. McCaw and Hilary Thurston and Paul Varghese and Chuan Hong and Jessica Gronsbell},
  doi          = {10.1002/sim.70234},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70234},
  shortjournal = {Stat. Med.},
  title        = {What is fair? defining fairness in machine learning for health},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient time-to-event dose-escalation guidance of multi-cycle cancer therapies. <em>SIM</em>, <em>44</em>(20-22), e70229. (<a href='https://doi.org/10.1002/sim.70229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment of cancer has rapidly evolved over time in quite dramatic ways, for example, from chemotherapies, targeted therapies to immunotherapies and chimeric antigen receptor T-cells. Nonetheless, the basic design of early phase I trials in oncology still follows predominantly a dose-escalation design. These trials monitor safety over the first treatment cycle to escalate the dose of the investigated drug. However, over time, studying additional factors such as drug combinations and/or variation in the timing of dosing became important as well. Existing designs were continuously enhanced and expanded to account for increased trial complexity. With toxicities occurring at later stages beyond the first cycle and the need to treat patients over multiple cycles, the focus on the first treatment cycle only is becoming a limitation in nowadays multi-cycle treatment therapies. Here, we introduce a multi-cycle time-to-event model (TITE-CLRM: Time-Interval-To-Event Complementary-Loglog Regression Model), allowing guidance of dose-escalation trials studying multi-cycle therapies. The challenge lies in balancing the need to monitor the safety of longer treatment periods with the need to continuously enroll patients safely. The proposed multi-cycle time-to-event model is formulated as an extension to established concepts like the escalation with overdose control principle. The model is motivated by a current drug development project and evaluated in a simulation study.},
  archive      = {J_SIM},
  author       = {Lukas A. Widmer and Sebastian Weber and Yunnan Xu and Hans-Jochen Weber},
  doi          = {10.1002/sim.70229},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70229},
  shortjournal = {Stat. Med.},
  title        = {Towards efficient time-to-event dose-escalation guidance of multi-cycle cancer therapies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiplicity control in oncology clinical trials with a binary surrogate endpoint-based drop-the-losers design. <em>SIM</em>, <em>44</em>(20-22), e70209. (<a href='https://doi.org/10.1002/sim.70209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typical phase 1 oncology studies identify the maximum tolerated dose as the “optimal” dose for subsequent phases. With the advancement of molecular targeted agents and immunotherapies, however, evaluating two or more doses has become increasingly critical for dose selection. Such evaluation is often done in phase 2 studies in a randomized manner. In this article, we evaluate the strategy of applying an adaptive phase 2/3 seamless design for dose selection in oncology studies. Specifically, we consider the “drop-the-losers” design, where multiple treatment arms and a control arm are administered during the initial stage, and a more effective arm is identified for later stages by a binary surrogate endpoint such as overall response. We derive the theoretical type I error inflation scale and conduct simulation studies to illustrate the impact of various factors on the type I error inflation in such designs. Furthermore, we demonstrate the findings through the design of a lung cancer trial and introduce a software that implements the proposed design.},
  archive      = {J_SIM},
  author       = {Weibin Zhong and Jing-ou Liu and Chenguang Wang},
  doi          = {10.1002/sim.70209},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70209},
  shortjournal = {Stat. Med.},
  title        = {Multiplicity control in oncology clinical trials with a binary surrogate endpoint-based drop-the-losers design},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted trigonometric regression for suboptimal designs in circadian transcriptome studies. <em>SIM</em>, <em>44</em>(20-22), e70201. (<a href='https://doi.org/10.1002/sim.70201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circadian transcriptome studies often use trigonometric regression to model gene expression over time. Ideally, protocols in these studies would collect tissue samples at evenly distributed and equally spaced time points over a 24-hour period. This sample collection protocol is known as an equispaced design, which is considered the optimal experimental design for trigonometric regression under multiple statistical criteria. However, implementing equispaced designs in studies involving individuals is logistically challenging, and failure to employ an equispaced design could introduce variability in the statistical power of a hypothesis test relative to a model's phase-shift parameter estimates. This article is motivated by the variability in power for hypothesis testing when tissue samples are not collected under an equispaced design, and considers a weighted trigonometric regression as a remedy. Specifically, the weights for this regression are the normalized reciprocals of estimates derived from a kernel density estimator for sample collection time, which inflates the weight of samples collected at underrepresented time points. A search procedure is also introduced to identify the hyperparameter for kernel density estimation that relates to maximizing the smallest eigenvalue of the Hessian of weighted squared loss, which is motivated by the -optimality criterion from experimental design literature. Simulation studies consistently demonstrate that this weighted regression mitigates variability in power for hypothesis tests performed with an estimated model. Illustrations with six circadian transcriptome datasets further indicate that this weighted regression consistently yields larger test statistics than its unweighted counterpart for first-order trigonometric regression, or cosinor regression, which is prevalent in circadian transcriptome studies.},
  archive      = {J_SIM},
  author       = {Michael T. Gorczyca and Justice D. Sefas},
  doi          = {10.1002/sim.70201},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70201},
  shortjournal = {Stat. Med.},
  title        = {Weighted trigonometric regression for suboptimal designs in circadian transcriptome studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating heterogeneity in mixed hidden markov models with an application to the sleep-wake cycle. <em>SIM</em>, <em>44</em>(20-22), e70197. (<a href='https://doi.org/10.1002/sim.70197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sleep–wake cycle plays an important and far-reaching role in health. By utilizing personal physical activity monitors (PAMs), inferences about the sleep–wake cycle can be made. Hidden Markov models (HMMs) have been applied in this area as an accurate unsupervised approach. To account for heterogeneity in activity levels, we developed a mixed HMM that allows for individual differences. Through extensive simulations, we quantified the added gains relative to a standard HMM from using a mixed HMM to account for heterogeneity across several realistic scenarios. We found that mixed HMMs are often more accurate than standard HMMs when follow-up times are shorter. In situations with many repeated measurements per individual, a standard and mixed HMM have similar levels of accuracy, although a standard HMM is faster and easier to implement. Afterward, we applied our HMMs to actigraphy data from the National Health and Nutrition Examination Survey. We present results on sleep summary statistics by age and BMI. Summary statistics about the sleep–wake cycle extracted by the standard and mixed HMM were qualitatively similar. Differences in results, however, were likely driven by the heterogeneity in physical activity due to BMI and age, which we identified using a post hoc investigation of the data-driven clusters produced by the mixed HMM.},
  archive      = {J_SIM},
  author       = {Jordan Aron and Paul S. Albert and Mark B. Fiecas},
  doi          = {10.1002/sim.70197},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70197},
  shortjournal = {Stat. Med.},
  title        = {Incorporating heterogeneity in mixed hidden markov models with an application to the sleep-wake cycle},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimum area confidence set optimality for simultaneous confidence bands for percentiles with applications to drug shelf-life estimation. <em>SIM</em>, <em>44</em>(20-22), e70184. (<a href='https://doi.org/10.1002/sim.70184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One important property of any drug product is its stability over time. A key objective in drug stability studies is to estimate the shelf-life of a drug, involving a suitable definition of the true shelf-life and the construction of an appropriate estimate of the true shelf-life. Simultaneous confidence bands (SCBs) for percentiles in linear regression are valuable tools for determining the shelf-life in drug stability studies. In this paper, we propose a novel criterion, the Minimum Area Confidence Set (MACS) criterion, for finding the optimal SCB for percentile regression lines. This criterion focuses on the area of the constrained regions for the newly proposed pivotal quantities, which are generated from the confidence set for the unknown parameters of an SCB. We employ the new pivotal quantities to construct exact SCBs over any finite covariate intervals and use the MACS criterion to compare several SCBs of different forms. The optimal SCB under the MACS criterion can be used to construct the interval estimate of the true shelf-life. Furthermore, a new computationally efficient method is proposed for calculating the critical constants of exact SCBs for percentile regression lines. A real data example on drug stability is provided for illustration.},
  archive      = {J_SIM},
  author       = {Lingjiao Wang and Yang Han and Wei Liu and Frank Bretz},
  doi          = {10.1002/sim.70184},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70184},
  shortjournal = {Stat. Med.},
  title        = {Minimum area confidence set optimality for simultaneous confidence bands for percentiles with applications to drug shelf-life estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Letter to the editors. <em>SIM</em>, <em>44</em>(20-22), e70119. (<a href='https://doi.org/10.1002/sim.70119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Saralees Nadarajah and Tibor K. Pogány},
  doi          = {10.1002/sim.70119},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20-22},
  pages        = {e70119},
  shortjournal = {Stat. Med.},
  title        = {Letter to the editors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric efficient inference for the probability of necessary and sufficient causation. <em>SIM</em>, <em>44</em>(18-19), e70242. (<a href='https://doi.org/10.1002/sim.70242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal attribution, which seeks to explain the reasons behind events or behaviors, plays a critical role in causal inference and deepens our understanding of cause-and-effect relationships in scientific research. The probabilities of necessary causation (PN) and sufficient causation (PS) are two of the most common quantities for attribution in causal inference. While several works have explored the identification or bounds of PN and PS, efficient estimation remains unaddressed. To fill this gap, this paper focuses on obtaining semiparametric efficient estimators of PN and PS under two sets of identifiability assumptions: strong ignorability and monotonicity, and strong ignorability and conditional independence. We derive efficient influence functions and semiparametric efficiency bounds for PN and PS under the two sets of identifiability assumptions, respectively. Based on this, we propose efficient estimators for PN and PS and show their large sample properties. Extensive simulations validate the superiority of our estimators compared to competing methods. We apply our methods to a real-world dataset to assess various risk factors affecting stroke.},
  archive      = {J_SIM},
  author       = {Zhaoqing Tian and Peng Wu},
  doi          = {10.1002/sim.70242},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70242},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric efficient inference for the probability of necessary and sufficient causation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic long-term prediction with intermediate event information: A flexible model with bivariate time-varying coefficients. <em>SIM</em>, <em>44</em>(18-19), e70240. (<a href='https://doi.org/10.1002/sim.70240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of time-to-intermediate event data and the evolving characteristics of patients to enhance long-term prediction has garnered significant interest, driven by the wealth of data generated from longitudinal cohorts. In this paper, we propose sequential/dynamic prediction rules by using regression models with time-varying coefficients. We introduce a class of dynamic models that not only incorporates intermediate event information but also leverages information across different landmark times. To address the challenge of right-censoring, we employ an inverse weighting technique in the estimation process. We establish the asymptotic properties of the estimated parameters and conduct extensive simulations to assess the finite sample performance. Our simulation studies confirm that the proposed method exhibits computational efficiency and yields estimations comparable to those of kernel-based approaches. We apply the proposed method to real-world data from the Atherosclerosis Risk in Communities (ARIC) study and predict mortality while incorporating information regarding a crucial intermediate event, the occurrence of a stroke, and other time-varying covariates dynamically.},
  archive      = {J_SIM},
  author       = {Yunyi Wang and Wen Li and Ruosha Li and Jing Ning},
  doi          = {10.1002/sim.70240},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70240},
  shortjournal = {Stat. Med.},
  title        = {Dynamic long-term prediction with intermediate event information: A flexible model with bivariate time-varying coefficients},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric inference for a two-phase failure-time-auxiliary-dependent sampling design. <em>SIM</em>, <em>44</em>(18-19), e70239. (<a href='https://doi.org/10.1002/sim.70239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large cohort studies under simple random sampling could be prohibitive to conduct for epidemiological studies with a limited budget, especially when exposure variables are expensive or hard to obtain. Failure-time-dependent sampling (FDS) is a commonly used cost-effective sampling strategy for studies with failure times as outcomes. To further enhance study efficiency upon FDS, we propose a two-phase failure-time-auxiliary-dependent sampling (FADS) design that allows the probability of obtaining the expensive exposures to depend on both the failure time and some cheaply available auxiliary variables to the main exposure of interest. To account for the sampling bias, we develop a semiparametric maximum pseudo-likelihood approach for inference and a nonparametric bootstrap procedure for variance estimation. The proposed estimator of regression coefficients is shown to be consistent and asymptotically normally distributed. The simulation studies indicate that our proposed method works well in practical settings and is more efficient than other competing sampling schemes or methods. We illustrate our method with the analysis of two real data sets, the ARIC Study and the National Wilms' Tumor Study.},
  archive      = {J_SIM},
  author       = {Xu Cao and Qingning Zhou and Jianwen Cai and Haibo Zhou},
  doi          = {10.1002/sim.70239},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70239},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric inference for a two-phase failure-time-auxiliary-dependent sampling design},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prior effective sample size when borrowing on the treatment effect scale. <em>SIM</em>, <em>44</em>(18-19), e70235. (<a href='https://doi.org/10.1002/sim.70235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the robust uptick in the applications of Bayesian external data borrowing, eliciting a prior distribution with the proper amount of information becomes increasingly critical. The prior effective sample size (ESS) is an intuitive and efficient measure for this purpose. The majority of ESS definitions have been proposed in the context of borrowing control information. Meanwhile, Bayesian borrowing is frequently conducted on the treatment effect scale to extrapolate evidence in pediatric or global trials. While many Bayesian models can be naturally extended to leveraging external information on the treatment effect scale, very little attention has been directed to computing the prior ESS in this setting. In this research, we bridge this methodological gap by extending the popular expected local information ratio (ELIR) ESS definition. We lay out the general framework, and derive the ESS for various types of endpoints and treatment effect measures. The desirable predictive consistency property of ELIR ESS is examined and found to only be preserved for the difference between two normal endpoints. The methods are implemented in R programs available on GitHub: https://github.com/squallteo/TrtEffESS .},
  archive      = {J_SIM},
  author       = {Hongtao Zhang and Keaven M. Anderson and Zachary Zimmer and Gregory Golm and Aditi Sapre and Joseph G. Ibrahim},
  doi          = {10.1002/sim.70235},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70235},
  shortjournal = {Stat. Med.},
  title        = {Prior effective sample size when borrowing on the treatment effect scale},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The cox model with adaptive fused group bridge penalty to incorporate historical data into the analysis of clinical trials with an application to BMT CTN 1101. <em>SIM</em>, <em>44</em>(18-19), e70233. (<a href='https://doi.org/10.1002/sim.70233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incorporation of historical data (HD) into a clinical trial analysis can improve the precision and efficiency of treatment evaluation if the HD are exchangeable with clinical trial data. Evaluating the exchangeability of these two data sets is challenging, however, as an incorrect assessment of exchangeability yields invalid inference on the treatment effect that may produce bias and inflate the Type I error rate. To address this practical problem, we propose an adaptive fused group bridge penalty to evaluate the comparability of parameters between HD and clinical trial data and make inferences on the treatment effect. The proposed penalty has oracle properties, including consistency for identifying the underlying model and the asymptotic normality of the estimators. Simulation studies show that the proposed method controls the Type I error rate better and has higher power than competing methods under both exchangeable and non-exchangeable settings. We apply the proposed method by reanalyzing a Phase III trial while also leveraging a corresponding HD set.},
  archive      = {J_SIM},
  author       = {Xi Fang and Soyoung Kim and Michael J. Martens and Brent R. Logan and Kwang Woo Ahn},
  doi          = {10.1002/sim.70233},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70233},
  shortjournal = {Stat. Med.},
  title        = {The cox model with adaptive fused group bridge penalty to incorporate historical data into the analysis of clinical trials with an application to BMT CTN 1101},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized method of moments in analyzing recurrent events for semiparametric transformation models with informative censoring. <em>SIM</em>, <em>44</em>(18-19), e70232. (<a href='https://doi.org/10.1002/sim.70232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the semiparametric transformation models with a shared frailty variable for the rate function of recurrent events, where a shared frailty model is imposed to enable a correlation between the recurrent event process and the censoring time. Unlike the commonly used shared-frailty proportional rate models, our models allow for the rate functions associated with any two sets of covariate values not to be proportional over time. The proposed estimating approaches are inspired by Wang and Huang, who decompose the rate function into shape and size components. We start with the inverse-rate weighting approach and subsequently introduce a novel generalized method of moments framework to improve estimation efficiency by combining the estimating procedures derived from the shape and size components, respectively. The proposed methods are robust because they do not require the assumption of a Poisson process for the recurrent events or distributional assumptions for the frailty and censoring times. The large sample properties of the proposed methods are established, and the finite sample properties are examined through the simulation studies. The proposed methods are applied to a real data set.},
  archive      = {J_SIM},
  author       = {Yu-Jen Cheng and Chang-Yu Tsai},
  doi          = {10.1002/sim.70232},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70232},
  shortjournal = {Stat. Med.},
  title        = {Generalized method of moments in analyzing recurrent events for semiparametric transformation models with informative censoring},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Score test for functional markov process with image predictor. <em>SIM</em>, <em>44</em>(18-19), e70231. (<a href='https://doi.org/10.1002/sim.70231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A functional multistate model is presented, which accommodates Markov processes governing disease transition in a finite set of states. Importantly, we consider a setting where the set of predictors contains a high-dimensional image with the goal of quantifying the association between the image and the transition of disease states. In the motivating application of breast cancer, women start from normal breast tissue, go through benign lesions, and then to the onset of DCIS/invasive cancer. As in the real data application, we consider the setting in which the individuals are observed intermittently and the transition times are interval censored. A score test is developed to test the nullity of the coefficient function for the image predictor at different transitions between states. The asymptotic distribution of the score statistic is provided. An application involving progression to the development of breast cancer with mammogram image data provides illustration. Our results demonstrate an important association between the mammogram image and the probability of transition in breast cancer.},
  archive      = {J_SIM},
  author       = {Yang Wang and Graham. A. Colditz and Shu Jiang},
  doi          = {10.1002/sim.70231},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70231},
  shortjournal = {Stat. Med.},
  title        = {Score test for functional markov process with image predictor},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of multi-category youden index based on the lehmann assumption. <em>SIM</em>, <em>44</em>(18-19), e70230. (<a href='https://doi.org/10.1002/sim.70230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Youden index is a widely used metric for assessing diagnostic accuracy in two-class classification problems, particularly for determining the optimal decision cutoff point. In this article, we extend this concept to a multiple-category classification framework by introducing semi-parametric estimators for the generalized Youden index and its associated optimal threshold, under the Lehmann assumption. Our proposed estimators are much easier to implement than the traditional nonparametric estimators. We further establish the theoretical properties of these estimators, ensuring their consistency and asymptotic normality. To evaluate the effectiveness of the proposed methods, we conduct extensive simulation studies and apply them to a real-world liver cancer dataset, demonstrating their practical applicability in medical diagnostics.},
  archive      = {J_SIM},
  author       = {Qunqiang Feng and Boyan Liu and Jialiang Li},
  doi          = {10.1002/sim.70230},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70230},
  shortjournal = {Stat. Med.},
  title        = {Estimation of multi-category youden index based on the lehmann assumption},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for generating realistic synthetic tabular data in a randomized controlled trial setting. <em>SIM</em>, <em>44</em>(18-19), e70227. (<a href='https://doi.org/10.1002/sim.70227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generation of realistic synthetic data has garnered considerable attention in recent years, particularly in the health research domain due to its utility in, for instance, sharing data while protecting patient privacy or determining optimal clinical trial design. While much work has been concentrated on synthetic image generation, generation of realistic and complex synthetic tabular data of the type most commonly encountered in classic epidemiological or clinical studies is still lacking, especially with regard to generating data for randomized controlled trials (RCTs). There is no consensus regarding the best way to generate synthetic tabular RCT data such that the underlying multivariate data distribution is preserved. Motivated by an RCT in the treatment of Human Immunodeficiency Virus, we empirically compared the ability of several strategies and three generation techniques (two machine learning, the other a more classical statistical method) to faithfully reproduce realistic data. Our results suggest that using a sequential generation approach with an R-vine copula model to generate baseline variables, followed by a simple random treatment allocation to mimic the RCT setting, and subsequent regression models for variables post-treatment allocation (such as the trial outcome) is the most effective way to generate synthetic tabular RCT data that capture important and realistic features of the real data.},
  archive      = {J_SIM},
  author       = {Niki Z. Petrakos and Erica E. M. Moodie and Nicolas Savy},
  doi          = {10.1002/sim.70227},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70227},
  shortjournal = {Stat. Med.},
  title        = {A framework for generating realistic synthetic tabular data in a randomized controlled trial setting},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accommodating spatial heterogeneity in geographically weighted regression with group penalty. <em>SIM</em>, <em>44</em>(18-19), e70226. (<a href='https://doi.org/10.1002/sim.70226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data composed of multiple datasets, for example, survey study data collected from multiple geographic locations, has become routine and received increasing attention in recent years. Many existing strategies for analyzing such data either ignore the spatial heterogeneity across locations or are inefficient, leading to “suboptimal” estimation. Partly motivated by a Chinese social survey on pension and healthcare, we develop a geographically weighted group lasso regression (GWGPL), which can effectively accommodate spatial heterogeneity and conduct shared variable selection while promoting structural similarity across locations. Another significant development is that the selection and estimation properties for the proposed approach are rigorously proved. Simulation results demonstrate the competitive advantages of the proposed approach compared to the alternatives. Finally, we apply the proposed approach to the “One Thousand People, One Hundred Villages” social survey data to identify variables associated with inpatient treatment costs, outpatient treatment costs, and self-treatment costs, respectively. For inpatient treatment costs, the number of times receiving inpatient treatment in the past year, household expenditure, whether self-treatments were sought, mental health status, medical institution status, insurance usage, and marital status are identified to be associated variables. Compared to the alternatives, the proposed approach has a grouping structure, smoother coefficient estimation results across locations, and a better prediction performance.},
  archive      = {J_SIM},
  author       = {Tengdi Zheng and Rong Li and Mixia Wu and Chenjin Ma},
  doi          = {10.1002/sim.70226},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70226},
  shortjournal = {Stat. Med.},
  title        = {Accommodating spatial heterogeneity in geographically weighted regression with group penalty},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric estimation of transition intensities in interval-censored markov multistate models without loops. <em>SIM</em>, <em>44</em>(18-19), e70225. (<a href='https://doi.org/10.1002/sim.70225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-censored multistate data is collected when the state of a subject is observed periodically. The analysis of such data using nonparametric multistate models was not possible until recently but is very desirable as it allows for more flexibility than its parametric counterparts. The single available result to date has some unique drawbacks. We propose a nonparametric estimator of the transition intensities for interval-censored multistate data using an Expectation Maximization algorithm. The method allows for a mix of interval-censored and right-censored (exactly observed) transitions. A condition to check for the convergence of the algorithm is given. A simulation study comparing the proposed estimator to a consistent estimator is performed and shown to yield near identical estimates at smaller computational cost. A data set on the emergence of teeth in children is analyzed. Software to perform the analyses is publicly available.},
  archive      = {J_SIM},
  author       = {Daniel Gomon and Hein Putter},
  doi          = {10.1002/sim.70225},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70225},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric estimation of transition intensities in interval-censored markov multistate models without loops},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel regression and poststratification using margins of poststratifiers: Improving inference for HIV health outcomes during the COVID-19 pandemic. <em>SIM</em>, <em>44</em>(18-19), e70223. (<a href='https://doi.org/10.1002/sim.70223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilevel regression and poststratification (MRP) has surged in popularity for population inference using survey samples. The method consists of two stages: First, fitting a regularized model that regresses the outcome on poststratification variables; second, predicting the outcome using the regularized model and aggregating the predictions to make population inferences. Existing MRP methods mostly focus on settings where the joint distribution of the poststratifiers is known in the population. However, in practice, such data information is often not available; instead, we are provided with the margins of the poststratifiers. Motivated by this challenge, we propose an adapted MRP in which we model both the survey outcome that we would like to estimate in the population and the population sizes of subgroups formed by the poststratifiers. We consider Poisson and negative binomial models for the population sizes of subgroups when the number of poststratifiers is small and Bayesian additive regression trees when there are many poststratifying variables. We apply the adapted MRP to estimate the proportion of viral load suppression and means of mental and physical health scales among persons with HIV in New York City using the 2018–2021 wave of the Community Health Advisory and Information Network survey, in which sampling and in-person data collection were disrupted by the COVID-19 pandemic.},
  archive      = {J_SIM},
  author       = {Amy J. Pitts and Maiko Yomogida and Angela Aidala and Andrew Gelman and Qixuan Chen},
  doi          = {10.1002/sim.70223},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70223},
  shortjournal = {Stat. Med.},
  title        = {Multilevel regression and poststratification using margins of poststratifiers: Improving inference for HIV health outcomes during the COVID-19 pandemic},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient two-dimensional functional mixed-effect model framework for repeatedly measured functional data. <em>SIM</em>, <em>44</em>(18-19), e70222. (<a href='https://doi.org/10.1002/sim.70222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in wearable device technology have enabled accelerometers to continuously record minute-by-minute physical activity over consecutive days, yielding curves serially correlated in dense and regular longitudinal design. Motivated by a large-scale cohort of physical activity data throughout a week, the collected repeatedly measured functional data exhibits longitudinal (interday) and functional (intraday) interactions on fine grids. To accommodate this complex data structure and investigate the relationship between health assessment results and weekly physical activity patterns, we propose an innovative and efficient two-dimensional functional mixed-effect model (2dFMM), characterizing the longitudinal and functional cross-variability while incorporating two-dimensional fixed effects and four-dimensional correlation structure in marginal representation. We develop a fast three-stage estimation procedure to provide accurate fixed-effect inference for model interpretability and improve computational efficiency when encountering large datasets. We find strong evidence of intraday and interday varying significant associations between physical activity and mental health assessments among our cohort population, which sheds light on possible intervention strategies targeting daily physical activity patterns to improve school adolescent mental health. Our method is also used in environmental data to illustrate the wide applicability.},
  archive      = {J_SIM},
  author       = {Cheng Cao and Jiguo Cao and Hao Pan and Yunting Zhang and Fan Jiang and Xinyue Li},
  doi          = {10.1002/sim.70222},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70222},
  shortjournal = {Stat. Med.},
  title        = {An efficient two-dimensional functional mixed-effect model framework for repeatedly measured functional data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified framework for modeling feedback and endogeneity in longitudinal binary outcomes using bayesian methods. <em>SIM</em>, <em>44</em>(18-19), e70221. (<a href='https://doi.org/10.1002/sim.70221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal studies with binary outcomes often involve time-dependent covariates that evolve alongside outcomes, introducing feedback effects and endogeneity. Standard approaches, such as Generalized Estimating Equations (GEE) and Generalized Linear Mixed Models (GLMM), typically assume covariate exogeneity, leading to biased and inconsistent estimates when feedback is present. We propose a unified three-step hierarchical Bayesian framework to address these challenges. First, the Generalized Method of Moments (GMM) identifies valid instruments to correct for endogeneity. Second, a Bayesian hierarchical logistic regression model estimates outcome probabilities while accounting for within-subject correlation. Third, a reversal model explicitly captures feedback by modeling time-dependent covariates as outcomes influenced by prior responses. Through simulations, we demonstrate that the proposed framework substantially reduces bias, lowers root mean squared error (RMSE), and improves uncertainty quantification compared to traditional methods, especially under moderate to strong feedback. We illustrate the practical utility of the approach using a synthetic longitudinal diabetes dataset, where feedback between prior glucose levels and self-monitoring behavior meaningfully alters inference. This integrated framework provides a flexible and interpretable solution for longitudinal binary data analysis, particularly in clinical, behavioral, and public health research.},
  archive      = {J_SIM},
  author       = {Lori P. Selby and Ruoqian Liu and Jeffrey R. Wilson},
  doi          = {10.1002/sim.70221},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70221},
  shortjournal = {Stat. Med.},
  title        = {A unified framework for modeling feedback and endogeneity in longitudinal binary outcomes using bayesian methods},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new dirichlet-multinomial mixture regression model for the analysis of microbiome data. <em>SIM</em>, <em>44</em>(18-19), e70220. (<a href='https://doi.org/10.1002/sim.70220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the challenges in analyzing gut microbiome and metagenomic data, this paper introduces a novel mixture distribution for multivariate counts and a regression model built upon it. The flexibility and interpretability of the proposed distribution accommodate both negative and positive dependence among taxa and are accompanied by numerous theoretical properties, including explicit expressions for inter- and intraclass correlations, thereby providing a powerful tool for understanding complex microbiome interactions. Furthermore, the regression model based on this distribution facilitates the clear identification and interpretation of relationships between taxa and covariates by modeling the marginal mean of the multivariate response (i.e., taxa counts). Inference is performed using a tailored Hamiltonian Monte Carlo estimation method combined with a spike-and-slab variable selection procedure. Extensive simulation studies and an application to a human gut microbiome dataset highlight the proposed model's substantial improvements over competing models in terms of fit, interpretability, and predictive performance.},
  archive      = {J_SIM},
  author       = {Roberto Ascari and Sonia Migliorati and Andrea Ongaro},
  doi          = {10.1002/sim.70220},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70220},
  shortjournal = {Stat. Med.},
  title        = {A new dirichlet-multinomial mixture regression model for the analysis of microbiome data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian expectile joint model with varying coefficient for longitudinal and semi-competing risks data. <em>SIM</em>, <em>44</em>(18-19), e70219. (<a href='https://doi.org/10.1002/sim.70219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of clinical medical research, semi-competing risks data are usually observed in practice, yet there are few studies on the joint models of longitudinal and semi-competing risks data. In this paper, a joint model for longitudinal and semi-competing risks data is proposed. Based on the expectile regression, a linear mixed-effects longitudinal sub-model is formulated, and a Cox proportional hazards survival sub-model is considered under the framework of semi-competing risks. The two sub-models are linked by a shared longitudinal trajectory function. To accommodate the time-varying relationship between the longitudinal response variable and covariates, as well as to introduce flexibility to the structural linkage between longitudinal and survival processes, we incorporate the time-varying coefficients into the joint model in the form of nonparametric functions. The simultaneous Bayesian inference method is utilized to estimate the model parameters, which not only overcomes the convergence problem, but also improves the accuracy of the parameter estimation while effectively reducing the computational burden. The simulation studies are conducted to assess the performance of the proposed joint model and methodology. Finally, we analyze a dataset from the Multicenter AIDS Cohort Study to illustrate the real application of the proposed model and method. In both simulation studies and empirical analyses, joint modeling methods demonstrate performance that meets expected effects.},
  archive      = {J_SIM},
  author       = {Feng Gu and Jiaqing Chen and Jinjing Wang and Yibo Long and Xiaofan Wang and Yangxin Huang},
  doi          = {10.1002/sim.70219},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70219},
  shortjournal = {Stat. Med.},
  title        = {Bayesian expectile joint model with varying coefficient for longitudinal and semi-competing risks data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random survival forest with multiple imputation analysis for case-cohort and generalized case-cohort studies. <em>SIM</em>, <em>44</em>(18-19), e70218. (<a href='https://doi.org/10.1002/sim.70218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case-cohort and generalized case-cohort study designs are widely used in epidemiological studies as a cost-effective alternative to much larger cohort studies while achieving similar efficiency. Current research mostly focuses on the estimation and inference of semiparametric survival models, with limited attention directed towards nonparametric survival predictions within these study designs. In this paper, we propose to use the random survival forest with multiple imputation by chained equation (RSF-MICE) and the random survival forest with substantive model compatible imputation (RSF-SMCI) for survival prediction in case-cohort and generalized case-cohort studies. Simulation studies demonstrate superior finite-sample performance for both proposed methods in various situations. We also apply our new approach to predict the incident diabetes in an analysis of a generalized case-cohort study conducted as a part of the Atherosclerosis Risk in Communities Study.},
  archive      = {J_SIM},
  author       = {Haolin Li and Haibo Zhou and David Couper and Jianwen Cai},
  doi          = {10.1002/sim.70218},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70218},
  shortjournal = {Stat. Med.},
  title        = {Random survival forest with multiple imputation analysis for case-cohort and generalized case-cohort studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Poisson beta regression for count data with an application to hospital length of stay data. <em>SIM</em>, <em>44</em>(18-19), e70217. (<a href='https://doi.org/10.1002/sim.70217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been growing awareness recently that conventional models for count data, such as the Negative Binomial model and zero inflated models, often yield poor fit and sub-optimal performance when applied to real-world count data problems. In response, a new, more flexible model for count data, the Poisson-Beta model, has started to attract attention. The Poisson-Beta model is a Poisson mixture where the underlying mixing distribution is a scaled Beta density. However, because its density function cannot be expressed in closed form, its use has been limited to very simple applications such as parameter estimation. This work presents a method of overcoming the computational complexity issues associated with the Poisson-Beta density to allow its application to problems of far greater complexity, enabling it to be used to model response variables in multivariate regression. This work additionally demonstrates that Poisson-Beta regression compares favorably to a range of commonly used regression models for count response data, achieving narrower confidence intervals and superior power.},
  archive      = {J_SIM},
  author       = {Alan Herschtal},
  doi          = {10.1002/sim.70217},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70217},
  shortjournal = {Stat. Med.},
  title        = {Poisson beta regression for count data with an application to hospital length of stay data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing population heterogeneity for HIV incidence estimation based on recency test. <em>SIM</em>, <em>44</em>(18-19), e70216. (<a href='https://doi.org/10.1002/sim.70216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-sectional HIV incidence estimation leverages recency test results to determine the HIV incidence of a population of interest, where recency test uses biomarker profiles to infer whether an HIV-positive individual was “recently” infected. This approach possesses an obvious advantage over the conventional cohort follow-up method since it avoids longitudinal follow-up and repeated HIV testing. In this manuscript, we consider the extension of cross-sectional incidence estimation to estimate the incidence of a different target population, addressing potential population heterogeneity. We propose a general framework that incorporates two scenarios: one when the target population is a subset of the population with cross-sectional recency testing data and the other with an external target population. In addition, we propose estimators to incorporate HIV subtypes, a special type of covariate that modifies the properties of recency tests, into our framework. Through simulation studies and a data application, we demonstrate the performance of the proposed methods. We conclude with a discussion on sensitivity analysis and future work to improve our framework.},
  archive      = {J_SIM},
  author       = {Qi Wang and Ann Duerr and Fei Gao},
  doi          = {10.1002/sim.70216},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70216},
  shortjournal = {Stat. Med.},
  title        = {Addressing population heterogeneity for HIV incidence estimation based on recency test},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Health utility survival for randomized clinical trials: Extensions and statistical properties. <em>SIM</em>, <em>44</em>(18-19), e70215. (<a href='https://doi.org/10.1002/sim.70215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overall survival has been used as the primary endpoint for many randomized trials that aim to examine whether a new treatment is non-inferior to the standard treatment or placebo control. When a new treatment is indeed non-inferior in terms of survival, it may be important to assess other outcomes including health utility. However, analyzing health utility scores in a secondary analysis may have limited power since the primary objectives of the original study design may not include health utility. To comprehensively consider both survival and health utility, we developed a composite endpoint, HUS (Health Utility-adjusted Survival), which combines both survival and utility. HUS has been shown to be able to increase statistical power and potentially reduce the required sample size compared to the standard overall survival endpoint. Nevertheless, the asymptotic properties of the test statistics of the HUS endpoint have yet to be fully established. Besides that, the standard version of HUS cannot be applied to or has limited performance in certain scenarios, where extensions are needed. In this manuscript, we propose various methodological extensions of HUS and derive the asymptotic distributions of the test statistics. By comprehensive simulation studies and a data application using retrospective data based on a translational patient cohort in Princess Margaret Cancer Centre, we demonstrate the better efficiency and feasibility of HUS compared to different methods.},
  archive      = {J_SIM},
  author       = {Yangqing Deng and Meiling Hao and Shao Hui Huang and Geoffrey Liu and John R. de Almeida and Wei Xu},
  doi          = {10.1002/sim.70215},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70215},
  shortjournal = {Stat. Med.},
  title        = {Health utility survival for randomized clinical trials: Extensions and statistical properties},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric estimation of the potential impact fraction and the population attributable fraction with individual-level and aggregated data. <em>SIM</em>, <em>44</em>(18-19), e70214. (<a href='https://doi.org/10.1002/sim.70214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of the potential impact fraction, including the population attributable fraction, with continuous exposure data frequently relies on strong distributional assumptions. However, these assumptions are often violated if the underlying exposure distribution is unknown. In this article, we discuss the impact of distributional assumptions in the estimation of the population impact fraction, showing that distributional violations lead to biased estimates. We propose nonparametric methods to estimate the potential impact fraction for aggregated data, where only the exposure mean and standard deviation are available, or individual data, where the full exposure distribution can be estimated from a sample of the target population. The finite sample performance of the proposed methods is demonstrated through simulation studies. We illustrate our methodology with a study of the impact of eliminating sugar-sweetened beverage consumption on the incidence of type 2 diabetes in Mexico. We also developed the R package pifpaf to implement these methods.},
  archive      = {J_SIM},
  author       = {Colleen E. Chan and Rodrigo Zepeda-Tello and Dalia Camacho-García-Formentí and Frederick Cudhea and Rafael Meza and Eliane Rodrigues and Donna Spiegelman and Tonatiuh Barrientos-Gutierrez and Xin Zhou},
  doi          = {10.1002/sim.70214},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70214},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric estimation of the potential impact fraction and the population attributable fraction with individual-level and aggregated data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScCOSMIX: A mixed-effects framework for differential coexpression and transcriptional interactions modeling in single-cell RNA-seq. <em>SIM</em>, <em>44</em>(18-19), e70213. (<a href='https://doi.org/10.1002/sim.70213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in single-cell RNA-sequencing (scRNA-seq) technologies generate a wealth of gene expression data that provide exciting opportunities for studying gene-gene interactions systematically at individual cell resolution. Genetic interactions within a cell are tightly regulated and often highly dynamic in response to internal cellular signals and external stimuli. Evidence of these dynamic interactions can often be observed in scRNA-seq data by examining conditional co-expression changes. Existing approaches for studying these dynamic interaction changes in scRNA-seq data do not address the multi-subject hierarchical design commonly considered in single-cell experiments. In this paper, we propose a Mixed-effects framework for differential Coexpression and transcriptional interaction modeling in Single-Cell RNA-seq (scCOSMiX) to account for the cell-cell correlation from the same individual. The proposed copula-based approach allows the zero-inflation, marginal, and association parameters to be modeled as functions of covariates with subject-level random effects, to enable analyses to be tailored to the data under consideration. A series of simulation analyses were conducted to evaluate and compare the performance of scCOSMiX to other existing approaches. We applied the proposed method to both droplet and plate-based scRNA-seq data sets GSE266919 and GSE108989 to illustrate its applicability across distinct scRNA-seq experimental protocols.},
  archive      = {J_SIM},
  author       = {Anderson Bussing and Giampiero Marra and Daping Fan and Russell Shinohara and Danni Tu and Yen-Yi Ho},
  doi          = {10.1002/sim.70213},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70213},
  shortjournal = {Stat. Med.},
  title        = {ScCOSMIX: A mixed-effects framework for differential coexpression and transcriptional interactions modeling in single-cell RNA-seq},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing covariate balance with small sample sizes. <em>SIM</em>, <em>44</em>(18-19), e70212. (<a href='https://doi.org/10.1002/sim.70212'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity score adjustment addresses confounding by balancing covariates in subject treatment groups through matching, stratification, or weighting. Diagnostics test the success of adjustment. For example, if the standardized mean difference (SMD) for a relevant covariate exceeds a threshold like 0.1, the covariate is considered imbalanced and the study may be biased. Unfortunately, for studies with small or moderate numbers of subjects, the probability of identifying a study as biased because of chance imbalance can be grossly larger than a given nominal level like 0.05, yet that chance imbalance may not cause significant bias. In this paper, we illustrate that chance imbalance is operative in real-world settings even for moderate sample sizes of 2000. We identify a previously unrecognized challenge that as meta-analyses increase the precision of an effect estimate, the diagnostics must also undergo meta-analysis for a corresponding increase in precision. We propose an alternative diagnostic that checks whether the SMD statistically significantly exceeds the threshold. Through simulation and real-world data, we find that this diagnostic achieves a better trade-off of type 1 error rate and power than standard nominal threshold tests and not testing for sample sizes from 250 to 4000 and for 20 to 100 000 covariates. We confirm that in network studies, meta-analysis of effect estimates must be accompanied by meta-analysis of the diagnostics or else systematic confounding may overwhelm the estimated effect. Our procedure supports the review of large numbers of covariates, enabling more rigorous diagnostics.},
  archive      = {J_SIM},
  author       = {George Hripcsak and Linying Zhang and Yong Chen and Kelly Li and Marc A. Suchard and Patrick B. Ryan and Martijn J. Schuemie},
  doi          = {10.1002/sim.70212},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70212},
  shortjournal = {Stat. Med.},
  title        = {Assessing covariate balance with small sample sizes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian mixture of exponential family factor models for uncovering disease progression subtypes. <em>SIM</em>, <em>44</em>(18-19), e70211. (<a href='https://doi.org/10.1002/sim.70211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients affected by neurological disorders usually present substantial heterogeneity in multi-domain biomarkers and clinical measures. This heterogeneity arises from differences in disease stage, unique characteristics, and membership in distinct latent subtypes. Exploring such complex heterogeneity and identifying disease progression-related markers is crucial for early diagnosis and developing timely and targeted interventions. This paper proposes a mixture exponential family trajectory model to integrate markers from multiple modalities to learn the disease progression. We incorporate continuous neuroimaging and microRNA sequencing biomarkers, categorical clinical symptoms, and ordinal cognitive markers using appropriate exponential family distributions with lower-dimensional latent factors. The mixture model assigns subtype-specific parameters to these distributions for each mixture component, enabling the characterization of patients in heterogeneous latent subgroups. The proposed model can also describe the nonlinear trajectory of disease deterioration and provide a temporal sequence of decline for each marker. We develop a Bayesian estimation procedure coupled with efficient Markov chain Monte Carlo (MCMC) sampling schemes to perform statistical inference for the mixture model. The proposed method is assessed through extensive simulation studies and an application to Parkinson's Progression Markers Initiative (PPMI) to learn the temporal ordering and subtypes of neurodegeneration of Parkinson's disease (PD).},
  archive      = {J_SIM},
  author       = {Kai Kang and Qinxia Wang and Zhanpeng Xu and Yuanjia Wang},
  doi          = {10.1002/sim.70211},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70211},
  shortjournal = {Stat. Med.},
  title        = {A bayesian mixture of exponential family factor models for uncovering disease progression subtypes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constructing a T-test for value function comparison of individualized treatment regimes in the presence of multiple imputation for missing data. <em>SIM</em>, <em>44</em>(18-19), e70210. (<a href='https://doi.org/10.1002/sim.70210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal individualized treatment decision-making has improved health outcomes in recent years. The value function is commonly used to evaluate the goodness of an individualized treatment decision rule. Despite recent advances, comparing value functions between different treatment decision rules or constructing confidence intervals around value functions remains difficult. We propose a t -test based method applied to a test set that generates valid p -values to compare value functions between a given pair of treatment decision rules when some of the data are missing. We demonstrate the ease in use of this method and evaluate its performance via simulation studies and apply it to the China Health and Nutrition Survey data.},
  archive      = {J_SIM},
  author       = {Minxin Lu and Annie Green Howard and Penny Gordon-Larsen and Katie A. Meyer and Hsiao-Chuan Tien and Shufa Du and Huijun Wang and Bing Zhang and Michael R. Kosorok},
  doi          = {10.1002/sim.70210},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70210},
  shortjournal = {Stat. Med.},
  title        = {Constructing a T-test for value function comparison of individualized treatment regimes in the presence of multiple imputation for missing data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A statistical method for Benefit–Risk assessment in clinical trials. <em>SIM</em>, <em>44</em>(18-19), e70208. (<a href='https://doi.org/10.1002/sim.70208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, benefit–risk assessment (BRA) has become a crucial tool in guiding regulatory decisions regarding the safety and efficacy of investigational treatments. Following the FDA's recent guidance on the BRA, we statistically develop a confidence ellipse approach that considers both safety and efficacy indices. This method addresses limitations in existing BRA frameworks by providing a visual and quantitative tool that accounts for uncertainties and heterogeneities in clinical trial data. We introduce three novel indices: the Net Benefit Index (NBI), Relative Benefit Index (RBI), and Success Rate Index (SRI), offering a comprehensive assessment of treatment performance. Through extensive simulations, we compared the confidence ellipse method with established BRA techniques such as global benefit–risk (GBR) assessment (GBR), multi-criteria decision analysis (MCDA), and stochastic multi-criteria acceptability analysis (SMAA). Our results showed the confidence ellipse method's robustness, particularly with larger sample sizes, and demonstrated decreasing sensitivity as sample sizes increased, reflecting enhanced stability. To illustrate the method's practical application, we present a hypothetical Phase 2 clinical trial comparing a new drug combined with chemoradiotherapy against placebo for locally advanced squamous cell carcinoma of the head and neck. The confidence ellipse approach successfully provided a nuanced benefit–risk assessment, incorporating both unweighted and weighted analyses for adverse events. Overall, the confidence ellipse method is a robust, adaptable, and interpretable tool for benefit–risk assessment, offering valuable visual and quantitative insights for decision-making in modern clinical trials and regulatory processes.},
  archive      = {J_SIM},
  author       = {Yinuo Zhang and Shein-Chung Chow},
  doi          = {10.1002/sim.70208},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70208},
  shortjournal = {Stat. Med.},
  title        = {A statistical method for Benefit–Risk assessment in clinical trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel clinical trial design with stratum-specific endpoints and global test methods for rare diseases with heterogeneous clinical manifestations. <em>SIM</em>, <em>44</em>(18-19), e70206. (<a href='https://doi.org/10.1002/sim.70206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many rare disease clinical trials are underpowered to detect a moderate treatment effect of an investigational product due to the limited number of participants available for the trials. In addition, given the complex, multisystemic nature of many rare diseases, it is challenging to confidently prespecify a single primary efficacy endpoint that is applicable to all trial participants with a heterogeneous clinical manifestation of their disease. Traditional trial designs and analysis methods often used in more common diseases to analyze the same endpoint(s) for all patients may be inefficient or impractical for a rare disease with heterogeneous clinical manifestations. To address these issues, we propose a novel trial design and analytic approach that allows for an evaluation of stratum-specific efficacy endpoints in a broader population of participants. We develop several nonparametric global test methods that can accommodate the novel design and provide global evaluation of treatment effects. Using a case example in patients with Fabry disease, our simulation studies illustrate that the novel design evaluated using the global test methods may be more sensitive to detect a treatment effect compared to the traditional design that uses the same endpoint(s) for all patients.},
  archive      = {J_SIM},
  author       = {Emily Shives and Yared Gurmu and Wonyul Lee and Emily Morris and Yan Wang},
  doi          = {10.1002/sim.70206},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70206},
  shortjournal = {Stat. Med.},
  title        = {Novel clinical trial design with stratum-specific endpoints and global test methods for rare diseases with heterogeneous clinical manifestations},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing and estimation of treatment effects in clinical trials for terminal and nonterminal events subject to competing risks. <em>SIM</em>, <em>44</em>(18-19), e70205. (<a href='https://doi.org/10.1002/sim.70205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, multiple outcomes are often of interest. For time-to-event outcomes, it is the norm rather than exception that one or more nonterminal events (e.g., heart failure hospitalization) and a terminal event (e.g., cardiovascular death) are encountered, subject to competing risks in addition to independent censoring. In this setting, traditionally, time-to-first event analysis is used to assess treatment effect, with popular procedures such as the log-rank test and the Cox proportional hazards regression. However, this approach fails to fully utilize available data, resulting in relatively wide confidence intervals and less powerful tests. Various methods have been proposed in recent years, some involving complex models and others having intuitive appeal but hidden conditions. Furthermore, each method was motivated to address specific scientific questions. Navigating this landscape can be challenging for users. In addition, although competing risks can often be accommodated by considering cause-specific hazards, that is not the case for some of the recently proposed methods, which may lead to difficulty in interpretation, requirement of strong assumptions, and less generalizability. This article provides a brief summary of several recently developed methods, discussing the strengths and limitations of each approach. Then, the problem is framed in a more general setup than in the literature for a wider range of competing risks scenarios. Furthermore, new nonparametric testing and estimation procedures are proposed, and their asymptotic validity is established. The methods are illustrated in a recent large scale clinical trial.},
  archive      = {J_SIM},
  author       = {Song Yang},
  doi          = {10.1002/sim.70205},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70205},
  shortjournal = {Stat. Med.},
  title        = {Testing and estimation of treatment effects in clinical trials for terminal and nonterminal events subject to competing risks},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A primer on inference and prediction with epidemic renewal models and sequential monte carlo. <em>SIM</em>, <em>44</em>(18-19), e70204. (<a href='https://doi.org/10.1002/sim.70204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Renewal models are widely used in statistical epidemiology as semi-mechanistic models of disease transmission. While primarily used for estimating the instantaneous reproduction number, they can also be used for generating projections, estimating elimination probabilities, modeling the effect of interventions, and more. We demonstrate how simple sequential Monte Carlo methods (also known as particle filters) can be used to perform inference on these models. Our goal is to acquaint a reader who has a working knowledge of statistical inference with these methods and models and to provide a practical guide to their implementation. We focus on these methods' flexibility and their ability to handle multiple statistical and other biases simultaneously. We leverage this flexibility to unify existing methods for estimating the instantaneous reproduction number and generating projections. A companion website SMC and epidemic renewal models provides additional worked examples, self-contained code to reproduce the examples presented here, and additional materials.},
  archive      = {J_SIM},
  author       = {Nicholas Steyn and Kris V. Parag and Robin N. Thompson and Christl A. Donnelly},
  doi          = {10.1002/sim.70204},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70204},
  shortjournal = {Stat. Med.},
  title        = {A primer on inference and prediction with epidemic renewal models and sequential monte carlo},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining missing data imputation and internal validation in clinical risk prediction models. <em>SIM</em>, <em>44</em>(18-19), e70203. (<a href='https://doi.org/10.1002/sim.70203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods to handle missing data have been extensively explored in the context of estimation and descriptive studies, with multiple imputation being the most widely used method in clinical research. However, in the context of clinical risk prediction models, where the goal is often to achieve high prediction accuracy and to make predictions for future patients, there are different considerations regarding the handling of missing covariate data. As a result, deterministic imputation is better suited to the setting of clinical risk prediction models, since the outcome is not included in the imputation model and the imputation method can be easily applied to future patients. In this paper, we provide a tutorial demonstrating how to conduct bootstrapping followed by deterministic imputation of missing covariate data to construct and internally validate the performance of a clinical risk prediction model in the presence of missing data. Simulation study results are provided to help guide when imputation may be appropriate in real-world applications.},
  archive      = {J_SIM},
  author       = {Junhui Mi and Rahul D. Tendulkar and Sarah M. C. Sittenfeld and Sujata Patil and Emily C. Zabor},
  doi          = {10.1002/sim.70203},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70203},
  shortjournal = {Stat. Med.},
  title        = {Combining missing data imputation and internal validation in clinical risk prediction models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence intervals for adaptive trial designs II: Case study and practical guidance. <em>SIM</em>, <em>44</em>(18-19), e70202. (<a href='https://doi.org/10.1002/sim.70202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In adaptive clinical trials, the conventional confidence interval (CI) for a treatment effect is prone to undesirable properties such as undercoverage and potential inconsistency with the final hypothesis testing decision. Accordingly, as is stated in recent regulatory guidance on adaptive designs, there is the need for caution in the interpretation of CIs constructed during and after an adaptive clinical trial. However, it may be unclear which of the available CIs in the literature are preferable. This paper is the second in a two-part series that explores CIs for adaptive trials. Part I provided a methodological review of approaches to construct CIs for adaptive designs. In this paper (Part II), we present an extended case study based around a two-stage group sequential trial, including a comprehensive simulation study of the proposed CIs for this setting. This facilitates an expanded description of considerations around what makes for an effective CI procedure following an adaptive trial. We show that the CIs can have notably different properties. Finally, we propose a set of guidelines for researchers around the choice of CIs and the reporting of CIs following an adaptive design.},
  archive      = {J_SIM},
  author       = {David S. Robertson and Thomas Burnett and Babak Choodari-Oskooei and Munya Dimairo and Michael Grayling and Philip Pallmann and Thomas Jaki},
  doi          = {10.1002/sim.70202},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70202},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals for adaptive trial designs II: Case study and practical guidance},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible empirical bayesian approaches to pharmacovigilance for simultaneous signal detection and signal strength estimation in spontaneous reporting systems data. <em>SIM</em>, <em>44</em>(18-19), e70195. (<a href='https://doi.org/10.1002/sim.70195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring adverse events (AEs) of medical products from Spontaneous Reporting Systems (SRS) databases is a core challenge in contemporary pharmacovigilance. Bayesian methods for pharmacovigilance are attractive for their rigorous ability to simultaneously detect potential AE signals and estimate their strengths/degrees of relevance. However, existing Bayesian and empirical Bayesian methods impose restrictive parametric assumptions and/or demand substantial computational resources, limiting their practical utility. This paper introduces a suite of novel, scalable empirical Bayes methods for pharmacovigilance that utilize flexible non-parametric priors and custom, efficient data-driven estimation techniques to enhance signal detection and signal strength estimation at a low computational cost. Our highly flexible methods accommodate a broader range of data and achieve signal detection performance comparable to or better than existing Bayesian and empirical Bayesian approaches. More importantly, they provide coherent and high-fidelity estimation and uncertainty quantification for potential AE signal strengths, offering deeper insights into the comparative importance and relevance of AEs. Extensive simulation experiments across diverse data-generating scenarios demonstrate the superiority of our methods in terms of accurate signal strength estimation, as measured by replication root mean squared errors. Additionally, our methods maintain or exceed the signal detection performance of state-of-the-art techniques, as evaluated by frequentist false discovery rates and sensitivity metrics. Applications on FDA FAERS data for the statin group of drugs reveal interesting insights through Bayesian posterior probabilities.},
  archive      = {J_SIM},
  author       = {Yihao Tan and Marianthi Markatou and Saptarshi Chakraborty},
  doi          = {10.1002/sim.70195},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70195},
  shortjournal = {Stat. Med.},
  title        = {Flexible empirical bayesian approaches to pharmacovigilance for simultaneous signal detection and signal strength estimation in spontaneous reporting systems data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biomarker combination based on the youden index with and without gold standard. <em>SIM</em>, <em>44</em>(18-19), e70189. (<a href='https://doi.org/10.1002/sim.70189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical practice, multiple biomarkers are often measured on the same subject for disease diagnosis, and combining them can improve diagnostic accuracy. Existing studies typically combine multiple biomarkers by maximizing the area under the ROC curve (AUC), assuming a gold standard exists or that biomarkers follow a multivariate normal distribution. However, practical diagnostic settings require both optimal combination coefficients and an effective cutoff value, and the reference test may be imperfect. In this article, we propose a two-stage method for identifying the optimal linear combination and cutoff value based on the Youden index. First, it maximizes an approximation of the empirical AUC to estimate the optimal linear coefficients for combining multiple biomarkers. Then, it maximizes the empirical Youden index to determine the optimal cutoff point for disease classification. Under the semiparametric single index model and regularity conditions, the estimators for the linear coefficients, cutoff point, and Youden index are consistent. This method is also applicable when the reference standard is imperfect. We demonstrate the performance of our method through simulations and apply it to construct a diagnostic scale for Chinese medicine.},
  archive      = {J_SIM},
  author       = {Ao Sun and Yanting Li and Xiao-Hua Zhou},
  doi          = {10.1002/sim.70189},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70189},
  shortjournal = {Stat. Med.},
  title        = {Biomarker combination based on the youden index with and without gold standard},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence intervals for adaptive trial designs i: A methodological review. <em>SIM</em>, <em>44</em>(18-19), e70174. (<a href='https://doi.org/10.1002/sim.70174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regulatory guidance notes the need for caution in the interpretation of confidence intervals (CIs) constructed during and after an adaptive clinical trial. Conventional CIs of the treatment effects are prone to undercoverage (as well as other undesirable properties) in many adaptive designs (ADs) because they do not take into account the potential and realized trial adaptations. This paper is the first in a two-part series that explores CIs for adaptive trials. It provides a comprehensive review of the methods to construct CIs for ADs, while the second paper illustrates how to implement these in practice and proposes a set of guidelines for trial statisticians. We describe several classes of techniques for constructing CIs for adaptive clinical trials before providing a systematic literature review of available methods, classified by the type of AD. As part of this, we assess, through a proposed traffic light system, which of several desirable features of CIs (such as achieving nominal coverage and consistency with the hypothesis test decision) each of these methods holds.},
  archive      = {J_SIM},
  author       = {David S. Robertson and Thomas Burnett and Babak Choodari-Oskooei and Munya Dimairo and Michael Grayling and Philip Pallmann and Thomas Jaki},
  doi          = {10.1002/sim.70174},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70174},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals for adaptive trial designs i: A methodological review},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmenting treatment arms with external data through propensity-score weighted power priors: An application in expanded access. <em>SIM</em>, <em>44</em>(18-19), e70168. (<a href='https://doi.org/10.1002/sim.70168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incorporation of real-world data to supplement the analysis of trials and improve decision-making has spurred the development of statistical techniques to account for introduced confounding. Recently, “hybrid” methods have been developed through which measured confounding is first attenuated via propensity scores and unmeasured confounding is addressed through (Bayesian) dynamic borrowing. Most efforts to date have focused on augmenting control arms with historical controls. Here we consider augmenting treatment arms through “expanded access”, which is a pathway of nontrial access to investigational medicine for patients with seriously debilitating or life-threatening illnesses. Motivated by a case study on expanded access, we developed a novel method (the ProPP) that provides a conceptually simple and easy-to-use combination of propensity score weighting and the modified power prior. Our weighting scheme is based on the estimation of the average treatment effect of the patients in the trial, with the constraint that external patients cannot receive higher weights than trial patients. The causal implications of the weighting scheme and propensity-score integrated approaches in general are discussed. In a simulation study, our method compares favorably with existing (hybrid) borrowing methods in terms of precision and type I error rate. We illustrate our method by jointly analyzing individual patient data from the trial and expanded access program for vemurafenib to treat metastatic melanoma. Our method provides a double safeguard against prior-data conflict and forms a straightforward addition to evidence synthesis methods of trial and real-world (expanded access) data.},
  archive      = {J_SIM},
  author       = {Tobias B. Polak and Jeremy A. Labrecque and Carin A. Uyl-de Groot and Joost van Rosmalen},
  doi          = {10.1002/sim.70168},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70168},
  shortjournal = {Stat. Med.},
  title        = {Augmenting treatment arms with external data through propensity-score weighted power priors: An application in expanded access},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cumulative logit ordinal regression with proportional odds under nonignorable missing Responses—Application to phase III trial. <em>SIM</em>, <em>44</em>(18-19), e70114. (<a href='https://doi.org/10.1002/sim.70114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data are inevitable in clinical trials, and trials that produce categorical ordinal responses are not exempted from this. Typically, missing values in the data occur due to different missing mechanisms, such as missing completely at random, missing at random, and missing not at random. Under a specific missing data regime, when the conditional distribution of the missing data is dependent on the ordinal response variable itself along with other predictor variables, then the missing data mechanism is called nonignorable . In this article, we propose an expectation maximization based algorithm for fitting a proportional odds regression model when the missing responses are nonignorable. We report the results from an extensive simulation study to illustrate the methodology and its finite sample properties. We also apply the proposed method to a recently completed Phase III psoriasis study using an investigational compound. The corresponding SAS program is provided.},
  archive      = {J_SIM},
  author       = {Arnab Kumar Maity and Huaming Tan and Vivek Pradhan and Soutir Bandyopadhyay},
  doi          = {10.1002/sim.70114},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70114},
  shortjournal = {Stat. Med.},
  title        = {Cumulative logit ordinal regression with proportional odds under nonignorable missing Responses—Application to phase III trial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex network and topological data analysis methods for county level COVID-19 vaccine acceptance analysis in the united states. <em>SIM</em>, <em>44</em>(18-19), e70109. (<a href='https://doi.org/10.1002/sim.70109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The benefits of vaccination to protect against the different variants of the SARS-CoV-2 Virus are well-known in the literature. In the United States, public health policy has led to a wide availability of COVID-19 vaccines that are usually freely available to everyone 6 months and older. However, several factors including misinformation create vaccine hesitancy and threaten to undercut the advances of the COVID-19 vaccination program. In this article, we take a network-based approach to investigate community acceptance of vaccines at the county level in the United States, using data from the Centers for Disease Control and Prevention (CDC). We use an exponential random graph model to discover important sociodemographic factors that influence the patterns of vaccination between counties and communities. In addition, we undertake an advanced topological data analysis (TDA) based network clustering method to discover more macrolevel communities that show common trends for COVID-19 vaccine acceptance in the United States. Our study uncovers that sociodemographic features, for example, higher education, household income, and US census regions have significant effects on COVID-19 vaccine acceptance. The cluster analysis demonstrates that different census regions as well as rural and urban areas have distinct preferences in COVID-19 vaccine acceptance.},
  archive      = {J_SIM},
  author       = {Asim K. Dey and Suprateek Kundu},
  doi          = {10.1002/sim.70109},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70109},
  shortjournal = {Stat. Med.},
  title        = {Complex network and topological data analysis methods for county level COVID-19 vaccine acceptance analysis in the united states},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doubly robust estimation of marginal cumulative incidence curves for competing risk analysis. <em>SIM</em>, <em>44</em>(18-19), e70066. (<a href='https://doi.org/10.1002/sim.70066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate imbalance between treatment groups makes it difficult to compare cumulative incidence curves in competing risk analyses. In this paper, we discuss different methods to estimate adjusted cumulative incidence curves, including inverse probability of treatment weighting and outcome regression modeling. For these methods to work, correct specification of the propensity score model or outcome regression model, respectively, is needed. We introduce a new doubly robust estimator, which requires correct specification of only one of the two models. We conduct a simulation study to assess the performance of these three methods, including scenarios with model misspecification of the relationship between covariates and treatment and/or outcome. We illustrate their usage in a cohort study of breast cancer patients estimating covariate-adjusted marginal cumulative incidence curves for recurrence, second primary tumor development, and death after undergoing mastectomy treatment or breast-conserving therapy. Our study points out the advantages and disadvantages of each covariate adjustment method when applied in competing risk analysis.},
  archive      = {J_SIM},
  author       = {Patrick van Hage and Saskia le Cessie and Marissa C. van Maaren and Hein Putter and Nan van Geloven},
  doi          = {10.1002/sim.70066},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18-19},
  pages        = {e70066},
  shortjournal = {Stat. Med.},
  title        = {Doubly robust estimation of marginal cumulative incidence curves for competing risk analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general linear mixed effect model to infer biomarker correlations by bridging retrospectively measured data across multiple studies. <em>SIM</em>, <em>44</em>(15-17), e70200. (<a href='https://doi.org/10.1002/sim.70200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge in biomedical research is that large sample sizes are necessary for sufficient statistical power to detect subtle but potentially important associations between biomarkers and clinical outcomes. Large sample sizes can be achieved by combining biomarker data from multiple studies, but because fluid biomarker platforms and imaging protocols often vary across studies, data from different studies must be bridged or harmonized. We conceptualize that, for a biomarker measured by different studies, a true and latent biomarker exists and underlies the different versions of the observed biomarker through a measurement error model. We then examine the true biological correlation of the latent biomarker with a standard clinical outcome by leveraging biomarker values from a subset of “bridging” samples or scans across studies. Because the true biological correlation with the clinical outcome is related to the correlations of the observed versions of the biomarker with the same clinical outcome and the intraclass correlation coefficient (ICC) of the biomarker across studies, we propose a general linear mixed effects model to estimate the true biological correlation by integrating these correlations estimated across the studies and the bridging cohorts. Our proposed model accounts for study heterogeneity through a random effect and allows both study-specific and the test–retest biomarker data in a joint model to estimate and infer on the true biological correlation. We apply the model to a real world multi-center biomarker study in Alzheimer disease to correlate concentrations of cerebrospinal fluid biomarkers with a standard functional and cognitive outcome. Our simulations and real world applications indicate that the proposed meta-analytic model leads to a bias of no more than 0.03 in the estimated biological correlation of a biomarker with a clinical outcome, even with small to mediocre ICC. When the ICC is large, only 10% of bridging samples may be needed to obtain unbiased estimates to the correlation with close to the nominal level of coverage from the proposed 95% CI estimates. Our proposed methodologies hence provide a novel approach to harmonize retrospectively obtained biomarker data across studies, offer guidance on size of the bridging samples when ICCs are known, and may also be used in a single study to account for batch effects.},
  archive      = {J_SIM},
  author       = {Chengjie Xiong and Ruijin Lu and David Wolk and Leslie M. Shaw and Carey E. Gleason and Sterling C. Johnson and Folasade Agboola and Suzanne E. Schindler and John C. Morris and Jingqin Luo},
  doi          = {10.1002/sim.70200},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70200},
  shortjournal = {Stat. Med.},
  title        = {A general linear mixed effect model to infer biomarker correlations by bridging retrospectively measured data across multiple studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast variational bayesian inference for correlated survival data: An application to invasive mechanical ventilation duration analysis. <em>SIM</em>, <em>44</em>(15-17), e70198. (<a href='https://doi.org/10.1002/sim.70198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated survival data are prevalent in various clinical settings and have been extensively discussed in the literature. A common example is clustered survival data, where survival times are associated due to shared characteristics within clusters. In our study, we analyze invasive mechanical ventilation data collected from multiple intensive care units (ICUs) across Ontario, Canada. Patients within the same ICU exhibit similarities in clinical profiles and mechanical ventilation settings, leading to a correlation in their ventilation durations. To address this association, we introduce a shared frailty log-logistic accelerated failure time model that accounts for intra-cluster correlation through a cluster-specific random intercept. We present a novel, fast variational Bayes (VB) algorithm for parameter inference and evaluate its performance using simulation studies varying the number of clusters and their sizes. We further compare the performance of our proposed VB algorithm with the h-likelihood method and a Markov Chain Monte Carlo (MCMC) algorithm. The proposed algorithm delivers satisfactory results and demonstrates computational efficiency over the MCMC algorithm. We apply our method to ICU ventilation data from Ontario to investigate the ICU-site random effect on ventilation duration.},
  archive      = {J_SIM},
  author       = {Chengqian Xian and Camila P. E. de Souza and Wenqing He and Felipe F. Rodrigues and Renfang Tian},
  doi          = {10.1002/sim.70198},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70198},
  shortjournal = {Stat. Med.},
  title        = {Fast variational bayesian inference for correlated survival data: An application to invasive mechanical ventilation duration analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated and coherent framework for point estimation and hypothesis testing with concurrent controls in platform trials. <em>SIM</em>, <em>44</em>(15-17), e70196. (<a href='https://doi.org/10.1002/sim.70196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A platform trial with a master protocol provides an infrastructure to ethically and efficiently evaluate multiple treatment options in multiple diseases. Given that certain study drugs can enter or exit a platform trial, the randomization ratio is possible to change over time, and this potential modification is not necessarily dependent on accumulating outcomes data. It is recommended that the analysis should account for time periods with different randomization ratios, with possible approaches such as inverse probability of treatment weighting or a weighted approach by the time period. To guide practical implementation, we specifically investigate the relationship between these two estimators, and further derive an optimal estimator within this class to gain efficacy. Practical guidance is provided on how to construct estimators based on the observed data to approximate this unknown weight. The connection between the proposed method and the weighted least squares is also studied. We conduct simulation studies to demonstrate that the proposed method can control type I error rate with a reduced estimation bias, and can also achieve satisfactory power and mean squared error with computational efficiency. Another appealing feature of our framework is the ability to provide consistent conclusions for both point estimation and hypothesis testing. This is critical to the interpretation of clinical trial results. The proposed method is further applied to the Accelerating COVID-19 Therapeutic Interventions and Vaccines platform trial.},
  archive      = {J_SIM},
  author       = {Tianyu Zhan and Jane Zhang and Lei Shu and Yihua Gu},
  doi          = {10.1002/sim.70196},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70196},
  shortjournal = {Stat. Med.},
  title        = {An integrated and coherent framework for point estimation and hypothesis testing with concurrent controls in platform trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple diagnostic for the positivity assumption for continuous exposures. <em>SIM</em>, <em>44</em>(15-17), e70194. (<a href='https://doi.org/10.1002/sim.70194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The positivity or experimental treatment assignment assumption is a fundamental requirement in causal analyses, invoked to ensure that identifiability holds without extrapolating beyond what the observed data can reveal. Positivity is well understood in the context of binary and categorical treatments, and has been thoroughly discussed—from how the assumption can be assessed to approaches that may be used when the assumption is suspected not to hold. Positivity extends to the context of continuous exposures, such as doses, however it has been given very little formal consideration. In this manuscript, we propose a method for assessing whether the positivity assumption is violated in a given dataset, relying on a principled concept in regression analysis. We demonstrate the diagnostic tool in various simulated settings, as well as in an application involving warfarin dosing.},
  archive      = {J_SIM},
  author       = {Erica E. M. Moodie and Juliana Schulz},
  doi          = {10.1002/sim.70194},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70194},
  shortjournal = {Stat. Med.},
  title        = {A simple diagnostic for the positivity assumption for continuous exposures},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model validation for survival analysis by smoothed predictive likelihood. <em>SIM</em>, <em>44</em>(15-17), e70193. (<a href='https://doi.org/10.1002/sim.70193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing the predictive performance is a crucial aspect in survival modeling, essential for model selection, tuning parameter determination, and evaluating additional predictive ability. The predictive log-likelihood has been recommended as a suitable evaluation measure, particularly for survival models, which generally return entire survival curves rather than point predictions. However, applying predictive likelihood in semiparametric and nonparametric survival models is problematic since the survival curves are step-functions, which result in zero predictive likelihood when events occur at previously unobserved time points. The most well-known existing solution, Verweij's predictive partial likelihood, is limited to Cox models. In this article, we propose a novel approach based on nearest-neighbor kernel smoothing that is usable in general semi- and nonparametric survival models. We show that our new method performs competitively with existing methods in the Cox setting while offering broader applicability, including testing for the presence of a frailty term and determining the optimal level of smoothness in penalized additive hazards models.},
  archive      = {J_SIM},
  author       = {Chengyuan Lu and Hein Putter and Mar Rodríguez Girondo and Jelle J. Goeman},
  doi          = {10.1002/sim.70193},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70193},
  shortjournal = {Stat. Med.},
  title        = {Model validation for survival analysis by smoothed predictive likelihood},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence intervals for AUC and pAUC by empirical likelihood. <em>SIM</em>, <em>44</em>(15-17), e70192. (<a href='https://doi.org/10.1002/sim.70192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area under the receiver operating characteristic curve (AUC) and Partial AUC (pAUC) are often used to measure the performance of medical diagnostic tests. Under nonparametric settings, we propose and illustrate in this paper a two-sample empirical likelihood approach to test hypotheses and construct confidence intervals for AUC and pAUC. The empirical likelihood ratio test in our setup yields an asymptotic chi-square distribution under null hypothesis. Thus, there is no need to estimate the complicated scale factor or the variance of the nonparametric AUC/pAUC estimators like most other competing methods do. Simulations show our method is very competitive. In fact, our method tops competitors in every situation we simulated. Real data examples (with R code) are presented illustrating the statistical tests and confidence intervals for AUC and pAUC.},
  archive      = {J_SIM},
  author       = {Yumin Zhao and Xue Ding and Mai Zhou},
  doi          = {10.1002/sim.70192},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70192},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals for AUC and pAUC by empirical likelihood},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exposure measurement error correction in longitudinal studies with discrete outcomes. <em>SIM</em>, <em>44</em>(15-17), e70191. (<a href='https://doi.org/10.1002/sim.70191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental epidemiologists are often interested in estimating the effect of time-varying functions of the exposure history on health outcomes. However, the individual exposure measurements that constitute the history upon which an exposure history function is constructed are usually subject to measurement errors. To obtain unbiased estimates of the effects of such mismeasured functions in longitudinal studies with discrete outcomes, a method applicable to the main study/validation study design is developed. Various estimation procedures are explored. Simulation studies were conducted to assess its performance compared to standard analysis, and we found that the proposed method had good performance in terms of finite sample bias reduction and nominal coverage probability improvement. As an illustrative example, we applied the new method to a study of long-term exposure to , in relation to the occurrence of anxiety disorders in the Nurses' Health Study II. Failing to correct the error-prone exposure can lead to an underestimation of the chronic exposure effect of .},
  archive      = {J_SIM},
  author       = {Ce Yang and Ning Zhang and Jiaxuan Li and Unnati V. Mehta and Jaime E. Hart and Donna L. Spiegelman and Molin Wang},
  doi          = {10.1002/sim.70191},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70191},
  shortjournal = {Stat. Med.},
  title        = {Exposure measurement error correction in longitudinal studies with discrete outcomes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network-based dynamic prediction for interval-censored data with time-varying covariates: Application to alzheimer's disease. <em>SIM</em>, <em>44</em>(15-17), e70190. (<a href='https://doi.org/10.1002/sim.70190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer's disease (AD) is a progressive neurodegenerative disorder accounting for a significant proportion of global dementia cases. Given the lack of effective treatments, there is growing interest in dynamic prediction methods for timely interventions. Notably, many at-risk individuals with periodic clinic visits provide dynamic cognitive and functional scores. When an individual receives a new score at each follow-up, the dynamic prediction model can integrate the individual's historical scores with the new follow-up scores to offer an updated risk prediction. This study utilizes a comprehensive dataset from the four phases of the Alzheimer's Disease Neuroimaging Initiative (ADNI) study, comprising 1702 individuals with multiple time-varying cognitive and functional scores and baseline covariates. We address several challenges: Interval-censored time-to-AD due to intermittent assessments, multiple time-varying covariates, and nonlinear covariate effects on AD development. The proposed approach integrates multivariate functional principal component analysis with a neural network; the former extracts important predictive features from multiple time-varying covariates, while the latter handles the nonlinear covariate effects on interval-censored time-to-AD. This method facilitates individualized and dynamic predictions for AD development. Based on simulation results and application to the ADNI dataset, the proposed method outperforms several other methods in terms of prediction accuracy. Furthermore, it identifies high- and low-risk subgroups with distinct progression risk profiles at each landmark time, enabling early and timely intervention of AD. To facilitate dynamic predictions in practice, we have developed an online prediction platform accessible at http://olap.ruc.edu.cn .},
  archive      = {J_SIM},
  author       = {Kexin Liu and Yining Zu and Danhui Yi and Ying Ding and Tao Sun},
  doi          = {10.1002/sim.70190},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70190},
  shortjournal = {Stat. Med.},
  title        = {Neural network-based dynamic prediction for interval-censored data with time-varying covariates: Application to alzheimer's disease},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The mathematics of serocatalytic models with applications to public health data. <em>SIM</em>, <em>44</em>(15-17), e70188. (<a href='https://doi.org/10.1002/sim.70188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serocatalytic models are powerful tools which can be used to infer historical infection patterns from age-structured serological surveys. These surveys are especially useful when disease surveillance is limited and have an important role to play in providing a ground truth gauge of infection burden. In this tutorial, we consider a wide range of serocatalytic models to generate epidemiological insights. With mathematical analysis, we explore the properties and intuition behind these models and include applications to real data for a range of pathogens and epidemiological scenarios. We also include practical steps and code in R and Stan for interested learners to build experience with this modeling framework. Our work highlights the usefulness of serocatalytic models and shows that accounting for the epidemiological context is crucial when using these models to understand infectious disease epidemiology.},
  archive      = {J_SIM},
  author       = {Everlyn Kamau and Junjie Chen and Sumali Bajaj and Nicolás Torres and Richard Creswell and Jaime A. Pavlich-Mariscal and Christl Donnelly and Zulma Cucunubá and Ben Lambert},
  doi          = {10.1002/sim.70188},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70188},
  shortjournal = {Stat. Med.},
  title        = {The mathematics of serocatalytic models with applications to public health data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improve the precision of area under the curve estimation for recurrent events through covariate adjustment. <em>SIM</em>, <em>44</em>(15-17), e70187. (<a href='https://doi.org/10.1002/sim.70187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area under the curve (AUC) of the mean cumulative function (MCF) has recently been introduced as a novel estimand for evaluating treatment effects in recurrent event settings, offering an alternative to the commonly used Lin-Wei-Yang-Ying (LWYY) model. The AUC of the MCF provides a clinically interpretable summary measure that captures the overall burden of disease progression, regardless of whether the proportionality assumption holds. To improve the precision of the AUC estimation while preserving its unconditional interpretability, we propose a nonparametric covariate adjustment approach. This approach guarantees efficiency gain compared to unadjusted analysis, as demonstrated by theoretical asymptotic distributions, and is universally applicable to various randomization schemes, including both simple and covariate-adaptive designs. Extensive simulations across different scenarios further support its advantage in increasing statistical power. Our findings highlight the importance of covariate adjustment for the analysis of AUC in recurrent event settings, offering practical guidance for its application in randomized clinical trials.},
  archive      = {J_SIM},
  author       = {Jiren Sun and Tuo Wang and Yanyao Yi and Ting Ye and Jun Shao and Yu Du},
  doi          = {10.1002/sim.70187},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70187},
  shortjournal = {Stat. Med.},
  title        = {Improve the precision of area under the curve estimation for recurrent events through covariate adjustment},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A latent-class model for time-to-event outcomes and high-dimensional imaging data. <em>SIM</em>, <em>44</em>(15-17), e70186. (<a href='https://doi.org/10.1002/sim.70186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural magnetic resonance imaging (MRI) is one of the primary predictors of Alzheimer's disease risk, enabling the identification of patients with similar risk profiles for precision medicine treatment. Motivated by the need for flexible modeling in AD research, we propose a latent-class model that addresses the heterogeneity within study populations. This model allows for varying relationships between covariates and survival outcomes, accommodating the dynamics of AD progression. The imaging predictors are characterized by bivariate splines over triangulation to accommodate the irregular domain of the brain images. We develop a generalized expectation-maximization (EM) algorithm that combines the computational methods for logistic regression and penalized proportional hazards models to implement the proposed approach. We demonstrate the advantages of the proposed method through extensive simulation studies and provide an application to the Alzheimer's Disease Neuroimaging Initiative (ADNI) study, which helps to reveal different subtypes or stages of the disease process in Alzheimer's Disease.},
  archive      = {J_SIM},
  author       = {Jiahui Feng and Haolun Shi and Ma Da and Mirza Faisal Beg and Jiguo Cao},
  doi          = {10.1002/sim.70186},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70186},
  shortjournal = {Stat. Med.},
  title        = {A latent-class model for time-to-event outcomes and high-dimensional imaging data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance of cross-validated targeted maximum likelihood estimation. <em>SIM</em>, <em>44</em>(15-17), e70185. (<a href='https://doi.org/10.1002/sim.70185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Matthew J. Smith and Rachael V. Phillips and Camille Maringe and Miguel Angel Luque-Fernandez},
  doi          = {10.1002/sim.70185},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70185},
  shortjournal = {Stat. Med.},
  title        = {Performance of cross-validated targeted maximum likelihood estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient estimation method for additive subdistribution hazards model with left-truncated competing risks data under the case-cohort study design. <em>SIM</em>, <em>44</em>(15-17), e70183. (<a href='https://doi.org/10.1002/sim.70183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case-cohort study design provides a cost-effective approach for large cohort studies with competing risks outcomes. The additive subdistribution hazards model assesses direct covariate effects on cumulative incidence when investigating risk differences among different groups instead of relative risk. The presence of left truncation, which commonly occurs in biomedical studies, introduces additional complexities to the analysis. Existing inverse-probability-weighting methods for case-cohort studies on competing risks are inefficient in parameter estimation of coefficients for baseline covariates. In addition, their methods do not address left truncation. To improve the efficiency of parameter estimation of coefficients for baseline covariates and account for left-truncated competing risks data, we propose an augmented-inverse-probability-weighted estimating equation for left-truncated competing risks data with additive subdistribution models under the case-cohort study design. For multiple case-cohort studies, we further improve parameter estimation efficiency by incorporating extra information from the other causes. We study large sample properties of the proposed estimators. Simulation studies demonstrate the unbiasedness of our proposed estimator and the superior efficiency in regression parameter estimation. We apply the proposed methods to analyze data from the Atherosclerosis Risk in Communities study.},
  archive      = {J_SIM},
  author       = {Xi Fang and Kwang Woo Ahn and Jianwen Cai and Soyoung Kim},
  doi          = {10.1002/sim.70183},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70183},
  shortjournal = {Stat. Med.},
  title        = {An efficient estimation method for additive subdistribution hazards model with left-truncated competing risks data under the case-cohort study design},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal linear combination of biomarkers by weighted youden index maximization. <em>SIM</em>, <em>44</em>(15-17), e70182. (<a href='https://doi.org/10.1002/sim.70182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical research, it is common practice to combine various biomarkers to improve the accuracy of disease diagnosis. The weighted Youden index (WYI), which assigns unequal weights to sensitivity and specificity based on their relative importance, serves as an important and flexible evaluation metric of diagnostic tests. However, no existing methods have been designed specifically to identify the optimal linear combination of biomarkers that maximizes the WYI. In this paper, we propose a novel method to construct an optimal diagnosis score and determine the best cut-off point at the same time. The estimated combination coefficients and cut-off point are shown to have cube root asymptotics, and their joint limiting distribution is established rigorously. Further, the asymptotic normality of the optimal in-sample WYI is established, and out-of-sample inference for score distribution and comparison is investigated. These results provide deep theoretical insights for methods of Youden index maximization for the first time. Computationally, an iterative marginal optimization algorithm, different from the existing literature, is adopted to deal with the objective function that is neither continuous nor smooth. Simulation studies support the theoretical results and demonstrate the superiority of the proposed method. Two real-world examples—coronary disease and Alzheimer's disease diagnosis—are presented for illustration.},
  archive      = {J_SIM},
  author       = {Sizhe Wang and Fang Fang and Jialiang Li},
  doi          = {10.1002/sim.70182},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70182},
  shortjournal = {Stat. Med.},
  title        = {Optimal linear combination of biomarkers by weighted youden index maximization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The design of large-scale plasma donation trials. <em>SIM</em>, <em>44</em>(15-17), e70181. (<a href='https://doi.org/10.1002/sim.70181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Devices for extracting blood from donors require rigorous evaluation for the safety of donors. Complications arise from the fact that outcomes on successive donations from the same donor are not independent, adverse event rates are extremely rare, and there is substantial heterogeneity in the propensity for donors to donate over time. We develop a statistical framework for the design of a superiority trial and a non-inferiority trial, aiming to demonstrate the safety of a new donation device compared to the standard one. Historical data on the intensity and heterogeneity of donation across donors, the adverse event rate, and the serial dependence in adverse events provide information on how to plan accrual and follow-up periods to give the expected number of donors and donations. The analysis is based on a binary donation-specific outcome modeled with a Poisson approximation (i.e., log link and identity variance function) using generalized estimating equations with a working independence assumption. The historical data enables calculation of the asymptotic robust variance estimate, which is used for planning. The formulae derived are found to provide good control of the type I error rate and statistical power. We illustrate the derivations with application to a plasma donation trial aiming to investigate the safety of a new device, with the outcome being serious hypotensive adverse events.},
  archive      = {J_SIM},
  author       = {Kecheng Li and Richard J. Cook},
  doi          = {10.1002/sim.70181},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70181},
  shortjournal = {Stat. Med.},
  title        = {The design of large-scale plasma donation trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An IPCW adjusted win statistics approach in clinical trials incorporating equivalence margins to define ties. <em>SIM</em>, <em>44</em>(15-17), e70180. (<a href='https://doi.org/10.1002/sim.70180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, multiple outcomes of different priorities commonly occur as the patient's response may not be adequately characterized by a single outcome. Win statistics are appealing summary measures for between-group difference at more than one endpoint. When defining the result of pairwise comparisons of a time-to-event endpoint, it is desirable to allow ties to account for incomplete follow-up and not clinically meaningful difference in endpoints of interest. In this article, we propose a class of win statistics for time-to-event endpoints with a user-specified equivalence margin. These win statistics are identifiable in the presence of right censoring and do not depend on the censoring distribution. We then develop estimation and inference procedures for the proposed win statistics based on inverse-probability-of-censoring weighting adjustment to handle right censoring. We conduct extensive simulations to investigate the operational characteristics of the proposed procedure in the finite sample setting. A real oncology trial is used to illustrate the proposed approach.},
  archive      = {J_SIM},
  author       = {Ying Cui and Bo Huang and Gaohong Dong and Ryuji Uozumi and Lu Tian},
  doi          = {10.1002/sim.70180},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70180},
  shortjournal = {Stat. Med.},
  title        = {An IPCW adjusted win statistics approach in clinical trials incorporating equivalence margins to define ties},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subgroup testing in the change-plane cox model. <em>SIM</em>, <em>44</em>(15-17), e70179. (<a href='https://doi.org/10.1002/sim.70179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival outcomes are frequently observed in numerous biomedical and epidemiological studies. The impact of treatment on these outcomes may vary across subgroups characterized by other covariates, for example, immune checkpoint blockade therapy may have different effects on the survival of solid tumor patients based on their tumor mutational burden. In such scenarios, change-plane Cox models provide a suitable approach to identify subgroups that exhibit an improved treatment effect in the analysis of survival data. While some literature is available for testing the presence of a change plane in these models, the existing methods primarily rely on the score test, which has limited power in small sample situations. In this paper, we introduce a novel method based on the likelihood ratio test to enhance the power. The asymptotic distributions of the proposed test statistic under both the null and local alternative hypotheses are established. Furthermore, the finite sample performance of the proposed approach is comprehensively evaluated through extensive simulation studies. Finally, the proposed test is applied to analyze nonsmall cell lung cancer data, further demonstrating its practical utility.},
  archive      = {J_SIM},
  author       = {Xiao Zhang and Panpan Ren and Xingjie Shi and Shuangge Ma and Xu Liu},
  doi          = {10.1002/sim.70179},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70179},
  shortjournal = {Stat. Med.},
  title        = {Subgroup testing in the change-plane cox model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separable effects of semicompeting risks: The effects of hepatitis b on liver cancer via liver cirrhosis. <em>SIM</em>, <em>44</em>(15-17), e70178. (<a href='https://doi.org/10.1002/sim.70178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in how patients with hepatitis B progress to liver cirrhosis (an intermediate outcome) and liver cancer (a primary outcome). The separable effect has recently been proposed to study causal effects in the setting of competing risks. In this work, we extend the separable effect approach to semicompeting risks involving a primary and intermediate outcome. We decompose the exposure to hepatitis B virus into two disjoint components: the first component affects liver cancer directly, that is, direct effect, and the other affects liver cancer through liver cirrhosis, that is, indirect effect. Under such an effect separation, the identification formula of counterfactual risk for liver cancer that we derive for semicompeting risks is a function of cause-specific hazards and transition hazards of multistate models. It can be reduced to the formula for competing risks as a special case. We propose nonparametric and semiparametric methods to estimate the causal effects and study their asymptotic properties. The model-free nonparametric method is robust but less efficient for confounder adjustment; the model-based semiparametric method flexibly accommodates confounders by treating them as covariates. We conduct comprehensive simulations to study the performance of the proposed methods. Our data analyses of the hepatitis study show that there exist both direct and indirect effects of hepatitis B infection on the incidence of liver cancer through liver cirrhosis.},
  archive      = {J_SIM},
  author       = {Jih-Chang Yu and Yen-Tsung Huang},
  doi          = {10.1002/sim.70178},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70178},
  shortjournal = {Stat. Med.},
  title        = {Separable effects of semicompeting risks: The effects of hepatitis b on liver cancer via liver cirrhosis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and analysis of N-of-1 trials that incorporate sequential monitoring. <em>SIM</em>, <em>44</em>(15-17), e70177. (<a href='https://doi.org/10.1002/sim.70177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many diseases and disorders, such as Alzheimer's disease, patients demonstrate considerable heterogeneity in their responses to treatment interventions. One treatment may be most effective for some patients, while another may be most effective for others, and neither may be effective for another subset of patients. This potentially renders the conventional parallel group design highly inefficient. An attractive alternative is the N-of-1 design, also called the multi-crossover randomized controlled trial. In this design, each participant serves as their own control in a series of randomized blocks of treatment assignments. We propose novel designs for both the single-person and multi-person N-of-1 trials that employ sequential monitoring. In particular, we allow for early stopping for a single participant as soon as there is sufficient evidence of a preferred treatment for them, and early stopping for the group of participants as soon as there is sufficient evidence of a preferred treatment for the population of patients. We provide sample size calculations and decision rules for terminating the trial early and illustrate their properties in simulation studies. We apply our proposed methods to N-of-1 studies of brain tumor excisions and of methylphenidate in mild cognitive impairment.},
  archive      = {J_SIM},
  author       = {Shu Jiang and Steven E. Arnold and Rebecca A. Betensky},
  doi          = {10.1002/sim.70177},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70177},
  shortjournal = {Stat. Med.},
  title        = {Design and analysis of N-of-1 trials that incorporate sequential monitoring},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing mediation in cross-sectional stepped wedge cluster randomized trials. <em>SIM</em>, <em>44</em>(15-17), e70175. (<a href='https://doi.org/10.1002/sim.70175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis has been studied for independent data, but relatively little work has been done for correlated data observed under stepped wedge cluster randomized trials (SW-CRTs). Motivated by the need to understand the effect mechanisms in SW-CRTs, we develop a set of regression-based methods for conducting mediation analysis. Based on linear and generalized linear mixed models, we explain how to estimate the natural indirect effect and mediation proportion in typical SW-CRTs with four data type combinations, including both continuous and binary mediators and outcomes. Furthermore, to address potential complexities due to exposure-time treatment effect heterogeneity, we further derive the mediation expressions in SW-CRTs when the total effect varies as a function of the exposure time. Simulations show that the proposed mediation estimators perform well across all data type combinations and treatment effect structures. Finally, we illustrate the use of the proposed approach in a real SW-CRT example. An R package mediateSWCRT has been developed to facilitate the practical implementation of the estimators.},
  archive      = {J_SIM},
  author       = {Zhiqiang Cao and Fan Li},
  doi          = {10.1002/sim.70175},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70175},
  shortjournal = {Stat. Med.},
  title        = {Assessing mediation in cross-sectional stepped wedge cluster randomized trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust bayesian inference in the multilevel zero-inflated generalized poisson model. <em>SIM</em>, <em>44</em>(15-17), e70173. (<a href='https://doi.org/10.1002/sim.70173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers, over-dispersion, and zero inflation are issues with count data. Traditional models like Poisson and negative binomial often fail to account for these issues, leading to biased estimates and poor model fit. These frameworks are extended by the Zero-Inflated Generalized Poisson (ZIGP) model, which takes into consideration not only zero inflation but also over-dispersion or under-dispersion. However, in the presence of outliers and hierarchical data structures. This study develops a robust Bayesian inference framework for the multilevel ZIGP model. Standard Bayesian methods often lack robustness under model misspecification and in the presence of outlier data. The framework uses a Robust expectation solution (RES) algorithm and generalized Bayesian inference (GBI) for robust estimation against outliers. These approaches improve estimation accuracy using robust loss functions and scaling parameters to minimize the influence of outliers. Simulation studies confirm that the Robust Expectation Solution (RES) algorithm significantly outperformed the Expectation-Maximization (EM) algorithm in reducing bias and mean squared error (MSE), especially in the presence of outliers. Regular Bayesian and EM algorithms were more sensitive to outliers, leading to potential bias and instability in parameter estimates. Our robust Bayesian framework, specifically the Generalized Bayesian Inference (GBI), demonstrated improved robustness and stability under model misspecification and outlier contamination. The main results show that tuning quantiles and optimizing scaling parameters improved parameter calibration and reduced bias and mean square error (MSE). We applied the framework to neonatal mortality data, identifying key risk factors such as maternal education, wealth status, rural residence, and age at first birth.},
  archive      = {J_SIM},
  author       = {Mekuanint Simeneh Workie and Xu Yi},
  doi          = {10.1002/sim.70173},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70173},
  shortjournal = {Stat. Med.},
  title        = {Robust bayesian inference in the multilevel zero-inflated generalized poisson model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample size calculations for partially clustered trials. <em>SIM</em>, <em>44</em>(15-17), e70172. (<a href='https://doi.org/10.1002/sim.70172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially clustered trials are defined as trials where some observations belong to a cluster and others are independent. For example, neonatal trials may include infants from a single, twin, or triplet birth. The clustering of observations in partially clustered trials should be accounted for when determining the target sample size to avoid being over or underpowered. However, sample size methods have only been developed for limited partially clustered trial designs (e.g., designs with maximum cluster sizes of 2). In this article, we present new design effects that can be used to determine the sample size for two-arm, parallel, partially clustered trials where clusters exist pre-randomization. Design effects are derived algebraically for continuous and binary outcomes, assuming a generalized estimating equations-based approach to estimation with either an independence or exchangeable working correlation structure. Both cluster and individual randomization are considered for the clustered observations. The design effects are shown to depend on the intracluster correlation coefficient, proportion of observations that belong to clusters of each size, method of randomization, type of outcome, and working correlation structure. The design effects are validated through a simulation study. Example sample size calculations are presented to illustrate how the design effects can be used to determine the target sample size for different partially clustered trial designs. The design effects depend on parameters that can be feasibly estimated when planning a trial and can be used to ensure that partially clustered trials are appropriately powered in the future.},
  archive      = {J_SIM},
  author       = {Kylie M. Lange and Jessica Kasza and Thomas R. Sullivan and Lisa N. Yelland},
  doi          = {10.1002/sim.70172},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70172},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculations for partially clustered trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A maximum likelihood method for high-dimensional structural equation modeling. <em>SIM</em>, <em>44</em>(15-17), e70171. (<a href='https://doi.org/10.1002/sim.70171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor analysis provides an intuitive approach for dimension reduction when working with big data, allowing researchers to represent an extensive number of correlated variables via a subset of underlying latent factors. Traditional methods of factor analysis, such as Structural Equation Modeling (SEM) and factor regression, lack properties desirable for analyzing big data, such as the ability to handle high-dimensionality or the ability to enforce sparsity on the estimates of the factor loading matrices. These methods also assume that the number of latent constructs is known beforehand, a problem unique to factor analysis that often goes unaddressed or overlooked, with ad hoc methods being the most common ways to deal with such a fundamental question. Although recent developments in the literature have attempted to remedy these issues, particularly with regard to expanding SEM to high-dimensional and sparse applications, there is a noticeable lack of such methods that do so using likelihood theory. To rectify this shortcoming, we propose a new SEM-based method for estimation that utilizes maximum likelihood theory while simultaneously addressing some of the most common problems associated with big data. We substantiate our method through simulation studies, indicating that the proposed method can correctly identify the latent factors underlying the independent and dependent sets of variables, while also accurately estimating the entries of and enforcing sparsity upon the factor loading matrix estimates. We apply this method to the COVIDiSTRESS Global Survey dataset, a global survey collected to further our understanding of how the COVID-19 pandemic affected the human experience. Doing so demonstrates the performance of the model while simultaneously identifying the latent constructs intrinsic to the data.},
  archive      = {J_SIM},
  author       = {Alexander Quinter and Xianming Tan and Donglin Zeng and Joseph G. Ibrahim},
  doi          = {10.1002/sim.70171},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70171},
  shortjournal = {Stat. Med.},
  title        = {A maximum likelihood method for high-dimensional structural equation modeling},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust transfer learning for high-dimensional GLM using γ-divergence with applications to cancer genomics. <em>SIM</em>, <em>44</em>(15-17), e70170. (<a href='https://doi.org/10.1002/sim.70170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis of complex diseases, high-dimensional profiling data is important for assessing risks and detecting biomarkers. With the increasing accessibility of cancer genomic data, the sample sizes remain limited in most studies. Hence, borrowing information from additional data sources is thus desirable to improve estimation and prediction. Transfer learning has been demonstrated to be flexible and effective in boosting modeling performance with a record in biomedical applications. In practice, outliers and even data contamination often occur. However, existing transfer learning methods often lack robustness to outliers and data contamination, issues commonly observed in real-world biomedical data. In this study, we propose a robust transfer learning approach based on the minimum -divergence under a generalized linear model (GLM) framework for high-dimensional data. Our method incorporates a data-driven source detection scheme that automatically identifies informative sources while mitigating the risk of negative transfer. We establish rigorous theoretical results, including consistency and high-dimensional estimation error bounds, ensuring robustness and reliable performance. A computationally efficient algorithm is developed based on proximal gradient descent to facilitate both the transfer and debiasing steps. Simulation demonstrates the superior and competitive performance of the proposed approach in selection and prediction/classification. We further validate its practical utility by analyzing data on breast cancer and glioblastoma, showcasing the method's effectiveness in real-world high-dimensional settings.},
  archive      = {J_SIM},
  author       = {Fuzhi Xu and Shuangge Ma and Qingzhao Zhang and Yaqing Xu},
  doi          = {10.1002/sim.70170},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70170},
  shortjournal = {Stat. Med.},
  title        = {Robust transfer learning for high-dimensional GLM using γ-divergence with applications to cancer genomics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous feature selection for optimal dynamic treatment regimens. <em>SIM</em>, <em>44</em>(15-17), e70169. (<a href='https://doi.org/10.1002/sim.70169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimens (DTRs), where treatment decisions are tailored to individual patient's characteristics and evolving health status over multiple stages, have gained increasing interest in the modern era of precision medicine. Identifying important features that drive these decisions over stages not only leads to parsimonious DTRs for practical use but also enhances the reliability of learning optimal DTRs. Existing methods for learning optimal DTRs, such as Q-learning and O-learning, rely on a sequential procedure to estimate the optimal decision at each stage backward. Incorporating feature selection in these methods through regularization at each stage of estimation only identifies unimportant tailoring variables at each stage but is not necessary for those variables that are not important across all the stages. As a result, false discovery errors are likely to accumulate over stages in these sequential methods. To overcome this limitation, we propose a framework, namely L1 multistage ramp loss (L1-MRL) learning, to learn the optimal decision rules and, at the same time, perform variable selection across all the stages simultaneously. This framework uses a single multistage ramp loss to estimate optimal DTRs for all stages. Furthermore, a group Lasso-type penalty is imposed to penalize the coefficients in the decision rules across all stages, which enables the identification of features that are important for at least one stage decision. Theoretically, we show that the estimator is consistent and enjoys the oracle property toward the optimal. We demonstrate that the proposed method performs equally well as or better than many existing DTR methods with variable selection capability via extensive simulation studies and an application to electronic health record (EHR) data for type 2 diabetes (T2D) patients.},
  archive      = {J_SIM},
  author       = {Mochuan Liu and Yuanjia Wang and Donglin Zeng},
  doi          = {10.1002/sim.70169},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70169},
  shortjournal = {Stat. Med.},
  title        = {Simultaneous feature selection for optimal dynamic treatment regimens},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). α-KIDS: A novel feature evaluation in the ultrahigh-dimensional right-censored setting, with application to head and neck cancer. <em>SIM</em>, <em>44</em>(15-17), e70167. (<a href='https://doi.org/10.1002/sim.70167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in sequencing technologies have allowed the collection of massive genome-wide information that substantially enhances the diagnosis and prognosis of head and neck cancer. Identifying predictive markers for survival time is crucial for devising prognostic systems and learning the underlying molecular drivers of the cancer course. In this paper, we introduce α $$ \alpha $$ -KIDS, a model-free feature screening procedure with false discovery rate (FDR) control for ultrahigh-dimensional right-censored data, which is robust against unknown censoring mechanisms. Specifically, our two-stage procedure initially selects a set of important features with a dual screening mechanism using nonparametric reproducing-kernel-based ANOVA statistics, followed by identifying a refined set of features under FDR control through a unified knockoff procedure. The finite sample properties of our method and its novelty (in light of existing alternatives) are evaluated via simulation studies. Furthermore, we illustrate our methodology via application to a motivating right-censored head and neck (HN) cancer survival data derived from The Cancer Genome Atlas, with further validation on a similar HN cancer data from the Gene Expression Omnibus database. The methodology can be implemented using the R package aKIDS , which is available on GitHub .},
  archive      = {J_SIM},
  author       = {Atika Farzana Urmi and Chenlu Ke and Dipankar Bandyopadhyay},
  doi          = {10.1002/sim.70167},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70167},
  shortjournal = {Stat. Med.},
  title        = {α-KIDS: A novel feature evaluation in the ultrahigh-dimensional right-censored setting, with application to head and neck cancer},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple imputation of missing covariates when using the Fine–Gray model. <em>SIM</em>, <em>44</em>(15-17), e70166. (<a href='https://doi.org/10.1002/sim.70166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fine–Gray model for the subdistribution hazard is commonly used for estimating associations between covariates and competing risks outcomes. When there are missing values in the covariates included in a given model, researchers may wish to multiply impute them. Assuming interest lies in estimating the risk of only one of the competing events, this paper develops a substantive-model-compatible multiple imputation approach that exploits the parallels between the Fine–Gray model and the standard (single-event) Cox model. In the presence of right-censoring, this involves first imputing the potential censoring times for those failing from competing events, and thereafter imputing the missing covariates by leveraging methodology previously developed for the Cox model in the setting without competing risks. In a simulation study, we compared the proposed approach to alternative methods, such as imputing compatibly with cause-specific Cox models. The proposed method performed well (in terms of estimation of both subdistribution log hazard ratios and cumulative incidences) when data were generated assuming proportional subdistribution hazards, and performed satisfactorily when this assumption was not satisfied. The gain in efficiency compared to a complete-case analysis was demonstrated in both the simulation study and in an applied data example on competing outcomes following an allogeneic stem cell transplantation. For individual-specific cumulative incidence estimation, assuming proportionality on the correct scale at the analysis phase appears to be more important than correctly specifying the imputation procedure used to impute the missing covariates.},
  archive      = {J_SIM},
  author       = {Edouard F. Bonneville and Jan Beyersmann and Ruth H. Keogh and Jonathan W. Bartlett and Tim P. Morris and Nicola Polverelli and Liesbeth C. de Wreede and Hein Putter},
  doi          = {10.1002/sim.70166},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70166},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation of missing covariates when using the Fine–Gray model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A note on the sample size formula for a win ratio endpoint. <em>SIM</em>, <em>44</em>(15-17), e70165. (<a href='https://doi.org/10.1002/sim.70165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sample size formula for a win ratio endpoint proposed by Yu and Ganju is re-evaluated from an implementation perspective. This formula relies on an approximated null variance, rather than the exact permutation variance originally introduced by Finkelstein and Schoenfeld. As a result, the formula is analytically straightforward, does not require patient-level data to determine sample size, and is advantageous for practitioners, as it avoids the need for simulations in power analysis. In this paper, we empirically demonstrate that the power based on this sample size formula is generally lower than the power derived using the exact permutation variance across various scenarios. This discrepancy arises primarily due to the overestimation of the true variance when the approximated null variance is used in place of the exact permutation variance. We hope this will encourage further discussion regarding the appropriate application of this formula.},
  archive      = {J_SIM},
  author       = {Se Yoon Lee},
  doi          = {10.1002/sim.70165},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70165},
  shortjournal = {Stat. Med.},
  title        = {A note on the sample size formula for a win ratio endpoint},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Agreement between two quantitative measurement methods when the underlying latent trait is not constant. <em>SIM</em>, <em>44</em>(15-17), e70164. (<a href='https://doi.org/10.1002/sim.70164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most statistical methods that have been developed to assess the agreement between two quantitative measurement methods have (implicitly) relied on the assumption of a constant “individual” latent trait. This might be inappropriate when the “individual” is not an object but a person. Therefore, the goal of this study was to extend the standard measurement error model to cope with this limit. Four different settings were investigated: first, where the true individual latent trait was constant; second, where it was variable but without exhibiting a time trend; third, where it followed a linear time trend; and fourth, where it exhibited an approximate linear time trend. Two competing methods to estimate the parameters of the general measurement error model were assessed: the GLS estimator of Sprent and the two-stage method of Taffé. It was found that the latter generally performed better than the former to estimate the bias. In addition, it can be used when there is only a single measurement per individual by one of the two measurement methods, which is not the case with the former method.},
  archive      = {J_SIM},
  author       = {Patrick Taffé},
  doi          = {10.1002/sim.70164},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70164},
  shortjournal = {Stat. Med.},
  title        = {Agreement between two quantitative measurement methods when the underlying latent trait is not constant},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning for error-contaminated poisson regression models. <em>SIM</em>, <em>44</em>(15-17), e70163. (<a href='https://doi.org/10.1002/sim.70163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poisson regression model has been a popular approach to characterize the count response and the covariates. With the rapid development of data collections, the additional source information can be easily recorded. To efficiently use the source data to improve the estimation under the original data, the transfer learning method is considered a strategy. However, challenging issues from the given datasets include measurement error and high-dimensionality in variables, which are not well explored in the context of transfer learning. In this paper, we propose a novel strategy to handle error-prone count responses and estimate the parameters in measurement error models by using the source data, and then employ the transfer learning method to derive the corrected estimator. Moreover, to improve the prediction and avoid the model uncertainty, we further establish the model averaging strategy. Simulation and breast cancer data studies verify the satisfactory performance of the proposed method and the validity of handling measurement error.},
  archive      = {J_SIM},
  author       = {Jou-Chin Wu and Li-Pang Chen},
  doi          = {10.1002/sim.70163},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70163},
  shortjournal = {Stat. Med.},
  title        = {Transfer learning for error-contaminated poisson regression models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison of two methods for adaptive multi-arm two-stage design. <em>SIM</em>, <em>44</em>(15-17), e70162. (<a href='https://doi.org/10.1002/sim.70162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of comparing multiple treatment arms to a common control arm over the two stages of a group sequential randomized clinical trial. At the end of stage one, arms may be tested and dropped for overwhelming efficacy, futility, safety, or any other arbitrary reason, and the sample sizes of the arms going forward for stage two testing may be adaptively altered. At the end of stage two, a final analysis is performed, and efficacious treatment arms are identified by a testing procedure that offers strong control of the family-wise error rate (FWER). Two testing procedures, the -value combination method and the conditional error rate method, are discussed theoretically and then compared by simulation. While both procedures control the FWER, the conditional error rate procedure is seen to provide greater power than the -value combination procedure for a wide range of scenarios and alternative hypotheses. Plausible reasons for this power differential are given.},
  archive      = {J_SIM},
  author       = {Cyrus Mehta and Martin Kappler},
  doi          = {10.1002/sim.70162},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70162},
  shortjournal = {Stat. Med.},
  title        = {A comparison of two methods for adaptive multi-arm two-stage design},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-varying bayesian network meta-analysis. <em>SIM</em>, <em>44</em>(15-17), e70160. (<a href='https://doi.org/10.1002/sim.70160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of methicillin-resistant Staphylococcus Aureus (MRSA) in complicated skin and soft structure infections (cSSSI) is associated with greater health risks and economic costs to patients. There is concern that MRSA is becoming resistant to other “gold standard” treatments such as vancomycin, and there is disagreement about the relative efficacy of vancomycin compared to linezolid. There are several review papers employing Bayesian Network Meta-Analyses (BNMAs) to investigate which treatments are best for MRSA-related cSSSIs, but none address time-based design inconsistencies. This paper proposes a time-varying BNMA (tBNMA), which models time-varying treatment effects across studies using a Gaussian Process kernel. A dataset is compiled from nine existing MRSA cSSSI NMA review papers containing 58 studies comparing 19 treatments over 19 years. The tBNMA finds evidence of a non-linear trend in the treatment effect of vancomycin–it became less effective than linezolid between 2002 and 2007, but has since recovered statistical equivalence.},
  archive      = {J_SIM},
  author       = {Patrick M. LeBlanc and David Banks},
  doi          = {10.1002/sim.70160},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70160},
  shortjournal = {Stat. Med.},
  title        = {Time-varying bayesian network meta-analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporation of patient and public involvement in statistical methodology research: Summary of workshop proceedings. <em>SIM</em>, <em>44</em>(15-17), e70159. (<a href='https://doi.org/10.1002/sim.70159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient and Public Involvement (PPI) is well-established in applied health research but remains under utilised in statistical methodology research due to perceived irrelevance and communication challenges. This paper summarises a one-day workshop held in February 2024 in Leicester, organised by the University of Leicester and the NIHR Statistics Group, aimed at addressing barriers to meaningful PPI in statistical methodology. The workshop brought together statisticians and experienced public contributors to discuss strategies, share case studies, and offer practical guidance on conducting effective PPI. Key barriers identified included: (1) uncertainty about the relevance of PPI in methodology-focused research; (2) public contributors' anxiety over mathematical complexity; and (3) mismatched expectations due to different backgrounds in applied versus methodological research. Case studies showcased how PPI led to improved model structures, identification of data issues, and enhanced study materials. The importance of communication was a recurrent theme, with recommendations including use of plain English, regular updates, and visual storytelling tools. Feedback from attendees indicated increased confidence and motivation to engage in PPI. Public contributors emphasised the need for respectful, non-patronising interactions and flexible roles within projects. Recommendations include managing expectations, enhancing accessibility, co-developing materials, and fostering diversity among contributors. This paper highlights the need for tailored strategies to integrate PPI into statistical methodology, including the development of resources (e.g., glossaries, animations) and further case study collection. Future work will focus on expanding these resources, addressing challenges of equity and inclusion, and supporting PPI in complex methodological areas like simulation and model development.},
  archive      = {J_SIM},
  author       = {Aiden Smith and Hannah Worboys and Samina Begum and Derrick Bennett and Jonathan Broomfield and Suzie Cro and Laura Evans-Hill and Justin Greenwood and Ania Henley and Mary Mancini and Kara-Louise Royle and Helen Saul and Jamie Sergeant and Derek Stewart and Freya Tyrer and James Wason and Christopher Yau and Laura J. Gray},
  doi          = {10.1002/sim.70159},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70159},
  shortjournal = {Stat. Med.},
  title        = {Incorporation of patient and public involvement in statistical methodology research: Summary of workshop proceedings},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MARGO: Machine learning-assisted adaptive randomization for group sequential trials based on overlap weights. <em>SIM</em>, <em>44</em>(15-17), e70158. (<a href='https://doi.org/10.1002/sim.70158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive randomization is a promising approach in clinical trials that aims to optimize patient outcomes by adjusting treatment allocation probabilities based on accumulating data. However, implementing adaptive randomization in group sequential trials, which include interim analyses for early stopping decisions, poses significant challenges, such as managing type I error inflation and ensuring robust statistical validity. This paper proposes Machine learning-assisted Adaptive Randomization for Group sequential trials based on Overlap weights (MARGO) as an innovative solution to these challenges. MARGO integrates machine learning (ML) models into the adaptive randomization process, allowing dynamic updates to randomization probabilities based on real-time predictions of treatment success. To control the overall type I error rate due to the covariate imbalance in group sequential trials, MARGO utilizes overlap weights (OW), which are employed to balance covariates across treatment groups, minimizing confounding and ensuring that the comparison between treatments remains unbiased. In our implementation, various ML algorithms are evaluated for their effectiveness in predicting treatment outcomes. Through extensive simulation studies, we demonstrate that MARGO not only enhances the flexibility and efficiency of group sequential trials but also maintains statistical rigor by effectively controlling type I error rates. Our results show that MARGO provides a more ethical and data-driven approach to patient allocation, potentially improving treatment success rates while preserving the integrity of the trial.},
  archive      = {J_SIM},
  author       = {Yeonhee Park and Samuel Nycklemoe},
  doi          = {10.1002/sim.70158},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70158},
  shortjournal = {Stat. Med.},
  title        = {MARGO: Machine learning-assisted adaptive randomization for group sequential trials based on overlap weights},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized reduced rank regression for multi-outcome survival data supports a common metabolic risk score for age-related diseases. <em>SIM</em>, <em>44</em>(15-17), e70156. (<a href='https://doi.org/10.1002/sim.70156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing availability of multi-outcome data in health research presents new opportunities for understanding complex health processes, such as aging. Aging is a multifaceted process, encompassing both lifespan and health span, as well as the onset of age-related diseases. To model this complexity, we propose the penalized reduced rank regression model for multi-outcome survival data (penalized survRRR), which identifies shared latent factors driving multiple outcomes. The model imposes a rank constraint on the coefficient matrix to capture underlying mechanisms of aging while accommodating high-dimensional and correlated predictors and outcomes by introducing penalization. We discuss the statistical properties of this doubly regularized approach and show how the optimal number of ranks can be estimated from the data. We apply a lasso-penalized reduced rank regression model to 78,553 participants of the UK Biobank, using over 200 metabolic variables as predictors and the onset of seven age-related diseases and mortality as the outcomes of interest. Our results indicate that a rank 1 model provides the best fit to the data, resulting in a single metabolite-based score of age-related disease susceptibility. This highlights the potential of the penalized survRRR model to provide new insights into the nature of the relationship between metabolomics and age-related diseases.},
  archive      = {J_SIM},
  author       = {Marije H. Sluiskes and Hein Putter and Marian Beekman and Jelle J. Goeman and Mar Rodríguez-Girondo},
  doi          = {10.1002/sim.70156},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70156},
  shortjournal = {Stat. Med.},
  title        = {Penalized reduced rank regression for multi-outcome survival data supports a common metabolic risk score for age-related diseases},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjusting for selection bias due to missing data in electronic health records-based research by blending multiple imputation and inverse probability weighting. <em>SIM</em>, <em>44</em>(15-17), e70151. (<a href='https://doi.org/10.1002/sim.70151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complex process by which electronic health records (EHR) are generated and collected, missing data is a significant challenge when conducting large observational studies using such data. However, most standard methods that seek to adjust for the potential selection bias induced by restricting to individuals with complete data fail to address the heterogeneous structure of EHR. To address this, a framework was previously proposed that modularizes the data provenance that gives rise to the observed data as a sequence of decisions made by patients, healthcare providers, and the health system. In this work, we formalize analyses with this framework, specifically by proposing a pragmatic, flexible, and scalable framework for estimation and inference that blends inverse probability weighting and multiple imputation. The proposed framework allows better alignment between consideration of missingness assumptions to the complexity of EHR data. In addition to formal theoretical justification and simulation studies, we illustrate the proposed framework with a motivating data application in which EHR data are used to investigate weight loss outcomes following bariatric surgery, and whether differences between two surgery types exhibit effect modification by presence/absence of chronic kidney disease.},
  archive      = {J_SIM},
  author       = {Tanayott Thaweethai and Rajarshi Mukherjee and David Arterburn and Heidi Fischer and Catherine Lee and Susan M. Shortreed and Sebastien Haneuse},
  doi          = {10.1002/sim.70151},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70151},
  shortjournal = {Stat. Med.},
  title        = {Adjusting for selection bias due to missing data in electronic health records-based research by blending multiple imputation and inverse probability weighting},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outlier detection in mendelian randomization. <em>SIM</em>, <em>44</em>(15-17), e70143. (<a href='https://doi.org/10.1002/sim.70143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) uses genetic variants as instrumental variables to infer causal effects of exposures on an outcome. One key assumption of MR is that the genetic variants used as instrumental variables are independent of the outcome conditional on the risk factor and unobserved confounders. Violations of this assumption, that is, the effect of the instrumental variables on the outcome through a path other than the risk factor included in the model (which can be caused by pleiotropy), are common phenomena in human genetics. Genetic variants, which deviate from this assumption, appear as outliers to the MR model fit and can be detected by the general heterogeneity statistics proposed in the literature, which are known to suffer from overdispersion, that is, too many genetic variants are declared as false outliers. We propose a method that corrects for overdispersion of the heterogeneity statistics in uni- and multivariable MR analysis by making use of the estimated inflation factor to correctly remove outlying instruments and therefore account for pleiotropic effects. Our method is applicable to summary-level data.},
  archive      = {J_SIM},
  author       = {Maximilian M. Mandl and Anne-Laure Boulesteix and Stephen Burgess and Verena Zuber},
  doi          = {10.1002/sim.70143},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70143},
  shortjournal = {Stat. Med.},
  title        = {Outlier detection in mendelian randomization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accounting for patient characteristics in a model-based kappa of agreement between two experts' ordinal ratings. <em>SIM</em>, <em>44</em>(15-17), e70141. (<a href='https://doi.org/10.1002/sim.70141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cohen's kappa and other summary measures are often used in clinical studies to describe agreement and association between two experts' ordered categorical ratings. However, a key limitation of Cohen's kappa and similar measures is their inability to evaluate the impact of patient-related factors such as family history and age on the agreement and association between experts. Strong agreement between experts is an essential component of effective clinical procedures where subjective interpretation of patients' images or test results by an expert is required, for example, in the visual assessment of breast density from a mammogram. Not accounting for important patient-related factors can lead to inflated and biased assessments of agreement and association. In this article, our objective is to propose novel model-based measures that appropriately account for the impact of patient-related covariates on chance-corrected agreement and association between two experts' ordinal ratings that overcome limitations of existing measures. Our population-based approach is based on an ordinal generalized linear mixed model (GLMM). Rigorous simulation studies evaluating performance of the new model-based measures in a broad range of settings are reported. Existing and new measures are compared in two clinical applications assessing breast density and multiple sclerosis. Key advantages of the new kappa measures over existing measures such as Cohen's kappa include incorporating patient-related factors, robustness to underlying disease prevalence and marginal distributions of experts' ratings, and appropriately correcting for chance agreement. Sample R code is provided by the authors for application of proposed measures in other studies.},
  archive      = {J_SIM},
  author       = {Kerrie P. Nelson and Thomas J. Zhou},
  doi          = {10.1002/sim.70141},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70141},
  shortjournal = {Stat. Med.},
  title        = {Accounting for patient characteristics in a model-based kappa of agreement between two experts' ordinal ratings},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating misclassified EHR outcomes with validated outcomes from a non-probability sample. <em>SIM</em>, <em>44</em>(15-17), e70127. (<a href='https://doi.org/10.1002/sim.70127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although increasingly used for research, electronic health records (EHR) often lack a gold-standard assessment of key data elements. Linking EHRs to other data sources with higher-quality measurements can improve statistical inference, but such analyses must account for selection bias if the linked data source arises from a non-probability sample. We propose a set of novel estimators targeting the average treatment effect (ATE) that combine information from binary outcomes measured with error in a large, population-representative EHR database with gold-standard outcomes obtained from a smaller validation sample subject to selection bias. We evaluate our approach in extensive simulations and an analysis of data from the Adult Changes in Thought (ACT) study, a longitudinal study of incident dementia in a cohort of Kaiser Permanente Washington members with linked EHR data. For a subset of deceased ACT participants who consented to brain autopsy prior to death, gold-standard measures of Alzheimer's disease neuropathology are available. Our proposed estimators reduced bias and improved efficiency for the ATE, facilitating valid inference with EHR data when key data elements are ascertained with error.},
  archive      = {J_SIM},
  author       = {Jenny Shen and Dane Isenberg and Kristin A. Linn and Rebecca A. Hubbard},
  doi          = {10.1002/sim.70127},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70127},
  shortjournal = {Stat. Med.},
  title        = {Integrating misclassified EHR outcomes with validated outcomes from a non-probability sample},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ascertainment conditional maximum likelihood for continuous outcome under two-phase response-selective design. <em>SIM</em>, <em>44</em>(15-17), e70111. (<a href='https://doi.org/10.1002/sim.70111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collection procedures are often time-consuming and expensive. An alternative to collecting full information from all subjects enrolled in a study is a two-phase design: Variables that are inexpensive or easy to measure are obtained for the study population, and more specific, expensive, or hard-to-measure variables are collected only for a well-selected sample of individuals. Often, only these subjects that provided full information are used for inference, while those that were partially observed are discarded from the analysis. Recently, semiparametric approaches that use the entire dataset, resulting in fully efficient estimators, have been proposed. These estimators, however, have challenges incorporating multiple covariates, are computationally expensive, and depend on tuning parameters that affect their performance. In this paper, we propose an alternative semiparametric estimator that does not pose any distributional assumptions on the covariates or measurement error mechanism and can be applied to a wider range of settings. Although the proposed estimator is not semiparametric efficient, simulations show that the loss of efficiency to estimate the parameters associated with the partially observed covariates is minimal. We highlight the estimator's applicability to real-world problems, where data structures are complex and rich, and complicated regression models are often necessary.},
  archive      = {J_SIM},
  author       = {Gustavo Amorim and Ran Tao and Thomas Lumley and Pamela A. Shaw and Bryan E. Shepherd},
  doi          = {10.1002/sim.70111},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70111},
  shortjournal = {Stat. Med.},
  title        = {Ascertainment conditional maximum likelihood for continuous outcome under two-phase response-selective design},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural effects in the presence of an intermediate confounder: Evaluation of pragmatic estimation strategies with an emphasis on the relationship between natural and interventional effects. <em>SIM</em>, <em>44</em>(15-17), e70038. (<a href='https://doi.org/10.1002/sim.70038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis using the so-called natural effects is an essential tool to uncover causal pathways between an exposure and an outcome. However, natural effects are not generally identified in the presence of an intermediate confounder ( ), a situation that arguably arises frequently in practice. Three pragmatic approaches can be used to estimate natural effects when such a confounder is present: Natural effects estimators that omit , natural effects estimators that consider as a pre-exposure confounder, or interventional effects estimators. Interventional effects are analogous to natural effects, but remain identified when is present. The goal of this study was two-fold: (1) to assess the extent to which natural and interventional estimands differ under a variety of data-generating mechanisms with intermediate confounding and (2) using simulations, to investigate the corresponding performance of the three aforementioned strategies to estimate natural effects. In the continuous outcome case, using interventional effects estimators was found to be a better analytic strategy for estimating natural effects than using standard natural effects estimators when the interaction term between and in the outcome model was null or moderate in comparison to the other parameters. However, the performance of interventional effects declined as the - interaction was increased. In the binary outcome case, the three estimation strategies yielded more similar results than in the continuous outcome case. The difference between the three analytic strategies is illustrated using data from the World Value Survey.},
  archive      = {J_SIM},
  author       = {Jesse Gervais and Geneviève Lefebvre and Erica E. M. Moodie},
  doi          = {10.1002/sim.70038},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e70038},
  shortjournal = {Stat. Med.},
  title        = {Natural effects in the presence of an intermediate confounder: Evaluation of pragmatic estimation strategies with an emphasis on the relationship between natural and interventional effects},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate adjustments for average equivalence testing. <em>SIM</em>, <em>44</em>(15-17), e10258. (<a href='https://doi.org/10.1002/sim.10258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate (average) equivalence testing is widely used to assess whether the means of two conditions of interest are “equivalent” for different outcomes simultaneously. In pharmacological research for example, many regulatory agencies require the generic product and its brand-name counterpart to have equivalent means both for the AUC and C max pharmacokinetics parameters. The multivariate Two One-Sided Tests (TOST) procedure is typically used in this context by checking if, outcome by outcome, the marginal confidence intervals for the difference in means between the two conditions of interest lie within predefined lower and upper equivalence limits. This procedure, already known to be conservative in the univariate case, leads to a rapid power loss when the number of outcomes increases, especially when one or more outcome variances are relatively large. In this work, we propose a finite-sample adjustment for this procedure, the multivariate -TOST, that consists in a correction of , the significance level, taking the (arbitrary) dependence between the outcomes of interest into account and making it uniformly more powerful than the conventional multivariate TOST. We present an iterative algorithm allowing to efficiently define , the corrected significance level, a task that proves challenging in the multivariate setting due to the inter-relationship between and the sets of values belonging to the null hypothesis space and defining the test size. We study the operating characteristics of the multivariate -TOST both theoretically and via an extensive simulation study considering cases relevant for real-world analyses—that is, relatively small sample sizes, unknown and possibly heterogeneous variances as well as different correlation structures—and show the superior finite-sample properties of the multivariate -TOST compared to its conventional counterpart. We finally re-visit a case study on ticlopidine hydrochloride and compare both methods when simultaneously assessing bioequivalence for multiple pharmacokinetic parameters.},
  archive      = {J_SIM},
  author       = {Younes Boulaguiem and Luca Insolia and Maria-Pia Victoria-Feser and Dominique-Laurent Couturier and Stéphane Guerrier},
  doi          = {10.1002/sim.10258},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15-17},
  pages        = {e10258},
  shortjournal = {Stat. Med.},
  title        = {Multivariate adjustments for average equivalence testing},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precision of treatment hierarchy: A metric for quantifying certainty in treatment hierarchies from network meta-analysis. <em>SIM</em>, <em>44</em>(13-14), e70176. (<a href='https://doi.org/10.1002/sim.70176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network meta-analysis (NMA) is an extension of pairwise meta-analysis that facilitates the estimation of relative effects for multiple competing treatments. A hierarchy of treatments is a useful output of an NMA. Treatment hierarchies are produced using ranking metrics. Common ranking metrics include the Surface Under the Cumulative RAnking curve (SUCRA) and P-scores, which are the frequentist analogue to SUCRAs. Both metrics consider the size and uncertainty of the estimated treatment effects, with larger values indicating a more preferred treatment. Although SUCRAs and P-scores themselves consider uncertainty, treatment hierarchies produced by these ranking metrics are typically reported without a measure of certainty, which might be misleading to practitioners. We propose a new metric, Precision of Treatment Hierarchy (POTH), which quantifies the certainty in producing a treatment hierarchy from SUCRAs or P-scores. The metric connects three statistical quantities: The variance of the SUCRA values, the variance of the mean rank of each treatment, and the average variance of the distribution of individual ranks for each treatment. POTH provides a single, interpretable value that quantifies the extent of certainty in producing a treatment hierarchy. We show how the metric can be adapted to apply to subsets of treatments in a network, for example, to quantify the certainty in the hierarchy of the top three treatments. We calculate POTH for a database of NMAs to investigate its empirical properties, and we demonstrate its use on two published networks.},
  archive      = {J_SIM},
  author       = {Augustine Wigle and Audrey Béliveau and Georgia Salanti and Gerta Rücker and Guido Schwarzer and Dimitris Mavridis and Adriani Nikolakopoulou},
  doi          = {10.1002/sim.70176},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70176},
  shortjournal = {Stat. Med.},
  title        = {Precision of treatment hierarchy: A metric for quantifying certainty in treatment hierarchies from network meta-analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of violation of the proportional hazards assumption on the calibration of the cox proportional hazards model. <em>SIM</em>, <em>44</em>(13-14), e70161. (<a href='https://doi.org/10.1002/sim.70161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox proportional hazards regression model is frequently used to develop clinical prediction models for time-to-event outcomes, allowing clinicians to estimate an individual's risk of experiencing the outcome within specified time horizons (e.g., estimate an individual's 10-year risk of death). The Cox regression model models the association between covariates and the hazard of the outcome. A key assumption of the Cox model is the proportional hazards assumption: the ratio of the hazard function for any two individuals is constant over time, and the ratio is a function of only their covariates and the regression coefficients. Calibration is an important aspect of the validation of clinical prediction models. Calibration refers to the concordance between predicted and observed risk. The impact of the violation of the proportional hazards assumption on the calibration of clinical prediction models developed using the Cox model has not been examined. We conducted a set of Monte Carlo simulations to assess the impact of the magnitude of the violation of the proportional hazards assumption on the calibration of the Cox model. We compared the calibration of predictions obtained using a Cox regression model that ignored the violation of the proportional hazards assumption with those obtained using accelerated failure time (AFT) models, Royston and Parmar's spline-based parametric survival models, and generalized linear models using pseudo-observations. We found that violation of the proportional hazards assumption had negligible impact on the calibration of predictions obtained using a Cox model.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Daniele Giardiello},
  doi          = {10.1002/sim.70161},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70161},
  shortjournal = {Stat. Med.},
  title        = {The impact of violation of the proportional hazards assumption on the calibration of the cox proportional hazards model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Futility monitoring in clinical trials. <em>SIM</em>, <em>44</em>(13-14), e70157. (<a href='https://doi.org/10.1002/sim.70157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the beginning of a phase III clinical trial, there is great optimism. After all, the phase II trial results were encouraging. Then, early data from the phase III trial trend in the wrong way, but there is still an opportunity for the trend to reverse and become statistically significant by the end. At what point does optimism become denial of reality? How do we decide when a clinical trial is futile? What does futility even mean? This tutorial reviews different concepts and tools for evaluating futility, including conditional and predictive power, reverse conditional power, predicted interval plots, revised unconditional power, and beta spending functions.},
  archive      = {J_SIM},
  author       = {Ana M. Ortega-Villa and Megan C. Grieco and Kevin Rubenstein and Jing Wang and Michael A. Proschan},
  doi          = {10.1002/sim.70157},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70157},
  shortjournal = {Stat. Med.},
  title        = {Futility monitoring in clinical trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistency-based approach to adjust for multiplicity in confirmatory subgroup analysis. <em>SIM</em>, <em>44</em>(13-14), e70154. (<a href='https://doi.org/10.1002/sim.70154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advance of medical sciences and better understanding of human biological systems, the next generation of treatment has shifted toward personalized medicine. It is expected that personalized medicine, such as molecularly targeted anti-cancer agents, is more efficacious in marker-positive patients, while marker-negative patients may or may not benefit from the treatment. Due to technology limitations in marker identification and incomplete understanding of the role of biomarkers in treatment effect, it is possible that the marker is not predictive. Therefore, it is often of interest to test the treatment on the overall population as well as the biomarker-positive subgroup. Testing both the overall population and the biomarker-positive subgroup introduces a multiplicity issue and leads to type I error inflation if not adjusted appropriately. The available multiplicity adjustment procedures may not consider the logic needed in the two tests. A new method is proposed by applying the logical connections between the two hypothesis tests, and arbitrages between different rejection regions to make the testing strategy not only powerful but also sensible.},
  archive      = {J_SIM},
  author       = {Qiqi Deng and Qian Li and Naitee Ting and Feng Yu},
  doi          = {10.1002/sim.70154},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70154},
  shortjournal = {Stat. Med.},
  title        = {Consistency-based approach to adjust for multiplicity in confirmatory subgroup analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric estimation of relative causal effects in randomized controlled trials with noncompliance. <em>SIM</em>, <em>44</em>(13-14), e70153. (<a href='https://doi.org/10.1002/sim.70153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials (RCTs) are the gold standard for causal inference and are widely used. However, valid analyses of RCTs are often complicated by non-compliance, which can lead to confounding bias and biased causal effect estimation. The main challenge comes from compliance datasets in both the treatment and control groups both following two-component mixture models. The maximum nonparametric likelihood estimator is inconsistent in a two-component mixture model even if the mixture proportion and one of the components are completely known, but the other component is unknown. In this paper, we instead assume parametric models for the ratios of risks among compliers assigned treatment, never-takers and always-takers, and leave the baseline compliers not assigned treatment unspecified. We develop a novel two-step maximum likelihood estimation procedure by making full use of the observed covariates and latent compliance classes, which theoretically can produce asymptotic root consistent estimators. In particular, our proposed estimator for the conditional local risk ratio always lies within the range of the parameter. Our numerical results show that the proposed method is generally more reliable than existing alternatives.},
  archive      = {J_SIM},
  author       = {Wenli Liu and Jing Qin and Yukun Liu},
  doi          = {10.1002/sim.70153},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70153},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric estimation of relative causal effects in randomized controlled trials with noncompliance},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian estimation of partial functional tobit censored quantile regression model. <em>SIM</em>, <em>44</em>(13-14), e70152. (<a href='https://doi.org/10.1002/sim.70152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information extracted from imaging data has become increasingly important in disease diagnosis as it uncovers associations between imaging features and diseases of interest. This study proposes a partial functional Tobit censored quantile regression (PFTCQR) model to investigate the quantile-specific relationships between the time of incidence of laryngeal cancer and a set of imaging and clinical predictors based on the data collected from a laryngeal cancer study in the Otolaryngology Department of a tertiary hospital in Jilin Province, China. The functional principal component analysis and moment method are employed to estimate the slope and covariance functions of the functional predictors. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed, leveraging the location-scale mixture representation of the asymmetric Laplace distribution (ALD) to perform the estimation. Furthermore, we extend the PFTCQR model to the composite quantile regression framework and incorporate variable selection for scalar covariates, further enhancing the robustness and efficiency of parameter estimation and improving model fitting. The proposed method is demonstrated through simulation studies and applied to the laryngeal carcinoma data. Results provide new insights into potential risk factors for laryngeal carcinoma and their effects varying across quantiles. Specific laryngeal regions are identified as significantly associated with the progression of the disease.},
  archive      = {J_SIM},
  author       = {Chunjie Wang and Zhexin Lu and Chuchu Wang and Xinyuan Song},
  doi          = {10.1002/sim.70152},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70152},
  shortjournal = {Stat. Med.},
  title        = {Bayesian estimation of partial functional tobit censored quantile regression model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for a general changepoint in medical and psychometric studies: Changes detection and sample size planning. <em>SIM</em>, <em>44</em>(13-14), e70150. (<a href='https://doi.org/10.1002/sim.70150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new method for change detection in medical and psychometric studies based on the recently introduced pseudo Score statistic, for which the sampling distribution under the alternative hypothesis has been determined. Our approach has the advantage of simplicity in its computation, eliminating the need for resampling or simulations to obtain critical values. Additionally, it comes with known null and alternative distributions, facilitating easy calculations for power levels and sample size planning. The paper indeed also discusses the topic of power analysis in segmented regression, namely the estimation of sample size or power level when the study data being collected focuses on a covariate expected to affect the mean response via a piecewise relationship with an unknown breakpoint. We run simulation studies showing that our method outperforms other Tests for a Change Point (TFCP) with both normally distributed and binary data and carry out two real data analyses on genomic data and SAT critical reading data. The proposed test contributes to the framework of medical and psychometric research, and it is available on the Comprehensive R Archive Network (CRAN) and in a more user-friendly Shiny App, both illustrated at the end of the paper.},
  archive      = {J_SIM},
  author       = {Nicoletta D'Angelo},
  doi          = {10.1002/sim.70150},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70150},
  shortjournal = {Stat. Med.},
  title        = {Testing for a general changepoint in medical and psychometric studies: Changes detection and sample size planning},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample size assessment and interim analysis strategies for survival trial designs with covariate-adaptive randomization. <em>SIM</em>, <em>44</em>(13-14), e70149. (<a href='https://doi.org/10.1002/sim.70149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the required sample size is an essential task for a clinical trial. It ensures statistical power and makes it possible to draw meaningful conclusions from the study results. Moreover, to minimize treatment imbalances within each covariate subgroup, covariate-adaptive randomization is a popular method. The first aim of this study is to investigate the required sample size for covariate-adaptive randomization based on survival outcomes. We evaluate the testing performance using the calculated sample size under simple randomization, stratified permuted block randomization, and covariate-adaptive biased coin randomization. To provide preliminary insights into the trial's progress and potential efficacy, an interim analysis is commonly conducted. The second aim of the study is to provide a strategy for interim analysis in covariate-adaptive randomization trials, which involves stopping the trial early based on accumulating data if one treatment arm proves to be significantly more effective or harmful than the others. Because the data collected before and after the interim monitoring time are dependent, performing multiple tests may inflate the overall Type I error rate. We thus re-estimate the required sample size, propose a valid hypothesis testing method for interim analysis, and study the underlying theoretical properties of the testing statistics, incorporating covariate-adaptive randomization. The performance of the proposed formula and interim strategy is evaluated through comprehensive simulations, including a sensitivity analysis. We also provide the R code to benefit the readers. Finally, two survival trials, the RE01 trial and a lung cancer study, are treated as pilot studies, and we show that the proposed strategies are valid under covariate-adaptive randomization.},
  archive      = {J_SIM},
  author       = {Pei-Fang Su and Chieh-Chi Wu},
  doi          = {10.1002/sim.70149},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70149},
  shortjournal = {Stat. Med.},
  title        = {Sample size assessment and interim analysis strategies for survival trial designs with covariate-adaptive randomization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating multiple data sources with interactions in multi-omics using cooperative learning. <em>SIM</em>, <em>44</em>(13-14), e70148. (<a href='https://doi.org/10.1002/sim.70148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling with multiomics data presents multiple challenges, such as the high dimensionality of the problem ( p ≫ n $$ p\gg n $$ ), the presence of interactions between features, and the need for integration between multiple data sources. We establish an interaction model that allows for the inclusion of multiple sources of data from the integration of two existing methods, pliable lasso and cooperative learning. The integrated model is tested both on simulation studies and on real multiomics datasets for predicting labor onset and cancer treatment response. The results show that the model is effective in modeling multisource data in various scenarios where interactions are present, both in terms of prediction performance and selection of relevant variables.},
  archive      = {J_SIM},
  author       = {Matteo D'Alessandro and Theophilus Quachie Asenso and Manuela Zucknick},
  doi          = {10.1002/sim.70148},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70148},
  shortjournal = {Stat. Med.},
  title        = {Integrating multiple data sources with interactions in multi-omics using cooperative learning},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bland–Altman plot for censored variables. <em>SIM</em>, <em>44</em>(13-14), e70147. (<a href='https://doi.org/10.1002/sim.70147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The comparison of two measurement methods turns out to be a statistical challenge if some of the observations are below the limit of quantification or detection. Here we show how the Bland–Altman plot can be modified for censored variables. The reference lines (bias and limits of agreement) in the Bland–Altman plot have to be estimated for censored variables. In a simulation study, we compared three different estimation methods: Restricting the data set to fully quantifiable pairs of observations (complete case analysis), naïvely substituting missing values with half of the limit of quantification, and a multiple imputation procedure based on a maximum likelihood approach for bivariate lognormally distributed variables with censoring. The results show that simple ad-hoc solutions may lead to bias in the results when comparing two measurement methods with censored observations, whereas the presented multiple imputation approach of the Bland–Altman method allows adequate consideration of censored variables. The method works similarly for other distribution assumptions.},
  archive      = {J_SIM},
  author       = {Anne Lotz and Thomas Behrens and Karl-Heinz Jöckel and Dirk Taeger},
  doi          = {10.1002/sim.70147},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70147},
  shortjournal = {Stat. Med.},
  title        = {Bland–Altman plot for censored variables},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage method for extending inferences from a collection of trials. <em>SIM</em>, <em>44</em>(13-14), e70146. (<a href='https://doi.org/10.1002/sim.70146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When considering the effect a treatment will cause in a population of interest, we often look to evidence from randomized controlled trials. In settings where multiple trials on a treatment are available, we may wish to synthesize the trials' participant data to obtain causally interpretable estimates of the average treatment effect in a specific target population. Traditional meta-analytic approaches to synthesizing data from multiple studies estimate the average effect among the studies. The resulting estimate is often not causally interpretable in any population, much less a particular target population, due to heterogeneity in the effect of treatment across studies. Inspired by traditional two-stage meta-analytic methods and methods for extending inferences from a single study, we propose a two-stage approach to extending inferences from a collection of randomized controlled trials that can be used to obtain causally interpretable estimates of treatment effects in a target population when there is between-study heterogeneity in conditional average treatment effects. We first introduce a collection of assumptions under which the target population's average treatment effect is identifiable when conditional average treatment effects are heterogeneous across studies. We then introduce an estimator that utilizes weighting in two stages, taking a weighted average of study-specific estimates of the treatment effect in the target population. We assess the performance of our proposed approach through simulation studies and two applications: A multi-center randomized clinical trial studying a Hepatitis-C treatment and a collection of studies on a therapy treatment for symptoms of pediatric traumatic brain injury.},
  archive      = {J_SIM},
  author       = {Nicole Schnitzler and Eloise Kaizar},
  doi          = {10.1002/sim.70146},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70146},
  shortjournal = {Stat. Med.},
  title        = {A two-stage method for extending inferences from a collection of trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for tensor-based covariates in cox regression modeling in integrated genome studies. <em>SIM</em>, <em>44</em>(13-14), e70145. (<a href='https://doi.org/10.1002/sim.70145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the form of multidimensional arrays, modeling considering tensor covariates is capable of capturing important feature interactions or encompassing a broader range of data structures. In the study, we proposed a statistical inference based on tensor-based covariates for the Cox regression model in the integrated genome study. The proposed method stands out for reducing the parameter dimension within the partial likelihood function through the application of rank-R decomposition. We introduced the method of estimation and rank selection. Subsequently, variable-specific tensor tests are proposed to evaluate the significance of the covariance effect. Asymptotic normality is obtained for all the regression estimators. Simulation studies, including rank selection, estimation performance, and testing performance, demonstrate the effectiveness of the proposed algorithm. Overall, the proposed method provides readers with the necessary information to understand the genome effect of tensor-based covariates. To benefit readers, the R code for implementing the method is provided. Lastly, we introduce the proposed method using a colorectal cancer dataset, addressing signal detection by simultaneously gathering information across different kinds of omics platforms. This dataset integrates clinical information with multi-omics data, including copy number variation, methylation, and mRNA sequencing data, as tensor-based covariates, and evaluates relationships involving right-censored data.},
  archive      = {J_SIM},
  author       = {Chin-Chun Chen and Sheng-Mao Chang and Peng-Chan Lin and Pei-Fang Su},
  doi          = {10.1002/sim.70145},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70145},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for tensor-based covariates in cox regression modeling in integrated genome studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A seamless hybrid phase II/III design with bayesian interim subgroup selection. <em>SIM</em>, <em>44</em>(13-14), e70144. (<a href='https://doi.org/10.1002/sim.70144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population selection is a crucial subject in clinical development nowadays as personalized medicine is growing in interest. Evolution in biomarker scanning techniques allows for the composition and detection of sub-populations of interest when analyzing new drug responses in a disease. Seamless adaptive trials could allow for subgroup analysis with the selection of the most promising population at interim analysis. We propose a hybrid Bayesian design for seamless Phase II/III trials with binary and time-to-event outcomes for the first and second phases, respectively. In this work, at interim analysis, several prior distributions, including shrinkage prior, are compared to possibly select/discard a population, and a final test using a conditional error function as a combination method testing procedure to control the frequentist type I error is used. Simulation studies showed that the logistic regression model performs better than frequentist testing for the population selection problem when the subgroup should be selected. Shrinkage prior distributions tend to be more conservative than simpler normal distributions as studies that would have ended positively are stopped at interim analysis.},
  archive      = {J_SIM},
  author       = {Benjamin Duputel and Nigel Stallard and François Montestruc and Sarah Zohar and Moreno Ursino},
  doi          = {10.1002/sim.70144},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70144},
  shortjournal = {Stat. Med.},
  title        = {A seamless hybrid phase II/III design with bayesian interim subgroup selection},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Negative control outcome adjustment in early-phase randomized trials: Estimating vaccine effects on immune responses in HIV exposed uninfected infants. <em>SIM</em>, <em>44</em>(13-14), e70142. (<a href='https://doi.org/10.1002/sim.70142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adjustment for prognostic baseline variables can reduce bias due to covariate imbalance and increase efficiency in randomized trials. While the use of covariate adjustment in late-phase trials is justified by favorable large-sample properties, it is seldom used in small, early-phase studies, due to uncertainty in which variables are prognostic and the potential for precision loss, type I error rate inflation, and undercoverage of confidence intervals. To address this problem, we consider adjustment for a valid negative control outcome (NCO), or an auxiliary post-randomization outcome believed to be completely unaffected by treatment but more highly correlated with the primary outcome than baseline covariates. We articulate the assumptions that permit adjustment for NCOs without producing post-randomization selection bias, and describe plausible data-generating models where NCO adjustment can improve upon adjustment for baseline covariates alone. In numerical experiments, we illustrate performance and provide practical recommendations regarding model selection and finite-sample variance corrections. We apply our methods to the reanalysis of two early-phase vaccine trials in HIV exposed uninfected (HEU) infants, where we demonstrate that adjustment for auxiliary post-baseline immunological parameters can enhance the precision of vaccine effect estimates relative to standard approaches that avoid adjustment or adjust for baseline covariates alone.},
  archive      = {J_SIM},
  author       = {Ethan Ashby and Bo Zhang and Genevieve G. Fouda and Youyi Fong and Holly Janes},
  doi          = {10.1002/sim.70142},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70142},
  shortjournal = {Stat. Med.},
  title        = {Negative control outcome adjustment in early-phase randomized trials: Estimating vaccine effects on immune responses in HIV exposed uninfected infants},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional multiresponse partially functional linear regression. <em>SIM</em>, <em>44</em>(13-14), e70140. (<a href='https://doi.org/10.1002/sim.70140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new class of high-dimensional multiresponse partially functional linear regressions (MR-PFLRs) to investigate the relationship between scalar responses and a set of explanatory variables, which include both functional and scalar types. In this framework, both the dimensionality of the responses and the number of scalar covariates can diverge to infinity. To account for within-subject correlation, we develop a functional principal component analysis (FPCA)-based penalized weighted least squares estimation procedure. In this approach, the precision matrix is estimated using penalized likelihoods, and the regression coefficients are then estimated through the penalized weighted least squares method, with the precision matrix serving as the weight. This method allows for the simultaneous estimation of both functional and scalar regression coefficients, as well as the precision matrix, while identifying significant features. Under mild conditions, we establish the consistency, rates of convergence, and oracle properties of the proposed estimators. Simulation studies demonstrate the finite-sample performance of our estimation method. Additionally, the practical utility of the MR-PFLR model is showcased through an application to Alzheimer's disease neuroimaging initiative (ADNI) data.},
  archive      = {J_SIM},
  author       = {Xiong Cai and Jiguo Cao and Xingyu Yan and Peng Zhao},
  doi          = {10.1002/sim.70140},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70140},
  shortjournal = {Stat. Med.},
  title        = {High-dimensional multiresponse partially functional linear regression},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sharp variance estimator and causal bootstrap in stratified randomized experiments. <em>SIM</em>, <em>44</em>(13-14), e70139. (<a href='https://doi.org/10.1002/sim.70139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized experiments are the gold standard for estimating treatment effects, and randomization serves as a reasoned basis for inference. In widely used stratified randomized experiments, randomization-based finite-population asymptotic theory enables valid inference for the average treatment effect, relying on normal approximation and a Neyman-type conservative variance estimator. However, when the sample size is small or the outcomes are skewed, the Neyman-type variance estimator may become overly conservative, and the normal approximation can fail. To address these issues, we propose a sharp variance estimator and two causal bootstrap methods to more accurately approximate the sampling distribution of the weighted difference-in-means estimator in stratified randomized experiments. The first causal bootstrap procedure is based on rank-preserving imputation, and we prove its second-order refinement over normal approximation. The second causal bootstrap procedure is based on constant-treatment-effect imputation and is further applicable in paired experiments. In contrast to traditional bootstrap methods, where randomness originates from hypothetical super-population sampling, our analysis for the proposed causal bootstrap is randomization-based, relying solely on the randomness of treatment assignment in randomized experiments. Numerical studies and two real data applications demonstrate the advantages of our proposed methods in finite samples. The R package CausalBootstrap implementing our method is publicly available.},
  archive      = {J_SIM},
  author       = {Haoyang Yu and Ke Zhu and Hanzhong Liu},
  doi          = {10.1002/sim.70139},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70139},
  shortjournal = {Stat. Med.},
  title        = {Sharp variance estimator and causal bootstrap in stratified randomized experiments},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BEAM: Bayesian hybrid design with adaptive sample size through multisource exchangeability modeling. <em>SIM</em>, <em>44</em>(13-14), e70137. (<a href='https://doi.org/10.1002/sim.70137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials (RCTs) are considered the gold standard for evaluating treatment efficacy, but they come with several practical challenges. These include high costs, lengthy timelines, ethical concerns for participants in placebo or control arms, and issues such as patient attrition and non-compliance. Recruiting patients for the control arm can be particularly challenging, especially in therapeutic areas with high unmet medical needs. To address these issues, hybrid trial designs that integrate external data sources, such as historical controls and real-world data, have emerged as a promising alternative. This paper introduces the Bayesian hybrid design with adaptive sample size through multisource exchangeability modeling (BEAM). The BEAM design leverages a modified multisource exchangeability model to dynamically borrow relevant information from multiple historical data sources, while adaptively adjusting the sample size throughout the trial. This approach ensures that the trial maintains statistical rigor and efficiency, even when heterogeneity exists between current and historical data, and mitigates the challenges associated with control arm accrual and compliance. Through extensive simulations, BEAM demonstrated robust performance in controlling type I error rate, reducing bias, and maintaining power compared to traditional methods and other adaptive designs. Additionally, the BEAM design offers a versatile and efficient computational framework for optimizing clinical trials, helping to reduce both the cost and time involved in drug development. We also illustrate the application of the proposed BEAM design in a case study on ankylosing spondylitis.},
  archive      = {J_SIM},
  author       = {Feng Tian and Meizi Liu and Yunqi Zhao and Jianchang Lin and Rachael Liu},
  doi          = {10.1002/sim.70137},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70137},
  shortjournal = {Stat. Med.},
  title        = {BEAM: Bayesian hybrid design with adaptive sample size through multisource exchangeability modeling},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple imputation confidence intervals for a risk difference with missing observations. <em>SIM</em>, <em>44</em>(13-14), e70136. (<a href='https://doi.org/10.1002/sim.70136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidence interval estimation for a risk difference is commonly used in various applications. The method of variance estimates recovery (MOVER) is a useful method for constructing the confidence interval of the risk difference. The confidence interval estimation with incomplete data has been widely studied in recent years, as missing values can occur during data collection. In this study, for the Poisson and binomial distributions, we propose proper multiple imputation procedures for the MOVER to estimate the confidence intervals for the risk difference, not only for missing at random but also for missing not at random. A simulation study shows that the coverage probabilities of the proposed intervals are closer to the nominal level than those of existing intervals, particularly when the true parameters are near the boundaries. These multiple imputation confidence intervals are illustrated with real data examples.},
  archive      = {J_SIM},
  author       = {Chung-Han Lee},
  doi          = {10.1002/sim.70136},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70136},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation confidence intervals for a risk difference with missing observations},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison between markov switching zero-inflated and hurdle models for spatio-temporal infectious disease counts. <em>SIM</em>, <em>44</em>(13-14), e70135. (<a href='https://doi.org/10.1002/sim.70135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In epidemiological studies, zero-inflated and hurdle models are commonly used to handle excess zeros in reported infectious disease cases. However, they cannot model the persistence (transition from presence to presence) and reemergence (transition from absence to presence) of a disease separately. Covariates can sometimes have different effects on the reemergence and persistence of a disease. Recently, a zero-inflated Markov switching negative binomial model was proposed to accommodate this issue. We introduce a Markov switching negative binomial hurdle model as a competitor of that approach, as hurdle models are often also used as alternatives to zero-inflated models for accommodating excess zeroes. We begin the comparison by inspecting the underlying assumptions made by both models. Hurdle models assume perfect detection of the disease cases while zero-inflated models implicitly assume the case counts can be under-reported, thus, we investigate when a negative binomial distribution can approximate the true distribution of reported counts. A comparison of the fit of the two types of Markov switching models is undertaken on chikungunya cases across the neighborhoods of Rio de Janeiro. We find that, among the fitted models, the Markov switching negative binomial zero-inflated model produces the best predictions, and both Markov switching models produce remarkably better predictions than more traditional negative binomial hurdle and zero-inflated models.},
  archive      = {J_SIM},
  author       = {Mingchi Xu and Dirk Douwes-Schultz and Alexandra M. Schmidt},
  doi          = {10.1002/sim.70135},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70135},
  shortjournal = {Stat. Med.},
  title        = {A comparison between markov switching zero-inflated and hurdle models for spatio-temporal infectious disease counts},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian hierarchical mixture cure modelling framework to utilize multiple survival datasets for long-term survivorship estimates: A case study from previously untreated metastatic melanoma. <em>SIM</em>, <em>44</em>(13-14), e70132. (<a href='https://doi.org/10.1002/sim.70132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time to an event of interest over a lifetime is a central measure of the clinical benefit of an intervention used in a health technology assessment (HTA). Within the same trial, multiple end-points may also be considered. For example, overall and progression-free survival time for different drugs in oncology studies. A common challenge is when an intervention is only effective for some proportion of the population who are not clinically identifiable. Therefore, latent group membership as well as separate survival models for identified groups need to be estimated. However, follow-up in trials may be relatively short leading to substantial censoring. We present a general Bayesian hierarchical framework that can handle this complexity by exploiting the similarity of cure fractions between end-points; accounting for the correlation between them and improving the extrapolation beyond the observed data. Assuming exchangeability between cure fractions facilitates the borrowing of information between end-points. We undertake a comprehensive simulation study to evaluate the model performance under different scenarios. We also show the benefits of using our approach with a motivating example, the CheckMate 067 phase 3 trial consisting of patients with metastatic melanoma treated with first line therapy.},
  archive      = {J_SIM},
  author       = {Nathan Green and Murat Kurt and Andriy Moshyk and James Larkin and Gianluca Baio},
  doi          = {10.1002/sim.70132},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70132},
  shortjournal = {Stat. Med.},
  title        = {A bayesian hierarchical mixture cure modelling framework to utilize multiple survival datasets for long-term survivorship estimates: A case study from previously untreated metastatic melanoma},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring causal effects of hormone- and radio-treatments in an observational study of breast cancer using copula-based semi-competing risks models. <em>SIM</em>, <em>44</em>(13-14), e70131. (<a href='https://doi.org/10.1002/sim.70131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer patients may experience relapse or death after surgery during the follow-up period, leading to dependent censoring of relapse. This phenomenon, known as semi-competing risk, imposes challenges in analyzing treatment effects on breast cancer and necessitates advanced statistical tools for unbiased analysis. Despite progress in estimation and inference within semi-competing risks regression, its application to causal inference is still in its early stages. This article aims to propose a frequentist and semi-parametric framework based on copula models that can facilitate valid causal inference, net quantity estimation and interpretation, and sensitivity analysis for unmeasured factors under right-censored semi-competing risks data. We also propose novel procedures to enhance parameter estimation and its applicability in practice. After that, we apply the proposed framework to a breast cancer study and detect the time-varying causal effects of hormone- and radio-treatments on patients' relapse and overall survival. Moreover, extensive numerical evaluations demonstrate the method's feasibility, highlighting minimal estimation bias and reliable statistical inference.},
  archive      = {J_SIM},
  author       = {Tonghui Yu and Mengjiao Peng and Yifan Cui and Elynn Chen and Chixiang Chen},
  doi          = {10.1002/sim.70131},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70131},
  shortjournal = {Stat. Med.},
  title        = {Exploring causal effects of hormone- and radio-treatments in an observational study of breast cancer using copula-based semi-competing risks models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and analysis of group sequential trials for repeated measurements when pipeline data occurs: A tutorial. <em>SIM</em>, <em>44</em>(13-14), e70130. (<a href='https://doi.org/10.1002/sim.70130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group sequential trials (GST) allow for early stopping of a clinical trial for efficacy or futility, without compromising its validity. Statistical methodology for GST is well established when the endpoint is observed immediately, but less so for endpoints that are measured with a delay, such as repeatedly measured outcomes for which the primary measurement of interest is taken after several months. The latter can result in pipeline subjects at an interim analysis. These subjects may have early outcome measurements available, but their final endpoint is yet to be observed. Accounting for these early measurements has been shown to increase statistical power. Most importantly, pipeline patients will contribute with additional data after a decision to stop enrollment has been taken at an interim analysis. To make the best use of all available data, these data are ideally incorporated in the final analysis in a formal way. In this tutorial paper, we provide guidance on how to plan a GST with repeated measurements and a delayed endpoint and how to analyze the data resulting from these trials. We discuss existing methods, and also expand on them, for example adding a nonbinding stopping rule for futility and working out the computational details to derive valid p -values and confidence intervals. We also provide an R package and R code to perform the methods discussed in this paper.},
  archive      = {J_SIM},
  author       = {Corine Baayen and Paul Blanche and Christopher Jennison and Brice Ozenne},
  doi          = {10.1002/sim.70130},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70130},
  shortjournal = {Stat. Med.},
  title        = {Design and analysis of group sequential trials for repeated measurements when pipeline data occurs: A tutorial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic information borrowing from external data in clinical trials: The elastic commensurate prior approach. <em>SIM</em>, <em>44</em>(13-14), e70129. (<a href='https://doi.org/10.1002/sim.70129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating external data into a clinical trial can introduce systematic bias in estimates and inflate the study's type I error due to differences in study design and enrollment criteria. Existing prior designs for information borrowing lack the ability to dynamically adjust the weight based on the similarity between concurrent and external data. To address this challenge, we thereby introduce a novel method called the elastic commensurate prior (ECP), which combines the commensurate prior with the elastic prior method. By dynamically adjusting the weight of external data using a measure of congruence, this method demonstrates strong performance in maintaining power while providing adequate type I error control across different scenarios, including congruence, approximate congruence, and incongruence between external and concurrent data. Compared to existing methods such as the modified power prior, meta-analytic-predictive (MAP) prior, robust MAP prior, non-informative prior, and fully informative prior, the ECP method is flexible and performs well across all settings. Furthermore, our method also allows for the integration of covariates in estimating data congruence for dynamic information borrowing, achieving both strong performance in power and adequate control of type I error. Overall, the ECP represents a promising option for leveraging external data in clinical trials, reducing costs by decreasing the sample size requirement, and thereby accelerating research and drug development timelines.},
  archive      = {J_SIM},
  author       = {Jike Huang and Fan Jia and Jiaxuan Li and Wanqiu Xie and Zhiwei Rong and Lan Mi and Yuqin Song and Yan Hou},
  doi          = {10.1002/sim.70129},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70129},
  shortjournal = {Stat. Med.},
  title        = {Dynamic information borrowing from external data in clinical trials: The elastic commensurate prior approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Radiomics of PET using neural networks for prediction of alzheimer's disease diagnosis. <em>SIM</em>, <em>44</em>(13-14), e70128. (<a href='https://doi.org/10.1002/sim.70128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positron emission tomography (PET) imaging technology is widely used for diagnosing Alzheimer's disease (AD) in people with dementia. Although various computational methods have been proposed for diagnosis of AD using PET images, prediction of disease diagnosis by leveraging longitudinal PET imaging data has not been widely studied. In this paper, we propose novel implementations of deep learning models graph neural network (GNN) and transformer encoder (TE), to leverage longitudinal sequences of PET images in addition to cognitive scores for prediction of AD diagnosis and prediction of conversion from cognitively unimpaired or mild cognitive impairment to AD using data collected in the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. In addition, we compare the performance of these two approaches with other methods including simple feed-forward networks (FFN) that only utilize PET images from a single time point and recurrent neural networks (RNN) that model longitudinal images by considering them as sequences, while failing to take into account between visit time variability. We show that GNN and TE have higher predictive performance than FFN and RNN in predicting disease diagnosis, where predictive performance is measured by accuracy, area under receiver operating characteristic (AUC) curve, and the Brier score. Furthermore, we illustrate the potential of PET images in predicting conversion from mild cognitive impairment or cognitively normal to AD.},
  archive      = {J_SIM},
  author       = {Yimo Zhang and Jon A. Steingrimsson and Akhil Ambekar and Hwamee Oh and Ani Eloyan},
  doi          = {10.1002/sim.70128},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70128},
  shortjournal = {Stat. Med.},
  title        = {Radiomics of PET using neural networks for prediction of alzheimer's disease diagnosis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian approach to the G-formula via iterative conditional regression. <em>SIM</em>, <em>44</em>(13-14), e70123. (<a href='https://doi.org/10.1002/sim.70123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal observational studies with time-varying confounders, the generalized computation algorithm formula (g-formula) is a principled tool to estimate the average causal effect of a treatment regimen. However, the standard non-iterative g-formula implementation requires specifying both the conditional distribution of the outcomes and the joint distribution of all time-varying covariates. This process can be cumbersome to implement and is prone to model misspecification bias. As an alternative, the iterative conditional expectation (ICE) g-formula estimator solely depends on a series of nested outcome regressions and avoids the need for specifying the full distribution of all time-varying covariates. This simplicity lends itself to the natural integration of flexible machine learning techniques to develop more robust average causal effect estimators with time-varying treatments. In this work, we introduce a Bayesian approach that includes parametric regressions and Bayesian Additive Regression Trees to flexibly model a series of outcome surfaces. We fit the ICE g-formula and develop a sampling algorithm to obtain samples from the posterior distribution of the final causal effect estimator. We illustrate the performance characteristics of the Bayesian ICE estimator and the associated variations via simulation studies and applications to two real world data examples.},
  archive      = {J_SIM},
  author       = {Ruyi Liu and Liangyuan Hu and Francis Perry Wilson and Joshua L. Warren and Fan Li},
  doi          = {10.1002/sim.70123},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70123},
  shortjournal = {Stat. Med.},
  title        = {A bayesian approach to the G-formula via iterative conditional regression},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic prediction of an event using multiple longitudinal markers: A model averaging approach. <em>SIM</em>, <em>44</em>(13-14), e70122. (<a href='https://doi.org/10.1002/sim.70122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic event prediction, using joint modeling of survival time and longitudinal variables, is extremely useful in personalized medicine. However, the estimation of joint models including many longitudinal markers is still a computational challenge because of the high number of random effects and parameters to be estimated. In this paper, we propose a model averaging strategy to combine predictions from several joint models for the event, including one longitudinal marker only or pairwise longitudinal markers. The prediction is computed as the weighted mean of the predictions from the one-marker or two-marker models, with the time-dependent weights estimated by minimizing the time-dependent Brier score. This method enables us to combine a large number of predictions issued from joint models to achieve a reliable and accurate individual prediction. Advantages and limits of the proposed methods are highlighted in a simulation study by comparison with the predictions from well-specified and misspecified all-marker joint models, as well as the one-marker and two-marker joint models. Using the PBC2 data set, the method is used to predict the risk of death in patients with primary biliary cirrhosis. The method is also used to analyze a French cohort study called the 3C data. In our study, seventeen longitudinal markers are considered to predict the risk of death.},
  archive      = {J_SIM},
  author       = {Reza Hashemi and Taban Baghfalaki and Viviane Philipps and Helene Jacqmin-Gadda},
  doi          = {10.1002/sim.70122},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70122},
  shortjournal = {Stat. Med.},
  title        = {Dynamic prediction of an event using multiple longitudinal markers: A model averaging approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive probabilities made simple: A fast and accurate method for clinical trial decision-making. <em>SIM</em>, <em>44</em>(13-14), e70120. (<a href='https://doi.org/10.1002/sim.70120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian predictive probabilities are commonly used for interim monitoring of clinical trials through efficacy and futility stopping rules. Despite their usefulness, the calculation of predictive probabilities, particularly in pre-experiment trial simulation, can be a significant challenge. We introduce an approximation for computing predictive probabilities using either a -value or a posterior probability that significantly reduces this burden. We show the approximation has a high degree of concordance with standard Monte Carlo imputation methods for computing predictive probabilities, and present five simulation studies comparing the approximation to the full predictive probability for a range of primary analysis strategies: Dichotomous, time-to-event, and ordinal endpoints, as well as historical borrowing and longitudinal modeling. We find that this faster method of predictive probability approximation works well in all five applications, thus significantly reducing the computational burden of trial simulation, allowing more virtual trials to be simulated to achieve greater precision in estimating trial operating characteristics.},
  archive      = {J_SIM},
  author       = {Joe Marion and Elizabeth Lorenzi and Cora Allen-Savietta and Scott Berry and Kert Viele},
  doi          = {10.1002/sim.70120},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70120},
  shortjournal = {Stat. Med.},
  title        = {Predictive probabilities made simple: A fast and accurate method for clinical trial decision-making},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). False discovery rate control for confounder selection using mirror statistics. <em>SIM</em>, <em>44</em>(13-14), e70116. (<a href='https://doi.org/10.1002/sim.70116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While data-driven confounder selection requires careful consideration, it is frequently employed in observational studies. Widely recognized criteria for confounder selection include the minimal-set approach, which involves selecting variables relevant to both treatment and outcome, and the union-set approach, which involves selecting variables associated with either treatment or outcome. These approaches are often implemented using heuristics and off-the-shelf statistical methods, where the degree of uncertainty may not be clear. In this paper, we focus on the false discovery rate (FDR) to measure uncertainty in confounder selection. We define the FDR specific to confounder selection and propose methods based on the mirror statistic, a recently developed approach for FDR control that does not rely on p -values. The proposed methods are p -value-free and require only the assumption of some symmetry in the distribution of the mirror statistic. It can be combined with sparse estimation and other methods that involve difficulties in deriving p -values. The properties of the proposed methods are investigated through exhaustive numerical experiments. Particularly in high-dimensional data scenarios, the proposed methods effectively control FDR and perform better than the p -value-based methods.},
  archive      = {J_SIM},
  author       = {Kazuharu Harada and Masataka Taguri},
  doi          = {10.1002/sim.70116},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70116},
  shortjournal = {Stat. Med.},
  title        = {False discovery rate control for confounder selection using mirror statistics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using a supervised principal components analysis for variable selection in high-dimensional datasets reduces false discovery rates. <em>SIM</em>, <em>44</em>(13-14), e70110. (<a href='https://doi.org/10.1002/sim.70110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional datasets, where the number of variables ‘ p $$ p $$ ’ is much larger than the number of samples ‘ ’, are ubiquitous and often render standard classification techniques unreliable due to overfitting. An important research problem is feature selection, which ranks candidate variables based on their relevance to the outcome variable and retains those that satisfy a chosen criterion. This article proposes a computationally efficient variable selection method based on principal component analysis tailored to a binary classification problem or case-control study. This method is accessible and is suitable for the analysis of high-dimensional datasets. We demonstrate the superior performance of our method through extensive simulations. A semi-real gene expression dataset, a challenging childhood acute lymphoblastic leukemia gene expression study, and a GWAS that attempts to identify single-nucleotide polymorphisms (SNPs) associated with rice grain length further demonstrate the usefulness of our method in genomic applications. We expect our method to accurately identify important features and reduce the False Discovery Rate (fdr) by accounting for the correlation between variables and by de-noising data in the training phase, which also makes it robust to mild outliers in the training data. Our method is almost as fast as univariate filters, so it allows valid statistical inference. The ability to make such inferences sets this method apart from most current multivariate statistical tools designed for today's high-dimensional data.},
  archive      = {J_SIM},
  author       = {Insha Ullah and Kerrie Mengersen and Anthony N. Pettitt and Benoit Liquet},
  doi          = {10.1002/sim.70110},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70110},
  shortjournal = {Stat. Med.},
  title        = {Using a supervised principal components analysis for variable selection in high-dimensional datasets reduces false discovery rates},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal allocation of observations in stepped-wedge and other cluster studies with correlated cluster-period effects. <em>SIM</em>, <em>44</em>(13-14), e70100. (<a href='https://doi.org/10.1002/sim.70100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped-wedge studies usually entail regular sampling of clusters over time. Yet the precision of the treatment effect estimator can sometimes be improved if the regular sampling scheme is replaced by one with preferential allocation of observations to particular time-epochs within each cluster. We present some exact results for optimizing the allocation for a general experimental layout under a mixed effects model with a time-varying cluster-autocorrelation structure, together with an algorithm for generating optimal allocations. An index of cluster variation is introduced, an increasing function of both the intra-class correlation and the total sample size, which encapsulates the influence of cluster-level variation on the optimal allocation. For any specified layout there is a sampling scheme (the ‘best natural allocation’) that solves the optimization problem for all values of this index up to a threshold value which depends only on the cluster autocorrelations. Under such a scheme the treatment effect estimator is equal to a simple difference between the means of the treated and control observations. Best natural allocations stand alongside conventional parallel and cross-over designs in giving equal weight to observations from all participants, even under stepped-wedge layouts with irreversible interventions. When applied to a recent study of primary care training programmes in low- and middle- income countries (The REaCH study), the results lead to substantial reductions in total sample size, without loss of precision. For stepped-wedge layouts with block-exchangeable or time-decaying cluster autocorrelations, we present explicit conditions for the optimality of staircase-type sampling schemes, which can arise as best natural allocations in such cases.},
  archive      = {J_SIM},
  author       = {Alan J. Girling and Samuel I. Watson},
  doi          = {10.1002/sim.70100},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70100},
  shortjournal = {Stat. Med.},
  title        = {Optimal allocation of observations in stepped-wedge and other cluster studies with correlated cluster-period effects},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the modeling of binary response regression based on new proposals for statistical diagnostics with applications to medical data. <em>SIM</em>, <em>44</em>(13-14), e70073. (<a href='https://doi.org/10.1002/sim.70073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary regression models utilizing logit or probit link functions have been extensively employed for examining the relationship between binary responses and covariates, particularly in medicine. Nonetheless, an erroneous specification of the link function may result in poor model fitting and compromise the statistical significance of covariate effects. In this study, we introduce a diagnostic method associated with a novel family of link functions enabling the assessment of sensitivity for symmetric links in relation to their asymmetric counterparts. This new family offers a comprehensive model encompassing nested symmetric cases. Our method proves beneficial in modeling medical data, especially when evaluating the sensitivity of the commonly used logit link function, prized for its interpretability via odds ratio. Moreover, our method advocates a general link based on the logit function when a standard link is unsatisfactory. We employ likelihood-based methods to estimate parameters of the general model and conduct local influence analysis under the case-weight perturbation scheme. Regarding local influence, we emphasize the relevance of employing appropriate perturbations to avoid misleading outcomes. Additionally, we introduce a diagnostic method for local influence, assessing the sensitivity of odds ratio under two perturbation schemes. Monte Carlo simulations are conducted to evaluate both the diagnostic method performance and parameter estimation of the general model, supplemented by illustrations using medical data related to menstruation and respiratory problems. The results confirm the efficacy of our proposal, highlighting the critical role of statistical diagnostics in modeling.},
  archive      = {J_SIM},
  author       = {Manuel Galea and Mónica Catalán and Alejandra Tapia and Viviana Giampaoli and Víctor Leiva},
  doi          = {10.1002/sim.70073},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70073},
  shortjournal = {Stat. Med.},
  title        = {Improving the modeling of binary response regression based on new proposals for statistical diagnostics with applications to medical data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rejoinder to commentaries on: On the uses and abuses of regression models: A call for reform of statistical practice and teaching. <em>SIM</em>, <em>44</em>(13-14), e70065. (<a href='https://doi.org/10.1002/sim.70065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {John B. Carlin and Margarita Moreno-Betancur},
  doi          = {10.1002/sim.70065},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70065},
  shortjournal = {Stat. Med.},
  title        = {Rejoinder to commentaries on: On the uses and abuses of regression models: A call for reform of statistical practice and teaching},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comments on “Novel non-linear models for clinical trial analysis with longitudinal data: A tutorial using SAS for both frequentist and bayesian methods”. <em>SIM</em>, <em>44</em>(13-14), e70058. (<a href='https://doi.org/10.1002/sim.70058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear longitudinal proportional effect models have been proposed to improve power and provide direct estimates of the proportional treatment effect in randomized clinical trials. These models assume a fixed proportional treatment effect over time, which can lead to bias and Type I error inflation when the assumption is violated. Even when the proportional effect assumption holds, these models are biased, and their inference is sensitive to the labeling of treatment groups. Typically, this bias favors the active group, inflates Type I error, and can result in one-sided testing. Conversely, the bias can make it more difficult to detect treatment harm, creating a safety concern.},
  archive      = {J_SIM},
  author       = {M. C. Donohue and P. S. Insel and O. Langford},
  doi          = {10.1002/sim.70058},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e70058},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Novel non-linear models for clinical trial analysis with longitudinal data: A tutorial using SAS for both frequentist and bayesian methods”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Commentary: Regression Models—Efforts are required to improve statistical practice and teaching. <em>SIM</em>, <em>44</em>(13-14), e10341. (<a href='https://doi.org/10.1002/sim.10341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Willi Sauerbrei and Federico Ambrogi and Riccardo de Bin and Anne-Laure Boulesteix and Els Goetghebeur and Marianne Huebner},
  doi          = {10.1002/sim.10341},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e10341},
  shortjournal = {Stat. Med.},
  title        = {Commentary: Regression Models—Efforts are required to improve statistical practice and teaching},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Some ways to make regression modeling more helpful than misleading. <em>SIM</em>, <em>44</em>(13-14), e10313. (<a href='https://doi.org/10.1002/sim.10313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Sander Greenland},
  doi          = {10.1002/sim.10313},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e10313},
  shortjournal = {Stat. Med.},
  title        = {Some ways to make regression modeling more helpful than misleading},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discussion of “On the uses and abuses of regression models: A call for reform of statistical practice and teaching”. <em>SIM</em>, <em>44</em>(13-14), e10312. (<a href='https://doi.org/10.1002/sim.10312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Stijn Vansteelandt and Johan Steen},
  doi          = {10.1002/sim.10312},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e10312},
  shortjournal = {Stat. Med.},
  title        = {Discussion of “On the uses and abuses of regression models: A call for reform of statistical practice and teaching”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). To explain, to predict, or to describe: Figuring out the study goal [Commentary on “On the uses and abuses of regression models” by carlin and moreno-betancur]. <em>SIM</em>, <em>44</em>(13-14), e10307. (<a href='https://doi.org/10.1002/sim.10307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I strongly support Carlin and Moreno-Betancur's assertion that regression modeling (and in fact, any modeling) should be driven by the type of research question: descriptive, predictive, or causal. I share six points to highlight and clarify further confusions and suggest additional tricks to identify the right type. These include a focus on actions, individual vs. collective, additional causal typologies, the significance of variable names, software-related issues, and the role of data.},
  archive      = {J_SIM},
  author       = {Galit Shmueli},
  doi          = {10.1002/sim.10307},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e10307},
  shortjournal = {Stat. Med.},
  title        = {To explain, to predict, or to describe: Figuring out the study goal [Commentary on “On the uses and abuses of regression models” by carlin and moreno-betancur]},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Commentary: Teaching statistics as minor Subject—Handing on fire, not worshipping ashes. <em>SIM</em>, <em>44</em>(13-14), e10284. (<a href='https://doi.org/10.1002/sim.10284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Mariana Nold and Georg Heinze},
  doi          = {10.1002/sim.10284},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e10284},
  shortjournal = {Stat. Med.},
  title        = {Commentary: Teaching statistics as minor Subject—Handing on fire, not worshipping ashes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the uses and abuses of regression models: A call for reform of statistical practice and teaching. <em>SIM</em>, <em>44</em>(13-14), e10244. (<a href='https://doi.org/10.1002/sim.10244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression methods dominate the practice of biostatistical analysis, but biostatistical training emphasizes the details of regression models and methods ahead of the purposes for which such modeling might be useful. More broadly, statistics is widely understood to provide a body of techniques for “modeling data,” underpinned by what we describe as the “true model myth”: that the task of the statistician/data analyst is to build a model that closely approximates the true data generating process. By way of our own historical examples and a brief review of mainstream clinical research journals, we describe how this perspective has led to a range of problems in the application of regression methods, including misguided “adjustment” for covariates, misinterpretation of regression coefficients and the widespread fitting of regression models without a clear purpose. We then outline a new approach to the teaching and application of biostatistical methods, which situates them within a framework that first requires clear definition of the substantive research question at hand, within one of three categories: descriptive, predictive, or causal. Within this approach, the development and application of (multivariable) regression models, as well as other advanced biostatistical methods, should proceed differently according to the type of question. Regression methods will no doubt remain central to statistical practice as they provide a powerful tool for representing variation in a response or outcome variable as a function of “input” variables, but their conceptualization and usage should follow from the purpose at hand.},
  archive      = {J_SIM},
  author       = {John B. Carlin and Margarita Moreno-Betancur},
  doi          = {10.1002/sim.10244},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13-14},
  pages        = {e10244},
  shortjournal = {Stat. Med.},
  title        = {On the uses and abuses of regression models: A call for reform of statistical practice and teaching},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized biopsy schedules using an interval-censored cause-specific joint model. <em>SIM</em>, <em>44</em>(10-12), e70134. (<a href='https://doi.org/10.1002/sim.70134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active surveillance (AS), where biopsies are conducted to detect cancer progression, has been acknowledged as an efficient way to reduce the overtreatment of prostate cancer. Most AS cohorts use fixed biopsy schedules for all patients. However, the ideal test frequency remains unknown, and the routine use of such invasive tests burdens the patients. An emerging idea is to generate personalized biopsy schedules based on each patient's progression-specific risk. To achieve that, we propose the interval-censored cause-specific joint model (ICJM), which models the impact of longitudinal biomarkers on cancer progression while considering the competing event of early treatment initiation. The underlying likelihood function incorporates the interval-censoring of cancer progression, the competing risk of treatment, and the uncertainty about whether cancer progression occurred since the last biopsy in patients that are right-censored or experience the competing event. The model can produce patient-specific risk profiles up to a horizon time. If the risk exceeds a certain threshold, a biopsy is conducted. The optimal threshold can be chosen by balancing two indicators of the biopsy schedules: The expected number of biopsies and the expected delay in detection of cancer progression. A simulation study showed that our personalized schedules could considerably reduce the number of biopsies per patient by 41%–52% compared to the fixed schedules, though at the cost of a slightly longer detection delay.},
  archive      = {J_SIM},
  author       = {Zhenwei Yang and Dimitris Rizopoulos and Eveline A. M. Heijnsdijk and Lisa F. Newcomb and Nicole S. Erler},
  doi          = {10.1002/sim.70134},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70134},
  shortjournal = {Stat. Med.},
  title        = {Personalized biopsy schedules using an interval-censored cause-specific joint model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and inference for the additive mean residual life model with change point in the continuous covariate. <em>SIM</em>, <em>44</em>(10-12), e70133. (<a href='https://doi.org/10.1002/sim.70133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean residual life model offers comprehensive insights into the relationship between a patient's remaining life expectancy and various covariates. In cancer research, a patient's lifespan may dramatically change when a covariate exceeds a threshold. Accurately detecting and estimating this change-point is crucial for predicting the patient's life expectancy and preventing diseases. To address these challenges, we investigate the additive mean residual life model with an unknown change-point in the continuous covariate. The martingale-based smoothed estimating equations are used to estimate the baseline mean residual life function, regression parameters, and the unknown change-point. Furthermore, we establish the large sample properties of the estimator under mild conditions, and propose a maximal Wald test statistic to test the existence of the change-point. Finally, we assess the finite sample performance of our proposed method through extensive simulation studies and apply it to two real data examples from studies on stomach cancer and primary biliary cirrhosis. Supplementary materials for this article are available online.},
  archive      = {J_SIM},
  author       = {Yun Lin and Xiaoran Yang and Fangfang Bai},
  doi          = {10.1002/sim.70133},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70133},
  shortjournal = {Stat. Med.},
  title        = {Estimation and inference for the additive mean residual life model with change point in the continuous covariate},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal inference with outcomes truncated by death and missing not at random. <em>SIM</em>, <em>44</em>(10-12), e70126. (<a href='https://doi.org/10.1002/sim.70126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analysis. In this article, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a human immunodeficiency virus study.},
  archive      = {J_SIM},
  author       = {Wei Li and Yuan Liu and Shanshan Luo and Zhi Geng},
  doi          = {10.1002/sim.70126},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70126},
  shortjournal = {Stat. Med.},
  title        = {Causal inference with outcomes truncated by death and missing not at random},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interconnections of multimorbidity-related clinical outcomes: Analysis of health administrative claims data with a dynamic network approach. <em>SIM</em>, <em>44</em>(10-12), e70125. (<a href='https://doi.org/10.1002/sim.70125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the rising complexity and burden of multimorbidity, it is crucial to provide evidence-based support for managing multimorbidity-related clinical outcomes. This study introduces a dynamic network approach to investigate conditional and time-varying interconnections in disease-specific clinical outcomes. Our method effectively tackles the issue of zero inflation, a frequent challenge in medical data that complicates traditional modeling techniques. The theoretical foundations of the proposed approach are rigorously developed and validated through extensive simulations. Using Taiwan's health administrative claims data from 2000 to 2013, we construct 14 yearly networks that are temporally correlated, featuring 125 nodes that represent different disease conditions. Key network properties, such as connectivity, module, and temporal variation are analyzed. To demonstrate how these networks can inform multimorbidity management, we focus on breast cancer and analyze the relevant network structures. The findings provide valuable clinical insights that enhance the current understanding of multimorbidity. The proposed methods offer promising applications in shaping treatment strategies, optimizing health resource allocation, and informing health policy development in the context of multimorbidity management.},
  archive      = {J_SIM},
  author       = {Hao Mei and Haonan Xiao and Ben-Chang Shia and Guanzhong Qiao and Yang Li},
  doi          = {10.1002/sim.70125},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70125},
  shortjournal = {Stat. Med.},
  title        = {Interconnections of multimorbidity-related clinical outcomes: Analysis of health administrative claims data with a dynamic network approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Closed MCP-mod for pairwise comparisons of several doses with a control. <em>SIM</em>, <em>44</em>(10-12), e70124. (<a href='https://doi.org/10.1002/sim.70124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The MCP-Mod approach by Bretz et al. is commonly applied for dose–response testing and estimation in clinical trials. The MCP part of MCP-Mod was originally developed to detect a dose–response signal using a multiple contrast test, but it is not appropriate to make a specific claim that the drug has a positive effect at an individual dose. In this paper, we extend the MCP-Mod approach to obtain confirmatory p -values for detecting a dose–response signal as well as for the pairwise comparisons of the individual doses against placebo. We apply the closed test principle from Marcus et al. to the optimal contrast tests based on a candidate set of plausible dose–response shapes available at the planning stage of a clinical trial. We show that the contrast coefficients have to be optimized under suitable constraints to guarantee strong Type 1 error rate control at a pre-specified significance level. Motivated by a recent clinical trial, we evaluate the operating characteristics of the proposed methods in a comprehensive simulation study.},
  archive      = {J_SIM},
  author       = {Franz Koenig and Sergey Krasnozhon and Bjoern Bornkamp and Frank Bretz and Ekkehard Glimm and Alexandra Graf and Dong Xi},
  doi          = {10.1002/sim.70124},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70124},
  shortjournal = {Stat. Med.},
  title        = {Closed MCP-mod for pairwise comparisons of several doses with a control},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating complex selection rules into the latent overlapping group lasso for the construction of coherent prediction models. <em>SIM</em>, <em>44</em>(10-12), e70121. (<a href='https://doi.org/10.1002/sim.70121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction models are important in medical research, as such models enable health researchers to gain deeper insights into disease epidemiology and clinicians to identify patients at higher risk of adverse outcomes. One commonly employed approach to developing prediction models is variable selection through penalized regression. Integrating natural variable structures and predefined inclusion requirements into variable selection not only enhances model interpretability but can also potentially boost prediction accuracy. For example, the latent overlapping group Lasso can force the inclusion of the main terms in the resulting model if their interaction term is selected. However, when variable structures are complex, it is challenging to integrate such structures into the penalized regression. In this work, we first demonstrate how to convert variable structures and predefined variable inclusion requirements into “selection rules” (which represent rules for which or how variables can be included in the final prediction model) and present these rules mathematically. Then, we provide a structured approach for integrating complex rules into variable selection through the latent overlapping group Lasso so that the resulting prediction model follows the given selection rules. To illustrate our methodology, we applied these techniques to construct a coherent prediction model for major bleeding in hypertensive patients recently hospitalized for atrial fibrillation and subsequently prescribed oral anticoagulants. In this application, we account for a proxy of anticoagulant adherence and its interaction with dosage and the type of oral anticoagulants, in addition to drug-drug interactions.},
  archive      = {J_SIM},
  author       = {Guanbo Wang and Sylvie Perreault and Robert W. Platt and Rui Wang and Marc Dorais and Mireille E. Schnitzer},
  doi          = {10.1002/sim.70121},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70121},
  shortjournal = {Stat. Med.},
  title        = {Integrating complex selection rules into the latent overlapping group lasso for the construction of coherent prediction models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample size calculation in dose optimization trials using the margin of practical non-inferiority. <em>SIM</em>, <em>44</em>(10-12), e70118. (<a href='https://doi.org/10.1002/sim.70118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dose optimization trial in oncology may be performed to compare an approved dose level of a given drug with a reduced dose level, testing the hypothesis that efficacy is maintained whilst reducing side effects and consequently improving adherence and quality-of-life. This is particularly relevant with modern therapeutic agents whose mechanisms of action imply that efficacy may not necessarily be linearly related to the dose. Using a conventional non-inferiority framework leads to large sample sizes that are often unfeasible in the phase IV setting. An alternative is to use a margin of practical non-inferiority, which we define in this paper and show how it can be exploited to justify a sample size. Whilst defining the extent of the margin, researchers also pre-specify the other dimensions of interest, such as receptor occupancy and/or side effects and quality-of-life, that will be used to establish practical non-inferiority if the observed efficacy of the reduced dose level lies within the margin. The comparison of efficacy is based on the observed difference between the reduced and the approved levels, instead of the confidence interval of this difference, leading to a reduction in sample size. The reduction in precision due to the smaller sample size is compensated by formally pre-specifying the additional dimensions to the decision process, allowing a more thorough assessment of the opportunity to reduce a dose in practice, with the many advantages that this may involve.},
  archive      = {J_SIM},
  author       = {Hakim-Moulay Dehbi and Sean Devlins and Alexia Iasonos and Matthew Nankivell and Duncan Gilbert and John O'Quigley},
  doi          = {10.1002/sim.70118},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70118},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation in dose optimization trials using the margin of practical non-inferiority},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The whys, whens, and hows of futility monitoring. <em>SIM</em>, <em>44</em>(10-12), e70117. (<a href='https://doi.org/10.1002/sim.70117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are different reasons to include futility monitoring in clinical trials. We often think of futility monitoring as being planned for business decisions, to limit time and money in a trial that will likely fail its primary objective. But there are also public health and ethical reasons for conducting, or not conducting, futility monitoring in clinical trials. Important considerations in futility monitoring include maintenance of trial integrity, the methods used and their operating characteristics, and the interplay with safety monitoring. This article will discuss some of the whys, whens, and hows of futility monitoring.},
  archive      = {J_SIM},
  author       = {Karen M. Higgins and Jyoti Zalkikar and Jialu Zhang and Yan Wang and Rebecca Chiu and Mat Soukup and Yeh-Fong Chen and Greg Levin},
  doi          = {10.1002/sim.70117},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70117},
  shortjournal = {Stat. Med.},
  title        = {The whys, whens, and hows of futility monitoring},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An augmented approach for regression analysis of case-cohort interval-censored failure time data. <em>SIM</em>, <em>44</em>(10-12), e70115. (<a href='https://doi.org/10.1002/sim.70115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case-cohort design is a valuable two-phase sampling scheme often used in situations with low disease incidence and challenges in obtaining covariate data due to cost constraints, and many methods have been proposed for its analysis. However, most of the existing approaches, particularly for interval-censored data, ignore the presence of non-expensive covariates that are typically observed for the entire cohort and thus can result in the significant loss of efficiency. To address this, under the framework of the semi-parametric transformation hazards model, we propose a supersampling approach that augments the case-cohort samples by introducing an additional sub-cohort, allowing us to incorporate non-expensive covariates. In the proposed method, the multiple imputation procedure is employed based on rejection sampling, and the asymptotic properties of the resulting estimator of regression parameters are established. To evaluate the finite sample performance of the proposed method, a simulation study is conducted, and it demonstrates its effectiveness in practical situations. Finally, the proposed methodology is applied to an HIV vaccine trial that motivated this study.},
  archive      = {J_SIM},
  author       = {Yichen Lou and Mingyue Du and Peijie Wang and Yuxiang Wu},
  doi          = {10.1002/sim.70115},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70115},
  shortjournal = {Stat. Med.},
  title        = {An augmented approach for regression analysis of case-cohort interval-censored failure time data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjustment of conditional bias in hazard ratios for group sequential testing of progression-free survival and overall survival. <em>SIM</em>, <em>44</em>(10-12), e70112. (<a href='https://doi.org/10.1002/sim.70112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In confirmatory randomized controlled trials of patients with metastatic cancer, progression-free survival (PFS) and overall survival (OS) are often used as multiple primary endpoints. The overall hierarchical strategy is a typical multiplicity adjustment method that analyzes PFS once and performs an interim OS analysis at the time of PFS analysis using an alpha-spending function—only if the statistical significance of PFS is demonstrated. A subsequent final OS analysis is conducted if the interim OS analysis does not result in early stopping for efficacy. In this study, we focused on the adjustment of conditional bias (CB) in hazard ratio estimates for OS in both interim and final analyses when a trial applied the overall hierarchical strategy. As CB-adjusting estimators for a single primary endpoint may have limited performance, we extended the conditional mean-adjusted estimator to the case of an overall hierarchical strategy. Motivated by an actual oncology trial, we evaluated the performance of the proposed estimators through a simulation study. In the case of early stopping for efficacy, the CB of the proposed estimator was smaller than that of the existing methods with comparable root mean squared error.},
  archive      = {J_SIM},
  author       = {Shoki Izumi and Shogo Nomura and Yutaka Matsuyama},
  doi          = {10.1002/sim.70112},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70112},
  shortjournal = {Stat. Med.},
  title        = {Adjustment of conditional bias in hazard ratios for group sequential testing of progression-free survival and overall survival},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-study factor regression model: An application in nutritional epidemiology. <em>SIM</em>, <em>44</em>(10-12), e70108. (<a href='https://doi.org/10.1002/sim.70108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diet is a risk factor for many diseases. In nutritional epidemiology, studying reproducible dietary patterns is critical to reveal important associations with health. However, this task is challenging: diverse cultural and ethnic backgrounds may critically impact eating patterns by showing heterogeneity, leading to incorrect dietary patterns and obscuring the components shared across different groups or populations. Moreover, covariate effects generated from observed variables, such as demographics and other confounders, can further bias these dietary patterns. Identifying the shared and group-specific dietary components and covariate effects is essential to drive accurate conclusions. To address these issues, we introduce a new modeling factor regression, the Multistudy Factor Regression (MSFR) model. The MSFR model analyzes different populations simultaneously, achieving three goals: capturing shared component(s) across populations, identifying group-specific structures, and correcting for covariate effects. We use this novel method to derive common and ethnic-specific dietary patterns in a multicenter epidemiological study in Hispanic/Latinos community. Our model improves the accuracy of common and group dietary signals, provides a robust estimation of factor cardinality, and yields better prediction than other techniques, revealing important associations with health and cardiovascular disease. In summary, we provide a tool to integrate different groups, providing accurate dietary signals crucial to inform public health policy.},
  archive      = {J_SIM},
  author       = {Roberta De Vito and Alejandra Avalos-Pacheco},
  doi          = {10.1002/sim.70108},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70108},
  shortjournal = {Stat. Med.},
  title        = {Multi-study factor regression model: An application in nutritional epidemiology},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COMIC: A bayesian dose optimization design for drug combination in multiple indications with application to CAR-T therapies. <em>SIM</em>, <em>44</em>(10-12), e70107. (<a href='https://doi.org/10.1002/sim.70107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Project Optimus, initiated by the US Food and Drug Administration (FDA), seeks to shift the focus of dose finding and selection from the maximum tolerated dose to the optimal dose that offers the most favorable risk-benefit balance. However, applying this paradigm shift to drug combination trials presents challenges, particularly due to limited sample sizes and a large two-dimensional dose exploration space. These challenges are amplified when trials involve multiple indications. To address this, we developed a two-stage Bayesian dose optimization design, called COMIC (Combination Optimization in Multiple IndiCations), to efficiently identify Optimal Biological Dose Combinations (OBDC) for multiple indications. The COMIC design follows a two-stage strategy: First, optimizing the dose for one indication based on a utility function that measures the risk-benefit tradeoff, and then using that data to inform and accelerate dose optimization for additional indications. This approach significantly reduces the required sample size. Additionally, we incorporate a pharmacodynamic endpoint (e.g., receptor occupancy) to prioritize which component of the combination should be escalated, further enhancing the efficiency of dose optimization. Simulation studies demonstrate the strong performance and robustness of the COMIC design across various scenarios. We illustrate the method using a CAR-T therapy trial.},
  archive      = {J_SIM},
  author       = {Kai Chen and Kentaro Takeda and Ying Yuan},
  doi          = {10.1002/sim.70107},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70107},
  shortjournal = {Stat. Med.},
  title        = {COMIC: A bayesian dose optimization design for drug combination in multiple indications with application to CAR-T therapies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence intervals and sample size for the ICC in two-way ANOVA models. <em>SIM</em>, <em>44</em>(10-12), e70106. (<a href='https://doi.org/10.1002/sim.70106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reliability of measurement instruments is vital in fields like medicine and psychology, where these tools are often used for diagnostic purposes. In reliability studies where participants are assessed by the same set of raters, the data can be modeled using a two-way ANOVA, with the intraclass correlation coefficient (ICC) as a key metric. This paper focuses on the ICC for agreement, which is crucial when the measurement values themselves, rather than just their rank ordering, are of interest. However, selecting appropriate confidence interval methods and determining adequate sample sizes for the ICC for agreement remains challenging. This work advances the understanding of confidence interval methods for the ICC for agreement, provides practical tools, and offers recommendations for selecting confidence interval methods and sample size procedures for planning reliability studies. In particular, we provide a comprehensive review and simulation-based comparison of six classes of confidence interval methods for the ICC for agreement identified in the literature. Our analysis includes a method based on the F-distribution, previously omitted, which demonstrates the best statistical properties in some cases. Then, in conjunction with the best-performing methods, we evaluate three sample size determination procedures based on the expected width of confidence intervals that we identified in the literature. To address the lack of accessible tools, we further developed an interactive R Shiny app, freely available to researchers, to compute confidence intervals and sample sizes. The utility of these methods is illustrated by a study on fetal heart rates.},
  archive      = {J_SIM},
  author       = {Dipro Mondal and Math J J M Candel and Alberto Cassese and Sophie Vanbelle},
  doi          = {10.1002/sim.70106},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70106},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals and sample size for the ICC in two-way ANOVA models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrative multi-omics and multivariate longitudinal data analysis for dynamic risk estimation in alzheimer's disease. <em>SIM</em>, <em>44</em>(10-12), e70105. (<a href='https://doi.org/10.1002/sim.70105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer's disease (AD) is a complex and progressive neurodegenerative disorder, characterized by diverse cognitive and functional impairments that manifest heterogeneously across individuals, domains, and time. The accurate assessment of AD's severity and progression requires integrating a variety of data modalities, including multivariate longitudinal neuropsychological tests and multi-omics datasets such as metabolomics and lipidomics. These data sources provide valuable insights into risk factors associated with dementia onset. However, effectively utilizing omics data in dynamic risk estimation for AD progression is challenging due to issues including high dimensionality, heterogeneity, and complex intercorrelations. To address these challenges, we develop a novel joint-modeling framework that effectively combines multi-omics factor analysis (MOFA) for dimension reduction and feature extraction with a multivariate functional mixed model (MFMM) for modeling longitudinal outcomes. This integrative joint modeling approach enables dynamic evaluation of dementia risk by leveraging both omics and longitudinal data. We validate the efficacy of our integrative model through extensive simulation studies and its practical application to the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset.},
  archive      = {J_SIM},
  author       = {Yuanyuan Guo and Haotian Zou and Mohammad Samsul Alam and Sheng Luo},
  doi          = {10.1002/sim.70105},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70105},
  shortjournal = {Stat. Med.},
  title        = {Integrative multi-omics and multivariate longitudinal data analysis for dynamic risk estimation in alzheimer's disease},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A perspective on the appropriate implementation of ICH E9(R1) addendum strategies for handling intercurrent events. <em>SIM</em>, <em>44</em>(10-12), e70104. (<a href='https://doi.org/10.1002/sim.70104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized clinical trials, stopping study medication, use of rescue treatment, and other intercurrent events can complicate interpretation of results. The ICH E9(R1) Addendum on estimands stimulated important cross-disciplinary discussions on trial objectives. Unfortunately, with influence of the Addendum, many trials have proposed analyzing primary endpoints using “while on treatment”, “hypothetical”, or “principal stratum” strategies that handle intercurrent events in ways that use post-randomization outcomes to exclude information from randomized participants and don't preserve integrity of randomization, or that don't reliably capture the intervention's meaningful net effects. These approaches have inherent limitations in ability to draw scientifically rigorous inference on clinically relevant causal effects important for decisions about adopting interventions. We describe advantages of trials with standard-of-care control arms targeting estimands using “treatment policy” approaches for intercurrent events, while potentially incorporating other meaningful intercurrent events, such as death, into the primary endpoint applying a composite strategy. Well-designed and -conducted trials targeting such estimands achieve scientifically rigorous causal inference through analyzes that protect the integrity of randomization. Such estimands also provide meaningful information relevant to real-world settings because they (1) are unconditional in nature i.e., they don't condition on post-treatment circumstances that might not be many participants' experiences; and (2) properly evaluate the experimental intervention within a regimen that includes possible ancillary care that would be clinically appropriate in real-world settings. We hope to add clarity about appropriate strategies for intercurrent events and how to improve design, conduct, and analysis of clinical trials to address questions of greatest clinical importance reliably.},
  archive      = {J_SIM},
  author       = {Thomas R. Fleming and Kevin J. Carroll and Janet T. Wittes and Scott S. Emerson and Mark D. Rothmann and Sylva Collins and Gregory Levin},
  doi          = {10.1002/sim.70104},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70104},
  shortjournal = {Stat. Med.},
  title        = {A perspective on the appropriate implementation of ICH E9(R1) addendum strategies for handling intercurrent events},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A biomarker signature-guided clinical trial design for precision medicine. <em>SIM</em>, <em>44</em>(10-12), e70103. (<a href='https://doi.org/10.1002/sim.70103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Targeted cancer therapies aim to effectively treat patients with specific biomarker profiles. Nevertheless, these therapies may not always precisely hit their intended targets, leading to uncertainty about the specific subset of patients who will benefit. To address this uncertainty, the identification of sensitive patient subsets in clinical trials becomes crucial. Our proposed phase IIB/III clinical trial design seeks to pinpoint a biomarker signature with precision, ensuring the accurate identification of patients who will respond to a specific treatment. This approach allows for the selective enrollment of sensitive patients to maximize benefits for trial participants. We incorporate Bayesian methodology to facilitate response-adaptive randomization, enhancing the likelihood that each participant receives his/her optimal treatment. Furthermore, our design uses inverse-probability-of-treatment-weighted analysis to avoid selection bias and control for the type I error rate. The evaluation of this trial design is based on four criteria: the statistical power, response rate of all patients participating in the current trial, their individual loss, and probabilities of receiving their optimal treatment for both current trial participants and future patients. Simulations demonstrate the proposed design's potential for maximizing trial participants' benefits with little sacrifice on statistical power. Its key advantages include an improved overall response rate within the trial and a higher percentage of patients receiving the optimal treatment.},
  archive      = {J_SIM},
  author       = {Yuan Li and Dejian Lai and Ruosha Li and Han Chen and Xuelin Huang and Jing Ning},
  doi          = {10.1002/sim.70103},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70103},
  shortjournal = {Stat. Med.},
  title        = {A biomarker signature-guided clinical trial design for precision medicine},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous-time causal inference with marked point process weights: An example on sodium-glucose co-transporters 2 inhibitor medications and urinary tract infection. <em>SIM</em>, <em>44</em>(10-12), e70102. (<a href='https://doi.org/10.1002/sim.70102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment-confounder feedback is present in time-to-recurrent or longitudinal event analysis when time-dependent confounders are themselves influenced by previous treatments. Conventional models produce misleading statistical inference of causal effects due to conditioning on these factors on the causal pathway. Marginal structural models are often applied to quantify the causal treatment effect, estimated using longitudinal weights that mimic the randomization procedure by balancing the covariate distributions across the treatment groups. The weights are usually constructed in discrete time intervals, which is appropriate if the follow-up visits are scheduled and regular. However, in primary care, visit times can be irregular and informative, and the treatment history consists of duration and doses. This can be modeled through a continuous-time marked point process. We constructed a continuous-time marginal structural model to estimate the effect of cumulative exposure to Sodium-Glucose co-Transporters 2 Inhibitor (SGLT-2i) medications on time-to-recurrent urinary tract infection (UTI). We used a cohort of type II diabetes patients with chronic kidney disease and constructed a marked point process that characterized the recurrent flare episodes of primary care visits (i.e., point process) with marks for the multinominal dose (none, low, high) of SGLT-2i medications and recurrent episodes of UTI. We applied the stabilized and optimal treatment weights to estimate the hypothesized causal effect. Our results are concordant with earlier findings in which the recurrent episodes of UTI did not increase when patients were prescribed low dose or high dose of SGLT-2i medications.},
  archive      = {J_SIM},
  author       = {Sumeet Kalia and Olli Saarela and Tao Chen and Braden O'Neill and Christopher Meaney and Rahim Moineddin and Babak Aliarzadeh and Frank Sullivan and Michelle Greiver},
  doi          = {10.1002/sim.70102},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70102},
  shortjournal = {Stat. Med.},
  title        = {Continuous-time causal inference with marked point process weights: An example on sodium-glucose co-transporters 2 inhibitor medications and urinary tract infection},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing events, not patients, maximizes power of the logrank test: And other insights on unequal randomization in survival trials. <em>SIM</em>, <em>44</em>(10-12), e70101. (<a href='https://doi.org/10.1002/sim.70101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the question of what randomization ratio (RR) maximizes the power of the logrank test (LRT) in event-driven survival trials under proportional hazards (PH). By comparing three approximations of the LRT (Schoenfeld, Freedman, and Rubinstein) to empirical simulations, we find that the RR that maximizes power is the RR that balances the number of events across treatment arms at the end of the trial. This contradicts the common misconception implied by Schoenfeld's approximation that 1:1 randomization maximizes power. Besides power, we consider other factors that might influence the choice of RR (accrual, trial duration, sample size, etc.). We perform simulations to better understand how unequal randomization might impact these factors in practice. Altogether, we derive 5 insights to guide statisticians in the design of survival trials considering unequal randomization.},
  archive      = {J_SIM},
  author       = {Godwin Yung and Kaspar Rufibach and Marcel Wolbers and Ray Lin and Yi Liu},
  doi          = {10.1002/sim.70101},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70101},
  shortjournal = {Stat. Med.},
  title        = {Balancing events, not patients, maximizes power of the logrank test: And other insights on unequal randomization in survival trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A co-segmentation algorithm to predict emotional stress from passively sensed mHealth data. <em>SIM</em>, <em>44</em>(10-12), e70099. (<a href='https://doi.org/10.1002/sim.70099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a data-driven cosegmentation algorithm of passively sensed and self-reported active variables collected through smartphones to identify emotionally stressful states in middle-aged and older patients with mood disorders undergoing therapy, some of whom also have chronic pain. Our method leverages the association between the different types of time series. These data are typically nonstationary, with meaningful associations often occurring only over short time windows. Traditional machine learning (ML) methods, when applied globally on the entire time series, often fail to capture these time-varying local patterns. Our approach first segments the passive sensing variables by detecting their change points, then examines segment-specific associations with the active variable to identify cosegmented periods that exhibit distinct relationships between stress and passively sensed measures. We then use these periods to predict future emotional stress states using standard ML methods. By shifting the unit of analysis from individual time points to data-driven segments of time and allowing for different associations in different segments, our algorithm helps detect patterns that only exist within short-time windows. We apply our method to detect periods of stress in patient data collected during ALACRITY Phase I study. Our findings indicate that the data-driven segmentation algorithm identifies stress periods more accurately than traditional ML methods that do not incorporate segmentation.},
  archive      = {J_SIM},
  author       = {Younghoon Kim and Sumanta Basu and Samprit Banerjee},
  doi          = {10.1002/sim.70099},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70099},
  shortjournal = {Stat. Med.},
  title        = {A co-segmentation algorithm to predict emotional stress from passively sensed mHealth data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UNITED: A unified transparent and efficient phase I/II trial design for dose optimization accounting for ordinal graded, continuous and mixed toxicity and efficacy endpoints. <em>SIM</em>, <em>44</em>(10-12), e70098. (<a href='https://doi.org/10.1002/sim.70098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primary objective of oncology dose-finding trials for novel therapies is to determine an optimal biological dose (OBD) that is both tolerable and therapeutically beneficial for patients in subsequent clinical trials. These new therapeutic agents are more likely to induce multiple low- or moderate-grade toxicities rather than dose-limiting toxicities. Additionally, efficacy is evaluated comprehensively, differentiating between complete remission and partial remission, as well as incorporating continuous efficacy endpoints. This important issue was highlighted in the American Statistical Association (ASA) Biopharmaceutical (BIOP) Section open forums and was a significant consideration of the FDA's “Project Optimus.” We proposed the UNITED design, a unified, transparent, and efficient Phase I/II trial design to incorporate toxicity and efficacy grades and types, as well as continuous efficacy responses, into dose-finding and optimization. The UNITED design can handle binary, quasi-binary, continuous, and mixed toxicity and efficacy endpoints. We further extended the UNITED design, referred to as TITE-UNITED, to accommodate delayed toxicity and efficacy outcomes. Simulation studies showed that the UNITED and TITE-UNITED designs have desirable operating characteristics, performing comparably to or better than existing designs. A user-friendly software is available for practical implementation.},
  archive      = {J_SIM},
  author       = {Mingyue Li and Zhonglong Guo and Yingjie Qiu},
  doi          = {10.1002/sim.70098},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70098},
  shortjournal = {Stat. Med.},
  title        = {UNITED: A unified transparent and efficient phase I/II trial design for dose optimization accounting for ordinal graded, continuous and mixed toxicity and efficacy endpoints},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample size and power calculations with win measures based on hierarchical endpoints. <em>SIM</em>, <em>44</em>(10-12), e70096. (<a href='https://doi.org/10.1002/sim.70096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Win measures, such as win ratio, win odds, net benefit, and desirability of outcome ranking (DOOR), have become popular approaches for the analysis of hierarchical endpoints in clinical studies. Sample size and power calculations with win measures based on hierarchical endpoints are often based on simulation studies that can be cumbersome. Existing sample size and power formulas require investigators to specify clinically significant and meaningful magnitudes of win measures and probability of ties that are difficult to elicit based on prior published literature or preliminary data. In this paper, we provide sample size and power calculation formulas for the four win measures. To facilitate the formula-based sample size or power calculations, we provide formulas to compute overall win measures and overall probability of ties needed by using the specification of marginal win measures and marginal probability of ties that are readily available from clinical investigators or literature. The latter formulas provide a novel way to specify a meaningful and justifiable magnitude of win measures and the magnitude of probability of ties. Therefore, they can be readily used to evaluate the powers based on the number of multiple endpoints, the ordering, and types of endpoints. Our extensive simulation studies show that the power estimations based on these formulas are often like the simulated powers for any type of correlated hierarchical endpoints except for scenarios with very high correlations between endpoints. We illustrate the usefulness of our formulas by using data from three trials with different types of hierarchical endpoints.},
  archive      = {J_SIM},
  author       = {Huiman Barnhart and Yuliya Lokhnygina and Roland Matsouaka and Susan Halabi and David Yanez and Robert J. Mentz and Frank Rockhold},
  doi          = {10.1002/sim.70096},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70096},
  shortjournal = {Stat. Med.},
  title        = {Sample size and power calculations with win measures based on hierarchical endpoints},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal surrogate-assisted sampling for cost-efficient validation of electronic health record outcomes. <em>SIM</em>, <em>44</em>(10-12), e70095. (<a href='https://doi.org/10.1002/sim.70095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic Health Record (EHR) databases are an increasingly valuable resource for observational studies. However, misclassification of EHR-derived outcomes due to imperfect phenotyping leads to bias, inflated type I error, and reduced power in risk-factor association studies. On the other hand, manual chart review to validate outcomes is both cost-prohibitive and time-consuming, and a randomly selected validation sample may not yield sufficient cases to support precise model estimation when the disease is rare. Sampling procedures have been developed for maximizing computational and statistical efficiency in settings where the true disease status is known. However, less work has been done in measurement constrained settings, particularly when an informative surrogate outcome is available. Motivated by this gap, we propose an Optimal Subsampling strategy with Surrogate-Assisted Two-step procedure (OSSAT) to guide cost-effective chart review in measurement constrained settings. The sampling weight in OSSAT leverages information contained in the potentially misclassified phenotype and covariates to prioritize observations most informative for the model of interest. We compare our proposed weight with existing approaches through simulations under various covariate distributions, differential misclassification rates and degrees of surrogate accuracy. We then apply our proposed weighting schemes to a study of risk factors for second breast cancer events using a real EHR data set.},
  archive      = {J_SIM},
  author       = {Arielle Marks-Anglin and Jianmin Chen and Chongliang Luo and Rebecca Hubbard and Yong Chen},
  doi          = {10.1002/sim.70095},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70095},
  shortjournal = {Stat. Med.},
  title        = {Optimal surrogate-assisted sampling for cost-efficient validation of electronic health record outcomes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why recommended visit intervals should be extracted when conducting longitudinal analyses using electronic health record data: Examining visit mechanism and sensitivity to assessment not at random. <em>SIM</em>, <em>44</em>(10-12), e70094. (<a href='https://doi.org/10.1002/sim.70094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) provide an efficient approach to generating rich longitudinal datasets. However, since patients visit as needed, the assessment times are typically irregular and may be related to the patient's health. Failing to account for this informative assessment process could result in biased estimates of the disease course. In this paper, we show how estimation of the disease trajectory can be enhanced by leveraging an underutilized piece of information that is often in the patient's EHR: physician-recommended intervals between visits. Specifically, we demonstrate how recommended intervals can be used in characterizing the assessment process and in investigating the sensitivity of the results to assessment not at random (ANAR). We illustrate our proposed approach in a clinic-based cohort study of juvenile dermatomyositis (JDM). In this study, we found that the recommended intervals explained 78% of the variability in the assessment times. Under a specific case of ANAR where we assumed that a worsening in disease led to patients visiting earlier than recommended, the estimated population average disease activity trajectory was shifted downward relative to the trajectory assuming assessment at random. These results demonstrate the crucial role recommended intervals play in improving the rigor of the analysis by allowing us to assess both the plausibility of the AAR assumption and the sensitivity of the results to departures from this assumption. Thus, we advise that studies using irregular longitudinal data should extract recommended visit intervals and follow our procedure for incorporating them into analyses.},
  archive      = {J_SIM},
  author       = {Rose H. Garrett and Masum Patel and Brian M. Feldman and Eleanor M. Pullenayegum},
  doi          = {10.1002/sim.70094},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70094},
  shortjournal = {Stat. Med.},
  title        = {Why recommended visit intervals should be extracted when conducting longitudinal analyses using electronic health record data: Examining visit mechanism and sensitivity to assessment not at random},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M-learning for individual treatment rule with survival outcomes. <em>SIM</em>, <em>44</em>(10-12), e70093. (<a href='https://doi.org/10.1002/sim.70093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment rules (ITRs) tailor treatments to individuals based on their unique characteristics to optimize clinical outcomes and resource allocation. Current approaches use outcome modeling or propensity score weighting to control confounding in complex medical data. To avoid model misspecification and the impact of extreme weights, matched-learning (M-learning) was recently proposed for continuous outcomes. In this paper, we expand the existing M-learning methodology to estimate optimal ITRs under right-censored data, as time-to-event outcomes are common in medical research. We construct matched sets for individuals by comparing observed times and incorporate an inverse probability censoring weight into the value function to handle censored observations. Additionally, we consider a full matching design as a possible alternative to the matching with replacement in M-learning. We demonstrate that the proposed value function is unbiased for the true value function without censoring. To gain insight into the empirical performance, we conduct an extensive simulation study that compares M-learning with two matching designs and a weighed learning approach. Results are evaluated based on winning probabilities and estimated values. The simulation reveals that all methods are generally fine in the absence of unmeasured confounders, and different methods show somewhat different performances under various scenarios. But their performance drops substantially in the presence of unmeasured confounders. Finally, we apply these methods to estimate optimal ITRs for patients with atrial fibrillation (AF) complications from an electronic medical record database, where full matching design shows slightly better performance.},
  archive      = {J_SIM},
  author       = {Zhizhen Zhao and Ai Ni and Xinyi Xu and Macarius Donneyong and Bo Lu},
  doi          = {10.1002/sim.70093},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70093},
  shortjournal = {Stat. Med.},
  title        = {M-learning for individual treatment rule with survival outcomes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomization-based inference for MCP-mod. <em>SIM</em>, <em>44</em>(10-12), e70092. (<a href='https://doi.org/10.1002/sim.70092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dose selection is critical in pharmaceutical drug development, as it directly impacts therapeutic efficacy and patient's safety of a drug. The Generalized Multiple Comparison Procedures and Modeling approach is commonly used in Phase II trials for testing and estimation of dose-response relationships. However, its effectiveness in small sample sizes, particularly with binary endpoints, is hindered by issues like complete separation in logistic regression, leading to non existence of estimates. Motivated by an actual clinical trial using the MCP-Mod approach, this paper introduces penalized maximum likelihood estimation (MLE) and randomization-based inference techniques to address these challenges. Randomization-based inference allows for exact finite sample inference, while population-based inference for MCP-Mod typically relies on asymptotic approximations. Simulation studies demonstrate that randomization-based tests can enhance statistical power in small to medium-sized samples while maintaining control over type-I error rates, even in the presence of time trends. Our results show that residual-based randomization tests using penalized MLEs not only improve computational efficiency but also outperform standard randomization-based methods, making them an adequate choice for dose-finding analyses within the MCP-Mod framework. Additionally, we apply these methods to pharmacometric settings, demonstrating their effectiveness in such scenarios. The results in this paper underscore the potential of randomization-based inference for the analysis of dose-finding trials, particularly in small sample contexts.},
  archive      = {J_SIM},
  author       = {Lukas Pin and Oleksandr Sverdlov and Frank Bretz and Björn Bornkamp},
  doi          = {10.1002/sim.70092},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70092},
  shortjournal = {Stat. Med.},
  title        = {Randomization-based inference for MCP-mod},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference for cumulative incidences and treatment effects in randomized controlled trials with time-to-event outcomes under ICH e9 (R1). <em>SIM</em>, <em>44</em>(10-12), e70091. (<a href='https://doi.org/10.1002/sim.70091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized controlled trials (RCTs) that focus on time-to-event outcomes, intercurrent events can arise in two ways: as semi-competing events, which modify the hazard of the primary outcome events, or as competing events, which make the definition of the primary outcome events unclear. Although five strategies have been proposed in the ICH E9 (R1) addendum to address intercurrent events in RCTs, these strategies are not easily applicable to time-to-event outcomes when aiming for causal interpretations. In this study, we show how to define, estimate, and make inferences concerning objectives that have causal interpretations within these contexts. Specifically, we derive the mathematical formulations of the causal estimands corresponding to the five strategies and clarify the data structure needed to identify these causal estimands. Furthermore, we introduce nonparametric methods for estimating and making inferences about these causal estimands, including the asymptotic variance of estimators and the construction of hypothesis tests. Finally, we illustrate our methods using data from the LEADER Trial, which aims to investigate the effect of liraglutide on cardiovascular outcomes.},
  archive      = {J_SIM},
  author       = {Yuhao Deng and Shasha Han and Xiao-Hua Zhou},
  doi          = {10.1002/sim.70091},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70091},
  shortjournal = {Stat. Med.},
  title        = {Inference for cumulative incidences and treatment effects in randomized controlled trials with time-to-event outcomes under ICH e9 (R1)},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TODO: A triple-outcome double-criterion optimal design for dose monitoring-and-optimization in multi-dose randomized trials. <em>SIM</em>, <em>44</em>(10-12), e70090. (<a href='https://doi.org/10.1002/sim.70090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting the efficacy signal and determining the optimal dose are critical steps to increase the probability of success and expedite the drug development in cancer treatment. After identifying a safe dose range through phase I studies, conducting a multidose randomized trial becomes an effective approach to achieve this objective. However, there have been limited formal statistical designs for such multidose trials, and dose selection in practice is often ad hoc, relying on descriptive statistics. We propose a Bayesian optimal two-stage design to facilitate rigorous dose monitoring and optimization. Utilizing a flexible Bayesian dynamic linear model for the dose–response relationship, we employ dual criteria to assess dose admissibility and desirability. Additionally, we introduce a triple-outcome trial decision procedure to consider dose selection beyond clinical factors. Under the proposed model and decision rules, we develop a systematic calibration algorithm to determine the sample size and Bayesian posterior probability cutoffs to optimize specific design operating characteristics. Furthermore, we demonstrate how to concurrently assess toxicity and efficacy within the proposed framework using a utility-based risk-benefit trade-off. To validate the effectiveness of our design, we conduct extensive simulation studies across a variety of scenarios, demonstrating its robust operating characteristics.},
  archive      = {J_SIM},
  author       = {Jingyi Zhang and Heng Zhou and Nolan A. Wages and Zifang Guo and Fang Liu and Thomas Jemielita and Fangrong Yan and Ruitao Lin},
  doi          = {10.1002/sim.70090},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70090},
  shortjournal = {Stat. Med.},
  title        = {TODO: A triple-outcome double-criterion optimal design for dose monitoring-and-optimization in multi-dose randomized trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An alternative measure for quantifying the heterogeneity in meta-analysis. <em>SIM</em>, <em>44</em>(10-12), e70089. (<a href='https://doi.org/10.1002/sim.70089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying the heterogeneity is an important issue in meta-analysis, and among the existing measures, the statistic is most commonly used. In this article, we first illustrate with a simple example that the statistic is heavily dependent on the study sample sizes, mainly because it is used to quantify the heterogeneity between the observed effect sizes. To reduce the influence of sample sizes, we introduce an alternative measure that aims to directly measure the heterogeneity between the study populations involved in the meta-analysis. We further propose a new estimator, namely the statistic, to estimate the newly defined measure of heterogeneity. For practical implementation, the exact formulas of the statistic are also derived under two common scenarios with the effect size as the mean difference (MD) or the standardized mean difference (SMD). Simulations and real data analyses demonstrate that the statistic provides an asymptotically unbiased estimator for the absolute heterogeneity between the study populations, and it is also independent of the study sample sizes as expected. To conclude, our newly defined statistic can be used as a supplemental measure of heterogeneity to monitor the situations where the study effect sizes are indeed similar with little biological difference. In such scenario, the fixed-effect model can be appropriate; nevertheless, when the sample sizes are sufficiently large, the statistic may still increase to 1 and subsequently suggest the random-effects model for meta-analysis.},
  archive      = {J_SIM},
  author       = {Ke Yang and Enxuan Lin and Wangli Xu and Liping Zhu and Tiejun Tong},
  doi          = {10.1002/sim.70089},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70089},
  shortjournal = {Stat. Med.},
  title        = {An alternative measure for quantifying the heterogeneity in meta-analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-profile based monitoring intervals for multivariate longitudinal biomarker measurements and competing events with applications in stable heart failure. <em>SIM</em>, <em>44</em>(10-12), e70088. (<a href='https://doi.org/10.1002/sim.70088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient monitoring is routinely used to detect disease aggravation in many chronic conditions. We propose an adaptive scheduling strategy based on dynamic individual risk predictions that can improve the efficiency of monitoring programs that incorporate multiple longitudinal measurements and competing events. It is motivated by stable chronic heart failure (CHF) patients who are periodically seen to assess the risk of disease aggravation based on multiple patient characteristics and circulating marker protein levels such as NT-proBNP and troponin. We assess the performance of the adaptive strategy versus fixed schedule alternatives using a simulation study based on the Bio-SHiFT study, a cohort of stable CHF patients.},
  archive      = {J_SIM},
  author       = {Teun B. Petersen and Eric Boersma and Isabella Kardys and Dimitris Rizopoulos},
  doi          = {10.1002/sim.70088},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70088},
  shortjournal = {Stat. Med.},
  title        = {Risk-profile based monitoring intervals for multivariate longitudinal biomarker measurements and competing events with applications in stable heart failure},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to “A comparison of methods to adjust survival curves for confounders”. <em>SIM</em>, <em>44</em>(10-12), e70087. (<a href='https://doi.org/10.1002/sim.70087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.70087},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70087},
  shortjournal = {Stat. Med.},
  title        = {Correction to “A comparison of methods to adjust survival curves for confounders”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved pharmacovigilance signal detection using bayesian generalized linear mixed models. <em>SIM</em>, <em>44</em>(10-12), e70086. (<a href='https://doi.org/10.1002/sim.70086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vaccine safety monitoring is a critical component of public health given the extensive vaccination rate among the general population. However, most signal detection approaches overlook the inherently related biological nature of adverse events (AEs). We hypothesize that integrating AE field knowledge into the statistical process can facilitate and improve the accuracy of identifying vaccine-AE associations. For this purpose, we propose a Bayesian generalized linear multiple low-rank mixed model (GLMLRM) for analyzing high-dimensional post-market drug safety databases. The GLMLRM combines integration of AE ontology in the form of outcome-level groupings, low-rank matrices corresponding to these groupings to approximate the high-dimensional regression coefficient matrix, a factor analysis model to describe the dependence among responses, and a sparse coefficient matrix to capture uncertainty in both the imposed low-rank structures and user-specified groupings. An efficient Metropolis/Gamerman-within-Gibbs sampling procedure is employed to obtain posterior estimates of the regression coefficients and other model parameters, from which testing of outcome-covariate pair associations is based. The proposed approach is evaluated through simulation studies and is further illustrated by an application to the Vaccine Adverse Event Reporting System (VAERS).},
  archive      = {J_SIM},
  author       = {Paloma Hauser and Xianming Tan and Fang Chen and Joseph G. Ibrahim},
  doi          = {10.1002/sim.70086},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70086},
  shortjournal = {Stat. Med.},
  title        = {Improved pharmacovigilance signal detection using bayesian generalized linear mixed models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjusted Nelson–Aalen estimators by inverse treatment probability weighting with an estimated propensity score. <em>SIM</em>, <em>44</em>(10-12), e70085. (<a href='https://doi.org/10.1002/sim.70085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability of treatment weighting (IPW) has been well applied in causal inference to estimate population-level estimands from observational studies. For time-to-event outcomes, the failure time distribution can be estimated by estimating the cumulative hazard in the presence of random right censoring. IPW can be performed by weighting the event counting process and at-risk process by the inverse treatment probability, resulting in an adjusted Nelson–Aalen estimator for the population-level counterfactual cumulative incidence function. We consider the adjusted Nelson–Aalen estimator with an estimated propensity score in the competing risks setting. When the estimated propensity score is regular and asymptotically linear, we derive the influence functions for the counterfactual cumulative hazard and cumulative incidence. Then we establish the asymptotic properties for the estimators. We show that the uncertainty in the estimated propensity score contributes to an additional variation in the estimators. However, through simulation and real-data application, we find that such an additional variation is usually small.},
  archive      = {J_SIM},
  author       = {Yuhao Deng and Rui Wang},
  doi          = {10.1002/sim.70085},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70085},
  shortjournal = {Stat. Med.},
  title        = {Adjusted Nelson–Aalen estimators by inverse treatment probability weighting with an estimated propensity score},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accounting for inconsistent use of covariate adjustment in group sequential trials. <em>SIM</em>, <em>44</em>(10-12), e70082. (<a href='https://doi.org/10.1002/sim.70082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group sequential designs in clinical trials allow for interim efficacy and futility monitoring. Adjustment for baseline covariates can increase power and precision of estimated effects. However, inconsistently applying covariate adjustment throughout the stages of a group sequential trial can result in inflation of type I error, biased point estimates, and anticonservative confidence intervals. We propose methods for performing correct interim monitoring, estimation, and inference in this setting that avoid these issues. We focus on two-arm trials with simple, balanced randomization and continuous outcomes. We study the performance of our boundary, estimation, and inference adjustments in simulation studies. We end with recommendations about the application of covariate adjustment in group sequential designs.},
  archive      = {J_SIM},
  author       = {Marlena S. Bannick and Sonya L. Heltshe and Noah Simon},
  doi          = {10.1002/sim.70082},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70082},
  shortjournal = {Stat. Med.},
  title        = {Accounting for inconsistent use of covariate adjustment in group sequential trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Response to the commentary on “Experimental design and power calculation in omics circadian rhythmicity detection using the cosinor model”. <em>SIM</em>, <em>44</em>(10-12), e70079. (<a href='https://doi.org/10.1002/sim.70079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We agree and highly appreciate the correction of the non-central parameter (ncp) in the power calculation by Dr. Westermark. We acknowledge the mathematical mistake in our derivation. As indicated in the commentary, under the “evenly spaced sampling design” (e.g., sampling every 2 h for 24 h with four biological replicates at each time point; samples), discrepancy of the non-central parameter and the resulting statistical power between the corrected calculation and ours is 0. Below, we conduct further simulations to investigate the degree of statistical power miscalculation in “unevenly-spaced sampling design” in case potential users may have applied our formula in their experimental planning. We consider two types of sampling designs throughout the 24 h of a day: (i) Uniformly distributed (uniform): ; (ii) Unimodal Gaussian distributed (unimodal): with . The following figure shows results for amplitude-to-noise ratio and phase . Note that results of are identical to due to symmetry. As expected, for uniformly or almost uniformly distributed designs (i.e., Uniform, Unimodal-sd6, and Unimodal-sd8), the difference between the corrected and previous power calculations is 0 or close to 0. For moderate to extremely uneven distributed design (Unimodal-sd2 and Unimodal-sd4), the discrepancies become more and more substantial. The previous incorrect npc tends to inflate the estimated statistical power. We have updated the CircaPower R package on the Github ( https://github.com/circaPower/circaPower ), highlighted the correction, and cited the commentary paper.},
  archive      = {J_SIM},
  author       = {RuoFei Yin and George C. Tseng},
  doi          = {10.1002/sim.70079},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70079},
  shortjournal = {Stat. Med.},
  title        = {Response to the commentary on “Experimental design and power calculation in omics circadian rhythmicity detection using the cosinor model”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A personalized predictive model that jointly optimizes discrimination and calibration. <em>SIM</em>, <em>44</em>(10-12), e70077. (<a href='https://doi.org/10.1002/sim.70077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine is accelerating rapidly in the field of health research. This includes fitting predictive models for individual patients based on patient similarity in an attempt to improve model performance. We propose an algorithm which fits a personalized predictive model (PPM) using an optimal size of a similar subpopulation that jointly optimizes model discrimination and calibration, as it is criticized that calibration is not assessed nearly as often as discrimination despite poorly calibrated models being potentially misleading. We define a mixture loss function that considers model discrimination and calibration, and allows for flexibility in emphasizing one performance measure over another. We empirically show that the relationship between the size of subpopulation and calibration is quadratic, which motivates the development of our jointly optimized model. We also investigate the effect of within-population patient weighting on performance and conclude that the size of subpopulation has a larger effect on the predictive performance of the PPM compared to the choice of weight function.},
  archive      = {J_SIM},
  author       = {Tatiana Krikella and Joel A. Dubin},
  doi          = {10.1002/sim.70077},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70077},
  shortjournal = {Stat. Med.},
  title        = {A personalized predictive model that jointly optimizes discrimination and calibration},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic single-index scalar-on-function model. <em>SIM</em>, <em>44</em>(10-12), e70064. (<a href='https://doi.org/10.1002/sim.70064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental exposures often exhibit temporal variability, prompting extensive research to understand their dynamic impacts on human health. There has been a growing interest in studying time-dependent exposure mixtures beyond a single exposure. However, current analytic methods typically assess each exposure individually or assume an additive relationship. This paper aims to fill the gap in method development for evaluating the joint effects of multiple time-dependent exposures on a scalar outcome. We introduce a dynamic single-index scalar-on-function model to characterize the exposure mixture's time-varying effect through a non-parametric bivariate exposure-time-outcome surface function. Utilizing B-spline tensor product bases to approximate the surface function, we propose a profiling algorithm for model estimation and establish large-sample properties for the resulting single-index estimators. In addition, we introduce a non-parametric hypothesis testing procedure to determine whether the surface function varies over time at each fixed mixture level and a model averaging solution to circumvent the issue of knot selection for spline approximations. The performance of our proposed methods is examined through extensive simulations and further illustrated using real-world applications.},
  archive      = {J_SIM},
  author       = {Yiwei Li and Yuyan Wang and Akhgar Ghassabian and Leonardo Trasande and Mengling Liu},
  doi          = {10.1002/sim.70064},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70064},
  shortjournal = {Stat. Med.},
  title        = {Dynamic single-index scalar-on-function model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating multimodal EHR data for mortality prediction in ICU sepsis patients. <em>SIM</em>, <em>44</em>(10-12), e70060. (<a href='https://doi.org/10.1002/sim.70060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid and accurate prediction of mortality risk among intensive care unit (ICU) sepsis patients is crucial for timely intervention and improving patient outcomes. However, due to the multimodal and dynamic time-series nature of patient visit information and the limited data samples, it is challenging to obtain discriminative patient representations, leading to suboptimal mortality prediction results. To address this issue, we design a time-aware graph embedding attention model (TGAM) to integrate multimodal data and predict mortality in ICU sepsis patients. Our approach involves modeling and generating patient representations that encompass not only demographic information but also dynamic time-series data reflecting patient health status. Additionally, the graph convolutional network is used to obtain informative concept embeddings from medical ontologies, and an improved transformer is used to capture the temporal information of the patient's health status and handle missing values, overcoming the limitations of small samples. The experimental results on the MIMIC-III and MIMIC-IV datasets demonstrate that TGAM significantly improves prediction accuracy, with AUROC scores of 87.65% and 87.00% on the MIMIC-III and MIMIC-IV datasets, respectively, outperforming baseline models by over 5 percentage points. TGAM also achieves higher sensitivity, specificity, and AUPRC metrics, and lower Brier Score compared with baseline models, highlighting its effectiveness in identifying high-risk patients. These findings suggest that TGAM has the potential to become a valuable tool for identifying high-risk sepsis patients, enabling clinicians to make more informed and timely intervention decisions.},
  archive      = {J_SIM},
  author       = {Yi Wang and Weihua Li},
  doi          = {10.1002/sim.70060},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70060},
  shortjournal = {Stat. Med.},
  title        = {Integrating multimodal EHR data for mortality prediction in ICU sepsis patients},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accounting for carryover toxicity in phase i clinical trials with intra-patient dose escalation. <em>SIM</em>, <em>44</em>(10-12), e70059. (<a href='https://doi.org/10.1002/sim.70059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intra-patient dose escalation (IPDE) provides a strategy for more efficient Phase I clinical trials. However, IPDE poses additional challenges due to the need to account for carryover toxicity from previous dosings a patient has received. To that end, we propose two CRM–based approaches to IPDE that incorporate potential carryover toxicity. We compare these methods to the CRM without IPDE and the AIDE-CRM, an existing Bayesian adaptive approach to IPDE. In simulations across a range of scenarios, we show that our approaches have similar operating characteristics to the CRM without IPDE, but with a 20% reduction in time and participants needed to complete the trial; these results hold even in the presence of strong carryover toxicity that hinders the performance of the AIDE-CRM.},
  archive      = {J_SIM},
  author       = {Chloe E. Falke and Joseph S. Koopmeiners},
  doi          = {10.1002/sim.70059},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70059},
  shortjournal = {Stat. Med.},
  title        = {Accounting for carryover toxicity in phase i clinical trials with intra-patient dose escalation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian estimation of hierarchical linear models from incomplete data: Cluster-level interaction effects and small sample sizes. <em>SIM</em>, <em>44</em>(10-12), e70051. (<a href='https://doi.org/10.1002/sim.70051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Bayesian estimation of a hierarchical linear model (HLM) from partially observed data, assumed to be missing at random, and small sample sizes. A vector of continuous covariates includes cluster-level partially observed covariates with interaction effects. Due to small sample sizes from 37 patient–physician encounters repeatedly measured at four time points, maximum-likelihood estimation is suboptimal. Existing Gibbs samplers impute missing values of by a Metropolis algorithm using proposal densities that have constant variances while the target posterior distributions have nonconstant variances. Therefore, these samplers may not ensure compatibility with the HLM and, as a result, may not guarantee unbiased estimation of the HLM. We introduce a compatible Gibbs sampler that imputes parameters and missing values directly from the exact posterior distributions. We apply our Gibbs sampler to the longitudinal patient–physician encounter data and compare our estimators with those from existing methods by simulation.},
  archive      = {J_SIM},
  author       = {Dongho Shin and Yongyun Shin and Nao Hagiwara},
  doi          = {10.1002/sim.70051},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70051},
  shortjournal = {Stat. Med.},
  title        = {Bayesian estimation of hierarchical linear models from incomplete data: Cluster-level interaction effects and small sample sizes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate marker evaluation: A tutorial using r. <em>SIM</em>, <em>44</em>(10-12), e70048. (<a href='https://doi.org/10.1002/sim.70048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practice of using a surrogate marker to replace a primary outcome in clinical studies has become widespread. Typically, the primary outcome requires long-term patient follow-up, is expensive, or is invasive or burdensome for patients to measure, while the surrogate marker is not (or less so). Of course, a surrogate marker must be validated before it should be used to make a decision about the effectiveness of a treatment. There has been a tremendous amount of statistical and clinical research focused on evaluating and validating surrogate markers over the past 35 years. Although there is ongoing debate over the optimal evaluation method, the development of new approaches and insights has greatly enriched the field. In this tutorial, we describe available statistical frameworks for evaluating a surrogate marker and specifically focus on the practical implementation of the proportion of treatment effect explained framework. We consider both uncensored and censored outcomes, parametric and non-parametric estimation, evaluating multiple surrogates, heterogeneity in the utility of the surrogate marker, surrogate evaluation from a prediction perspective, and the surrogate paradox. We include R code to implement these procedures with a follow-along R markdown. We close with a discussion on open problems in this research area, particularly in terms of using the surrogate marker to test for treatment in a future study, which is the ultimate goal of surrogate marker evaluation.},
  archive      = {J_SIM},
  author       = {Layla Parast},
  doi          = {10.1002/sim.70048},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70048},
  shortjournal = {Stat. Med.},
  title        = {Surrogate marker evaluation: A tutorial using r},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted repeated measures correlation coefficient: A new correlation coefficient for handling missing data with repeated measures. <em>SIM</em>, <em>44</em>(10-12), e70046. (<a href='https://doi.org/10.1002/sim.70046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between two variables measured multiple times per individual has often been evaluated in clinical studies. These data are not independent; therefore, the Pearson correlation coefficient is inappropriate, and some correlation coefficients for these data have been proposed. However, in the presence of missing data, the existing methods can be biased. In this article, we proposed a weighted repeated measures correlation coefficient that provides an accurate estimate, even with missing data, in a study in which participants ideally have the same number of measurements. We also provided a bootstrap confidence interval for the weighted repeated measures correlation coefficients. We evaluated the performance of the proposed and existing methods (i.e., simple Pearson correlation coefficient, the Pearson correlation coefficient for average, average of the Pearson correlation coefficient, correlation coefficient based on analysis of covariance, and correlation coefficient based on the linear mixed-effects model) through simulations and application to actual data. In numerical evaluations using simulations, the proposed method consistently outperformed existing methods. We recommend using a weighted repeated measures correlation coefficient to handle missing values in multiple-measurement data.},
  archive      = {J_SIM},
  author       = {Masahiro Kondo and Kengo Nagashima and Shiroh Isono and Yasunori Sato},
  doi          = {10.1002/sim.70046},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70046},
  shortjournal = {Stat. Med.},
  title        = {Weighted repeated measures correlation coefficient: A new correlation coefficient for handling missing data with repeated measures},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minding averages: Comment on “Experimental design and power calculation in omics circadian rhythmicity detection using the cosinor model” by zong et al.. <em>SIM</em>, <em>44</em>(10-12), e70044. (<a href='https://doi.org/10.1002/sim.70044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Pål O. Westermark},
  doi          = {10.1002/sim.70044},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10-12},
  pages        = {e70044},
  shortjournal = {Stat. Med.},
  title        = {Minding averages: Comment on “Experimental design and power calculation in omics circadian rhythmicity detection using the cosinor model” by zong et al.},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of rolling surveillance methods in context of prior aberrations: A simulation study with routine data from low- and middle-income countries. <em>SIM</em>, <em>44</em>(8-9), e70075. (<a href='https://doi.org/10.1002/sim.70075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Syndromic surveillance integrated into routine health management information systems could improve timely detection of disease outbreaks, particularly in low- and middle-income countries that have limited diagnostic data. This study evaluates the impact of prior anomalies referred to as “aberrations,” such as historical outbreaks, that can distort “baseline data” on the accuracy of rolling surveillance methods that track ongoing disease trends. We assessed five widely used outbreak detection algorithms—EARS, Farrington, Holt–Winters, and two versions of the Weinberger–Fulcher model (negative binomial (WF NB) and quasipoisson (WF QP))—under simulation scenarios motivated by 5 years of acute respiratory infection data from Liberia. We evaluated seven data-generating mechanisms that cover a wide range of temporal and seasonal patterns. We assessed the accuracy of the outbreak detection algorithms under varied size and timing of outbreaks and aberrations. Accuracy was measured through sensitivity and specificity, with a joint assessment of both metrics using pseudo-ROC curves. Results showed that the introduction of aberrations reduced sensitivity in general, but the algorithms' relative performances were highly context-dependent. EARS and WF models demonstrated high sensitivity for detecting outbreaks when no recent aberrations were present. However, when aberrations occurred within the last year of baseline data, Holt–Winters—unless there was evidence of strong time trends—and WF QP maintained better overall balance between sensitivity and specificity. The Farrington algorithm exhibited strong sensitivity with recent aberrations but at the cost of lower specificity. These findings provide actionable insights and practical recommendations for implementing rolling surveillance in resource-constrained environments, emphasizing the need to consider historical data disturbances and rigorously evaluate sensitivity and specificity jointly.},
  archive      = {J_SIM},
  author       = {Anuraag Gopaluni and Nicholas B. Link and Emma Boley and Isabel Fulcher and Muhammed Semakula and Bethany Hedt-Gauthier},
  doi          = {10.1002/sim.70075},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8-9},
  pages        = {e70075},
  shortjournal = {Stat. Med.},
  title        = {Evaluation of rolling surveillance methods in context of prior aberrations: A simulation study with routine data from low- and middle-income countries},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A variance estimator for marginal cox regression models fit to non-nested multilevel data. <em>SIM</em>, <em>44</em>(8-9), e70074. (<a href='https://doi.org/10.1002/sim.70074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In health services research, researchers often use clustered data to estimate the independent association between individual outcomes and cluster-level covariates after adjusting for individual-level characteristics. Marginal generalized linear models estimated using generalized estimating equation (GEE) methods or hierarchical (or multilevel) regression models can be used when there is a single source of clustering (e.g., patients nested within hospitals). Hierarchical regression models can also be used when there are multiple sources of clustering (e.g., patients nested within surgeons who in turn are nested within hospitals). Methods for estimating marginal regression models are less well-developed when there are multiple sources of non-nested clustering (e.g., patients are clustered both within hospitals and within in neighborhoods, but neither neighborhoods or hospitals are nested in the other). Miglioretti and Heagerty developed a GEE-type variance estimator for use when fitting marginal generalized linear models to non-nested multilevel data. We propose a variance estimator for a marginal Cox regression model fit to non-nested multilevel data that combined their approach with Lin and Wei's robust variance estimator for the Cox model. We evaluated the performance of the proposed variance estimator using an extensive set of Monte Carlo simulations. We illustrated the use of the variance estimator in a case study consisting of patients hospitalized with an acute myocardial infarction who were clustered within hospitals and who were also clustered in neighborhoods. In summary, a variance estimator motivated by that proposed by Miglioretti and Heagerty can be used with marginal Cox regression models fit to non-nested multilevel data.},
  archive      = {J_SIM},
  author       = {Peter C. Austin},
  doi          = {10.1002/sim.70074},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8-9},
  pages        = {e70074},
  shortjournal = {Stat. Med.},
  title        = {A variance estimator for marginal cox regression models fit to non-nested multilevel data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving efficiency and robustness of the prognostic accuracy of biomarkers with partial incomplete failure-time data and auxiliary outcome: Application to prostate cancer active surveillance study. <em>SIM</em>, <em>44</em>(8-9), e70072. (<a href='https://doi.org/10.1002/sim.70072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When novel biomarkers are developed for the clinical management of patients diagnosed with cancer, it is critical to quantify the accuracy of a biomarker-based decision tool. The evaluation can be challenging when the definite outcome T $$ T $$ , such as time to disease progression, is only partially ascertained on a limited set of study patients. Under settings where is only observed on a subset but an auxiliary outcome correlated with is available on all subjects, we propose an augmented estimation procedure for commonly used time-dependent accuracy measures. The augmented estimators are easy to implement without imposing modeling assumptions between the two types of time-to-event outcomes and are more efficient than the complete-case estimator. When the ascertainment of the outcome is non-random and subject to informative censoring, we further augment our proposed method with inverse probability weighting to improve robustness. Results from simulation studies confirm the robustness and efficiency properties of the proposed estimators. The method is illustrated with data from the Canary Prostate Active Surveillance Study.},
  archive      = {J_SIM},
  author       = {Yunro Chung and Tianxi Cai and Lisa Newcomb and Daniel W. Lin and Yingye Zheng},
  doi          = {10.1002/sim.70072},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8-9},
  pages        = {e70072},
  shortjournal = {Stat. Med.},
  title        = {Improving efficiency and robustness of the prognostic accuracy of biomarkers with partial incomplete failure-time data and auxiliary outcome: Application to prostate cancer active surveillance study},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian longitudinal network regression with application to brain connectome genetics. <em>SIM</em>, <em>44</em>(8-9), e70069. (<a href='https://doi.org/10.1002/sim.70069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing availability of large-scale brain imaging genetics studies enables more comprehensive exploration of the genetic underpinnings of brain functional organizations. However, fundamental analytical challenges arise when considering the complex network topology of brain functional connectivity, influenced by genetic contributions and sample relatedness, particularly in longitudinal studies. In this paper, we propose a novel method named Bayesian Longitudinal Network-Variant Regression (BLNR), which models the association between genetic variants and longitudinal brain functional connectivity. BLNR fills the gap in existing longitudinal genome-wide association studies that primarily focus on univariate or multivariate phenotypes. Our approach jointly models the biological architecture of brain functional connectivity and the associated genetic mixed-effect components within a Bayesian framework. By employing plausible prior settings and posterior inference, BLNR enables the identification of significant genetic signals and their associated brain sub-network components, providing robust inference. We demonstrate the superiority of our model through extensive simulations and apply it to the Adolescent Brain Cognitive Development (ABCD) study. This application highlights BLNR's ability to estimate the genetic effects on changes in brain network configurations during neurodevelopment, demonstrating its potential to extend to other similar problems involving sample relatedness and network-variate outcomes.},
  archive      = {J_SIM},
  author       = {Chenxi Li and Xinyuan Tian and Simiao Gao and Selena Wang and Gefei Wang and Yi Zhao and Yize Zhao},
  doi          = {10.1002/sim.70069},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8-9},
  pages        = {e70069},
  shortjournal = {Stat. Med.},
  title        = {Bayesian longitudinal network regression with application to brain connectome genetics},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying less burdensome and more cost-efficient incomplete stepped wedge designs for continuous outcomes collected via repeated cross-sections. <em>SIM</em>, <em>44</em>(8-9), e70067. (<a href='https://doi.org/10.1002/sim.70067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge trials can be costly and burdensome. Recent work has investigated the iterative removal of cluster-period cells from stepped wedge designs, producing a series of candidate incomplete designs that are less burdensome. We propose a novel way to explore the space of incomplete stepped wedge designs, by considering their cost efficiency, seeking to identify designs that retain high power while limiting the total trial cost. We define the cost efficiency of a design as the ratio of the precision of the treatment effect estimator to the total trial cost. Total trial cost incorporates the costs per cluster, costs per participant in intervention and control conditions, and the costs of restarting data collection in a cluster under intervention and control conditions following a pause. We consider linear mixed models for continuous outcomes with a repeated cross-sectional sampling scheme and use an iterative procedure to remove individual cells with the lowest contribution to the cost efficiency metric, producing a series of progressively reduced designs. We define the optimal design within this design space as that which maximizes the cost efficiency relative to the complete design, subject to a minimum acceptable power constraint. We illustrate our methods with an example motivated by a real-world trial. Our methods enable trialists to identify incomplete stepped wedge designs that are less burdensome and more cost-efficient than complete designs. We find that “staircase”-type designs, where clusters only contribute measurements immediately before and after the treatment switch, are often particularly cost-efficient variants of the stepped wedge design.},
  archive      = {J_SIM},
  author       = {Ehsan Rezaei-Darzi and Jessica Kasza and Anisa R. Assifi and Danielle Mazza and Andrew B. Forbes and Kelsey L. Grantham},
  doi          = {10.1002/sim.70067},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8-9},
  pages        = {e70067},
  shortjournal = {Stat. Med.},
  title        = {Identifying less burdensome and more cost-efficient incomplete stepped wedge designs for continuous outcomes collected via repeated cross-sections},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction modeling with many correlated and zero-inflated predictors: Assessing the nonnegative garrote approach. <em>SIM</em>, <em>44</em>(8-9), e70062. (<a href='https://doi.org/10.1002/sim.70062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building prediction models from mass-spectrometry data is challenging due to the abundance of correlated features with varying degrees of zero-inflation, leading to a common interest in reducing the features to a concise predictor set with good predictive performance given the experiments' resource-intensive nature. In this study, we established and examined regularized regression approaches designed to address zero-inflated and correlated predictors. In particular, we describe a novel two-stage regularized regression approach (ridge-garrote) explicitly modeling zero-inflated predictors using two component variables, comprising a ridge estimator in the first stage and subsequently applying a nonnegative garrotte estimator in the second stage. We contrasted ridge-garrote with one-stage methods (ridge, lasso) and other two-stage regularized regression approaches (lasso-ridge, ridge-lasso) for zero-inflated predictors. We assessed the predictive performance and predictor selection properties of these methods in a comparative simulation study and a real-data case study with the aim to predict kidney function using peptidomic features derived from mass-spectrometry. In the simulation study, the predictive performance of all assessed approaches was comparable, yet the ridge-garrote approach consistently selected more parsimonious models compared to its competitors in most scenarios. While lasso-ridge achieved higher predictive accuracy than its competitors, it exhibited high variability in the number of selected predictors. Ridge-lasso exhibited slightly superior predictive accuracy than ridge-garrote but at the expense of selecting more noise predictors. Overall, ridge emerged as a favorable option when variable selection is not a primary concern, while ridge-garrote demonstrated notable practical utility in selecting a parsimonious set of predictors, with only minimal compromise in predictive accuracy.},
  archive      = {J_SIM},
  author       = {Mariella Gregorich and Michael Kammer and Harald Mischak and Georg Heinze},
  doi          = {10.1002/sim.70062},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8-9},
  pages        = {e70062},
  shortjournal = {Stat. Med.},
  title        = {Prediction modeling with many correlated and zero-inflated predictors: Assessing the nonnegative garrote approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A joint model for (Un)Bounded longitudinal markers, competing risks, and recurrent events using patient registry data. <em>SIM</em>, <em>44</em>(8-9), e70057. (<a href='https://doi.org/10.1002/sim.70057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models for longitudinal and survival data have become a popular framework for studying the association between repeatedly measured biomarkers and clinical events. Nevertheless, addressing complex survival data structures, especially handling both recurrent and competing event times within a single model, remains a challenge. This causes important information to be disregarded. Moreover, existing frameworks rely on a Gaussian distribution for continuous markers, which may be unsuitable for bounded biomarkers, resulting in biased estimates of associations. To address these limitations, we propose a Bayesian shared-parameter joint model that simultaneously accommodates multiple (possibly bounded) longitudinal markers, a recurrent event process, and competing risks. We use the beta distribution to model responses bounded within any interval without sacrificing the interpretability of the association. The model offers various forms of association, discontinuous risk intervals, and both gap and calendar timescales. A simulation study shows that it outperforms simpler joint models. We utilize the US Cystic Fibrosis Foundation Patient Registry to study the associations between changes in lung function and body mass index, and the risk of recurrent pulmonary exacerbations, while accounting for the competing risks of death and lung transplantation. Our efficient implementation allows fast fitting of the model despite its complexity and the large sample size from this patient registry. Our comprehensive approach provides new insights into cystic fibrosis disease progression by quantifying the relationship between the most important clinical markers and events more precisely than has been possible before. The model implementation is available in the R package JMbayes2 .},
  archive      = {J_SIM},
  author       = {Pedro Miranda Afonso and Dimitris Rizopoulos and Anushka K. Palipana and Emrah Gecili and Cole Brokamp and John P. Clancy and Rhonda D. Szczesniak and Eleni-Rosalina Andrinopoulou},
  doi          = {10.1002/sim.70057},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8-9},
  pages        = {e70057},
  shortjournal = {Stat. Med.},
  title        = {A joint model for (Un)Bounded longitudinal markers, competing risks, and recurrent events using patient registry data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint latent class models: A tutorial on practical applications in clinical research. <em>SIM</em>, <em>44</em>(8-9), e70047. (<a href='https://doi.org/10.1002/sim.70047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint latent class model is a statistical approach allowing to simultaneously account for two outcomes related to disease progression: A longitudinal measure (for example a biomarker) and time-to-event, in the context of a heterogeneous population. Within this approach, the linear mixed model, describing the longitudinal measure, is connected to the survival model, describing the risk of event occurrence, via a model for latent classes, describing an unobserved population heterogeneity; thus, the behavior of the two outcomes is assumed to be specific to each latent class. The theoretical properties of the model are established and the model is implemented in software. However, its complexity makes it difficult to manipulate by clinicians. In this paper, we propose a detailed tutorial for clinicians and applied statisticians on how to specify the model in R software in order to respond to concrete clinical questions, how to explore, manipulate, interpret the provided results. The tutorial is based on a real clinical dataset; for each clinical question the mathematical model specification and the R script for implementation are provided, and the estimation results and goodness-of-fit measures are detailed and interpreted.},
  archive      = {J_SIM},
  author       = {Maéva Kyheng and Génia Babykina and Alain Duhamel},
  doi          = {10.1002/sim.70047},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8-9},
  pages        = {e70047},
  shortjournal = {Stat. Med.},
  title        = {Joint latent class models: A tutorial on practical applications in clinical research},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extensions of heterogeneity in integration and prediction (HIP) with r shiny application. <em>SIM</em>, <em>44</em>(8-9), e70036. (<a href='https://doi.org/10.1002/sim.70036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple data views measured on the same set of participants are becoming more common and have the potential to deepen our understanding of many complex diseases by analyzing these different views simultaneously. Equally important, many of these complex diseases show evidence of subgroup heterogeneity (e.g., by sex or race). HIP (Heterogeneity in Integration and Prediction) is among the first methods proposed to integrate multiple data views while also accounting for subgroup heterogeneity to identify common and subgroup-specific markers of a particular disease. However, HIP is applicable to continuous outcomes and requires programming expertise by the user. Here we propose extensions to HIP that accommodate multi-class, Poisson, and Zero-Inflated Poisson outcomes while retaining the benefits of HIP. Additionally, we introduce an R Shiny application, accessible on shinyapps.io at https://multi-viewlearn.shinyapps.io/HIP_ShinyApp/ , that provides an interface with the Python implementation of HIP to allow more researchers to use the method anywhere and on any device. We applied HIP to identify genes and proteins common and specific to males and females that are associated with exacerbation frequency. Although some of the identified genes and proteins show evidence of a relationship with chronic obstructive pulmonary disease (COPD) in existing literature, others may be candidates for future research investigating their relationship with COPD. We demonstrate the use of the Shiny application with publicly available data. An R-package for HIP is available at https://github.com/lasandrall/HIP .},
  archive      = {J_SIM},
  author       = {Jessica Butts and Leif Verace and Christine Wendt and Russell Bowler and Craig P. Hersh and Qi Long and Lynn Eberly and Sandra E. Safo},
  doi          = {10.1002/sim.70036},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8-9},
  pages        = {e70036},
  shortjournal = {Stat. Med.},
  title        = {Extensions of heterogeneity in integration and prediction (HIP) with r shiny application},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust score test in G-computation for covariate adjustment in randomized clinical trials leveraging different variance estimators via influence functions. <em>SIM</em>, <em>44</em>(7), e70080. (<a href='https://doi.org/10.1002/sim.70080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {G-computation has become a widely used robust method for estimating unconditional (marginal) treatment effects with covariate adjustment in the analysis of randomized clinical trials. Statistical inference in this context typically relies on the Wald test or Wald interval, which can be easily implemented using a consistent variance estimator. However, existing literature suggests that when sample sizes are small or when parameters of interest are near boundary values, Wald-based methods may be less reliable due to type I error rate inflation and insufficient interval coverage. In this article, we propose a robust score test for g-computation estimators in the context of two-sample treatment comparisons. The proposed test is asymptotically valid under simple and stratified (biased-coin) randomization schemes, even when regression models are misspecified. These test statistics can be conveniently computed using existing variance estimators, and the corresponding confidence intervals have closed-form expressions, making them convenient to implement. Through extensive simulations, we demonstrate the superior finite-sample performance of the proposed method. Finally, we apply the proposed method to reanalyze a completed randomized clinical trial. The new analysis using our proposed score test achieves statistical significance, whilst reducing the issue of type I error inflation.},
  archive      = {J_SIM},
  author       = {Xin Zhang and Haitao Chu and Lin Liu and Satrajit Roychoudhury},
  doi          = {10.1002/sim.70080},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70080},
  shortjournal = {Stat. Med.},
  title        = {A robust score test in G-computation for covariate adjustment in randomized clinical trials leveraging different variance estimators via influence functions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequentist grouped weighted quantile sum regression for correlated chemical mixtures. <em>SIM</em>, <em>44</em>(7), e70078. (<a href='https://doi.org/10.1002/sim.70078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As individuals are exposed to a myriad of potentially harmful pollutants every day, it is important to determine which actors have the greatest influence on health outcomes. However, jointly modeling the associations of multiple pollutant exposures is often hindered by the presence of highly correlated chemicals originating from a common source. A popular approach to analyzing associations between a disease outcome and several highly correlated exposures is Weighted Quantile Sum Regression (WQSR) modeling. WQSR provides increased stability in estimating model parameters but requires data splitting to estimate individual and group effects of chemicals, which reduces the power of the approach. A recent Bayesian implementation of WQSR regression provides a model fitting procedure that avoids data splitting at the cost of high computational expense on large data. In this paper, we introduce a Frequentist Grouped Weighted Quantile Sum Regression (FGWQSR) model that can be fitted efficiently to large datasets without requiring data splitting. FGWQSR produces estimates of the joint effect of mixture groups and of individual chemicals, and likelihood-ratio-based tests that account for FGWQSR's non-standard asymptotics. We demonstrate that FGWQSR is well calibrated for type-I errors while outperforming both Bayesian Grouped Weighted Quantile Sum Regression and Quantile Logistic Regression in terms of statistical power to detect the effects of mixture groups and individual chemicals. In addition, we show that FGWQSR is robust to model misspecification and can be fitted on large datasets in a fraction of the time required for BGWQSR. We apply FGWQSR to a dataset of 317 767 mother-child pairs with exposure profiles generated by chemical transport models to study the associations between several components found in particulate matter with an aerodynamic diameter smaller than 2.5 (PM ) and child Autism Spectrum Disorder (ASD) diagnosis before age 5. PM copper and PM crustal material are found to be statistically significantly associated with ASD diagnosis by five years of age.},
  archive      = {J_SIM},
  author       = {Daniel Rud and Md Mostafijur Rahman and Anny H. Xiang and Rob McConnell and Fred Lurmann and Michael J. Kleeman and Joel Schwartz and Zhanghua Chen and Sandy Eckel and Juan Pablo Lewinger},
  doi          = {10.1002/sim.70078},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70078},
  shortjournal = {Stat. Med.},
  title        = {Frequentist grouped weighted quantile sum regression for correlated chemical mixtures},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instrumental variable methods to target hypothetical estimands with longitudinal repeated measures data: Application to the STEP 1 trial. <em>SIM</em>, <em>44</em>(7), e70076. (<a href='https://doi.org/10.1002/sim.70076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The STEP 1 randomized trial evaluated the effect of taking semaglutide versus placebo on body weight over a 68-week duration. As with any study evaluating an intervention delivered over a sustained period, nonadherence was observed. This was addressed in the original trial analysis within the Estimand Framework by viewing nonadherence as an intercurrent event. The primary analysis applied a treatment policy strategy which viewed it as an aspect of the treatment regimen, and thus made no adjustment for its presence. A supplementary analysis used a hypothetical strategy, targeting an estimand that would have been realized had all participants adhered, under the assumption that no post-baseline variables confounded adherence and change in body weight. In this article, we propose an alternative instrumental variable (IV) method to adjust for nonadherence which does not rely on the same “unconfoundedness” assumption and is less vulnerable to positivity violations (e.g., it can give valid results even under conditions where nonadherence is guaranteed). Unlike many previous IV approaches, it makes full use of the repeatedly measured outcome data, and allows for a time-varying effect of treatment adherence on a participant's weight. We show that it provides a natural vehicle for defining two distinct hypothetical estimands: the treatment effect if all participants would have adhered to semaglutide, and the treatment effect if all participants would have adhered to both semaglutide and placebo. When applied to the STEP 1 study, they suggest a sustained, slowly decaying weight loss effect of semaglutide treatment.},
  archive      = {J_SIM},
  author       = {Jack Bowden and Jesper Madsen and Bryan Goldman and Aske Thorn Iversen and Xiaoran Liang and Stijn Vansteelandt},
  doi          = {10.1002/sim.70076},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70076},
  shortjournal = {Stat. Med.},
  title        = {Instrumental variable methods to target hypothetical estimands with longitudinal repeated measures data: Application to the STEP 1 trial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BayTetra: A bayesian semiparametric approach for testing trajectory differences. <em>SIM</em>, <em>44</em>(7), e70071. (<a href='https://doi.org/10.1002/sim.70071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing differences in longitudinal trajectories among distinct groups of population is an important task in many biomedical applications. Motivated by an application in Alzheimer's disease, we develop BayTetra, an innovative Bayesian semiparametric approach for estimating and testing group differences in multivariate longitudinal trajectories. BayTetra jointly models multivariate longitudinal data by directly accounting for correlations among different responses, and uses a semiparametric framework based on B-splines to capture the non-linear trajectories with great flexibility. To avoid overfitting, BayTetra encourages parsimonious trajectory estimation by imposing penalties on the smoothness of the spline functions. The proposed method converts the challenging task of hypothesis testing for longitudinal trajectories into a more manageable equivalent form based on hypothesis testing for spline coefficients. More importantly, by leveraging posterior inference with natural uncertainty quantification, our Bayesian method offers a more robust and straightforward hypothesis testing procedure than frequentist methods. Extensive simulations demonstrate BayTetra's superior performance over alternatives. Applications to the Biomarkers of Cognitive Decline Among Normal Individuals (BIOCARD) study yield interpretable and valuable clinical insights. A major contribution of this paper is that we have developed an R package BayTetra , which implements the proposed Bayesian semiparametric approach and is the first publicly available software for hypothesis testing on trajectory differences based on a flexible modeling framework.},
  archive      = {J_SIM},
  author       = {Wei Jin and Qiuxin Gao and Yanxun Xu},
  doi          = {10.1002/sim.70071},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70071},
  shortjournal = {Stat. Med.},
  title        = {BayTetra: A bayesian semiparametric approach for testing trajectory differences},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic treatment effect analysis in crossover designs through repeated measures. <em>SIM</em>, <em>44</em>(7), e70070. (<a href='https://doi.org/10.1002/sim.70070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an extended model that harnesses the power of convolution operations to represent time-varying treatment and carry-over effects in a crossover study design. Unlike the traditional model, the proposed approach unifies the treatment and carry-over effects through time-varying response functions, one for each treatment. The model is not only flexible enough to accommodate a variety of treatment plans, including multiple administrations at different doses, but also allows for the inclusion of more treatment periods. The advantages of this approach are accentuated by its ability to be generalized, to avoid prior assumptions about the carry-over effect, and to maintain consistent estimation and hypothesis testing procedures. In this paper, we explore the details of hypothesis testing under this extended model, focusing in particular on the comparison of two response functions within specified intervals. The goal of this work is to improve the modeling of carry-over effects, thereby strengthening the applicability of the model to a variety of experimental settings.},
  archive      = {J_SIM},
  author       = {Jianping Sun and Peiran Guo and Xiaoyang Chen and Xianming Tan},
  doi          = {10.1002/sim.70070},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70070},
  shortjournal = {Stat. Med.},
  title        = {Dynamic treatment effect analysis in crossover designs through repeated measures},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the transitivity assumption in network meta-analysis: A novel approach and its implications. <em>SIM</em>, <em>44</em>(7), e70068. (<a href='https://doi.org/10.1002/sim.70068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The feasibility of network meta-analysis depends on several factors, one of which is the validity of the transitivity assumption that posits no systematic differences in the distribution of effect modifiers across treatment comparisons within a connected network. However, evaluating transitivity is complex for relying on epidemiological grounds. Therefore, establishing a methodological framework to evaluate this assumption is challenging. We propose a novel approach, which involves calculating dissimilarities between treatment comparisons based on study-level aggregate participant and methodological characteristics reported across studies and applying hierarchical clustering to cluster similar comparisons. This approach detects “hot spots” of potential intransitivity in the network, enabling empirical exploration of transitivity and semi-objective judgments. Our approach quantifies clinical and methodological (non-statistical) heterogeneity within and between treatment comparisons by computing the dissimilarities across studies in key characteristics acting as effect modifiers. The investigated networks showed varying between-comparison dissimilarities, indicating variability in the clinical and methodological heterogeneity of the networks. Several pairs of treatment comparisons with “likely concerning” non-statistical heterogeneity were identified, and some studies were organized into several clusters, suggesting potential intransitivity in the networks. These findings necessitate a closer examination of the evidence base, and such scrutiny becomes pivotal in determining whether concerns about the feasibility of network meta-analysis are justified. Similar to statistical heterogeneity, heterogeneity in clinical and methodological characteristics of the collected studies should be expected and appropriately assessed. Our proposed approach facilitates the evaluation of transitivity using well-established methods and can be applied to newly planned and published systematic reviews.},
  archive      = {J_SIM},
  author       = {Loukia M. Spineli and Katerina Papadimitropoulou and Chrysostomos Kalyvas},
  doi          = {10.1002/sim.70068},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70068},
  shortjournal = {Stat. Med.},
  title        = {Exploring the transitivity assumption in network meta-analysis: A novel approach and its implications},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of cancer incidence trends adjusted for changes in screening and detection processes. <em>SIM</em>, <em>44</em>(7), e70063. (<a href='https://doi.org/10.1002/sim.70063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is a major public health issue, and monitoring its incidence is important to suggest and evaluate the impact of preventive interventions. However, estimating trends in cancer incidence is often difficult due to changes in screening or other detection processes over time, which can artificially inflate or deflate the observed incidences. We propose a new method for estimating trends in cancer incidence adjusted for such changes, using a constrained Almon distributed lag model. Unlike other approaches, our method does not rely on any knowledge of cancer progression, or detailed evolution of screening practice over time. It requires the registration of the stages (I–IV) of detected cancers while assuming that the distribution of these stages remains constant in the absence of any change in screening practice. Our method is able to recover the real underlying cancer incidence in simulated data reproducing either no change or a gradual or sudden change in screening practice. For illustration, it is applied to registry data from the canton of Geneva, Switzerland, to estimate breast cancer incidence for the period 1991–2016, where it downwardly corrects the observed incidence when an organized screening program was started.},
  archive      = {J_SIM},
  author       = {Bastien Trächsel and Valentin Rousson and Jean-Luc Bulliard and Isabella Locatelli},
  doi          = {10.1002/sim.70063},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70063},
  shortjournal = {Stat. Med.},
  title        = {Estimation of cancer incidence trends adjusted for changes in screening and detection processes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal multistate models to evaluate treatment delay. <em>SIM</em>, <em>44</em>(7), e70061. (<a href='https://doi.org/10.1002/sim.70061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistate models allow for the study of scenarios where individuals experience different events over time. While effective for descriptive and predictive purposes, multistate models are not typically used for causal inference. We propose an estimator that combines a multistate model with g-computation to estimate the causal effect of treatment delay strategies. In particular, we estimate the impact of strategies such as awaiting natural recovery for 3 months, on the marginal probability of recovery. We use an illness–death model, where illness and death represent, respectively, treatment and recovery. We formulate the causal assumptions needed for identification and the modeling assumptions needed to estimate the quantities of interest. In a simulation study, we present scenarios where the proposed method can make more efficient use of data compared to an alternative approach using cloning–censoring–reweighting. We then showcase the proposed methodology on real data by estimating the effect of treatment delay on a cohort of 1896 couples with unexplained subfertility who seek intrauterine insemination.},
  archive      = {J_SIM},
  author       = {Ilaria Prosepe and Saskia le Cessie and Hein Putter and Nan van Geloven},
  doi          = {10.1002/sim.70061},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70061},
  shortjournal = {Stat. Med.},
  title        = {Causal multistate models to evaluate treatment delay},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel stratified analysis method for testing and estimating overall treatment effects on time-to-event outcomes using average hazard with survival weight. <em>SIM</em>, <em>44</em>(7), e70056. (<a href='https://doi.org/10.1002/sim.70056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the limitations of using the Cox hazard ratio to summarize the magnitude of the treatment effect, alternative measures that do not have these limitations are gaining attention. One of the recently proposed alternative methods uses the average hazard with survival weight (AH). This population quantity can be interpreted as the average intensity of the event occurrence in a given time window that does not involve study-specific censoring. Inference procedures for the ratio of AH and difference in AH have already been proposed in simple randomized controlled trial settings to compare two groups. However, methods with stratification factors have not been well discussed, although stratified analysis is often used in practice to adjust for confounding factors and increase the power to detect a between-group difference. The conventional stratified analysis or meta-analysis approach, which integrates stratum-specific treatment effects using an optimal weight, directly applies to the ratio of AH and difference in AH. However, this conventional approach has significant limitations similar to the Cochran-Mantel-Haenszel method for a binary outcome and the stratified Cox procedure for a time-to-event outcome. To address this, we propose a new stratified analysis method for AH using standardization. With the proposed method, one can summarize the between-group treatment effect in both absolute difference and relative terms, adjusting for stratification factors. This can be a valuable alternative to the traditional stratified Cox procedure to estimate and report the magnitude of the treatment effect on time-to-event outcomes using hazard.},
  archive      = {J_SIM},
  author       = {Zihan Qian and Lu Tian and Miki Horiguchi and Hajime Uno},
  doi          = {10.1002/sim.70056},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70056},
  shortjournal = {Stat. Med.},
  title        = {A novel stratified analysis method for testing and estimating overall treatment effects on time-to-event outcomes using average hazard with survival weight},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring the performance of survival models to personalize treatment choices. <em>SIM</em>, <em>44</em>(7), e70050. (<a href='https://doi.org/10.1002/sim.70050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various statistical and machine learning algorithms can be used to predict treatment effects at the patient level using data from randomized clinical trials (RCTs). Such predictions can facilitate individualized treatment decisions. Recently, a range of methods and metrics were developed for assessing the accuracy of such predictions. Here, we extend these methods, focusing on the case of survival (time-to-event) outcomes. We start by providing alternative definitions of the participant-level treatment benefit; subsequently, we summarize existing and propose new measures for assessing the performance of models estimating participant-level treatment benefits. We explore metrics assessing discrimination and calibration for benefit and decision accuracy. These measures can be used to assess the performance of statistical as well as machine learning models and can be useful during model development (i.e., for model selection or for internal validation) or when testing a model in new settings (i.e., in an external validation). We illustrate methods using simulated data and real data from the OPERAM trial, an RCT in multimorbid older people, which randomized participants to either standard care or a pharmacotherapy optimization intervention. We provide R codes for implementing all models and measures.},
  archive      = {J_SIM},
  author       = {Orestis Efthimiou and Jeroen Hoogland and Thomas P. A. Debray and Valerie Aponte Ribero and Wilma Knol and Huiberdina L. Koek and Matthias Schwenkglenks and Séverine Henrard and Matthias Egger and Nicolas Rodondi and Ian R. White},
  doi          = {10.1002/sim.70050},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70050},
  shortjournal = {Stat. Med.},
  title        = {Measuring the performance of survival models to personalize treatment choices},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized phase I/II dose optimization trial design with multi-categorical and multi-graded outcomes. <em>SIM</em>, <em>44</em>(7), e70049. (<a href='https://doi.org/10.1002/sim.70049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pursuing accurate observations and rational assumptions always drives advances in clinical trial design. In recent years, more trials have begun to collect multi-graded outcomes for more informative analyses. At the same time, assumptions other than the traditional monotonicity relationship have been considered in the dose-efficacy curve to be more realistic. Inspired by these two trends, we propose a phase I/II design that simultaneously considers multi-categorical toxicity and efficacy with multi-graded outcomes, measured as quasi-continuous probability based on prespecified weight matrices of clinical significance. Following keyboard design, our approach aims to screen out overly toxic doses by the toxicity probability intervals and adaptively makes dose escalation or de-escalation decisions by comparing the posterior distributions of dose desirability (utility) among the adjacent levels of the current dose. It helps to more accurately identify the OBD in a non-monotonically increasing dose-efficacy relationship. We also comprehensively present the safety, accuracy and reliability performance through numerical simulations in multiple scenarios and compare the results with several already available designs. The benchmarking results of multiple operating characteristics convincingly support that our design leads in overall performance while ensuring robustness.},
  archive      = {J_SIM},
  author       = {Yichen Yan and Ruitao Lin and Tianyu Guan and Haolun Shi and Xiaolei Lin},
  doi          = {10.1002/sim.70049},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70049},
  shortjournal = {Stat. Med.},
  title        = {A generalized phase I/II dose optimization trial design with multi-categorical and multi-graded outcomes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of covariance in general factorial designs through multiple contrast tests under variance heteroscedasticity. <em>SIM</em>, <em>44</em>(7), e70018. (<a href='https://doi.org/10.1002/sim.70018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common goal in clinical trials is to conduct tests on estimated treatment effects adjusted for covariates such as age or sex. Analysis of Covariance (ANCOVA) is often used in these scenarios to test the global null hypothesis of no treatment effect using an F $$ F $$ -test. However, in several samples, the F $$ F $$ -test does not provide any information about individual null hypotheses and has strict assumptions such as variance homoscedasticity. We extend the method proposed by Konietschke et al. [“Analysis of Covariance Under Variance Heteroscedasticity in General Factorial Designs,” Statistics in Medicine 40 (2021): 4732–4749] to a multiple contrast test procedure (MCTP), which allows us to test arbitrary linear hypotheses and provides information about the global- as well as the individual null hypotheses. Further, we can calculate compatible simultaneous confidence intervals for the individual effects. We derive a small sample size approximation of the distribution of the test statistic via a multivariate t-distribution. As an alternative, we introduce a Wild-bootstrap method. Extensive simulations show that our methods are applicable even when sample sizes are small. Their application is further illustrated within a real data example.},
  archive      = {J_SIM},
  author       = {Matthias Becher and Ludwig A. Hothorn and Frank Konietschke},
  doi          = {10.1002/sim.70018},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {e70018},
  shortjournal = {Stat. Med.},
  title        = {Analysis of covariance in general factorial designs through multiple contrast tests under variance heteroscedasticity},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian generalized linear models for analyzing compositional and sub-compositional microbiome data via EM algorithm. <em>SIM</em>, <em>44</em>(7), e70084. (<a href='https://doi.org/10.1002/sim.70084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of compositional microbiome data is critical for exploring the functional roles of microbial communities in human health and disease. Recent advances have shifted from traditional log-ratio transformations of compositional covariates to zero constraint on the sum of the corresponding coefficients. Various approaches, including penalized regression and Markov Chain Monte Carlo (MCMC) algorithms, have been extended to enforce this sum-to-zero constraint. However, these methods exhibit limitations: penalized regression yields only point estimates, limiting uncertainty assessment, while MCMC methods, although reliable, are computationally intensive, particularly in high-dimensional data settings. To address the challenges posed by existing methods, we proposed Bayesian generalized linear models for analyzing compositional and sub-compositional microbiome data. Our model employs a spike-and-slab double-exponential prior on the microbiome coefficients, inducing weak shrinkage on large coefficients and strong shrinkage on irrelevant ones, making it ideal for high-dimensional microbiome data. The sum-to-zero constraint is handled through soft-centers by applying prior distribution on the sum of compositional or subcompositional coefficients. To alleviate computational intensity, we have developed a fast and stable algorithm incorporating expectation–maximization (EM) steps into the routine iteratively weighted least squares (IWLS) algorithm for fitting GLMs. The performance of the proposed method was assessed by extensive simulation studies. The simulation results show that our approach outperforms existing methods with higher accuracy of coefficient estimates and lower prediction error. We also applied the proposed method to one microbiome study to find microorganisms linked to inflammatory bowel disease (IBD). The methods have been implemented in a freely available R package BhGLM https://github.com/nyiuab/BhGLM .},
  archive      = {J_SIM},
  author       = {Li Zhang and Zhenying Ding and Jinhong Cui and Xiaoxiao Zhou and Nengjun Yi},
  doi          = {10.1002/sim.70084},
  journal      = {Statistics in Medicine},
  number       = {7},
  pages        = {e70084},
  shortjournal = {Stat. Med.},
  title        = {Bayesian generalized linear models for analyzing compositional and sub-compositional microbiome data via EM algorithm},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-index measurement error jump regression model in alzheimer's disease studies. <em>SIM</em>, <em>44</em>(7), e70081. (<a href='https://doi.org/10.1002/sim.70081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer's disease (AD) is the major cause of dementia in the elderly, and investigations on the impact of risk factors on neurocognitive performance are crucial in preventative treatment. While existing statistical regression models, such as single-index models, have proven effective tools for uncovering the relationship between the neurocognitive scores and covariates of interest such as demographic information, clinical variables, and neuroimaging features, limited research has explored scenarios where jump discontinuities exist in the regression patterns and the covariates are unobservable but measured with errors, which are common in real applications. To address these challenges, we propose a single-index measurement error jump regression model (SMEJRM) that can handle both jump discontinuities and measurement errors in image covariates introduced by different image processing software. This development is motivated by data from 168 patients in the Alzheimer's Disease Neuroimaging Initiative. We establish both the estimation procedure and the corresponding asymptotic results. Simulation studies are conducted to evaluate the finite sample performance of our SMEJRM and the estimation procedure. The real application reveals that jump discontinuities do exist in the relationship between neurocognitive scores and some covariates of interest in this study.},
  archive      = {J_SIM},
  author       = {Yan-Yong Zhao and Kaizhou Lei and Yuan Liu and Yuanyao Tan and Noriszura Ismail and Razik Ridzuan Mohd Tajuddin and Rongjie Liu and Chao Huang},
  doi          = {10.1002/sim.70081},
  journal      = {Statistics in Medicine},
  number       = {7},
  pages        = {e70081},
  shortjournal = {Stat. Med.},
  title        = {Single-index measurement error jump regression model in alzheimer's disease studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining straight-line and map-based distances to investigate the connection between proximity to healthy foods and disease. <em>SIM</em>, <em>44</em>(7), e70054. (<a href='https://doi.org/10.1002/sim.70054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthy foods are essential for a healthy life, but accessing healthy food can be more challenging for some people than others. This disparity in food access may lead to disparities in well-being, potentially with disproportionate rates of diseases in communities that face more challenges in accessing healthy food (i.e., low-access communities). Identifying low-access, high-risk communities for targeted interventions is a public health priority, but current methods to quantify food access rely on distance measures that are either computationally simple (like the length of the shortest straight-line route) or accurate (like the length of the shortest map-based driving route), but not both. We propose a multiple imputation approach to combine these distance measures, allowing researchers to harness the computational ease of one with the accuracy of the other. The approach incorporates straight-line distances for all neighborhoods and map-based distances for just a subset, offering comparable estimates to the “gold standard” model using map-based distances for all neighborhoods and improved efficiency over the “complete case” model using map-based distances for just the subset. Through the adoption of a measurement error framework, information from the straight-line distances can be leveraged to compute informative placeholders (i.e., impute) for any neighborhoods without map-based distances. Using simulations and data for the Piedmont Triad region of North Carolina, we quantify and compare the associations between two health outcomes (diabetes and obesity) and neighborhood-level access to healthy foods. The imputation procedure also makes it possible to predict the full landscape of food access in an area without requiring map-based measurements for all neighborhoods.},
  archive      = {J_SIM},
  author       = {Sarah C. Lotspeich and Ashley E. Mullan and Lucy D'Agostino McGowan and Staci A. Hepler},
  doi          = {10.1002/sim.70054},
  journal      = {Statistics in Medicine},
  number       = {7},
  pages        = {e70054},
  shortjournal = {Stat. Med.},
  title        = {Combining straight-line and map-based distances to investigate the connection between proximity to healthy foods and disease},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group sequential test for two-sample ordinal outcome measures. <em>SIM</em>, <em>44</em>(6), e70053. (<a href='https://doi.org/10.1002/sim.70053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group sequential trials include interim monitoring points to potentially reach futility or efficacy decisions early. This approach to trial design can safeguard patients, provide efficacious treatments for patients early, and save money and time. Group sequential methods are well developed for bell-shaped continuous, binary, and time-to-event outcomes. In this paper, we propose a group sequential design using the Mann-Whitney-Wilcoxon test for general two-sample ordinal data. We establish that the proposed test statistic has asymptotic normality and that sequential statistics satisfy the assumptions of Brownian motion. We also include results of finite sample simulation studies that show our proposed approach has the advantage over existing methods for controlling Type I errors while maintaining power for small sample sizes. A real data set is used to illustrate the proposed method and a sample size calculation approach is proposed for designing new studies.},
  archive      = {J_SIM},
  author       = {Yuan Wu and Ryan A. Simmons and Baoshan Zhang and Jesse D. Troy},
  doi          = {10.1002/sim.70053},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70053},
  shortjournal = {Stat. Med.},
  title        = {Group sequential test for two-sample ordinal outcome measures},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive regression trees for group testing data. <em>SIM</em>, <em>44</em>(6), e70052. (<a href='https://doi.org/10.1002/sim.70052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When screening for low-prevalence diseases, pooling specimens (e.g., blood, urine, swabs, etc.) through group testing has the potential to substantially reduce costs when compared to testing specimens individually. A common goal in group testing applications is to estimate the relationship between an individual's true disease status and their individual-level covariate information. However, estimating such a relationship is a non-trivial problem because true individual disease statuses are unknown due to the group testing protocol and the possibility of imperfect testing. While several regression methods have been developed in recent years to accommodate the complexity of group testing data, the functional form of covariate effects is typically assumed to be known. To avoid model misspecification and to provide a more flexible approach, we propose a Bayesian additive regression trees framework to model the individual-level probability of disease with potentially misclassified group testing data. Our methods can be used to analyze data arising from any group testing protocol with the goal of estimating unknown functions of covariates and assay classification accuracy probabilities.},
  archive      = {J_SIM},
  author       = {Madeleine E. St. Ville and Christopher S. McMahan and Joe D. Bible and Joshua M. Tebbs and Christopher R. Bilder},
  doi          = {10.1002/sim.70052},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70052},
  shortjournal = {Stat. Med.},
  title        = {Bayesian additive regression trees for group testing data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive weight selection for time-to-event data under non-proportional hazards. <em>SIM</em>, <em>44</em>(6), e70045. (<a href='https://doi.org/10.1002/sim.70045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When planning a clinical trial for a time-to-event endpoint, we require an estimated effect size and need to consider the type of effect. Usually, an effect of proportional hazards is assumed with the hazard ratio as the corresponding effect measure. Thus, the standard procedure for survival data is generally based on a single-stage log-rank test. Knowing that the assumption of proportional hazards is often violated and sufficient knowledge to derive reasonable effect sizes is usually unavailable, such an approach is relatively rigid. We introduce a more flexible procedure by combining two methods designed to be more robust in case we have little to no prior knowledge. First, we employ a more flexible adaptive multi-stage design instead of a single-stage design. Second, we apply combination-type tests in the first stage of our suggested procedure to benefit from their robustness under uncertainty about the deviation pattern. We can then use the data collected during this period to choose a more specific single-weighted log-rank test for the subsequent stages. In this step, we employ Royston-Parmar spline models to extrapolate the survival curves to make a reasonable decision. Based on a real-world data example, we show that our approach can save a trial that would otherwise end with an inconclusive result. Additionally, our simulation studies demonstrate a sufficient power performance while maintaining more flexibility.},
  archive      = {J_SIM},
  author       = {Moritz Fabian Danzer and Ina Dormuth},
  doi          = {10.1002/sim.70045},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70045},
  shortjournal = {Stat. Med.},
  title        = {Adaptive weight selection for time-to-event data under non-proportional hazards},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new semiparametric power-law regression model with long-term survival, change-point detection and regularization. <em>SIM</em>, <em>44</em>(6), e70043. (<a href='https://doi.org/10.1002/sim.70043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney cancer, a potentially life-threatening malignancy affecting the kidneys, demands early detection and proactive intervention to enhance prognosis and survival. Advancements in medical and health sciences and the emergence of novel treatments are expected to lead to a favorable response in a subset of patients. This, in turn, is anticipated to enhance overall survival and disease-free survival rates. Cure fraction models have become essential for estimating the proportion of individuals considered cured and free from adverse events. This article presents a novel piecewise power-law cure fraction model with a piecewise decreasing hazard function, deviating from the traditional piecewise constant hazard assumption. By analyzing real medical data, we evaluate various factors to explain the survival of individuals. Consistently, positive outcomes are observed, affirming the significant potential of our approach. Furthermore, we use a local influence analysis to detect potentially influential individuals and perform a postdeletion analysis to analyze their impact on our inferences.},
  archive      = {J_SIM},
  author       = {Nixon Jerez-Lillo and Alejandra Tapia and Victor Hugo Lachos and Pedro Luiz Ramos},
  doi          = {10.1002/sim.70043},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70043},
  shortjournal = {Stat. Med.},
  title        = {A new semiparametric power-law regression model with long-term survival, change-point detection and regularization},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential monitoring of covariate-adaptive randomized clinical trials with non-parametric approaches. <em>SIM</em>, <em>44</em>(6), e70042. (<a href='https://doi.org/10.1002/sim.70042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of covariate adjustment in clinical trials has been underscored by the U.S. FDA's guidance. Inference, with or without covariates, after implementing covariate adaptive randomization (CAR), is garnering increased interest. This paper investigates the sequential monitoring of covariate-adaptive randomized clinical trials through non-parametric methods, a critical advancement for enhancing the precision and efficiency of medical research. CAR, which incorporates baseline patient characteristics into the randomization process, aims to mitigate the risk of confounding and improve the balance of covariates across treatment groups, thereby addressing patients' heterogeneity. Although CAR is known for its benefits in reducing biases and enhancing statistical power, its integration into sequentially monitored clinical trials—a standard practice—poses methodological challenges, particularly in controlling the type I error rate. By employing a non-parametric approach, we demonstrate through theoretical proofs and numerical analyses that our methods effectively control the type I error rate and surpass traditional randomization and analysis methods. This paper not only fills a gap in the literature on sequential monitoring of CAR without model misspecification but also proposes practical solutions for enhancing trial design and analysis, thereby contributing significantly to the field of clinical research.},
  archive      = {J_SIM},
  author       = {Xiaotian Chen and Jun Yu and Hongjian Zhu and Li Wang},
  doi          = {10.1002/sim.70042},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70042},
  shortjournal = {Stat. Med.},
  title        = {Sequential monitoring of covariate-adaptive randomized clinical trials with non-parametric approaches},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can the unit size predict outcomes? testing for informativeness in three-level designs. <em>SIM</em>, <em>44</em>(6), e70041. (<a href='https://doi.org/10.1002/sim.70041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilevel data are frequently encountered in biomedical research, and several statistical methods have been developed to analyze such data. Informativeness of the number of units on certain levels often manifests itself in multilevel data analysis and failure to account for this phenomenon will lead to biased inference. Moreover, utilizing an incorrect marginalization approach will also lead to invalid conclusions. To identify the appropriate marginal distribution to be tested in multilevel designs, we propose a sequential testing procedure to test for informativeness of unit sizes in multilevel structures with three levels. At a given level of the design, a bootstrap method is developed to estimate the null distribution of no informativeness of unit size. Simulation studies confirm the efficacy of our sequential procedure in maintaining an overall Type I error rate. Additionally, we extend our testing procedure to a multilevel regression setting, enhancing its practical applicability. We demonstrate the utility of our proposed methods through the analysis of data from a study on periodontal disease and a study on stress levels of preschoolers.},
  archive      = {J_SIM},
  author       = {Samuel Anyaso-Samuel and Somnath Datta and Eva Roos and Jaakko Nevalainen},
  doi          = {10.1002/sim.70041},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70041},
  shortjournal = {Stat. Med.},
  title        = {Can the unit size predict outcomes? testing for informativeness in three-level designs},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pattern mixture sensitivity analyses via multiple imputations for non-ignorable dropout in joint modeling of cognition and risk of dementia. <em>SIM</em>, <em>44</em>(6), e70040. (<a href='https://doi.org/10.1002/sim.70040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the Swedish Betula study, we consider the joint modeling of longitudinal memory assessments and the hazard of dementia. In the Betula data, the time-to-dementia onset or its absence is available for all participants, while some memory measurements are missing. In longitudinal studies of aging, one cannot rule out the possibility of dropout due to health issues resulting in missing not at random longitudinal measurements. We, therefore, propose a pattern-mixture sensitivity analysis for missing not-at-random data in the joint modeling framework. The sensitivity analysis is implemented via multiple imputation as follows: (i) multiply impute missing not at random longitudinal measurements under a set of plausible pattern-mixture imputation models that allow for acceleration of memory decline after dropout, (ii) fit the joint model to each imputed longitudinal memory and time-to-dementia dataset, and (iii) combine the results of step (ii). Our work illustrates that sensitivity analyses via multiple imputations are an accessible, pragmatic method to evaluate the consequences of missing not at-random data on inference and prediction. This flexible approach can accommodate a range of models for the longitudinal and event-time processes. In particular, the pattern-mixture modeling approach provides an accessible way to frame plausible missing not at random assumptions for different missing data patterns. Applying our approach to the Betula study shows that worse memory levels and steeper memory decline were associated with a higher risk of dementia for all considered scenarios.},
  archive      = {J_SIM},
  author       = {Tetiana Gorbach and James R. Carpenter and Chris Frost and Maria Josefsson and Jennifer Nicholas and Lars Nyberg},
  doi          = {10.1002/sim.70040},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70040},
  shortjournal = {Stat. Med.},
  title        = {Pattern mixture sensitivity analyses via multiple imputations for non-ignorable dropout in joint modeling of cognition and risk of dementia},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating additional evidence as prior information to resolve non-identifiability in bayesian disease model calibration: A tutorial. <em>SIM</em>, <em>44</em>(6), e70039. (<a href='https://doi.org/10.1002/sim.70039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease models are used to examine the likely impact of therapies, interventions, and public policy changes. Ensuring that these are well calibrated on the basis of available data and that the uncertainty in their projections is properly quantified is an important part of the process. The question of non-identifiability poses a challenge to disease model calibration where multiple parameter sets generate identical model outputs. For statisticians evaluating the impact of policy interventions such as screening or vaccination, this is a critical issue. This study explores the use of the Bayesian framework to provide a natural way to calibrate models and address non-identifiability in a probabilistic fashion in the context of disease modeling. We present Bayesian approaches for incorporating expert knowledge and external data to ensure that appropriately informative priors are specified on the joint parameter space. These approaches are applied to two common disease models: a basic susceptible-infected-susceptible (SIS) model and a much more complex agent-based model which has previously been used to address public policy questions in HPV and cervical cancer. The conditions that allow the problem of non-identifiability to be resolved are demonstrated for the SIS model. For the larger HPV model, an overview of the findings is presented, but of key importance is a discussion on how the non-identifiability impacts the calibration process. Through case studies, we demonstrate how informative priors can help resolve non-identifiability and improve model inference. We also discuss how sensitivity analysis can be used to assess the impact of prior specifications on model results. Overall, this work provides an important tutorial for researchers interested in applying Bayesian methods to calibrate models and handle non-identifiability in disease models.},
  archive      = {J_SIM},
  author       = {Daria Semochkina and Cathal D. Walsh},
  doi          = {10.1002/sim.70039},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70039},
  shortjournal = {Stat. Med.},
  title        = {Incorporating additional evidence as prior information to resolve non-identifiability in bayesian disease model calibration: A tutorial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal inference in presence of intra-patient correlation due to repeated measurements of exposure and outcome in longitudinal settings. <em>SIM</em>, <em>44</em>(6), e70037. (<a href='https://doi.org/10.1002/sim.70037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Antoine Gavoille and Fabien Rollot and Romain Casey and Sandra Vukusic and Muriel Rabilloud and Fabien Subtil},
  doi          = {10.1002/sim.70037},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70037},
  shortjournal = {Stat. Med.},
  title        = {Causal inference in presence of intra-patient correlation due to repeated measurements of exposure and outcome in longitudinal settings},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spline-based approach to smoothly constrain hazard ratios with a view to apply treatment effect waning. <em>SIM</em>, <em>44</em>(6), e70035. (<a href='https://doi.org/10.1002/sim.70035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Angus C. Jennings and Mark J. Rutherford and Paul C. Lambert},
  doi          = {10.1002/sim.70035},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70035},
  shortjournal = {Stat. Med.},
  title        = {A spline-based approach to smoothly constrain hazard ratios with a view to apply treatment effect waning},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guidelines and best practices for the use of targeted maximum likelihood and machine learning when estimating causal effects of exposures on time-to-event outcomes. <em>SIM</em>, <em>44</em>(6), e70034. (<a href='https://doi.org/10.1002/sim.70034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Targeted maximum likelihood estimation (TMLE) is an increasingly popular framework for the estimation of causal effects. It requires modeling both the exposure and outcome but is doubly robust in the sense that it is valid if at least one of these models is correctly specified. In addition, TMLE allows for flexible modeling of both the exposure and outcome with machine learning methods. This provides better control for measured confounders since the model specification automatically adapts to the data, instead of needing to be specified by the analyst a priori . Despite these methodological advantages, TMLE remains less popular than alternatives in part because of its less accessible theory and implementation. While some tutorials have been proposed, none address the case of a time-to-event outcome. This tutorial provides a detailed step-by-step explanation of the implementation of TMLE for estimating the effect of a point binary or multilevel exposure on a time-to-event outcome, modeled as counterfactual survival curves and causal hazard ratios. The tutorial also provides guidelines on how best to use TMLE in practice, including aspects related to study design, choice of covariates, controlling biases and use of machine learning. R-code is provided to illustrate each step using simulated data ( https://github.com/detal9/SurvTMLE ). To facilitate implementation, a general R function implementing TMLE with options to use machine learning is also provided. The method is illustrated in a real-data analysis concerning the effectiveness of statins for the prevention of a first cardiovascular disease among older adults in Québec, Canada, between 2013 and 2018.},
  archive      = {J_SIM},
  author       = {Denis Talbot and Awa Diop and Miceline Mésidor and Yohann Chiu and Caroline Sirois and Andrew J. Spieker and Antoine Pariente and Pernelle Noize and Marc Simard and Miguel Angel Luque Fernandez and Michael Schomaker and Kenji Fujita and Danijela Gnjidic and Mireille E. Schnitzer},
  doi          = {10.1002/sim.70034},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70034},
  shortjournal = {Stat. Med.},
  title        = {Guidelines and best practices for the use of targeted maximum likelihood and machine learning when estimating causal effects of exposures on time-to-event outcomes},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing coarsened and missing data by imputation methods. <em>SIM</em>, <em>44</em>(6), e70032. (<a href='https://doi.org/10.1002/sim.70032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various missing data problems, values are not entirely missing, but are coarsened. For coarsened observations, instead of observing the true value, a subset of values - strictly smaller than the full sample space of the variable - is observed to which the true value belongs. In our motivating example for patients with endometrial carcinoma, the degree of lymphovascular space invasion (LVSI) can be either absent, focally present, or substantially present. For a subset of individuals, however, LVSI is reported as being present, which includes both non-absent options. In the analysis of such a dataset, difficulties arise when coarsened observations are to be used in an imputation procedure. To our knowledge, no clear-cut method has been described in the literature on how to handle an observed subset of values, and treating them as entirely missing could lead to biased estimates. Therefore, in this paper, we evaluated the best strategy to deal with coarsened and missing data in multiple imputation. We tested a number of plausible ad hoc approaches, possibly already in use by statisticians. Additionally, we propose a principled approach to this problem, consisting of an adaptation of the SMC-FCS algorithm (SMC-FCS : Coarsening compatible), that ensures that imputed values adhere to the coarsening information. These methods were compared in a simulation study. This comparison shows that methods that prevent imputations of incompatible values, like the SMC-FCS method, perform consistently better in terms of a lower bias and RMSE, and achieve better coverage than methods that ignore coarsening or handle it in a more naïve way. The analysis of the motivating example shows that the way the coarsening information is handled can matter substantially, leading to different conclusions across methods. Overall, our proposed SMC-FCS method outperforms other methods in handling coarsened data, requires limited additional computation cost and is easily extendable to other scenarios.},
  archive      = {J_SIM},
  author       = {Lars L. J. van der Burg and Stefan Böhringer and Jonathan W. Bartlett and Tjalling Bosse and Nanda Horeweg and Liesbeth C. de Wreede and Hein Putter},
  doi          = {10.1002/sim.70032},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70032},
  shortjournal = {Stat. Med.},
  title        = {Analyzing coarsened and missing data by imputation methods},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preservation of type i error for partially-unblinded sample size re-estimation. <em>SIM</em>, <em>44</em>(6), e70030. (<a href='https://doi.org/10.1002/sim.70030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample size re-estimation (SSR) at an interim analysis allows for adjustments based on accrued data. Existing strategies rely on either blinded or unblinded methods to inform such adjustments and, ideally, perform these adjustments in a way that preserves Type I error at the nominal level. Here, we propose an approach that uses partially-unblinded methods for SSR for both binary and continuous endpoints. Although this approach has operational unblinding, its partial use of the unblinded information for SSR does not include the interim effect size, hence the term ‘partially-unblinded.’ Through proof-of-concept and simulation studies, we demonstrate that these adjustments can be made without compromising the Type I error rate. We also investigate different mathematical expressions for SSR under different variance scenarios: homogeneity, heterogeneity, and a combination of both. Of particular interest is the third form of dual variance, for which we provide additional clarifications for binary outcomes and derive an analogous form for continuous outcomes. We show that the corresponding mathematical expressions for the dual variance method are a compromise between those for variance homogeneity and heterogeneity, resulting in sample size estimates that are bounded between those produced by the other expressions, and extend their applicability to adaptive trial design.},
  archive      = {J_SIM},
  author       = {Ann Marie K. Weideman and Kevin J. Anstrom and Gary G. Koch and Xianming Tan},
  doi          = {10.1002/sim.70030},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70030},
  shortjournal = {Stat. Med.},
  title        = {Preservation of type i error for partially-unblinded sample size re-estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian calibration of stochastic agent based model via random forest. <em>SIM</em>, <em>44</em>(6), e70029. (<a href='https://doi.org/10.1002/sim.70029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent-based models (ABM) provide an excellent framework for modeling outbreaks and interventions in epidemiology by explicitly accounting for diverse individual interactions and environments. However, these models are usually stochastic and highly parametrized, requiring precise calibration for predictive performance. When considering realistic numbers of agents and properly accounting for stochasticity, this high-dimensional calibration can be computationally prohibitive. This paper presents a random forest-based surrogate modeling technique to accelerate the evaluation of ABMs and demonstrates its use to calibrate an epidemiological ABM named CityCOVID via Markov chain Monte Carlo (MCMC). The technique is first outlined in the context of CityCOVID's quantities of interest, namely hospitalizations and deaths, by exploring dimensionality reduction via temporal decomposition with principal component analysis (PCA) and via sensitivity analysis. The calibration problem is then presented, and samples are generated to best match COVID-19 hospitalization and death numbers in Chicago from March to June in 2020. These results are compared with previous approximate Bayesian calibration (IMABC) results, and their predictive performance is analyzed, showing improved performance with a reduction in computation.},
  archive      = {J_SIM},
  author       = {Connor Robertson and Cosmin Safta and Nicholson Collier and Jonathan Ozik and Jaideep Ray},
  doi          = {10.1002/sim.70029},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70029},
  shortjournal = {Stat. Med.},
  title        = {Bayesian calibration of stochastic agent based model via random forest},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric modeling of biomarker trajectory and variability with correlated measurement errors. <em>SIM</em>, <em>44</em>(6), e70028. (<a href='https://doi.org/10.1002/sim.70028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prognostic significance of biomarker variability in predicting associated disease risk is well-established. However, prevailing methods that assess the relationship between biomarker variability and time to event often overlook within-subject correlation in longitudinal measurement errors, resulting in biased parameter estimates and erroneous statistical inference. Additionally, these methods typically assume that biomarker trajectory can be represented as a linear combination of spline basis functions with normally distributed random effects. This not only leads to significant computational demands due to the necessity of high-dimensional integration over the random effects but also limits the applicability because of the normality restriction imposed on the random effects. This paper addresses these limitations by incorporating correlated longitudinal measurement errors and proposing a novel semiparametric multiplicative random effects model. This model does not assume normality for the random effects and eliminates the need for integration with respect to them. The biomarker variability is incorporated as a covariate within a Cox model for time-to-event data, thus facilitating a joint modeling strategy. We demonstrate the asymptotic properties of the proposed estimators and validate their performance through simulation studies. The methodology is applied to assess the impact of systolic blood pressure variability on cardiovascular mortality using data from the Atherosclerosis Risk in Communities study.},
  archive      = {J_SIM},
  author       = {Renwen Luo and Chuoxin Ma and Jianxin Pan},
  doi          = {10.1002/sim.70028},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70028},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric modeling of biomarker trajectory and variability with correlated measurement errors},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable selection for progressive multistate processes under intermittent observation. <em>SIM</em>, <em>44</em>(6), e70023. (<a href='https://doi.org/10.1002/sim.70023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistate models offer a natural framework for studying many chronic disease processes. Interest often lies in identifying which among a large list of candidate variables play a role in the progression of such processes. We consider the problem of variable selection for progressive multistate processes under intermittent observation based on penalized log-likelihood. An Expectation-Maximization (EM) algorithm is developed such that the maximization step can exploit existing software for penalized Poisson regression thereby allowing for the use of common penalty functions. Simulation studies show good performance in identifying important markers with different penalty functions. In a motivating application involving a cohort of patients with psoriatic arthritis, we identify which, among a large group of candidate HLA markers, are associated with rapid disease progression.},
  archive      = {J_SIM},
  author       = {Xianwei Li and Richard J. Cook and Liqun Diao},
  doi          = {10.1002/sim.70023},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70023},
  shortjournal = {Stat. Med.},
  title        = {Variable selection for progressive multistate processes under intermittent observation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active-controlled trial design for HIV prevention trials with a counterfactual placebo. <em>SIM</em>, <em>44</em>(6), e70022. (<a href='https://doi.org/10.1002/sim.70022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the quest for enhanced HIV prevention methods, the advent of antiretroviral drugs as pre-exposure prophylaxis (PrEP) has marked a significant stride forward. However, the ethical challenges in conducting placebo-controlled trials for new PrEP agents against a backdrop of highly effective existing PrEP options necessitate innovative approaches. This manuscript delves into the design and implementation of active-controlled trials that incorporate a counterfactual placebo estimate—a theoretical estimate of what HIV incidence would have been without effective prevention. We introduce a novel statistical framework for regulatory approval of new PrEP agents, predicated on the assumption of an available and consistent counterfactual placebo estimate. Our approach aims to assess the absolute efficacy (i.e., against placebo) of the new PrEP agent relative to the absolute efficacy of the active control. We propose a two-step procedure for hypothesis testing and further develop an approach that addresses potential biases inherent in non-randomized comparisons to counterfactual placebos. By exploring different scenarios with moderately and highly effective active controls and counterfactual placebo estimates from various sources, we demonstrate how our design can significantly reduce sample sizes compared to traditional non-inferiority trials and offer a robust framework for evaluating new PrEP agents. This work contributes to the methodological repertoire for HIV prevention trials and underscores the importance of adaptability in the face of ethical and practical challenges.},
  archive      = {J_SIM},
  author       = {Fei Gao and Holly Janes and Susan Buchbinder and Deborah Donnell},
  doi          = {10.1002/sim.70022},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70022},
  shortjournal = {Stat. Med.},
  title        = {Active-controlled trial design for HIV prevention trials with a counterfactual placebo},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and estimation of causal effects using non-concurrent controls in platform trials. <em>SIM</em>, <em>44</em>(6), e70017. (<a href='https://doi.org/10.1002/sim.70017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform trials are multi-arm designs that simultaneously evaluate multiple treatments for a single disease within the same overall trial structure. Unlike traditional randomized controlled trials, they allow treatment arms to enter and exit the trial at distinct times while maintaining a control arm throughout. This control arm comprises both concurrent controls, where participants are randomized concurrently to either the treatment or control arm, and non-concurrent controls, who enter the trial when the treatment arm under study is unavailable. While flexible, platform trials introduce the challenge of using non-concurrent controls, raising questions about estimating treatment effects. Specifically, which estimands should be targeted? Under what assumptions can these estimands be identified and estimated? Are there any efficiency gains? In this article, we discuss issues related to the identification and estimation assumptions of common choices of estimand. We conclude that the most robust strategy to increase efficiency without imposing unwarranted assumptions is to target the concurrent average treatment effect (cATE), the ATE among only concurrent units, using a covariate-adjusted doubly robust estimator. Our studies suggest that, for the purpose of obtaining efficiency gains, collecting important prognostic variables is more important than relying on non-concurrent controls. We also discuss the perils of targeting ATE due to an untestable extrapolation assumption that will often be invalid. We provide simulations illustrating our points and an application to the ACTT platform trial, resulting in a 20% improvement in precision compared to the naive estimator that ignores non-concurrent controls and prognostic variables.},
  archive      = {J_SIM},
  author       = {Michele Santacatterina and Federico Macchiavelli Giron and Xinyi Zhang and Iván Díaz},
  doi          = {10.1002/sim.70017},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70017},
  shortjournal = {Stat. Med.},
  title        = {Identification and estimation of causal effects using non-concurrent controls in platform trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A seamless design for the combination of a Case–Control and a cohort diagnostic accuracy study. <em>SIM</em>, <em>44</em>(6), e70016. (<a href='https://doi.org/10.1002/sim.70016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In determining the accuracy of a new diagnostic test, often two steps are performed. In the first step, a case–control study is performed as an efficient but potentially biased design. In a second step, a population-based cohort study is performed as an unbiased but less efficient design. In order to accelerate diagnostic research, it has recently been suggested to combine the two designs in one seamless design. In this article, we present a more in-depth description of this idea. The seamless diagnostic accuracy study design is formally introduced by comparison with the traditional pathway, and the basic design decisions are discussed: A stopping rule and a stopping time. An appealing feature of the design is the possibility to ignore the seamless design in the final analysis, although part of the data is used already in an interim analysis. The justification for this strategy is provided by a large-scale simulation study. The simulation study suggests also that the risk of a loss of power due to using a seamless design can be limited by a reasonable choice of the futility boundaries, defining the stopping rule. We conclude that the seamless diagnostic accuracy study design seems to be ready to use. It promises to accelerate diagnostic research, in particular if population-based cohort studies can be started without great efforts and if the reference standard can be evaluated with little delay.},
  archive      = {J_SIM},
  author       = {Eric Bibiza-Freiwald and Werner Vach and Antonia Zapf},
  doi          = {10.1002/sim.70016},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70016},
  shortjournal = {Stat. Med.},
  title        = {A seamless design for the combination of a Case–Control and a cohort diagnostic accuracy study},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for a two-stage adaptive seamless design using different binary endpoints. <em>SIM</em>, <em>44</em>(6), e70003. (<a href='https://doi.org/10.1002/sim.70003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive seamless design, which integrates phases II and III into a single trial comprising two stages, is garnering increasing interest in the efficient drug development. The first stage involves selecting promising treatment group(s), followed by comparing the efficacy between the selected and control groups in the second stage. This study focused on a two-stage adaptive seamless design where treatment selection is based on a short-term binary endpoint, while the comparison is based on a long-term binary endpoint. Recently, exact and mid- p $$ p $$ tests were proposed in this setting. However, treatment effects at the second stage were estimated using the conventional maximum likelihood estimator (MLE), leading to upward bias owing to treatment selection. We propose the conditional mean-adjusted estimator (CMAE) and uniformly minimum variance conditional unbiased estimator (UMVCUE) to address the bias in this setting. Additionally, confidence intervals for exact and mid- tests were constructed using the Clopper-Pearson method. Simulation studies were performed to compare the six inference methods defined by combinations of the three estimators and two statistical tests. The simulation results showed that MLE of the treatment effect at the second stage exhibited a notable bias, while CMAE and UMVCUE substantially reduced the bias. The exact test was conservative in terms of the type-I error rate of the comparison at the second stage, while the mid- test yielded results close to the nominal level. In conclusion, we recommend statistical inferences based on the CMAE + mid- test or UMVCUE + mid- test in our setting.},
  archive      = {J_SIM},
  author       = {Ryota Ishii and Kenichi Takahashi and Kazushi Maruo and Masahiko Gosho},
  doi          = {10.1002/sim.70003},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70003},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for a two-stage adaptive seamless design using different binary endpoints},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble of sequential learning models with distributed data centers and its applications. <em>SIM</em>, <em>44</em>(6), e70002. (<a href='https://doi.org/10.1002/sim.70002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling massive datasets poses a significant challenge in modern data analysis, particularly within epidemiology and medicine. In this study, we introduce a novel approach using sequential ensemble learning to effectively analyze extensive datasets. Our method prioritizes efficiency from both statistical and computational perspectives, addressing challenges such as data communication and privacy, as discussed in federated learning literature. To demonstrate the efficacy of our approach, we present compelling real-world examples using COVID-19 data alongside simulation studies.},
  archive      = {J_SIM},
  author       = {Zhanfeng Wang and Jingyu Huang and Yuan-chin Ivan Chang},
  doi          = {10.1002/sim.70002},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e70002},
  shortjournal = {Stat. Med.},
  title        = {Ensemble of sequential learning models with distributed data centers and its applications},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and hypothesis testing of strain-specific vaccine efficacy with missing strain types with application to a COVID-19 vaccine trial. <em>SIM</em>, <em>44</em>(6), e10345. (<a href='https://doi.org/10.1002/sim.10345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on data from a randomized, controlled vaccine efficacy trial, this article develops statistical methods for assessing vaccine efficacy (VE) to prevent COVID-19 infections by a discrete set of genetic strains of SARS-CoV-2. Strain-specific VE adjusting for possibly time-varying covariates is estimated using augmented inverse probability weighting to address missing viral genotypes under a competing risks model that allows separate baseline hazards for different risk groups. Hypothesis tests are developed to assess whether the vaccine provides at least a specified level of VE against some viral genotypes and whether VE varies across genotypes. Asymptotic properties providing analytic inferences are derived and finite-sample properties of the estimators and hypothesis tests are studied through simulations. This research is motivated by the fact that previous analyses of COVID-19 vaccine efficacy did not account for missing genotypes, which can cause severe bias and efficiency loss. The theoretical properties and simulations demonstrate superior performance of the new methods. Application to the Moderna COVE trial identifies several SARS-CoV-2 genotype features with differential vaccine efficacy across genotypes, including lineage (Reference, Epsilon, Gamma, Zeta), indicators of residue match vs. mismatch to the vaccine-strain residue at Spike amino acid positions (identifying signatures of differential VE), and a weighted Hamming distance to the vaccine strain. The results show VE decreases against genotypes more distant from the vaccine strain, highlighting the need to update COVID-19 vaccine strains.},
  archive      = {J_SIM},
  author       = {Fei Heng and Yanqing Sun and Li Li and Peter B. Gilbert},
  doi          = {10.1002/sim.10345},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e10345},
  shortjournal = {Stat. Med.},
  title        = {Estimation and hypothesis testing of strain-specific vaccine efficacy with missing strain types with application to a COVID-19 vaccine trial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling volume-outcome relationships in health care. <em>SIM</em>, <em>44</em>(6), e10339. (<a href='https://doi.org/10.1002/sim.10339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the ongoing strong interest in associations between quality of care and the volume of health care providers, a unified statistical framework for analyzing them is missing, and many studies suffer from poor statistical modelling choices. We propose a flexible, additive mixed model for studying volume-outcome associations in health care that takes into account individual patient characteristics as well as provider-specific effects through a hierarchical approach. More specifically, we treat volume as a continuous variable, and its effect on the considered outcome is modeled as a smooth function. We take account of different case-mixes by including patient-specific risk factors and clustering on the provider level through random intercepts. This strategy enables us to extract a smooth volume effect as well as volume-independent provider effects. These two quantities can be compared directly in terms of their magnitude, which gives insight into the sources of variability of quality of care. Based on a causal DAG, we derive conditions under which the volume-effect can be interpreted as a causal effect. The paper provides confidence sets for each of the estimated quantities relying on joint estimation of all effects and parameters. Our approach is illustrated through simulation studies and an application to German health care data about mortality of very low birth weight infants.},
  archive      = {J_SIM},
  author       = {Maurilio Gutzeit and Johannes Rauh and Maximilian Kähler and Jona Cederbaum},
  doi          = {10.1002/sim.10339},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e10339},
  shortjournal = {Stat. Med.},
  title        = {Modelling volume-outcome relationships in health care},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient computation of high-dimensional penalized piecewise constant hazard random effects models. <em>SIM</em>, <em>44</em>(6), e10311. (<a href='https://doi.org/10.1002/sim.10311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying and characterizing relationships between treatments, exposures, or other covariates and time-to-event outcomes has great significance in a wide range of biomedical settings. In research areas such as multi-center clinical trials, recurrent events, and genetic studies, proportional hazard mixed effects models (PHMMs) are used to account for correlations observed in clusters within the data. In high dimensions, proper specification of the fixed and random effects within PHMMs is difficult and computationally complex. In this paper, we approximate the proportional hazards mixed effects model with a piecewise constant hazard mixed effects survival model. We estimate the model parameters using a modified Monte Carlo expectation conditional minimization (MCECM) algorithm, allowing us to perform variable selection on both the fixed and random effects simultaneously. We also incorporate a factor model decomposition of the random effects in order to more easily scale the variable selection method to larger dimensions. We demonstrate the utility of our method using simulations, and we apply our method to a multi-study pancreatic ductal adenocarcinoma gene expression dataset to select features important for survival.},
  archive      = {J_SIM},
  author       = {Hillary M. Heiling and Naim U. Rashid and Quefeng Li and Xianlu L. Peng and Jen Jen Yeh and Joseph G. Ibrahim},
  doi          = {10.1002/sim.10311},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e10311},
  shortjournal = {Stat. Med.},
  title        = {Efficient computation of high-dimensional penalized piecewise constant hazard random effects models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overcoming model uncertainty — How equivalence tests can benefit from model averaging. <em>SIM</em>, <em>44</em>(6), e10309. (<a href='https://doi.org/10.1002/sim.10309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, for example, based on gender, age, or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. Classical approaches are based on testing the equivalence of single quantities, for example, the mean, the area under the curve or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them. In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures. In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth Bayesian information criterion weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and illustrate its practical relevance considering a time-response case study with toxicological gene expression data.},
  archive      = {J_SIM},
  author       = {Niklas Hagemann and Kathrin Möllenhoff},
  doi          = {10.1002/sim.10309},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {e10309},
  shortjournal = {Stat. Med.},
  title        = {Overcoming model uncertainty — How equivalence tests can benefit from model averaging},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating mean viral load trajectory from intermittent longitudinal data and unknown time origins. <em>SIM</em>, <em>44</em>(5), e70033. (<a href='https://doi.org/10.1002/sim.70033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viral load (VL) in the respiratory tract is the leading proxy for assessing infectiousness potential. Understanding the dynamics of disease-related VL within the host is of great importance, as it helps to determine different policies and health recommendations. However, normally the VL is measured on individuals only once, in order to confirm infection, and furthermore, the infection date is unknown. It is therefore necessary to develop statistical approaches to estimate the typical VL trajectory. We show here that, under plausible parametric assumptions, two measures of VL on infected individuals can be used to accurately estimate the VL mean function. Specifically, we consider a discrete-time likelihood-based approach to modeling and estimating partial observed longitudinal samples. We study a multivariate normal model for a function of the VL that accounts for possible correlation between measurements within individuals. We derive an expectation-maximization (EM) algorithm which treats the unknown time origins and the missing measurements as latent variables. Our main motivation is the reconstruction of the daily mean VL, given measurements on patients whose VLs were measured multiple times on different days. Such data should and can be obtained at the beginning of a pandemic with the specific goal of estimating the VL dynamics. For demonstration purposes, the method is applied to SARS-Cov-2 cycle-threshold-value data collected in Israel.},
  archive      = {J_SIM},
  author       = {Yonatan Woodbridge and Micha Mandel and Yair Goldberg and Amit Huppert},
  doi          = {10.1002/sim.70033},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70033},
  shortjournal = {Stat. Med.},
  title        = {Estimating mean viral load trajectory from intermittent longitudinal data and unknown time origins},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling the restricted mean survival time using pseudo-value random forests. <em>SIM</em>, <em>44</em>(5), e70031. (<a href='https://doi.org/10.1002/sim.70031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted mean survival time (RMST) has become a popular measure to summarize event times in longitudinal studies. Defined as the area under the survival function up to a time horizon τ > 0 $$ \tau >0 $$ , the RMST can be interpreted as the life expectancy within the time interval [ 0 , τ ] $$ \left[0,\tau \right] $$ . In addition to its straightforward interpretation, the RMST allows for the definition of valid estimands for the causal analysis of treatment contrasts in medical studies. In this work, we introduce a non-parametric approach to model the RMST conditional on a set of baseline variables (including, e.g., treatment variables and confounders). Our method is based on a direct modeling strategy for the RMST, using leave-one-out jackknife pseudo-values within a random forest regression framework. In this way, it can be employed to obtain precise estimates of both patient-specific RMST values and confounder-adjusted treatment contrasts. Since our method (termed “pseudo-value random forest”, PVRF) is model-free, RMST estimates are not affected by restrictive assumptions like the proportional hazards assumption. Particularly, PVRF offers a high flexibility in detecting relevant covariate effects from higher-dimensional data, thereby expanding the range of existing pseudo-value modeling techniques for RMST estimation. We investigate the properties of our method using simulations and illustrate its use by an application to data from the SUCCESS-A breast cancer trial. Our numerical experiments demonstrate that PVRF yields accurate estimates of both patient-specific RMST values and RMST-based treatment contrasts.},
  archive      = {J_SIM},
  author       = {Alina Schenk and Vanessa Basten and Matthias Schmid},
  doi          = {10.1002/sim.70031},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70031},
  shortjournal = {Stat. Med.},
  title        = {Modeling the restricted mean survival time using pseudo-value random forests},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network meta-analysis with individual participant-level data of time-to-event outcomes using cox regression. <em>SIM</em>, <em>44</em>(5), e70027. (<a href='https://doi.org/10.1002/sim.70027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accessibility of individual participant-level data (IPD) enhances the evaluation of moderation effects of patient covariates. It facilitates the provision of accurate estimation of intervention effects and confidence intervals by incorporating covariate correlations across multiple clinical trials. With a time-to-event outcome, Cox regression can be applied for network meta-analysis (NMA) using IPD. However, there lacks comprehensive reviews and comparisons of the specifications and assumptions of these Cox models and their impact on the interpretation of hazard ratios, effect moderation, and trial heterogeneity in IPD-NMA. In this paper, we examine various Cox models for IPD-NMA and compare different approaches to modeling trial, treatment, and covariate effects. We employ multiple graphical tools and statistical tests to assess proportional hazard assumptions and discuss their implications. Additionally, we explore the application of extended Cox models when the proportional hazard assumption is violated. Practical guidance on interpreting and reporting NMA results is provided. A simulation study is conducted to compare the performance of different models. We illustrate the methods to conduct IPD-NMA through a real data example.},
  archive      = {J_SIM},
  author       = {Kaiyuan Hua and Daniel Wojdyla and Anthony Carnicelli and Christopher Granger and Xiaofei Wang and Hwanhee Hong},
  doi          = {10.1002/sim.70027},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70027},
  shortjournal = {Stat. Med.},
  title        = {Network meta-analysis with individual participant-level data of time-to-event outcomes using cox regression},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to assess the predictiveness of a continuous biomarker in early phases of drug development. <em>SIM</em>, <em>44</em>(5), e70026. (<a href='https://doi.org/10.1002/sim.70026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying and quantifying predictive biomarkers is a critical issue of personalized medicine approaches and patient-centric clinical development strategies. In early stages of the development process, significant challenges and numerous uncertainties arise. One of the challenges is the ability to assess the predictive value of a biomarker, i.e., the difference in primary outcomes between experimental and placebo arms above and below a certain threshold of the biomarker. Indeed, when the accumulated information is very limited and the sample size is small, preliminary conclusions about the predictive properties of the biomarker might be misleading. To date, the majority of investigations regarding the predictiveness of biomarkers were in the setting of moderate-to-large sample sizes. In this work, we propose a novel flexible approach inspired by the Kolmogorov-Smirnov Distance in order to assess the predictiveness of a continuous biomarker in a clinical setting where the sample size is small. Via simulations we show that the proposed method allows to achieve a higher power to declare predictiveness compared to the existing methods under a range of scenarios, whilst still maintaining a control of the type I error at a pre-specified level.},
  archive      = {J_SIM},
  author       = {Alessandra Serra and Julia Geronimi and Sandrine Guilleminot and Hugo Hadjur and Marie-Karelle Riviere and Gaëlle Saint-Hilary and Pavel Mozgunov},
  doi          = {10.1002/sim.70026},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70026},
  shortjournal = {Stat. Med.},
  title        = {A novel approach to assess the predictiveness of a continuous biomarker in early phases of drug development},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A double machine learning approach for the evaluation of COVID-19 vaccine effectiveness under the test-negative design: Analysis of québec administrative data. <em>SIM</em>, <em>44</em>(5), e70025. (<a href='https://doi.org/10.1002/sim.70025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The test-negative design (TND), which is routinely used for monitoring seasonal flu vaccine effectiveness (VE), has recently become integral to COVID-19 vaccine surveillance, notably in Québec, Canada. Some studies have addressed the identifiability and estimation of causal parameters under the TND, but efficiency bounds for nonparametric estimators of the target parameter under the unconfoundedness assumption have not yet been investigated. Motivated by the goal of improving adjustment for measured confounders when estimating COVID-19 VE among community-dwelling people aged ≥ 60 $$ \ge 60 $$ years in Québec, we propose a one-step doubly robust and locally efficient estimator called TNDDR (TND doubly robust), which utilizes cross-fitting (sample splitting) and can incorporate machine learning techniques to estimate the nuisance functions and thus improve control for measured confounders. We derive the efficient influence function (EIF) for the marginal expectation of the outcome under a vaccination intervention, explore the von Mises expansion, and establish the conditions for n $$ \sqrt{n} $$ -consistency, asymptotic normality, and double robustness of TNDDR. The proposed estimator is supported by both theoretical and empirical justifications.},
  archive      = {J_SIM},
  author       = {Cong Jiang and Denis Talbot and Sara Carazo and Mireille E. Schnitzer},
  doi          = {10.1002/sim.70025},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70025},
  shortjournal = {Stat. Med.},
  title        = {A double machine learning approach for the evaluation of COVID-19 vaccine effectiveness under the test-negative design: Analysis of québec administrative data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using win odds to improve commit-to-phase-3 decision-making in oncology. <em>SIM</em>, <em>44</em>(5), e70024. (<a href='https://doi.org/10.1002/sim.70024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making good decisions about whether to commit-to-phase 3 clinical trials is challenging. This is especially true in oncology because the relationships between the registration endpoint, overall survival, and endpoints such as progression-free survival and confirmed objective response are often poorly understood. We present a framework for decision-making based on a three-endpoint win odds. We discuss properties of the win odds and suggest that it can be interpreted, for decision-making, as the reciprocal of an average hazard ratio for overall survival. We confirm the performance of the decision-making method using simulation studies and a clinical trial case study. As part of this work, we describe the simulation of correlated patient-level oncology endpoints using a multi-state model of disease. This model can provide clinically realistic data for testing the performance of analysis methods. We conclude that the win odds can improve commit-to-phase-3 decision-making compared with other methods.},
  archive      = {J_SIM},
  author       = {Benjamin F. Hartley and Thomas Drury and Brian Di Pace and Helen Zhou and Tai-Tsang Chen and Inna Perevozskaya},
  doi          = {10.1002/sim.70024},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70024},
  shortjournal = {Stat. Med.},
  title        = {Using win odds to improve commit-to-phase-3 decision-making in oncology},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly modeling time-to-event and longitudinal data with individual-specific change points: A case study in modeling tumor burden. <em>SIM</em>, <em>44</em>(5), e70021. (<a href='https://doi.org/10.1002/sim.70021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In oncology clinical trials, tumor burden (TB) stands as a crucial longitudinal biomarker, reflecting the toll a tumor takes on a patient's prognosis. With certain treatments, the disease's natural progression shows the tumor burden initially receding before rising once more. Biologically, the point of change may be different between individuals and must have occurred between the baseline measurement and progression time of the patient, implying a random effects model obeying a bound constraint. However, in practice, patients may drop out of the study due to progression or death, presenting a non-ignorable missing data problem. In this paper, we introduce a novel joint model that combines time-to-event data and longitudinal data, where the latter is parameterized by a random change point augmented by random pre-slope and post-slope dynamics. Importantly, the model is equipped to incorporate covariates across the longitudinal and survival models, adding significant flexibility. Adopting a Bayesian approach, we propose an efficient Hamiltonian Monte Carlo (HMC) algorithm for parameter inference. We demonstrate the superiority of our approach compared to a longitudinal-only model via simulations and apply our method to a data set in oncology. The code for implementation is publicly available on https://github.com/quyixiang/chgptModel .},
  archive      = {J_SIM},
  author       = {Ethan M. Alt and Yixiang Qu and Emily Meghan Damone and Jing-ou Liu and Chenguang Wang and Joseph G. Ibrahim},
  doi          = {10.1002/sim.70021},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70021},
  shortjournal = {Stat. Med.},
  title        = {Jointly modeling time-to-event and longitudinal data with individual-specific change points: A case study in modeling tumor burden},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph-theoretic approach to detection of parkinsonian freezing of gait from videos. <em>SIM</em>, <em>44</em>(5), e70020. (<a href='https://doi.org/10.1002/sim.70020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freezing of Gait (FOG) is a prevalent symptom in advanced Parkinson's Disease (PD), characterized by intermittent transitions between normal gait and freezing episodes. This study introduces a novel graph-theoretic approach to detect FOG from video data of PD patients. We construct a sequence of pose graphs that represent the spatial relations and temporal progression of a patient's posture over time. Each graph node corresponds to an estimated joint position, while the edges reflect the anatomical connections and their proximity. We propose a hypothesis testing procedure that deploys the Fréchet statistics to identify break points in time between regular gait and FOG episodes, where we model the central tendency and dispersion of the pose graphs in the presentation of graph Laplacian matrices by computing their Fréchet mean and variance. We implement binary segmentation and incremental computation in our algorithm for efficient calculation. The proposed framework is validated on two datasets, Kinect3D and AlphaPose, demonstrating its effectiveness in detecting FOG from video data. The proposed approach that extracts matrix features is distinct from the prevailing pixel-based deep learning methods. It provides a new perspective on feature extraction for FOG detection and potentially contributes to improved diagnosis and treatment of PD.},
  archive      = {J_SIM},
  author       = {Qi Liu and Jie Bao and Xu Zhang and Chuan Shi and Catherine Liu and Rui Luo},
  doi          = {10.1002/sim.70020},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70020},
  shortjournal = {Stat. Med.},
  title        = {A graph-theoretic approach to detection of parkinsonian freezing of gait from videos},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison of statistical methods for time-to-event analyses in randomized controlled trials under non-proportional hazards. <em>SIM</em>, <em>44</em>(5), e70019. (<a href='https://doi.org/10.1002/sim.70019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While well-established methods for time-to-event data are available when the proportional hazards assumption holds, there is no consensus on the best inferential approach under non-proportional hazards (NPH). However, a wide range of parametric and non-parametric methods for testing and estimation in this scenario have been proposed. To provide recommendations on the statistical analysis of clinical trials where non-proportional hazards are expected, we conducted a simulation study under different scenarios of non-proportional hazards, including delayed onset of treatment effect, crossing hazard curves, subgroups with different treatment effects, and changing hazards after disease progression. We assessed type I error rate control, power, and confidence interval coverage, where applicable, for a wide range of methods, including weighted log-rank tests, the MaxCombo test, summary measures such as the restricted mean survival time (RMST), average hazard ratios, and milestone survival probabilities, as well as accelerated failure time regression models. We found a trade-off between interpretability and power when choosing an analysis strategy under NPH scenarios. While analysis methods based on weighted logrank tests typically were favorable in terms of power, they do not provide an easily interpretable treatment effect estimate. Also, depending on the weight function, they test a narrow null hypothesis of equal hazard functions, and rejection of this null hypothesis may not allow for a direct conclusion of treatment benefit in terms of the survival function. In contrast, non-parametric procedures based on well-interpretable measures like the RMST difference had lower power in most scenarios. Model-based methods based on specific survival distributions had larger power; however, often gave biased estimates and lower than nominal confidence interval coverage. The application of the studied methods is illustrated in a case study with reconstructed data from a phase III oncologic trial.},
  archive      = {J_SIM},
  author       = {Florian Klinglmüller and Tobias Fellinger and Franz König and Tim Friede and Andrew C. Hooker and Harald Heinzl and Martina Mittlböck and Jonas Brugger and Maximilian Bardo and Cynthia Huber and Norbert Benda and Martin Posch and Robin Ristl},
  doi          = {10.1002/sim.70019},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70019},
  shortjournal = {Stat. Med.},
  title        = {A comparison of statistical methods for time-to-event analyses in randomized controlled trials under non-proportional hazards},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power and sample size calculations for cluster randomized hybrid type 2 effectiveness-implementation studies. <em>SIM</em>, <em>44</em>(5), e70015. (<a href='https://doi.org/10.1002/sim.70015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid studies allow investigators to simultaneously study an intervention effectiveness outcome and an implementation research outcome. In particular, type 2 hybrid studies support research that places equal importance on both outcomes rather than focusing on one and secondarily on the other (i.e., type 1 and type 3 studies). Hybrid 2 studies introduce the statistical issue of multiple testing, complicated by the fact that they are typically also cluster randomized trials. Standard statistical methods do not apply in this scenario. Here, we describe the design methodologies available for validly powering hybrid type 2 studies and producing reliable sample size calculations in a cluster-randomized design with a focus on binary outcomes. Through a literature search, 18 publications were identified that included methods relevant to the design of hybrid 2 studies. Five methods were identified, two of which did not account for clustering but are extended in this article to do so, namely the combined outcomes approach and the single 1-degree of freedom combined test. Procedures for powering hybrid 2 studies using these five methods are described and illustrated using input parameters inspired by a study from the Community Intervention to Reduce CardiovascuLar Disease in Chicago (CIRCL-Chicago) Implementation Research Center. In this illustrative example, the intervention effectiveness outcome was controlled blood pressure, and the implementation outcome was reach. The conjunctive test resulted in higher power than the popular p value adjustment methods, and the newly extended combined outcomes and single 1-DF test were found to be the most powerful among all of the tests.},
  archive      = {J_SIM},
  author       = {Melody A. Owen and Geoffrey M. Curran and Justin D. Smith and Yacob Tedla and Chao Cheng and Donna Spiegelman},
  doi          = {10.1002/sim.70015},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70015},
  shortjournal = {Stat. Med.},
  title        = {Power and sample size calculations for cluster randomized hybrid type 2 effectiveness-implementation studies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DOD-SSR: An adaptive seamless phase II/III design with dose optimization decision and sample size re-estimation. <em>SIM</em>, <em>44</em>(5), e70014. (<a href='https://doi.org/10.1002/sim.70014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of seamless Phase II/III designs has grown in popularity as a strategy to potentially accelerate the drug development. Making well-informed decisions regarding the drug's potential and addressing important clinical inquiries at the conclusion of the exploratory phase has become a critical step. In response to the increased emphasis on dose optimization, it becomes logical to integrate treatment arm/dose selections into Phase II and implement corresponding design adjustments. Within this framework, employing a fixed sample size presents challenges due to limited information availability before the trial planning and elevated development risks. Furthermore, practical and feasibility considerations have led to the increased utilization of surrogate endpoints for making interim decisions. In this study, we introduce a novel framework for a seamless Phase II/III design involving multiple treatment arms, leveraging Bayesian predictive probability of success (PPoS) for both treatment arm selection and interim sample size re-estimation (SSR) using surrogate endpoints. The proposed design demonstrates improved performance, including a higher likelihood of selecting favorable treatment arm, increased overall statistical power, and reduced average event sizes and trial durations compared to traditional separate Phase II and III designs, as well as other seamless Phase II/III designs without SSR or of which treatment arm selection is based on conditional power. We also showcase the implementation of the proposed design through a case study in non-small cell lung cancer (NSCLC).},
  archive      = {J_SIM},
  author       = {Meizi Liu and Jianchang Lin and Yefei Zhang and Rachael Liu},
  doi          = {10.1002/sim.70014},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70014},
  shortjournal = {Stat. Med.},
  title        = {DOD-SSR: An adaptive seamless phase II/III design with dose optimization decision and sample size re-estimation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Challenges for predictive modeling with neural network techniques using error-prone dietary intake data. <em>SIM</em>, <em>44</em>(5), e70013. (<a href='https://doi.org/10.1002/sim.70013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dietary intake data are routinely drawn upon to explore diet-health relationships, and inform clinical practice and public health. However, these data are almost always subject to measurement error, distorting true diet-health relationships. Beyond measurement error, there are likely complex synergistic and sometimes antagonistic interactions between different dietary components, complicating the relationships between diet and health outcomes. Flexible models are required to capture the nuance that these complex interactions introduce. This complexity makes research on diet-health relationships an appealing candidate for the application of modern machine learning techniques, and in particular, neural networks. Neural networks are computational models that can capture highly complex, nonlinear relationships, so long as sufficient data are available. While these models have been applied in many domains, the impacts of measurement error on the performance of predictive modeling have not been widely investigated. In this work, we demonstrate the ways in which measurement error erodes the performance of neural networks and illustrate the care that is required for leveraging these models in the presence of error. We demonstrate the role that sample size and replicate measurements play in model performance, indicate a motivation for the investigation of transformations to additivity, and illustrate the caution required to prevent model overfitting. While the past performance of neural networks across various domains makes them an attractive candidate for examining diet-health relationships, our work demonstrates that substantial care and further methodological development are both required to observe increased predictive performance when applying these techniques compared to more traditional statistical procedures.},
  archive      = {J_SIM},
  author       = {Dylan Spicker and Amir Nazemi and Joy Hutchinson and Paul Fieguth and Sharon Kirkpatrick and Michael Wallace and Kevin W. Dodd},
  doi          = {10.1002/sim.70013},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70013},
  shortjournal = {Stat. Med.},
  title        = {Challenges for predictive modeling with neural network techniques using error-prone dietary intake data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RMST for interval-censored data in oncology clinical trials. <em>SIM</em>, <em>44</em>(5), e70012. (<a href='https://doi.org/10.1002/sim.70012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In oncology studies, the assumption of proportional hazards is often questionable due to factors such as the presence of cured patients, a delayed treatment benefit, and possible treatment switching. The restricted mean survival time (RMST) has emerged as a valuable alternative summary measure to the hazard ratio (HR) in this scenario as it provides a clinically meaningful interpretation of treatment benefit without additional assumptions. As a commonly used primary endpoint, progression-free survival (PFS) is defined as the time from randomization to the first occurrence of death or progression of disease (PD). However, PFS involves dual observation processes where, in practice, the exact death time is typically recorded, but PD is interval-censored. This feature is also present in other commonly used primary endpoints, including event-free survival, disease-free survival, and relapse-free survival. The conventional approach imputes the PD time with the right boundary of the time interval during which the PD occurs. This paper presents alternative estimation and inference approaches to estimate RMST with a mixture of right-censored and interval-censored data. Different approaches are explored by simulation under various plausible scenarios for oncology clinical trials with regard to the assessment frequency, randomness in the actual assessment times, and size of treatment effect. The choice of the restricted time point in RMST is also explored. The simulation results indicate that the RMST estimators that take account of the interval censoring inherent in the data are unbiased and more accurate than the conventional estimators, while the performance for two-group comparisons is comparable. Furthermore, the performance of the proposed estimators is contingent on the scheduled assessment plan and patients' visit window.},
  archive      = {J_SIM},
  author       = {Xiyuan Gao and Tianmeng Lyu and Menghao Xu and Lisa V. Hampson and Yan Du and Renxin Lin and Nigel Yateman and Lu Tian and Jianguo Sun},
  doi          = {10.1002/sim.70012},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70012},
  shortjournal = {Stat. Med.},
  title        = {RMST for interval-censored data in oncology clinical trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instability of the AUROC of clinical prediction models. <em>SIM</em>, <em>44</em>(5), e70011. (<a href='https://doi.org/10.1002/sim.70011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Florian D. van Leeuwen and Ewout W. Steyerberg and David van Klaveren and Ben Wessler and David M. Kent and Erik W. van Zwet},
  doi          = {10.1002/sim.70011},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70011},
  shortjournal = {Stat. Med.},
  title        = {Instability of the AUROC of clinical prediction models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A calibrated sensitivity analysis for weighted causal decompositions. <em>SIM</em>, <em>44</em>(5), e70010. (<a href='https://doi.org/10.1002/sim.70010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disparities in health or well-being experienced by minority groups can be difficult to study using the traditional exposure-outcome paradigm in causal inference, since potential outcomes in variables such as race or sexual minority status are challenging to interpret. Causal decomposition analysis addresses this gap by positing causal effects on disparities under interventions to other intervenable exposures that may play a mediating role in the disparity. While invoking weaker assumptions than causal mediation approaches, decomposition analyses are often conducted in observational settings and require uncheckable assumptions that eliminate unmeasured confounders. Leveraging the marginal sensitivity model, we develop a sensitivity analysis for weighted causal decomposition estimators and use the percentile bootstrap to construct valid confidence intervals for causal effects on disparities. We also propose a two-parameter reformulation that enhances interpretability and facilitates an intuitive understanding of the plausibility of unmeasured confounders and their effects. We illustrate our framework on a study examining the effect of parental support on disparities in suicidal ideation among sexual minority youth. We find that the effect is small and sensitive to unmeasured confounding, suggesting that further screening studies are needed to identify mitigating interventions in this vulnerable population.},
  archive      = {J_SIM},
  author       = {Andy A. Shen and Elina Visoki and Ran Barzilay and Samuel D. Pimentel},
  doi          = {10.1002/sim.70010},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70010},
  shortjournal = {Stat. Med.},
  title        = {A calibrated sensitivity analysis for weighted causal decompositions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inverse probability of treatment weighting using the propensity score with competing risks in survival analysis. <em>SIM</em>, <em>44</em>(5), e70009. (<a href='https://doi.org/10.1002/sim.70009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability of treatment weighting (IPTW) using the propensity score allows estimation of the effect of treatment in observational studies. We had three objectives: first, to describe methods for using IPTW to estimate the effects of treatments in settings with competing risks; second, to illustrate the application of these methods using empirical analyses; and third, to conduct Monte Carlo simulations to evaluate the relative performance of three methods for estimating time-specific risk differences and time-specific relative risks in settings with competing risks. In doing so, we provide guidance to applied biostatisticians and clinical investigators on the use of IPTW in settings with competing risks. We examined three estimators of time-specific risk differences and relative risks: the weighted Aalen–Johansen estimator, an estimator that combines IPTW with inverse probability of censoring weights (IPTW-IPCWs), and a double-robust augmented IPTW estimator combined with IPCW (AIPTW-IPCW). The design of our simulations reflected clinically realistic scenarios. Our simulations found that all three estimators tended to result in unbiased estimations of time-specific risk differences and time-specific relative risks. However, the weighted Aalen–Johansen estimator and the AIPTW-IPCW estimator tended to result in estimates with greater precision compared to the IPTW-IPCW estimator. In our empirical analyses, we illustrated the application of these methods by estimating the effect of statin prescribing on the risk of subsequent cardiovascular death in patients discharged from the hospital with a diagnosis of acute myocardial infarction.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Jason P. Fine},
  doi          = {10.1002/sim.70009},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70009},
  shortjournal = {Stat. Med.},
  title        = {Inverse probability of treatment weighting using the propensity score with competing risks in survival analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proportional mean residual life model with varying coefficients for right censored data. <em>SIM</em>, <em>44</em>(5), e70008. (<a href='https://doi.org/10.1002/sim.70008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean residual life provides the remaining life expectancy of a subject who has survived to a specific time point. This paper considers a proportional mean residual life model with varying coefficients, which allows one to explore the nonlinear interactions between some covariates and an exposure variable. In a semiparametric setting, we construct local estimating equations to obtain the varying coefficients and establish the asymptotic normality of the proposed estimators. Moreover, the weak convergence property for the local estimator of the baseline mean residual life function is developed. We conduct simulation studies to empirically examine the finite-sample performance of the proposed methods and apply the methodology to a real-life dataset on type 2 diabetic complications.},
  archive      = {J_SIM},
  author       = {Bing Wang and Xinyuan Song and Qian Zhao},
  doi          = {10.1002/sim.70008},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70008},
  shortjournal = {Stat. Med.},
  title        = {Proportional mean residual life model with varying coefficients for right censored data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification and estimation of the average causal effects under dietary substitution strategies. <em>SIM</em>, <em>44</em>(5), e70007. (<a href='https://doi.org/10.1002/sim.70007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2020–2025 Dietary Guidelines suggest that most people can improve their diet by making some changes to what they eat and drink. In many cases, these changes involve simple substitutions. For instance, the Dietary Guidelines recommend choosing chicken instead of processed red meat to reduce sodium intake and switching from refined grains to whole grains to increase dietary fiber intake. The question about such dietary substitution strategies seeks to estimate the average counterfactual outcome under a hypothetical intervention that replaces a food an individual would have consumed in the absence of intervention with a healthier substitute. In this work, we will show the conditions under which the average causal effects of substitution strategies can be non-parametrically identified, and provide efficient estimators for our proposed dietary substitution strategies. We evaluate the performance of our proposed methods via simulation studies and apply them to estimate the effect of substituting processed red meat with chicken on mortality, using data from the Nurses' Health Study.},
  archive      = {J_SIM},
  author       = {Yu-Han Chiu and Lan Wen},
  doi          = {10.1002/sim.70007},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70007},
  shortjournal = {Stat. Med.},
  title        = {Identification and estimation of the average causal effects under dietary substitution strategies},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating average treatment effects with support vector machines. <em>SIM</em>, <em>44</em>(5), e70006. (<a href='https://doi.org/10.1002/sim.70006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is one of the most popular classification algorithms in the machine learning literature. We demonstrate that SVM can be used to balance covariates and estimate average causal effects under the unconfoundedness assumption. Specifically, we adapt the SVM classifier as a kernel-based weighting procedure that minimizes the maximum mean discrepancy between the treatment and control groups while simultaneously maximizing effective sample size. We also show that SVM is a continuous relaxation of the quadratic integer program for computing the largest balanced subset, establishing its direct relation to the cardinality matching method. Another important feature of SVM is that the regularization parameter controls the trade-off between covariate balance and effective sample size. As a result, the existing SVM path algorithm can be used to compute the balance-sample size frontier. We characterize the bias of causal effect estimation arising from this trade-off, connecting the proposed SVM procedure to the existing kernel balancing methods. Finally, we conduct simulation and empirical studies to evaluate the performance of the proposed methodology and find that SVM is competitive with the state-of-the-art covariate balancing methods.},
  archive      = {J_SIM},
  author       = {Alexander Tarr and Kosuke Imai},
  doi          = {10.1002/sim.70006},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70006},
  shortjournal = {Stat. Med.},
  title        = {Estimating average treatment effects with support vector machines},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individualized time-varying nonparametric model with an application in mobile health. <em>SIM</em>, <em>44</em>(5), e70005. (<a href='https://doi.org/10.1002/sim.70005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized modeling has become increasingly popular in recent years with its growing application in fields such as personalized medicine and mobile health studies. With rich longitudinal measurements, it is of great interest to model certain subject-specific time-varying covariate effects. In this paper, we propose an individualized time-varying nonparametric model by leveraging the subgroup information from the population. The proposed method approximates the time-varying covariate effect using nonparametric B-splines and aggregates the estimated nonparametric coefficients that share common patterns. Moreover, the proposed method can effectively handle various missing data patterns that frequently arise in mobile health data. Specifically, our method achieves subgrouping by flexibly accommodating varying dimensions of B-spline coefficients due to missingness. This capability sets it apart from other fusion-type approaches for subgrouping. The subgroup information can also potentially provide meaningful insight into the characteristics of subjects and assist in recommending an effective treatment or intervention. An efficient ADMM algorithm is developed for implementation. Our numerical studies and application to mobile health data on monitoring pregnant women's deep sleep and physical activities demonstrate that the proposed method achieves better performance compared to other existing methods.},
  archive      = {J_SIM},
  author       = {Jenifer Rim and Qi Xu and Xiwei Tang and Yuqing Guo and Annie Qu},
  doi          = {10.1002/sim.70005},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70005},
  shortjournal = {Stat. Med.},
  title        = {Individualized time-varying nonparametric model with an application in mobile health},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive use of co-data through empirical bayes for bayesian additive regression trees. <em>SIM</em>, <em>44</em>(5), e70004. (<a href='https://doi.org/10.1002/sim.70004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For clinical prediction applications, we are often faced with small sample size data compared to the number of covariates. Such data pose problems for variable selection and prediction, especially when the covariate-response relationship is complicated. To address these challenges, we propose to incorporate external information on the covariates into Bayesian additive regression trees (BART), a sum-of-trees prediction model that utilizes priors on the tree parameters to prevent overfitting. To incorporate external information, an empirical Bayes (EB) framework is developed that estimates, assisted by a model, prior covariate weights in the BART model. The proposed EB framework enables the estimation of the other prior parameters of BART as well, rendering an appealing and computationally efficient alternative to cross-validation. We show that the method finds relevant covariates and that it improves prediction compared to default BART in simulations. If the covariate-response relationship is non-linear, the method benefits from the flexibility of BART to outperform regression-based learners. Finally, the benefit of incorporating external information is shown in an application to diffuse large B-cell lymphoma prognosis based on clinical covariates, gene mutations, DNA translocations, and DNA copy number data.},
  archive      = {J_SIM},
  author       = {Jeroen M. Goedhart and Thomas Klausch and Jurriaan Janssen and Mark A. van de Wiel},
  doi          = {10.1002/sim.70004},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70004},
  shortjournal = {Stat. Med.},
  title        = {Adaptive use of co-data through empirical bayes for bayesian additive regression trees},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework to assess complex heterogeneity in the strength of a surrogate marker. <em>SIM</em>, <em>44</em>(5), e70001. (<a href='https://doi.org/10.1002/sim.70001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A surrogate marker is a biological measurement in a clinical trial that aims to replace the primary outcome in evaluating the treatment effect, and can be measured earlier, with less cost, or with less patient burden. In theory, once a surrogate is validated, future studies can evaluate treatment efficacy using only the surrogate. While there are many methods to evaluate a surrogate, these methods rarely account for heterogeneity in surrogacy, that is, when a surrogate is valid for only certain people. We propose a general framework for the assessment of complex heterogeneity in the strength of a surrogate marker, as well as corresponding parametric and semiparametric estimation procedures. Our framework defines the proportion of the treatment effect on the primary outcome that is explained by the treatment effect on the surrogate, as a function of multiple baseline covariates, W $$ \mathbf{W} $$ . We additionally propose a formal test of heterogeneity and a method to identify a region of the covariate space where the surrogate is sufficiently strong. We examine the performance of our methods via a simulation study featuring varying levels of heterogeneity and use our methods to examine potential heterogeneity in the strength of a surrogate in an AIDS clinical trial.},
  archive      = {J_SIM},
  author       = {Rebecca Knowlton and Lu Tian and Layla Parast},
  doi          = {10.1002/sim.70001},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e70001},
  shortjournal = {Stat. Med.},
  title        = {A general framework to assess complex heterogeneity in the strength of a surrogate marker},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to “DL 101: Basic introduction to deep learning with its application in biomedical related fields”. <em>SIM</em>, <em>44</em>(5), e10349. (<a href='https://doi.org/10.1002/sim.10349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.10349},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10349},
  shortjournal = {Stat. Med.},
  title        = {Correction to “DL 101: Basic introduction to deep learning with its application in biomedical related fields”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying the estimands framework to non-inferiority trials: Guidance on choice of hypothetical estimands for non-adherence and comparison of estimation methods. <em>SIM</em>, <em>44</em>(5), e10348. (<a href='https://doi.org/10.1002/sim.10348'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common concern in non-inferiority (NI) trials is that non-adherence due, for example, to poor study conduct can make treatment arms artificially similar. Because intention-to-treat analyses can be anti-conservative in this situation, per-protocol analyses are sometimes recommended. However, such advice does not consider the estimands framework, nor the risk of bias from per-protocol analyses. We therefore sought to update the above guidance using the estimands framework, and compare estimators to improve on the performance of per-protocol analyses. We argue the main threat to validity of NI trials is the occurrence of “trial-specific” intercurrent events (IEs), that is, IEs which occur in a trial setting, but would not occur in practice. To guard against erroneous conclusions of non-inferiority, we suggest an estimand using a hypothetical strategy for trial-specific IEs should be employed, with handling of other non-trial-specific IEs chosen based on clinical considerations. We provide an overview of estimators that could be used to estimate a hypothetical estimand, including inverse probability weighting (IPW), and two instrumental variable approaches (one using an informative Bayesian prior on the effect of standard treatment, and one using a treatment-by-covariate interaction as an instrument). We compare them, using simulation in the setting of all-or-nothing compliance in two active treatment arms, and conclude both IPW and the instrumental variable method using a Bayesian prior are potentially useful approaches, with the choice between them depending on which assumptions are most plausible for a given trial.},
  archive      = {J_SIM},
  author       = {Katy E. Morgan and Ian R. White and Clémence Leyrat and Simon Stanworth and Brennan C. Kahan},
  doi          = {10.1002/sim.10348},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10348},
  shortjournal = {Stat. Med.},
  title        = {Applying the estimands framework to non-inferiority trials: Guidance on choice of hypothetical estimands for non-adherence and comparison of estimation methods},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of cohort stepped wedge cluster-randomized trials with nonignorable dropout via joint modeling. <em>SIM</em>, <em>44</em>(5), e10347. (<a href='https://doi.org/10.1002/sim.10347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster-randomized trial (CRTs) designs randomize clusters of individuals to intervention sequences, ensuring that every cluster eventually transitions from a control period to receive the intervention under study by the end of the study period. The analysis of stepped wedge CRTs is usually more complex than parallel-arm CRTs due to more complex intra-cluster correlation structures. A further challenge in the analysis of closed-cohort stepped wedge CRTs, which follow groups of individuals enrolled in each period longitudinally, is the occurrence of dropout. This is particularly problematic in studies of individuals at high risk for mortality, which causes nonignorable missing outcomes. If not appropriately addressed, missing outcomes from death will erode statistical power, at best, and bias treatment effect estimates, at worst. Joint longitudinal-survival models can accommodate informative dropout and missingness patterns in longitudinal studies. Specifically, within the joint longitudinal-survival modeling framework, one directly models the dropout process via a time-to-event submodel together with the longitudinal outcome of interest. The two submodels are then linked using a variety of possible association structures. This work extends linear mixed-effects models by jointly modeling the dropout process to accommodate informative missing outcome data in closed-cohort stepped wedge CRTs. We focus on constant intervention and general time-on-treatment effect parametrizations for the longitudinal submodel and study the performance of the proposed methodology using Monte Carlo simulation under several data-generating scenarios. We illustrate the joint modeling methodology in practice by reanalyzing data from the “Frail Older Adults: Care in Transition” (ACT) trial, a stepped wedge CRT of a multifaceted geriatric care model versus usual care in 35 primary care practices in the Netherlands.},
  archive      = {J_SIM},
  author       = {Alessandro Gasparini and Michael J. Crowther and Emiel O. Hoogendijk and Fan Li and Michael O. Harhay},
  doi          = {10.1002/sim.10347},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10347},
  shortjournal = {Stat. Med.},
  title        = {Analysis of cohort stepped wedge cluster-randomized trials with nonignorable dropout via joint modeling},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random survival forest for censored functional data. <em>SIM</em>, <em>44</em>(5), e10344. (<a href='https://doi.org/10.1002/sim.10344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a Random Survival Forest (RSF) method for functional data. The focus is specifically on defining a new functional data structure, the Censored Functional Data (CFD), for addressing the challenge of accurately modelling time-to-event data in the presence of censoring and irregular temporal structures. Traditional survival models struggle to incorporate complex functional patterns, making the proposed approach particularly valuable for improving prediction and interpretation. This approach allows for precise modelling of functional survival trajectories, leading to improved interpretation and prediction of survival dynamics across different groups. A medical survival study on the benchmark Sequential Organ Failure Assessment (SOFA) dataset and an extensive simulation study are presented. Results show good performance of the proposed approach, particularly in ranking the importance of predicting variables.},
  archive      = {J_SIM},
  author       = {Giuseppe Loffredo and Elvira Romano and Fabrizio Maturo},
  doi          = {10.1002/sim.10344},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10344},
  shortjournal = {Stat. Med.},
  title        = {Random survival forest for censored functional data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing for the functional form of a continuous covariate in the shared-parameter joint model. <em>SIM</em>, <em>44</em>(5), e10340. (<a href='https://doi.org/10.1002/sim.10340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared-parameter joint modeling is a useful technique for properly associating longitudinal and time-to-event data. When the interest is in the survival outcome, the conditional logarithm of the hazard function for an event is conventionally presumed to be linearly related over time to a set of explanatory covariates, among other terms. However, this hypothesis is quite restrictive and may yield misleading results. Our objective here is to easily check such a modeling assumption for any continuous fixed covariate. For this purpose, we examine the appropriateness of a nonparametric test criterion based on a penalty-modified version of the Akaike information criterion. An extensive numerical study is conducted to check the validity of the test within the joint modeling framework, while determining the extent to which the function embedding the continuous covariate deviates from linearity. Furthermore, once a deviation from linearity is detected, the improvement in the model's predictive performance is examined. The usefulness of the testing procedure is illustrated using a clinical trial with HIV-infected subjects. Specifically, our example focuses on properly accounting for the effect of nadir CD4 cell count within a predictive joint model for the time to immune recovery.},
  archive      = {J_SIM},
  author       = {Xavier Piulachs and Anouar El Ghouch and Ingrid Van Keilegom},
  doi          = {10.1002/sim.10340},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10340},
  shortjournal = {Stat. Med.},
  title        = {Testing for the functional form of a continuous covariate in the shared-parameter joint model},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). So many choices: A guide to selecting among methods to adjust for observed confounders. <em>SIM</em>, <em>44</em>(5), e10336. (<a href='https://doi.org/10.1002/sim.10336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-randomised studies (NRS) typically assume that there are no differences in unobserved baseline characteristics between the treatment groups under comparison. Traditionally regression models have been deployed to estimate treatment effects adjusting for observed confounders but can lead to biased estimates if the model is missspecified, by making incorrect functional form assumptions. A multitude of alternative methods have been developed which can reduce the risk of bias due to model misspecification. Investigators can now choose between many forms of matching, weighting, doubly robust, and machine learning methods. We review key concepts related to functional form assumptions and how those can contribute to bias from model misspecification. We then categorize the three frameworks for modeling treatment effects and the wide variety of estimation methods that can be applied to each framework. We consider why machine learning methods have been widely proposed for estimation and review the strengths and weaknesses of these approaches. We apply a range of these methods in re-analyzing a landmark case study. In the application, we examine how several widely used methods may be subject to bias from model misspecification. We conclude with a set of recommendations for practice.},
  archive      = {J_SIM},
  author       = {Luke Keele and Richard Grieve},
  doi          = {10.1002/sim.10336},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10336},
  shortjournal = {Stat. Med.},
  title        = {So many choices: A guide to selecting among methods to adjust for observed confounders},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScRecover: Discriminating true and false zeros in single-cell RNA-seq data for imputation. <em>SIM</em>, <em>44</em>(5), e10334. (<a href='https://doi.org/10.1002/sim.10334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput single-cell RNA-seq (scRNA-seq) data contains an excess of zero values, which can be contributed by unexpressed genes and detection signal dropouts. Existing imputation methods fail to distinguish between these two types of zeros. In this study, we introduce a statistical framework that effectively differentiates true zeros (lack of expression) from false zeros (dropouts). By focusing only on imputing the dropout zeros, we developed a new imputation tool, scRecover. Our approach utilizes a zero-inflated negative binomial framework to model the gene expression of each gene in each cell, enabling the estimation of zero-dropout probability. Additionally, we employ a modified version of the Good and Toulmin model to identify true zeros for each gene. To achieve imputation, scRecover is combined with other imputation methods such as scImpute, SAVER and MAGIC. Down-sampling experiments show that it recovers dropout zeros with higher accuracy and avoids over-imputing true zero values. Experiments conducted on real world data highlight the ability of scRecover to enhance downstream analysis and visualization.},
  archive      = {J_SIM},
  author       = {Zhun Miao and Xinyi Lin and Jiaqi Li and Joshua Ho and Qiuchen Meng and Xuegong Zhang},
  doi          = {10.1002/sim.10334},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10334},
  shortjournal = {Stat. Med.},
  title        = {ScRecover: Discriminating true and false zeros in single-cell RNA-seq data for imputation},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal control of directional false discovery rates in large-scale testing. <em>SIM</em>, <em>44</em>(5), e10329. (<a href='https://doi.org/10.1002/sim.10329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-throughput biomedical technology enables measurement of thousands of gene expression levels contemporaneously. A major task in analyzing these gene expression data is to identify both over-expressed and under-expressed genes. The popular two-group models select the non-null genes without further classifying them as overexpression or underexpression. Consequently, two-group decision rules are unable to constrain the numbers of falsely discovered over-expressed or under-expressed genes respectively. We propose a general three-group model that allows dependence between the test statistics and develop a decision rule that separately controls the two types of false discoveries. We show that the optimal decision rule in our three-group model has a special monotonic structure. By making use of this monotonic structure, we can linearize the two-directional false discovery rate constraints. We prove that our decision rule optimizes the expected number of true discoveries while controlling the proportions of falsely discovered over-expressed and under-expressed genes at desired levels simultaneously. The data-driven versions of the proposed procedures are suggested, and their consistency is established. Comparisons with state-of-the-art approaches and applications to genomic studies show that our procedures work well.},
  archive      = {J_SIM},
  author       = {Guozhu Tang and Yicheng Kang and Dongdong Xiang},
  doi          = {10.1002/sim.10329},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10329},
  shortjournal = {Stat. Med.},
  title        = {Optimal control of directional false discovery rates in large-scale testing},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-dependent ROC curve for multiple longitudinal biomarkers and its application in diagnosing cardiovascular events. <em>SIM</em>, <em>44</em>(5), e10318. (<a href='https://doi.org/10.1002/sim.10318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since they can help people detect the early signs of diseases, accurate diagnostic techniques based on biomarkers are crucial in biomedical research. This article proposes a novel bivariate time-varying coefficients logistic regression model for addressing the combined longitudinal biomarkers. Using the B-splines method to estimate the proposed model, we can effectively combine multiple longitudinal biomarkers and improve diagnostic accuracy. We show that the proposed method is theoretically consistent. And it exhibits superior performance compared to the existing method, as presented through numerical results. The proposed method is verified in a study on predicting the probability of onset of future cardiovascular events for type 2 diabetic patients. The longitudinal biomarkers, HbA1c and LDL-C, are considered in this study. We demonstrate that the combined longitudinal biomarkers significantly improved disease diagnostic accuracy over only a combination of the latest measured biomarkers in most cases.},
  archive      = {J_SIM},
  author       = {Lizhe Sun and Pingyuan Wei and Jie Zhou and Xiao-Hua Zhou},
  doi          = {10.1002/sim.10318},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10318},
  shortjournal = {Stat. Med.},
  title        = {Time-dependent ROC curve for multiple longitudinal biomarkers and its application in diagnosing cardiovascular events},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal mediation analysis: A summary-data mendelian randomization approach. <em>SIM</em>, <em>44</em>(5), e10317. (<a href='https://doi.org/10.1002/sim.10317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Summary-data Mendelian randomization (MR), a widely used approach in causal inference, has recently attracted attention for improving causal mediation analysis. Two existing methods corresponding to the difference method and product method of linear mediation analysis have been developed to perform MR-based mediation analysis using the inverse-variance weighted method (MR-IVW). Despite these developments, there is still a need for more rigorous, efficient, and precise MR-based mediation methodologies. In this study, we develop summary-data MR-based frameworks for causal mediation analysis. We improve the accuracy, statistical efficiency and robustness of the existing MR-based mediation analysis by implementing novel variance estimators for the mediation effects, deriving rigorous procedures for statistical inference, and accounting for widespread pleiotropic effects. Specifically, we propose Diff-IVW and Prod-IVW to improve upon the existing methods and provide the pleiotropy-robust methods (Diff-Egger, Diff-Median, Prod-Egger, and Prod-Median), adapted from MR-Egger and MR-Median, to enhance the robustness of the MR-based mediation analysis. We conduct comprehensive simulation studies to compare the existing and proposed methods. The results show that the proposed methods, Diff-IVW and Prod-IVW, improve statistical efficiency and type I error control over the existing approaches. Although all IVW-based methods suffer from directional pleiotropy biases, the median-based methods (Diff-Median and Prod-Median) can mitigate such biases. The differences among the methods can lead to discrepant statistical conclusions as demonstrated in real data applications. Based on our simulation results, we recommend the three proposed methods in practice: Diff-IVW, Prod-IVW, and Prod-Median, which are complementary under various scenarios.},
  archive      = {J_SIM},
  author       = {Shu-Chin Lin and Sheng-Hsuan Lin and Tian Ge and Chia-Yen Chen and Yen-Feng Lin},
  doi          = {10.1002/sim.10317},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10317},
  shortjournal = {Stat. Med.},
  title        = {Causal mediation analysis: A summary-data mendelian randomization approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for association studies in the presence of binary outcome misclassification. <em>SIM</em>, <em>44</em>(5), e10316. (<a href='https://doi.org/10.1002/sim.10316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical and public health association studies, binary outcome variables may be subject to misclassification, resulting in substantial bias in effect estimates. The feasibility of addressing binary outcome misclassification in regression models is often hindered by model identifiability issues. In this paper, we characterize the identifiability problems in this class of models as a specific case of “label-switching” and leverage a pattern in the resulting parameter estimates to solve the permutation invariance of the complete data log-likelihood. Our proposed algorithm in binary outcome misclassification models does not require gold standard labels and relies only on the assumption that the sum of the sensitivity and specificity exceeds 1. A label-switching correction is applied within estimation methods to recover unbiased effect estimates and to estimate misclassification rates. Open-source software is provided to implement the proposed methods. We give a detailed simulation study for our proposed methodology and apply these methods to data from the 2020 Medical Expenditure Panel Survey (MEPS).},
  archive      = {J_SIM},
  author       = {Kimberly A. Hochstedler Webb and Martin T. Wells},
  doi          = {10.1002/sim.10316},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10316},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for association studies in the presence of binary outcome misclassification},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dir-GLM: A bayesian GLM with data-driven reference distribution. <em>SIM</em>, <em>44</em>(5), e10305. (<a href='https://doi.org/10.1002/sim.10305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently developed semi-parametric generalized linear model (SPGLM) offers more flexibility as compared to the classical GLM by including the baseline or reference distribution of the response as an additional parameter in the model. However, some inference summaries are not easily generated under existing maximum-likelihood-based inference (GLDRM). This includes uncertainty in estimation for model-derived functionals such as exceedance probabilities. The latter are critical in a clinical diagnostic or decision-making setting. In this article, by placing a Dirichlet prior on the baseline distribution, we propose a Bayesian model-based approach for inference to address these important gaps. We establish consistency and asymptotic normality results for the implied canonical parameter. Simulation studies and an illustration with data from an aging research study confirm that the proposed method performs comparably or better in comparison with GLDRM. The proposed Bayesian framework is most attractive for inference with small sample training data or in sparse-data scenarios.},
  archive      = {J_SIM},
  author       = {Entejar Alam and Peter Müller and Paul J. Rathouz},
  doi          = {10.1002/sim.10305},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10305},
  shortjournal = {Stat. Med.},
  title        = {Dir-GLM: A bayesian GLM with data-driven reference distribution},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian hierarchical penalized spline models for immediate and time-varying intervention effects in stepped wedge cluster randomized trials. <em>SIM</em>, <em>44</em>(5), e10304. (<a href='https://doi.org/10.1002/sim.10304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster randomized trials (SWCRTs) often face challenges related to potential confounding by time. Traditional frequentist methods may not provide adequate coverage of an intervention's true effect using confidence intervals, whereas Bayesian approaches show potential for better coverage of intervention effects. However, Bayesian methods remain underexplored in the context of SWCRTs. To bridge this gap, we propose two innovative Bayesian hierarchical penalized spline models. Our first model accommodates large numbers of clusters and time periods, focusing on immediate intervention effects. To evaluate this approach, we compared this model to traditional frequentist methods. We then extend our approach to account for time-varying intervention effects, conducting a comprehensive comparison with an existing Bayesian monotone effect curve model and alternative frequentist methods. The proposed models were applied in the Primary Palliative Care for Emergency Medicine stepped wedge trial to evaluate the effectiveness of the intervention. Through extensive simulations and real-world application, we demonstrate the robustness of our proposed Bayesian models. Notably, the Bayesian immediate effect model consistently achieves the nominal coverage probability, providing more reliable interval estimations while maintaining high estimation accuracy. Furthermore, our proposed Bayesian time-varying effect model represents a significant advancement over the existing Bayesian monotone effect curve model, offering improved accuracy and reliability in estimation while also achieving higher coverage probability than alternative frequentist methods. To the best of our knowledge, this marks the first development of Bayesian hierarchical spline modeling for SWCRTs. Our proposed models offer promising tools for researchers and practitioners, enabling more precise evaluation of intervention impacts.},
  archive      = {J_SIM},
  author       = {Danni Wu and Hyung G. Park and Corita R. Grudzen and Keith S. Goldfeld},
  doi          = {10.1002/sim.10304},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {e10304},
  shortjournal = {Stat. Med.},
  title        = {Bayesian hierarchical penalized spline models for immediate and time-varying intervention effects in stepped wedge cluster randomized trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian modeling of cancer outcomes using genetic variables assisted by pathological imaging data. <em>SIM</em>, <em>44</em>(3-4), e10350. (<a href='https://doi.org/10.1002/sim.10350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing maturity of genetic profiling, an essential and routine task in cancer research is to model disease outcomes/phenotypes using genetic variables. Many methods have been successfully developed. However, oftentimes, empirical performance is unsatisfactory because of a “lack of information.” In cancer research and clinical practice, a source of information that is broadly available and highly cost-effective comes from pathological images, which are routinely collected for definitive diagnosis and staging. In this article, we consider a Bayesian approach for selecting relevant genetic variables and modeling their relationships with a cancer outcome/phenotype. We propose borrowing information from (manually curated, low-dimensional) pathological imaging features via reinforcing the same selection results for the cancer outcome and imaging features. We further develop a weighting strategy to accommodate the scenario where information borrowing may not be equally effective for all subjects. Computation is carefully examined. Simulations demonstrate competitive performance of the proposed approach. We analyze TCGA (The Cancer Genome Atlas) LUAD (lung adenocarcinoma) data, with overall survival and gene expressions being the outcome and genetic variables, respectively. Findings different from the alternatives and with sound properties are made.},
  archive      = {J_SIM},
  author       = {Yunju Im and Rong Li and Shuangge Ma},
  doi          = {10.1002/sim.10350},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10350},
  shortjournal = {Stat. Med.},
  title        = {Bayesian modeling of cancer outcomes using genetic variables assisted by pathological imaging data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing one primary and two secondary endpoints in a two-stage group sequential trial with extensions. <em>SIM</em>, <em>44</em>(3-4), e10346. (<a href='https://doi.org/10.1002/sim.10346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of testing multiple secondary endpoints conditional on a primary endpoint being significant in a two-stage group sequential procedure, focusing on two secondary endpoints. This extends our previous work with one secondary endpoint. The test for the secondary null hypotheses is a closed procedure. Application of the Bonferroni test for testing the intersection of the secondary hypotheses results in the Holm procedure while application of the Simes test results in the Hochberg procedure. The focus of the present paper is on developing normal theory analogs of the abovementioned p $$ p $$ -value based tests that take into account (i) the gatekeeping effect of the test on the primary endpoint and (ii) correlations between the endpoints. The normal theory boundaries are determined by finding the least favorable configuration of the correlations and so their knowledge is not needed to apply these procedures. The p $$ p $$ -value based procedures are easy to apply but they are less powerful than their normal theory analogs because they do not take into account the correlations between the endpoints and the gatekeeping effect referred to above. On the other hand, the normal theory procedures are restricted to two secondary endpoints and two stages mainly because of computational difficulties with more than two secondary endpoints and stages. Comparisons between the two types of procedures are given in terms of secondary powers. The sensitivity of the secondary type I error rate and power to unequal information times is studied. Numerical examples and a real case study illustrate the procedures.},
  archive      = {J_SIM},
  author       = {Ajit C. Tamhane and Dong Xi and Cyrus R. Mehta and Alexander Romanenko and Jiangtao Gou},
  doi          = {10.1002/sim.10346},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10346},
  shortjournal = {Stat. Med.},
  title        = {Testing one primary and two secondary endpoints in a two-stage group sequential trial with extensions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Allocation predictability of individual assignments in restricted randomization designs for two-arm equal allocation trials. <em>SIM</em>, <em>44</em>(3-4), e10343. (<a href='https://doi.org/10.1002/sim.10343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript derives the allocation predictability measured by the correct guess probability and the probability of being deterministic for individual treatment assignments, as well as the averages of a randomization sequence, based on the treatment imbalance transition matrix and the conditional allocation probability. The methods described are applicable to restricted randomization designs that satisfy the following criteria: (1) two-arm equal allocation, (2) restriction of maximum tolerated imbalance, and (3) conditional allocation probability fully determined by the observed current treatment imbalance. Analytical results indicate that, for two-arm equal allocation trials, allocation predictability alternates by the odd/even sequence order of the treatment assignment. Additionally, the sequence average allocation predictability converges to its asymptotic value significantly more slowly than the allocation predictability for individual assignment does. Consequently, comparisons of allocation predictability between different randomization designs based on sequence averages are sensitive to sequence length. Using sequence average allocation predictability may underestimate the risk of selection bias for individual assignment. This discrepancy is particularly pronounced for short sequence lengths, where individual assignment predictability can be substantially higher than the sequence average.},
  archive      = {J_SIM},
  author       = {Wenle Zhao and Sherry Livingston},
  doi          = {10.1002/sim.10343},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10343},
  shortjournal = {Stat. Med.},
  title        = {Allocation predictability of individual assignments in restricted randomization designs for two-arm equal allocation trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matching-assisted power prior for incorporating real-world data in randomized clinical trial analysis. <em>SIM</em>, <em>44</em>(3-4), e10342. (<a href='https://doi.org/10.1002/sim.10342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging external data information to supplement randomized clinical trials has been a popular topic in recent years, especially for medical device and drug discovery. In rare diseases, it is very challenging to recruit patients and run a large-scale randomized trial. To take advantage of real-world data from historical trials on the same disease, we can run a small hybrid trial and borrow historical controls to increase the power. But the borrowing needs to be conducted in a statistically principled manner. Bayesian power prior methods and propensity score adjustments have been discussed in the literature. In this paper, we propose a matching-assisted power prior approach to better mitigate observed bias when incorporating external data. A subset of comparable external subjects is selected by groups through template matching, and different weights are assigned to these groups based on their similarity to the current study population. Power priors are then implemented to incorporate the information into Bayesian inference. Unlike conventional power prior methods, which discount all control patients similarly, matching pre-selects good controls, hence improved the quality of external data being borrowed. We compare its performance with the existing propensity score-integrated power prior approach through simulation studies and illustrate the implementation using data from a real acupuncture clinical trial.},
  archive      = {J_SIM},
  author       = {Ruoyuan Qian and Biqing Yang and Xinyi Xu and Bo Lu},
  doi          = {10.1002/sim.10342},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10342},
  shortjournal = {Stat. Med.},
  title        = {Matching-assisted power prior for incorporating real-world data in randomized clinical trial analysis},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reverse time-to-death as time-scale in time-to-event analysis for studies of advanced illness and palliative care. <em>SIM</em>, <em>44</em>(3-4), e10338. (<a href='https://doi.org/10.1002/sim.10338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incidence of adverse outcome events rises as patients with advanced illness approach end-of-life. Exposures that tend to occur near end-of-life, for example, use of wheelchair, oxygen therapy and palliative care, may therefore be found associated with the incidence of the adverse outcomes. We propose a concept of reverse time-to-death (rTTD) and its use for the time-scale in time-to-event analysis based on partial likelihood to mitigate the time-varying confounding. We used data on community-based palliative care uptake (exposure) and emergency department visits (outcome) among patients with advanced cancer in Singapore to illustrate. We compare the results against that of the common practice of using time-on-study (TOS) as time-scale. Graphical analysis demonstrated that cancer patients receiving palliative care had higher rate of emergency department visits than non-recipients mainly because they were closer to end-of-life, and that rTTD analysis made comparison between patients at the same time-to-death. In analysis of a decedent cohort, emergency department visits in relation to palliative care using TOS time-scale showed significant increase in hazard ratio estimate when observed time-varying covariates were omitted from statistical adjustment (% change-in-estimate = 16.2%; 95% CI 6.4% to 25.6%). There was no such change in otherwise the same analysis using rTTD (% change-in-estimate = 3.1%; 95% CI -1.0% to 8.5%), demonstrating the ability of rTTD time-scale to mitigate confounding that intensifies in relation to time-to-death. A similar pattern was found in the full cohort. Simulations demonstrated that the proposed method had smaller relative bias and root mean square error than TOS-based analysis. In conclusion, use of rTTD as time-scale in time-to-event analysis provides a simple and robust approach to control time-varying confounding in studies of advanced illness, even if the confounders are unmeasured.},
  archive      = {J_SIM},
  author       = {Yin Bun Cheung and Xiangmei Ma and Isha Chaudhry and Nan Liu and Qingyuan Zhuang and Grace Meijuan Yang and Chetna Malhotra and Eric Andrew Finkelstein},
  doi          = {10.1002/sim.10338},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10338},
  shortjournal = {Stat. Med.},
  title        = {Reverse time-to-death as time-scale in time-to-event analysis for studies of advanced illness and palliative care},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On inclusion of covariates in model based dose finding clinical trial designs. <em>SIM</em>, <em>44</em>(3-4), e10337. (<a href='https://doi.org/10.1002/sim.10337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing number of Phase I dose-finding studies that use a model-based approach, such as the CRM or the EWOC method to estimate the dose-toxicity relationship. It is common to assume that all patients will have similar toxicity risk given the dose regardless of patients' individual characteristics. In many trials, however, some patients' covariates (e.g., a concomitant drug assigned by a clinician) might have an impact on the dose-toxicity relationship. In this work, motivated by a real trial, we evaluate an impact of taking into account (or omitting) some patients' covariates on the individual target dose recommendations and patients' safety in Phase I model-based dose-finding study. We investigate several variable penalisation criteria and found that, for continuous and binary covariates, omitting a prognostic covariate leads to a drastically low proportion of correct selections and an increase of overdosing. At the same time, including a covariate can lead to good operating characteristics in all scenarios but can sometimes slightly decrease the proportion of good selections and increase the overdosing. To tackle this, we propose to use a Bayesian Lasso Bayesian Logistic Regression Model (BLRM) and Spike-and-Slab BLRM. We have found that the BLRM coupled to the Bayesian LASSO and the BLRM with Spike-and-Slab are on average better appropriate to consider variable inclusion.},
  archive      = {J_SIM},
  author       = {Adrien Ollier and Pavel Mozgunov},
  doi          = {10.1002/sim.10337},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10337},
  shortjournal = {Stat. Med.},
  title        = {On inclusion of covariates in model based dose finding clinical trial designs},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating meta-learners to analyze treatment heterogeneity in survival data: Application to electronic health records of pediatric asthma care in COVID-19 pandemic. <em>SIM</em>, <em>44</em>(3-4), e10333. (<a href='https://doi.org/10.1002/sim.10333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important aspect of precision medicine focuses on characterizing diverse responses to treatment due to unique patient characteristics, also known as heterogeneous treatment effects (HTE) or individualized treatment effects (ITE), and identifying beneficial subgroups with enhanced treatment effects. Estimating HTE with right-censored data in observational studies remains challenging. In this paper, we propose a pseudo-ITE-based framework for analyzing HTE in survival data, which includes a group of meta-learners for estimating HTE, a variable importance metric for identifying predictive variables to HTE, and a data-adaptive procedure to select subgroups with enhanced treatment effects. We evaluate the finite sample performance of the framework under various observational study settings. Furthermore, we applied the proposed methods to analyze the treatment heterogeneity of a written asthma action plan (WAAP) on time-to-ED (Emergency Department) return due to asthma exacerbation using a large asthma electronic health records dataset with visit records expanded from pre- to post-COVID-19 pandemic. We identified vulnerable subgroups of patients with poorer asthma outcomes but enhanced benefits from WAAP and characterized patient profiles. Our research provides valuable insights for healthcare providers on the strategic distribution of WAAP, particularly during disruptive public health crises, ultimately improving the management and control of pediatric asthma.},
  archive      = {J_SIM},
  author       = {Na Bo and Jong-Hyeon Jeong and Erick Forno and Ying Ding},
  doi          = {10.1002/sim.10333},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10333},
  shortjournal = {Stat. Med.},
  title        = {Evaluating meta-learners to analyze treatment heterogeneity in survival data: Application to electronic health records of pediatric asthma care in COVID-19 pandemic},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-parametric estimation for semi-competing risks data with event misascertainment. <em>SIM</em>, <em>44</em>(3-4), e10332. (<a href='https://doi.org/10.1002/sim.10332'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semi-competing risks data model is a special type of disease-state model that focuses on studying the association between an intermediate event and a terminal event and proves to be a useful tool in modeling disease progression. The study of the semi-competing risk data model not only allows us to evaluate whether a disease episode is related to death but also provides a toolkit to predict death, given that the episode occurred at a certain time. However, the computation of the semi-competing risk models is a numerically challenging task. The Gamma-Frailty conditional Markov model has been shown to be an efficient computation model for studying semi-competing risks data. Building on recent advances in studying semi-competing risks data, this work proposes a non-parametric pseudo-likelihood method equipped with an EM-like algorithm to study semi-competing risks data with event misascertainment under the restricted Gamma-Frailty conditional Markov model. A thorough simulation study is conducted to demonstrate the inference validity of the proposed method and its numerical stability. The proposed method is applied to a large HIV cohort study, EA-IeDEA, that has a severe death under-reporting issue to assess the degree of adverse impact of the interruption of ART care on HIV mortality.},
  archive      = {J_SIM},
  author       = {Ruiqian Wu and Ying Zhang and Giorgos Bakoyannis},
  doi          = {10.1002/sim.10332},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10332},
  shortjournal = {Stat. Med.},
  title        = {Non-parametric estimation for semi-competing risks data with event misascertainment},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-label classification with gene-environment interactions in disease modeling. <em>SIM</em>, <em>44</em>(3-4), e10330. (<a href='https://doi.org/10.1002/sim.10330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, gene-environment (G-E) interactions have been demonstrated to have important implications for analyzing disease outcomes beyond the main G and main E effects. Many approaches have been developed for G-E interaction analysis, yielding important findings. However, hierarchical multi-label classification, which provides insightful information on disease outcomes, remains unexplored in G-E analysis literature. Moreover, unlabeled data are commonly observed in practical settings but omitted by many existing methods of hierarchical multi-label classification. In this study, we consider a semi-supervised scenario and develop a novel approach for the two-layer hierarchical response with G-E interactions. A two-step penalized estimation is then proposed using an efficient expectation-maximization (EM) algorithm. Simulation shows that it has superior performance in classification and feature selection. The analysis of The Cancer Genome Atlas (TCGA) data on lung cancer demonstrates the practical utility of the proposed method. Overall, this study can fill the important knowledge gap in G-E interaction analysis by providing a widely applicable framework for hierarchical multi-label classification of complex disease outcomes.},
  archive      = {J_SIM},
  author       = {Jingmao Li and Qingzhao Zhang and Shuangge Ma and Kuangnan Fang and Yaqing Xu},
  doi          = {10.1002/sim.10330},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10330},
  shortjournal = {Stat. Med.},
  title        = {Hierarchical multi-label classification with gene-environment interactions in disease modeling},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample size adjustment in sequential multiple assignment randomized trials. <em>SIM</em>, <em>44</em>(3-4), e10328. (<a href='https://doi.org/10.1002/sim.10328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are often designed based on limited information about effect sizes and precision parameters with risks of underpowered studies. This is more problematic for SMARTs where strategy effects are based on sequences of treatments. Sample size adjustment offers flexibility through re-estimating sample size during the trial to ensure adequate power at the final analysis. While this adaptation is common for standard clinical trials, corresponding methods to perform sample size adjustment have not been adapted to SMARTs. In this paper, we propose a sample size adjustment procedure for SMARTs. Sample sizes are re-calculated at the interim analysis based on the conditional power derived from a bivariate non-central chi-square distribution. We demonstrate through simulation studies that even with an underpowered initial sample size due to miss-specified parameters at the design stage, the proposed method can maintain desirable power at the end of the study, and additional resources are only invested in trials that show promising conditional power at the interim analysis.},
  archive      = {J_SIM},
  author       = {Liwen Wu and Junyao Wang and Abdus S. Wahed},
  doi          = {10.1002/sim.10328},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10328},
  shortjournal = {Stat. Med.},
  title        = {Sample size adjustment in sequential multiple assignment randomized trials},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric path-specific effects on a survival outcome through multiple time-to-event mediators. <em>SIM</em>, <em>44</em>(3-4), e10327. (<a href='https://doi.org/10.1002/sim.10327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A causal mediation model with multiple time-to-event mediators is exemplified by the natural course of human disease marked by sequential milestones with a time-to-event nature. For example, from hepatitis B infection to death, patients may experience intermediate events such as liver cirrhosis and liver cancer. The sequential events of hepatitis, cirrhosis, cancer, and death are susceptible to right censoring; moreover, the latter events may preclude the former events. Casting the natural course of human diseases in the framework of causal mediation modeling, we establish a model with intermediate and terminal events as the mediators and outcomes, respectively. We define the interventional analog of path-specific effects (iPSEs) as the effect of an exposure on a terminal event mediated (or not mediated) by any combination of intermediate events without parametric models. The expression of a counting process-based counterfactual hazard is derived under the sequential ignorability assumption. We employ composite nonparametric likelihood estimation to obtain maximum likelihood estimators for the counterfactual hazard and iPSEs. Our proposed estimators achieve asymptotic unbiasedness, uniform consistency, and weak convergence. Applying the proposed method, we show that hepatitis B induced mortality is mostly mediated through liver cancer and/or cirrhosis whereas hepatitis C induced mortality may be through extrahepatic diseases.},
  archive      = {J_SIM},
  author       = {Yen-Tsung Huang and Ju-Sheng Hong},
  doi          = {10.1002/sim.10327},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10327},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric path-specific effects on a survival outcome through multiple time-to-event mediators},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Between- and within-cluster spearman rank correlations. <em>SIM</em>, <em>44</em>(3-4), e10326. (<a href='https://doi.org/10.1002/sim.10326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered data are common in practice. Clustering arises when subjects are measured repeatedly, or subjects are nested in groups (e.g., households, schools). It is often of interest to evaluate the correlation between two variables with clustered data. There are three commonly used Pearson correlation coefficients (total, between-, and within-cluster), which together provide an enriched perspective of the correlation. However, these Pearson correlation coefficients are sensitive to extreme values and skewed distributions. They also vary with data transformation, which is arbitrary and often difficult to choose, and they are not applicable to ordered categorical data. Current nonparametric correlation measures for clustered data are only for the total correlation. Here we define population parameters for the between- and within-cluster Spearman rank correlations. The definitions are natural extensions of the Pearson between- and within-cluster correlations to the rank scale. We show that the total Spearman rank correlation approximates a linear combination of the between- and within-cluster Spearman rank correlations, where the weights are functions of rank intraclass correlations of the two random variables. We also discuss the equivalence between the within-cluster Spearman rank correlation and the covariate-adjusted partial Spearman rank correlation. Furthermore, we describe estimation and inference for the three Spearman rank correlations, conduct simulations to evaluate the performance of our estimators, and illustrate their use with data from a longitudinal biomarker study and a clustered randomized trial.},
  archive      = {J_SIM},
  author       = {Shengxin Tu and Chun Li and Bryan E. Shepherd},
  doi          = {10.1002/sim.10326},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10326},
  shortjournal = {Stat. Med.},
  title        = {Between- and within-cluster spearman rank correlations},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian multivariate model with temporal dependence on random partition of areal data for mosquito-borne diseases. <em>SIM</em>, <em>44</em>(3-4), e10325. (<a href='https://doi.org/10.1002/sim.10325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More than half of the world's population is exposed to mosquito-borne diseases, leading to millions of cases and hundreds of thousands of deaths every year. Analyzing this type of data is complex and poses several interesting challenges, mainly due to the usually vast geographic area involved, the peculiar temporal behavior, and the potential correlation between infections. Motivation for this work stems from the analysis of tropical disease data, namely, the number of cases of dengue and chikungunya, for the 145 microregions in Southeast Brazil from 2018 to 2022. As a contribution to the literature on multivariate disease data, we develop a flexible Bayesian multivariate spatio-temporal model where temporal dependence is defined for areal clusters. The model features a prior distribution for the random partition of areal data that incorporates neighboring information. It also incorporates an autoregressive structure and terms related to seasonal patterns into temporal components that are disease- and cluster-specific. Furthermore, it considers a multivariate directed acyclic graph autoregressive structure to accommodate spatial and inter-disease dependence. We explore the properties of the model through simulation studies and show results that prove our proposal compares well to competing alternatives. Finally, we apply the model to the motivating dataset with a twofold goal: finding clusters of areas with similar temporal trends for some of the diseases and exploring the existence of correlation between two diseases transmitted by the same mosquito.},
  archive      = {J_SIM},
  author       = {Jessica Pavani and Fernando Andrés Quintana},
  doi          = {10.1002/sim.10325},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10325},
  shortjournal = {Stat. Med.},
  title        = {A bayesian multivariate model with temporal dependence on random partition of areal data for mosquito-borne diseases},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised functional principal component analysis under the mixture cure rate model: An application to Alzheimer'S disease. <em>SIM</em>, <em>44</em>(3-4), e10324. (<a href='https://doi.org/10.1002/sim.10324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain imaging data is one of the primary predictors for assessing the risk of Alzheimer's disease (AD). This study aims to extract image-based features associated with the possibly right-censored time-to-event outcomes and to improve predictive performance. While the functional proportional hazards model is well-studied in the literature, these studies often do not consider the existence of patients who have a very low risk and are approximately insusceptible to AD. We introduce a functional mixture cure rate model that extends the proportional hazards model by allowing a proportion of event-free patients. We propose a novel supervised functional principal component analysis (sFPCA) method to extract image features associated with AD risk while accounting for the complexity arising from right censoring. The proposed method accommodates the irregular boundary issue inherent in brain images with bivariate splines over triangulations. We demonstrate the advantages of the proposed method through extensive simulation studies and provide an application to the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.},
  archive      = {J_SIM},
  author       = {Jiahui Feng and Haolun Shi and Da Ma and Mirza Faisal Beg and Jiguo Cao},
  doi          = {10.1002/sim.10324},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10324},
  shortjournal = {Stat. Med.},
  title        = {Supervised functional principal component analysis under the mixture cure rate model: An application to Alzheimer'S disease},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adjusting for ascertainment bias in meta-analysis of penetrance for cancer risk. <em>SIM</em>, <em>44</em>(3-4), e10323. (<a href='https://doi.org/10.1002/sim.10323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-gene panel testing allows efficient detection of pathogenic variants in cancer susceptibility genes including moderate-risk genes such as ATM and PALB2. A growing number of studies examine the risk of breast cancer (BC) conferred by pathogenic variants of these genes. A meta-analysis combining the reported risk estimates can provide an overall estimate of age-specific risk of developing BC, that is, penetrance for a gene. However, estimates reported by case–control studies often suffer from ascertainment bias. Currently, there is no method available to adjust for such bias in this setting. We consider a Bayesian random effect meta-analysis method that can synthesize different types of risk measures and extend it to incorporate studies with ascertainment bias. This is achieved by introducing a bias term in the model and assigning appropriate priors. We validate the method through a simulation study and apply it to estimate BC penetrance for carriers of pathogenic variants in the ATM and PALB2 genes. Our simulations show that the proposed method results in more accurate and precise penetrance estimates compared to when no adjustment is made for ascertainment bias or when such biased studies are discarded from the analysis. The overall estimated BC risk for individuals with pathogenic variants are (1) 5.77% (3.22%–9.67%) by age 50 and 26.13% (20.31%–32.94%) by age 80 for ATM; (2) 12.99% (6.48%–22.23%) by age 50, and 44.69% (34.40%–55.80%) by age 80 for PALB2. The proposed method allows meta-analyses to include studies with ascertainment bias, resulting in inclusion of more studies and thereby more accurate estimates.},
  archive      = {J_SIM},
  author       = {Thanthirige Lakshika M. Ruberu and Danielle Braun and Giovanni Parmigiani and Swati Biswas},
  doi          = {10.1002/sim.10323},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10323},
  shortjournal = {Stat. Med.},
  title        = {Adjusting for ascertainment bias in meta-analysis of penetrance for cancer risk},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian joint model of multiple nonlinear longitudinal and competing risks outcomes for dynamic prediction in multiple myeloma: Joint estimation and corrected two-stage approaches. <em>SIM</em>, <em>44</em>(3-4), e10322. (<a href='https://doi.org/10.1002/sim.10322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting cancer-associated clinical events is challenging in oncology. In Multiple Myeloma (MM), a cancer of plasma cells, disease progression is determined by changes in biomarkers, such as serum concentration of the paraprotein secreted by plasma cells (M-protein). Therefore, the time-dependent behavior of M-protein and the transition across lines of therapy (LoT), which may be a consequence of disease progression, should be accounted for in statistical models to predict relevant clinical outcomes. Furthermore, it is important to understand the contribution of the patterns of longitudinal biomarkers, upon each LoT initiation, to time-to-death or time-to-next-LoT. Motivated by these challenges, we propose a Bayesian joint model for trajectories of multiple longitudinal biomarkers, such as M-protein, and the competing risks of death and transition to the next LoT. Additionally, we explore two estimation approaches for our joint model: simultaneous estimation of all parameters (joint estimation) and sequential estimation of parameters using a corrected two-stage strategy aiming to reduce computational time. Our proposed model and estimation methods are applied to a retrospective cohort study from a real-world database of patients diagnosed with MM in the US from January 2015 to February 2022. We split the data into training and test sets in order to validate the joint model using both estimation approaches and make dynamic predictions of times until clinical events of interest, informed by longitudinally measured biomarkers and baseline variables available up to the time of prediction.},
  archive      = {J_SIM},
  author       = {Danilo Alvares and Jessica K. Barrett and François Mercier and Spyros Roumpanis and Sean Yiu and Felipe Castro and Jochen Schulze and Yajing Zhu},
  doi          = {10.1002/sim.10322},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10322},
  shortjournal = {Stat. Med.},
  title        = {A bayesian joint model of multiple nonlinear longitudinal and competing risks outcomes for dynamic prediction in multiple myeloma: Joint estimation and corrected two-stage approaches},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bioequivalence design with sampling distribution segments. <em>SIM</em>, <em>44</em>(3-4), e10321. (<a href='https://doi.org/10.1002/sim.10321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In bioequivalence design, power analyses dictate how much data must be collected to detect the absence of clinically important effects. Power is computed as a tail probability in the sampling distribution of the pertinent test statistics. When these test statistics cannot be constructed from pivotal quantities, their sampling distributions are approximated via repetitive, time-intensive computer simulation. We propose a novel simulation-based method to quickly approximate the power curve for many such bioequivalence tests by efficiently exploring segments (as opposed to the entirety) of the relevant sampling distributions. Despite not estimating the entire sampling distribution, this approach prompts unbiased sample size recommendations. We illustrate this method using two-group bioequivalence tests with unequal variances and overview its broader applicability in clinical design. All methods proposed in this work can be implemented using the developed dent package in R.},
  archive      = {J_SIM},
  author       = {Luke Hagar and Nathaniel T. Stevens},
  doi          = {10.1002/sim.10321},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10321},
  shortjournal = {Stat. Med.},
  title        = {Bioequivalence design with sampling distribution segments},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The harms of class imbalance corrections for machine learning based prediction models: A simulation study. <em>SIM</em>, <em>44</em>(3-4), e10320. (<a href='https://doi.org/10.1002/sim.10320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Alex Carriero and Kim Luijken and Anne de Hond and Karel G. M. Moons and Ben van Calster and Maarten van Smeden},
  doi          = {10.1002/sim.10320},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10320},
  shortjournal = {Stat. Med.},
  title        = {The harms of class imbalance corrections for machine learning based prediction models: A simulation study},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A varying-coefficient additive hazard model for recurrent events data. <em>SIM</em>, <em>44</em>(3-4), e10319. (<a href='https://doi.org/10.1002/sim.10319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The additive hazard model, which focuses on risk differences rather than risk ratios, has been widely applied in practice. In this paper, we consider an additive hazard model with varying coefficients to analyze recurrent events data. The model allows for both varying and constant coefficients. We first propose an estimating equation-based approach with spline basis smoothing for all functional coefficients. Then, we provide theoretical justifications for the resulting estimates, including consistency, rate of convergence, and asymptotic distribution. Furthermore, we construct a Cramér–von Mises test procedure to investigate whether the functional coefficients should be treated as constant, and its asymptotic null distribution is also derived. Extensive simulation experiments are conducted to evaluate the finite-sample performance of the proposed approaches. A Chronic Granulotamous Disease data set was analyzed to illustrate our methodology.},
  archive      = {J_SIM},
  author       = {Zhao Da and Xia Xiaochao and Li Jialiang},
  doi          = {10.1002/sim.10319},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10319},
  shortjournal = {Stat. Med.},
  title        = {A varying-coefficient additive hazard model for recurrent events data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of diagnostic test accuracy without gold standards. <em>SIM</em>, <em>44</em>(3-4), e10315. (<a href='https://doi.org/10.1002/sim.10315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ideal evaluation of diagnostic test performance requires a reference test that is free of errors. However, for many diseases, obtaining such a “gold standard” reference is either impossible or prohibitively expensive. Estimating test accuracy in the absence of a gold standard is therefore a significant challenge. In this article, we introduce and categorize existing methods for evaluating diagnostic tests without a gold standard, considering factors such as the type and number of tests, as well as the structure of the observed data. For each method, we provide a comprehensive introduction and analysis of its underlying assumptions, model architecture, identifiability, estimation techniques, and inference procedures. We use R to conduct simulations for widely applicable models, validating assumptions, comparing models, and assessing their reliability. Additionally, we present real-world examples along with the corresponding R code for these models, enabling readers to better understand how to apply them effectively. Beyond diagnostic medicine, we underscore that the issue of imperfect gold standards affects other fields, drawing parallels to the noisy label problem in machine learning. By highlighting similarities and differences across these domains, we open pathways for further research. The primary aim of this article is to consolidate existing methods for assessing test accuracy in the absence of a gold standard and to provide practical guidance for researchers seeking to apply these methods effectively.},
  archive      = {J_SIM},
  author       = {Ao Sun and Xiao-Hua Zhou},
  doi          = {10.1002/sim.10315},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10315},
  shortjournal = {Stat. Med.},
  title        = {Estimation of diagnostic test accuracy without gold standards},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the effectiveness of sample size re-estimation: An operating characteristic focused, hybrid frequentist-bayesian approach. <em>SIM</em>, <em>44</em>(3-4), e10310. (<a href='https://doi.org/10.1002/sim.10310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample size re-estimation (SSR) is perhaps the most used adaptive procedure in both frequentist and Bayesian adaptive designs for clinical trials. The primary focus of all current frequentist and Bayesian SSR procedures is type I error control. We propose a hybrid frequentist-Bayesian SSR approach that focuses on optimizing operating characteristics (OC), which uses simulations to investigate the associated OC and adjusts accordingly. The hybrid approach incorporates the Bayesian predictive power into the frequentist framework of SSR. Simulations show that the hybrid approach can substantially outperform popular frequentist type error-focused SSR procedure. The hybrid approach can substantially improve the effectiveness of SSR using Bayesian predictive power.},
  archive      = {J_SIM},
  author       = {Ping Gao},
  doi          = {10.1002/sim.10310},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10310},
  shortjournal = {Stat. Med.},
  title        = {Improving the effectiveness of sample size re-estimation: An operating characteristic focused, hybrid frequentist-bayesian approach},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing covariates effects on bivariate reference regions. <em>SIM</em>, <em>44</em>(3-4), e10308. (<a href='https://doi.org/10.1002/sim.10308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated clinical measurements are routinely interpreted via comparisons with univariate reference intervals examined side by side. Multivariate reference regions (MVRs), i.e., regions that characterize the distribution of multivariate results, have been proposed as a more adequate interpretation tool in such situations. However, MVR estimation methods have not yet been fully developed and are rarely used by physicians. The multivariate distribution of correlated measurements might change with certain patient characteristics (e.g., age or gender), and their effect on the shape of an MVR can be complex, involving interaction terms. For instance, the reference region shape for a given set of continuous covariates might vary across groups with respect to the value of a categorical variable. This paper examines the use of a bootstrap-based hypothesis test for examining the effect of covariates on bivariate reference regions, testing the effect of factor-by-region interactions. An estimation algorithm based on smoothing splines was used to construct the bivariate reference region for a pediatric anthropometric dataset, and the bootstrapping procedure was used to determine the effect of age and gender on the shape of the reference region. (Height, weight) bivariate distribution was shown to depend on the interaction between age and gender. The bootstrapping procedure confirmed that a bivariate growth chart is desirable over univariate age-gender body mass index (BMI) percentile curves. Whereas the well-known BMI criterion detects only two atypical situations (i.e., underweight, overweight), the bootstrap-tested bivariate reference region detected abnormally large or small body frames for different ages and genders.},
  archive      = {J_SIM},
  author       = {Óscar Lado-Baleato and Javier Roca-Pardiñas and Carmen Cadarso-Suárez and Francisco Gude},
  doi          = {10.1002/sim.10308},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10308},
  shortjournal = {Stat. Med.},
  title        = {Testing covariates effects on bivariate reference regions},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian borrowing with multiple heterogeneous historical studies using order restricted normalized power prior. <em>SIM</em>, <em>44</em>(3-4), e10302. (<a href='https://doi.org/10.1002/sim.10302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent U.S. Food and Drug Administration guidance on complex innovative trial designs acknowledges the use of Bayesian strategies to incorporate historical information based on clinical expertise and data similarity. Also, data from multiple previous studies with similar settings often qualify for historical borrowing. Although several classes of informative priors can semi-automatically leverage historical information based on data compatibility, it is common that some exogenous factors, such as the year of patient enrollment, can also influence the relevance of each historical study to the current trial. Consequently, a natural a priori ordering among historical trials often arises, a constraint that many current informative priors fail to accommodate. Motivated by a pediatric lupus clinical study and an oncology trial, we introduce a variant of the power prior, named the ordered normalized power prior , which ensures a targeted order restriction on the power parameters and maintains data-adaptive borrowing. We further explore and compare two distinct normalization strategies and outline computational details with efficient sampling algorithms. The clinical datasets mentioned are analyzed, and extensive simulations are conducted for comparison. An efficient implementation is provided in our updated package NPP available on the Comprehensive R Archive Network.},
  archive      = {J_SIM},
  author       = {Zifei Han and Qiang Zhang and Ram Tiwari and Tianyu Bai},
  doi          = {10.1002/sim.10302},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10302},
  shortjournal = {Stat. Med.},
  title        = {Bayesian borrowing with multiple heterogeneous historical studies using order restricted normalized power prior},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reference-based multiple imputation for longitudinal binary data. <em>SIM</em>, <em>44</em>(3-4), e10301. (<a href='https://doi.org/10.1002/sim.10301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Suzie Cro and Matteo Quartagno and Ian R. White and James R. Carpenter},
  doi          = {10.1002/sim.10301},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10301},
  shortjournal = {Stat. Med.},
  title        = {Reference-based multiple imputation for longitudinal binary data},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian analysis of censored linear mixed-effects models for heavy-tailed irregularly observed repeated measures. <em>SIM</em>, <em>44</em>(3-4), e10295. (<a href='https://doi.org/10.1002/sim.10295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of mixed-effect models to understand the evolution of the human immunodeficiency virus (HIV) and the progression of acquired immune deficiency syndrome (AIDS) has been the cornerstone of longitudinal data analysis in recent years. However, data from HIV/AIDS clinical trials have several complexities. Some of the most common recurrences are related to the situation where the HIV viral load can be undetectable, and the measures of the patient can be registered irregularly due to some problems in the data collection. Although censored mixed-effects models assuming conditionally independent normal random errors are commonly used to analyze this data type, this model may need to be more appropriate for accommodating outlying observations and responses recorded at irregular intervals. Consequently, in this paper, we propose a Bayesian analysis of censored linear mixed-effects models that replace Gaussian assumptions with a flexible class of distributions, such as the scale mixture of normal family distributions, considering a damped exponential correlation structure that was employed to account for within-subject autocorrelation among irregularly observed measures. For this complex structure, Stan 's default No-U-Turn sampler is utilized to obtain posterior simulations. The feasibility of the proposed methods was demonstrated through several simulation studies and their application to two AIDS case studies.},
  archive      = {J_SIM},
  author       = {Kelin Zhong and Fernanda L. Schumacher and Luis M. Castro and Víctor H. Lachos},
  doi          = {10.1002/sim.10295},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10295},
  shortjournal = {Stat. Med.},
  title        = {Bayesian analysis of censored linear mixed-effects models for heavy-tailed irregularly observed repeated measures},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comments on “Sample size adaptation designs and efficiency comparison with group sequential designs”. <em>SIM</em>, <em>44</em>(3-4), e10294. (<a href='https://doi.org/10.1002/sim.10294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Meinhard Kieser and Maximilian Pilz},
  doi          = {10.1002/sim.10294},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10294},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Sample size adaptation designs and efficiency comparison with group sequential designs”},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized bayesian stochastic block model for microbiome community detection. <em>SIM</em>, <em>44</em>(3-4), e10291. (<a href='https://doi.org/10.1002/sim.10291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in next-generation sequencing technology have enabled the high-throughput profiling of metagenomes and accelerated microbiome studies. Recently, there has been a rise in quantitative studies that aim to decipher the microbiome co-occurrence network and its underlying community structure based on metagenomic sequence data. Uncovering the complex microbiome community structure is essential to understanding the role of the microbiome in disease progression and susceptibility. Taxonomic abundance data generated from metagenomic sequencing technologies are high-dimensional and compositional, suffering from uneven sampling depth, over-dispersion, and zero-inflation. These characteristics often challenge the reliability of the current methods for microbiome community detection. To study the microbiome co-occurrence network and perform community detection, we propose a generalized Bayesian stochastic block model that is tailored for microbiome data analysis where the data are transformed using the recently developed modified centered-log ratio transformation. Our model also allows us to leverage taxonomic tree information using a Markov random field prior. The model parameters are jointly inferred by using Markov chain Monte Carlo sampling techniques. Our simulation study showed that the proposed approach performs better than competing methods even when taxonomic tree information is non-informative. We applied our approach to a real urinary microbiome dataset from postmenopausal women. To the best of our knowledge, this is the first time the urinary microbiome co-occurrence network structure in postmenopausal women has been studied. In summary, this statistical methodology provides a new tool for facilitating advanced microbiome studies.},
  archive      = {J_SIM},
  author       = {Kevin C. Lutz and Michael L. Neugent and Tejasv Bedi and Nicole J. De Nisco and Qiwei Li},
  doi          = {10.1002/sim.10291},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10291},
  shortjournal = {Stat. Med.},
  title        = {A generalized bayesian stochastic block model for microbiome community detection},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple sensitivity analysis method for unmeasured confounders via linear programming with estimating equation constraints. <em>SIM</em>, <em>44</em>(3-4), e10288. (<a href='https://doi.org/10.1002/sim.10288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In estimating the average treatment effect in observational studies, the influence of confounders should be appropriately addressed. To this end, the propensity score is widely used. If the propensity scores are known for all the subjects, bias due to confounders can be adjusted by using the inverse probability weighting (IPW) by the propensity score. Since the propensity score is unknown in general, it is usually estimated by the parametric logistic regression model with unknown parameters estimated by solving the score equation under the strongly ignorable treatment assignment (SITA) assumption. Violation of the SITA assumption and/or misspecification of the propensity score model can cause serious bias in estimating the average treatment effect (ATE). To relax the SITA assumption, the IPW estimator based on the outcome-dependent propensity score has been successfully introduced. However, it still depends on the correctly specified parametric model and its identification. In this paper, we propose a simple sensitivity analysis method for unmeasured confounders. In the standard practice, the estimating equation is used to estimate the unknown parameters in the parametric propensity score model. Our idea is to make inferences on the (ATE) by removing restrictive parametric model assumptions while still utilizing the estimating equation. Using estimating equations as constraints, which the true propensity scores asymptotically satisfy, we construct the worst-case bounds for the ATE with linear programming. Differently from the existing sensitivity analysis methods, we construct the worst-case bounds with minimal assumptions. We illustrate our proposal by simulation studies and a real-world example.},
  archive      = {J_SIM},
  author       = {Chengyao Tang and Yi Zhou and Ao Huang and Satoshi Hattori},
  doi          = {10.1002/sim.10288},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10288},
  shortjournal = {Stat. Med.},
  title        = {A simple sensitivity analysis method for unmeasured confounders via linear programming with estimating equation constraints},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian spatial relative survival model to estimate the loss in life expectancy and crude probability of death for cancer patients. <em>SIM</em>, <em>44</em>(3-4), e10287. (<a href='https://doi.org/10.1002/sim.10287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date, there have not been any population-based cancer studies quantifying geographical patterns of the loss in life expectancy (LLE) and crude probability of death due to cancer ( C r C $$ C{r}_C $$ ). These absolute measures of survival are complementary to the more typically used relative measures of excess mortality and relative survival, and, together, they provide a fuller understanding of geographical disparities in survival outcomes for cancer patients. We propose using a spatially flexible parametric relative survival model in the Bayesian framework, which allows for the inclusion of spatial effects in hazard-level model components. The relative survival framework is the preferred approach to analyze cancer survival data because it does not require information on the cause of death, and the Bayesian spatial modeling approach allows complex and robust small-area estimation. The calculation of spatial estimates for LLE and C r C $$ C{r}_C $$ are demonstrated using publicly available simulated datasets. The associated computer program scripts are available to support the understanding and implementation of our methodology in other spatial cancer modelling applications.},
  archive      = {J_SIM},
  author       = {Yuliya Leontyeva and Yuxin Huang and Susanna Cramb and Jessica Cameron and Peter Baade and Kerrie Mengersen and Helen Thompson},
  doi          = {10.1002/sim.10287},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10287},
  shortjournal = {Stat. Med.},
  title        = {Bayesian spatial relative survival model to estimate the loss in life expectancy and crude probability of death for cancer patients},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple imputation for longitudinal data: A tutorial. <em>SIM</em>, <em>44</em>(3-4), e10274. (<a href='https://doi.org/10.1002/sim.10274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal studies are frequently used in medical research and involve collecting repeated measures on individuals over time. Observations from the same individual are invariably correlated and thus an analytic approach that accounts for this clustering by individual is required. While almost all research suffers from missing data, this can be particularly problematic in longitudinal studies as participation often becomes harder to maintain over time. Multiple imputation (MI) is widely used to handle missing data in such studies. When using MI, it is important that the imputation model is compatible with the proposed analysis model. In a longitudinal analysis, this implies that the clustering considered in the analysis model should be reflected in the imputation process. Several MI approaches have been proposed to impute incomplete longitudinal data, such as treating repeated measurements of the same variable as distinct variables or using generalized linear mixed imputation models. However, the uptake of these methods has been limited, as they require additional data manipulation and use of advanced imputation procedures. In this tutorial, we review the available MI approaches that can be used for handling incomplete longitudinal data, including where individuals are clustered within higher-level clusters. We illustrate implementation with replicable R and Stata code using a case study from the Childhood to Adolescence Transition Study.},
  archive      = {J_SIM},
  author       = {Rushani Wijesuriya and Margarita Moreno-Betancur and John B. Carlin and Ian R. White and Matteo Quartagno and Katherine J. Lee},
  doi          = {10.1002/sim.10274},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3-4},
  pages        = {e10274},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation for longitudinal data: A tutorial},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric estimation for propensity scores with misclassified treatments. <em>SIM</em>, <em>44</em>(1-2), e10306. (<a href='https://doi.org/10.1002/sim.10306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the framework of causal inference, average treatment effect (ATE) is one of crucial concerns. To estimate it, the propensity score based estimation method and its variants have been widely adopted. However, most existing methods were developed by assuming that binary treatments are precisely measured. In addition, propensity scores are usually formulated as parametric models with respect to confounders. However, in the presence of measurement error in binary treatments and nonlinear relationship between treatments and confounders, existing methods are no longer valid and may yield biased inference results if these features are ignored. In this paper, we first analytically examine the impact of estimation of ATE and derive biases for the estimator of ATE when treatments are contaminated with measurement error. After that, we develop a valid method to address binary treatments with misclassification. Given the corrected treatments, we adopt the random forest method to estimate the propensity score with nonlinear confounders accommodated and then derive the estimator of ATE. Asymptotic properties of the error-eliminated estimator are established. Numerical studies are also conducted to assess the finite sample performance of the proposed estimator, and numerical results verify the importance of correcting for measurement error effects.},
  archive      = {J_SIM},
  author       = {Li-Pang Chen},
  doi          = {10.1002/sim.10306},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1-2},
  pages        = {e10306},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric estimation for propensity scores with misclassified treatments},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic properties of matthews correlation coefficient. <em>SIM</em>, <em>44</em>(1-2), e10303. (<a href='https://doi.org/10.1002/sim.10303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating classifications is crucial in statistics and machine learning, as it influences decision-making across various fields, such as patient prognosis and therapy in critical conditions. The Matthews correlation coefficient (MCC), also known as the phi coefficient, is recognized as a performance metric with high reliability, offering a balanced measurement even in the presence of class imbalances. Despite its importance, there remains a notable lack of comprehensive research on the statistical inference of MCC. This deficiency often leads to studies merely validating and comparing MCC point estimates—a practice that, while common, overlooks the statistical significance and reliability of results. Addressing this research gap, our paper introduces and evaluates several methods to construct asymptotic confidence intervals for the single MCC and the differences between MCCs in paired designs. Through simulations across various scenarios, we evaluate the finite-sample behavior of these methods and compare their performances. Furthermore, through real data analysis, we illustrate the potential utility of our findings in comparing binary classifiers, highlighting the possible contributions of our research in this field.},
  archive      = {J_SIM},
  author       = {Yuki Itaya and Jun Tamura and Kenichi Hayashi and Kouji Yamamoto},
  doi          = {10.1002/sim.10303},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1-2},
  pages        = {e10303},
  shortjournal = {Stat. Med.},
  title        = {Asymptotic properties of matthews correlation coefficient},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear mixed modeling of federated data when only the mean, covariance, and sample size are available. <em>SIM</em>, <em>44</em>(1-2), e10300. (<a href='https://doi.org/10.1002/sim.10300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical research, individual-level patient data provide invaluable information, but the patients' right to confidentiality remains of utmost priority. This poses a huge challenge when estimating statistical models such as a linear mixed model, which is an extension of linear regression models that can account for potential heterogeneity whenever data come from different data providers. Federated learning tackles this hurdle by estimating parameters without retrieving individual-level data. Instead, iterative communication of parameter estimate updates between the data providers and analysts is required. In this article, we propose an alternative framework to federated learning for fitting linear mixed models. Specifically, our approach only requires the mean, covariance, and sample size of multiple covariates from different data providers once. Using the principle of statistical sufficiency within the likelihood framework as theoretical support, this proposed strategy achieves estimates identical to those derived from actual individual-level data. We demonstrate this approach through real data on 15 068 patient records from 70 clinics at the Children's Hospital of Pennsylvania. Assuming that each clinic only shares summary statistics once, we model the COVID-19 polymerase chain reaction test cycle threshold as a function of patient information. Simplicity, communication efficiency, generalisability, and wider scope of implementation in any statistical software distinguish our approach from existing strategies in the literature.},
  archive      = {J_SIM},
  author       = {Marie Analiz April Limpoco and Christel Faes and Niel Hens},
  doi          = {10.1002/sim.10300},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1-2},
  pages        = {e10300},
  shortjournal = {Stat. Med.},
  title        = {Linear mixed modeling of federated data when only the mean, covariance, and sample size are available},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smooth hazards with multiple time scales. <em>SIM</em>, <em>44</em>(1-2), e10297. (<a href='https://doi.org/10.1002/sim.10297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazard models are the most commonly used tool to analyze time-to-event data. If more than one time scale is relevant for the event under study, models are required that can incorporate the dependence of a hazard along two (or more) time scales. Such models should be flexible to capture the joint influence of several time scales, and nonparametric smoothing techniques are obvious candidates. P $$ P $$ -splines offer a flexible way to specify such hazard surfaces, and estimation is achieved by maximizing a penalized Poisson likelihood. Standard observation schemes, such as right-censoring and left-truncation, can be accommodated in a straightforward manner. Proportional hazards regression with a baseline hazard varying over two time scales is presented. Efficient computation is possible by generalized linear array model (GLAM) algorithms or by exploiting a sparse mixed model formulation. A companion R-package is provided.},
  archive      = {J_SIM},
  author       = {Angela Carollo and Paul Eilers and Hein Putter and Jutta Gampe},
  doi          = {10.1002/sim.10297},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1-2},
  pages        = {e10297},
  shortjournal = {Stat. Med.},
  title        = {Smooth hazards with multiple time scales},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple information criterion for variable selection in high-dimensional regression. <em>SIM</em>, <em>44</em>(1-2), e10275. (<a href='https://doi.org/10.1002/sim.10275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional regression problems, for example with genomic or drug exposure data, typically involve automated selection of a sparse set of regressors. Penalized regression methods like the LASSO can deliver a family of candidate sparse models. To select one, there are criteria balancing log-likelihood and model size, the most common being AIC and BIC. These two methods do not take into account the implicit multiple testing performed when selecting variables in a high-dimensional regression, which makes them too liberal. We propose the extended AIC (EAIC), a new information criterion for sparse model selection in high-dimensional regressions. It allows for asymptotic FWER control when the candidate regressors are independent. It is based on a simple formula involving model log-likelihood, model size, the total number of candidate regressors, and the FWER target. In a simulation study over a wide range of linear and logistic regression settings, we assessed the variable selection performance of the EAIC and of other information criteria (including some that also use the number of candidate regressors: mBIC, mAIC, and EBIC) in conjunction with the LASSO. Our method controls the FWER in nearly all settings, in contrast to the AIC and BIC, which produce many false positives. We also illustrate it for the automated signal detection of adverse drug reactions on the French pharmacovigilance spontaneous reporting database.},
  archive      = {J_SIM},
  author       = {Matthieu Pluntz and Cyril Dalmasso and Pascale Tubert-Bitter and Ismaïl Ahmed},
  doi          = {10.1002/sim.10275},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1-2},
  pages        = {e10275},
  shortjournal = {Stat. Med.},
  title        = {A simple information criterion for variable selection in high-dimensional regression},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On GEE for mean-variance-correlation models: Variance estimation and model selection. <em>SIM</em>, <em>44</em>(1-2), e10271. (<a href='https://doi.org/10.1002/sim.10271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized estimating equations (GEE) are of great importance in analyzing clustered data without full specification of multivariate distributions. A recent approach by Luo and Pan jointly models the mean, variance, and correlation coefficients of clustered data through three sets of regressions. We note that it represents a specific case of the more general estimating equations proposed by Yan and Fine which further allow the variance to depend on the mean through a variance function. In certain scenarios, the proposed variance estimators for the variance and correlation parameters in Luo and Pan may face challenges due to the subtle dependence induced by the nested structure of the estimating equations. We characterize specific model settings where their variance estimation approach may encounter limitations and illustrate how the variance estimators in Yan and Fine can correctly account for such dependencies. In addition, we introduce a novel model selection criterion that enables the simultaneous selection of the mean-scale-correlation model. The sandwich variance estimator and the proposed model selection criterion are tested by several simulation studies and real data analysis, which validate its effectiveness in variance estimation and model selection. Our work also extends the R package geepack with the flexibility to apply different working covariance matrices for the variance and correlation structures.},
  archive      = {J_SIM},
  author       = {Zhenyu Xu and Jason P. Fine and Wenling Song and Jun Yan},
  doi          = {10.1002/sim.10271},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1-2},
  pages        = {e10271},
  shortjournal = {Stat. Med.},
  title        = {On GEE for mean-variance-correlation models: Variance estimation and model selection},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of longitudinal lupus data using multivariate t-linear models. <em>SIM</em>, <em>44</em>(1-2), e10248. (<a href='https://doi.org/10.1002/sim.10248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of healthcare utilization, such as hospitalization duration and medical costs, is crucial for policymakers and doctors in experimental and epidemiological investigations. Herein, we examine the healthcare utilization data of patients with systemic lupus erythematosus (SLE). The characteristics of the SLE data were measured over a 10-year period with outliers. Multivariate linear models with multivariate normal error distributions are commonly used to evaluate long series of multivariate longitudinal data. However, when there are outliers or heavy tails in the data, such as those based on healthcare utilization, the assumption of multivariate normality may be too strong, resulting in biased estimates. To address this, we propose multivariate t -linear models (MTLMs) with an autoregressive moving-average (ARMA) covariance matrix. Modeling the covariance matrix for multivariate longitudinal data is difficult since the covariance matrix is high dimensional and must be positive-definite. To address these, we employ a modified ARMA Cholesky decomposition and hypersphere decomposition. Several simulation studies are conducted to demonstrate the performance, robustness, and flexibility of the proposed models. The proposed MTLMs with ARMA structured covariance matrix are applied to analyze the healthcare utilization data of patients with SLE.},
  archive      = {J_SIM},
  author       = {Eun Jin Jang and Anbin Rhee and Soo-Kyung Cho and Keunbaik Lee},
  doi          = {10.1002/sim.10248},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1-2},
  pages        = {e10248},
  shortjournal = {Stat. Med.},
  title        = {Analysis of longitudinal lupus data using multivariate t-linear models},
  volume       = {44},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
