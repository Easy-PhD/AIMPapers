<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietip">IETIP - 276</h2>
<ul>
<li><details>
<summary>
(2025). Instance-category feature representation and association learning for multi-human parsing. <em>IETIP</em>, <em>19</em>(1), e70240. (<a href='https://doi.org/10.1049/ipr2.70240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-human parsing faces critical challenges in handling small-scale accessories, distinguishing overlapping instances, and maintaining accuracy across diverse poses. We propose a unified framework with three fundamental innovations. First, unlike standard FPN, our hierarchical multi-scale feature builder employs six sequential deformable encoders with specialized encoding sets (body parts, keypoints, accessories) for adaptive multi-scale aggregation. Second, while UniParser lacks pose guidance, we pioneer the integration of whole-body pose estimation (133 keypoints) integration in single-stage parsing, providing structural constraints for accurate instance disambiguation. Third, instead of convolution-based fusion, we introduce direct indexing association in cosine space, eliminating learned parameters while achieving 22% faster inference. Extensive experiments demonstrate substantial improvements: our ResNet-101 model achieves 68.4% on CIHP, outperforming UniParser by 10.5% in , and 52.0% on MHP v2.0. Remarkably, even our ResNet-50 variant achieves 48.3% on MHP v2.0, surpassing several ResNet-101-based approaches and validating our fundamental architectural advances.},
  archive      = {J_IETIP},
  author       = {Lanqing Ye and Li Liu and Xiaodong Fu and Lijun Liu and Wei Peng},
  doi          = {10.1049/ipr2.70240},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70240},
  shortjournal = {IET Image Process.},
  title        = {Instance-category feature representation and association learning for multi-human parsing},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of a face mask detection and masked facial recognition model based on a hybrid convolutional neural network. <em>IETIP</em>, <em>19</em>(1), e70239. (<a href='https://doi.org/10.1049/ipr2.70239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of facial recognition technology has faced hindrances due to the COVID-19 pandemic, where mandatory face mask usage obscured facial features, challenging existing authentication methods. Despite the rapid development of several methods for face mask detection and recognition that highlighted prevalent issues such as poor lighting, varied angles, failed detection for improper use of face masks, computational complexity, difficulty in detecting smaller faces and low-resolution targets, these have led to suboptimal accuracy rates. Hence, this work addresses these challenges by introducing a hybrid convolutional neural network (CNN) architecture tailored for face mask detection (FMD) and masked facial recognition (MFR). The models are developed using MobileNetV2 and FaceNet InceptionResNetV1 with CNN, for FMD and MFR, respectively. Experimental results on both models utilising a total of five distinct datasets, with two for FMD and three for MFR, show the superiority of the developed model in comparison to state-of-the art models. In addition, the models are tested in real-time for both FMD and MFR to determine their robustness, efficiency and accuracy in a real-time context. For this purpose, a ‘custom real-time masked face recognition’ (CRMFR) dataset was developed to perform real-time MFR. Leveraging advanced FMD and MFR technologies, the models contribute to the real-world need for enhanced security in scenarios where traditional methods are insufficient.},
  archive      = {J_IETIP},
  author       = {Chezlyn Pillay and Seena Joseph and Brett van Niekerk},
  doi          = {10.1049/ipr2.70239},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70239},
  shortjournal = {IET Image Process.},
  title        = {Development of a face mask detection and masked facial recognition model based on a hybrid convolutional neural network},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detail loss in super-resolution models based on the laplacian pyramid and repeated upscaling and downscaling process. <em>IETIP</em>, <em>19</em>(1), e70238. (<a href='https://doi.org/10.1049/ipr2.70238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.},
  archive      = {J_IETIP},
  author       = {Sangjun Han and Youngmi Hur},
  doi          = {10.1049/ipr2.70238},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70238},
  shortjournal = {IET Image Process.},
  title        = {Detail loss in super-resolution models based on the laplacian pyramid and repeated upscaling and downscaling process},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPU-accelerated optimization of discrete ricci flow for high-resolution triangular meshes. <em>IETIP</em>, <em>19</em>(1), e70237. (<a href='https://doi.org/10.1049/ipr2.70237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discrete Ricci flow is a powerful tool for designing Riemannian metrics on surfaces by user-defined Gaussian curvatures, which has been widely applied in computer graphics, medical image processing, and computer vision. The discrete Ricci flow algorithm performs surface parameterization by dynamically adjusting edge lengths to conform to a prescribed target curvature. During execution, it requires frequent updates of mesh edge lengths, repeated construction of Hessian matrices, and solving systems of equations, resulting in substantial computational overhead. Classical methods typically employ half-edge data structures to traverse mesh edges, vertices, and faces for geometric computations. However, as model scale increases, their computational complexity grows exponentially, making large-scale model processing impractical within acceptable timeframes. To address this computational bottleneck, this paper proposes a GPU-accelerated Ricci flow computation framework. By reformulating the original iterative process into parallelizable matrix operations, this framework leverages GPU hardware advantages in parallel computing, significantly improving algorithm efficiency. Experimental results demonstrate that the GPU implementation reduces computation time substantially compared to traditional methods, with acceleration effects becoming more pronounced as model resolution increases. This approach not only provides a viable pathway for handling high-resolution mesh parameterization but also offers new insights for real-time geometric processing and interactive applications.},
  archive      = {J_IETIP},
  author       = {Zhiheng Wei and Kun Qian and Yinghua Li and Siyuan Li and Jialing Zhang and Junrong Song and Ming Yang and Ming Ni},
  doi          = {10.1049/ipr2.70237},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70237},
  shortjournal = {IET Image Process.},
  title        = {GPU-accelerated optimization of discrete ricci flow for high-resolution triangular meshes},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CPIG: Controlling the portrait image generation by distilling 3D GAN's latent directions. <em>IETIP</em>, <em>19</em>(1), e70236. (<a href='https://doi.org/10.1049/ipr2.70236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesis of 3D-aware facial images from latent spaces has garnered significant attention in multimedia content generation due to its ability to model images with rich semantics and diverse appearances. However, existing methods often rely on labeled data or suffer from incomplete attribute control and ambiguous latent space semantics. This paper proposes an efficient semantic distillation method that learns attribute directions of pre-trained 3D GAN models, without the supervised semantic labels. We consider the latent space of GAN models as the mixture of two featured subspaces, namely the geometry-aware space and appearance-aware space. Following this hypothesis, we define two sets of learnable latent bases and use linear composition to represent controllable geometry and appearance feature space, respectively. To learn semantic-wise latent bases for attribute-controllable image generation, we design a framework and propose a three-staged training strategy, which optimizes the appearance-aware and the geometry-aware latent bases. With the two sets of latent bases, we obtain the combined latent vectors using different weights for those bases and synthesize images with specified attributes. Compared to existing methods, our approach eliminates the need for labeled data and enables more controllable attribute disentanglement while ensuring identity consistency, which can be directly applied to real-world scenarios such as virtual avatars and augmented reality applications. Experiments demonstrate the effectiveness and insight of our approach in aiding a better understanding of the latent space of 3D GANs.},
  archive      = {J_IETIP},
  author       = {Ruiyan Wang and Jun Ling and Rong Xie and Li Song},
  doi          = {10.1049/ipr2.70236},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70236},
  shortjournal = {IET Image Process.},
  title        = {CPIG: Controlling the portrait image generation by distilling 3D GAN's latent directions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An image emotion classification framework based on instruction-guided triplets with descriptive captions. <em>IETIP</em>, <em>19</em>(1), e70235. (<a href='https://doi.org/10.1049/ipr2.70235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image emotion classification remains a challenging task due to the intrinsic subjectivity of emotional perception and the semantic ambiguity inherent in visual content. Although recent studies have applied language supervision to exploit semantic cues, but most methods rely on fixed templates that exhibit limited emotional relevance, while generating high-quality affective textual descriptions typically involves substantial manual effort and cost. To overcome these limitations, this paper proposes an emotion classification framework that integrates language supervision with instruction tuning. The proposed approach significantly improves emotion classification performance through three key components: (1) instruction-guided generation of descriptive emotion captions (DECs), (2) cross-modal pseudo-label construction, and (3) adaptive multimodal fusion. We design emotion-centric instructional prompts to guide large pre-trained vision-language models in generating semantically rich DECs, thereby surpassing the constraints of conventional template-based methods. Instruction tuning is further employed to generate structured emotion pseudo-labels, forming image-caption-pseudo-label triplets that strengthen cross-modal alignment. Finally, an adaptive fusion mechanism combined with a multi-branch loss function is introduced to optimize classification efficacy. Extensive experiments conducted across multiple domain-specific datasets demonstrate that our method achieves state-of-the-art accuracy. Ablation studies confirm the critical role of multimodal collaboration in enhancing model performance. Furthermore, detailed linguistic analysis shows that DECs achieve high levels of emotional expressiveness, as evidenced by their length, degree of abstraction, and affective distribution, closely approximating the quality of human-authored descriptions. This work demonstrates fine-grained emotion description generation without manual annotation and offers a potential solution for multi-source image emotion recognition.},
  archive      = {J_IETIP},
  author       = {Fuxiao Zhang and Jingjing Zhang and Chunxiao Wang and Yanhao Li},
  doi          = {10.1049/ipr2.70235},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70235},
  shortjournal = {IET Image Process.},
  title        = {An image emotion classification framework based on instruction-guided triplets with descriptive captions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-branch enhancement and multi-modal fusion for low-light visible polarization image object detection in dense smog environments. <em>IETIP</em>, <em>19</em>(1), e70234. (<a href='https://doi.org/10.1049/ipr2.70234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In scenarios with heavy smog, the accuracy of object detection in low-light visible polarization images significantly decreases. To address this issue, we propose a dual-branch enhancement and multi-modal fusion network for object detection in low-light visible polarization images in dense smog environments. Specifically, the network consists of an image enhancement stage and an object detection stage. In the image enhancement stage, a dual-branch enhancement structure comprising greyscale feature map prediction and atmospheric light transmission network is proposed to remove noise from the images and enhance texture information, jointly generating enhanced visible polarization images. In the object detection stage, feature maps of the enhanced visible polarization images and the degree of visible polarization images are fused, and their fused texture-enhanced feature maps are fed into the detection module for object detection. Additionally, we have collected a dataset of low-light visible polarization images under real smog conditions. Extensive experiments demonstrate that our method can generate visually improved enhanced images and significantly increase detection accuracy and the number of detected objects in low-light and dense smog environments.},
  archive      = {J_IETIP},
  author       = {Xin Zhang and Jingjing Zhang and Fudong Nian and Jianguo Huang and Teng Li},
  doi          = {10.1049/ipr2.70234},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70234},
  shortjournal = {IET Image Process.},
  title        = {Dual-branch enhancement and multi-modal fusion for low-light visible polarization image object detection in dense smog environments},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAHF —Net: Multi-scale alignment and high-order fusion network for unregistered infrared-visible image fusion. <em>IETIP</em>, <em>19</em>(1), e70232. (<a href='https://doi.org/10.1049/ipr2.70232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared-visible image fusion enhances perceptual quality by integrating the detailed textures of visible images with the thermal target saliency of infrared images, making it highly valuable for applications such as military reconnaissance and autonomous driving. However, existing methods encounter three major limitations: performance degradation in unregistered scenarios due to non-rigid deformations, restricted cross-modal feature interaction caused by low-order operations, and fusion distortions resulting from variations in object scale. To overcome these challenges, this study introduces a multi-scale alignment and high-order fusion network ( MAHF-Net ). Under a shared cross-modal multi-scale encoding-decoding framework, three novel components are introduced: an Infrared-Guided Spatial Deformable Alignment (ISDA) module, a High-Order Spatial-Channel Interaction Block (HOSCIB), and a Cross-Scale Dynamic Aggregation (CSDA) module. The ISDA module exploits infrared salient regions to generate spatial attention, which regulates offset fields that guide deformable convolution for adaptive non-rigid deformation correction. The HOSCIB applies a high-order spatial–channel joint interaction mechanism, enabling iterative feature refinement through dynamic spatial attention gating combined with cascaded channel attention. The CSDA module incorporates dynamically gated attention to balance semantic and detail contributions. Extensive experiments have been conducted on the RoadScene , LLVIP and AVIID datasets. The results demonstrate that the proposed MAHF-Net consistently outperforms state-of-the-art registered and unregistered fusion methods, achieving mutual information improvements of 24.6% and 4.35%, respectively.},
  archive      = {J_IETIP},
  author       = {Yixuan An and Haixiao Wu and Ning Wang and Tao Liu},
  doi          = {10.1049/ipr2.70232},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70232},
  shortjournal = {IET Image Process.},
  title        = {MAHF —Net: Multi-scale alignment and high-order fusion network for unregistered infrared-visible image fusion},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Storage-and-memory-efficient learned image compression with quality-aware hyperprior pruning. <em>IETIP</em>, <em>19</em>(1), e70231. (<a href='https://doi.org/10.1049/ipr2.70231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned image compression (LIC) has become more and more important in recent years. The hyperprior-module-based LIC models, which use hyperprior module to predict the distribution of image features and improve entropy coder performance, have achieved remarkable rate-distortion (RD) performance. However, the storage and memory costs of these LIC models are too high, resulting in higher difficulty to be applied to various devices, especially portable or edge devices. The storage and memory cost are directly linked to the parameter number. As a preliminary experiment, we manually assigned half channels for the hyperprior module in LIC models, reducing about 30% parameters in the model. The pruned models still kept similar RD performance to the original ones. This reveals that the hyperprior module in LIC models is highly redundant. In the meanwhile, LIC models with different reconstruction qualities require different amounts of parameters for the hyperprior module. Based on these phenomena, we propose a quality-aware hyperprior pruning method that efficiently reduces the storage and memory cost of the hyperprior module and various context models. It consists of two parts. The first part is the pruning method itself, called enhanced ResRep on hyper path (ERHP). The second part is a quality-aware threshold searching method, called pruning threshold searching (PTS), which prunes the hyperprior module based on the reconstruction qualities of LIC models. The experiments on various LIC models show that our methods reduce large volumes of storage cost (up to 74.6%) and memory cost (up to 41.5%), while keeping the performance the same before pruning.},
  archive      = {J_IETIP},
  author       = {Ao Luo and Diego Fujii and Keisuke Nonaka and Heming Sun and Jiro Katto},
  doi          = {10.1049/ipr2.70231},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70231},
  shortjournal = {IET Image Process.},
  title        = {Storage-and-memory-efficient learned image compression with quality-aware hyperprior pruning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFE-dehaze: Image dehazing method based on adaptive feature enhancement contrastive learning. <em>IETIP</em>, <em>19</em>(1), e70230. (<a href='https://doi.org/10.1049/ipr2.70230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issue of recovery imbalance caused by the spatial heterogeneity of haze concentration in real scenarios, this paper proposes an adaptive feature enhanced contrastive learning framework (AFE-Dehaze). The framework achieves breakthroughs through three major collaborative mechanisms: (1) a hierarchical multi-scale fusion architecture that combines diffusion convolution and channel attention, preserving edge textures (such as leaf veins and building contours) in thin haze areas, while semantically guiding the reconstruction of structural details in dense haze areas, improving texture retention by 18% compared to traditional U-Net; (2) a concentration-sensitive contrastive learning paradigm that uses pre-trained VGG features as semantic anchors, applying pixel-level constraints in thin haze and feature space constraints in dense haze, which reduces color distortion ( ) by 23%, significantly outperforming methods like refusion; (3) a gradient dynamic balancing strategy that automatically adjusts the optimization direction by analyzing positive and negative sample gradient contributions, enhancing PSNR by 1.2dB and SSIM by 0.05 in non-uniform haze scenarios. Experiments on a mixed dataset (RESIDE OTS real scenes) demonstrate that AFE-Dehaze achieves an average PSNR of 28.7dB and SSIM of 0.91, especially improving structural similarity in dense haze areas by 9% compared to Mamba, validating its generalization capability in complex haze environments. This framework provides a solution that balances accuracy and robustness for dehazing in real scenarios such as vehicular vision and remote sensing imaging.},
  archive      = {J_IETIP},
  author       = {Lanqing Zhang and Zhigao Cui and Yanzhao Su and Nian Wang and Yunwei Lan and Liangyu Zhu and Cheng Chen},
  doi          = {10.1049/ipr2.70230},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70230},
  shortjournal = {IET Image Process.},
  title        = {AFE-dehaze: Image dehazing method based on adaptive feature enhancement contrastive learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining density entropy calculation with structure from motion for indoor image 3D point cloud modeling and rendering reconstruction in unreal engine. <em>IETIP</em>, <em>19</em>(1), e70229. (<a href='https://doi.org/10.1049/ipr2.70229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) reconstruction plays a crucial role in applications such as autonomous driving and precision engineering; however, environmental noise and image feature misidentification often compromise its accuracy. This paper presents a novel 3D point cloud reconstruction method that integrates structure from motion (SfM) with entropy-based noise filtering prior to feature point extraction, thereby enhancing the richness of epipolar geometry. An entropy bundle adjustment (BA) is proposed to further improve the precision of 3D point cloud generation. The proposed reconstructed model is integrated into Unreal Engine, enabling rendering with highly realistic visual effects. In a series of comparative experiments, the proposed method demonstrated significant improvements over existing approaches in terms of point cloud density, as well as the quality and uniformity of the corresponding 3D reconstructions, thereby opening new possibilities for enhancing the realism and practical applicability of simulation and visualization.},
  archive      = {J_IETIP},
  author       = {Jim-Wei Wu and Ko-Fan Teng and Jia-Cheng Li},
  doi          = {10.1049/ipr2.70229},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70229},
  shortjournal = {IET Image Process.},
  title        = {Combining density entropy calculation with structure from motion for indoor image 3D point cloud modeling and rendering reconstruction in unreal engine},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LGL-net: A lightweight global-local multiscale network with region-aware interpretability for alzheimer's disease diagnosis. <em>IETIP</em>, <em>19</em>(1), e70228. (<a href='https://doi.org/10.1049/ipr2.70228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer's disease (AD) is a progressive neurodegenerative disorder marked by gradual cognitive decline and structural brain degeneration. Magnetic resonance imaging (MRI), due to its non-invasive nature and high spatial resolution, plays a pivotal role in the clinical diagnosis of AD. However, considerable challenges persist, primarily due to the heterogeneity of brain structural alterations across individuals and the high computational burden associated with deploying deep learning models in clinical practice. Although recent deep learning-based approaches have significantly improved diagnostic accuracy, most models fail to identify the specific contributions of individual brain regions, limiting their interpretability and clinical applicability. To address these limitations, we propose LGL-Net, a novel lightweight 3D convolutional neural network tailored for efficient extraction and integration of both global and local anatomical features from MRI data. The architecture adopts a dual-branch design, wherein one branch captures whole-brain atrophy patterns, while the other focuses on fine-grained, region-specific structural variations. This design achieves a favourable trade-off between computational efficiency and diagnostic performance, significantly reducing the model's parameter count and computational load without compromising accuracy. Importantly, LGL-Net explicitly maps learnt features onto anatomically defined brain regions, enabling region-level interpretability of classification outcomes. By independently evaluating the contributions of each region to both global and local representations, the model elucidates how multiscale anatomical features collectively influence diagnostic decisions. Experimental results demonstrate that LGL-Net achieves classification performance comparable to existing methods, while substantially lowering model complexity and computational demands. Overall, this framework offers a scalable, interpretable and resource-efficient solution for intelligent AD diagnosis.},
  archive      = {J_IETIP},
  author       = {Juan Zhou and Ruiyang Tao and Weiqiang Zhou and Xia Chen and Xiong Li},
  doi          = {10.1049/ipr2.70228},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70228},
  shortjournal = {IET Image Process.},
  title        = {LGL-net: A lightweight global-local multiscale network with region-aware interpretability for alzheimer's disease diagnosis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-powered human activity detection and tracking in dense crowds using YOLOv8-DeepSORT. <em>IETIP</em>, <em>19</em>(1), e70227. (<a href='https://doi.org/10.1049/ipr2.70227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many organisations face challenges in accurately and efficiently identifying human activities using manually operated CCTV cameras and small datasets, particularly in densely populated and dynamic environments. This paper presents a real-time human activity detection and recognition system that leverages You Only Look Once version 8 (YOLOv8) for object detection and integrates the deep simple online real-time tracking algorithm for associate frames, robust tracking and individual identity preservation. Evaluated on a combination of public and experimental datasets containing over 10,000 frames, the system achieves notable enhancements in efficiency along with accuracy. Through data preprocessing and algorithm optimisation, the Adam optimiser outperformed stochastic gradient descent (SGD) by more than 5% in accuracy. K -fold cross-validation was applied to reduce overfitting, while anchor boxes of varying scales and aspect ratios improved the detection of objects of different sizes. The aspect ratio, which is used to mitigate the effect of object size and control edge-orientation issues by computing the ratio of width to height of the bounding boxes in YOLOv8, plays a crucial role in object detection.YOLOv8l, combined with DeepSORT, delivers state-of-the-art results, achieving a mean average precision of 96.1% in mixed datasets. The system assigns unique IDs to track individuals, enabling precise frame association during motion in crowded areas, which is crucial for maintaining accuracy in complex environments. This study presents a robust and scalable model, offering a significant improvement for public surveillance technology.},
  archive      = {J_IETIP},
  author       = {Abebe Belay Adege and Muluye Fentie Admas and Yitayal Tehone and Boaz Berhanu Tulu and Eyosiyas Tibebu Endalamaw},
  doi          = {10.1049/ipr2.70227},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70227},
  shortjournal = {IET Image Process.},
  title        = {AI-powered human activity detection and tracking in dense crowds using YOLOv8-DeepSORT},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient and lightweight point cloud recognition network based on neighborhood learning. <em>IETIP</em>, <em>19</em>(1), e70226. (<a href='https://doi.org/10.1049/ipr2.70226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud recognition has wide applications in fields such as autonomous driving and shape classification. Although significant progress has been made in point cloud processing in recent years, most of it has been achieved by designing more complex networks to attain better performance. This paper proposes a novel lightweight point cloud recognition network by introducing a new local neighborhood optimization layer (LNOL), which improves traditional sampling methods by correlation learning in local area. The LNOL is embedded within a single-layer local transformer architecture, significantly reducing computational complexity and parameters while maintaining the model's expressive power. Experimental results on the ModelNet40 benchmark dataset demonstrate that our method achieves a classification accuracy of 93.3% and an average precision of 92.0% without using a voting strategy. Compared to the mainstream local transformer model point transformer, our network requires only 9.95G FLOPs and 2.33M parameters, reducing computational cost by 94.7% and parameter count by 75.7%, with only a 0.4% drop in accuracy. This study provides an efficient solution for real-time 3D recognition applications, significantly lowering computational resource requirements while maintaining performance.},
  archive      = {J_IETIP},
  author       = {Yanxia Bao and Zilong Liu and Yahong Chen and Yang Shen},
  doi          = {10.1049/ipr2.70226},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70226},
  shortjournal = {IET Image Process.},
  title        = {An efficient and lightweight point cloud recognition network based on neighborhood learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object handle segmentation in 3D point cloud for robot grasping using scale invariant heat kernel signature with optimized XGBoost classifier. <em>IETIP</em>, <em>19</em>(1), e70225. (<a href='https://doi.org/10.1049/ipr2.70225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting graspable regions is crucial for robotic manipulation tasks like pick-and-place and pouring. This study proposes a robust method for detecting handle-like regions in common objects, focusing on slender handles distinct from the main body. This characteristic is prevalent in many daily-use objects that are often manipulated. Our method employs the scale-invariant heat kernel signature (SI-HKS) descriptor to capture local and global shape features of 3D objects. By utilizing SI-HKS properties, we extract meaningful geometric information. Points are classified into segments using the XGBoost classifier, known for its efficiency and accuracy, while hyperparameters are optimized through random search. A post-processing step refines handle detection by filtering out non-graspable regions based on geometric skeleton curvature. The proposed approach is evaluated on a custom dataset in two configurations: five categories of handle-equipped objects and extended version with eleven categories. In the 5-class setup, the method achieves a mean intersection-over-union (mIoU) of 97.6%, outperforming leading deep learning models like PointNet, PointNet++, and DGCNN with statistically significant improvements confirmed by t -tests. In the extended 11-class setup, the method maintains a strong performance with a mean IoU of 97.5%. The use of intrinsic geometric features enhances rotation invariance, ensuring consistent segmentation across different orientations.},
  archive      = {J_IETIP},
  author       = {Haniye Merrikhi and Hossein Ebrahimnezhad},
  doi          = {10.1049/ipr2.70225},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70225},
  shortjournal = {IET Image Process.},
  title        = {Object handle segmentation in 3D point cloud for robot grasping using scale invariant heat kernel signature with optimized XGBoost classifier},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSSNet: An anchor-free rotated object detection network with dynamic sample selection for remote sensing images. <em>IETIP</em>, <em>19</em>(1), e70224. (<a href='https://doi.org/10.1049/ipr2.70224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in remote sensing imagery requires precise localisation and identification of targets under challenging conditions. Facing the challenges of arbitrary target orientations, wide-scale variations, dense distributions, and small objects in remote sensing object detection, anchor-based methods suffer from inadequate rotated target representation using rectangular boxes. This necessitates excessive angle-specific anchors, leading to heavy computational overhead, severe sample imbalance, and slow speeds unsuitable for mobile deployment. To address these accuracy-efficiency trade-offs, we propose DSSNet: an anchor-free rotated object detection network with dynamic sample selection for remote sensing images. DSSNet replaces traditional backbones with the parameter-efficient ConvNeXt-T and utilises an FPN for accelerated multi-scale feature extraction. During prediction, it employs a shape-adaptive selection strategy combined with a contour point quality assessment strategy to dynamically refine target contour points, enabling real-time rotated object detection. The efficacy of DSSNet has been thoroughly validated through benchmark comparisons on diverse datasets. On the DOTA dataset, DSSNet clearly outperforms baseline methods in detection performance, achieving a mean Average Precision (mAP) of 76.97% and the fastest detection speed of 26.2 frames per second (FPS).},
  archive      = {J_IETIP},
  author       = {Longbao Wang and Yongheng Yu and Xiaoliang Luo and Lvchun Wang and Mu He and Yican Shen and Zhijun Zhou and Hongmin Gao},
  doi          = {10.1049/ipr2.70224},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70224},
  shortjournal = {IET Image Process.},
  title        = {DSSNet: An anchor-free rotated object detection network with dynamic sample selection for remote sensing images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring data modalities and advances in related AI technologies for oral cancer detection. <em>IETIP</em>, <em>19</em>(1), e70223. (<a href='https://doi.org/10.1049/ipr2.70223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oral cancer diagnosis represents a significant public health burden; late-stage detection of oral cancer is a major issue for ineffective treatment. Multimodal approaches from artificial intelligence have emerged as a pretty promising approach to address this challenge. In this paper, a comprehensive review of recent studies of oral cancer detection across varied data modalities that utilise technologies such as computer vision, natural language processing, acoustics analysis, Internet of Things, and machine learning and Deep Learning (DL) is presented. Across the reviewed literature, unique datasets spanning imaging, histopathology, spectroscopy, and clinical text are identified and represented. Reported performance metrics vary by modality, such as image-based DL methods, which achieved accuracies between 91% and 99% and area under the curve values up to 0.95, spectroscopy-based approaches reported accuracies above 92%. These results highlight the diagnostic potential of varied data modalities for future research direction, and small, imbalanced datasets, lack of external validation, and personalisation are major concerns to be addressed.},
  archive      = {J_IETIP},
  author       = {Sahil Sharma and Seema Wazarkar and Geeta Kasana},
  doi          = {10.1049/ipr2.70223},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70223},
  shortjournal = {IET Image Process.},
  title        = {Exploring data modalities and advances in related AI technologies for oral cancer detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Palmprint features fusion recognition based on conformer and gabor. <em>IETIP</em>, <em>19</em>(1), e70222. (<a href='https://doi.org/10.1049/ipr2.70222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to better extract the discriminative features of palmprint images and improve the accuracy of palmprint recognition, this paper proposes a palmprint feature fusion recognition algorithm based on Gabor and Conformer. First, Gabor filtering is applied to the palmprint region image to obtain the global distribution feature vector of image texture features. Meanwhile, a deep network feature extractor based on the Conformer neural network is constructed to obtain the local distribution feature vector of image deep features. Then, the kernel canonical correlation analysis (KCCA) method is used to extract the correlation features between the two groups of feature vectors as effective discriminative information, eliminate the information redundancy between the features, and obtain more accurate palmprint fusion features. Finally, a knowledge graph is used to pre-classify the palmprint fusion features to reduce the feature search space, and the cosine distance classifier is used to recognise the palmprint. Simulation results show that the accuracy of the proposed method improves by about 15.02% compared with the basic features and about 3.92% compared with other algorithms, which proves the effectiveness of the proposed algorithm. The main contributions of this work lie in the joint encoding of global Gabor texture features and Conformer-based local–global representations, the correlation-aware fusion via KCCA, and the acceleration of large-scale recognition through a knowledge-graph-based two-stage matching mechanism.},
  archive      = {J_IETIP},
  author       = {Xiancheng Zhou and Yulong Liu and Xinran Ji and Kaijun Zhou},
  doi          = {10.1049/ipr2.70222},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70222},
  shortjournal = {IET Image Process.},
  title        = {Palmprint features fusion recognition based on conformer and gabor},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). No-reference image quality assessment via semantic-guided multi-scale feature extraction. <em>IETIP</em>, <em>19</em>(1), e70221. (<a href='https://doi.org/10.1049/ipr2.70221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality assessment is crucial in the development of digital technology. No-reference image quality assessment aims to predict image quality accurately without depending on reference images. In this paper, we propose a semantic-guided multi-scale feature extraction network for no-reference image quality assessment. The network begins with a scale-wise attention module to capture both global and local features. Subsequently, we design a layer-wise feature guidance block that leverages high-level semantic information to guide low-level feature learning for effective feature fusion. Finally, it predicts quality scores through quality regression using the Kolmogorov–Arnold network. Experimental results with 19 existing methods on six public IQA datasets—LIVE, CSIQ, TID2013, KADID-10k, LIVEC and KonIQ-10k—demonstrate that the proposed method can effectively simulate human perceptions of image quality and is highly adaptable to different distortion types.},
  archive      = {J_IETIP},
  author       = {Peng Ji and Wanjing Wang and Zhongyou Lv and Junhua Wu},
  doi          = {10.1049/ipr2.70221},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70221},
  shortjournal = {IET Image Process.},
  title        = {No-reference image quality assessment via semantic-guided multi-scale feature extraction},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated detection of diabetic retinopathy by using global channel attention mechanism. <em>IETIP</em>, <em>19</em>(1), e70220. (<a href='https://doi.org/10.1049/ipr2.70220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR), a major ocular complication of diabetes, poses a significant global health challenge. Although convolutional neural networks (CNNs) have demonstrated effectiveness in DR grading tasks, their ability to capture long-range dependencies scattered across fundus images remains limited. To address this limitation, we propose a global channel attention mechanism that incorporates the global feature extraction capability of Vision Transformer (ViT) while maintaining compatibility with CNN architectures, thereby enhancing their ability to model long-range dependencies. Experimental results show that our model achieves test accuracies of 88.49% and 77.33% on the augmented APTOS 2019 and Messidor-2 datasets, respectively, validating the efficacy of the proposed mechanism.},
  archive      = {J_IETIP},
  author       = {Jing Qin and Xiaolong Bu},
  doi          = {10.1049/ipr2.70220},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70220},
  shortjournal = {IET Image Process.},
  title        = {Automated detection of diabetic retinopathy by using global channel attention mechanism},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quaternion-based image restoration via saturation-value total variation and pseudo-norm regularization. <em>IETIP</em>, <em>19</em>(1), e70219. (<a href='https://doi.org/10.1049/ipr2.70219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color image restoration is a fundamental task in computer vision and image processing, with extensive real-world applications. In practice, color images often suffer from degradations caused by sensor noise, optical blur, compression artifacts, and data loss during the acquisition, transmission, or storage. Unlike grayscale images, color images exhibit high correlations among their RGB channels. Directly extending grayscale restoration methods to color images often leads to issues such as color distortion and structural artifacts. To address these challenges, this paper proposes a novel quaternion-based color image restoration framework. The method integrates low-rank pseudo-norm constraints with saturation-value total variation (SVTV) regularization, effectively enhancing restoration quality in tasks including denoising, deblurring, and inpainting of degraded color images. The proposed algorithm is efficiently solved using the alternating direction method of multipliers (ADMM), and restoration performance is rigorously evaluated through quantitative metrics including peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and S-CIELAB error. Extensive experimental results demonstrate the superior performance of our method compared to existing approaches.},
  archive      = {J_IETIP},
  author       = {Zipeng Fu and Xiaoling Ge and Weixian Qian and Xuelian Yu},
  doi          = {10.1049/ipr2.70219},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70219},
  shortjournal = {IET Image Process.},
  title        = {Quaternion-based image restoration via saturation-value total variation and pseudo-norm regularization},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight reversible data hiding system for microcontrollers using integer reversible meixner transform. <em>IETIP</em>, <em>19</em>(1), e70218. (<a href='https://doi.org/10.1049/ipr2.70218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of secure data communication, reversible data hiding (RDH) has emerged as a promising strategy to ensure both confidentiality and integrity. However, in resource-constrained environments, such as microcontroller platforms, conventional RDH techniques encounter challenges due to factors like minimal memory resources and speed, which restrict the use of microcontrollers for implementing image RDH. Addressing this gap, we introduce a lightweight RDH system tailored for microcontrollers, employing the integer reversible Meixner transform (IRMMT), a variant of the Meixner moment transform optimised for integer operations. Unlike its floating-point version, IRMMT ensures complete preservation of data, even with the use of low finite precision arithmetic, thereby demonstrating its efficacy for lossless applications and its suitability for resource-limited embedded devices. Leveraging IRMMT, we propose a novel RDH algorithm designed to operate efficiently within the limitations of microcontroller resources while preserving image quality and integrity. The algorithm is implemented and evaluated on the Arduino Due board, which features the AT91SAM3X8E 32-bit ARM Cortex-M3 microcontroller, demonstrating the feasibility and effectiveness of the proposed approach in enabling secure wireless data communication. Through theoretical formulation, algorithm design and embedded implementation, this paper contributes to advancing RDH methodologies for resource-limited embedded devices.},
  archive      = {J_IETIP},
  author       = {Mohamed Yamni and Achraf Daoui and Chakir El-Kasri and May Almousa and Ali Abdullah S. AlQahtani and Ahmed A. Abd El-Latif},
  doi          = {10.1049/ipr2.70218},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70218},
  shortjournal = {IET Image Process.},
  title        = {Lightweight reversible data hiding system for microcontrollers using integer reversible meixner transform},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and flexible omnidirectional depth estimation with multiple 360-degree cameras. <em>IETIP</em>, <em>19</em>(1), e70217. (<a href='https://doi.org/10.1049/ipr2.70217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional depth estimation has received much attention from researchers in recent years. However, challenges arise due to camera soiling and variations in camera layouts, affecting the robustness and flexibility of the algorithm. In this paper, we use the geometric constraints and redundant information of multiple 360 cameras to achieve robust and flexible multi-view omnidirectional depth estimation. We implement two algorithms, in which the two-stage algorithm obtains initial depth maps by pairwise stereo matching of multiple cameras and fuses the multiple depth maps for the final depth estimation; the one-stage algorithm adopts spherical sweeping based on hypothetical depths to construct a uniform spherical matching cost of the multi-camera images and obtain the depth. Additionally, a generalized epipolar equirectangular projection is introduced to simplify the spherical epipolar constraints. To overcome panorama distortion, a spherical feature extractor is implemented. Furthermore, a synthetic 360 dataset of outdoor road scenes is presented, which takes soiled camera lenses and glare into consideration and is more consistent with the real-world environment. Experiments show that our two algorithms achieve state-of-the-art performance, accurately predicting depth maps even when provided with soiled panorama inputs. The flexibility of the algorithms is experimentally validated in terms of camera layouts and numbers.},
  archive      = {J_IETIP},
  author       = {Ming Li and Xueqian Jin and Xuejiao Hu and Jinghao Cao and Sidan Du and Yang Li},
  doi          = {10.1049/ipr2.70217},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70217},
  shortjournal = {IET Image Process.},
  title        = {Robust and flexible omnidirectional depth estimation with multiple 360-degree cameras},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointGS: Point-wise feature-aware gaussian splatting for sparse view synthesis. <em>IETIP</em>, <em>19</em>(1), e70216. (<a href='https://doi.org/10.1049/ipr2.70216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a point-wise feature-aware Gaussian splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialisation. Then we encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbours. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multilayer perceptrons for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods.},
  archive      = {J_IETIP},
  author       = {Lintao Xiang and Hongpei Zheng and Yating Huang and Qijun Yang and Hujun Yin},
  doi          = {10.1049/ipr2.70216},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70216},
  shortjournal = {IET Image Process.},
  title        = {PointGS: Point-wise feature-aware gaussian splatting for sparse view synthesis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comrade-secure adversarial noise for 3D point cloud classification model. <em>IETIP</em>, <em>19</em>(1), e70215. (<a href='https://doi.org/10.1049/ipr2.70215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are effective across many domains, including text, audio, and image. Recently, DNNs have been used in autonomous driving, robotics, and even drones owing to the increasing utilization of 3D data. However, 3D data point clouds are vulnerable to adversarial examples, much like any other form of data. An adversarial example slightly alters the original sample or adds a small amount of noise, making it appear normal to humans, which results in its misclassification by the models. In this study, we propose a method that can be used to generate a “comrade-secure” adversarial point cloud example. In the proposed method, we subtly adjust the positions of certain points in the point cloud to create an adversarial example. This alteration causes the enemy model to misclassify, while the friendly model remains accurate. We use the ModelNet40 dataset for experimental evaluation and utilize PointNet++ and PointNet, which are representative models to classify 3D point clouds, as friendly and enemy models, respectively. In the experiments, the adversarial point cloud examples generated by the proposed method showed that the friendly model achieved an accuracy of 97.65%, and the enemy model was misclassified with an attack success rate of 99.55%.},
  archive      = {J_IETIP},
  author       = {Taehwa Lee and Soojin Lee and Hyun Kwon},
  doi          = {10.1049/ipr2.70215},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70215},
  shortjournal = {IET Image Process.},
  title        = {Comrade-secure adversarial noise for 3D point cloud classification model},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient visual state space model for remote sensing binary change detection. <em>IETIP</em>, <em>19</em>(1), e70214. (<a href='https://doi.org/10.1049/ipr2.70214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer and convolutional neural network (CNN) have made significant progress in the issue of remote sensing binary change detection. However, Transformer has high quadratic computational complexity, while CNN is limited by a fixed receptive field, which may hinder their capability of learning spatial contextual features. Inspired by the remarkable performance of Mamba on the task of natural language processing, which can effectively make up for the deficiencies of the above two architectures, we tailor the structure of Mamba to solve the issue of binary change detection. In this work, we explore the potential of visual Mamba to address the task of binary change detection in remote sensing imageries, which is abbreviated as Mam-BCD. The entire network is designed as an encoder–decoder architecture. The encoder employs the effective visual Mamba to fully learn global spatial contextual features from input images. For the decoder, we introduce three spatio-temporal feature learning strategies, which can be organically integrated into the Mamba architecture to achieve spatio-temporal interaction between different temporal features. Comprehensive experiments are conducted on three public available datasets to verify the efficacy of the proposed Mam-BCD. Compared to the advanced CTDFormer, Mam-BCD achieves 4.49%, 8.73% and 3.44% gain in accuracy metric on SYSU-CD, LEVIR-CD+ and WHU-CD datasets, respectively.},
  archive      = {J_IETIP},
  author       = {Huagang Jin and Yu Zhou},
  doi          = {10.1049/ipr2.70214},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70214},
  shortjournal = {IET Image Process.},
  title        = {An efficient visual state space model for remote sensing binary change detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task learning for chinese character and radical recognition with dynamic channel-spatial attention and rotational positional encoding. <em>IETIP</em>, <em>19</em>(1), e70213. (<a href='https://doi.org/10.1049/ipr2.70213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical character recognition (OCR) plays a crucial role in digitizing archives and documents. However, recognizing complex Chinese characters remains challenging owing to their intricate structures and sequential patterns. This study introduces an advanced OCR model that integrates EfficientNetV2 as the backbone within a transformer-based architecture to enhance feature extraction. To address the limitations of traditional adaptive feature selection, we propose a dynamic collaborative channel–spatial attention (DCCSA) module. This module combines channel attention, spatial attention, and channel shuffling to dynamically capture global dependencies and optimize feature representations across both spatial and channel dimensions. Additionally, rotational position encoding (RoPE) is incorporated into the transformer to accurately capture the spatial relationships between characters and radicals, ensuring precise representation of complex hierarchal structures. Further, the model adopts a multitask learning framework that jointly decodes characters and radicals, enabling cross-task optimization and significantly enhancing recognition performance. Experimental results on four benchmark datasets demonstrate that the proposed model outperforms existing methods, achieving significant improvements on both printed and handwritten Chinese text. Moreover, the model shows strong generalization capabilities on challenging scene-text datasets, underscoring its effectiveness in addressing the OCR challenges associated with intricate scripts.},
  archive      = {J_IETIP},
  author       = {Wei Deng and XuHong Yu and HongWei Li and ShaoWen Du and Bing He},
  doi          = {10.1049/ipr2.70213},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70213},
  shortjournal = {IET Image Process.},
  title        = {Multi-task learning for chinese character and radical recognition with dynamic channel-spatial attention and rotational positional encoding},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vis-a-vis: A tool for face components replacement. <em>IETIP</em>, <em>19</em>(1), e70212. (<a href='https://doi.org/10.1049/ipr2.70212'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a tool that can replace one or more specific facial components in images for face analysis. The tool can replace the texture and shape of facial components such as eyes, nose, and mouth. The source and destination of the components can be real faces or an average face computed from a dataset. A seamless method is applied to smooth the component boundaries after replacement. The tool is developed using the Python language and is available in open source and online, with a web interface. We also provide a desktop version that can manage multiple files or a dataset as input. The tool can, for instance, be used to investigate the contribution of face components to face recognition, face perception analysis, the change of identity, and fun applications. Some illustrative examples are provided.},
  archive      = {J_IETIP},
  author       = {Nova Hadi Lestriandoko and Luuk J. Spreeuwers and Raymond N. J. Veldhuis},
  doi          = {10.1049/ipr2.70212},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70212},
  shortjournal = {IET Image Process.},
  title        = {Vis-a-vis: A tool for face components replacement},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pose and illumination invariant face recognition in video via motion analysis and texture feature optimization. <em>IETIP</em>, <em>19</em>(1), e70211. (<a href='https://doi.org/10.1049/ipr2.70211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition and person re-identification in video remains a challenging area in computer vision community. A variety of texture feature sets are evaluated for better recognition of individuals in video for varying pose and illumination conditions—as found in real world video surveillance. This paper focuses on face recognition in video based changing environment using Fuzzy-ARTMAP (FAM) neural network classifier. Individual specific facial model of each target subject is structured using two-class classification to solve a multi-class classification problem. Two lower dimensional linear subspace techniques, PCA and LDA, are applied. All evaluations are carried out by executing 10 iterations and average results are presented. We split dataset randomly in each permutation: two-thirds of the data model is reserved for training of the classifier, and rest for testing. Experimental results of our study are determined on two publicly available databases: the ChokePoint database and the Extended YaleB database. Our findings show that FAM classifier performs better with the feature set “Pixel intensity combined with Local Binary Patterns (LBP) and Local Phase Quantization (LPQ)”  in varying illumination and pose scenarios during face recognition problem.},
  archive      = {J_IETIP},
  author       = {Basharat Hussain and Ayyaz Hussain and Muhammad Islam},
  doi          = {10.1049/ipr2.70211},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70211},
  shortjournal = {IET Image Process.},
  title        = {Pose and illumination invariant face recognition in video via motion analysis and texture feature optimization},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time efficient detection of substation electrical equipment in infrared images based on improved YOLOv11. <em>IETIP</em>, <em>19</em>(1), e70210. (<a href='https://doi.org/10.1049/ipr2.70210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges in automatic infrared image detection within complex substation environments, including missed detection of small targets, severe interference from thermal backgrounds, and balancing detection accuracy with efficiency–this paper proposes an intelligent detection method for electrical equipment based on an improved lightweight YOLOv11n architecture. Specifically, we propose a novel sparse attention and convolution mixing (SACM) module that integrates sparse attention with convolutional operations, aiming to enhance small object detection capability and feature selection efficiency while maintaining low computational complexity. Furthermore, the backbone network is augmented with the self-calibrated convolutions (SCConv) module, which performs structured feature reconstruction to suppress redundant information and improve computational efficiency, thereby providing a low-noise foundation for subsequent multi-scale feature fusion. In addition, the original PANet is replaced with bidirectional weighted feature pyramid network (BiFPN) to construct an efficient fusion pathway that aligns with the SCConv-enhanced features, thus strengthening cross-scale feature interaction. A lightweight up sampling module, CARAFE, is also introduced and combined with SACM to establish a synergy between feature enhancement and fine-grained reconstruction, effectively recovering detailed features of small objects and reducing missed detections. Finally, the shape-intersection over union (Shape-IoU) loss function is adopted in place of traditional bounding box regression losses to further improve localization accuracy. Experimental results demonstrate significant performance improvements in complex substation scenarios: the proposed method achieves an mAP@50 of 97.0%, representing a 3.2 percentage point increase over the baseline model, while maintaining low computational complexity with only 2.82 M parameters and 6.1G FLOPs. This study provides an innovative solution that balances high accuracy and efficiency for intelligent electrical equipment detection, offering both theoretical significance and practical engineering value for advancing intelligent substation operation and maintenance.},
  archive      = {J_IETIP},
  author       = {Runtong Zhu and Hu Liu and Yanhao Gao and Yue Yan},
  doi          = {10.1049/ipr2.70210},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70210},
  shortjournal = {IET Image Process.},
  title        = {Real-time efficient detection of substation electrical equipment in infrared images based on improved YOLOv11},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCUX-net: Integrating multi-scale features and channel-spatial attention model for intracranial aneurysm segmentation. <em>IETIP</em>, <em>19</em>(1), e70209. (<a href='https://doi.org/10.1049/ipr2.70209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intracranial aneurysm is a common cerebrovascular condition, due to the small size and complex anatomical location of intracranial aneurysms, it remains a challenging task to accurately segmenting the intracranial aneurysms in computed tomography angiography (CTA) images. To address these challenges, we propose SCUX-Net, a novel lightweight convolutional neural network designed to facilitate the segmentation of intracranial aneurysms. SCUX-Net builds upon the 3D UX-Net by introducing two key innovations: (1) a spatial adaptive feature module, integrated before each 3D UX-Net block, enabling multi-scale feature fusion for long-range information interaction; (2) a convolutional block attention module, applied after each downsampling block to emphasize important features across channel and spatial dimensions, suppressing irrelevant information. Experimental results substantiate the effectiveness of SCUX-Net in segmenting intracranial aneurysms on CTA images, achieving a dice similarity coefficient of 80% on the test set. Notably, SCUX-Net excels in detecting small aneurysms ( 3 mm) and multiple aneurysms, showcasing its potential for clinical application.},
  archive      = {J_IETIP},
  author       = {Aiping Wu and Mingquan Ye and Jiaqi Wang and Ye Shi and Yunfeng Zhou},
  doi          = {10.1049/ipr2.70209},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70209},
  shortjournal = {IET Image Process.},
  title        = {SCUX-net: Integrating multi-scale features and channel-spatial attention model for intracranial aneurysm segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning and IoT fusion for crop health monitoring: A high-accuracy, edge-optimised model for smart farming. <em>IETIP</em>, <em>19</em>(1), e70208. (<a href='https://doi.org/10.1049/ipr2.70208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crop diseases and adverse field conditions threaten global food security, particularly in resource-limited regions. Current deep-learning models for disease detection suffer from insufficient accuracy, high prediction instability under field noise, and a lack of integration with environmental context. To address these limitations, we present a hybrid deep learning architecture combining EfficientNetV2, MobileNetV2, and Vision Transformers, augmented with attention mechanisms and multiscale feature fusion. Optimised for edge deployment via TensorFlow Lite and integrated with IoT sensors for real-time soil and field monitoring, the model achieved state-of-the-art performance with 99.2% accuracy, 0.993 precision, 0.993 recall, and a near-perfect AUC of 0.999998, outperforming benchmarks like DenseNet50 (88.4%) and ShuffleNet (95.8%). Training on 76 classes (22 diseases) demonstrated rapid convergence and robustness, with validation accuracy reaching 98.7% and minimal overfitting. Statistical validation confirmed superior stability, with 69% lower prediction variance (0.000010) than DenseNet50 (0.000035), ensuring reliable performance under real-world noise. Bayesian testing showed a 100% probability of superiority over DenseNet50 and 85.1% over ShuffleNet, while field trials on 249 real-world images achieved 97.97% accuracy, highlighting strong generalisation. IoT integration reduced false diagnoses by 92% through environmental correlation, and edge optimisation enabled real-time inference via a 30.4 MB mobile application (0.094-second latency). This work advances precision agriculture through a scalable, cloud-independent framework that unifies hybrid deep learning with edge-compatible IoT sensing. By addressing critical gaps in accuracy, stability, and contextual awareness, the system enhances crop health management in low-resource settings, offering a statistically validated tool for sustainable farming practices.},
  archive      = {J_IETIP},
  author       = {Thomas Kinyanjui Njoroge and Edwin Juma Omol and Vincent Omollo Nyangaresi},
  doi          = {10.1049/ipr2.70208},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70208},
  shortjournal = {IET Image Process.},
  title        = {Deep learning and IoT fusion for crop health monitoring: A high-accuracy, edge-optimised model for smart farming},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive convolutional strategy for robust image dehazing in diverse environments. <em>IETIP</em>, <em>19</em>(1), e70207. (<a href='https://doi.org/10.1049/ipr2.70207'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adverse weather conditions such as haze, fog, and smog degrade image visibility, adversely affecting the performance of vision-based systems. Existing dehazing methods often struggle with non-uniform haze distributions, limited detail restoration, and poor generalization across diverse scenes. To overcome these limitations, this paper presents a deep learning-based dehazing framework that jointly restores image clarity and detail. Unlike conventional algorithms that often neglect fine structure recovery, our architecture incorporates four specialized sub-modules: (i) a noise attention module for enhancing noise suppression and feature preservation; (ii) an adaptive ConvNet module; (iii) a feature extraction module for capturing salient image features; and (iv) a detail refinement module to enhance spatial fidelity. The architecture is trained in an end-to-end manner to restore both structural integrity and colour consistency under challenging conditions. Extensive experiments conducted on synthetic and real-world datasets, including indoor, outdoor, underwater, night-time, and remote sensing scenarios, demonstrate superior generalization capability. In the SOTS indoor dataset, our method achieves a PSNR of 28.44 dB and an SSIM of 0.967, outperforming several state-of-the-art methods. Evaluations using additional metrics such as CIEDE2000 and MSE confirm the effectiveness of the proposed method in handling dense and heterogeneous haze while preserving fine textures and visual fidelity.},
  archive      = {J_IETIP},
  author       = {Hira Khan and Sung Won Kim},
  doi          = {10.1049/ipr2.70207},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70207},
  shortjournal = {IET Image Process.},
  title        = {Adaptive convolutional strategy for robust image dehazing in diverse environments},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of module transferability in single image super-resolution: Universality assessment and cycle residual blocks. <em>IETIP</em>, <em>19</em>(1), e70206. (<a href='https://doi.org/10.1049/ipr2.70206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has substantially advanced the single image super-resolution (SISR). However, existing researches have predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. In this paper, we introduce the concept of “Universality” and its associated definitions which extend the traditional notion of “Generalization” to encompass the modules' ease of transferability. Then we propose the universality assessment equation (UAE), a metric which quantifies how readily a given module could be transplanted across models and reveals the combined influence of multiple existing metrics on transferability. Guided by the UAE results of standard residual blocks and other plug-and-play modules, we further design two optimized modules, cycle residual block (CRB) and depth-wise cycle residual block (DCRB). Through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets and other low-level tasks, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-arts, reaching a PSNR enhancement of up to 0.83 dB or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity. Similar optimization approaches could be applied to a broader range of basic modules, offering a new paradigm for the design of plug-and-play modules.},
  archive      = {J_IETIP},
  author       = {Haotong Cheng and Zhiqi Zhang and Hao Li and Xinshang Zhang},
  doi          = {10.1049/ipr2.70206},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70206},
  shortjournal = {IET Image Process.},
  title        = {Optimization of module transferability in single image super-resolution: Universality assessment and cycle residual blocks},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid medical image semantic segmentation network based on novel mamba and transformer. <em>IETIP</em>, <em>19</em>(1), e70205. (<a href='https://doi.org/10.1049/ipr2.70205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning has greatly advanced medical image segmentation. Convolutional neural networks (CNNs) excel in capturing local image features, whereas ViT adeptly models long-range dependencies through multi-head self-attention mechanisms. Despite their strengths, both CNN and ViT face challenges in efficiently processing long-range dependencies in medical images and often require substantial computational resources. To address this, we propose a novel hybrid model combining Mamba and Transformer architectures. Our model integrates ViT's self-attention modules within a pure-vision Mamba U-shaped encoder, capturing both global and local information through nested Transformer and Mamba modules. Additionally, a multi-scale feed-forward neural network is incorporated within the Mamba blocks to enhance feature diversity by capturing fine-grained local details. Finally, a channel-adaptive feature (CAF) fusion module is introduced at the original skip connections to mitigate feature loss during information fusion and to improve segmentation accuracy in boundary regions. Quantitative and qualitative experiments were conducted on two public datasets: breast ultrasound image (BUSI) and clinic. The Dice score, Intersection over Union (IoU) score, recall score, F 1 score and 95% Hausdorff distance (HD95) of the proposed model on the BUSI dataset were 0.7918, 0.7016, 0.8508, 0.7919 and 12.04 mm, respectively. On ClinicDB, these metrics reach 0.9239, 0.8671, 0.9278, 0.9239 and 5.49 mm, respectively. The proposed model outperforms existing state-of-the-art CNN-, Transformer- and Mamba-based methods in segmentation accuracy, according to experimental data.},
  archive      = {J_IETIP},
  author       = {Jianting Shi and Huanhuan Liu and Zhijun Li},
  doi          = {10.1049/ipr2.70205},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70205},
  shortjournal = {IET Image Process.},
  title        = {A hybrid medical image semantic segmentation network based on novel mamba and transformer},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPWS-transformer: A study of 3D target detection method based on lightweight depth prediction with multi-scale fusion. <em>IETIP</em>, <em>19</em>(1), e70204. (<a href='https://doi.org/10.1049/ipr2.70204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced driver assistance systems (ADAS) mainly consist of three components: environmental perception, decision planning, and motion control. As a fundamental component of the ADAS environmental perception system, 3D object detection enables vehicles to avoid obstacles and ensure driving safety only through accurate and real-time prediction and localization of three-dimensional targets such as vehicles and pedestrians in road scenes. Therefore, to improve both the real-time performance and accuracy of 3D object detection, we propose a lightweight depth prediction-based 3D object detection model with multi-scale fusion—SPWS-Transformer. First, to enhance the model's accuracy, we propose a feature extraction network incorporating multi-scale feature fusion and depth prediction. By designing a multi-scale feature fusion module, we effectively combine multi-scale semantic and fine-grained information from feature maps of different scales to enhance the network's feature extraction capability. To capture spatial information from the feature maps, we apply convolution, group normalization, and nonlinear activation operations on the fused feature maps to generate depth feature maps. Both the fused feature maps and depth feature maps serve as inputs for subsequent network stages. To further improve accuracy, we leverage the long-range modelling advantages of Transformers by designing a feature enhancement encoder to strengthen the representation capability of depth feature maps. We incorporate a dilated encoder to perform positional encoding on depth feature maps and utilize multi-head self-attention mechanisms to capture contextual relationships within the input scene, thereby enhancing the detection capability of the 3D object detection network. Then, to improve real-time performance, we design a decoder structure with scale-aware attention. By predefining masks of different scales, we adaptively learn a scale-aware filter using depth and visual features to enhance object queries. Finally, on the KITTI dataset, the improved algorithm achieves an AP of 24.66% for the car category, with more significant improvements in detection accuracy under the ‘hard’ difficulty level. The model achieves an inference time of 24 ms.},
  archive      = {J_IETIP},
  author       = {Chang'an Zhang and Yian Wang and Ke Xu and ChunHong Yuan and Fusen Guo},
  doi          = {10.1049/ipr2.70204},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70204},
  shortjournal = {IET Image Process.},
  title        = {SPWS-transformer: A study of 3D target detection method based on lightweight depth prediction with multi-scale fusion},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of U-net optimizations: Advancing tumour segmentation in medical imaging. <em>IETIP</em>, <em>19</em>(1), e70203. (<a href='https://doi.org/10.1049/ipr2.70203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since its inception in 2015, U-Net has emerged as a cornerstone architecture that is particularly well-designed for medical image segmentation. Despite its robustness, precise tumour segmentation persists as a challenge because of tumour heterogeneity, boundary ambiguity, and the partial volume effects exhibited by tumours. Therefore, the U-Net architecture has been altered many times to expand its capabilities with complex segmentation challenges, particularly with tumours. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology, this systematic review critically evaluates and analyses the effectiveness of recent enhancement strategies developed to optimize the performance of the traditional U-Net architecture in attaining accurate tumour segmentation in CT and MRI images. The strategies have been divided into five main areas: U-Net architectural enhancements, including U-Net backbone optimization; skip connection refinements; bottleneck optimizations; transformer-based integrations; and metaheuristic algorithms as a self-adaptive optimization technique. Afterward, each category is thoroughly examined to determine how the strategies address specific limitations inherent to the traditional U-Net model. In addition, this paper reviews the pivotal role of preprocessing techniques in determining segmentation performance. This review identifies persistent research gaps and offers valuable insights for future research to improve the robustness, accuracy, and clinical applicability of the U-Net model.},
  archive      = {J_IETIP},
  author       = {Omar Abueed and Yong Wang and Mohammad Khasawneh},
  doi          = {10.1049/ipr2.70203},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70203},
  shortjournal = {IET Image Process.},
  title        = {A systematic review of U-net optimizations: Advancing tumour segmentation in medical imaging},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards safer roads: A deep learning and fuzzy logic-based driver fatigue detection system. <em>IETIP</em>, <em>19</em>(1), e70202. (<a href='https://doi.org/10.1049/ipr2.70202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a real-time, vision-based framework for detecting driver fatigue using a single low-cost, road-facing camera, eschewing direct visual monitoring of the driver. Unlike conventional systems that rely on in-cabin facial or physiological analysis, the proposed architecture prioritizes privacy by inferring fatigue through vehicle dynamics and road interaction alone. Built upon the YOLOP deep learning model, the system performs lane segmentation and object detection to extract two critical indicators: lane deviation and inter-vehicle distance, both computed from monocular vision. These signals are interpreted via a fuzzy logic module that incorporates trapezoidal, triangular, and Gaussian membership functions, enabling context-sensitive and explainable fatigue assessment. Comparative evaluation of these functions illustrates trade-offs in responsiveness and generalization. Initial validation against expert human assessments shows promising alignment in perceived fatigue levels, suggesting the system can meaningfully approximate fatigue-related judgments. By aligning with emerging ethical frameworks for non-intrusive AI in mobility, the system marks a step toward socially responsible and practically deployable fatigue monitoring in intelligent transportation.},
  archive      = {J_IETIP},
  author       = {Marios Akrivopoulos and Socratis Gkelios and Angelos Amanatiadis and Yiannis Boutalis and Savvas Chatzichristofis},
  doi          = {10.1049/ipr2.70202},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70202},
  shortjournal = {IET Image Process.},
  title        = {Towards safer roads: A deep learning and fuzzy logic-based driver fatigue detection system},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HoneyFL: Using honeypots to catch backdoors in federated learning. <em>IETIP</em>, <em>19</em>(1), e70201. (<a href='https://doi.org/10.1049/ipr2.70201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has been revealed as vulnerable to backdoor attacks since the server cannot directly access the locally collected data of clients, even if they are malicious. Many efforts either try to validate the global model with trusted clients, or try to make it difficult or costly to upload malicious updates. Unfortunately, the existing solutions are still challenged in defending against stealthy backdoor attacks or negative impacts brought to the aggregation. Especially in the non-independent and identically distributed setting. Moreover, these methods overlook the threat of adaptive attacks, that is, attackers fully know the defense implementation. To address these issues, we propose a novel run-time defense against diverse backdoor attacks, dubbed HoneyFL . It differs from previous works in three key aspects: (1) effectiveness - it is capable of defending against stealthy backdoors through leveraging honeypot clients; (2) aggregation - it promises effective aggregation since only a limited number of honeypot clients are used; (3) robustness - it can handle adaptive backdoor attacks based on differential prediction. Compared with five state-of-the-art defense baselines, extensive experiments show that HoneyFL produces a higher backdoor detection success rate above 97% and a lower false positive rate below 3%, where seven attacks generate backdoor examples. Its impact on the aggregation results of the main task is negligible. We also show that the defense success rate of HoneyFL against adaptive attacks is approximately 3.52 of the baselines on average.},
  archive      = {J_IETIP},
  author       = {Haibin Zheng and Wenjie Shen and Jinyin Chen},
  doi          = {10.1049/ipr2.70201},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70201},
  shortjournal = {IET Image Process.},
  title        = {HoneyFL: Using honeypots to catch backdoors in federated learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A 3D imaging method for UAV swarm polarimetric SAR based on joint sparse and low-rank structure constraints. <em>IETIP</em>, <em>19</em>(1), e70200. (<a href='https://doi.org/10.1049/ipr2.70200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tomographic synthetic aperture radar (SAR) and array SAR achieve high-resolution three-dimensional imaging through aperture dimension extension, but the synthetic aperture lengths in the azimuth and elevation directions are limited by the single-platform motion trajectory, resulting in low coverage efficiency and poor survivability. In contrast, UAV swarm SAR systems significantly improve imaging efficiency through multi-perspective collaborative detection by distributed platforms. However, efficiently fusing multi-platform echo data to achieve high-precision 3D reconstruction of targets poses a challenge. This paper proposes a polarimetric SAR 3D imaging method based on the joint constraint of sparsity and low rank: first, the imaging problem is modeled as a 3D spatial sparse reconstruction problem, and the non-convex minimax concave (MC) penalty function is employed to enhance the characterization of spatial sparsity. Second, an enhanced 3D total variation (E-3DTV) constraint is designed to effectively exploit the low-rank structural characteristics among polarimetric channels. Finally, a distributed optimization solution is implemented under the alternating direction method of multipliers (ADMM) framework. Experimental validation on the CVDomes dataset demonstrates that the proposed method significantly improves the accuracy and robustness of 3D reconstruction while maintaining high resolution, providing an effective solution for UAV swarm SAR 3D imaging.},
  archive      = {J_IETIP},
  author       = {Wei Li and Yanheng Ma and Xiaolin Ma and Bingxuan Li and Xingyu Zhao and Leiping Xi},
  doi          = {10.1049/ipr2.70200},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70200},
  shortjournal = {IET Image Process.},
  title        = {A 3D imaging method for UAV swarm polarimetric SAR based on joint sparse and low-rank structure constraints},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer-aided volumetric quantification of pre- and post-treatment intracranial aneurysms in MRA. <em>IETIP</em>, <em>19</em>(1), e70199. (<a href='https://doi.org/10.1049/ipr2.70199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intracranial aneurysm, a cerebrovascular condition involving abnormal arterial dilation, poses a high risk of subarachnoid hemorrhage upon rupture. Accurate quantification is crucial for diagnosis and follow-up treatment. This paper introduces a novel multi-scale dual-attention network (MSDA-Net) for quantification of intracranial aneurysms in MRA images. The proposed framework includes a context aware patch (CAP) module, multi-scale convolutional blocks, and a dual-attention block, where the CAP module extracts center-line patches to address foreground-background imbalance, the multi-scale and dual-attention blocks enable feature extraction of anatomical dependencies for fine-grained segmentation. The framework leverages three morphological features such as locations of aneurysms, vascular bifurcations, and vessel topology using a multi-task learning scheme for better segmentation. MSDA-Net surpasses state-of-the-art models such as U-Net, residual U-Net, attention U-Net, and nnU-net with an improved dice similarity coefficient of 0.71 and a volume similarity of 0.85. Experiments conducted on the publicly available ADAM challenge dataset and a private post-treatment database demonstrate the reliability and performance of this approach. The method could be used in clinical decision-making in aneurysm follow-up and has profound potential for integration into clinical workflows.},
  archive      = {J_IETIP},
  author       = {Subhash Chandra Pal and Chirag Kamal Ahuja and Dimitrios Toumpanakis and Johan Wikstrom and Robin Strand and Ashis Kumar Dhara},
  doi          = {10.1049/ipr2.70199},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70199},
  shortjournal = {IET Image Process.},
  title        = {Computer-aided volumetric quantification of pre- and post-treatment intracranial aneurysms in MRA},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual perception-based quality assessment for stitched panoramic images. <em>IETIP</em>, <em>19</em>(1), e70198. (<a href='https://doi.org/10.1049/ipr2.70198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panoramic image stitching algorithms inevitably introduce distortions due to their inherent limitations. Due to the characteristics of the Human Visual System (HVS), slight stitching distortions have a minimal impact on perceived image quality, whereas severe distortions significantly degrade it. To address this issue, a visual perception-based quality assessment method for stitched panoramic images is proposed. Existing no-reference image quality assessment (NR-IQA) methods often struggle to accurately model the HVS and its subjective perception of stitching distortions, leading to significant discrepancies between predicted and actual quality scores. Therefore, we use the learnt perceptual image patch similarity (LPIPS) to capture perceptual similarity from the HVS, thereby generating a high-quality pseudo-reference image (PRI). To further improve its perceptual quality, the PRI is refined using a visual restoration network (VRN) to reduce residual distortions. Subsequently, both the pseudo-reference and distorted images are fed into a quality assessment network with shared parameters to extract deep feature representations. To improve distortion-aware quality evaluation, we design an Adaptive Feature Fusion (AFF) module, where features from the pseudo-reference and distorted images are first enhanced through an attention mechanism and then adaptively fused via learnt weights for more effective quality assessment. Finally, the fused features are processed through fully connected layers for regression, predicting the perceptual quality score of the stitched panoramic image. Extensive experimental results demonstrate that the proposed method outperforms state-of-the-art approaches, achieving significant improvements across multiple evaluation metrics.},
  archive      = {J_IETIP},
  author       = {Yu Fan and Chunyi Chen and Xiaojuan Hu and Yanfeng Li and Haiyang Yu and Ripei Zhang and Luguang Han},
  doi          = {10.1049/ipr2.70198},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70198},
  shortjournal = {IET Image Process.},
  title        = {Visual perception-based quality assessment for stitched panoramic images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Panoramic depth and semantic estimation with frequency and distortion aware convolutions. <em>IETIP</em>, <em>19</em>(1), e70197. (<a href='https://doi.org/10.1049/ipr2.70197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional images reveal advantages when addressing the understanding of the environment due to the 360-degree contextual information. However, the inherent characteristics of the omnidirectional images add additional problems to obtain an accurate detection and segmentation of objects or a good depth estimation. To overcome these problems, we exploit convolutions in the frequency domain, obtaining a wider receptive field in each convolutional layer, and convolutions in the equirectangular projection, to cope with the image distortion. Both convolutions allow to leverage the whole context information from omnidirectional images. Our experiments show that our proposal has better performance on non-gravity-oriented panoramas than state-of-the-art methods and similar performance on oriented panoramas as specific state-of-the-art methods for semantic segmentation and for monocular depth estimation, outperforming the sole other method which provides both tasks.},
  archive      = {J_IETIP},
  author       = {Bruno Berenguel-Baeta and Jesus Bermudez-Cameo and Jose J. Guerrero},
  doi          = {10.1049/ipr2.70197},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70197},
  shortjournal = {IET Image Process.},
  title        = {Panoramic depth and semantic estimation with frequency and distortion aware convolutions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust low-rank kernel dictionary learning for enhancing noisy images. <em>IETIP</em>, <em>19</em>(1), e70196. (<a href='https://doi.org/10.1049/ipr2.70196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising is a key preprocessing to enhance contaminated images. There have been lots of literature tackling such an issue. Most of them focus on pixel-level processing and ignore intrinsic sparsity and low-rank properties of noise and objects, respectively, in images. To argue this issue, a novel image enhancement model is proposed to eliminate or suppress noise, which integrates robust principal component analysis with kernel dictionary learning (KDL). Specifically, the Kernel trick is introduced to nonlinearly map image data and low-rank dictionary to be learned to high-dimensional spaces so as to separate sparsity noise from contaminated images. The proposed model is abbreviate as RSKDL. In RSKDL, the intrinsic structural characteristic of images is unclosed by adaptively learning the affinity graph of image data so as to ensure the enhanced images inheriting the manifold structure of original images. Meantime, the non-convex sparsity regularization on the residual between original images and enhanced ones is imposed to exclude noise from original data. Extensive experiments on several image datasets show that the proposed model outperforms existing methods for image denoising.},
  archive      = {J_IETIP},
  author       = {Tingquan Deng and Jiaxuan Hui and Qingwei Jia and Jingyu Wang},
  doi          = {10.1049/ipr2.70196},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70196},
  shortjournal = {IET Image Process.},
  title        = {Robust low-rank kernel dictionary learning for enhancing noisy images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-informed SSD object detection for real-time industrial applications. <em>IETIP</em>, <em>19</em>(1), e70195. (<a href='https://doi.org/10.1049/ipr2.70195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates dataset-adaptive optimisation strategies for a Single Shot MultiBox Detector (SSD) network, focusing on a critical case study of parts detection on a manufacturing line. We address the challenges of achieving high-performance object detection in real-time industrial environments by implementing a structural optimisation approach that dynamically reduces feature maps and prior boxes based on dataset characteristics. The network self-identifies and prunes redundant features by analysing the statistical properties of the images and objects in the datasets, leading to a leaner and more efficient SSD network. This dataset-informed reduction of feature maps and anchor boxes accelerates training and inference speeds and enhances detection precision, which is crucial for accurate part identification in high-throughput production. Experimental results on a custom manufacturing line parts dataset, consisting of multi-coloured and differently sized bolts and nuts, demonstrate a mean Average Precision (mAP@50) of approximately 96%-99%, while the inference time was reduced by more than four times (to approximately 3–8 ms in the study case) according to feature extraction backbones, compared to the original network.},
  archive      = {J_IETIP},
  author       = {Quoc Chi Nguyen and Quoc Vinh Ngo and Phuong-Tung Pham and Thanh Huy Phung},
  doi          = {10.1049/ipr2.70195},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70195},
  shortjournal = {IET Image Process.},
  title        = {Data-informed SSD object detection for real-time industrial applications},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLSDNet: A real-time stop-line detection network integrating the line segment detection method. <em>IETIP</em>, <em>19</em>(1), e70194. (<a href='https://doi.org/10.1049/ipr2.70194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stop-line detection aids autonomous driving systems in accurately determining vehicle position and driving status. Existing methods typically rely on bounding boxes, failing to capture stop-line shape. Complex road conditions, such as deteriorated markings or intense lighting, challenge these methods’ detection robustness. To address the above problems, we propose a novel representation approach for stop-lines that balances high precision with real-time detection requirements. Inspired by the prevalence of line features in road markings, we propose SLSDNet—a stop-line segment detection network that fuses image data with line features to prioritise line-rich regions for detection. Furthermore, we employ a multi-task learning scheme to extract stop-line features across multiple dimensions and incorporate a verification mechanism to ensure robust performance. In addition, to address the lack of a stop-line dataset, we collected images from multiple sources and published our stop-line dataset at https://github.com/ChengkangLiu/Stop-Line-Dataset . Experimental results demonstrate that our method achieves the best F1-score (97.02) and PR-AUC (0.9684), outperforming state-of-the-art methods. In terms of efficiency, our method achieves real-time operation speed at 109 FPS with 4.41 M parameters, capable of running on devices with limited computing resources.},
  archive      = {J_IETIP},
  author       = {Chengkang Liu and Yafei Liu and Ding Hu and Xiaoguo Zhang},
  doi          = {10.1049/ipr2.70194},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70194},
  shortjournal = {IET Image Process.},
  title        = {SLSDNet: A real-time stop-line detection network integrating the line segment detection method},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SF-net: Video frame interpolation with a 3D square funnel network. <em>IETIP</em>, <em>19</em>(1), e70193. (<a href='https://doi.org/10.1049/ipr2.70193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame interpolation (VFI) is a problem of designing in-between frames from both the previous and the subsequent frames for enhancing the quality of video. The majority of traditional methods, particularly U-Net-based approaches, suffer from high computational complexity and memory usage in terms of high numbers of parameters. We propose the square funnel network (SF-Net), a novel network structure with significantly fewer parameters but comparable performance, in this paper. SF-Net follows a unique configuration that increases the third dimension of the input frames in deeper layers instead of increasing the number of filters, which results in a more efficient and more compact model. Our model makes use of a maximum of 64 filters in nearly all of its layers, except for the last two layers, which employ 128 filters each. With both objective and subjective evaluation, SF-Net has outstanding visual quality and efficiency, which makes it suitable for low-computational-resource applications. The paper depicts a good direction of VFI, which is to decrease the number of parameters without sacrificing performance.},
  archive      = {J_IETIP},
  author       = {Hamid Azadegan and Ali-Asghar Beheshti Shirazi},
  doi          = {10.1049/ipr2.70193},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70193},
  shortjournal = {IET Image Process.},
  title        = {SF-net: Video frame interpolation with a 3D square funnel network},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive two-tier deep learning for content-based image retrieval and classification with dynamic similarity fusion. <em>IETIP</em>, <em>19</em>(1), e70192. (<a href='https://doi.org/10.1049/ipr2.70192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-Based Image Retrieval (CBIR) systems have difficulties with computing efficiency, illumination robustness and noise sensitivity. Traditional methods rely on handcrafted features or monolithic deep learning architectures, which either lack adaptability to diverse image domains or suffer from high computational complexity. To bridge this gap, a unique two-tier deep learning system is presented in this research to overcome these drawbacks. First, a supervised neural network (SNN) reduces dimensionality and improves interpretability by converting HSV colour space into semantic 2D colour labels through pixel-level classification. This addresses the inefficiency of processing raw RGB data while preserving illumination-invariant colour semantics. Second, a Convolutional Neural Network (CNN) greatly increases computing efficiency by processing these labels rather than raw images. By operating on compressed 2D representations, the system achieves faster inference compared to standard 3D CNN pipelines. The framework presents Variable Weight Overall Similarity (VWOS), a versatile similarity metric that combines semantic (softmax) and structural (MaxPool3) elements with dynamically predicted weights using a neural network to automatically optimise retrieval performance based on image content. This adaptive fusion resolves the limitations of fixed-weight similarity measures in handling heterogeneous query types. The system has achieved a performance with precision@10 scores of 0.9-1.0 and classification accuracies of 0.85-0.98 when tested on the PH 2 , Oxford Flowers, Corel-1k, Caltech-101 and Kvasir datasets. Notably, it outperforms current handcrafted, deep learning and hybrid approaches, achieving 1.0 precision@10 on four datasets and 0.96 accuracy on medical Kvasir images. Quantitative comparisons show 9%–14% higher precision than handcrafted methods, 3%–35% improvement over deep learning baselines, and 12% better than hybrid systems. This approach is especially promising for applications involving multimedia retrieval and medical imaging, where interpretability and accuracy are crucial.},
  archive      = {J_IETIP},
  author       = {Aqeel M. Humadi and Mehdi Sadeghzadeh and Hameed A. Younis and Mahdi Mosleh},
  doi          = {10.1049/ipr2.70192},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70192},
  shortjournal = {IET Image Process.},
  title        = {Adaptive two-tier deep learning for content-based image retrieval and classification with dynamic similarity fusion},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HFGR: Hierarchical fine-grained regression for visibility estimation. <em>IETIP</em>, <em>19</em>(1), e70191. (<a href='https://doi.org/10.1049/ipr2.70191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate visibility estimation is crucial in enormous real-world applications like transportation and autonomous driving. Recent image-based solution has attracted more interests due to the low cost and promising performance. However, existing methods typically regards the visibility estimation as a simple regression problem, which often fails to capture the subtle variations of different visibility levels. To tackle the problem, in this paper, we propose a novel hierarchical fine-grained regression (HFGR) model, which employs a multi-level structure that categorizes visibility into coarse-to-fine ranges and refines these estimated values with a fine-grained regression. Specifically, our designed hierarchical model perceives a global continuity weights, swiftly narrowing down the prediction range, while the small interval regression can also capture local features, achieving a satisfying fine-grained perception. By the proposed hierarchical regression, we can differentiate the subtle varieties between different visibility levels. The proposed HFGR model achieves the significant performance improvements in different fog conditions on our collected real-world highway foggy image dataset, offering a reliable solution for real-world applications.},
  archive      = {J_IETIP},
  author       = {Zhilong Xu and Yaping Huang and Fei Li and Rongmei Guo and Di Zhang and Wenqing Li and Jianyong Guo},
  doi          = {10.1049/ipr2.70191},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70191},
  shortjournal = {IET Image Process.},
  title        = {HFGR: Hierarchical fine-grained regression for visibility estimation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAIR: Privacy attack against image recognition with sample augmentation. <em>IETIP</em>, <em>19</em>(1), e70190. (<a href='https://doi.org/10.1049/ipr2.70190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prevalence of image recognition applications, machine learning (ML) poses a severe privacy risk. Its private data can be stolen through a variety of privacy attacks. However, most of them typically need detailed prior knowledge of the intended ML model, such as its structure and parameters, which is unrealistic for usage in real-world scenarios. To address this issue, a novel privacy attack approach against image recognition (PAIR) is proposed. Instead of involving numerous training examples, PAIR only considers small samples and utilizes a sample augmentation technique to create new instances with balanced attributes. Meanwhile, it also uses an upsampling interpolation method to resolve the high distortion issue of recovery images. In contrast, PAIR reduces reliance on the target model's architecture while still requiring certain prior knowledge, such as the model's input features and attack objectives. PAIR's performance is evaluated across a variety of settings and datasets. The experiments demonstrate that PAIR has performance benefits in image recovery and significantly outperforms representative privacy attacks. PAIR retains the attack effectiveness while reducing prior knowledge requirements, thereby enhancing the flexibility and applicability of the attack. In addition, dropout-based and noise-based defenses against privacy attacks are presented, and their performance is assessed.},
  archive      = {J_IETIP},
  author       = {Yanru Feng and Qingjie Liu},
  doi          = {10.1049/ipr2.70190},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70190},
  shortjournal = {IET Image Process.},
  title        = {PAIR: Privacy attack against image recognition with sample augmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSAG: Semantic enhancement and image generation based on multimodal large models for supporting few-shot learning. <em>IETIP</em>, <em>19</em>(1), e70189. (<a href='https://doi.org/10.1049/ipr2.70189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, existing few-shot learning methods encounter significant bottlenecks in semantic enhancement and data augmentation. Traditional prompt templates are limited by their fixed format, making it difficult to fully capture the characteristics of categories. On the other hand, category description methods based on large language models are susceptible to category polysemy, which can result in semantic bias. Therefore, we propose a multimodal semantic enhancement (MSE) module, which jointly analyzes the visual-semantic relationship between category names and example samples through a multimodal large model. By leveraging visual information to guide the generation of discriminative category descriptions, MSE effectively mitigates semantic polysemy issues. To mitigate the issue of insufficient support set data, we introduce a multimodal image generation (MIG) module, which utilizes the image generation capability of text-to-image models and generates diverse images based on various textual information. Additionally, we draw inspiration from the prototypical networks and combine it with gaussian discriminant analysis to build a training-free visual-textual classifier. Our method (MSAG) significantly improves classification accuracy across 15 benchmark datasets, validating the effectiveness of the multimodal information collaborative enhancement strategy in alleviating the problem of data scarcity.},
  archive      = {J_IETIP},
  author       = {Jia Zhao and Ziyang Cao and Huiling Wang and Xu Wang and Yingzhou Chen},
  doi          = {10.1049/ipr2.70189},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70189},
  shortjournal = {IET Image Process.},
  title        = {MSAG: Semantic enhancement and image generation based on multimodal large models for supporting few-shot learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced traffic sign detection network using YOLOv9 with integrated multiscale feature fusion and context-aware aggregation. <em>IETIP</em>, <em>19</em>(1), e70188. (<a href='https://doi.org/10.1049/ipr2.70188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign detection (TSD) plays a critical role in real-world applications. Advanced TSD frameworks have achieved promising results on public TSD datasets. Identifying and detecting traffic signs in challenging environments has garnered significant attention, where the accurate recognition of obscured and small signs is a critical challenge. Recent frameworks have achieved remarkable performance but struggle to detect small, occluded signs and recognise them under diverse atmospheric conditions. To address these problems, we propose a multiscale feature fusion and multiscale context-aware aggregation (MSCA) framework using You Only Look Once Version 9 (YOLOv9) for TSD. Multiscale feature fusion helps filter out less useful features, leading to more informative representations, and combines fine-grained lower-level features with semantically rich higher-level features. In addition, MSCA fusion captures broader contexts without compromising spatial resolution and increases the receptive field without increasing parameters or computational cost, unlike methods that involve increasing kernel size or pooling. The MSCA-YOLO method enhances the extracted features, focusing more on meaningful input areas, minimising the effects of size and weather differences, and improving the overall robustness of the system. The extensive experiments on the Tsinghua-Tencent 100K and Changsha University of Science and Technology Chinese Traffic Sign Detection Benchmark 2021 datasets thoroughly assess the proposed method, achieving state-of-the-art results.},
  archive      = {J_IETIP},
  author       = {Ruturaj Mahadshetti and Cho-Rong Yu and Jinsul Kim and Tai-Won Um},
  doi          = {10.1049/ipr2.70188},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70188},
  shortjournal = {IET Image Process.},
  title        = {Enhanced traffic sign detection network using YOLOv9 with integrated multiscale feature fusion and context-aware aggregation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel concatenated feature pyramid network for dehazing a single image on smartphone images. <em>IETIP</em>, <em>19</em>(1), e70187. (<a href='https://doi.org/10.1049/ipr2.70187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smartphones capturing images in outdoor environments are often affected by adverse weather conditions, resulting in low-quality images. This paper introduces the Parallel Concatenated Feature Pyramid Network (C-FPN) to address the challenge of dehazing single smartphone images. Dehazing a single image on smartphones is considered an ill-posed problem. While the Feature Pyramid Network (FPN) is widely used in computer vision tasks, its feature extraction is limited by the max-pooling operator. Furthermore, it cannot retain the hazy feature and restore the image at the same time, which fails to preserve critical hazy image features. Additionally, most existing methods struggle to balance preserving haze-relevant information with effective image restoration. To address these limitations, this study proposes a novel parallel concatenated FP architecture that estimates atmosphere light and calculates transmission information on smartphones. The key contributions of this paper include (1) designing a parallel concatenated FP architecture capable of retrieving hazy features across various environments in deeper layers, (2) incorporating a concatenation structure to retain hazy information, enabling depth estimation and the generation of a transmission map, (3) using the transmission map as an input for a convolutional neural network with a dehazing loss function to calculate atmosphere light under different environments, and (4) implementing a skipping connection in the C-FPN to retain essential features, facilitating an end-to-end learning structure. The proposed method demonstrates superior performance on the SOTS, NH-HAZE 2, and synthetic hazy image indoor datasets. The PSNR/SSIM achieve 26.58/0.948, 26.28/0.966 and 17.15/0.761, respectively. In addition to dehazing, the method achieves excellent object detection performance.},
  archive      = {J_IETIP},
  author       = {Yu-Shiuan Tsai and Yi-Zeng Hsieh and Kai-En Lin and Pin-Hsiang Wang},
  doi          = {10.1049/ipr2.70187},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70187},
  shortjournal = {IET Image Process.},
  title        = {Parallel concatenated feature pyramid network for dehazing a single image on smartphone images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized gene and image-based feature selection using modified genetic algorithms and deep learning for predictive skin cancer detection. <em>IETIP</em>, <em>19</em>(1), e70186. (<a href='https://doi.org/10.1049/ipr2.70186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene selection is critical for cancer diagnosis because the ability to discover specific biomarkers has a major impact on diagnostic accuracy. Traditional approaches frequently struggle with high-dimensional genomic data, where irrelevant or redundant characteristics might impair machine learning algorithms. Despite advances in computational approaches, there is a gap in the optimization of deep learning models for gene selection, particularly in terms of selecting the best model architecture and hyperparameters. This paper addresses three critical challenges in genomic biomarker discovery for cancer diagnosis: (1) the high-dimensional nature of gene expression data, (2) the need for biologically interpretable feature selection, and (3) the optimization of deep learning architectures for genomic analysis. We present a novel hybrid approach combining modified genetic algorithms with deep neural networks to overcome limitations of traditional methods in handling feature redundancy and computational complexity. Our methodology introduces three key innovations: a dynamic mutation operator that adapts to population diversity, multi-objective optimization balancing classification accuracy with biological pathway relevance, and simultaneous co-evolution of both gene subsets and neural network architectures. The proposed system achieves state-of-the-art performance, with 99.1% accuracy, 98.9% AUC-ROC, and 99.0% F1-score on the ISIC 2020 dataset, while maintaining clinically relevant sensitivity (98.0%) and specificity (98.5%). Extensive validation across six benchmark datasets demonstrates consistent improvements over existing machine learning and deep learning techniques, particularly in handling rare cancer subtypes and low-resolution images. Future research directions include: (1) integration of multi-modal clinical data to enhance rare subtype detection, (2) development of federated learning frameworks for privacy-preserving distributed analysis, and (3) creation of explainability tools to bridge the gap between computational feature selection and clinical interpretation. The results establish our evolutionary optimization approach as both a high-performance diagnostic tool and a flexible framework for advancing precision oncology research.},
  archive      = {J_IETIP},
  author       = {Saadya Fahad Jabbar and M. A. Balafar and Ali Jameel Hashim},
  doi          = {10.1049/ipr2.70186},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70186},
  shortjournal = {IET Image Process.},
  title        = {Optimized gene and image-based feature selection using modified genetic algorithms and deep learning for predictive skin cancer detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MemAttn-CL: Unified memory, attention, and contrastive learning for enhanced text-to-image generation. <em>IETIP</em>, <em>19</em>(1), e70185. (<a href='https://doi.org/10.1049/ipr2.70185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating photo-realistic images from natural language descriptions is a challenging task at the intersection of natural language processing and computer vision. Text-to-image synthesis involves generating visual images in a way which naturally matches the semantic meaning of the input text. Recent diffusion-based models have demonstrated strong performance in image fidelity but are slow in inference and exhibit coarse semantic alignment. To overcome the two problems above and allow images to be more faithful (realistic) to texts and semantics in the wild, we propose a novel hybrid architecture called DM-GAN+ATT+CL (dynamic memory GAN + contrastive learning and attention mechanisms). Our method proceeds in a two-step manner: we first produce low-resolution images based on the DM-GAN model with dual attention modules and then refine the results through a memory-based feature refinement mechanism. Contrastive learning was then utilized on a separate dataset with high resolution image-text pairs to enhance feature discrimination and strengthen semantic consistency. The result is richer semantic relevance, stronger image variation and better visual quality. Extensive experimental results across multiple benchmark datasets—CUB, Oxford-102, MS-COCO, and MM-CelebA-HQ—demonstrate that the proposed DM-GAN+ATT+CL framework consistently outperforms state-of-the-art baselines. Notably, it achieved an R-precision of 95.24, an inception score (IS) of 38.43, and a Fréchet inception distance (FID) of 11.30 on the MS-COCO dataset, with similarly strong and consistent performance observed across the other datasets. These findings indicate that our approach substantially enriches the diversity and reality of synthetic images, promising a better future for text-image matching.},
  archive      = {J_IETIP},
  author       = {Md. Ahsan Habib and Md. Anwar Hussen Wadud and Mohammad Motiur Rahman and M. F. Mridha},
  doi          = {10.1049/ipr2.70185},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70185},
  shortjournal = {IET Image Process.},
  title        = {MemAttn-CL: Unified memory, attention, and contrastive learning for enhanced text-to-image generation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FCUnet: An underwater image enhancement hybrid network via fused feature-guided cross-attention. <em>IETIP</em>, <em>19</em>(1), e70184. (<a href='https://doi.org/10.1049/ipr2.70184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images captured by optical vision systems often suffer from degradation due to the absorption and scattering of reflected light. To mitigate the impact of the complex underwater environment on imaging quality, data-driven methods for underwater image enhancement have emerged as an effective strategy. However, convolutional neural networks exhibit limitations in processing global information interactions due to their inductive biases. Concurrently, transformers often overlook local details. To address these challenges, we propose FCUnet, a hybrid network designed to enhance the quality of underwater images. First, we propose a colour deviation preprocessing module that incorporates a multi-scale channel-wise attention for the fusion feature map to capture differences in rich spatial hierarchical information across channels. Second, we design the cross-attention block guided by multiple features to efficiently acquire local fine details and global scene characteristics. Finally, a universal feature fusion unit and a colour loss term are proposed to enhance the network's sensitivity to colour and texture deviation information. Ablation studies confirm the effectiveness of each individual component. Experimental results on public benchmark datasets demonstrate that the proposed FCUnet outperforms other competitive methods, achieving superior performance in both qualitative and quantitative evaluations.},
  archive      = {J_IETIP},
  author       = {Jie Zhu and Huibin Wang and Zhe Chen and Lili Zhang and Chunyan Ma},
  doi          = {10.1049/ipr2.70184},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70184},
  shortjournal = {IET Image Process.},
  title        = {FCUnet: An underwater image enhancement hybrid network via fused feature-guided cross-attention},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time underwater vision sensing system for AUV tracking. <em>IETIP</em>, <em>19</em>(1), e70183. (<a href='https://doi.org/10.1049/ipr2.70183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of emerging technologies such as big data and artificial intelligence, the related technologies of perception tasks on which underwater object tracking relies have made great progress. However, a significant barrier still exists in implementing high-performance real-time underwater object tracking on low-power edge devices. To achieve real-time tracking of underwater objects for edge devices, this article develops an underwater real-time visual sensing system applied to AUV tracking. First, an underwater object tracking device is designed in this article employing a stereo binocular camera, an edge embedded NVIDIA Jetson Xavier NX and an STM32 control board. Then, after preprocessing, the input image with the effective USM algorithm, we propose a quick approach for detecting underwater objects based on SIoU-YOLOv8n, which enables automatic object recognition and selection. At the same time, this article proposes a twin network UW-Siam for continuous tracking of underwater objects, which achieves more accurate underwater object tracking. Finally, the algorithm is deployed to the designed real-time underwater vision sensing system and tested in real-world scenarios. The tracking accuracy reached 0.652, and the detection mAP reached 0.97. The results indicate that the system can rapidly detect and continuously monitor objects, performing well in real-world scenarios with high accuracy and robustness.},
  archive      = {J_IETIP},
  author       = {Canrong Chen and Tingzhuang Liu and Linglu He and Yi Zhu and Fei Yuan},
  doi          = {10.1049/ipr2.70183},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70183},
  shortjournal = {IET Image Process.},
  title        = {Real-time underwater vision sensing system for AUV tracking},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DyLoFViT: A novel approach for real-time metal 3D printing surface quality classification. <em>IETIP</em>, <em>19</em>(1), e70182. (<a href='https://doi.org/10.1049/ipr2.70182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time surface quality monitoring of selective laser melting (SLM) is critical for defect identification and process stability but is still plagued by the trade-off between high precision and efficiency. We propose DyLoFViT, a light-weight vision transformer specifically optimized for industrial SLM defect classification. DyLoFViT makes four significant architectural contributions: (1) dynamic positional encoding that retains spatial coherence and facilitates translation invariance, (2) dynamic sparse gating that adaptively filters the most informative tokens, (3) Nyström-based sparse self-attention that estimates long-range dependencies with lower computation, and (4) frequency-domain compression with discrete cosine transform that filters high-frequency noise with preservation of low-frequency structure vital for defect classification. Experiments on an industrial test set of 1320 high-resolution images of four melt quality classes show that DyLoFViT achieves a Top-1 accuracy of 96.1% and an F1-score of 95.6% with merely 13.2M parameters and 2.9 GFLOPs. Compared with the prior CNN and transformer architectures, DyLoFViT achieves superior precision with an order of magnitude lower inference latency (0.3 ms/image), making it extremely well-suited for real-time, edge-device deployment in additive manufacturing. DyLoFViT hence sets a new standard for efficient, accurate in situ metal 3D printing defect classification.},
  archive      = {J_IETIP},
  author       = {Yuqin Zeng and Lianli Liu and Ze Wen and Jiquan Liu and Shuqian Fan},
  doi          = {10.1049/ipr2.70182},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70182},
  shortjournal = {IET Image Process.},
  title        = {DyLoFViT: A novel approach for real-time metal 3D printing surface quality classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A DNA-dynamic permutation-diffusion algorithm for image encryption using scaling chaotification models and advanced DNA operations. <em>IETIP</em>, <em>19</em>(1), e70181. (<a href='https://doi.org/10.1049/ipr2.70181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise in cyber threats to digital images over networks is a primary problem for both private and government organisations. Image encryption is considered a useful way to secure the digital image; however, it faces critical challenges such as weak key generation, chosen-plaintext attacks, high overhead, and scalability. To overcome these challenges, this paper proposes the DNA-Dynamic Concurrent Permutation-Diffusion Algorithm (DNA-DCP-DA), which introduces four advanced encryption mechanisms. Firstly, new scaling chaotification models are introduced to enhance chaotic properties, achieving superior results in bifurcation, Lyapunov Exponent (LE), Sample Entropy (SEn), Kolmogorov Entropy (KEn) and key generation. Secondly, a Key Vectorisation Method (KVM) is proposed to optimise execution time and reduce the computational overhead of chaotic map iterations. Thirdly, robust non-commutative DNA operations are introduced, including DNA hybrid and circular shift operations to enhance encryption security. Finally, integrate permutation and dynamic diffusion processes, strengthening security and improving efficiency. To evaluate the proposed algorithm, extensive experiments have been conducted, and results have been compared with the latest encryption algorithms. This shows the proposed encryption algorithm is better, with superior results for correlation results close to zero and Information Entropy (IE) larger than 7.999. The Number of Pixel Change Rates (NPCR) exceeds 99.6%, and the Uniform Average Change Intensity (UACI) is above 33.4%. The algorithm encrypts an image of size 256 × 256 in 0.1255 s, with a key space reaching 2 697 . As a result, the proposed system establishes a new benchmark for secure and efficient image encryption against cyber threats.},
  archive      = {J_IETIP},
  author       = {Mustafa Kamil Khairullah and Mohd Zafri Bin Baharuddin and Reema Thabit and Mohammad Ahmed Alomari and Gamal Alkawsi and Faten A. Saif},
  doi          = {10.1049/ipr2.70181},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70181},
  shortjournal = {IET Image Process.},
  title        = {A DNA-dynamic permutation-diffusion algorithm for image encryption using scaling chaotification models and advanced DNA operations},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDM-TLI: Multi-defect detection model for transmission line insulators. <em>IETIP</em>, <em>19</em>(1), e70180. (<a href='https://doi.org/10.1049/ipr2.70180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant progress has been made in detecting defects in transmission line insulators using aerial imagery. However, the application of these methods in complex power environments continues to face challenges such as intricate backgrounds, small target sizes, multiple scales and varied morphologies. To address these issues, this study introduces a multi-defect detection model for transmission line insulators (MDM-TLI). Firstly, MDM-TLI incorporates a hybrid attention model based on a channel separation strategy, which enhances the model's focus on defect regions, effectively mitigating the difficulty of extracting small target features amidst complex backgrounds. Secondly, the model employs a scale-balanced loss function, that introduces a scale balance factor designed to eliminate discrepancies in the position and shape calculations of bounding boxes across different scales. Lastly, an adaptive feature fusion module is utilised, allowing the model to dynamically adjust the fusion ratios of various features during the integration of multi-level features. This reinforces the model's capability to represent multi-morphological features comprehensively. Experimental results demonstrate that MDM-TLI significantly enhances the performance of insulator defect detection. Specifically, the detection accuracies for insulator shedding, damage, and flashover defects reached 97.6%, 84.5% and 80.7%, respectively. These results outperform those of existing models, highlighting the effectiveness of MDM-TLI and providing robust technical support for the intelligent maintenance of transmission lines. The project code is released at https://github.com/albertsolus/powercv .},
  archive      = {J_IETIP},
  author       = {Li-Hui Zhao and Yang An and Yurong Li and Lizhen Fu and Shihan Liu and Lu Wang},
  doi          = {10.1049/ipr2.70180},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70180},
  shortjournal = {IET Image Process.},
  title        = {MDM-TLI: Multi-defect detection model for transmission line insulators},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of hand-crafted and data-driven feature extraction on signature identification: A comparative study. <em>IETIP</em>, <em>19</em>(1), e70179. (<a href='https://doi.org/10.1049/ipr2.70179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signature identification is the process of verifying the authenticity of a signature by analyzing its unique characteristics. In this paper, we have done a comparative study to investigate how hand-crafted and data-driven-based features could impact signature identification. In hand-crafted features, we extract the key points using a well-known scale-invariant feature transform (SIFT) algorithm. Then, we create two different representations for the signature image: (1) a graph representation, which shows the structural relationship between keypoints; and (2) a visual word histogram representation, which ignores the structural knowledge. In data-driven features, we fed a raw image into two pre-trained residual network (ResNet) and VGGNet to extract local structural features from the images. Then, models were fine-tuned by our data. Two well-known datasets, the MCYT-75 and the UT-Sig, datasets, are used to assess the approach. The obtained results show that the graph representation for the sign image has a more discriminative ability to differentiate the original signatures from the forgeries.In this paper, we have done a comparative study investigating how hand-crafted and data-driven-based features could impact signature identification. In hand-crafted features, we extract the key points using a well-known SIFT algorithm.},
  archive      = {J_IETIP},
  author       = {Ali Badie and Hedieh Sajedi},
  doi          = {10.1049/ipr2.70179},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70179},
  shortjournal = {IET Image Process.},
  title        = {The impact of hand-crafted and data-driven feature extraction on signature identification: A comparative study},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight zero-shot superresolution reconstruction of fundus images based on residual information distillation and multi-feature fusion. <em>IETIP</em>, <em>19</em>(1), e70178. (<a href='https://doi.org/10.1049/ipr2.70178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fundus photography provides imaging techniques for the diagnosis of retinal diseases. The diagnostic accuracy, however, heavily relies on the clarity of subtle lesions, which can be significantly affected by image resolution. Achieving a balance between reconstruction quality, model complexity, and training efficiency remains a key challenge, particularly under limited data conditions. To address these issues, a lightweight end-to-end model LiteZSSR is proposed for super-resolution reconstruction of fundus images, incorporating a residual information distillation module to extract multi-scale features within a shallow network architecture, effectively retaining both local and global contextual information. In addition, a multi-feature fusion group composed of multiple large kernel attention blocks is designed to strengthen feature representation while minimizing redundancy and computational overhead. Unsupervised training based on internal image learning is adopted to eliminate dependence on large-scale datasets and to suppress artifacts commonly produced by CNN-based SRR methods. Extensive experiments on publicly available fundus image datasets, including DRIVE, STARE, and CHASEDB1, demonstrate that LiteZSSR outperforms existing state-of-the-art methods in terms of PSNR and SSIM, while significantly reducing model parameters. These results highlight its potential for practical deployment in clinical fundus image enhancement tasks.},
  archive      = {J_IETIP},
  author       = {Xiaoxin Guo and Guangqi Yang and Weijia Wu and Yihuan Wei and Zhengyang Yu and Hongliang Dong and Songtian Che},
  doi          = {10.1049/ipr2.70178},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70178},
  shortjournal = {IET Image Process.},
  title        = {Lightweight zero-shot superresolution reconstruction of fundus images based on residual information distillation and multi-feature fusion},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAP-YOLOv8: Hierarchical feature optimization with dual-stage attention for object detection in remote sensing scenes. <em>IETIP</em>, <em>19</em>(1), e70177. (<a href='https://doi.org/10.1049/ipr2.70177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image target recognition plays an important role in environmental monitoring, disaster management, urban planning, and traffic systems. To address issues such as high missing detection rates of dense small targets and insufficient multi-scale feature interaction in remote sensing image target detection, an improved PAP-YOLOv8 network architecture was proposed in this study based on YOLOv8. This framework included three core innovations: (1) the pixel dynamic convolution attention module (PDCA), which enhanced small target detection capabilities in complex backgrounds through direction-sensitive convolution and a dual-stage attention mechanism; (2) the adaptive channel attention network (ACAN), which optimized multi-scale feature fusion and suppressed background interference via dynamic weight allocation and channel recalibration; (3) the pyramidal wavelet attention fusion block (PWAFB), which synergistically integrated wavelet multi-resolution analysis with composite attention mechanisms to model spatial details and channel relationships in remote sensing images through a collaborative decomposition and reconstruction workflow. Experiments on the Detection in Optical Remote Sensing (DIOR) dataset showed that the mAP@0.5 of PAP-YOLOv8 reached 86.9%, an increase of 5.3% over the baseline YOLOv8, while demonstrating enhanced detection capability for small and multi-scale targets. Compared with other detection models, PAP-YOLOv8 achieves a balanced trade-off between network size and detection capability, demonstrating superior overall performance with an F1 score of 83.1%. This study confirms the potential of PAP-YOLOv8 for satellite monitoring applications and provides a valuable reference for improving target detection performance in optical remote sensing satellite imagery.},
  archive      = {J_IETIP},
  author       = {Pengfei Zhang and Jian Liu and Jianqiang Zhang and Xingda Li},
  doi          = {10.1049/ipr2.70177},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70177},
  shortjournal = {IET Image Process.},
  title        = {PAP-YOLOv8: Hierarchical feature optimization with dual-stage attention for object detection in remote sensing scenes},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Referring semantic segmentation with implicit patch aligned distillation learning. <em>IETIP</em>, <em>19</em>(1), e70175. (<a href='https://doi.org/10.1049/ipr2.70175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CLIP model has demonstrated impressive zero-shot open-vocabulary classification capabilities. Several strategies have been proposed for open-vocabulary segmentations without mask annotations, either by explicit patch tokens alignment or a grouping method with large-scale contrastive learning. To balance the performance and training efficiency, we propose a referring semantic segmentation model with implicit patch-aligned distillation learning (RiPAD). RiPAD associates the object-ness features from the region-based method with the patch tokens from the vision encoder, then the patch tokens with region-guidance are aligned with the text embedding from the text encoder by distillation. RiPAD achieves comparable performance on three public datasets and establishes a new baseline for open-vocabulary semantic segmentation based on CLIP without mask annotations.},
  archive      = {J_IETIP},
  author       = {Xiaohu Liu and Yichuang Luo and Wei Sun},
  doi          = {10.1049/ipr2.70175},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70175},
  shortjournal = {IET Image Process.},
  title        = {Referring semantic segmentation with implicit patch aligned distillation learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FMSA: Frequency-matching and spatial attention for image style transfer. <em>IETIP</em>, <em>19</em>(1), e70174. (<a href='https://doi.org/10.1049/ipr2.70174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary style transfer, a highly flexible technique with broad application potential, has gained significant attention recently. However, effectively disentangling content and style to synthesize novel stylized images remains a major challenge in arbitrary style transfer tasks. Existing methods primarily perform content and style disentanglement in either a single spatial domain or the frequency domain. We propose a style transfer method based on frequency-matching and spatial attention called FMSA. FMSA consists of two components: frequency feature matching (FFM) and adaptive spatial attention (ASA). In the spatial domain, an attention mechanism is employed to learn attention scores from the content and style feature maps. These scores are then used to perform point-wise normalization, generating synthesized features. In the frequency domain, filters are first applied to process the content and style features. The style-transferred features are subsequently aligned with the magnitude of the style features and the phase of the content features, thereby achieving content-style disentanglement and reconstruction. We introduce a frequency domain loss to reinforce content consistency in the stylized image while effectively transferring the style patterns to improve image synthesis quality. In addition, we design a decoder based on fast fourier convolution (FFC) that can efficiently decode both global and local information to produce the output. Extensive quantitative and qualitative experiments have demonstrated that the proposed method exhibits superior performance in style transfer tasks, generating results with enhanced stylistic patterns and visual quality. Project page: https://github.com/crFlower/FMSA},
  archive      = {J_IETIP},
  author       = {Junyan Zhu and Xin Yu and Libo Xu and Huanda Lu and Hui Xiao and Yiting Li},
  doi          = {10.1049/ipr2.70174},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70174},
  shortjournal = {IET Image Process.},
  title        = {FMSA: Frequency-matching and spatial attention for image style transfer},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCS-UGAN: Multiple colour space underwater GAN for underwater image enhancement. <em>IETIP</em>, <em>19</em>(1), e70173. (<a href='https://doi.org/10.1049/ipr2.70173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured by underwater robots often suffer from issues such as blurring and colour distortion, which hinder effective feature extraction and target recognition in underwater environments. To address these challenges, this paper proposes a novel underwater image enhancement method based on generative adversarial networks (GANs), termed multiple colour space underwater generative adversarial network (MCS-UGAN). The proposed method is built upon a GAN framework, consisting of a generator and a discriminator. The generator comprises two main modules: a deblurring module and a colour correction module. The deblurring module innovatively incorporates an efficient multi-scale feature extraction technique and an attention mechanism, which enhances object contours while preserving fine image details. The colour correction module integrates residual blocks into the U-Net architecture, effectively mitigating the problems of gradient vanishing and explosion during backpropagation in underwater image enhancement networks, thereby enhancing the network's feature learning capability. This design corrects colour distortions while preserving edge information in the image. The discriminator adopts the PatchGAN structure, which focuses on the local regions of the image, significantly improving the generator's ability to restore high-frequency details and thus enhancing the quality of the generated images. Experimental results on benchmark datasets demonstrate that, compared to existing methods, MCS-UGAN achieves superior performance in terms of peak signal-to-noise ratio, structural similarity index measure, underwater image quality measure, and underwater colour image quality evaluation, with average values of 26.24, 0.91, 3.13, and 0.64, respectively. Results from real-world applications further show that MCS-UGAN effectively increases the number of extracted corner points, validating its practicality and effectiveness. The code is available at https://github.com/invincibility6/MCS-UGAN.git},
  archive      = {J_IETIP},
  author       = {Zihang Zhou and Chaoliang Zhong and Qiang Lu},
  doi          = {10.1049/ipr2.70173},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70173},
  shortjournal = {IET Image Process.},
  title        = {MCS-UGAN: Multiple colour space underwater GAN for underwater image enhancement},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure color traffic image transmission through enhanced visual encryption. <em>IETIP</em>, <em>19</em>(1), e70172. (<a href='https://doi.org/10.1049/ipr2.70172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of intelligent transportation systems (ITS), secure processing of color traffic images has become critical for traffic management, safety monitoring, and flow analysis. However, ITS are increasingly susceptible to malicious attacks and data tampering. To address this, we propose an advanced visual encryption method specifically for color traffic images. Our approach employs discrete wavelet transform (DWT) to represent images sparsely, followed by dynamic three-dimensional spiral disruption for encryption. Chaotic systems then compress the encrypted data, embedding it into a carrier image via least significant bit (LSB) techniques. Experimental results demonstrate robust visual security, achieving a peak signal-to-noise ratio (PSNR) above 42 dB, strong resilience against common attacks, and a substantial a key space of 2 512 ×10 60 , effectively resisting brute-force attacks. Neighboring pixel correlation coefficients drop below 0.15, compared to original images (>0.87), and under shear resistance attack with 128×128 data loss, PSNR remains above 23 dB. Additionally, employing P-tensor product compressed sensing significantly reduces measurement matrix dimensionality, enhancing transmission efficiency. This method offers a viable solution for secure storage and transmission in modern ITS, significantly bolstering privacy and system robustness. The implementation is available at: https://gitee.com/zhangshuo-aly/image-encryption .},
  archive      = {J_IETIP},
  author       = {Shuo Zhang and Yiqi Huang and Yueping Wang and Wei Jiang and Bin Wang},
  doi          = {10.1049/ipr2.70172},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70172},
  shortjournal = {IET Image Process.},
  title        = {Secure color traffic image transmission through enhanced visual encryption},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor truncated schatten-p norm approximation tensor completion algorithm. <em>IETIP</em>, <em>19</em>(1), e70171. (<a href='https://doi.org/10.1049/ipr2.70171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image data is often degraded during transmission due to hardware limitations or human error, which may hinder subsequent image analysis tasks. Therefore, research on image restoration has significant practical value. Traditional matrix-based algorithms struggle with high-dimensional data, often failing to preserve spatial structures and risking overfitting. In this paper, we investigate tensor recovery problems under the tensor singular value decomposition framework. We introduce a non-convex surrogate for the tensor rank—the tensor truncated Schatten- norm—and propose two recovery models based on this theory: a tensor completion model and a tensor robust principal component analysis model. Efficient solutions based on the alternating direction method of multipliers are developed for both models. Moreover, we provide a thorough analysis of the computational complexity and convergence behavior of our algorithms. At last, extensive experiments on synthetic data, color images, video sequences, multispectral images, and medical images demonstrate the effectiveness and robustness of the proposed methods.},
  archive      = {J_IETIP},
  author       = {Jianwei Liu and Liangfu Lu and Ping Wang and Haipeng Liu and Yuanchen Huang and Yunliang Zang},
  doi          = {10.1049/ipr2.70171},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70171},
  shortjournal = {IET Image Process.},
  title        = {Tensor truncated schatten-p norm approximation tensor completion algorithm},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight detection network for small target defects in shaft lining crack. <em>IETIP</em>, <em>19</em>(1), e70170. (<a href='https://doi.org/10.1049/ipr2.70170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress-induced cracks in shaft linings, if left unmonitored during their developmental stages, may ultimately lead to structural rupture and severe safety incidents. To address this critical issue, this study proposes S-YOLOv8n, a lightweight intelligent detection model designed for the automated and precise identification of micro-cracks in shaft linings. The proposed framework incorporates two key innovations: (1) A lightweight convolutional downsampling enhancement (CDE) module that synergistically combines standard convolution, depthwise separable convolution, and ECA attention mechanisms. This architecture effectively mitigates complex background interference while preserving crack features during downsampling, enhancing shallow network feature extraction capabilities. (2) A GS-C2f feature fusion module that optimizes both spatial characteristics of crack morphology and semantic representations of crack severity, concurrently improving inference efficiency. Experimental validation on a custom shaft lining dataset demonstrates that S-YOLOv8n achieves 1.4% higher detection accuracy than baseline YOLOv8n while reducing model parameters and computational costs by 32.0% and 27.8%, respectively. These advancements significantly contribute to practical implementations of automated crack detection in underground infrastructure maintenance.},
  archive      = {J_IETIP},
  author       = {Rui Hu and Xiaofen Jia and Xiaolei Han and Zailiang Jiang and Baiting Zhao},
  doi          = {10.1049/ipr2.70170},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70170},
  shortjournal = {IET Image Process.},
  title        = {Lightweight detection network for small target defects in shaft lining crack},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SUNeXt: Lightweight medical image segmentation network based on grouped feature fusion and shifted large kernel convolution. <em>IETIP</em>, <em>19</em>(1), e70168. (<a href='https://doi.org/10.1049/ipr2.70168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve efficient image segmentation in practical medical applications in resource-constrained point-of-care environments, the lightweight medical image segmentation network is proposed based on grouped feature fusion and large kernel convolution, which introduces a U-shaped, convolution-based architecture that significantly reduces parameters and computational cost. The proposed model combines shifted large kernel convolution with grouped feature fusion technique in a lightweight and attention-free way, which is specifically designed to fuse features to capture global context. Meanwhile, the grouped multi-scale feature fusion module is proposed to achieve effective cross-layer connectivity and efficient fusion of multi-scale features by grouping deep and shallow features and subsequently applying a lightweight grouped large kernel convolution. The extensive experiments on multiple datasets verify that our model outperforms current popular models in image segmentation with lower parameter quantity and computational cost, and achieves industry-leading performance with low resource consumption.},
  archive      = {J_IETIP},
  author       = {Cong Chen and Xiaoxin Guo and Hangyuan Cheng and Guangqi Yang and Hongliang Dong},
  doi          = {10.1049/ipr2.70168},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70168},
  shortjournal = {IET Image Process.},
  title        = {SUNeXt: Lightweight medical image segmentation network based on grouped feature fusion and shifted large kernel convolution},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ellipse fitting of planar points with outliers using random samples filtered by fitting qualities. <em>IETIP</em>, <em>19</em>(1), e70167. (<a href='https://doi.org/10.1049/ipr2.70167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ellipse fitting is a traditional approach to construct elliptical models from points. Outliers can significantly distort a fittied ellipse from the actual model. A novel ellipse fitting algorithm is proposed to be applied to planar points with outliers. The fitting qualities of candidate ellipses generated by five-point random sampling are evaluated, and the median curve of the best fittings yields the final result through a classic fitting algorithm. A fitting error metric is introduced. The proposed algorithm achieved median fitting errors of 0.067 on a synthetic dataset and 0.057 on an image dataset, respectively, both the best among the algorithms compared. The execution speed of the novel algorithm is on average 0.091 s on the synthetic dataset and 0.087 s on the image dataset. The algorithm is advantageous also for use due to the comprehensibility and insensitivity of the algorithmic parameters.},
  archive      = {J_IETIP},
  author       = {Qi Zeng and Xin Li and Siyu Guo},
  doi          = {10.1049/ipr2.70167},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70167},
  shortjournal = {IET Image Process.},
  title        = {Ellipse fitting of planar points with outliers using random samples filtered by fitting qualities},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Histopathology image enhancement using multi-resolution deep learning techniques. <em>IETIP</em>, <em>19</em>(1), e70166. (<a href='https://doi.org/10.1049/ipr2.70166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate analysis of histopathology images is essential for disease diagnosis and treatment planning. However, the quality of digital pathology slides is often limited by scanner resolution, which can compromise diagnostic precision and patient care. To address this challenge, we conducted a comparative study evaluating four state of the art image enhancement methods: real enhanced super resolution generative adversarial network (Real-ESRGAN), SwinIR, multi scale image restoration network v2 (MIRNet-v2) and super resolution CNN (SRCNN). Our assessment focused on both quantitative metrics peak signal to noise ratio (PSNR) and structural similarity index (SSIM) and qualitative visual analysis to evaluate detail preservation. The experimental results revealed that SwinIR achieved the best quantitative performance among all evaluated methods, attaining the highest PSNR (35.81) and SSIM (0.95) for lung images from the LC2500 dataset at a 2 upscaling factor. In contrast, real-ESRGAN excelled in perceptual quality, preserving finer image details more effectively, though it recorded slightly lower numerical scores (PSNR: 33.53, SSIM: 0.92) on the same dataset. These outcomes highlight essential trade off between perceptual fidelity and reconstruction quality, indicating that the optimal choice of enhancement method may vary depending on clinical or diagnostic priorities. The MIRNetv2 method delivered reasonable performance but ranked below both real-ESRGAN and SwinIR. Specifically, it achieved PSNR/SSIM scores of 30.67/0.94 on PR-IHC patches, 32.90/0.95 on lung images, and 31.87/0.95 on colon images, while scoring 29.11 for PR-IHC images in a separate evaluation. SRCNN demonstrated a balanced performance across datasets, achieving PSNR/SSIM values of 31.45/0.88 for lung images, 30.76/0.87 for PR-IHC patches, 32.62/0.93 for colon images, and 33.76/0.91 for PR-IHC. These findings underscore the real ESRGAN as the most effective method for improving the resolution and quality of histopathology images, supporting its potential integration into digital pathology workflows to enhance diagnostic accuracy and patient outcomes.},
  archive      = {J_IETIP},
  author       = {Meriem Touhami and Zaka Ur Rehman and Md Jahid Hasan and Mohammad Faizal Ahmad Fauzi and Sarina Binti Mansor},
  doi          = {10.1049/ipr2.70166},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70166},
  shortjournal = {IET Image Process.},
  title        = {Histopathology image enhancement using multi-resolution deep learning techniques},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A band selection approach based on a mass-based metric and shared nearest-neighbours for hyperspectral images. <em>IETIP</em>, <em>19</em>(1), e70165. (<a href='https://doi.org/10.1049/ipr2.70165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Band selection in hyperspectral imaging is a burgeoning research area whose aim is to select a small number of bands in order to reduce data redundancy and noise bands. The existing ranking-based methods face two challenges: (1) The density calculation using nearest neighbours only considers distances between bands, ignoring shared neighbours. Thus, it fails to reflect the local distribution of bands. (2) The high dimensionality of the bands limits the effectiveness of the Euclidean distance-based metric in accurately capturing their similarity. To address the issues, we've proposed an innovative approach for selecting bands, grounded in a mass-based metric and shared nearest neighbours called MBSNN. Initially, we leverage a mass-based metric computation technique to supplant the conventional distance metric between disparate bands. This substitution mitigates the distortions that high-dimensional data can inflict on distance calculations. Subsequently, the natural nearest neighbour method is combined to calculate the local density of the band, reflecting its local distribution characteristics. Finally, an information entropy and peak synergy band selection technique is constructed. To substantiate the merits of our proposed approach, we executed experiments utilising support vector machines across four benchmark datasets. The results of these experiments affirm the effectiveness of our band selection approach.},
  archive      = {J_IETIP},
  author       = {Jikui Wang and Chengzhu Ji and Feifei Liu and Baocheng Yao and Qingsheng Shang and Feiping Nie},
  doi          = {10.1049/ipr2.70165},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70165},
  shortjournal = {IET Image Process.},
  title        = {A band selection approach based on a mass-based metric and shared nearest-neighbours for hyperspectral images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spatial and global correlation-aware network for multiple sclerosis lesion segmentation from multi-modal MR images. <em>IETIP</em>, <em>19</em>(1), e70164. (<a href='https://doi.org/10.1049/ipr2.70164'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple sclerosis (MS) lesion segmentation from MR imaging is a prerequisite step in clinical diagnosis and treatment of brain diseases. However, automated segmentation of MS lesions remains a challenging task, owing to the variant morphology and uncertain distribution of lesions across subjects. Despite the achieved success by existing methods, two problems still persist in automated segmentation of MS lesions, namely the lack of an effective feature enhancement approach for capturing locality context and the lack of global coherence in prediction for pixels. Hence, we propose a correlation learning network for both local and global context in this work. Specifically, we propose a sparse spatial correlation module to learn the spatial correlations within neighbours for local context. Besides, we propose a global coherence module to encode long-range dependencies for global context. The proposed method is evaluated on a public ISBI2015 datatset and a private in-house dataset collected from hospital. Experimental results show the competitive performance of our method against state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Zhanlan Chen and Xiuying Wang and Jing Huang and Jie Lu and Jiangbin Zheng},
  doi          = {10.1049/ipr2.70164},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70164},
  shortjournal = {IET Image Process.},
  title        = {A spatial and global correlation-aware network for multiple sclerosis lesion segmentation from multi-modal MR images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of deep learning-based medical image segmentation. <em>IETIP</em>, <em>19</em>(1), e70163. (<a href='https://doi.org/10.1049/ipr2.70163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation, the process of precisely delineating regions of interest (e.g. organs, lesions, cells) within medical images, is a pivotal technique in medical image analysis. It finds widespread application in computer-aided diagnosis, surgical planning, radiation therapy, and pathological analysis, thus playing a crucial role in enabling precision medicine and enhancing the quality of clinical care. Traditional medical image segmentation methods often rely on hand-crafted features and rule-based approaches, which struggle to handle the inherent complexity and variability of medical imagery, leading to limitations in segmentation accuracy and robustness. Recently, deep learning methodologies, driven by their powerful capabilities in automatic feature learning and non-linear modelling, have overcome the limitations of traditional methods and achieved significant advancements in the field of medical image segmentation. This review provides a comprehensive overview and summary of recent progress in deep learning-based medical image segmentation, with a particular focus on fully supervised learning paradigms leveraging convolutional neural networks, transformers, and the segment anything model. We delve into the underlying principles, network architectures, advantages, and limitations of these approaches. Furthermore, we systematically compare their performance across diverse imaging modalities, anatomical structures, and pathological targets. We also present a curated compilation of commonly used datasets, evaluation metrics, and loss functions relevant to medical image segmentation. Finally, we discuss future research directions and potential challenges, offering insights into the evolving landscape of this critical field.},
  archive      = {J_IETIP},
  author       = {Xinyue Zhang and Jianfeng Wang and Xiaochun Cheng and Junran Li},
  doi          = {10.1049/ipr2.70163},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70163},
  shortjournal = {IET Image Process.},
  title        = {A review of deep learning-based medical image segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive logit reconstruction in knowledge distillation. <em>IETIP</em>, <em>19</em>(1), e70162. (<a href='https://doi.org/10.1049/ipr2.70162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the logit-based knowledge distillation method, the student model learns the classification information of the teacher network by transmitting high-dimensional and abstract logits. Nevertheless, the teacher network is not an optimal learning target. On common datasets such as CIFAR100 and ImageNet, the majority of models exhibit classification accuracies of only 60% to 80%. These errors in the teacher models are a significant part of knowledge distillation that cannot be ignored. In order to facilitate the acquisition of more accurate knowledge by students, we propose the implementation of adaptive logit reconstruction knowledge distillation (ALRKD). ALRKD corrects errors by using the standard deviation, which represents the fluctuation degree of the logit distribution. Furthermore, in order to compensate for the loss of information that occurs during the correction process, an additional branch is designed to provide supplementary knowledge regarding the relationships between other classes. The results of several experiments on common datasets demonstrate the significant superiority of ALRKD.},
  archive      = {J_IETIP},
  author       = {Han Chen and Cunkang Wu and Meng Han and Xuyang Teng},
  doi          = {10.1049/ipr2.70162},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70162},
  shortjournal = {IET Image Process.},
  title        = {Adaptive logit reconstruction in knowledge distillation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSFRT: Cross-spectral feature-refined transformer for hyperspectral image classification. <em>IETIP</em>, <em>19</em>(1), e70161. (<a href='https://doi.org/10.1049/ipr2.70161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) contain rich spectral information, which enables accurate target classification. However, HSIs also contain a significant amount of redundant spectral information, and the spectra of different objects often overlap. This overlap in spectral features increases the similarity between different objects' spectra, posing a challenge for classification. Thus, suppressing redundant spectral information and refining key features are critical tasks. To address these challenges, we propose a cross-spectral feature-refined transformer (CSFRT) based on the vision transformer (ViT). In the proposed CSFRT, a two-branch gated-refined feed-forward network (TBGFN) module is introduced to suppress redundant information and enhance key spectral features by utilizing branches with and without gated mechanisms. Additionally, a cross-layer spectral feature-fusion (CLSF) module is proposed to integrate feature information and facilitate information complementarity across different encoder blocks. Extensive experiments are conducted on five different HSI datasets to verify the classification performance of the proposed CSFRT, demonstrating the effectiveness of the architecture.},
  archive      = {J_IETIP},
  author       = {Xin Wang and Sihao Huang and Gao Chen and Dejiang Wang and Jingwen Yan},
  doi          = {10.1049/ipr2.70161},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70161},
  shortjournal = {IET Image Process.},
  title        = {CSFRT: Cross-spectral feature-refined transformer for hyperspectral image classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pose-guided re-identification of amur tigers under wild environmental constraints. <em>IETIP</em>, <em>19</em>(1), e70160. (<a href='https://doi.org/10.1049/ipr2.70160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conservation of endangered species is contingent upon accurate and efficient wildlife monitoring, which is essential for informed decision-making and effective preservation strategies. With the global population of Amur tigers (Panthera tigris altaica) falling below 600, innovative conservation strategies are critically needed. Traditional monitoring methods have fallen short in accuracy and efficiency, leading to a shift towards leveraging big data and artificial intelligence for effective wildlife surveillance. Existing re-identification techniques struggle with natural habitat challenges like occlusions, changing poses, varying light, and limited data. To overcome these issues, we propose the pose-guided dual branch re-identification network (PDBRNet). Our approach integrates pose estimation to guide feature disentanglement and alignment, crucial for accurate re-identification, while an image preprocessing method considering illumination factors mitigates lighting variations' impact on accuracy. Through validation on the occluded and illumination-varying amur tiger (OIAT) dataset, PDBRNet demonstrates exceptional performance. Specifically, in single-camera scenarios, PDBRNet achieves an outstanding mean average precision (mAP) of 79.4, surpassing the performance of PGCFL (51.6) and PPGNet (69.7). Moreover, in cross-camera scenarios, PDBRNet maintains its superiority with a remarkable mAP of 54.0, along with Rank-1 and Rank-5 scores of 97.8 and 98.9, respectively, showcasing its robustness in real-world surveillance applications. The introduction of PDBRNet significantly enhances re-identification accuracy and holds promise for addressing complexities in field environments, contributing significantly to wildlife conservation efforts.},
  archive      = {J_IETIP},
  author       = {Tianyu Wang and Boxuan Ma and Xinrui Zhao and Chao Mou and Jiahua Fan},
  doi          = {10.1049/ipr2.70160},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70160},
  shortjournal = {IET Image Process.},
  title        = {Pose-guided re-identification of amur tigers under wild environmental constraints},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flow-based SR optimisation method based on dual-dimensional feature co-enhancement module. <em>IETIP</em>, <em>19</em>(1), e70159. (<a href='https://doi.org/10.1049/ipr2.70159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The super-resolution (SR) task aims to reconstruct high-resolution content from low-resolution images, and its core challenge is to solve the problem of ill-posedness while balancing the fidelity and perceptual quality of the generated images. Although flow-based SR models have made significant progress in modelling high-resolution image distributions, their generated images still have shortcomings in terms of detail representation. To address the issues above, this paper proposes an optimised method that incorporates a conditionally learnt prior (latent module). Specifically, a Dual-Dimensional Feature Co-Enhancement (DFCE) module is developed to perform joint optimisation on the channel and spatial dimensions of the features. The experimental results on the public datasets show that this framework effectively improves the generation quality of images with almost no increase in the amount of computation. Furthermore, this framework can be seamlessly integrated into fixed-scale and arbitrary-scale streaming models without the need to modify their pre-training weights or architecture, which provides efficient and flexible solutions for practical applications.},
  archive      = {J_IETIP},
  author       = {Zhengjie Wei and Xiaomin Yang},
  doi          = {10.1049/ipr2.70159},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70159},
  shortjournal = {IET Image Process.},
  title        = {Flow-based SR optimisation method based on dual-dimensional feature co-enhancement module},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reversible data hiding via bit-plane block rearrangement and intra-block compression coding for encrypted images. <em>IETIP</em>, <em>19</em>(1), e70158. (<a href='https://doi.org/10.1049/ipr2.70158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding in encrypted images (RDHEI) enables secret data embedding within encrypted images while allowing for the lossless recovery of the original image after data extraction. This technique holds significant applications in various domains such as cloud storage and data security. However, many existing RDHEI methods suffer from limited embedding capacity. To address this limitation, we present a novel and high capacity RDHEI algorithm via bit-plane block rearrangement and intra-block compression coding (hereafter BRBCC algorithm). First, the prediction error (PE) image is generated by using a median edge detection predictor, and the high-order zero-valued bit-planes are compressed. The non-zero-valued bit-planes are then separated into non-overlapping blocks that can be classified as all-zero blocks, embeddable blocks, or non-embeddable blocks. These blocks are then sorted and grouped in terms of block type for block coding. Finally, a new intra-block compression coding technique with small coded data for locating block elements is proposed to conduct effective compression and thereby reserve more space for embedding secret data. Experimental results indicate that the embedding rates of the BRBCC algorithm reach 3.9381 and 3.8436 bpp on the public datasets of BOSSbase and BOWS-2, respectively, outperforming some state-of-the-art RDHEI algorithms and exhibiting good application potential.},
  archive      = {J_IETIP},
  author       = {Shuyi Deng and Nianqiao Li and Chunqiang Yu and Xianquan Zhang and Zhenjun Tang},
  doi          = {10.1049/ipr2.70158},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70158},
  shortjournal = {IET Image Process.},
  title        = {Reversible data hiding via bit-plane block rearrangement and intra-block compression coding for encrypted images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subspace-guided feature reconstruction for unsupervised anomaly localization. <em>IETIP</em>, <em>19</em>(1), e70157. (<a href='https://doi.org/10.1049/ipr2.70157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly localization aims to identify anomalous regions that deviate from normal sample patterns. Most recent methods perform feature matching or reconstruction for the target sample with pre-trained deep neural networks. However, they still struggle to address challenging anomalies because the deep embeddings stored in the memory bank can be less powerful and informative. Specifically, prior methods often overly rely on the finite resources stored in the memory bank, which leads to low robustness to unseen targets. In this paper, we propose a novel subspace-guided feature reconstruction framework to pursue adaptive feature approximation for anomaly localization. It first learns to construct low-dimensional subspaces from the given nominal samples, and then learns to reconstruct the given deep target embedding by linearly combining the subspace basis vectors using the self-expressive model. Our core is that, despite the limited resources in the memory bank, the out-of-bank features can be alternatively “mimicked” to adaptively model the target. Moreover, we propose a sampling method that leverages the sparsity of subspaces and allows the feature reconstruction to depend only on a small resource subset, contributing to less memory overhead. Extensive experiments on three benchmark datasets demonstrate that our approach generally achieves state-of-the-art anomaly localization performance.},
  archive      = {J_IETIP},
  author       = {Katsuya Hotta and Chao Zhang and Yoshihiro Hagihara and Takuya Akashi},
  doi          = {10.1049/ipr2.70157},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70157},
  shortjournal = {IET Image Process.},
  title        = {Subspace-guided feature reconstruction for unsupervised anomaly localization},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy-YOLO model for rail anomaly detection: Robustness under limited sample and interference conditions. <em>IETIP</em>, <em>19</em>(1), e70156. (<a href='https://doi.org/10.1049/ipr2.70156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of surface anomalies in railway tracks is critical for ensuring train operation safety and enabling intelligent railway management. However, the scarcity and pronounced imbalance of anomaly samples significantly constrain model training and generalisation. Moreover, complex environmental factors such as illumination variability, sensor noise, and motion blur pose additional challenges to model robustness in real-world applications. This study presents a Fuzzy-YOLO model tailored for limited sample datasets. Built upon YOLOv11, Fuzzy-YOLO incorporates a fuzzy-non-maximum suppression (NMS) mechanism and integrates a lightweight fuzzy residual neural network (RFNN-Res) module based on fuzzy logic for anomaly classification. The final anomaly type is determined via a weighted voting strategy. Experimental evaluations demonstrate that Fuzzy-YOLO achieves a mean average precision (mAP) of 98.90%, exhibiting notably enhanced stability compared to YOLOv11 under conditions of varying illumination, noise, and motion-induced blur.},
  archive      = {J_IETIP},
  author       = {Liyuan Yang and Ming Yang and Ghazali Osman and Safawi Abdul Rahman and Muhammad Firdaus Mustapha},
  doi          = {10.1049/ipr2.70156},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70156},
  shortjournal = {IET Image Process.},
  title        = {Fuzzy-YOLO model for rail anomaly detection: Robustness under limited sample and interference conditions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech2Face3D: A two-stage transfer-learning framework for speech-driven 3D facial animation. <em>IETIP</em>, <em>19</em>(1), e70155. (<a href='https://doi.org/10.1049/ipr2.70155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-fidelity, speech-driven 3D facial animation is crucial for immersive applications and virtual avatars. Nevertheless, advancement is impeded by two principal challenges: (1) a lack of high-quality 3D data, and (2) inadequate modelling of the multi-scale characteristics of speech signals. In this paper, we present Speech2Face3D, a novel two-stage transfer-learning framework that pretrains on large-scale pseudo-3D facial data derived from 2D videos and subsequently finetunes on smaller yet high-fidelity 3D datasets. This design leverages the richness of easily accessible 2D resources while mitigating reconstruction noise through a simple temporal smoothing step. Our approach further introduces a Multi-Scale Hierarchical Audio Encoder to capture subtle phoneme transitions, mid-range prosody, and longer-range emotional cues. Extensive experiments on public 3D benchmarks demonstrate that our method achieves state-of-the-art performance on lip synchronization, expression fidelity, and temporal coherence metrics. Qualitative user evaluations validate these quantitative improvements. Speech2Face3D is a robust and scalable framework for utilizing extensive 2D data to generate precise and realistic 3D facial animations only based on speech.},
  archive      = {J_IETIP},
  author       = {Liming Pang and Zhi Zeng and Yahui Li and Guixuan Zhang and Shuwu Zhang},
  doi          = {10.1049/ipr2.70155},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70155},
  shortjournal = {IET Image Process.},
  title        = {Speech2Face3D: A two-stage transfer-learning framework for speech-driven 3D facial animation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An attention augmentation-based transformer network for unsupervised medical image registration. <em>IETIP</em>, <em>19</em>(1), e70154. (<a href='https://doi.org/10.1049/ipr2.70154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models have achieved significant success in medical image registration in recent years. Since the self-attention operation has quadratic complexity, it usually causes huge computational overhead for these methods. So, how to provide higher quality registration while being efficient in terms of parameters and computational cost is a research hotspot. For this goal, we propose A 2 TNet, an attention augmentation-based transformer network, wherein the attention augmentation is achieved via combining the spatial attention and channel attention together. Meanwhile, a shifted window mechanism is introduced to further reduce the calculation complexity of the proposed attention module. Experiments carried out on two different brain MRI datasets, LPBA and Mindboggle, demonstrate that A 2 TNet can improve registration accuracy while effectively controlling complexity compared to existing deep learning registration models.},
  archive      = {J_IETIP},
  author       = {Chuanhui Li and Hao Wang and Hangyu Bai and Xin Sun and Tao Zhang},
  doi          = {10.1049/ipr2.70154},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70154},
  shortjournal = {IET Image Process.},
  title        = {An attention augmentation-based transformer network for unsupervised medical image registration},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CR-YOLOv8-based detection method for identifying non-functional satellite components. <em>IETIP</em>, <em>19</em>(1), e70153. (<a href='https://doi.org/10.1049/ipr2.70153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting non-functional satellite components is critical for on-orbit servicing. Current detection methods struggle with complex image noise, motion blur in space environments, and the limited realism of artificially synthesised sample data. To address these challenges, we propose an enhanced you only look once version 8 (YOLOv8)-based method. In terms of network architecture, we introduce innovative designs for the backbone and neck components. A novel hybrid attention mechanism replaces the conventional approach, improving the perception and processing of intricate image features and significantly enhancing feature extraction. Additionally, we integrate modules inspired by residual networks into the neck structure, improving training adaptability and ensuring robust information transmission. This design highlights key target features while minimising feature attenuation. We also establish the satellite key element (SAKE) dataset under simulated real space conditions, including image noise and jitter blur. This dataset features components such as satellite bodies and solar panels and uses an encoder–decoder network architecture to refine context information. By merging this with a branch preserving high-resolution details, we enhance dataset expressiveness. Experiments demonstrate that the enhanced algorithm achieves a mean average precision (mAP) of 78.98% on the SAKE dataset, a 2.57% improvement over the original YOLOv8. The refined model effectively detects critical satellite components, showing superior performance in noisy and blurry scenarios.},
  archive      = {J_IETIP},
  author       = {He Bian and Derui Zhang and Cheng Li and Zhe Zhang and Wenjie Liu and Jianzhong Cao and Chao Mei and Gaopeng Zhang},
  doi          = {10.1049/ipr2.70153},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70153},
  shortjournal = {IET Image Process.},
  title        = {CR-YOLOv8-based detection method for identifying non-functional satellite components},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepFake detection: Evaluating the performance of EfficientNetV2-b2 on real vs. fake image classification. <em>IETIP</em>, <em>19</em>(1), e70152. (<a href='https://doi.org/10.1049/ipr2.70152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in digitally altered images has necessitated advanced solutions for reliable image verification, impacting sectors from media to cybersecurity. This work provides an effective method of real vs. deepfake image distinction through utilization of the EfficientNetV2-B2 model, the latest in convolutional neural networks known for its accuracy and effectiveness. The research utilized a big dataset of 100,000 images equally divided between deepfake and real classes to create a balanced sample. The methodology involved preprocessing images to a fixed size, utilizing augmentation techniques to enhance model robustness, and employing a systematic training schedule along with accuracy parameter optimization. Significantly, the research utilized an automated learning rate adjustment mechanism to optimize training performance, contributing to a complex model calibration. Outcome of the experiment design was showing 99.89% classification accuracy and an equally impressive F1 score, which is a measure of the efficiency of the model in identifying deepfakes. The results provided in-depth analysis with some misclassifications, providing recommendations for potential image processing and model training improvements. The outcome points to the suitability of applying EfficientNetV2-B2 where there is a requirement for high accuracy in image authentication.},
  archive      = {J_IETIP},
  author       = {Surbhi Bhatia Khan and Muskan Gupta and Bakkiyanathan Gopinathan and Mahesh Thyluru RamaKrishna and Mo Saraee and Arwa Mashat and Ahlam Almusharraf},
  doi          = {10.1049/ipr2.70152},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70152},
  shortjournal = {IET Image Process.},
  title        = {DeepFake detection: Evaluating the performance of EfficientNetV2-b2 on real vs. fake image classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing fetal plane classification accuracy with data augmentation using diffusion models. <em>IETIP</em>, <em>19</em>(1), e70151. (<a href='https://doi.org/10.1049/ipr2.70151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound imaging is widely used in medical diagnosis, especially for fetal health assessment. However, the availability of high-quality annotated ultrasound images is limited, which restricts the training of machine learning models. In this paper, we investigate the use of diffusion models to generate synthetic ultrasound images to improve the performance on fetal plane classification. We train different classifiers first on synthetic images and then fine-tune them with real images. Extensive experimental results demonstrate that incorporating generated images into training pipelines leads to better classification accuracy than training with real images alone. The findings suggest that generating synthetic data using diffusion models can be a valuable tool in overcoming the challenges of data scarcity in ultrasound medical imaging.},
  archive      = {J_IETIP},
  author       = {Yueying Tian and Elif Ucurum and Xudong Han and Rupert Young and Chris Chatwin and Philip Birch},
  doi          = {10.1049/ipr2.70151},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70151},
  shortjournal = {IET Image Process.},
  title        = {Enhancing fetal plane classification accuracy with data augmentation using diffusion models},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal context adapting framework for visual object tracking. <em>IETIP</em>, <em>19</em>(1), e70150. (<a href='https://doi.org/10.1049/ipr2.70150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking is widely applied in intelligent transportation systems and visual surveillance systems that serve smart cities, as well as in autonomous vehicles. Existing methods usually utilise a relation-modelling framework to model the visual object tracking problem, with auxiliary spatial context and temporal information. The spatial context is often extracted by enlarging the target template, which can introduce more background and positional information. The temporal correlation is obtained by associating the search image with previous images. However, due to noise interference, existing methods often partially exploit auxiliary data, leading to underutilisation of spatiotemporal information. To address these issues, we propose a novel and concise tracking framework, uniformly encoding all auxiliary data, including the enlarged target template, previous images, and corresponding target bounding boxes. Specifically, to mitigate the unstable factors introduced by these raw inputs, we propose a spatiotemporal context adaptive encoder, which can adaptively select appropriate information in noisy data. Extensive experiments show that the proposed method achieves state-of-the-art performance on various benchmarks, demonstrating its superiority.},
  archive      = {J_IETIP},
  author       = {Kunlong Zhao and Dawei Zhao and Xu Wang and Liang Xiao and Yulong Huang and Yiming Nie and Yonggang Zhang and Bin Dai},
  doi          = {10.1049/ipr2.70150},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70150},
  shortjournal = {IET Image Process.},
  title        = {Spatiotemporal context adapting framework for visual object tracking},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DF-3DNet: A lightweight approach based on deep learning for 3D telecommunication tower asset classification. <em>IETIP</em>, <em>19</em>(1), e70149. (<a href='https://doi.org/10.1049/ipr2.70149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition from 4G to 5G communication systems and the phase-out of 3G equipment have increased the demand for efficient telecommunication tower inspection and maintenance. Traditional manual methods are time-consuming and risky, prompting the adoption of unmanned aerial vehicles (UAVs) equipped with LiDAR sensors. This research introduces a framework for telecommunication tower asset inspection, utilising a lightweight, deep learning-based 3D classifier called DF-3DNet. The process involves raw 3D point cloud data collection using DJI's Zenmuse L1 LiDAR, optimal flight planning, data pre-processing, augmentation, and classification. The study focuses on two key asset classes—radio frequency (RF) panels and microwave (MW) dishes—which are prevalent in telecommunication towers. DF-3DNet, an enhanced version of PointNet, incorporates advanced data augmentation methods and class balance compensation to optimise performance, particularly when working with limited datasets. The model achieved classification accuracies of 0.6613 on ScanObjectNN, 0.8171 on ModelNet40, and 0.869 on the telecommunication tower dataset, demonstrating its effectiveness in handling noisy, small-scale data. By streamlining inspection workflows and leveraging AI-driven classification, this framework significantly reduces costs, time, and risks associated with traditional methods, paving the way for scalable, real-time telecommunication tower asset management.},
  archive      = {J_IETIP},
  author       = {Amzar Omairi and Zool Hilmi Ismail and Gianmarco Goycochea Casas},
  doi          = {10.1049/ipr2.70149},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70149},
  shortjournal = {IET Image Process.},
  title        = {DF-3DNet: A lightweight approach based on deep learning for 3D telecommunication tower asset classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high-speed dynamic measurement method for checked luggage dimensions. <em>IETIP</em>, <em>19</em>(1), e70148. (<a href='https://doi.org/10.1049/ipr2.70148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring compliance with stringent luggage size regulations is critical for operational efficiency and cost control in modern airports. However, conventional measurement methods often face a trade-off between speed and accuracy in the dynamic environment of check-in counters. To address these limitations, we propose a real-time luggage dimension and orientation measurement system based on a single RGB-D camera and the YOLOv8 object detection model. As luggage travels at 0.75 m/s along a conveyor, the system first detects and classifies each item, then combines two-dimensional image analysis with three-dimensional point cloud processing to compute length, width, height, and deflection angle. Trained on 7000 annotated images and validated on 100 physical samples, our method achieves average dimensional errors below 4% and angular deviations within 3°, with a mean processing time of 40 ms per item. Comparative experiments demonstrate that, under similar computational constraints, the proposed approach outperforms traditional techniques in both accuracy and robustness, thereby offering a reliable solution for enhancing real-time luggage assessment at airport check-in terminals.},
  archive      = {J_IETIP},
  author       = {Yuzhou Chen and Bin Zhang and Hongqing Song and Mingqian Du},
  doi          = {10.1049/ipr2.70148},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70148},
  shortjournal = {IET Image Process.},
  title        = {A high-speed dynamic measurement method for checked luggage dimensions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FUSION: Uncertainty-guided federated semi-supervised learning for medical image segmentation. <em>IETIP</em>, <em>19</em>(1), e70147. (<a href='https://doi.org/10.1049/ipr2.70147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) for medical image segmentation poses critical challenges, including non-IID data distributions, limited access to labelled annotations, and stringent privacy constraints across institutions. To address these, we propose FUSION (Federated Unified Semi-Supervised Optimisation Network), a novel dual-path training framework that integrates both Federated Labelled Data Learning (FLDL) and Federated Unlabelled Data Training (FUDT). Central to FUSION is a two-stage pseudo-label refinement strategy designed to ensure robustness under real-world federated constraints. First, synthetic label denoising is performed using Monte Carlo dropout-based uncertainty estimation, enabling clients to identify and exclude low-confidence predictions. Second, prototype-based correction is applied to further refine pseudo-labels by aligning them with class-specific feature centroids, mitigating errors caused by domain shifts and inter-client variability. These refined labels are used for localised training on unlabelled clients, while a dynamic aggregation scheme modulated by a reliability-based hyperparameter μ adjusts the influence of labelled versus unlabelled clients during global model updates. This tightly coupled interaction between pseudo-label quality and federated optimisation ensures stability, accelerates convergence, and enhances generalisation across heterogeneous clients. FUSION is evaluated on three diverse datasets: TCGA-LGG (brain MRI), Kvasir-SEG (colonoscopy), and UDIAT (ultrasound) and consistently outperforms state-of-the-art FL models in Dice, IoU, HD95, and ASD metrics. Results confirm the critical role of synthetic label refinement in enhancing segmentation accuracy, boundary precision, and model scalability. FUSION provides a technically grounded, privacy-preserving, and label-efficient solution for real-world multi-institutional medical image segmentation tasks.},
  archive      = {J_IETIP},
  author       = {Abdul Raheem and Zhen Yang and Haiyang Yu and Malik Abdul Manan and Fahad Sabah and Shahzad Ahmed},
  doi          = {10.1049/ipr2.70147},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70147},
  shortjournal = {IET Image Process.},
  title        = {FUSION: Uncertainty-guided federated semi-supervised learning for medical image segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coarse-to-fine detection framework for automated lung tumour detection from 3D PET/CT images. <em>IETIP</em>, <em>19</em>(1), e70146. (<a href='https://doi.org/10.1049/ipr2.70146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer remains the leading cause of cancer-related mortality worldwide. Early detection is critical to improving treatment outcomes and survival rates. Positron emission tomography/computed tomography (PET/CT) is a widely used imaging modality for identifying lung tumours. However, limitations in imaging resolution and the complexity of cancer characteristics make detecting small lesions particularly challenging. To address this issue, we propose a novel coarse-to-fine detection framework to reduce missed diagnoses of small lung lesions in PET/CT images. Our method integrates a stacked detection structure with a multi-attention guidance mechanism, effectively leveraging spatial and contextual information from small lesions to enhance lesion localisation. Experimental evaluations on a PET/CT dataset of 225 patients demonstrate the effectiveness of our method, achieving remarkable results with a precision of 81.74%, a recall of 76.64%, and an mAP of 84.72%. The proposed framework not only improves the detection accuracy of small target lesions in the lung but also provides a more reliable solution for early diagnosis.},
  archive      = {J_IETIP},
  author       = {Yunlong Zhao and Qiang Lin and Junfeng Mao and Jingjun Wei and Yongchun Cao and Zhengxing Man and Caihong Liu and Jingyan Ma and Xiaodi Huang},
  doi          = {10.1049/ipr2.70146},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70146},
  shortjournal = {IET Image Process.},
  title        = {A coarse-to-fine detection framework for automated lung tumour detection from 3D PET/CT images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCD-YOLOv10n: A small object detection algorithm for UAVs. <em>IETIP</em>, <em>19</em>(1), e70145. (<a href='https://doi.org/10.1049/ipr2.70145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks deployed on UAVs have made significant progress in data acquisition in recent years. However, traditional algorithms and deep learning models still face challenges in small and unevenly distributed object detection tasks. To address this problem, we propose the MCD-YOLOv10n model by introducing the MEMAttention module, which combines EMAttention with multiscale convolution, uses Softmax and AdaptiveAvgPool2d to adaptively compute feature weights, dynamically adjusts the region of interest, and captures cross-scale features. In addition, the C2f_MEMAttention and C2f_DSConv modules are formed by the fusion of C2f with MEMAttention and DSConv, which enhances the model's ability of extracting and adapting to irregular target features. Experiments on three datasets, VisDrone-DET2019, Exdark and DOTA-v1.5, show that the evaluation metric mAP50 achieves the best detection accuracy of 32.9%, 52.9% and 68.2% when the number of holdout parameters is at the minimum value of 2.24M. Moreover, the mAP50-95 metrics (19.5% for VisDrone-DET2019 and 45.0% for DOTA-v1.5) are 1.1 and 1.2 percentage points ahead of the second place, respectively. In terms of Recall, the VisDrone-DET2019 and DOTA-v1.5 datasets improved by 1.0% and 0.7% over the baseline model. These results validate that MCD-YOLOv10n has strong adaptability and generalization ability for small object detection in complex scenes.},
  archive      = {J_IETIP},
  author       = {Jinshuo Shi and Xitai Na and Shiji Hai and Qingbin Sun and Zhihui Feng and Xinyang Zhu},
  doi          = {10.1049/ipr2.70145},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70145},
  shortjournal = {IET Image Process.},
  title        = {MCD-YOLOv10n: A small object detection algorithm for UAVs},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CG-VTON: Controllable generation of virtual try-on images based on multimodal conditions. <em>IETIP</em>, <em>19</em>(1), e70144. (<a href='https://doi.org/10.1049/ipr2.70144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transforming fashion design sketches into realistic garments remains a challenging task due to the reliance on labor-intensive manual workflows that limit efficiency and scalability in traditional fashion pipelines. While recent advances in image generation and virtual try-on technologies have introduced partial automation, existing methods still lack controllability and struggle to maintain semantic consistency in garment pose and structure, restricting their applicability in real-world design scenarios. In this work, we present CG-VTON, a controllable virtual try-on framework designed to generate high-quality try-on images directly from clothing design sketches. The model integrates multi-modal conditional inputs, including dense human pose maps and textual garment descriptions, to guide the generation process. A novel pose constraint module is introduced to enhance garment-body alignment, while a structured diffusion-based pipeline performs progressive generation through latent denoising and global-context refinement. Extensive experiments conducted on benchmark datasets demonstrate that CG-VTON significantly outperforms existing state-of-the-art methods in terms of visual quality, pose consistency, and computational efficiency. By enabling high-fidelity and controllable try-on results from abstract sketches, CG-VTON offers a practical and robust solution for bridging the gap between conceptual design and realistic garment visualization.},
  archive      = {J_IETIP},
  author       = {Haopeng Lei and Xuan Zhao and Yaqin Liang and Yuanlong Cao},
  doi          = {10.1049/ipr2.70144},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70144},
  shortjournal = {IET Image Process.},
  title        = {CG-VTON: Controllable generation of virtual try-on images based on multimodal conditions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved image denoising: A combination method using multiscale contextual fusion and recursive learning. <em>IETIP</em>, <em>19</em>(1), e70143. (<a href='https://doi.org/10.1049/ipr2.70143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of imaging technology has led to a surge in visual content creation, necessitating advanced image denoising algorithms. Conventional methods, which frequently rely on predefined rules and filters, are inadequate for managing intricate noise patterns while maintaining image features. In order to tackle the issue of real-world image denoising, we investigate and integrate a new novel technique named recursive context fusion network (RCFNet) employing a deep convolutional neural network, demonstrating superior performance compared to current state-of-the-art approaches. RCFNet consists of a coarse feature extraction module and a reconstruction unit, where the former provides a broad contextual understanding and the latter refines the denoising output by preserving spatial and contextual details. Deep CNN learns features instead of using conventional methods, allowing us to improve and refine images. Dual attention units (DUs), in conjunction with the multi-scale resizing Block (MSRB) and selective kernel feature fusion (SKFF), are incorporated into the network to ensure efficient and reliable feature extraction. To demonstrate the advantages and challenges of combining many configurations into a single pipeline, we take a more detailed look at the results. By leveraging the complementary properties of these networks and computational models, we prefer to contribute to the creation of techniques that enhance image restoration while preserving crucial information, therefore encouraging further research and applications in image processing and artificial intelligence. The RCFNet achieves a high structural similarity index (SSIM) of 0.98 and a peak signal-to-noise ratio (PSNR) of 43.4 dB, outperforming many state-of-the-art methods on two benchmark datasets (DND and SIDD) and demonstrating its superior real-world image denoising ability.},
  archive      = {J_IETIP},
  author       = {Sonia Rehman and Muhammad Habib and Aftab Farrukh and Aarif Alutaybi},
  doi          = {10.1049/ipr2.70143},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70143},
  shortjournal = {IET Image Process.},
  title        = {Improved image denoising: A combination method using multiscale contextual fusion and recursive learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adhered buckwheat seed segmentation method based on improved UNet. <em>IETIP</em>, <em>19</em>(1), e70142. (<a href='https://doi.org/10.1049/ipr2.70142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issue of adhesion segmentation caused by the small volume, diverse morphology, large quantity, and fuzzy adherence boundaries of seeds in the output image of the buckwheat hulling machine, this paper proposes a semantic segmentation model, ResCo-UNet. The model integrates the ResNet18 network structure in the encoder of UNet, enhancing feature extraction capabilities and accelerating network training speed. To improve the recognition of adhered target boundaries, a novel parallel attention mechanism, CA 2 , is designed and incorporated into ResNet18, thereby enhancing the extraction of high-level semantic information. In the decoder stage, ConvNeXt modules are introduced to expand the receptive field, enhancing the model's ability to reconstruct complex boundary features. Results demonstrate that ResCo-UNet exhibits stronger generalization capabilities compared to other models, showing significant enhancements across multiple metrics: 87.81% mIoU, 92.71% recall, and 93.33% F 1-score. Compared to the baseline model, these metrics increased by 6.71%, 5.13%, and 4.32%, respectively. Analysis of detection results across images with different distribution densities revealed an average counting accuracy of 98.89% for adherent seeds. The model effectively segments adhered seed images with varying density distributions, providing reliable parameter feedback for adaptive control of intelligent hulling equipment.},
  archive      = {J_IETIP},
  author       = {Shaozhong Lv and Shuaiming Guan and Zhengbing Xiong},
  doi          = {10.1049/ipr2.70142},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70142},
  shortjournal = {IET Image Process.},
  title        = {Adhered buckwheat seed segmentation method based on improved UNet},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of the image segmentation methods using rough sets. <em>IETIP</em>, <em>19</em>(1), e70141. (<a href='https://doi.org/10.1049/ipr2.70141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is a major problem in image processing, and at the same time, it is a classical problem. Rough set theory is a set of theories that study the representation, learning, and induction of incomplete data imprecise knowledge, and so on. Rough set theory has good applicability in image segmentation because of its good ability to deal with vague and uncertain problems and its characteristics of fast convergence and avoiding local minima in solving optimization problems. The main content of this paper is to review the existing methods for image segmentation based on rough sets, categorize them, and describe the main ideas, advantages, disadvantages, and conditions of use of each method. Some of the methods for image segmentation based on rough sets utilize only rough sets, but most of them combine rough sets with other theories or methods. Therefore, this paper classifies existing methods for image segmentation based on rough sets according to whether they combine with other theories or methods, and what kind of theories or methods they combine with. This paper also provides an outlook on the development trends of the methods for image segmentation based on rough sets. This paper is written with the aim of making the researchers who are engaged in the methods for image segmentation based on rough sets understand the current status of the research works in this field within a short time.},
  archive      = {J_IETIP},
  author       = {Yuanyuan Tian and Hongliang Wang and Xiaolong Zhu and Haitao Guo},
  doi          = {10.1049/ipr2.70141},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70141},
  shortjournal = {IET Image Process.},
  title        = {A review of the image segmentation methods using rough sets},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swimming post recognition using novel method based on score estimation. <em>IETIP</em>, <em>19</em>(1), e70140. (<a href='https://doi.org/10.1049/ipr2.70140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swimming sports are treated as modern competitive sports, and athletes need to standardize and correct their posture. Therefore, the recognition of swimming postures is considered as an important section the coaches implement training plans. Usually, the recognition of swimming postures is achieved through coach observation; however, this approach is inefficient and lacks sufficient accuracy. To address this issue, a novel recognition method is proposed. In the proposed method, different swimming postures are assigned a different score via using a two-stage scoring mechanism. The feature regions of swimming postures can be accurately identified. Following that, the assigned score is put into the Softmax layer of the proposed convolutional neural networks. Finally, 4000 images including six swimming postures are used as an experimental set. The experimental results show that the proposed method achieves 92.73% testing accuracy and 89.03% validation accuracy in the recognition of the six swimming postures, defeating against the opponents. Meanwhile, our method outperforms some competitors in terms of training efficiency. The proposed two-stage scoring mechanism can be used for image recognition in large-scale scenarios. Moreover, the two-stage scoring mechanism is independently of specific scenarios in process of assigning a score value for feature regions of images. Not only that, the two-stage scoring mechanism can replace complex network structures, so as to reduce the work of training parameters.},
  archive      = {J_IETIP},
  author       = {Xie Lina and Xianfeng Huang and Luo Jie and Jian Zheng},
  doi          = {10.1049/ipr2.70140},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70140},
  shortjournal = {IET Image Process.},
  title        = {Swimming post recognition using novel method based on score estimation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scale-wise interaction fusion network for land cover classification of urban scene imagery. <em>IETIP</em>, <em>19</em>(1), e70139. (<a href='https://doi.org/10.1049/ipr2.70139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate land cover classification of urban aerial imagery presents significant challenges, particularly in recognising small objects and similar-appearing features (e.g., flat land, prepared land for cultivation, crop growing areas and built-up regions along with ground water resource areas). These challenges arise due to the irregular scaling of extracted features at various rates from complex urban scenes and the mismatch in feature information flow across channels, ultimately affecting the overall accuracy (OA) of the network. To address these issues, we propose the scale-wise interaction fusion network (SIFN) for land cover classification of urban scene imagery. The SIFN comprises four key modules: multi-scale feature extraction, scale-wise interaction, feature shuffle-fusion and adaptive mask generation. The multi-scale feature extraction module captures contextual information across different dilation rates of convolutional layers, effectively handling varying object sizes. The scale-wise interaction module enhances the learning of multi-scale contextual features, while the feature shuffle-fusion module facilitates cross-scale information exchange, improving feature representation. Lastly, adaptive mask generation ensures precise boundary delineation and reduces misclassification in transitional zones. The proposed network significantly improves boundary masking accuracy for small and similar objects, thereby enhancing the overall land cover classification performance.},
  archive      = {J_IETIP},
  author       = {Muhammad Shafiq and Waeal J. Obidallah and Quanrun Fan and Anas Bilal and Yousef A. Alduraywish},
  doi          = {10.1049/ipr2.70139},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70139},
  shortjournal = {IET Image Process.},
  title        = {Scale-wise interaction fusion network for land cover classification of urban scene imagery},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new weighted nuclear norm regularization model for removing salt and pepper noise with applications. <em>IETIP</em>, <em>19</em>(1), e70138. (<a href='https://doi.org/10.1049/ipr2.70138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of weighted kernel norm to image denoising has gained significant research interest in recent years by using the non-local self-similarity of images. In this paper, we propose a novel model for removing salt and pepper noise that integrates weighted kernel norm with higher-order total variation regularization. Subsequently, we use the classical method of alternating direction of multipliers and introduce some auxiliary variables to transform the original problem into saddle point problem. To illustrate the analytical results, a series of numerical simulations are conducted. Finally, experimental comparisons demonstrate the superior performance of the proposed model, which outperforms other competitive methods in terms of both signal-to-noise ratio and structural similarity index.},
  archive      = {J_IETIP},
  author       = {Lili Meng and Zhiyi Lu and Huizhong Xue and Kexin Shi and Kai Zhou},
  doi          = {10.1049/ipr2.70138},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70138},
  shortjournal = {IET Image Process.},
  title        = {A new weighted nuclear norm regularization model for removing salt and pepper noise with applications},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image compression algorithm based on region of interest extraction for unmanned aerial vehicles communication. <em>IETIP</em>, <em>19</em>(1), e70137. (<a href='https://doi.org/10.1049/ipr2.70137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) are widely used but face challenges of limited storage and bandwidth. In this research, we propose an image compression algorithm tailored for UAV communication, termed region of interest extraction for UAV communication (ROIE-UC). First, image pixels are clustered into super pixel blocks using the simple linear iterative clustering (SLIC). Second, these super pixels are grouped into regions of interest (ROI) using the density-based spatial clustering of applications with noise (DBSCAN). The image is then segmented into ROI and non-ROI areas based on these clusters. Lossless compression is applied to the ROI, while lossy compression with a high ratio is used for non-ROI regions. At the receiving end, the image is decompressed and reconstructed. Experiments show ROIE-UC gets a peak signal-to-noise ratio (PSNR) of 46.37 dB and an feature similarity index (FSIM) of 99.99% for ROI. It outperforms JPEG in PSNR (up to 28.52% improvement), FSIM (0.15% improvement), and compression ratio. When PSNR and FSIM are similar, its max compression ratio is 5.89 times that of JPEG. It also has up to 51.49% higher PSNR than other methods. ROIE-UC is an effective solution for UAV image processing and data compression.},
  archive      = {J_IETIP},
  author       = {Yanxia Liang and Tong Jia and Xin Liu and Huanhuan Zhang},
  doi          = {10.1049/ipr2.70137},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70137},
  shortjournal = {IET Image Process.},
  title        = {Image compression algorithm based on region of interest extraction for unmanned aerial vehicles communication},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage homography matrix prediction approach for trajectory generation in multi-object tracking on sports fields. <em>IETIP</em>, <em>19</em>(1), e70136. (<a href='https://doi.org/10.1049/ipr2.70136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homography estimation is a fundamental topic in computer vision, especially in scenarios that require perspective changes for intelligent analysis of sports fields, where it plays a crucial role. Existing methods predict the homography matrix either indirectly by evaluating the 4-key-point coordinate deviation in paired images with the same visual content or directly by fine-tuning the 8 degrees of freedom numerical values that define the matrix. However, these approaches often fail to effectively incorporate coordinate positional information and overlook optimal application scenarios, leading to significant accuracy bottlenecks, particularly for paired images with differing visual content. To address these issues, we propose an approach that integrates both methods in a staged manner, leveraging their respective advantages. In the first stage, positional information is embedded to enhance convolutional computations, replacing serial concatenation in traditional feature fusion with parallel concatenation, while using 4-key-point coordinate deviation to predict the macroscopic homography matrix. In the second stage, positional information is further integrated into the input images to refine the direct 8 degrees of freedom numerical predictions, improving matrix fine-tuning accuracy. Comparative experiments with state-of-the-art methods demonstrate that our approach achieves superior performance, yielding a root mean square error as low as 1.25 and an average corner errror as low as 14.1 in homography transformation of competitive sports image pairs.},
  archive      = {J_IETIP},
  author       = {Pan Zhang and Jiangtao Luo and Guoliang Xu and Xupeng Liang},
  doi          = {10.1049/ipr2.70136},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70136},
  shortjournal = {IET Image Process.},
  title        = {A two-stage homography matrix prediction approach for trajectory generation in multi-object tracking on sports fields},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An X-ray contraband detection method based on improved YOLOv8. <em>IETIP</em>, <em>19</em>(1), e70135. (<a href='https://doi.org/10.1049/ipr2.70135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray detection of contraband is crucial for public safety; however, it often faces challenges due to cluttered backgrounds and overlapping objects in security inspection images. This study proposes a novel detection framework based on You Only Look Once version 8 (YOLOv8), incorporating three key innovations: multi-scale cross-axis attention (MCA), which captures global dependencies through horizontal and vertical collaborative attention, effectively mitigating irrelevant features in complex X-ray scenarios; a lightweight bottleneck architecture using partial convolution (PConv), which significantly reduces floating point operations (FLOPs) while preserving positional sensitivity; and the focal-enhanced intersection over union (Focaler-IoU) loss function, which dynamically weights difficult samples to enhance regression accuracy. Experiments on the prohibited item detection in the X-ray dataset revealed that our model achieves a mean average precision (IoU = 0.5) (mAP@0.5) of 97.3%, outperforming YOLOv8s by 1.2 percentage points, and maintains real-time performance of 121 frames per second, surpassing YOLOv10-S (96.5%) and YOLOv12-S (96.8%). Ablation studies highlight the contribution of each module: MCA enhances mAP by 0.7%, PConv decreases FLOPs by 31%, and Focaler-IoU increases precision by 0.9% and recall by 2.4%. The proposed method exhibits substantial potential for real-time security inspections.},
  archive      = {J_IETIP},
  author       = {Jianing Chen and Juan Hao and Xiaoqun Liu},
  doi          = {10.1049/ipr2.70135},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70135},
  shortjournal = {IET Image Process.},
  title        = {An X-ray contraband detection method based on improved YOLOv8},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEM-YOLO: A small target defect detection model for photovoltaic modules. <em>IETIP</em>, <em>19</em>(1), e70134. (<a href='https://doi.org/10.1049/ipr2.70134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection is key to extending the lifetime of PV modules. However, existing methods still face significant challenges in detecting small and ambiguous targets. To this end, this paper proposes a PV module defect detection model, SEM-YOLO, based on YOLOv8. The model improves the performance through the following improvements: first, the SPD-Conv module is introduced to replace the traditional convolution in the backbone and neck sections to reduce the information loss caused by excessive down-sampling, thus enhancing the detection of small targets. Second, the neck section C2f-EMA module is introduced, in which the efficient multiscale attention module (EMA) enhances feature extraction by redistributing weights and prioritizing relevant features to improve the perception and recognition of small target defects (hot spots). Finally, we add a small target detection layer and increase the MultiSEAM detection header, so that the model can capture and detect small targets more efficiently at the output stage. The experimental results show that the mAP of the improved model reaches 93.8%, among which the mAP of small target defects reaches 83%, which is an improvement of 2.23% and 7.62% compared with YOLOv8. In addition, compared with the mainstream models (RT-DETR, YOLOv9s, YOLOv10n, and YOLOv11), the detection accuracies in terms of overall and small-target defects are significantly improved, which further validates the effectiveness of the model.},
  archive      = {J_IETIP},
  author       = {Wang Yun and Yin Wang and Gang Xie and Zhicheng Zhao},
  doi          = {10.1049/ipr2.70134},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70134},
  shortjournal = {IET Image Process.},
  title        = {SEM-YOLO: A small target defect detection model for photovoltaic modules},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on face-swapping methods for identity manipulation in deepfake applications. <em>IETIP</em>, <em>19</em>(1), e70132. (<a href='https://doi.org/10.1049/ipr2.70132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A face-swapping framework is designed to generate an image or video that merges the pose and characteristics of the input image with the identity from the source image. It has found significant applications in entertainment, privacy protection and digital content creation. However, this process is inherently complex, involving challenges like identity preservation, expression consistency and photorealism. Despite the rapid advancements in face-swapping technology, there has been a noticeable lack of in-depth analysis of the intricate mechanisms and recent developments in this field. This work attempts to bridge that gap by providing an extensive overview of face-swapping methods based on deep learning. Researchers, developers and practitioners interested in learning about the state of face-swapping technology and its possible uses may find this survey to be an invaluable resource. It will provide insights that can inform future research and innovation in this fast-evolving area.},
  archive      = {J_IETIP},
  author       = {Ramamurthy Dhanyalakshmi and Gabriel Stoian and Daniela Danciulescu and Duraisamy Jude Hemanth},
  doi          = {10.1049/ipr2.70132},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70132},
  shortjournal = {IET Image Process.},
  title        = {A survey on face-swapping methods for identity manipulation in deepfake applications},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Face forgery detection via multi-scale and multi-domain features fusion. <em>IETIP</em>, <em>19</em>(1), e70131. (<a href='https://doi.org/10.1049/ipr2.70131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake, as a popular form of visual forgery technique on the Internet, poses a serious threat to individuals' data privacy and security. In consumer electronics, fraudulent schemes leveraging Deepfake technology are widespread, making it urgent to safeguard users' data privacy and security. However, many Deepfake detection methods based on Convolutional Neural Networks (CNNs) struggle to achieve satisfactory performance on mainstream datasets, especially with heavily compressed images. Observing that tampered images leave traces in the frequency domain, which are imperceptible to the naked eye but detectable through spectrum analysis, this study proposes a novel face forgery detection framework integrating spatial and frequency domain features. The framework introduces three innovative modules: the cross-attention fusion module (CAFM), the guided attention module (GAM), and the multi-scale feature fusion module (MSFFM), Specifically, CAFM combines spatial and frequency-domain features through cross-attention to enhance feature interaction. GAM generates attention maps to refine the integration of spatial and frequency features, while MSFFM fuses multi-scale hierarchical features to capture both global and local tampering artifacts. These modules collectively improve the richness and discrimination of the extracted features, contributing to the overall detection performance. The proposed method demonstrates its effectiveness and superiority in forgery detection tasks, achieving a 3.9% average improvement in AUC compared to the state-of-the-art method GocNet [1] on FaceForensics++ (FF++) and WildDeepfake datasets. Extensive experiments further validate the effectiveness of our approach.},
  archive      = {J_IETIP},
  author       = {Rongrong Gong and Jiahao Chen and Dengyong Zhang and Arun Kumar Sangaiah and Mohammed J. F. Alenazi},
  doi          = {10.1049/ipr2.70131},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70131},
  shortjournal = {IET Image Process.},
  title        = {Face forgery detection via multi-scale and multi-domain features fusion},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-driven solution for large-scale open-pit mines excavation monitoring based on 3D point cloud. <em>IETIP</em>, <em>19</em>(1), e70130. (<a href='https://doi.org/10.1049/ipr2.70130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an adaptive point cloud workflow that withstands heavy environmental noise and the large datasets typical of open-pit mines. The workflow automatically tunes its parameters from the statistics of each input scene, eliminating manual parameter tuning. For instance, it sets the ICP correspondence distance and the clustering threshold without user input. Additionally, our method integrates a coarse-to-fine registration strategy, robust change detection, and precise volumetric estimation based on digital elevation models. Experiments on simulated mining datasets show our method remains robust under heavy noise and misalignment, with volume errors consistently below . A field pilot study at a limestone quarry further underscores its practical reliability and operational robustness. This research provides a precise, automated solution for real-time mining monitoring, effectively advancing sustainable and intelligent mining practices. Source code and datasets are publicly available at github.com/deemoe404/volcal_baseline .},
  archive      = {J_IETIP},
  author       = {Taiming He and Jiasui Zhang and Lu Yang},
  doi          = {10.1049/ipr2.70130},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70130},
  shortjournal = {IET Image Process.},
  title        = {A data-driven solution for large-scale open-pit mines excavation monitoring based on 3D point cloud},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on cell nucleus instance segmentation. <em>IETIP</em>, <em>19</em>(1), e70129. (<a href='https://doi.org/10.1049/ipr2.70129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell nucleus instance segmentation plays a pivotal role in medical research and clinical diagnosis by providing insights into cell morphology, disease diagnosis, and treatment evaluation. Despite significant efforts from researchers in this field, there remains a lack of a comprehensive and systematic review that consolidates the latest advancements and challenges in this area. In this survey, we offer a thorough overview of existing approaches to nucleus instance segmentation, exploring both traditional and deep learning-based methods. Traditional methods include watershed, thresholding, active contour model, and clustering algorithms, while deep learning methods include one-stage methods and two-stage methods. For these methods, we examine their principles, procedural steps, strengths, and limitations, offering guidance on selecting appropriate techniques for different types of data. Furthermore, we comprehensively investigate the formidable challenges encountered in the field, including ethical implications, robustness under varying imaging conditions, computational constraints, and the scarcity of annotated data. Finally, we outline promising future directions for research, such as privacy-preserving and fair AI systems, domain generalization and adaptation, efficient and lightweight model design, learning from limited annotations, as well as advancing multimodal segmentation models.},
  archive      = {J_IETIP},
  author       = {Yulin Chen and Qian Huang and Meng Geng and Zhijian Wang and Yi Han},
  doi          = {10.1049/ipr2.70129},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70129},
  shortjournal = {IET Image Process.},
  title        = {A systematic review on cell nucleus instance segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MBSM-net: A multi-branch structure model for pneumoconiosis screening and grading of chest X-ray images. <em>IETIP</em>, <em>19</em>(1), e70128. (<a href='https://doi.org/10.1049/ipr2.70128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN)-based auxiliary diagnostic systems have been widely proposed. However, CNNs have limitations in perceiving global features and more subtle features, which makes existing methods unable to achieve ideal accuracy in tasks such as pneumoconiosis screening. To overcome these limitations, we propose MBSM-Net, a new multi-branch structure-enhanced model for pneumoconiosis screening and grading based on X-ray images. MBSM-Net introduces an adaptive feature selection and fusion module to achieve synchronous extraction and hierarchical fusion of global and local features. In the local feature extraction module, we designed a CNN-Mamba module. This module integrates prior information through a detailed enhancement module to compensate for the shortcomings of traditional convolutions and significantly enhances the expression of subtle lesion information. Meanwhile, the Mamba module simulates pixel-level long-range dependencies to extract finer-grained texture features. In the global feature extraction module, we cleverly utilize the windowed multi-head self-attention (W-MSA) mechanism, enabling the model to better understand the overall distribution and degree of fibrosis of pulmonary lesions. We validated the MBSM-Net model on 1,760 real anonymized patient X-ray chest films. The results showed that the accuracy of the MBSM-Net model reached 78.6%, and the F 1 score reached 79%, both of which are superior to existing models.},
  archive      = {J_IETIP},
  author       = {Shuzhi Su and Yifan Wang and Yanmin Zhu and Yong Dai and Zekuan Yu and Zhi-Ri Tang and Bo Li and Shengzhi Wang},
  doi          = {10.1049/ipr2.70128},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70128},
  shortjournal = {IET Image Process.},
  title        = {MBSM-net: A multi-branch structure model for pneumoconiosis screening and grading of chest X-ray images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCTMIF: Hybrid CNN-transformer multi information fusion network for low light image enhancement. <em>IETIP</em>, <em>19</em>(1), e70127. (<a href='https://doi.org/10.1049/ipr2.70127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured with poor hardware and insufficient light sources suffer from visual degradation such as low visibility, strong noise, and color casts. Low-light image enhancement methods focus on solving the problem of brightness in dark areas while eliminating the degradation of low-light images. To solve the above problems, we proposed a hybrid CNN-transformer multi information fusion network (HCTMIF) for low-light image enhancement. In this paper, the proposed network architecture is divided into three stages to progressively improve the degraded features of low-light images using the divide-and-conquer principle. First, both the first stage and the second stage adopt the encoder–decoder architecture composed of transformer and CNN to improve the long-distance modeling and local feature extraction capabilities of the network. We add a visual enhancement module (VEM) to the encoding block to further strengthen the network's ability to learn global and local information. In addition, the multi-information fusion block (MIFB) is used to complement the feature maps corresponding to the same scale of the coding block and decoding block of each layer. Second, to improve the mobility of useful information across stages, we designed the self-supervised module (SSM) to readjust the weight parameters to enhance the characterization of local features. Finally, to retain the spatial details of the enhanced images more precisely, we design the detail supplement unit (DSU) to enrich the saturation of the enhanced images. After qualitative and quantitative analyses on multiple benchmark datasets, our method outperforms other methods in terms of visual effects and metric scores.},
  archive      = {J_IETIP},
  author       = {Han Wang and Hengshuai Cui and Jinjiang Li and Zhen Hua},
  doi          = {10.1049/ipr2.70127},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70127},
  shortjournal = {IET Image Process.},
  title        = {HCTMIF: Hybrid CNN-transformer multi information fusion network for low light image enhancement},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification algorithm for sitting postures using weighted random forest. <em>IETIP</em>, <em>19</em>(1), e70126. (<a href='https://doi.org/10.1049/ipr2.70126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of computers has led to a significant rise in neck and back disorders caused by poor sitting posture. While various posture analysis methods have been proposed to mitigate these issues, existing approaches are often limited by constrained data acquisition environments, low accuracy, and restricted posture classification capabilities. In this paper, we propose a method for classifying sitting postures that negatively impact health. By capturing front-facing images and detecting the coordinates and angles of the face and shoulders, our method utilises a random forest algorithm for posture classification. As a result of the experiment, the proposed approach achieved high performance with an accuracy, TPR, FPR, and F1-score of 0.983, 0.988, 0.004, and 0.983, respectively, outperforming previous studies.},
  archive      = {J_IETIP},
  author       = {Jaeeun Lee and Hongseok Choi and Jongnam Kim},
  doi          = {10.1049/ipr2.70126},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70126},
  shortjournal = {IET Image Process.},
  title        = {Classification algorithm for sitting postures using weighted random forest},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improvement of dam crack detection algorithm for YOLOv9. <em>IETIP</em>, <em>19</em>(1), e70124. (<a href='https://doi.org/10.1049/ipr2.70124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dams, as crucial water conservancy engineering facilities, play a role in safe guarding people's livelihoods and providing economic benefits. However, due to the impact of natural factors and human activities, dams may develop cracks and other potential safety hazards during operation. Crack detection can identify these potential issues in a timely manner, allowing for appropriate measures to be taken for repair and reinforcement, thereby preventing catastrophic consequences such as dam breaches under extreme weather or geological conditions. In the process of dam crack detection, this paper presents a method, YOLOv9-LAE, which may solve missed or false detections. Firstly, the large separable kernel attention (LSKA) module is introduced, which emphasises positional information while focusing on channel features. Secondly, the SPPFELAN in YOLOV9 is replaced by the AIFI module, as capturing the key information needed in the image will enable the following modules to accurately detect the crack information. Finally, the EIOU to calculate the loss, accelerating training convergence and improving the accuracy of crack detection. The research results indicate that YOLOV9-LAE achieves a precision of 90.7%, the recall rate is 75.1%, with at 81.5% and at 60.6%. Compared to YOLOv9, the precision has improved by 9.9%, the recall has increased by 2%, has risen by 1.5% and has been enhanced by 1.5%.},
  archive      = {J_IETIP},
  author       = {Huixia Zhang and Xuhui Jiang and Yitong Liu and JinHua Qian and Lixue Ni},
  doi          = {10.1049/ipr2.70124},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70124},
  shortjournal = {IET Image Process.},
  title        = {Improvement of dam crack detection algorithm for YOLOv9},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing natural adversarial examples through activation map analysis. <em>IETIP</em>, <em>19</em>(1), e70123. (<a href='https://doi.org/10.1049/ipr2.70123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples are an intriguing and critical topic in the field of machine learning. The impact of malignant perturbations on deep learning-based systems, especially in safety-critical applications, highlights a significant security concern. While most research has focused on artificially generated adversarial attacks–crafted through optimization algorithms and constrained perturbations, it is important to note that adversarial examples can also occur naturally, without any artificial manipulation, during the prediction of real-world images. These naturally occurring adversarial examples pose unique challenges, as they are harder to detect and interpret. Despite their importance, the study of natural adversarial examples remains in its early stages. Fundamental questions remain unanswered: Do natural adversarial examples exhibit similar behaviours or properties as artificially generated ones? How should models be adapted to improve their robustness against such natural inputs? To address these questions, this work proposes an in-depth analysis of activation maps to compare the internal behaviour of neural networks when processing clean images, artificially perturbed inputs and natural adversarial examples. A set of quantitative metrics is extracted from activation heatmaps at various network layers, including mean activation intensity, centroid displacement and standard reference image quality metrics. These measurements enable a systematic comparison of how the network attends to different image regions under varying conditions. The experimental results demonstrate that natural adversarial examples exhibit statistically significant differences in activation patterns compared to their artificial counterparts, suggesting that they may require distinct strategies for detection and defence.},
  archive      = {J_IETIP},
  author       = {Anibal Pedraza and Nerea Leon and Harbinder Singh and Oscar Deniz and Gloria Bueno},
  doi          = {10.1049/ipr2.70123},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70123},
  shortjournal = {IET Image Process.},
  title        = {Characterizing natural adversarial examples through activation map analysis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A DNeRF image denoising method based on MSAF-DT. <em>IETIP</em>, <em>19</em>(1), e70122. (<a href='https://doi.org/10.1049/ipr2.70122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering novel and realistic images is crucial in applications such as augmented reality, virtual reality, 3D content creation, gaming, and the film industry. However, dynamic image rendering often suffers from significant noise, which compromises clarity and realism. Dynamic-Neural Radiance Fields (D-NeRF), an extension of the original NeRF model, addresses this challenge by enabling the rendering of dynamic images. Despite its advantages, D-NeRF often generates significant noise in the rendered images. Addressing this limitation, this paper proposes a Transformer-based model, Multi-Scale Attention Fusion Denoise Transformer (MSAF-DT), designed to enhance the clarity of rendered images. MSAF-DT constructs a deep neural network by stacking multiple Transformer blocks, with each block adaptively extracting complex features and dependencies from the data. The multi-head self-attention (MHSA) mechanism effectively captures long-range dependencies, which is crucial for processing sequences in dynamic radiance fields. Additionally, the model supports parallel processing of the entire sequence, significantly enhancing training efficiency. This design enables MSAF-DT to handle the noise present in D-NeRF outputs while preserving essential features. Experimental results on the Nerf_Synthetic dataset demonstrate that the proposed method outperforms D-NeRF in both image clarity and processing efficiency, achieving higher PSNR scores and faster convergence during training.},
  archive      = {J_IETIP},
  author       = {Wenxuan Xu and Meng Huang and Qian Xu},
  doi          = {10.1049/ipr2.70122},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70122},
  shortjournal = {IET Image Process.},
  title        = {A DNeRF image denoising method based on MSAF-DT},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced small-object detection in UAV images using modified YOLOv5 model. <em>IETIP</em>, <em>19</em>(1), e70121. (<a href='https://doi.org/10.1049/ipr2.70121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a modified YOLOv5 algorithm specifically designed to enhance small-object detection in unmanned aerial vehicle (UAV) images. Traditional object detection in UAV images is particularly challenging due to the high altitude of the cameras, which results in small object sizes and varying viewing angles. To address these challenges, the algorithm incorporates an additional prediction head to detect objects across a wide range of scales, a channel feature fusion with involution (CFFI) block to minimize information loss, a convolutional block attention module (CBAM) to highlight the crucial spatial and channel features, and a C3 structure with a Transformer block (C3TR) to capture contextual information. The algorithm additionally employs soft non-maximum suppression to enhance the bounding box scoring of overlapping objects in dense scenes. Extensive experiments were conducted on the VisDrone-DET2019 dataset, which demonstrated the effectiveness of the proposed algorithm. The results showed improvements with precision scores of 55.0%, recall scores of 44.6%, mean average precision scores of mAP50 = 50.9% and mAP50:95 = 33.0% on the VisDrone-DET2019 validation set, and precision of 50.8%, recall of 37.3%, mAP50 = 44.2%, and mAP50:95 = 27.3% on the VisDrone-DET2019 testing set. The improved performance is due to the incorporation of attention mechanisms, which allow the proposed model to stay lightweight while still extracting the features needed to detect small objects.},
  archive      = {J_IETIP},
  author       = {Bach-Thanh Lieu and Chi-Khang Nguyen and Huynh-Lam Nguyen and Thanh-Hai Le},
  doi          = {10.1049/ipr2.70121},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70121},
  shortjournal = {IET Image Process.},
  title        = {Enhanced small-object detection in UAV images using modified YOLOv5 model},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AEC-CapsNet: Enhanced capsule networks with attention and expansion-contraction mechanisms for advanced feature extraction in medical imaging. <em>IETIP</em>, <em>19</em>(1), e70120. (<a href='https://doi.org/10.1049/ipr2.70120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of medical image analysis faces challenges due to the complexity of medical data. Convolutional neural networks (CNNs), while popular, often miss critical hierarchical and spatial structures essential for accurate diagnosis. Capsule networks (CapsNets) address some of these issues but struggle with extracting information from complex datasets. We propose the Expanded Attention and Contraction Enhanced Capsule Network with Attention (AEC-CapsNet), designed specifically for medical imaging tasks. AEC-CapsNet leverages attention mechanisms for improved feature representation and expansion-contraction modules for efficient management of global and local features, enabling superior feature extraction. The model is tested on six medical datasets: Jun Cheng Brain MRI (98.14% accuracy, 99.33% AUC), Breast_BreaKHis (98.50% accuracy, 98.96% AUC), HAM10000 (98.43% accuracy, 1.00% AUC), heel X-ray (97.47% accuracy, 99.30% AUC), LC250000 colon cancer histopathology (99.80% accuracy, 99.50%AUC) and LC250000 lung cancer histopathology (99.59% accuracy, 99.20%AUC). Additionally, on the general CIFAR-10 dataset, it achieves 83.48% accuracy, demonstrating robustness and generalisability. To ensure a comprehensive complete assessment, we applied cross-validation for each experiment, which allowed us to evaluate the model's stability and performance across different training datasets. The model was trained for multiple epochs (20, 40, 60, 80, 100, 120 and 140 epochs) to examine its learning and convergence patterns. Without dataset-specific augmentation or architectural modifications, AEC-CapsNet corrects critical weaknesses of existing methods, making it efficient, accurate, and reliable for automated medical image diagnostics.},
  archive      = {J_IETIP},
  author       = {Yasir Adil Mukhlif and Nehad T. A. Ramaha and Alaa Ali Hameed},
  doi          = {10.1049/ipr2.70120},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70120},
  shortjournal = {IET Image Process.},
  title        = {AEC-CapsNet: Enhanced capsule networks with attention and expansion-contraction mechanisms for advanced feature extraction in medical imaging},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight channel correlation invertible network for image denoising. <em>IETIP</em>, <em>19</em>(1), e70119. (<a href='https://doi.org/10.1049/ipr2.70119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning has made significant progress in image denoising. However, the complexity of advanced methods' systems is also increasing, which will increase the calculation cost and hinder the convenient analysis and comparison of methods. Therefore, a lightweight model based on invertible networks is proposed. The invertible network has great advantages in image denoising. It is lightweight, memory-saving, and information-lossless in backpropagation. To effectively remove the noise and restore a clean image, the high-frequency part of the image is resampled and modeled to remove the impact of noise better. The channel context block is proposed to better focus on useful channels and improve the network's perception of useful information in images while ensuring the complexity and computing cost. At the same time, the residual structure with channel correlation modeling is used to extract the features in the convolutional flow, to effectively retain the details and texture of the image, and learn more details of the spatial features of the image, so as to prevent the blur and distortion of the image in the denoising process. The proposed method allows the model to enjoy lower computational complexity on the premise of ensuring performance.},
  archive      = {J_IETIP},
  author       = {Fuxian Sui and Hua Wang and Fan Zhang},
  doi          = {10.1049/ipr2.70119},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70119},
  shortjournal = {IET Image Process.},
  title        = {A lightweight channel correlation invertible network for image denoising},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on facial expression recognition method based on improved ConvNeXt. <em>IETIP</em>, <em>19</em>(1), e70118. (<a href='https://doi.org/10.1049/ipr2.70118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced facial expression recognition technology can significantly enhance human-computer interaction and improve intelligent services for humans. This paper introduces a novel facial expression recognition method utilizing an enhanced ConvNeXt network. By integrating the SENET attention mechanism into the ConvNeXt block, key feature information extraction is effectively enhanced. Additionally, the incorporation of the focal loss (FL) function optimizes the classification performance of the network model. Experimental results show that the improved ConvNeXt network achieves higher accuracy compared to other deep learning models, with accuracy rates of 83.8% and 70.4% on the RAF-DB and FER2013 datasets, respectively.},
  archive      = {J_IETIP},
  author       = {Dan Chen and Yu Cao and Xu Cheng},
  doi          = {10.1049/ipr2.70118},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70118},
  shortjournal = {IET Image Process.},
  title        = {Research on facial expression recognition method based on improved ConvNeXt},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical image registration via spatial feature extraction mamba and substrate iterative refinement. <em>IETIP</em>, <em>19</em>(1), e70117. (<a href='https://doi.org/10.1049/ipr2.70117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major challenges in medical image registration is balancing computational efficiency with the ability to capture large deformations in complex anatomical structures. Existing methods often struggle with high computational costs due to the need for extensive feature extraction and attention computations at various levels of the network. Moreover, some methods do not take into account the spatial relationships of the feature images during registration, and the loss of these spatial relationships leads to suboptimal results for these methods. To this end, we introduce a novel medical image registration network, PSMamba-Net, which leverages optimized iteration and the Mamba framework within a dual-stream pyramid architecture. The network reduces the computational burden by narrowing attention computations at each decoding level, while an optimized iterative registration module at the bottom of the pyramid captures large deformations. This approach eliminates the need for repeated feature extraction, significantly accelerating the registration process. Additionally, the SMB module is incorporated as a decoder to enhance spatial relationship modelling and leverage Mamba's strengths in long-sequence processing. PSMamba-Net balances efficiency and accuracy, surpassing state-of-the-art methods across LPBA40, Mindboggle, and Abdomen CT datasets. Our source code is available at: https://github.com/VCMHE/PSMamba .},
  archive      = {J_IETIP},
  author       = {Zilong Xue and Kangjian He and Dan Xu and Jian Gong},
  doi          = {10.1049/ipr2.70117},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70117},
  shortjournal = {IET Image Process.},
  title        = {Medical image registration via spatial feature extraction mamba and substrate iterative refinement},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unpaired fundus image enhancement using image decomposition. <em>IETIP</em>, <em>19</em>(1), e70116. (<a href='https://doi.org/10.1049/ipr2.70116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-quality fundus images pose significant challenges for both ophthalmologists and computer-aided diagnosis systems. While many existing deep learning-based image quality enhancement algorithms require low- and high-quality image pairs for training, such pairs are often difficult to obtain in practice. On the other hand, unpaired image enhancement algorithms tend to struggle in preserving small structures and suppressing artefacts, which are crucial for medical applications. To address these issues, we propose an unpaired structure-preserving cycle quality alternating network for low-quality fundus image enhancement. Our method consists of three main components: (1) a cycle quality alternating framework to provide pixel-wise supervision for unpaired image enhancement, (2) a quality-aware disentangle module to enhance the extrinsic representation of the low-quality image with the high-quality reference image, and (3) an instance normalized skip to improve the network's structure-preserving capability. We tested our method on both synthetic and authentic clinical images with pathological structures and found it to be superior to state-of-the-art algorithms in terms of improving image quality while preserving delicate structures. Additionally, the proposed network demonstrated strong generalization ability in improving the quality of unseen images, as tested on 135-degree neonatal fundus images.},
  archive      = {J_IETIP},
  author       = {Kun Chen and Yu Ye and Huazhu Fu and Yuhao Luo and Ronald X. Xu and Mingzhai Sun},
  doi          = {10.1049/ipr2.70116},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70116},
  shortjournal = {IET Image Process.},
  title        = {Unpaired fundus image enhancement using image decomposition},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A modified hierarchical vision transformer model for poultry disease detection. <em>IETIP</em>, <em>19</em>(1), e70115. (<a href='https://doi.org/10.1049/ipr2.70115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poultry production faces challenges from diseases like newcastle, salmonella, and coccidiosis, which are critical to global food security, resulting in economic losses and public health concerns. Current detection technologies, such as human inspections and PCR-based procedures, are time-consuming and costly, limiting scalability. Convolutional neural networks (CNNs) like ResNet50 and VGG16 have shown promise for automating disease identification, but they struggle with generalization and collecting fine-grained local and global information. In this study, we propose a deep learning solution based on a hierarchical vision transformer (HViT) model to detect poultry diseases from fecal images. We compare the performance of our HViT model with traditional CNNs (ResNet50, VGG16), lightweight architectures (MobileNetV3_Large_100, XceptionNet), and standard vision transformers (ViT) (ViT-B/16). The experimental results demonstrate that our HViT model outperforms other models, achieving an average validation accuracy of 90.90% with a validation loss of 0.2647. The HViT's ability to balance local and global feature recognition highlights its potential as a scalable solution for real-time poultry disease detection. These findings underscore the significance of hierarchical attention in addressing complex image analysis tasks, with implications for broader applications in agriculture and medical imaging.},
  archive      = {J_IETIP},
  author       = {Michael Agbo Tettey Soli and Dacosta Agyei and Waliyyullah Umar Bandawu and Leonard Mensah Boante and Justice Kwame Appati},
  doi          = {10.1049/ipr2.70115},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70115},
  shortjournal = {IET Image Process.},
  title        = {A modified hierarchical vision transformer model for poultry disease detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vorticity transport equation-based shadow removal approach for image inpainting. <em>IETIP</em>, <em>19</em>(1), e70114. (<a href='https://doi.org/10.1049/ipr2.70114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadows are common in many types of images, causing information loss or disturbance. Shadow removal can help improve the quality of the digital image. If there is no effective information available to restore the original image in the shaded area, the interpolation-based inpainting technique can be used to remove the shadow from the digital image. This image inpainting technique typically involves establishing and solving partial differential equations (PDEs), an iterative solving process that is very time-consuming. To solve the time-consuming problem, a method that introduces the fast marching method (FMM) into the vorticity transport equation (VTE) is demonstrated. VTE is a type of partial differential equation describing two-dimensional fluids. FMM is a numerical scheme for tracking the evolution of monotonically advancing interfaces via finite difference solution of the eikonal equation. The proposed method contains three main steps: (a) by investigating the relationship between VTE and the traditional PDE-based image inpainting method, a new image inpainting model using VTE is developed;(b) the area to be inpainted is divided into boundaries that shrink in layers from the outside inwards using FMM; and (c) the VTE image inpainting model is converted into a weighted average form to coordinate with FMM. The visual and quantitative evaluation of the experimental results of shadow removal shows that the proposed method outperforms PDE-based and state-of-the-art methods in terms of shadow-removal effect and running time. The results also show that our method excels at inpainting images with near-smooth textures and simple geometric structures and where the pixels to be inpainted are continuous with neighbouring pixels.},
  archive      = {J_IETIP},
  author       = {Xiaoying Ti and Li Yu and Quanhua Zhao},
  doi          = {10.1049/ipr2.70114},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70114},
  shortjournal = {IET Image Process.},
  title        = {Vorticity transport equation-based shadow removal approach for image inpainting},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAIP-VAE: Balancing reconstruction and disentanglement in VAE with group and individual priors. <em>IETIP</em>, <em>19</em>(1), e70113. (<a href='https://doi.org/10.1049/ipr2.70113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disentangled representation learning demonstrates great success in enhancing the explainability, robustness and generalization capability of models across computer vision domains. While adopting the variational auto-encoders (VAEs) to learn disentangled representations holds great promise, these models are prone to suffer from the poor disentanglement capability in complicated datasets, for example, colourful portrait images. These datasets often contain strong correlation among attributes, making it difficult to disentangle them. To alleviate this issue, a novel approach named group and individual priors-based VAE (GAIP-VAE) is proposed, which constrains the semantic attributes by customizing prior information to improve the disentanglement capability of the VAE. Specifically, we start from modelling the joint distribution of the observed data, and then derive three compatible loss terms in the objective function. The first one is the reconstruction term, utilizing the Laplace distribution to improve the image quality. The second one is the individual prior regularizer, encouraging the model to learn more interpretable factors via dimensional-level regularizer. The third one is the group prior regularizer, constraining the approximate posterior distribution through multivariate normal distribution with the tailored correlation. Both quantitative and qualitative experimental results demonstrate that GAIP-VAE can achieve a great balance between image quality and disentanglement capability.},
  archive      = {J_IETIP},
  author       = {Yi Tian and Zengjie Song},
  doi          = {10.1049/ipr2.70113},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70113},
  shortjournal = {IET Image Process.},
  title        = {GAIP-VAE: Balancing reconstruction and disentanglement in VAE with group and individual priors},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ACDR-CRAFF net: A multi-scale network based on adaptive channel and coordinate relational attention network for remote sensing scene classification. <em>IETIP</em>, <em>19</em>(1), e70112. (<a href='https://doi.org/10.1049/ipr2.70112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of remote sensing scene images is crucial for diverse applications, from environmental monitoring to urban planning. While convolutional neural networks (CNNs) have dramatically improved classification accuracy, challenges remain due to the complex distribution of small objects, varied spatial configurations, and intra-class multimodality in remote sensing images. In this work, we make three key contributions to address these challenges. (1) We propose the adaptive channel and coordinate relational attention network (ACDR-CRAFF), a novel multi-scale feature fusion framework designed to enhance feature representation across scales. (2) We introduce two innovative modules: the adaptive channel dimensionality reduction (ACDR) module, which dynamically adjusts channel representations to retain essential low-dimensional features, and the coordinate relational attention multi-scale feature fusion (CRAFF) module, which effectively captures and transfers spatial information between feature levels. (3) By integrating ACDR and CRAFF, our model achieves a progressive fusion of local to global features, ensuring robust feature expressiveness at multiple scales. Experimental results on four widely used benchmark datasets demonstrate that ACDR-CRAFF consistently outperforms several state-of-the-art methods, achieving significant improvements in classification accuracy and setting a new benchmark for complex remote sensing scene classification tasks. These results underscore the effectiveness of our approach in addressing the limitations of existing methods and advancing the state of the art in remote sensing image analysis.},
  archive      = {J_IETIP},
  author       = {Wei Dai and Haixia Xu and Furong Shi and Liming Yuan and Xinyu Wang and Xianbin Wen},
  doi          = {10.1049/ipr2.70112},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70112},
  shortjournal = {IET Image Process.},
  title        = {ACDR-CRAFF net: A multi-scale network based on adaptive channel and coordinate relational attention network for remote sensing scene classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AW-YOLO: A multi-object detection network for autonomous driving under all weather conditions. <em>IETIP</em>, <em>19</em>(1), e70111. (<a href='https://doi.org/10.1049/ipr2.70111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years, object detection technology based on deep learning has attracted extensive research in autonomous driving. Achieving a robust object detection network under all weather conditions (e.g., sunny, fog, nighttime, rain and snow) is highly significant for autonomous driving systems, which ensure safety by recognising pedestrians, vehicles, traffic lights, etc. This paper proposes a robust multi-object detection network named All Weather-You Only Look Once (AW-YOLO) based on YOLOv8, with a trade-off between precision and lightweightness. Considering the blurring or absence of the salient object features of the image under all weather conditions, we propose a developed dilation-wise residual (D-DWR) module. Specifically, it combines the dilatation-wise residual module with the dilated re-param block using a large kernel convolution to see wide without going deep, greatly improving the feature extraction ability. Moreover, we introduce an efficient dynamic upsampler (DySample) that formulates upsampling from the viewpoint of point sampling and avoids dynamic convolution, which can improve the network's ability to feature fusion. Lightweight is an essential requirement for autonomous driving. To this end, we adopt a multi-scale shared detection head (MSSD-Head) to achieve lightweight deployment in autonomous vehicles. Experimental results show that the mAP50-95 values of AW-YOLO on the KITTI and Adverse Conditions Dataset with Correspondences (ACDC) datasets exceed the baseline model YOLOv8 by 1.7% and 1.5%, respectively. Meanwhile, the parameters and model size of AW-YOLO have decreased by 21.4% and 20.4%, respectively.},
  archive      = {J_IETIP},
  author       = {Dingping Chen and Lu Dai and Xiangdi Yue and Qian Gu and Siming Huang and Jiaji Pan and Yihuan Zhang and Miaolei He},
  doi          = {10.1049/ipr2.70111},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70111},
  shortjournal = {IET Image Process.},
  title        = {AW-YOLO: A multi-object detection network for autonomous driving under all weather conditions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMFE-YOLO: A small object detection model for drone images. <em>IETIP</em>, <em>19</em>(1), e70110. (<a href='https://doi.org/10.1049/ipr2.70110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drones, due to their high efficiency and flexibility, have been widely applied. However, small objects captured by drones are easily affected by various conditions, resulting in suboptimal surveying performance. While the YOLO series has achieved significant success in detecting large targets, it still faces challenges in small target detection. To address this, we propose an innovative model, AMFE-YOLO, aimed at overcoming the bottlenecks in small target detection. Firstly, we introduce the AMFE module to focus on occluded targets, thereby improving detection capabilities in complex environments. Secondly, we design the SFSM module to merge shallow spatial information from the input features with deep semantic information obtained from the neck, enhancing the representation ability of small target features and reducing noise. Additionally, we implement a novel detection strategy that introduces an auxiliary detection head to identify very small targets. Finally, we reconfigured the detection head, effectively addressing the issue of false positives in small-object detection and improving the precision of small object detection. AMFE-YOLO outperforms methods like YOLOv10 and YOLOv11 in terms of mAP on the VisDrone2019 public dataset. Compared to the original YOLOv8s, the average precision improved by 5.5%, while the model parameter size was reduced by 0.7 M.},
  archive      = {J_IETIP},
  author       = {Qi Wang and Chengxin Yu},
  doi          = {10.1049/ipr2.70110},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70110},
  shortjournal = {IET Image Process.},
  title        = {AMFE-YOLO: A small object detection model for drone images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CDRWF: Compressed domain based robust watermarking framework for colored images. <em>IETIP</em>, <em>19</em>(1), e70109. (<a href='https://doi.org/10.1049/ipr2.70109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the volume of digital data increases, there is an increasing need for effective compression methods to address storage demands. Concurrently, the importance of robust image watermarking for authentication and ownership verification cannot be overstated. This work tackles the dual challenge of optimizing image compression for storage conservation and implementing strong image watermarking for copyright protection. The suggested approach integrates the K-means clustering compression algorithm to enhance storage efficiency along with a resilient image watermarking technique based on spatial-domain embedding. We introduce a blind robust watermarking approach that uses zero-frequency coefficient alteration independently in the spatial domain instead of using the discrete cosine transformation (DCT) to verify the ownership of colored images. To enhance the robustness of the system, we have incorporated two watermarks into the cover image. This precaution ensures that even if one watermark undergoes deterioration due to attacks, authentication can still be assured by recovering the other watermark. Compared to frequency-domain approaches, our scheme yields better robustness and reduced computing complexity. The average peak signal-to-noise ratio (PSNR) for the test images using our approach is above 39 dB with a compression ratio equal to 5.9978, removing up to 83% of the redundancy of the host image. After comparing our approach with several state-of-the-art methods, its robustness is exposed by the values of normalized correlation coefficient (NCC) close to one and bit error rate (BER) values close to zero. Besides, the scheme is able to embed a total of 8192 watermark bits in the host image of size 512 × 512 × 3. Experimental results affirm the effectiveness of the proposed methodology, marking it as a valuable contribution to the domains of image processing and information security.},
  archive      = {J_IETIP},
  author       = {Samrah Mehraj and Subreena Mushtaq and Shabir A. Parah},
  doi          = {10.1049/ipr2.70109},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70109},
  shortjournal = {IET Image Process.},
  title        = {CDRWF: Compressed domain based robust watermarking framework for colored images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic classification and recognition of qinghai embroidery images based on the SE-ResNet152V2 model. <em>IETIP</em>, <em>19</em>(1), e70108. (<a href='https://doi.org/10.1049/ipr2.70108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential part of traditional Chinese handicrafts, Qinghai embroidery embodies rich cultural connotations and unique artistic value. However, with the development of modern society, traditional handicrafts face severe challenges in inheritance and protection. To effectively address these challenges and promote Qinghai embroidery's digital safety and inheritance, this study realizes the automatic classification and identification of Qinghai embroidery images based on the SE-ResNet152V2 model. First, we constructed an image dataset containing five kinds of Qinghai embroidery patterns, including Tu nationality Pan embroidery, Huangzhong Dui embroidery, Hehuang embroidery, Mongolian embroidery, and Tibetan Guinan embroidery. The regions that contribute the most when the model judges the image categories are revealed by the GRAD-CAM technique, and the data are preprocessed by the image enhancement technique to enhance the data diversity and improve the model's generalization ability. For the complexity and detailed features of Qinghai embroidery patterns, this paper introduces the squeeze-and-excitation (SE) attention module to enhance the model's ability to capture key features. By systematically comparing the effects of multiple optimizers and attention mechanisms combination models, the optimal combination of the Nadam optimizer and SE attention mechanism is finally selected. The experimental results show that the accuracy of the optimized SE-ResNet152V2 model on the self-built Qinghai embroidery image dataset is 91.73%, which is 11.43% higher than that of the original ResNet152V2 model. Further experiments show that the SE-ResNet152V2 model is better than other popular neural network models such as MobileNetV1, EfficientNetB2, vision transformer (VIT), and swin transformer, regarding classification accuracy. It has proven its effectiveness and superiority in processing Qinghai embroidery pattern recognition tasks and provided strong technical support for digital protection and inheritance of traditional crafts.},
  archive      = {J_IETIP},
  author       = {Yajuan Zhao and Zhe Fan and Hehua Yao and Tong Zhang and Bingfeng Seng},
  doi          = {10.1049/ipr2.70108},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70108},
  shortjournal = {IET Image Process.},
  title        = {Automatic classification and recognition of qinghai embroidery images based on the SE-ResNet152V2 model},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative data distillation and augmentation for enhancing deep learning performance with limited image training data. <em>IETIP</em>, <em>19</em>(1), e70107. (<a href='https://doi.org/10.1049/ipr2.70107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models require large training datasets. Incorporating additional data into small training datasets can enhance the model's performance. However, acquiring additional data may sometimes be challenging or beyond one's control. In such situations, data augmentation becomes essential to overcome the limited supply of labeled data by generating new data that preserves the essential properties of the original dataset. The primary objective of our research is to develop an iterative data distillation and augmentation (IDDA) method that enlarges the size of a limited image training dataset while preserving its properties. At every iteration, our method distills a set of images from the training set of the previous iteration utilizing the kernel inducing point (KIP) method, and the union of the training and distilled sets creates the new training set. However, our experiments show that IDDA is computationally expensive, increasing processing time by approximately 17%–27 for MNIST and Fashion-MNIST, 31%–39 for CIFAR-10, and up to 48%–49 for CIFAR-100 compared to state-of-the-art augmentation methods, due to the additional step of applying KIP for image distillation. We have experimentally determined that for a few iterations the classification accuracy increases and then drops afterward. We validate the IDDA capabilities by comparing it with conventional augmenting methods and MixUp on the following publicly available image datasets: MNIST digit, Fashion-MNIST, CIFAR-10, and CIFAR-100. Our approach proves highly effective for very limited datasets, addressing the challenge of database expansion for improved performance of deep learning models.},
  archive      = {J_IETIP},
  author       = {Avinash Singh and Namasivayam Ambalavanan and Nikolay M. Sirakov and Arie Nakhmani},
  doi          = {10.1049/ipr2.70107},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70107},
  shortjournal = {IET Image Process.},
  title        = {Iterative data distillation and augmentation for enhancing deep learning performance with limited image training data},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MD-YOLOv8: A multi-object detection algorithm for remote sensing satellite images. <em>IETIP</em>, <em>19</em>(1), e70106. (<a href='https://doi.org/10.1049/ipr2.70106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technology for target recognition in remote sensing satellite images is widely applied in daily life, and research on detecting and recognizing targets in remote sensing images holds significant academic and practical importance. To address the challenges of extreme scale variations, dense target distributions, and low-resolution artefacts in remote sensing images, this paper proposes a new multi-object detection network based on the YOLOv8 architecture—MD-YOLOv8. The main contributions of this paper are threefold: (1) the design of the multi-frequency attention downsampling module, which integrates the ADown module with Haar wavelet transforms and pixel attention; (2) the proposal of the adaptive attention network (DMAA) module, an enhanced multiscale feature extractor based on the multiscale feature extraction attention mechanism; (3) the integration of both modules into the YOLOv8 backbone to achieve superior performance in remote sensing image detection. Based on the DOTA-1.0 dataset for training, experimental results show that the MD-YOLOv8 network achieves improvements in precision, recall rate, and mAP@0.5, reaching 82.69%, 78.28%, and 82.05%, respectively; these represent increases of 3.76%, 3.43%, and 4.37% compared to the original model. In practical image detection, MD-YOLOv8 demonstrates higher recognition quality and can flexibly respond to various target types. The MD-YOLOv8 network effectively meets the accuracy requirements for target detection in remote sensing satellite images.},
  archive      = {J_IETIP},
  author       = {Pengfei Zhang and Jian Liu and Jianqiang Zhang and Yiping Liu and Xingda Li},
  doi          = {10.1049/ipr2.70106},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70106},
  shortjournal = {IET Image Process.},
  title        = {MD-YOLOv8: A multi-object detection algorithm for remote sensing satellite images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale feature guided transformer for image inpainting. <em>IETIP</em>, <em>19</em>(1), e70105. (<a href='https://doi.org/10.1049/ipr2.70105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, image restoration has witnessed remarkable advancements. However, reconstructing visually plausible textures while preserving global structural coherence remains a persistent challenge. Existing convolutional neural network (CNN)-based approaches are inherently limited by their local receptive fields, often struggling to capture global structure. Previously proposed methods mostly focus on structural priors to address the limitation of CNN's receptive field, but we believe that texture priors are also critical factors that influence the quality of image inpainting. To tackle semantic inconsistency and texture blurriness in current methods, we introduce a novel multi-stage restoration framework. Specifically, our architecture incorporates a dual-stream U-Net with attention mechanisms to extract multi-scale features. The mixed attention-gated feature fusion module exchanges and combines structure and texture features to generate multi-scale fused feature maps, which are progressively merged into the decoder to guide the Transformer to generate more realistic images. Additionally, we propose a feature selection feedforward network to replace traditional MLPs in Transformer blocks for adaptive feature refinement. Extensive experiments on CelebA-HQ and Paris StreetView datasets demonstrate superior performance both qualitatively and quantitatively compared to state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Zeji Huang and Huanda Lu and Xin Yu and Hui Xiao},
  doi          = {10.1049/ipr2.70105},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70105},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale feature guided transformer for image inpainting},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PartConverter: A part-oriented transformation framework for point clouds. <em>IETIP</em>, <em>19</em>(1), e70104. (<a href='https://doi.org/10.1049/ipr2.70104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With generative AI technologies advancing rapidly, the capabilities for 3D model generation and transformation are expanding across industries like manufacturing, healthcare, and virtual reality. However, existing methods based on generative adversarial networks (GANs), autoencoders, or transformers still have notable limitations. They primarily generate entire objects without providing flexibility for independent part transformation or precise control over model components. These constraints pose challenges for applications requiring complex object manipulation and fine-grained adjustments. To overcome these limitations, we propose PartConverter, a novel part-oriented point cloud transformation framework emphasizing flexibility and precision in 3D model transformations. PartConverter leverages attention mechanisms and autoencoders to capture crucial details within each part while modeling the relationships between components, thereby enabling highly customizable, part-wise transformations that maintain overall consistency. Additionally, our part assembler ensures that transformed parts align coherently, resulting in a consistent and realistic final 3D shape. This framework significantly enhances control over detailed part modeling, increasing the flexibility and efficiency of 3D model transformation workflows.},
  archive      = {J_IETIP},
  author       = {Sheng-Yun Zeng and Tyng-Yeu Liang},
  doi          = {10.1049/ipr2.70104},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70104},
  shortjournal = {IET Image Process.},
  title        = {PartConverter: A part-oriented transformation framework for point clouds},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study on region-aware fashion design based on probabilistic diffusion model. <em>IETIP</em>, <em>19</em>(1), e70103. (<a href='https://doi.org/10.1049/ipr2.70103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issue of image distortion caused by substantial structural discrepancies between original garment images and reference images in the application of image transfer technology for fashion design, this study proposes a region-aware fashion design method based on a probabilistic diffusion model. During the image feature extraction and output stage, the method integrates vision transformer (ViT) with a mask-guided mechanism, enabling the Diffusion model to precisely focus on the transferable regions of the original and reference images, thereby preserving the structural integrity and semantic consistency of the source images effectively. In the image colour and pattern style transfer stage, this study introduces an asymmetric gradient guidance (AGG) strategy to optimise the reverse sampling process of the diffusion model, substantially improving the quality and visual fidelity of the generated images. Experimental results indicate that this method achieves a Fréchet inception distance (FID) score of 103.4, surpassing existing fashion synthesis models. This facilitates the generation of more stable and realistic images for garment design tasks.},
  archive      = {J_IETIP},
  author       = {Jie Sun and Kejun Cen and Xiaojun Ding and Sarah Haidar and Fengyuan Zou},
  doi          = {10.1049/ipr2.70103},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70103},
  shortjournal = {IET Image Process.},
  title        = {A study on region-aware fashion design based on probabilistic diffusion model},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented multiple perturbation dual mean teacher model for semi-supervised intracranial haemorrhage segmentation. <em>IETIP</em>, <em>19</em>(1), e70102. (<a href='https://doi.org/10.1049/ipr2.70102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, there are two problems restrict the intracranial haemorrhage (ICH) segmentation task: scarcity of labelled data, and poor accuracy of ICH segmentation. To address these two issues, we propose a semi-supervised ICH segmentation model and a dedicated ICH segmentation backbone network. Our approach aims at leveraging semi-supervised modelling so as to alleviate the challenge of limited labelled data availability, while the dedicated ICH segmentation backbone network further enhances the segmentation precision. An augmented multiple perturbation dual mean teacher model is designed. Based on it, the prediction accuracy may be improved by a more stringent confidence-weighted cross-entropy (CW-CE) loss, and the feature perturbation may be increased using adversarial feature perturbation for the purpose of improving the generalization ability and efficiency of consistent learning. In the ICH segmentation backbone network, we promote the segmentation accuracy by extracting both local and global features of ICH and fusing them in depth. We also fuse the features with rich details from the upper encoder during the up-sampling process to reduce the loss of feature information. Experiments on our private dataset ICHDS, and the public dataset IN22SD demonstrate that our model outperforms current state-of-the-art ICH segmentation models, achieving a maximum improvement of over 10% in Dice and exhibiting the best overall performance.},
  archive      = {J_IETIP},
  author       = {Yan Dong and Xiangjun Ji and Ting Wang and Chiyuan Ma and Zhenxing Li and Yanling Han and Kurosh Madani and Wenhui Wan},
  doi          = {10.1049/ipr2.70102},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70102},
  shortjournal = {IET Image Process.},
  title        = {Augmented multiple perturbation dual mean teacher model for semi-supervised intracranial haemorrhage segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on early warning and monitoring systems for contact between live equipment and foreign objects. <em>IETIP</em>, <em>19</em>(1), e70101. (<a href='https://doi.org/10.1049/ipr2.70101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the safety hazards arising from inadequate clearance between tree growth and power facilities, this paper innovatively proposes an online monitoring and early warning system for distribution networks based on advanced image processing technology. The system integrates three core functions: automatic tree species identification, precise detection of safety distances between tree crowns and live equipment, and real-time foreign object contact warnings. By deploying maintenance-free online monitoring terminals, the system can monitor tree growth around the distribution network around the clock, continuously. It utilizes efficient backend algorithms to intelligently analyse the collected image data, promptly detecting and warning of potential tree-obstacle hazards. Compared to traditional manual inspections and UAV inspections, this system not only significantly improves monitoring accuracy and real-time performance but also overcomes the limitations of UAV inspections, such as high costs and poor real-time performance. The solution proposed in this paper is expected to fundamentally enhance the safe operation level of distribution networks and effectively reduce power accidents such as tripping and short circuits caused by tree obstacles. Future plans include further optimizing system functions to promote the wider application and promotion of the technology.},
  archive      = {J_IETIP},
  author       = {Fang Yuan and BoYuan Chen and YunFei Tan and KaiYang Liao and WeiMin Xia},
  doi          = {10.1049/ipr2.70101},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70101},
  shortjournal = {IET Image Process.},
  title        = {Research on early warning and monitoring systems for contact between live equipment and foreign objects},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Realistic object reconstruction under different depths through light field imaging for virtual reality. <em>IETIP</em>, <em>19</em>(1), e70099. (<a href='https://doi.org/10.1049/ipr2.70099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) immerses users in digital environments and is used in various applications. VR content is created using either computer-generated or conventional imaging. However, conventional imaging captures only 2D spatial information, which limits the realism of VR content. Advanced technologies like light field (LF) imaging can overcome this limitation by capturing both 2D spatial and 2D angular information in 4D LF images. This paper proposes a depth reconstruction model through LF imaging to aid in creating realistic VR content. Comprehensive calibrations are performed, including adjustments for camera parameters, depth calibration, and field of view (FOV) estimation. Aberration corrections, like distortion and vignetting effect correction, are conducted to enhance the quality of the reconstruction. To achieve realistic scene reconstruction, experiments were conducted by setting up a scenario with multiple objects positioned at three different depths. Quality assessments were carried out to evaluate the reconstruction quality across these varying depths. The results demonstrate that depth reconstruction quality improves with the proposed method. It also indicates that the model reduces LF image size and processing time. The depth images reconstructed by the proposed model have the potential to generate realistic VR content and can also facilitate the integration of refocusing capabilities within VR environments.},
  archive      = {J_IETIP},
  author       = {Ali Khan and Md. Moinul Hossain and Alexandra Covaci and Konstantinos Sirlantzis and Qi Qi},
  doi          = {10.1049/ipr2.70099},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70099},
  shortjournal = {IET Image Process.},
  title        = {Realistic object reconstruction under different depths through light field imaging for virtual reality},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for domain generalization with a multi-classifier ensemble approach. <em>IETIP</em>, <em>19</em>(1), e70098. (<a href='https://doi.org/10.1049/ipr2.70098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization poses significant challenges, particularly as models must generalize effectively to unseen target domains after training on multiple source domains. Traditional approaches typically aim to minimize domain discrepancies; however, they often fall short when handling complex data variations and class imbalance. In this paper, we propose an innovative model, the self-supervised learning multi-classifier ensemble (SSL-MCE), to address these limitations. SSL-MCE integrates self-supervised learning within a dynamic multi-classifier ensemble framework, leveraging ResNet as a shared feature extraction backbone. By combining four distinct classifiers, it captures diverse and complementary features, thereby enhancing adaptability to new domains. A self-supervised rotation prediction task enables SSL-MCE to focus on intrinsic data structures rather than domain-specific details, learning robust domain-invariant features. To mitigate class imbalance, we incorporate adaptive focal attention loss (AFAL), which dynamically emphasizes challenging and rare instances, ensuring improved accuracy on difficult samples. Furthermore, SSL-MCE adopts a dynamic loss-based weighting scheme to prioritize more reliable classifiers in the final prediction. Extensive experiments conducted on public benchmark datasets, including PACS and DomainNet, indicate that SSL-MCE outperforms state-of-the-art methods, achieving superior generalization and resource efficiency through its streamlined ensemble framework.},
  archive      = {J_IETIP},
  author       = {Zhenkai Qin and Qining Luo and Xunyi Nong and Xiaolong Chen and Hongfeng Zhang and Cora Un In Wong},
  doi          = {10.1049/ipr2.70098},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70098},
  shortjournal = {IET Image Process.},
  title        = {Self-supervised learning for domain generalization with a multi-classifier ensemble approach},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Point cloud registration based on multiple neighborhood feature difference. <em>IETIP</em>, <em>19</em>(1), e70097. (<a href='https://doi.org/10.1049/ipr2.70097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense point cloud registration is a critical problem in computer vision and 3D reconstruction, with widespread applications in scenarios such as robotic navigation, autonomous driving, and 3D measurement. However, dense point cloud registration faces significant challenges, including high computational complexity and prolonged processing times. To address these issues, this paper proposes a point cloud registration method based on multiple neighborhood feature difference (MNFD) that employs a coarse-to-fine strategy to effectively enhance both registration efficiency and accuracy. The proposed method consists of two stages: coarse registration and fine registration. In the coarse registration stage, a novel feature point extraction approach based on MNFD is introduced, capable of identifying highly stable and distinctive feature points in the point cloud. These feature points are then utilized in combination with the fast point feature histogram (FPFH) algorithm to achieve an initial alignment between the target and template point clouds. In the fine registration stage, the results from the coarse alignment are refined using algorithms such as iterative closest point (ICP) to ensure both efficiency and precision during the registration process. Experiments conducted on publicly available datasets demonstrate the superiority of the proposed method compared to existing approaches.},
  archive      = {J_IETIP},
  author       = {Haixia Wang and Teng Wang and Zhiguo Zhang and Xiao Lu and Qiaoqiao Sun and Shibin Song and Jun Nie},
  doi          = {10.1049/ipr2.70097},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70097},
  shortjournal = {IET Image Process.},
  title        = {Point cloud registration based on multiple neighborhood feature difference},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMD-DAAN: A wasserstein distance-based dynamic adversarial domain adaptation network model for breast ultrasound image classification. <em>IETIP</em>, <em>19</em>(1), e70096. (<a href='https://doi.org/10.1049/ipr2.70096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is commonly diagnosed through ultrasound imaging as a primary method in clinical practice. However, the lack of large annotated datasets for breast ultrasound images, along with issues such as inconsistent edge and conditional distributions across different datasets, poses significant challenges to both manual and AI-assisted diagnosis. To address these issues, this paper proposes a dynamic adversarial domain adaptation model based on Wasserstein distance (EMD_DAAN). The EMD_DAAN model enhances the existing dynamic adversarial domain adaptation framework by incorporating an adaptive layer, further aligning the feature distributions between the source and target domain datasets. The Wasserstein distance is employed to optimize this adaptive layer, minimizing the distributional discrepancy between the feature spaces of the two domains by constructing the least-cost transport path. This approach improves the model's cross-domain generalization ability and robustness to noise interference. Through dual feature alignment via the adaptive layer and adversarial learning, the model's classification performance on breast ultrasound images is significantly enhanced. Experimental results demonstrate that the EMD_DAAN model achieves an accuracy of 82.75% on breast ultrasound images, substantially outperforming typical adversarial domain adaptation models such as DAAN in terms of classification performance.},
  archive      = {J_IETIP},
  author       = {Ying Wu and Hao Huang and Bo Xu},
  doi          = {10.1049/ipr2.70096},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70096},
  shortjournal = {IET Image Process.},
  title        = {EMD-DAAN: A wasserstein distance-based dynamic adversarial domain adaptation network model for breast ultrasound image classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of advancement in lip reading models: Techniques and future directions. <em>IETIP</em>, <em>19</em>(1), e70095. (<a href='https://doi.org/10.1049/ipr2.70095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lip reading models improve information processing and decision-making by quickly and accurately comprehending enormous amounts of text. This study dives into the important role that lip reading plays in making communication more inclusive, especially for individuals with hearing impairments. From 2020 to 2024, the researchers carefully examine the progress made in lip-reading algorithms. They take a close look at the methods, innovations and principles used to decode spoken content from videos, specifically using visual speech recognition techniques. The study also emphasises the use of datasets like LRW, LRS2 and LRS3, which are crucial for this exploration. This paper offers valuable insights into recent advancements and highlights the importance of diverse datasets in improving lip-reading models. Its findings aim to guide future research efforts in making communication more accessible for people with hearing impairments.},
  archive      = {J_IETIP},
  author       = {Sampada Deshpande and Kalyani Shirsath and Amey Pashte and Pratham Loya and Sandip Shingade and Vijay Sambhe},
  doi          = {10.1049/ipr2.70095},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70095},
  shortjournal = {IET Image Process.},
  title        = {A comprehensive survey of advancement in lip reading models: Techniques and future directions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectionally guided multi-scale feature decoding network for high-resolution salient object detection. <em>IETIP</em>, <em>19</em>(1), e70094. (<a href='https://doi.org/10.1049/ipr2.70094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of modern camera technology, the resolution and quality of images have been significantly improved. High-resolution images can provide more detailed and clearer information, but they may also introduce more noise, which interferes with the detection and localization of salient objects. To address this issue, existing high-resolution salient object detection methods either design complex network structures or adopt multi-modal fusion. However, these approaches often consume significant computing and storage resources. This leads to redundancy of irrelevant features and loss of critical details. In this paper, we propose a network called bidirectionally guided multi-scale feature decoding network for high-resolution salient object detection. The model incorporates a bidirectional guidance method to explore the complementarity between encoding and decoding features, thereby achieving a comprehensive combination and enhancement of features. Additionally, in the decoder, multi-scale encoding features are obtained and utilized sequentially to enhance feature learning and improve the accuracy of salient object detection. Specifically, our model consists of an encoder, a guided multi-scale feature enhancement (GMFE) module, a guided feature fusion (GFF) module, and a multi-scale feature decoder (MFD) module. First, multi-scale encoding features are extracted through the encoder. These features are then fed into the GMFE module to enhance the multi-scale encoding features under the guidance of saliency map derived from the decoding features of the previous layer. Subsequently, in the GFF module, the enhanced encoding features are fused with the decoding features from the previous layer. Finally, in the MFD module, the bidirectionally guided multi-scale encoding features is integrated to generate an accurate saliency map. Experiments on two high-resolution and two low-resolution datasets demonstrate that our model outperforms on high-resolution datasets while maintaining competitive performance on low-resolution datasets, underscoring its effectiveness across varying image qualities.},
  archive      = {J_IETIP},
  author       = {Jiangping Tang and Shuyao Guo and Xiaofei Zhou and Liuxin Bao and Jiyong Zhang and Tong Qiao},
  doi          = {10.1049/ipr2.70094},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70094},
  shortjournal = {IET Image Process.},
  title        = {Bidirectionally guided multi-scale feature decoding network for high-resolution salient object detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal attention transformer for video text retrieval. <em>IETIP</em>, <em>19</em>(1), e70093. (<a href='https://doi.org/10.1049/ipr2.70093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the metaverse, video text retrieval is an urgent and challenging need for users in social entertainment. The current attention-based video text retrieval models have not fully explored the interaction between video and text, and only brute force feature embedding. Moreover, Due to the unsupervised nature of attention weight training, existing models have weak generalization performance for dataset bias. Essentially, the model learns that false relevant information in the data is caused by confounding factors. Therefore, this article proposes a video text retrieval method based on causal attention transformer. Assuming that the confounding factors affecting the performance of video text retrieval all come from the dataset, a structural causal model that conforms to the video text retrieval task is constructed, and the impact of confounding effects during data training is reduced by adjusting the front door. In addition, we use causal attention transformer to construct a causal inference network to extract causal features between video text pairs, and replace the similarity statistical probability with causal probability in the video text retrieval framework. Experiments are conducted on the MSR-VTT, MSVD, and LSMDC datasets, which proves the effectiveness of the retrieval model proposed in this paper.},
  archive      = {J_IETIP},
  author       = {Hua Lan and Chaohui Lv},
  doi          = {10.1049/ipr2.70093},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70093},
  shortjournal = {IET Image Process.},
  title        = {Causal attention transformer for video text retrieval},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain adaptation of foreground and scale sensing for gastric polyp detection. <em>IETIP</em>, <em>19</em>(1), e70092. (<a href='https://doi.org/10.1049/ipr2.70092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated detection of gastric polyps has been proven crucial for improving diagnostic accuracy. However, when there is a domain shift in the data, deep learning-based detection methods may not perform well. Unsupervised domain adaptation has been demonstrated as a good approach to address this issue. However, existing unsupervised domain adaptation detection methods struggle to handle the problem of foreground–background similarity and the diverse appearances of polyps at different scales in gastric polyp images. In this paper, we propose a boundary-guided transferable attention module and a transferable prototype alignment module to address the foreground–background similarity issue, and a multi-scale enhanced alignment method to tackle the problem of information loss when aligning polyps at multiple scales. The boundary-guided transferable attention module fully explores spatial information of the image with a boundary-guided multi-field attention mechanism while considering the transferability of features to mine the easily transferable foreground regions. The transferable prototype alignment module adopts a prototype-based method to facilitate the transfer of difficult-to-align regions. The multi-scale enhanced alignment method prevents information loss across feature maps and scales with an attention filtering module, enhancing features at each scale. In experiments, this work outperforms advanced domain adaptation detection methods like SIGMA and CAT in polyp detection.},
  archive      = {J_IETIP},
  author       = {Ying Zheng and Junhe Zhang and Yao Yu and Changyin Sun},
  doi          = {10.1049/ipr2.70092},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70092},
  shortjournal = {IET Image Process.},
  title        = {Domain adaptation of foreground and scale sensing for gastric polyp detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing quantum image encryption with QLSTM and chaos synchronisation control: A deep neural network approach. <em>IETIP</em>, <em>19</em>(1), e70091. (<a href='https://doi.org/10.1049/ipr2.70091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum image encryption is crucial for data protection, but current methods lack attack resistance and have complex encryption processes. This paper proposes a quantum long short - term memory (QLSTM)-based quantum image encryption method to enhance chaotic sequences and achieve chaos synchronisation control. The QLSTM network improves the Lorenz chaotic sequence, increasing its unpredictability. An adaptive synchronisation control algorithm, using the enhanced chaotic sequence from QLSTM, ensures sender-receiver synchronization. Optimised through deep neural networks, the system maintains stable synchronization under interference. New cryptographic quantum infrastructure (NCQI) was constructed, and images were encrypted using third-order radial diffusion, quantum generalised Arnold transform, and quantum W transform. The QLSTM-improved chaotic sequence showed excellent LLE and 0–1 test results. Information entropy was near 8, with R, G and B channels exceeding 7.999. Anti-attack analysis revealed high information entropy, strong attack resistance, and number of pixels change rate/unified average changing intensity (NPCR/UACI) values of 99.698% and 33.460%, respectively, indicating significant pixel-level changes. Combining quantum chaotic system prediction with the QLSTM model enhanced quantum communication stability and anti-interference ability. This QLSTM-based quantum encryption method, with chaos synchronisation control, significantly improves encryption security and reliability, maintaining high information entropy and complexity under attacks, proving its effectiveness in image encryption.},
  archive      = {J_IETIP},
  author       = {Yuebo Wu and Duansong Wang and Jian Zhou and Huifang Bao},
  doi          = {10.1049/ipr2.70091},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70091},
  shortjournal = {IET Image Process.},
  title        = {Enhancing quantum image encryption with QLSTM and chaos synchronisation control: A deep neural network approach},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aluminum surface defect detection method based on DAS-YOLO network. <em>IETIP</em>, <em>19</em>(1), e70090. (<a href='https://doi.org/10.1049/ipr2.70090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the accuracy limitations of current methods in detecting aluminum surface defects, particularly those with small sizes and high variation, an aluminum surface defect detection algorithm named DAS-YOLO, based on an improved YOLOv8n, is proposed. The C2f module in YOLOv8's backbone is enhanced by incorporating DCNv2, which improves the model's ability to handle irregular shapes and geometric transformations during feature extraction. An auxiliary training head (Aux Head) is added to capture multi-scale and multi-level features, significantly boosting small defect detection. Additionally, the traditional CIoU loss function is replaced with the Wise-SIoU loss, accelerating convergence and enhancing both detection and regression accuracy. Experimental results on the Alibaba Tianchi aluminum surface defect dataset show that DAS-YOLO achieves a mean average precision (mAP) of 85.3%. Compared to YOLOv8n, mAP50 improves by 3%, while precision and recall increase by 1.1% and 4.6%, respectively. Furthermore, to validate the model's performance on small defects and its generalization ability, it achieves a detection accuracy of 94.8% on the PCB dataset, with an mAP increase of 3.1% compared to YOLOv8n. These results demonstrate that DAS-YOLO significantly enhances detection accuracy while maintaining speed and exhibits outstanding performance in small defect detection.},
  archive      = {J_IETIP},
  author       = {Jun Tie and Jiating Ma and Lu Zheng and Chengao Zhu and Mian Wu and HaiJiao Wang and ChongWei Ruan and Shuangyang Li},
  doi          = {10.1049/ipr2.70090},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70090},
  shortjournal = {IET Image Process.},
  title        = {Aluminum surface defect detection method based on DAS-YOLO network},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing image decomposition with large separable kernel attention in generative adversarial networks. <em>IETIP</em>, <em>19</em>(1), e70089. (<a href='https://doi.org/10.1049/ipr2.70089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The original generative adversarial network (GAN) model may struggle to adequately capture global information in images, particularly during complex decomposition tasks, leading to limitations in image clarity, detail retention and overall consistency. To address this challenge, we propose the large separable kernel attention generative adversarial network (LSKA-GAN) model, building upon the blind image decomposition network (BIDeN). The LSKA module enhances BIDeN's global information capturing capability, thereby improving the quality and clarity of generated images. Experimental results demonstrate that LSKA-GAN achieves obvious improvements in hybrid image decomposition. Compared to BIDeN, LSKA-GAN exhibits an increase of 1.39 dB in peak signal-to-noise ratio (PSNR) and 0.04 in structural similarity index (SSIM). These improvements enable LSKA-GAN to generate clearer images with more complete details, marking a notable advancement in image decomposition technology.},
  archive      = {J_IETIP},
  author       = {Mingzhan Zhao and Ziyun Su and Xiaoyi Du},
  doi          = {10.1049/ipr2.70089},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70089},
  shortjournal = {IET Image Process.},
  title        = {Enhancing image decomposition with large separable kernel attention in generative adversarial networks},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage feature selection for fine-grained image recognition via partial order analysis and heterogeneity evaluation. <em>IETIP</em>, <em>19</em>(1), e70088. (<a href='https://doi.org/10.1049/ipr2.70088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core challenge of fine-grained image recognition (FGIR) tasks is distinguishing highly similar subclasses within the same base category. Most CNN-based deep learning methods typically focus on extracting information from local regions while overlook the inherent structure between subclasses and the complex relationships between features. This paper presents a two-stage feature selection method based on partial order analysis (POA) and heterogeneity evaluation (HE) for FGIR tasks, guiding the model to focus on distinctive features while reducing uncertainty caused by interfering information. Specifically, in the POA stage, clustering first groups similar subcategories into a medium-granularity category. Formal concept analysis then models their hierarchical partial order, identifying “shared features” among subcategories and “exclusive features” unique to each. This structured representation highlights key contrastive cues. In the HE stage, a novel heterogeneity index is introduced to measure the fluctuation of low-level features within each fine-grained category. This index guides the model to suppress pseudo-discriminative features with high heterogeneity, mitigating the impact of noisy and unstable information on decision-making. We perform comprehensive experiments on three commonly used benchmark datasets (CUB-200-2011, Stanford Cars, and FGVC-Aircraft). Experimental results show that the proposed method outperforms classic FGIC methods, validating the effectiveness of our approach.},
  archive      = {J_IETIP},
  author       = {Hongli Gao and Sulan Zhang and Huiyuan Zhou and Lihua Hu and Jifu Zhang},
  doi          = {10.1049/ipr2.70088},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70088},
  shortjournal = {IET Image Process.},
  title        = {Two-stage feature selection for fine-grained image recognition via partial order analysis and heterogeneity evaluation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on intelligent and high-precision structure-recognition methods for field geological outcrop images. <em>IETIP</em>, <em>19</em>(1), e70087. (<a href='https://doi.org/10.1049/ipr2.70087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate recognition of geological structures in field outcrop images is critical for applications such as geological hazard analysis, seismic risk assessment, and urban geological planning. However, traditional manual interpretation of geological images is time-consuming, labor-intensive, and subjective, limiting its scalability and precision. To address this gap, this study proposes an intelligent, automated recognition method for field geological outcrop images based on deep learning techniques. The methodology integrates Fourier transform, Canny edge detection, and Mask R-CNN instance segmentation, enhanced with image normalization and data augmentation strategies such as grayscale conversion, Gaussian filtering, and rotation. A custom dataset comprising 4260 images was constructed and annotated using a hybrid approach involving edge detection and expert labeling. The proposed model, improved with PrRoI Pooling, outperforms conventional models such as YOLOv3, Faster R-CNN, and standard Mask R-CNN, achieving a mean average precision (mAP) of 90.77% in detecting fault, fold, and sausage-like geological structures. The results demonstrate the model's robustness, accuracy, and suitability for complex geological environments. This study not only advances the state-of-the-art in geological image recognition but also lays a foundation for future research into broader structural classification, multi-modal geological data integration, and real-time field deployment.},
  archive      = {J_IETIP},
  author       = {Mingguang Diao and Kaixuan Liu and Shupeng Wang and Chuyan Zhang},
  doi          = {10.1049/ipr2.70087},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70087},
  shortjournal = {IET Image Process.},
  title        = {Research on intelligent and high-precision structure-recognition methods for field geological outcrop images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Infrared and visible image fusion based on autoencoder network. <em>IETIP</em>, <em>19</em>(1), e70086. (<a href='https://doi.org/10.1049/ipr2.70086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the problems of texture information loss and insufficiently prominent targets in existing fusion networks, an information decomposition-based autoencoder fusion network for infrared and visible images is proposed in this paper. Two salient information encoders with unshared weights and two scene information encoders with shared weights are designed to extract different features from infrared and visible images, respectively. The constraint is added to the loss function in order to ensure the ability of the salient information encoders to extract representative features and the scene information encoder to extract the cross-modality feature. In addition, by introducing the pre-trained semantic segmentation networks to guide the network training and constructing a feature saliency-based fusion strategy, the ability of the fusion network is further enhanced to distinguish between targets and backgrounds. Extensive experiments are carried out on five datasets. Comparison experiments with state-of-the-art fusion networks and ablation experiments indicate that the proposed method can obtain fused images with richer and more comprehensive information and is more robust to challenging factors, such as strong and weak light smoke and fog environments. At the same time, the fused images by our proposed method are more beneficial for downstream tasks such as target detection.},
  archive      = {J_IETIP},
  author       = {Hongmei Wang and Xuanyu Lu and Zhuofan Wu and Ruolin Li and Jingyu Wang},
  doi          = {10.1049/ipr2.70086},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70086},
  shortjournal = {IET Image Process.},
  title        = {Infrared and visible image fusion based on autoencoder network},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high-accuracy YOLOv8-ResAttNet framework for maritime vessel detection using residual attention. <em>IETIP</em>, <em>19</em>(1), e70085. (<a href='https://doi.org/10.1049/ipr2.70085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Against the backdrop of constantly upgrading maritime security requirements and dynamic marine environments, satellite based ship detection has become a key technology for national maritime surveillance, resource management, and environmental protection. However, existing methods often struggle to address ongoing challenges, including insufficient sensitivity to small vessels and susceptibility to errors or missed detections in complex ocean backgrounds caused by wave reflections, cloud cover, and lighting changes. To address these limitations, this study proposes YOLOv8 ResAttNet, an enhanced model that integrates residual learning and attention mechanisms into the YOLOv8 framework. The core innovation lies in a custom designed backbone network that combines multi-scale feature aggregation with an improved ICBAM attention module to achieve precise localization of ship targets while suppressing irrelevant background noise. This architecture dynamically recalibrates feature channel weights through residual attention blocks, enhancing the model's ability to distinguish subtle ship features (such as hull contours and superstructures) in different maritime scenarios. Extensive experiments on high-resolution HRSID datasets have demonstrated the superiority of this model: the average accuracy (mAP50) of YOLOv8 ResAttNet is 95.2%, which is 4.9% higher than the original YOLOv8 and over 4% higher than state-of-the-art models such as YOLO SENet and YOLO11. These improvements highlight its robustness in handling scale changes and complex background interference. The research results emphasize the effectiveness of combining residual connectivity with attention driven feature refinement for maritime target detection, especially in small target scenes. This work not only advances the technological frontier of remote sensing image analysis, but also provides a scalable framework for real-world applications such as illegal fishing monitoring, maritime traffic management, and disaster response. Future research directions include extending the model to multimodal satellite data fusion, optimizing the computational efficiency of edge device deployment, and further bridging the gap between theoretical innovation and maritime surveillance systems.},
  archive      = {J_IETIP},
  author       = {Peixue Liu and Mingze Sun and Xinyue Han and Shu Liu and Yujie Chen and Han Zhang},
  doi          = {10.1049/ipr2.70085},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70085},
  shortjournal = {IET Image Process.},
  title        = {A high-accuracy YOLOv8-ResAttNet framework for maritime vessel detection using residual attention},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPFA-UNet: Dual-path fusion attention for accurate brain tumor segmentation. <em>IETIP</em>, <em>19</em>(1), e70084. (<a href='https://doi.org/10.1049/ipr2.70084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gliomas are the most common primary brain tumors within the central nervous system, typically observed through magnetic resonance imaging (MRI). Precise segmentation of brain tumor in MRI is highly significant for both clinical diagnosis and treatment. However, due to complexity of tumor structures, existing deep-learning-based methods for brain tumor segmentation still face challenges in accurately delineating tumor core (TC) and enhancing tumor (ET) regions, which are primary targets for actual treatment. To address this problem, this work proposes dual-path fusion attention-based UNet (DPFA-UNet) that leverages a dual-path attention block (DPA) and a concurrent attention fusion block (CAF) within a U-shaped architecture. Specifically, DPA enhances adaptability to lesions of varying sizes by using multi-scale branches that capture fine details and global features. CAF fuses high- and low-level semantic features using a parallel attention mechanism, effectively focusing on the focal regions. It also incorporates a mask generated by deep supervision mechanism to further guide feature fusion. Additionally, to reduce demand for hardware resources, we incorporate depthwise separable convolution into the model. Experiments are conducted on public BraTS 2021 and BraTS 2019 datasets. The results verify that DPFA-UNet outperforms existing brain tumor segmentation methods, particularly in segmenting TC and ET regions.},
  archive      = {J_IETIP},
  author       = {Jing Sha and Xu Wang and Zhongyuan Wang and Lu Wang},
  doi          = {10.1049/ipr2.70084},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70084},
  shortjournal = {IET Image Process.},
  title        = {DPFA-UNet: Dual-path fusion attention for accurate brain tumor segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMG-MATSM: Scene memory generation based on motion-aware temporal style modulation. <em>IETIP</em>, <em>19</em>(1), e70083. (<a href='https://doi.org/10.1049/ipr2.70083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene memory generation (SMG) refers to training AI agents to recall scene memories similarly to the human brain. This is the key work to realize the artificial memory system. The challenge is to generate scenes rich in motion and keep it realistic while ensuring temporal consistency. Inspired by the principles of memory function in brain neuroscience, this paper proposes a motion-aware scene generation model named SMG based on motion-aware temporal style modulation (SMG-MATSM), which ensures temporal consistency by redesigning the temporal latent representation and constructing a motion matrix to guide the motion of intermediate latent variables. The motion matrix preserves motion consistency in the scene memory through both the cosine similarity and the Mahalanobis distance of intermediate latent variables of adjacent frames. Additionally, SMG-MATSM uses a style-based approach and enhances conditional features through the motion matrix during the scene memory synthesis process. Experimental results show that SMG-MATSM has better effect of action-enriched scene memory generation, and has varying degrees of efficiency improvement on different datasets with Frechet video distance and Frechet inception distance evaluation metrics.},
  archive      = {J_IETIP},
  author       = {Liang Wang and Zhao Wang and Shaokang Zhang and Meng Wang and Haibo Liu},
  doi          = {10.1049/ipr2.70083},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70083},
  shortjournal = {IET Image Process.},
  title        = {SMG-MATSM: Scene memory generation based on motion-aware temporal style modulation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RSINS-GS: Reconstruction from single image with noise-added strategy and 3D-GS. <em>IETIP</em>, <em>19</em>(1), e70082. (<a href='https://doi.org/10.1049/ipr2.70082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-input reconstruction methods such as 3D-GS and NeRF excel in fidelity, yet they impose stringent requirements on the sequentiality of the input images. In contrast, single-view reconstruction methods are designed to extract certain features of the image even under limited input conditions. However, the majority of current single-view methods demand considerable graphics card performance for rendering at high resolutions and attaining high-fidelity image reconstruction at lower resolutions remains a formidable challenge. To enhance the fidelity of reconstructed images while considering the constraints of graphics card performances, we propose a novel pipeline based on novel-view synthetic (NVS), super-resolution (SR) and 3D-GS, named RSINS-GS. First, we introduce a divide-and-conquer strategy tailored to reap pixel-reinforced novel sequential views to render the reconstruction result without overburdening the graphics card, maintaining optimal performance and visual fidelity. Furthermore, to enhance the high fidelity of reconstructed images both in terms of qualitative and quantitative measures, we integrate 2D prior images with their corresponding geometric structural complements. Additionally, we introduce an innovative, generalised noise-added strategy to refine the overall reconstruction process. Extensive experimental evaluations on Nerf_synthetic datasets and Google scanned datasets show that our method achieves high quality results.},
  archive      = {J_IETIP},
  author       = {Shengyi Qian and Lan Cheng and Pengyue Li and Xinying Xu},
  doi          = {10.1049/ipr2.70082},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70082},
  shortjournal = {IET Image Process.},
  title        = {RSINS-GS: Reconstruction from single image with noise-added strategy and 3D-GS},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pano-matching: Panoramic local feature matching using interconnected dynamic transformer. <em>IETIP</em>, <em>19</em>(1), e70081. (<a href='https://doi.org/10.1049/ipr2.70081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of consumer-level panoramic cameras, panoramic projection is increasingly used in various industries, such as social communication, entertainment, and education. The utilization of image information, particularly in panoramic projection, predominantly depends on image matching. This technology involves extracting key features from images to meet the requirements of subsequent tasks. However, due to the distortion caused by geometric deformation in panoramic projection, traditional planar image matching methods encounter challenges in key point detection, matching accuracy, and pose estimation, often leading to failure or suboptimal performance. To address this challenge, a novel detector-free matching method is proposed. Pano-matching introduces two key innovations: a clustering-based dynamic pruning scheme to accelerate attention convergence by focusing on valid feature pairs, and a redesigned fine-level matching approach that effectively leverages both the central feature vector and its local neighbourhood. These innovations allow pano-matching to handle the distortions in panoramic images and outperform existing convolutional neural network-based methods in both accuracy and computational efficiency. Experimental results demonstrate that pano-matching achieves state-of-the-art performance in pose estimation and feature matching, significantly improving over current panoramic and planar matching methods.},
  archive      = {J_IETIP},
  author       = {Yali Xue and Xiaorui Wang and Taili Li and Quan Ouyang and Shan Cui},
  doi          = {10.1049/ipr2.70081},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70081},
  shortjournal = {IET Image Process.},
  title        = {Pano-matching: Panoramic local feature matching using interconnected dynamic transformer},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). More realistic edges, textures, and colors for image non-homogeneous dehazing. <em>IETIP</em>, <em>19</em>(1), e70079. (<a href='https://doi.org/10.1049/ipr2.70079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing image dehazing algorithms perform suboptimal in non-homogeneous and/or dense haze scenarios. The loss of feature information and alteration of color distribution cause images to deviate from real-world scenes when haze suppresses image details. To address these issues, we design a dual-branch non-homogeneous dehazing network integrating discrete wavelet transform (DWT), multi-scale feature fusion, and color constraints to achieve dehazed images with more realistic edges, textures, and colors. Specifically, we first introduce DWT into a multi-scale encoder–decoder network structure to capture more details and edge information. Then, a feature supplement and enhancement module (FSEM) combining features from hazy images at different scales and features from the previous stage is devised to enhance the multi-scale feature capture capability of rich textures in complex scenes. Finally, we propose a pixel-wise color consistency loss that combines pixel similarity and angular difference to constrain the dehazed images to closely match the color distribution of clear images. Experimental results indicate that the proposed dehazing network outperforms the state-of-the-art non-homogeneous dehazing methods on relevant public benchmarks and has more realistic edges, textures, and colors.},
  archive      = {J_IETIP},
  author       = {Hairu Guo and Yaning Li and Zhanqiang Huo and Shan Zhao and Yingxu Qiao},
  doi          = {10.1049/ipr2.70079},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70079},
  shortjournal = {IET Image Process.},
  title        = {More realistic edges, textures, and colors for image non-homogeneous dehazing},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The patch-based multi-task multi-scale reborn network for global gaze following in 360-degree images. <em>IETIP</em>, <em>19</em>(1), e70078. (<a href='https://doi.org/10.1049/ipr2.70078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a global gaze following method using the patched-based multi-task multi-scale reborn network (MMRGaze360) specifically designed for panorama images. Unlike existing approaches that rely on spherical networks or process only local regions, our architecture thoroughly accounts for the distortions introduced by the sphere-to-plane projection, enabling gaze following in comprehensive 360-degree images. MMRGaze360 incorporates field-of-view (360-FoV) and sight line (360-Gaze) generators to model gaze behaviours and scene information in 360-degree images. A multi-task multi-scale module is introduced to capture features from multiple patches centred around the estimated points located in the 360-Gaze, using multi-scale attention maps. These features, along with the 360-FoV, are fused to produce a final heatmap. Additionally, we employ multi-layer perceptions and convolutional networks using the reborn mechanism to enhance information usage and feature representation. Moreover, we establish a novel dataset, SRGaze360, which contains more conditions of the sphere-to-plane distortion. Experimental results on the GazeFollow360 and SRGaze360 datasets demonstrate the superiority of our method over previous works. It can be validated that our approach effectively addresses the limitations of 2D gaze following in handling out-of-frame gaze positions and distortions in 360-degree images.},
  archive      = {J_IETIP},
  author       = {Jingzhao Dai and Yang Li and Sidan Du},
  doi          = {10.1049/ipr2.70078},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70078},
  shortjournal = {IET Image Process.},
  title        = {The patch-based multi-task multi-scale reborn network for global gaze following in 360-degree images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised retinex exposure control: A novel approach to image enhancement. <em>IETIP</em>, <em>19</em>(1), e70077. (<a href='https://doi.org/10.1049/ipr2.70077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In domains such as autonomous driving and remote sensing, images often suffer from challenging lighting conditions, including low-light, backlighting and overexposure, which hinder the recognition of pedestrians, vehicles and traffic signs. While numerous methods have been proposed to address poor image exposure, they often struggle with images containing both low-light and overexposed regions. This paper presents an unsupervised learning-based exposure control method, providing a novel approach to improving image quality under diverse lighting conditions. Leveraging the inherent properties of Retinex theory, we introduce a novel yet simple formula that adjusts image exposure to produce visually pleasing results without requiring paired training data. Experiments on diverse image datasets validate the effectiveness of our approach in addressing various exposure challenges while preserving critical visual details. Our framework not only simplifies the exposure control process but also achieves state-of-the-art performance, highlighting its potential for real-world applications in computer vision and image processing.},
  archive      = {J_IETIP},
  author       = {Yukun Yang and Libo Sun and Weipeng Shi and Wenhu Qin},
  doi          = {10.1049/ipr2.70077},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70077},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised retinex exposure control: A novel approach to image enhancement},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual attention transformers: Adaptive linear and hybrid cross attention for remote sensing scene classification. <em>IETIP</em>, <em>19</em>(1), e70076. (<a href='https://doi.org/10.1049/ipr2.70076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global contextual information compared to convolutional neural networks, making them promising for remote sensing image analysis. However, ViTs often overlook critical local features, limiting their ability to accurately interpret intricate scenes. To address this issue, we propose an adaptive linear hybrid cross attention transformer (ALHCT). It integrates adaptive linear (AL) attention and hybrid cross (HC) attention to simultaneously learn local and global features. AL is introduced into ViT, as it helps reduce computational complexity from exponential to linear scale. Furthermore, ALHCT incorporates two adaptive linear swin transformers (ALST) to achieve multi-scale feature representation, enabling the model to capture high-level semantics and fine details. Finally, to enhance global perception and discriminative power, HC attention fuse local and global features which captured by the two ALST. Experiments on three remote sensing datasets demonstrate that ALHCT significantly improves classification accuracy, outperforming several state-of-the-art methods, validating its effectiveness in classifying complex remote sensing scenes.},
  archive      = {J_IETIP},
  author       = {Yake Zhang and Yufan Zhao and Jianlong Wang and Zhengwei Xu and Dong Liu},
  doi          = {10.1049/ipr2.70076},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70076},
  shortjournal = {IET Image Process.},
  title        = {Dual attention transformers: Adaptive linear and hybrid cross attention for remote sensing scene classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breaking tradition with perception: Debiasing strategies in cloth-changing person re-identification. <em>IETIP</em>, <em>19</em>(1), e70075. (<a href='https://doi.org/10.1049/ipr2.70075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person ReID aims to match images of individuals captured from different camera views for identity retrieval. Traditional ReID methods primarily rely on clothing features, assuming that individuals do not change clothes in a short time frame. This assumption significantly reduces recognition accuracy when clothing changes, particularly in long-term ReID tasks cloth-changing person re-identification (CC-ReID). Thus, achieving effective re-identification in clothing-change scenarios has become a critical challenge. This paper proposes an automatic perception model (APM) to address the break posed by clothing changes. The model uses a dual-branch with a dynamic perception learning (DPL) strategy and a perception branch, minimizing the bias introduced by clothing on identity recognition while preserving semantic features. The DPL strategy dynamically adjusts training weights to enhance the model's ability to learn from varying sample difficulties and feature distributions. The perception branch captures deeper feature relationships, alleviating the impact of clothing bias and improving the model's ability to distinguish intra-class transformations. Validated on Celeb-Reid and Celeb-Reid-light datasets, APM achieves a mean average precision (mAP) of 22.6% and 25.9%, with Rank-1 accuracy of 77.3% and 79.5%, respectively. It also excels in short-term ReID, achieving 90% mAP and 96.3% Rank-1 accuracy on Markt1501, demonstrating robustness across scenarios.},
  archive      = {J_IETIP},
  author       = {YiPeng Yin and Jian Wu and Bo Li},
  doi          = {10.1049/ipr2.70075},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70075},
  shortjournal = {IET Image Process.},
  title        = {Breaking tradition with perception: Debiasing strategies in cloth-changing person re-identification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-directional transformer image super-resolution network based on information enhancement. <em>IETIP</em>, <em>19</em>(1), e70074. (<a href='https://doi.org/10.1049/ipr2.70074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of deep learning, single-image super-resolution (SISR) has achieved significant progress. Recently, vision transformer-based super-resolution models have demonstrated remarkable performance; however, their high computational cost hinders their practical application. In this paper, we introduce a lightweight transformer-based super-resolution model termed information-enhanced efficient multi-directional transformer(IEMT). The model employs a dual-branch architecture that integrates the strengths of both convolutional neural network (CNN) and transformer networks. The proposed high-frequency extraction block (HEB) effectively captures high-frequency information from the enhanced image. Furthermore, a multi-directional attention mechanism is incorporated into the transformer branch to comprehensively learn latent features and details, thereby enhancing reconstruction quality. For attention computation, we propose a dynamic parameter-sharing mechanism that adaptively adjusts parameter sharing based on local image features, significantly reducing the model's parameter count. Experimental results demonstrate that the proposed IEMT achieves superior performance on five benchmark datasets, with a significantly reduced parameter count, computational complexity, and memory usage.},
  archive      = {J_IETIP},
  author       = {RongGui Wang and Xu Chen and Juan Yang and LiXia Xue},
  doi          = {10.1049/ipr2.70074},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70074},
  shortjournal = {IET Image Process.},
  title        = {Multi-directional transformer image super-resolution network based on information enhancement},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational weighted ℓp−ℓq regularization for hyperspectral image restoration under mixed noise. <em>IETIP</em>, <em>19</em>(1), e70073. (<a href='https://doi.org/10.1049/ipr2.70073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose to use weighted ℓ 2 $\ell _2$ -norm for approximating the solution of general ℓ p − ℓ q $\ell _p-\ell _q$ -norm regularization problem for recovering hyperspectral images (HSI) corrupted by a mixture of Gaussian-impulse noise. As a special case of p , q ∈ { 1 , 2 } $p,q\in \lbrace 1,2\rbrace$ , we design an optimization framework to accommodate the combined effect of different noise sources. An initial impulse noise pre-detection phase decouples the raw noisy HSI data into impulse and Gaussian corrupted pixels. Gaussian corrupted pixels are handled by data-fidelity term in while impulse corrupted pixels possess more Laplacian like behavior; modeled using . Solutions of problems involving in data fidelity and regularization terms complicate the optimization process but are less sensitive to the outlier pixels. On the other hand, the least square solutions for the data misfit are computationally efficient but generates solutions which are quite sensitive to the outlier pixels; which is the characteristic of impulse corrupted pixels. Therefore, in this paper, we decouple the set of pixels into two distinct parts; handled using two separate data fidelity terms. Total variation (TV) is used on the Casorati matrix representation of the input data to exploit similarity along both spatial and spectral dimensions. The resulting optimization problem is reformulated as iteratively reweighted least square for the general -norm problem for for data fidelity terms and for the TV regularization term. Experiments conducted over synthetically corrupted HSI data and images obtained from real HSI sensors confirm the suitability of the proposed weighted norm optimization framework (WNOF) over a wide range of degradation scenarios.},
  archive      = {J_IETIP},
  author       = {Hazique Aetesam and V. B. Surya Prasath},
  doi          = {10.1049/ipr2.70073},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70073},
  shortjournal = {IET Image Process.},
  title        = {Variational weighted ℓp−ℓq regularization for hyperspectral image restoration under mixed noise},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing semantic information representation in multi-view geo-localization through dual-branch network with feature consistency enhancement and multi-level feature mining. <em>IETIP</em>, <em>19</em>(1), e70071. (<a href='https://doi.org/10.1049/ipr2.70071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric learning is fundamental to multi-view geo-localization, as it aims to establish a distance metric that minimizes the feature space distance between similar data points while maximizing the separation between dissimilar ones. However, in Siamese networks employed for metric learning, individual branches may exhibit discrepancies in their interpretation of semantic information from input data, resulting in semantically inconsistent feature representations. To address this issue, a method is designed to enhance significant region consistency within multi-view spaces by integrating feature consistency enhancement (FCE) and multi-level feature mining (MLFM) techniques into a dual-branch network. The FCE method emphasizes critical components of the input data, ensuring feature consistency between the two branches. Additionally, the MLFM mechanism facilitates feature integration across multiple levels, thereby enabling a more comprehensive extraction of semantic information. This approach enhances semantic understanding and promotes feature consistency across branches. The proposed method achieves AP values of 82.38% for drone-to-satellite and 77.36% for satellite-to-drone image matching. Notably, the method maintains computational efficiency without significantly affecting inference time. Additionally, improvements are observed in R@1, R@5 and R@10 metrics. The experimental results show that integrating FCE and MLFM into the dual-branch network improves semantic representation and outperforms existing methods.},
  archive      = {J_IETIP},
  author       = {Yang Zheng and Qing Li and Jiangyun Li and Zhenghao Xi and Jie Liu},
  doi          = {10.1049/ipr2.70071},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70071},
  shortjournal = {IET Image Process.},
  title        = {Enhancing semantic information representation in multi-view geo-localization through dual-branch network with feature consistency enhancement and multi-level feature mining},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Craniopharyngioma detection and segmentation in MRI images. <em>IETIP</em>, <em>19</em>(1), e70070. (<a href='https://doi.org/10.1049/ipr2.70070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A tumour is an abnormal growth of human body tissues. Tumours are classified as benign or malignant. Malignant tumours cause serious health complications that may threaten a patient's life. The diagnosis of such tumours requires experienced and trained medical specialists. Alternatively, computerised tumour detection and localisation can help physicians to reach accurate, fast and reliable diagnosis. Craniopharyngioma (CP) is a brain tumour located in the sellar and parasellar regions of the central nervous system. It causes various symptoms such as headaches, visual and neurological disturbances, growth retardation and delayed puberty. In addition to histological examinations, multiple tissue characteristics are evaluated for accurate diagnosis of CP tumours. Patients with craniopharyngiomas are treated by total excision and post-operative radiotherapy in cases that have no hypothalamic invasion or sub-total resection. Early detection and diagnosis of the tumour can minimise the complications associated with surgical and radiotherapy treatments. In this article, an image processing technique for the segmentation and detection of brain tumours in general and craniopharyngioma in particular using MRI brain images, is presented. The technique is based on K-means clustering, multiple thresholding and iterative morphological operations. It was tested on 104 MRI images and the quantitative analysis of its effectiveness showed performance values of 98%, 93%, 100%, 95% and 100% for precision, recall, specificity, Dice score eoefficient and accuracy, respectively.},
  archive      = {J_IETIP},
  author       = {Mohamed Nasor and Walid Obaid},
  doi          = {10.1049/ipr2.70070},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70070},
  shortjournal = {IET Image Process.},
  title        = {Craniopharyngioma detection and segmentation in MRI images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Military aircraft recognition method based on attention mechanism in remote sensing images. <em>IETIP</em>, <em>19</em>(1), e70069. (<a href='https://doi.org/10.1049/ipr2.70069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images play a crucial role in fields such as reconnaissance and early warning, intelligence analysis, etc. Due to factors such as climate, season, lighting, occlusion and even atmospheric scattering during remote sensing image acquisition, targets of the same model exhibit significant intra-class variability. This article applies deep learning technology to the field of military aircraft recognition in remote sensing images and proposes a You Only Look Once Version 8 Small (YOLOv8s) remote sensing image military aircraft recognition algorithm based on an attention mechanism—YOLOv8s-TDP (YOLOv8s+TripletAttention+dysample+PIoU). First, the TripletAttention attention module is used in the neck network, which captures cross-dimensional interactions and utilises a three-branch structure to calculate attention weights. This further enhances the network's ability to preserve details and restore colours in the process of image fusion. Secondly, an efficient dynamic upsampler, dysample, is used to achieve dynamic upsampling through point sampling, which improves the problems of detail loss, jagged edges, and image distortion that may occur with nearest neighbour interpolation. Finally, replacing the original model loss function with PIoU (Pixels Intersection over Union), IoU (Intersection over Union) is calculated at the pixel level to more accurately capture small overlapping areas, reduce missed detection rates, and improve accuracy. On the publicly available dataset The Remote Sensing Image Military Aircraft Target Recognition Dataset(MAR20), our proposed YOLOv8s-TDP model achieved a of 82.96%, of 80.71%, of 87.11% and of 65.88%, outperforming the original YOLOv8s model, RT-DETR model, YOLOv5 series model, YOLOv7 series model, and YOLOv11 series model. Compared with the original YOLOv8s model, the YOLOv8s-TDP model improves by 0.23%, by 2.61%, by 2.76%, and by 2.49%, verifying that the algorithm has good performance in remote sensing image military aircraft recognition.},
  archive      = {J_IETIP},
  author       = {Kun Liu and Zhengfan Xu and Yang Liu and Guofeng Xu},
  doi          = {10.1049/ipr2.70069},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70069},
  shortjournal = {IET Image Process.},
  title        = {Military aircraft recognition method based on attention mechanism in remote sensing images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater image quality evaluation: A comprehensive review. <em>IETIP</em>, <em>19</em>(1), e70068. (<a href='https://doi.org/10.1049/ipr2.70068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image quality evaluation (UIQE) is crucial in improving image processing techniques and optimizing the design of the imaging system to obtain object information more accurately. However, existing UIQE methods are designed based on limited images or consider only a few natural scene statistics (NSS) metrics, lacking consideration for generalization across various underwater imaging applications. In this paper, an in-depth review of the existing UIQE methods based on evaluation operations is provided, emphasizing the bias present when evaluating UIQE methods using individual metrics. To address this, a novel metric called quadrilateral datum evaluation (QDE) is designed for UIQE methods. It comprehensively considers robustness across different datasets, as well as correlation and ranking consistency with mean opinion scores (MOS). This is the first solution to measure an UIQE method from an all-encompassing visual perspective. By using QDE, UIQE methods characterized by greater feature strength and small imbalance demonstrate good consistency and robustness across multiple aspects, providing a basis for the design of UIQE methods.},
  archive      = {J_IETIP},
  author       = {Mengjiao Shen and Miao Yang and Jinyang Zhong and Hantao Liu and Can Pan},
  doi          = {10.1049/ipr2.70068},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70068},
  shortjournal = {IET Image Process.},
  title        = {Underwater image quality evaluation: A comprehensive review},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dynamic range compression method for panchromatic images based on detail regions gradient preservation. <em>IETIP</em>, <em>19</em>(1), e70067. (<a href='https://doi.org/10.1049/ipr2.70067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective panchromatic remote sensing image dynamic range compression method is proposed to solve the issue in existing panchromatic remote sensing image grayscale conversion algorithms, which tend to cause overexposure in certain areas or overall excessive darkness. This method employs an empirical approach of detail region extraction and optimal parameter selection based on gradient to achieve dynamic range compression. First, a novel adaptive detail segmentation method based on the expansion of detail points within image blocks is introduced. Second, a detail optimisation module is established based on local detail preservation, which optimises the extraction of detail regions using gradient-based Otsu segmentation results and improved CLAHE-gradient-based Otsu segmentation results. Then, candidate adaptive dynamic range compression coefficients are determined based on the extracted detail layers, and the optimal adaptive dynamic range compression parameters are selected based on the high gradient proportion of the detail regions. Simulation experiments are conducted on multiple panchromatic remote sensing images with different scenes using the proposed method, and the effects of various dynamic range compression methods are evaluated based on multiple metrics. The results indicate that the proposed dynamic range compression method demonstrates excellent performance.},
  archive      = {J_IETIP},
  author       = {Peng Zhang and Qiang Xu and Yuwei Zhai and Tao Guo and Jiale Wang and Jinlong Xie},
  doi          = {10.1049/ipr2.70067},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70067},
  shortjournal = {IET Image Process.},
  title        = {Adaptive dynamic range compression method for panchromatic images based on detail regions gradient preservation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFF-YOLOv8: Small object detection based on multi-scale feature fusion for UAV remote sensing images. <em>IETIP</em>, <em>19</em>(1), e70066. (<a href='https://doi.org/10.1049/ipr2.70066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular task in drone-captured scenes, object detection involves images with a large number of small objects, but current networks often suffer missed and false detections. To address this problem, we propose a YOLO algorithm MFF-YOLOv8 based on multi-scale feature fusion for small target detection in UAV aerial images. First, a high-fesolution feature fusion pyramid (HFFP) is designed, which utilizes high-resolution feature maps containing much information about small objects to guide the feature fusion module, weighting and fusing feature maps to enhance the network's ability to represent small targets. Meanwhile, a reconstruction feature selection (RFS) module is employed to remove the large amounts of noise produced by high-resolution feature maps. Second, a hybrid efficient multi-scale attention (HEMA) mechanism is designed in the backbone network to maximize the retention and extraction of feature information related to small objects while simultaneously suppressing background noise interference. Finally, an Inner-Wise IoU loss function (Inner-WIoU) is designed for joint auxiliary bounding box and dynamic focal bounding box regression, which enhances the accuracy of network regression results, thus improving the detection precision of the model for small objects. MFF-YOLOv8 was experimented on the VisDrone2019 dataset, achieving a 47.9% mAP50, 9.3% up compared with that of the baseline network YOLOv8s. Also, in order to verify the generalization of the overall network, it was evaluated on the DOTA and UAVDT datasets, and the mAP50 was improved by 3.7% and 1.8%, respectively. The results demonstrate that MFF-YOLOv8 significantly enhances detection precision for small objects in UAV aerial scenes.},
  archive      = {J_IETIP},
  author       = {Kun Hu and Jinzheng Lu and Chaoquan Zheng and Qiang Xiang and Ling Miao},
  doi          = {10.1049/ipr2.70066},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70066},
  shortjournal = {IET Image Process.},
  title        = {MFF-YOLOv8: Small object detection based on multi-scale feature fusion for UAV remote sensing images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LC-YOLO: An improved YOLOv8-based lane detection model for enhanced lane intrusion detection. <em>IETIP</em>, <em>19</em>(1), e70065. (<a href='https://doi.org/10.1049/ipr2.70065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lane intrusion detection is an essential component of road safety, as vehicles crossing into lanes without proper signalling can lead to accidents, congestion and traffic violations. In order to overcome these challenges, it has become critical for the future autonomous vehicles and ADAS to possess a precise and reliable lane detection technique which could then further monitor the lane violation in real-time. However, lane detection is still challenging due to variants in lighting conditions, obstructions and weak markers. This research paper proposes a new YOLOv8 architecture for lane detection and traffic monitoring systems. The modifications considered in the paper are the addition of the large separable kernel attention (LSKA) module and the coordinate attention (CA) mechanism, which enhance the model's feature extraction and its performance in various real-world scenarios. Furthermore, a new lane intrusion detection (LID) algorithm was created which effectively distinguishes between actual lane intrusions forbidden ones (e.g., crossing solid lane lines) and permissible ones (e.g., crossing dashed lane lines), a crucial aspect for traffic management. The model was successfully tested by transferring the data which was personally recorded on Chinese highways and that show its function in a real environment. The model was tested using a custom dataset which included videos taken on Chinese highways, demonstrating its ability to work under real-world conditions. In this way, the results show that the proposed YOLOv8 model improves the accuracy and reliability of the lane detection tasks, with the model achieving a mAP of 97.9%, which will be useful and a significant advancement in the application of AI to public safety and highlights the critical role of state-of-the-art deep learning algorithms for enhancing road safety and traffic control.},
  archive      = {J_IETIP},
  author       = {Abdulkareem Abdullah and Guo Ling and Mohammed Al-Soswa and Ali Desbi},
  doi          = {10.1049/ipr2.70065},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70065},
  shortjournal = {IET Image Process.},
  title        = {LC-YOLO: An improved YOLOv8-based lane detection model for enhanced lane intrusion detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving biomedical image pattern identification by deep b4-GraftingNet: Application to pneumonia detection. <em>IETIP</em>, <em>19</em>(1), e70064. (<a href='https://doi.org/10.1049/ipr2.70064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VGG-16 and Inception are widely used CNN architectures for image classification, but they face challenges in target categorization. This study introduces B4-GraftingNet, a novel deep learning model that integrates VGG-16's hierarchical feature extraction with Inception's diversified receptive field strategy. The model is trained on the OCT-CXR dataset and evaluated on the NIH-CXR dataset to ensure robust generalization. Unlike conventional approaches, B4-GraftingNet incorporates binary particle swarm optimization (BPSO) for feature selection and grad-CAM for interpretability. Additionally, deep feature extraction is performed, and multiple machine learning classifiers (SVM, KNN, random forest, naïve Bayes) are evaluated to determine the optimal feature representation. The model achieves 94.01% accuracy, 94.22% sensitivity, 93.36% specificity, and 95.18% F1-score on OCT-CXR and maintains 87.34% accuracy on NIH-CXR despite not being trained on it. These results highlight the model's superior classification performance, feature adaptability, and potential for real-world deployment in both medical and general image classification tasks.},
  archive      = {J_IETIP},
  author       = {Syed Adil Hussain Shah and Syed Taimoor Hussain Shah and Abdul Muiz Fayyaz and Syed Baqir Hussain Shah and Mussarat Yasmin and Mudassar Raza and Angelo Di Terlizzi and Marco Agostino Deriu},
  doi          = {10.1049/ipr2.70064},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70064},
  shortjournal = {IET Image Process.},
  title        = {Improving biomedical image pattern identification by deep b4-GraftingNet: Application to pneumonia detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the diagnosis method of pancreatic lesions by endoscopic ultrasound based on twin network structure. <em>IETIP</em>, <em>19</em>(1), e70063. (<a href='https://doi.org/10.1049/ipr2.70063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To tackle the endoscopic ultrasound (EUS) pancreatic visual information, we propose a novel twin diagnostic network architecture (TDN) which consists of two identical feature extraction network structures. Model 1 is used to distinguish the categories of pancreatic visual information. Model 2 distinguishes whether there is cancerous information in pancreatic visual information. If cancerous information is included, gradient-weighted class activation mapping (Grad-CAM) is employed to calculate the activation heat map of visual information to present the specific location of the cancerous area in the visual information. To effectively integrate detailed texture information with abstract semantic information, we find the optimal proportion relationship required for feature fusion in each stage output feature vector dimension. The experimental results show that the classification accuracy of the TDN network can reach 98.344% for the pancreatic part and 99.471% for the specific part of the pancreas whether canceration occurs.},
  archive      = {J_IETIP},
  author       = {Xiao Xin and Huang Danping and Hu Shanshan and Shen Yang},
  doi          = {10.1049/ipr2.70063},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70063},
  shortjournal = {IET Image Process.},
  title        = {Research on the diagnosis method of pancreatic lesions by endoscopic ultrasound based on twin network structure},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on camouflaged object segmentation based on feature fusion and attention mechanism. <em>IETIP</em>, <em>19</em>(1), e70062. (<a href='https://doi.org/10.1049/ipr2.70062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to detect objects that ‘blend in’ with their surroundings and the lack of a clear boundary between the target object and the background in COD tasks makes accurate detection of targets difficult. Although many innovative algorithms and methods have been developed to improve the results of camouflaged object detection, the problem of poor detection accuracy in complex scenes still exists. To improve the accuracy of camouflage target segmentation, a camouflaged object detection algorithm using contextual feature enhancement and an attention mechanism called amplify and predict network (APNet) is proposed. In this paper, context feature enhancement module (CFEM) and reverse attention prediction module (RAPM) are designed.CFEM can accept multi-level features extracted from the backbone network, and convey the features with enhancement processing to achieve the fusion of multi-level features.RAPM focuses on the edge feature information through the reverse attention mechanism to mine deeper camouflaged target information to achieve and further refine the predicted results. The proposed algorithm achieves weighted F-measure and mean absolute error (MAE) of 0.708 and 0.033 on the COD10K dataset, respectively, and the experimental results on other publicly available datasets are also significantly better than the other 14 state-of-the-art models, and achieves the optimal performance on the four objective evaluation metrics, and the proposed algorithm obtains sharper edge details on COD tasks and improves the prediction performance.},
  archive      = {J_IETIP},
  author       = {Yixuan Wang and Jingke Yan},
  doi          = {10.1049/ipr2.70062},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70062},
  shortjournal = {IET Image Process.},
  title        = {Research on camouflaged object segmentation based on feature fusion and attention mechanism},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved object detection algorithm for UAV images based on orthogonal channel attention mechanism and triple feature encoder. <em>IETIP</em>, <em>19</em>(1), e70061. (<a href='https://doi.org/10.1049/ipr2.70061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in Unmanned Aerial Vehicle (UAV) imagery plays an important role in many fields. However, UAV images usually exhibit characteristics different from those of natural images, such as complex scenes, dense small targets, and significant variations in target scales, which pose considerable challenges for object detection tasks. To address these issues, this paper presents a novel object detection algorithm for UAV images based on YOLOv8 (referred to as OATF-YOLO). First, an orthogonal channel attention mechanism is added to the backbone network to imporve the algorithm's ability to extract features and clear up any confusion between features in the foreground and background. Second, a triple feature encoder and a scale sequence feature fusion module are integrated into the neck network to bolster the algorithm's multi-scale feature fusion capability, thereby mitigating the impact of substantial differences in target scales. Finally, an inner factor is introduced into the loss function to further upgrade the robustness and detection accuracy of the algorithm. Experimental results on the VisDrone2019-DET dataset indicate that the proposed algorithm significantly outperforms the baseline model. On the validation set, the OATF-YOLO algorithm achieves a precision of 59.1%, a recall of 40.5%, an mAP50 of 42.5%, and an mAP50:95 of 25.8%. These values represent improvements of 3.8%, 3.0%, 4.1%, and 3.3%, respectively. Similarly, on the test set, the OATF-YOLO algorithm achieves a precision of 52.3%, a recall of 34.7%, an mAP50 of 33.4%, and an mAP50:95 of 19.1%, reflecting enhancements of 4.0%, 3.3%, 4.0%, and 2.6%, respectively. To further validate the model's robustness and scalability, experiments are conducted on the NWPU-VHR10 dataset, and OATF-YOLO also achieves excellent performance. Furthermore, compared to several classical object detection algorithms, OATF-YOLO demonstrates superior detection performance on both datasets and indicates that it is better suited for UAV image object detection scenarios.},
  archive      = {J_IETIP},
  author       = {Wenfeng Wang and Chaomin Wang and Sheng Lei and Min Xie and Binbin Gui and Fang Dong},
  doi          = {10.1049/ipr2.70061},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70061},
  shortjournal = {IET Image Process.},
  title        = {An improved object detection algorithm for UAV images based on orthogonal channel attention mechanism and triple feature encoder},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TLEAR-net: A network for defect detection in train wheelset treads based on transfer learning and edge adaptive reinforcement attention. <em>IETIP</em>, <em>19</em>(1), e70060. (<a href='https://doi.org/10.1049/ipr2.70060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a critical load-bearing and running component of railway systems, the wheelset's operational safety fundamentally depends on precise detection and localisation of tread defects. Current deep learning-based detection methods face significant challenges in extracting discriminative edge features under small-sample conditions, leading to suboptimal defect localisation accuracy. To address these limitations, this study proposes TLEAR-Net, a novel defect detection framework integrating transfer learning with an edge-adaptive reinforcement attention mechanism. The methodology employs RetinaNet as the baseline architecture, enhanced through multi-stage domain adaptation using COCO 2017 pretraining and parameter-shared ResNet-50 backbone optimisation to bridge cross-domain feature discrepancies. An innovative edge-adaptive reinforcement (EAR) attention module is developed to selectively amplify defect boundary features through learnable gradient operators and hybrid spatial-channel attention mechanisms. Comprehensive evaluations on a proprietary data set annotated defect samples demonstrate the framework's superior performance, achieving state-of-the-art detection accuracy (89.22% mAP) while maintaining real-time processing capability (42.45 FPS).},
  archive      = {J_IETIP},
  author       = {Xinliang Hu and Jing He and Changfan Zhang and Xiang Cheng},
  doi          = {10.1049/ipr2.70060},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70060},
  shortjournal = {IET Image Process.},
  title        = {TLEAR-net: A network for defect detection in train wheelset treads based on transfer learning and edge adaptive reinforcement attention},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A retinex-based network for low-light image enhancement with multi-scale denoising and focal-aware reflections. <em>IETIP</em>, <em>19</em>(1), e70059. (<a href='https://doi.org/10.1049/ipr2.70059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement addresses critical challenges in computer vision, including insufficient brightness, excessive noise, and loss of detail in low-light images, thus improving the quality and applicability of image data for various vision tasks. We propose an unsupervised Retinex-based network for low-light image enhancement, incorporating a multi-scale denoiser and focal-aware reflections. Our approach begins with a multi-scale denoising network that removes noise and redundant features while preserving both global and local image details. Subsequently, we employ an illumination separation network and a focal-aware reflection network to extract the illumination and reflection components, respectively. To enhance the accuracy of the reflection component and capture finer image details, we introduce a depthwise convolutional focal modulation block. This block improves the representative capacity of the reflection component feature map. Finally, we adjust the illumination component and synthesize it with the reflection component to generate the enhanced image. Extensive experiments conducted on 9 datasets and 13 methods, using metrics such as SSIM, PSNR, LPIPS, NIQE, and BRISQUE, demonstrate that the proposed method outperforms existing unsupervised approaches and shows competitive performance when compared to supervised methods.},
  archive      = {J_IETIP},
  author       = {Peng Ji and Zhongyou Lv and Zhao Zhang},
  doi          = {10.1049/ipr2.70059},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70059},
  shortjournal = {IET Image Process.},
  title        = {A retinex-based network for low-light image enhancement with multi-scale denoising and focal-aware reflections},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A clustering-based color reordering method for reversible data hiding in palette images. <em>IETIP</em>, <em>19</em>(1), e70058. (<a href='https://doi.org/10.1049/ipr2.70058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent research work pointed out that the reversible data hiding algorithms proposed for gray-scale images can be implemented on the reconstructed palette images to improve embedding capacity and visual quality by reordering the color table. However, the reordering effect has a significant impact on performance improvement. Therefore, we propose a clustering-based color reordering method for reversible data hiding in palette images to improve the reordering effect and further enhance the performance. In this method, we first design a centroid initialization method to select the initial centroids and then exploit the K-means algorithm to generate clusters for the colors in the original color table. In the following, our proposed method, respectively, reorders the colors of these clusters by a greedy strategy and concatenates them into the reordered color table. Based on the relationship between the original and the reordered color tables, a novel index matrix can be reconstructed. Finally, state-of-the-art reversible data hiding algorithms can be implemented on the reconstructed index matrix for performance improvement. Since our proposed method improves the reordering effect, enhances the correlation of the reconstructed index matrix, and reduces the length of the encoded location map, the maximal embedding capacities and the visual quality under the fixed embedding capacities are improved. We conducted experiments on two image datasets and six standard images to verify that the performance improvement of our proposed reordering method is better than that of the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Jianxuan Deng and Yi Chen and Hongxia Wang and Chun Guo and Yunhe Cui and Guowei Shen},
  doi          = {10.1049/ipr2.70058},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70058},
  shortjournal = {IET Image Process.},
  title        = {A clustering-based color reordering method for reversible data hiding in palette images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coarse-grained ore distribution on conveyor belts with TRCU neural networks. <em>IETIP</em>, <em>19</em>(1), e70057. (<a href='https://doi.org/10.1049/ipr2.70057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The particle size distribution of ore is a key evaluation indicator of the degree of ore fragmentation and plays a key role in the separation of mineral processing. Traditional ore size detection is often done by manual sieving, which takes a great deal of time and labor. In this work, a deep learning network model (referred to as TRCU), combining Transformer with residual blocks and CBAM attention mechanism in an encoder-decoder structure was developed for particle size detection of medium and large particles in a wide range of particle sizes in an ore material transportation scenario. This model presents a unique approach to improve the accuracy of identifying ore regions in images, utilizing three key features. Firstly, the model utilizes the CBAM attention mechanism to increase the weighting of ore regions in the feature fusion channel; secondly, a Transformer module is used to enhance the correlation of features in coarse-grained ore image regions in the deepest encoding and decoding stages; finally, the residual module is used to enhance useful feature information and reduce noise. The validation experiments are conducted on a transport belt dataset with large variation in particle size and low contrast. The results show that the proposed model can capture the edges of different particle sizes and achieve accurate segmentation of large particle size ore images. The MIoU values of 82.44%, MPA of 90.21%, and accuracy of 94.91% are higher than those of other existing methods. This work proposes a reliable method for automated detection of mineral particle size and will promote the automation level of ore processing.},
  archive      = {J_IETIP},
  author       = {Weinong Liang and Xiaolu Sun and Yutao Li and Yang Liu and Guanghui Wang and Jincheng Wang and Chunxia Zhou},
  doi          = {10.1049/ipr2.70057},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70057},
  shortjournal = {IET Image Process.},
  title        = {Coarse-grained ore distribution on conveyor belts with TRCU neural networks},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EANet: Integrate edge features and attention mechanisms multi-scale networks for vessel segmentation in retinal images. <em>IETIP</em>, <em>19</em>(1), e70056. (<a href='https://doi.org/10.1049/ipr2.70056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately extracting blood vessel structures from retinal fundus images is critical for the early diagnosis and treatment of various ocular and systemic diseases. However, retinal vessel segmentation continues to face significant challenges. Firstly, capturing the boundary information of small vessels is particularly difficult. Secondly, uneven vessel thickness and irregular distribution further complicate the multi-scale feature modelling. Lastly, low-contrast images lead to increased background noise, further affecting the segmentation accuracy. To tackle these challenges, this article presents a multi-scale segmentation network that combines edge features and attention mechanisms, referred to as EANet. It demonstrates significant advantages over existing methods. Specifically, EANet consists of three key modules: the edge feature enhancement module, the multi-scale information interaction encoding module, and the multi-class attention mechanism decoding module. Experimental results validate the effectiveness of the method. Specifically, EANet outperforms existing advanced methods in the precise segmentation of small and multi-scale vessels and in effectively filtering background noise to maintain segmentation continuity.},
  archive      = {J_IETIP},
  author       = {Jiangyi Zhang and Yuxin Tan and Duantengchuan Li and Guanghui Xu and Fuling Zhou},
  doi          = {10.1049/ipr2.70056},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70056},
  shortjournal = {IET Image Process.},
  title        = {EANet: Integrate edge features and attention mechanisms multi-scale networks for vessel segmentation in retinal images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-shadow scenarios tennis ball detection by an improved RTMdet-light model. <em>IETIP</em>, <em>19</em>(1), e70054. (<a href='https://doi.org/10.1049/ipr2.70054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time and rapid recording of sport sensor data related to tennis ball trajectories facilitates the analysis of this information and the development of intelligent training regimes. However, there are three essential challenges in the task of tennis ball recognition using sport vision sensors: the small size of the ball, its high speed, and the complex match scenarios. As a result, this paper considers a lightweight object detection model named improved RTMDet-light to deal with these challenges. Specifically, it has compatible capacities in the backbone and neck, constructed by a basic building block that consists of large-kernel depth-wise convolutions. Furthermore, GhosNet and ShuffleNet are used to replace the CSPLayers which reduce the parameters of our model. The lightweight model proposed addresses the inherent challenges of detecting small objects and muti scenarios in the match. After training, the proposed model performed better on four scenarios with different shades of tennis ball match, with results visualized through heatmaps and performance metrics tabulated for detailed analysis. The recall, FLOPs and number of parameters of the improved RTMDet-light are 71.4%, 12.543G, and 4.874M, respectively. The results demonstrate robustness and effectiveness of our model in accurate tennis ball detecting across various scales. In conclusion, our model for real-time detection in tennis ball detection offers a lightweight and faster solution for sport sensors.},
  archive      = {J_IETIP},
  author       = {Yukun Zhu and Yanxia Peng and Cong Yu},
  doi          = {10.1049/ipr2.70054},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70054},
  shortjournal = {IET Image Process.},
  title        = {Multi-shadow scenarios tennis ball detection by an improved RTMdet-light model},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human behaviour recognition method based on SME-net. <em>IETIP</em>, <em>19</em>(1), e70053. (<a href='https://doi.org/10.1049/ipr2.70053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal, motion and channel information are pivotal in video-based behaviour recognition. Traditional 2D CNNs demonstrate low computational complexity but fail to capture temporal dynamics effectively. Conversely, 3D CNNs excel in recognising temporal patterns but at the cost of significantly higher computational demands. To address these challenges, we propose a generic and effective SME module composed of three parallel sub-modules, namely Spatio-Temporal Excitation (STE), Motion Excitation (ME) and Efficient Channel Excitation (ECE). Specifically, the STE module enhances the spatiotemporal representation using a single-channel 3D convolution, enabling the model to focus on both temporal and spatial features. The ME module emphasises motion-sensitive channels by calculating feature map differences at adjacent time steps, guiding the model toward motion-centric regions. The ECE module efficiently captures cross-channel interactions without dimensionality reduction, ensuring robust performance while significantly reducing model complexity. Pre-trained on the ImageNet dataset, the proposed method achieved Top-1 accuracy of 49.0% on the Something-Something V1 (Sth-Sth V1) dataset and 40.8% on the Diving48 dataset. Extensive ablation studies and comparative experiments further demonstrate that the proposed method strikes an optimal balance between recognition accuracy and computational efficiency.},
  archive      = {J_IETIP},
  author       = {Ruimin Li and Yajuan Jia and Dan Yao and Fuquan Pan},
  doi          = {10.1049/ipr2.70053},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70053},
  shortjournal = {IET Image Process.},
  title        = {Human behaviour recognition method based on SME-net},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-fatigue feature selection and fuzzy logic-based intelligent driver drowsiness detection. <em>IETIP</em>, <em>19</em>(1), e70052. (<a href='https://doi.org/10.1049/ipr2.70052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driver drowsiness poses a critical threat, frequently resulting in highly perilous traffic accidents. The drowsiness detection is complicated by various challenges such as lighting conditions, occluded facial features, eyeglasses, and false alarms, making the accuracy, robustness across environments, and computational efficiency a major challenge. This study proposes a non-intrusive driver drowsiness detection system, leveraging image processing techniques and advanced fuzzy logic methods. It also introduces improvements to the Viola-Jones algorithm for swift and precise driver face, eye, and mouth identification. Extensive experiments involving diverse individuals and scenarios were conducted to assess the system's performance in detecting eye and mouth states. The results are highly promising, with eye detection accuracy at 91.8% and mouth detection achieving a remarkable 94.6%, surpassing existing methods. Real-time testing in varied conditions, including day and night scenarios and subjects with and without glasses, demonstrated the system's robustness, yielding a 97.5% test accuracy in driver drowsiness detection.},
  archive      = {J_IETIP},
  author       = {Mohan Arava and Divya Meena Sundaram},
  doi          = {10.1049/ipr2.70052},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70052},
  shortjournal = {IET Image Process.},
  title        = {Multi-fatigue feature selection and fuzzy logic-based intelligent driver drowsiness detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LCM-YOLO: A small object detection method for UAV imagery based on YOLOv5. <em>IETIP</em>, <em>19</em>(1), e70051. (<a href='https://doi.org/10.1049/ipr2.70051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the challenges of detecting small targets and targets with significant scale variations in UAV aerial images. We propose an improved YOLOv5 model, named LCM-YOLO, to tackle these challenges. Initially, a local fusion mechanism is introduced into the C3 module, forming the C3-LFM module to enhance feature information acquisition during feature extraction. Subsequently, the CCFM is employed as the neck structure of the network, leveraging its lightweight convolution and cross-scale feature fusion characteristics to effectively improve the model's ability to integrate target features at different levels, thereby enhancing its adaptability to scale variations and detection performance for small targets. Additionally, a multi-head attention mechanism is integrated at the front end of the detection head, allowing the model to focus more on the detailed information of small targets through weight distribution. Experiments on the VisDrone2019 dataset show that LCM-YOLO has excellent detection capabilities. Compared to the original YOLOv5 model, its mAP50 and mAP50-95 metrics are improved by 7.2% and 5.1%, respectively, reaching 40.7% and 22.5%. This validates the effectiveness of the LCM-YOLO model for detecting small and multi-scale targets in complex backgrounds.},
  archive      = {J_IETIP},
  author       = {Shaodong Liu and Faming Shao and Weijun Chu and Heng Zhang and Dewei Zhao and Jinhong Xue and Qing Liu},
  doi          = {10.1049/ipr2.70051},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70051},
  shortjournal = {IET Image Process.},
  title        = {LCM-YOLO: A small object detection method for UAV imagery based on YOLOv5},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMFNet: A three-stage feature matching network with geometric consistency and attentional enhancement. <em>IETIP</em>, <em>19</em>(1), e70050. (<a href='https://doi.org/10.1049/ipr2.70050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current feature matching methods typically employ a two-stage process, consisting of coarse and fine matching. However, the transition from the coarse to the fine stage often lacks an effective intermediate state, leading to abrupt changes in the matching process. This can hinder smooth transitions and precise localization. To address these limitations, this study introduces Coarse-Mid-Fine Match Net (CMFNet), a novel three-stage image feature matching method. CMFNet incorporates an intermediate-grained matching phase between the coarse and fine stages to facilitate a more gradual and seamless transition. In the proposed method, the intermediate-grained matching refines the correspondences obtained from the coarse-grained stage using Adaptive-random sample consensus (RANSAC). Subsequently, the midtransformer, which integrates sparse self-attention (SSA) mechanisms with local-feature-based cross-attention, is employed for feature extraction. This approach enhances the feature extraction capabilities and improves the adaptability to various types of image data, thereby boosting overall matching performance. Additionally, a cross-attention mechanism based on local region features is introduced. The network undergoes fully self-supervised training, aiming to minimize a match loss that is autonomously generated from the training data using a multi-scale cross-entropy method. A series of thorough experiments was carried out on diverse real-world datasets, including both unaltered and extensively processed images.The results demonstrate that the proposed method outperforms state-of-the-art approaches, achieving 0.776 mAUC on the HPatches dataset and 0.442 mAUC on the ISC-HE dataset.},
  archive      = {J_IETIP},
  author       = {RenKai Xiao and ShengZhi Yuan and Kai Jin and Min Li and Yan Tang and Sen Shen},
  doi          = {10.1049/ipr2.70050},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70050},
  shortjournal = {IET Image Process.},
  title        = {CMFNet: A three-stage feature matching network with geometric consistency and attentional enhancement},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Depth completion and inpainting for specular objects. <em>IETIP</em>, <em>19</em>(1), e70049. (<a href='https://doi.org/10.1049/ipr2.70049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth images or point clouds offer true three-dimensional insights into scene geometry, making depth perception essential for downstream tasks in computer vision. However, current commercial depth sensors often produce dense estimations with lower accuracy, especially on specular surfaces, leading to noisy and incomplete data. To address this challenge, we propose a novel framework based on latent diffusion models conditioned on RGBD images and semantic labels for depth completion and inpainting, effectively restoring depth values for both visible and occluded parts of specular objects. We enhance geometric guidance by designing various visual descriptors as conditions and introduce channel and spatial attention mechanisms in the conditional encoder to improve multi-modal feature fusion. Using the MP6D dataset, we render complete and dense depth images for benchmarking, enabling a comprehensive evaluation of our method against existing approaches. Extensive experiments demonstrate that our model outperforms previous methods, significantly improving the performance of downstream tasks by incorporating the predicted depth maps restored by our model.},
  archive      = {J_IETIP},
  author       = {He Liu and Yi Sun},
  doi          = {10.1049/ipr2.70049},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70049},
  shortjournal = {IET Image Process.},
  title        = {Depth completion and inpainting for specular objects},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frame extraction person retrieval framework based on improved YOLOv8s and the stage-wise clustering person re-identification. <em>IETIP</em>, <em>19</em>(1), e70046. (<a href='https://doi.org/10.1049/ipr2.70046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID), a crucial research area in smart city security, faces challenges due to person posture changes, object occlusion and other factors, making it difficult for existing methods to accurately retrieving target person in video surveillance. To resolve this problem, we propose a person retrieval framework that integrates YOLOv8s and person Re-ID. Improved YOLOv8s is employed to extract person categories from the video on a frame-by-frame basis, and when combined with the stage-wise clustering person Re-ID network (SCPN), it enables collaborative person retrieval across multiple cameras. Notably, a feature precision (FP) module is added in the YOLOv8s network to form FP-YOLOv8s, and SCPN incorporates innovative enhancements including the stage-wise learning rate scheduler, centralized clustering loss and adaptive representation joint attention module into the person Re-ID baseline model. Comprehensive experiments on COCO, Market-1501 and DukeMTMC-ReID datasets demonstrate that our proposed framework outperforms several other leading methods. Given the scarcity of image-video person Re-ID datasets, we also provide an extended image-video person (EIVP) dataset, which contains 102 videos and 814 bounding boxes of 57 identities captured by 8 cameras. The video reasoning detection score of this framework reaches 78.8% on this dataset, indicating a 3.2% increase compared to conventional models.},
  archive      = {J_IETIP},
  author       = {Jianjun Zhuang and Nan Wang and Yuchen Zhuang and Yong Hao},
  doi          = {10.1049/ipr2.70046},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70046},
  shortjournal = {IET Image Process.},
  title        = {Frame extraction person retrieval framework based on improved YOLOv8s and the stage-wise clustering person re-identification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic uncertainty-awared for semantic segmentation of remote sensing images. <em>IETIP</em>, <em>19</em>(1), e70045. (<a href='https://doi.org/10.1049/ipr2.70045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image segmentation is crucial for applications ranging from urban planning to environmental monitoring. However, traditional approaches struggle with the unique challenges of aerial imagery, including complex boundary delineation and intricate spatial relationships. To address these limitations, we introduce the semantic uncertainty-aware segmentation (SUAS) method, an innovative plug-and-play solution designed specifically for remote sensing image analysis. SUAS builds upon the rotated multi-scale interaction network (RMSIN) architecture and introduces the prompt refinement and uncertainty adjustment module (PRUAM). This novel component transforms original textual prompts into semantic uncertainty-aware descriptions, particularly focusing on the ambiguous boundaries prevalent in remote sensing imagery. By incorporating semantic uncertainty, SUAS directly tackles the inherent complexities in boundary delineation, enabling more refined segmentations. Experimental results demonstrate SUAS's effectiveness, showing improvements over existing methods across multiple metrics. SUAS achieves consistent enhancements in mean intersection-over-union (mIoU) and precision at various thresholds, with notable performance in handling objects with irregular and complex boundaries—a persistent challenge in aerial imagery analysis. The results indicate that SUAS's plug-and-play design, which leverages semantic uncertainty to guide the segmentation task, contributes to improved boundary delineation accuracy in remote sensing image analysis.},
  archive      = {J_IETIP},
  author       = {Xiangfeng Qiu and Zhilin Zhang and Xin Luo and Xiang Zhang and Youcheng Yang and Yundong Wu and Jinhe Su},
  doi          = {10.1049/ipr2.70045},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70045},
  shortjournal = {IET Image Process.},
  title        = {Semantic uncertainty-awared for semantic segmentation of remote sensing images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tri-plane dynamic neural radiance fields for high-fidelity talking portrait synthesis. <em>IETIP</em>, <em>19</em>(1), e70044. (<a href='https://doi.org/10.1049/ipr2.70044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural radiation field (NeRF) has been widely used in the field of talking portrait synthesis. However, the inadequate utilisation of audio information and spatial position leads to the inability to generate images with high audio-lip consistency and realism. This paper proposes a novel tri-plane dynamic neural radiation field (Tri-NeRF) that employs an implicit radiation field to study the impacts of audio on facial movements. Specifically, Tri-NeRF propose tri-plane offset network (TPO-Net) to offset spatial positions in three 2D planes guided by audio. This allows for sufficient learning of audio features from image features in a low dimensional state to generate more accurate lip movements. In order to better preserve facial texture details, we innovatively propose a new gated attention fusion module (GAF) to dynamically fuse features based on strong and weak correlation of cross-modal features. Extensive experiments have demonstrated that Tri-NeRF can generate talking portraits with audio-lip consistency and realism.},
  archive      = {J_IETIP},
  author       = {Xueping Wang and Xueni Guo and Jun Xu and Yuchen Wu and Feihu Yan and Guangzhe Zhao},
  doi          = {10.1049/ipr2.70044},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70044},
  shortjournal = {IET Image Process.},
  title        = {Tri-plane dynamic neural radiance fields for high-fidelity talking portrait synthesis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An unsupervised image enhancement method based on adaptation region divisions. <em>IETIP</em>, <em>19</em>(1), e70043. (<a href='https://doi.org/10.1049/ipr2.70043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel image enhancement method that integrates traditional image processing techniques with deep learning frameworks. Initially, images are transformed from the red, green and blue (RGB) color space to the Lab color space, and the luminance component (L) is extracted to quantify texture. Subsequently, texture complexity is assessed using features derived from the gray-level co-occurrence matrix (GLCM), including contrast, correlation, homogeneity, and energy. These features are weighted to compute an overall texture complexity score, which facilitates the segmentation of the image into distinct regions. Regions characterized by simple textures are aggregated into larger segments, whereas regions with complex textures are subdivided into smaller segments. Following segmentation, histogram equalization is applied along with noise reduction and image enhancement via a convolutional autoencoder model. The model extracts relevant features and reduces dimensionality in the encoder phase, and reconstructs the image through the decoder. This methodology effectively preserves semantic information and enhances image clarity. Some experiments are conducted using the ExDark dataset, which comprises twelve categories, and the enhancement results are quantitatively evaluated using image quality metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), learned perceptual image patch similarity (LPIPS), and neural image quality evaluator (NIQE). Experimental results demonstrate that the proposed method significantly surpasses existing enhancement techniques in terms of image quality and visual perception, thereby affirming its efficacy in improving the visual quality and detail of low-light images. The implementation code will be made publicly available at: https://github.com/Winnie0320/Image-Enhancement-Method .},
  archive      = {J_IETIP},
  author       = {Kaijun Zhou and Weiyi Yuan and Yemei Qin},
  doi          = {10.1049/ipr2.70043},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e70043},
  shortjournal = {IET Image Process.},
  title        = {An unsupervised image enhancement method based on adaptation region divisions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A face quality assessment system for unattended face recognition: Design and implementation. <em>IETIP</em>, <em>19</em>(1), e70042. (<a href='https://doi.org/10.1049/ipr2.70042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a face quality assessment approach that selects the highest-quality face image using a two-stage process from video streaming. In high-traffic environments, traditional face recognition methods can cause crowd congestion, emphasizing the need for unconscious face recognition, which requires no active cooperation from individuals. Due to the nature of unconscious face recognition, it is necessary to capture high-quality face images. In this paper, the FSA-Net head pose estimation network is enhanced to FSA-Shared_Nadam by replacing the Adam optimizer with Nadam and improving stage fusion. In the first stage, FSA-Shared_Nadam estimates head pose angles, MediaPipe detects facial landmarks to calculate eye distance and aspect ratios, and sharpness is calculated using the Laplacian operator. Images are considered valid if they meet the criteria. A model trains a face quality scoring formula, learning how different head pose angles affect face recognition accuracy. In the second stage, face images are clustered, and the formula is applied to select the highest-scoring face within each cluster. The approach was tested across multiple datasets, and a simulated security checkpoint scenario was created for practical testing. The results demonstrate the effectiveness of the FSA-Shared_Nadam head pose estimation algorithm and the proposed face quality assessment approach.},
  archive      = {J_IETIP},
  author       = {Dunli Hu and Xin Bi and Wei Zhao and Xiaoping Zhang and Xingchen Duan},
  doi          = {10.1049/ipr2.70042},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70042},
  shortjournal = {IET Image Process.},
  title        = {A face quality assessment system for unattended face recognition: Design and implementation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retinal fundus image enhancement with detail highlighting and brightness equalizing based on image decomposition. <em>IETIP</em>, <em>19</em>(1), e70041. (<a href='https://doi.org/10.1049/ipr2.70041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality retinal fundus images are widely used by ophthalmologists for the detection and diagnosis of eye diseases, diabetes, and hypertension. However, in retinal fundus imaging, the reduction in image quality, characterized by poor local contrast and non-uniform brightness, is inevitable. Image enhancement becomes an essential and practical strategy to address these issues. In this paper, we propose a retinal fundus image enhancement method that emphasizes details and equalizes brightness, based on image decomposition. First, the original image is decomposed into three layers using an edge-preserving filter: a base layer, a detail layer, and a noise layer. Second, an adaptive local power-law approach is applied to the base layer for brightness equalization, while detail enhancement is achieved for the detail layer through saliency analysis and blue channel removal. Finally, the base and detail layers are combined, excluding the noise layer, to synthesize the final image. The proposed method is evaluated and compared with both classical and recent approaches using two widely adopted datasets. According to the experimental results, both subjective and objective assessments demonstrate that the proposed method effectively enhances retinal fundus images by highlighting details, equalizing brightness, and suppressing noise and artifacts, all without causing color distortion.},
  archive      = {J_IETIP},
  author       = {Zhiyi Wu and Lucy J. Kessler and Xiang Chen and Yiguo Pan and Xiaoxia Yang and Ling Zhao and Jufeng Zhao and Gerd U. Auffarth},
  doi          = {10.1049/ipr2.70041},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70041},
  shortjournal = {IET Image Process.},
  title        = {Retinal fundus image enhancement with detail highlighting and brightness equalizing based on image decomposition},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian process-driven semi-supervised single-image rain removal: Enhancing real-scene generalizability. <em>IETIP</em>, <em>19</em>(1), e70040. (<a href='https://doi.org/10.1049/ipr2.70040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a semi-supervised single-image rain removal method using Gaussian processes to decouple rain components and background features. Existing methods often fail to generalize to real scenes due to synthetic data's limited diversity in rain direction and density. To address this, we integrate synthetic and real rainy images, where Gaussian processes model synthetic intermediate features to generate pseudo-labels for real image supervision. A two-stage encoder–decoder architecture with squeeze-and-excitation residual and context feature fusion modules enhances feature disentanglement. Experimental results on both synthetic and real datasets demonstrate superior performance, achieving a peak signal-to-noise ratio of 26.11 dB and structural similarity of 0.89 on synthetic images, while preserving more background details and effectively supporting downstream tasks like object segmentation.},
  archive      = {J_IETIP},
  author       = {Lisha Liu and Peiquan Xiong and Fei Liu},
  doi          = {10.1049/ipr2.70040},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70040},
  shortjournal = {IET Image Process.},
  title        = {Gaussian process-driven semi-supervised single-image rain removal: Enhancing real-scene generalizability},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial blemish detection based on YOLOv8n optimised with space-to-depth and GCNet attention mechanisms. <em>IETIP</em>, <em>19</em>(1), e70039. (<a href='https://doi.org/10.1049/ipr2.70039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial blemishes are small and often similar in colour to the surrounding skin, making detection even more challenging. This paper proposes an improved algorithm based on YOLOv8 to address the limitations of the original YOLOv8n in facial blemish detection. First, we introduce space-to-depth-convolution (SPD-Conv), which replaces traditional downsampling methods in convolutional neural networks, preserving spatial details without reducing the image resolution. This enhances the model's ability to detect small imperfections. Additionally, the integration of GCNet helps detect blemishes that closely resemble surrounding skin tones by leveraging global context modelling. The improved model better understands the overall structure and features of the face. Experimental results show that our model achieves a 5.3% and 5.6% improvement in mAP50 and mAP50-95, respectively, over YOLOv8n. Furthermore, it outperforms the latest YOLOv11n model by 6.9% and 7.2% in mAP50 and mAP50-95.},
  archive      = {J_IETIP},
  author       = {Shuxi Zhou and Lijun Liang},
  doi          = {10.1049/ipr2.70039},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70039},
  shortjournal = {IET Image Process.},
  title        = {Facial blemish detection based on YOLOv8n optimised with space-to-depth and GCNet attention mechanisms},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved few-shot object detection method based on faster R-CNN. <em>IETIP</em>, <em>19</em>(1), e70038. (<a href='https://doi.org/10.1049/ipr2.70038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uneven distribution of object features and insufficient feature learning significantly affect the accuracy and generalizability of existing detection methods. This paper proposes an improved two-stage few-shot object detection method that builds upon the faster region-based convolutional neural network framework to enhance its performance in detecting objects with limited training data. First, a modified data augmentation method for optical images is introduced, and a Gaussian optimization module of sample feature distribution is constructed to enhance the model's generalizability. Second, a parameter-less 3D space attention module without additional parameters, is added to enhance the space features of a sample, where a neuron linear separability measurement and feature optimization module based on mathematical operations are used to adjust the feature distribution and reduce data distribution bias. Finally, a class feature vector extractor based on meta-learning is provided to reconstruct the feature map by overlaying a class feature vector from the target domain onto the query image. This process improves accuracy and generalization performance, and multiple experiments on the PASCAL VOC dataset show that the proposed method has higher detection accuracy and stronger generalizability than other methods. Especially, the experiment using practical images under complicated environments indicates its potential effectiveness in real-world scenarios.},
  archive      = {J_IETIP},
  author       = {YangJie Wei and Shangwei Long and Yutong Wang},
  doi          = {10.1049/ipr2.70038},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70038},
  shortjournal = {IET Image Process.},
  title        = {Improved few-shot object detection method based on faster R-CNN},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater image enhancement based on depth and light attenuation estimation. <em>IETIP</em>, <em>19</em>(1), e70037. (<a href='https://doi.org/10.1049/ipr2.70037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light attenuation and complex water environments seriously deteriorate underwater imaging quality. Current underwater image restoration algorithms cannot handle low-quality colour-distorted images in aquatic environments. This study proposed a novel underwater image processing algorithm based on a light attenuation estimation model and a depth estimation network. First, a pseudo-depth map strategy was proposed to train the underwater image depth estimation network to realise underwater image depth estimation. Second, the attenuation coefficient of the current image was estimated based on the background light using a light attenuation model. Finally, the images were restored using an underwater imaging model. The proposed algorithm is superior to state-of-the-art underwater image processing algorithms regarding subjective and objective qualities.},
  archive      = {J_IETIP},
  author       = {Lianjun Zhang and Tingna Liu and Qichao Shi and Fen Chen},
  doi          = {10.1049/ipr2.70037},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70037},
  shortjournal = {IET Image Process.},
  title        = {Underwater image enhancement based on depth and light attenuation estimation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale frequency enhancement network for blind image deblurring. <em>IETIP</em>, <em>19</em>(1), e70036. (<a href='https://doi.org/10.1049/ipr2.70036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deblurring is a fundamental preprocessing technique aimed at recovering clear and detailed images from blurry inputs. However, existing methods often struggle to effectively integrate multi-scale feature extraction with frequency enhancement, limiting their ability to reconstruct fine textures, especially in the presence of non-uniform blur. To address these challenges, we propose a multi-scale frequency enhancement network (MFENet) for blind image deblurring. MFENet introduces a multi-scale feature extraction module (MS-FE) based on depth-wise separable convolutions to capture rich multi-scale spatial and channel information. Furthermore, the proposed method employs a frequency enhanced blur perception module (FEBP) that utilizes wavelet transforms to extract high-frequency details and multi-strip pooling to perceive non-uniform blur. Experimental results on the GoPro and HIDE datasets demonstrate that our method achieves superior deblurring performance in both visual quality and objective evaluation metrics. Notably, in downstream object detection tasks, our blind image deblurring algorithm significantly improves detection accuracy, further validating its effectiveness and robustness in practical applications.},
  archive      = {J_IETIP},
  author       = {YaWen Xiang and Heng Zhou and Xi Zhang and ChengYang Li and ZhongBo Li and YongQiang Xie},
  doi          = {10.1049/ipr2.70036},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70036},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale frequency enhancement network for blind image deblurring},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing self-supervised monocular depth estimation in endoscopy via feature-based perceptual loss. <em>IETIP</em>, <em>19</em>(1), e70035. (<a href='https://doi.org/10.1049/ipr2.70035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, self-supervised learning methods for monocular depth estimation have garnered significant attention due to their ability to learn from large amounts of unlabelled data. In this study, we propose further improvements for endoscopic scenes based on existing self-supervised monocular depth estimation methods. The previous method introduce an appearance flow to address brightness inconsistencies caused by lighting changes and uses a unified self-supervised framework to estimate both depth and camera motion simultaneously. However, to further enhance the model's supervisory signals, we introduce a new feature-based perceptual loss. This module utilizes a pre-trained encoder to extract features from both the synthesized and target frames and calculates their cosine dissimilarity as an additional source of supervision. In this way, we aim to improve the model's robustness in handling complex lighting and surface reflection conditions in endoscopic scenes. We compare the performance of using two pre-trained CNN-based models and four foundational models as encoder. Experimental results show that our improve method further enhances the accuracy of depth estimation in medical imaging. Additionally, it demonstrates that features extracted by CNN-based models, which are sensitive to local details, outperform foundation models. This suggests that encoders for extracting medical image features may not require extensive pre-training, and relatively simple traditional convolutional neural networks can suffice.},
  archive      = {J_IETIP},
  author       = {Kejin Zhu and Li Cui},
  doi          = {10.1049/ipr2.70035},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70035},
  shortjournal = {IET Image Process.},
  title        = {Enhancing self-supervised monocular depth estimation in endoscopy via feature-based perceptual loss},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GDS-YOLO: A rice diseases identification model with enhanced feature extraction capability. <em>IETIP</em>, <em>19</em>(1), e70034. (<a href='https://doi.org/10.1049/ipr2.70034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of rice diseases is a prerequisite for improving rice yield and quality. However, the rice diseases are complex, and the existing identification models have the problem of weak ability to extract rice disease features. To address this issue, this paper proposes a rice disease identification model with enhanced feature extraction capability, named GDS-YOLO. The proposed GDS-YOLO model improves the YOLOv8n model by introducing the GsConv module, the Dysample module, the spatial context-aware module (SCAM) and WIoU v3 loss functions. The GsConv module reduces the model's number of parameters and computational complexity. The Dysample module reduces the loss of the rice diseases feature during the extraction process. The SCAM module allows the model to ignore the influence of complex backgrounds and focus on extracting rice disease features. The WIoU v3 loss function optimises the regression box loss of rice disease features. Compared with the YOLOv8n model, the P and mAP50 of GDS-YOLO increased by 5.4% and 4.1%, respectively, whereas the number of parameters and GFLOPS decreased by 23% and 10.1%, respectively. The experimental results show that the model proposed in this paper reduces the model complexity to a certain extent and achieves good rice diseases identification results.},
  archive      = {J_IETIP},
  author       = {Yourui Huang and Xi Feng and Tao Han and Hongping Song and Yuwen Liu and Meiping Bao},
  doi          = {10.1049/ipr2.70034},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70034},
  shortjournal = {IET Image Process.},
  title        = {GDS-YOLO: A rice diseases identification model with enhanced feature extraction capability},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-channel deep pulse-coupled net: A novel bearing fault diagnosis framework. <em>IETIP</em>, <em>19</em>(1), e70033. (<a href='https://doi.org/10.1049/ipr2.70033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bearings are a critical part of various industrial equipment. Existing bearing fault detection methods face challenges such as complicated data preprocessing, difficulty in analysing time series data, and inability to learn multi-dimensional features, resulting in insufficient accuracy. To address these issues, this study proposes a novel bearing fault diagnosis model called multi-channel deep pulse-coupled net (MC-DPCN) inspired by the mechanisms of image processing in the primary visual cortex of the brain. Initially, the data are transformed into greyscale spectrograms, allowing the model to handle time series data effectively. The method introduces a convolutional coupling mechanism between multiple channels, enabling the framework can learn the features on all channels well. This study conducted experiments using the bearing fault dataset from Case Western Reserve University. On this dataset, a 6-channel (adjustable to specific tasks) MC-DPCN was utilized to analyse one normal class and three fault classes. Compared to state-of-the-art bearing fault diagnosis methods, our model demonstrates one of the highest diagnostic accuracies. This method achieved an accuracy of 99.96% in normal vs. fault discrimination and 99.89% in fault type diagnosis (average result of ten-fold cross-validation).},
  archive      = {J_IETIP},
  author       = {Yanxi Wu and Yalin Yang and Zhuoran Yang and Zhizhuo Yu and Jing Lian and Bin Li and Jizhao Liu and Kaiyuan Yang},
  doi          = {10.1049/ipr2.70033},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70033},
  shortjournal = {IET Image Process.},
  title        = {Multi-channel deep pulse-coupled net: A novel bearing fault diagnosis framework},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-aware transformer for shadow detection. <em>IETIP</em>, <em>19</em>(1), e70031. (<a href='https://doi.org/10.1049/ipr2.70031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection helps reduce ambiguity in object detection and tracking. However, existing shadow detection methods tend to misidentify complex shadows and their similar patterns, such as soft shadow regions and shadow-like regions, since they treat all cases equally, leading to an incomplete structure of the detected shadow regions. To alleviate this issue, we propose a structure-aware transformer network (STNet) for robust shadow detection. Specifically, we first develop a transformer-based shadow detection network to learn significant contextual information interactions. To this end, a context-aware enhancement (CaE) block is also introduced into the backbone to expand the receptive field, thus enhancing semantic interaction. Then, we design an edge-guided multi-task learning framework to produce intermediate and main predictions with a rich structure. By fusing these two complementary predictions, we can obtain an edge-preserving refined shadow map. Finally, we introduce an auxiliary semantic-aware learning to overcome the interference from complex scenes, which facilitates the model to perceive shadow and non-shadow regions using a semantic affinity loss. By doing these, we can predict high-quality shadow maps in different scenarios. Experimental results demonstrate that our method reduces the balance error rate (BER) by 4.53%, 2.54%, and 3.49% compared to state-of-the-art (SOTA) methods on the benchmark datasets SBU, ISTD, and UCF, respectively.},
  archive      = {J_IETIP},
  author       = {Wanlu Sun and Liyun Xiang and Wei Zhao},
  doi          = {10.1049/ipr2.70031},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70031},
  shortjournal = {IET Image Process.},
  title        = {Structure-aware transformer for shadow detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new genetic algorithm-based network for text localization in degraded social media images. <em>IETIP</em>, <em>19</em>(1), e70030. (<a href='https://doi.org/10.1049/ipr2.70030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel model for understanding social image content through text localization. For text localization, we explore maximally stable extremal regions (MSER) for detecting components that work by clustering pixels with similar properties. The output of component detection includes several non-text components due to the degradations of social media images. To select the best components among many, we explore the genetic algorithm by convolving different kernels with components, which results in a feature matrix that is further fed to EfficientNet for choosing actual text components. Therefore, the proposed model is called genetic algorithm based network for text localization in degraded social media images (TLDSMI). For evaluating text localization, we consider the images of the standard dataset of natural scenes by uploading and downloading from different social media platforms, namely, WhatsApp, Telegram, and Instagram. The effectiveness of our method is shown by testing on original and degraded standard datasets. For example, for the degraded images of different complexities including degradations caused by social media platforms, the proposed method performs well in almost all situations. In addition, the proposed model achieves the best F1-Score, 0.76, 0.77, 0.70, and 0.78 for the degraded images of CUTE, ICDAR 2013, Total-Text, and CTW1500, respectively, compared to the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Shivakumara Palaiahnakote and Chandrahas Pavan Kumar and Pranjal Aggarwal and Shubham Sharma and Pasupuleti Chandana and Mahadveppa Basavanna and Umapada Pal},
  doi          = {10.1049/ipr2.70030},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70030},
  shortjournal = {IET Image Process.},
  title        = {A new genetic algorithm-based network for text localization in degraded social media images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning discriminative palmprint anti-spoofing features via high-frequency spoofing regions adaptation. <em>IETIP</em>, <em>19</em>(1), e70029. (<a href='https://doi.org/10.1049/ipr2.70029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the majority of palmprint recognition studies have focused on feature extraction while neglecting security issues. Among the various attack types, spoofing attack poses a significant threat due to high success rates and minimal technical requirements. In this study, we explore the differences between real and fake palmprint images. Based on these differences, we propose the concept of ‘high-frequency spoofing regions’ to capture key discriminative spoofing clues. Specifically, the high-frequency spoofing regions adaptation ( HFSRA ) model is proposed to address palmprint anti-spoofing. The HFSRA consists of two key modules: the texture analysis module (TAM) and the spoofing attention module (SAM). In particular, the TAM divides the input feature map into several patches and evaluates the texture distribution within each patch. Next, the SAM dynamically constructs an attention map by mapping the texture distribution to an attention weight matrix. This adaptive structure forces the model to focus on high-frequency spoofing regions, which improves the model's ability to extract meaningful spoofing clues effectively. Furthermore, we establish three experimental protocols for evaluating the performance of palmprint anti-spoofing models. These protocols provide a standardized evaluation framework for future studies. Extensive experiments conducted under these protocols demonstrate the effectiveness and competitiveness of HFSRA.},
  archive      = {J_IETIP},
  author       = {Chengcheng Liu and Huikai Shao and Dexing Zhong},
  doi          = {10.1049/ipr2.70029},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70029},
  shortjournal = {IET Image Process.},
  title        = {Learning discriminative palmprint anti-spoofing features via high-frequency spoofing regions adaptation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN-based super-resolution with enhanced multi-scale laplacian pyramid and frequency domain loss. <em>IETIP</em>, <em>19</em>(1), e70028. (<a href='https://doi.org/10.1049/ipr2.70028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution techniques play an important role in the fields of image processing and computer vision. However, existing super-resolution methods based on generative adversarial networks still exhibit significant shortcomings in recovering high-frequency details and effectively utilising multi-scale information. To address these issues, this paper proposes an improved generative adversarial network. Specifically, an enhanced multi-scale Laplacian pyramid structure is designed to capture and process image details at different scales. Then, convolutional operations are added to each layer of the pyramid to further improve the recovery of multi-scale details. Additionally, a frequency domain loss is introduced, where the generated and real images are transformed into the frequency domain using Fourier transforms for comparison. This method enhances the reconstruction of high-frequency details. The experiments are validated on four publicly available datasets and the results show that the proposed network significantly outperforms existing methods in both reconstruction quality and visual performance.},
  archive      = {J_IETIP},
  author       = {Hao Chen and Xi Lu and Jixining Zhu},
  doi          = {10.1049/ipr2.70028},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70028},
  shortjournal = {IET Image Process.},
  title        = {GAN-based super-resolution with enhanced multi-scale laplacian pyramid and frequency domain loss},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SF-YOLO: A novel YOLO framework for small object detection in aerial scenes. <em>IETIP</em>, <em>19</em>(1), e70027. (<a href='https://doi.org/10.1049/ipr2.70027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection models are widely applied in the fields such as video surveillance and unmanned aerial vehicles to enable the identification and monitoring of various objects on a diversity of backgrounds. The general CNN-based object detectors primarily rely on downsampling and pooling operations, often struggling with small objects that have low resolution and failing to fully leverage contextual information that can differentiate objects from complex background. To address the problems, we propose a novel YOLO framework called SF-YOLO for small object detection. Firstly, we present a spatial information perception (SIP) module to extract contextual features for different objects through the integration of space to depth operation and large selective kernel module, which dynamically adjusts receptive field of the backbone and obtains the enhanced features for richer understanding of differentiation between objects and background. Furthermore, we design a novel multi-scale feature weighted fusion strategy, which performs weighted fusion on feature maps by combining fast normalized fusion method and CARAFE operation, accurately assessing the importance of each feature and enhancing the representation of small objects. The extensive experiments conducted on VisDrone2019, Tiny-Person and PESMOD datasets demonstrate that our proposed method enables comparable detection performance to state-of-the-art detectors.},
  archive      = {J_IETIP},
  author       = {Meng Sun and Le Wang and Wangyu Jiang and Fayaz Ali Dharejo and Guojun Mao and Radu Timofte},
  doi          = {10.1049/ipr2.70027},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70027},
  shortjournal = {IET Image Process.},
  title        = {SF-YOLO: A novel YOLO framework for small object detection in aerial scenes},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deformable attention network for efficient space-time video super-resolution. <em>IETIP</em>, <em>19</em>(1), e70026. (<a href='https://doi.org/10.1049/ipr2.70026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-time video super-resolution (STVSR) aims to construct high space-time resolution video sequences from low frame rate and low-resolution video sequences. While recent STVSR works combine temporal interpolation and spatial super-resolution in a unified framework, they face challenges in computational complexity across both temporal and spatial dimensions, particularly in achieving accurate intermediate frame interpolation and efficient temporal information utilisation. To address these, we propose a deformable attention network for efficient STVSR. Specifically, we introduce a deformable interpolation block that employs hierarchical feature fusion to effectively handle complex inter-frame motions at multiple scales, enabling more accurate intermediate frame generation. To fully utilise temporal information, we design a temporal feature shuffle block (TFSB) to efficiently exchange complementary information across multiple frames. Additionally, we develop a motion feature enhancement block incorporating channel attention mechanism to selectively emphasise motion-related features, further boosting TFSB's effectiveness. Experimental results on benchmark datasets definitively demonstrate that our proposed method achieves competitive performance in STVSR tasks.},
  archive      = {J_IETIP},
  author       = {Hua Wang and Rapeeporn Chamchong and Phatthanaphong Chomphuwiset and Pornntiwa Pawara},
  doi          = {10.1049/ipr2.70026},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70026},
  shortjournal = {IET Image Process.},
  title        = {Deformable attention network for efficient space-time video super-resolution},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust text watermarking based on modifying the stroke components of chinese characters. <em>IETIP</em>, <em>19</em>(1), e70025. (<a href='https://doi.org/10.1049/ipr2.70025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional codebooks used for tracing information leakage in text documents often suffer from limitations in embedding capacity, robustness, and efficiency due to their manual generation process. This paper proposes a robust text watermarking method based on the stroke components of Chinese characters. By designing an innovative approach, Chinese character strokes are divided into several distinct components, with only specific ones being selectively modified to generate new glyphs, thus forming a unique codebook. The watermark signals are embedded by substituting the carrier glyph with the newly generated one, and the signals are extracted using a template matching method. Experimental results demonstrate that, compared to traditional manually designed codebooks, the proposed method significantly reduces human labor and computational overhead while maintaining high visual quality. Moreover, it exhibits superior robustness and adaptability across various challenging scenarios, including digital noise attacks, print-scanning attacks, and print-camera capture, making it a highly effective solution for protecting textual information.},
  archive      = {J_IETIP},
  author       = {Hai Chen and Yanli Chen and Zhicheng Dong and Yongrong Wang and Asad Malik and Hanzhou Wu},
  doi          = {10.1049/ipr2.70025},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70025},
  shortjournal = {IET Image Process.},
  title        = {Robust text watermarking based on modifying the stroke components of chinese characters},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight accelerated unfolding network with collaborative attention for snapshot spectral compressive imaging. <em>IETIP</em>, <em>19</em>(1), e70024. (<a href='https://doi.org/10.1049/ipr2.70024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In coded aperture snapshot spectral imaging (CASSI) systems, deep unfolding networks (DUNs) have made significant strides in recovering 3D hyperspectral images (HSIs) from a single 2D measurement. However, the inherent nonlinearity and ill-posed nature of HSI reconstruction continue to challenge existing methods in terms of accuracy and stability. To address these challenges, we propose a lightweight collaborative attention-enhanced accelerated unfolding network ( ), which integrates a DUN framework with a streamlined prior extractor. Our integrated approach introduces a generically accelerated half-quadratic splitting algorithm (A-HQS) for degradation estimation, overcoming the limitations of first-order optimization and enabling effective long-range dependency modeling. Within the prior extractor, we introduce cross-convergence attention, facilitating iterative information exchange between local and non-local Transformers to capture holistic features and enhance inductive capacity. Notably, the concept of collaborative cross-convergence is embedded throughout all submodules, ensuring effective information flow. The proposed not only accelerates the convergence of spectral reconstruction, but also fully exploits compressed spatial-spectral information. Numerical and visual comparisons on both synthetic and real datasets demonstrate the superior performance of this approach. Comparisons on both synthetic and real datasets illustrate the superiority of this approach. The source code is available at https://github.com/Mengjie-s/CA2UN .},
  archive      = {J_IETIP},
  author       = {Mengjie Qin and Yuchao Feng},
  doi          = {10.1049/ipr2.70024},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70024},
  shortjournal = {IET Image Process.},
  title        = {Lightweight accelerated unfolding network with collaborative attention for snapshot spectral compressive imaging},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flood-MATE: A flood segmentation model in urban regions through adaptation of mean teacher and ensemble approach. <em>IETIP</em>, <em>19</em>(1), e70023. (<a href='https://doi.org/10.1049/ipr2.70023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flood disasters remain one of the most recurring natural phenomena worldwide, resulting from excessive water flow submerging land for an extended period of time. The escalating occurrences of floods, particularly in urban areas, can be attributed to climate change, extreme weather patterns, uncontrolled urbanization, and complex geographical conditions. To mitigate the destructive impacts, such as loss of life and economic ramifications, automatic flood analysis and remote-sensing imagery segmentation offer valuable decision-making insights. However, the segmentation process for flood detection faces challenges due to the scarcity of labelled data and diverse resolutions, including medium resolution data. In response, the authors propose Flood-MATE, a novel semi-supervised learning approach based on the mean-teacher model. Our approach leverages the deep learning architecture and introduces a new loss function scenario for training. The dataset utilized in this study comprises SAR images of Sentinel-1 C-band that have undergone thorough processing. Promisingly, the results demonstrate a 4% improvement in the IoU metric compared to the baseline method employing pseudo-labelling.},
  archive      = {J_IETIP},
  author       = {Bella Septina Ika Hartanti and Adila Alfa Krisnadhi and Laksmita Rahadianti and Wiwiek Dwi Susanti and Achmad Fakhrus Shomim},
  doi          = {10.1049/ipr2.70023},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70023},
  shortjournal = {IET Image Process.},
  title        = {Flood-MATE: A flood segmentation model in urban regions through adaptation of mean teacher and ensemble approach},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRSE-YOLO: Efficient and lightweight architecture for accurate waste detection. <em>IETIP</em>, <em>19</em>(1), e70022. (<a href='https://doi.org/10.1049/ipr2.70022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces DRSE-YOLO, an efficient waste detection model designed to address detection accuracy and lightweight design challenges. The RCCA module in the model's neck enhances multi-scale feature representation, thereby improving detection performance. The DySample module optimizes upsampling through adaptive point-sampling, reducing computational demands and improving resource efficiency. The Slim-Neck module is applied to select convolutional layers and C2f modules to streamline the model and enhance computational efficiency. The ECC-Head integrates asymmetric depth convolution, point convolution, and an attention mechanism, balancing accuracy with reduced parameters and computational load. Evaluated on a custom dataset comprising 46 waste classes and approximately 25,000 images, DRSE-YOLO achieves significant improvements over YOLOv8n, including a higher mAP@0.5 (+1.59%) and mAP@0.5:95 (+2.08%), alongside a reduced parameter count (2.43 M vs. 3.2 M) and GFLOPs (5.8 vs. 8.2, a 24.4% reduction). These results underscore DRSE-YOLO's efficiency and accuracy.},
  archive      = {J_IETIP},
  author       = {Guangling Sun and Fenqi Zhang},
  doi          = {10.1049/ipr2.70022},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70022},
  shortjournal = {IET Image Process.},
  title        = {DRSE-YOLO: Efficient and lightweight architecture for accurate waste detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibration method for ultra-wide FOV fisheye cameras based on improved camera model and SE(3) image pre-correction. <em>IETIP</em>, <em>19</em>(1), e70021. (<a href='https://doi.org/10.1049/ipr2.70021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The severe radial distortion of ultra-wide field of view (FOV) fisheye camera results in poor model fitting and challenges in calibration board detection. In this paper, a novel calibration method for ultra-wide FOV fisheye cameras is proposed based on improved camera model and SE(3) image pre-correction. Initially, a method to extend the maximum fitting FOV of the camera model to over 180 degrees is proposed. Subsequently, a calibration board detection approach is proposed using SE(3) image pre-correction. Specifically, image pre-correction is incorporated into the camera calibration process, utilizing SE(3) to define the pre-correction plane. Calibration boards are detected within the pre-corrected images, enhancing the reliability, accuracy and speed of board detection in distorted images, consequently increasing the control point's maximum FOV. Lastly, the improved camera model and SE(3) image pre-correction are integrated into a feedback-based camera calibration system for ultra-wide FOV fisheye cameras. Operating with real-time or offline video streams as input, this system autonomously selects calibration key frames, optimizes camera parameters and calibration board poses in real-time. Simulation and real-world experiments verify the effectiveness of the proposed method, leading to a 62% increase in the achievable maximum FOV.},
  archive      = {J_IETIP},
  author       = {Rui Xing and Fenghua He and Yu Yao},
  doi          = {10.1049/ipr2.70021},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70021},
  shortjournal = {IET Image Process.},
  title        = {Calibration method for ultra-wide FOV fisheye cameras based on improved camera model and SE(3) image pre-correction},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of overlay target's centre positioning algorithms using customizable shape fitting for high-precision wafer bonding. <em>IETIP</em>, <em>19</em>(1), e70020. (<a href='https://doi.org/10.1049/ipr2.70020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wafer bonding is a critical process in 3D integration, and overlay (OVL) metrology is essential for its success. Accurately positioning the centre of OVL targets is fundamental for effective metrology. However, the identification and localization of target centres become challenging due to complex shapes and unexpected features, such as rounded corners, that can arise during manufacturing. An algorithm is proposed to tackle this challenge by employing customizable shape fitting. This method begins with the extraction of sub-pixel edge points, followed by applying a Hough transform to group and smooth these points, thereby enhancing contour quality. By parameterizing the target shape based on specific points, the algorithm integrates sub-pixel traversal techniques with an optimization objective, achieving sub-pixel accuracy in centre positioning. Simulation results indicate that the algorithm can achieve a positioning accuracy of ±0.03 pixels and demonstrates robustness against noise and blur. Finally, the proposed algorithm was used to test the OVL target pair arrays fabricated by electron beam etching, confirming an accuracy of ±0.04 pixels (±6.9 nm). These results validate the algorithm's capability to meet high precision requirements for OVL target centre positioning in wafer applications.},
  archive      = {J_IETIP},
  author       = {Rui Wang and Yixian Zhu and Sen Lu and Kaiming Yang and Yu Zhu},
  doi          = {10.1049/ipr2.70020},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70020},
  shortjournal = {IET Image Process.},
  title        = {Development of overlay target's centre positioning algorithms using customizable shape fitting for high-precision wafer bonding},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RBS-YOLO: A lightweight YOLOv5-based surface defect detection model for castings. <em>IETIP</em>, <em>19</em>(1), e70018. (<a href='https://doi.org/10.1049/ipr2.70018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure precise and rapid identification of casting surface defects and to support the subsequent realisation of high-precision grinding, this study introduces a method for detecting casting surface defects using a lightweight YOLOv5 framework. The enhanced model integrates the ShuffleNetV2 high-efficiency CNN architecture into the YOLOv5 foundation, substantially reducing network parameters to achieve a lightweight model. Additionally, the Convolutional Block Attention Module (CBAM) attention mechanism is incorporated to enhance the model's capability to detect defects. The ReLU activation function replaces the SiLU function in the convolutional layer, decreasing the computational load and boosting efficiency. Subsequently, the optimised model is quantised and implemented on the RV1126 embedded development board, successfully performing image inference. To validate the effectiveness of the proposed method, a dataset of casting surface defects was designed and constructed. The optimised model has a file size of 7.6 MB, representing 55.4% of the original model, with about 50.6% of the original model's parameters. The onboard inference speed of the improved model is 50 ms per image, which is 9.1% faster than the traditional YOLOv5 model. These results offer valuable insights for future casting surface defect detection technologies.},
  archive      = {J_IETIP},
  author       = {KeZhu Wu and ShaoMing Sun and YiNing Sun and CunYi Wang and YiFan Wei},
  doi          = {10.1049/ipr2.70018},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70018},
  shortjournal = {IET Image Process.},
  title        = {RBS-YOLO: A lightweight YOLOv5-based surface defect detection model for castings},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image segmentation refinement based on region expansion and minor contour adjustments. <em>IETIP</em>, <em>19</em>(1), e70017. (<a href='https://doi.org/10.1049/ipr2.70017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-precision image segmentation tasks, even slight deviations in the segmentation results can bring about significant consequences, especially in certain application areas such as medical imaging and remote sensing image classification. The precision of segmentation has become the main factor limiting its development. Researchers typically refine image segmentation algorithms to enhance accuracy, but it is challenging for any improvement strategy to be effectively applied to images of different objects and scenes. To address this issue, we propose a two-step refinement method for image segmentation, comprising region expansion and minor contour adjustments. First, we design an adaptive gradient thresholding module to provide gradient-based constraints for the refinement process. Next, the region expansion module iteratively refines each segmented region based on colour differences and gradient thresholds. Finally, the minor contour adjustments module leverages local strong gradient features to refine the contour positions further. This method integrates region-level and pixel-level information to refine various image segmentation results. This method was applied to the BSDS500, Cells, and WHU Building datasets. The results demonstrate that the refined closed contours align more closely with the ground truth, with the most notable improvement observed at contour inflection points (corner points). Among the results, the Cells dataset showed the most significant improvement in segmentation accuracy, with the F-score increasing from 87.51% to 89.73% and IoU from 86.83% to 88.40%.},
  archive      = {J_IETIP},
  author       = {Li-yue Yan and Xing Zhang and Kafeng Wang and Siting Xiong and De-jin Zhang},
  doi          = {10.1049/ipr2.70017},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70017},
  shortjournal = {IET Image Process.},
  title        = {Image segmentation refinement based on region expansion and minor contour adjustments},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task collaboration for cross-modal generation and multi-modal ophthalmic diseases diagnosis. <em>IETIP</em>, <em>19</em>(1), e70016. (<a href='https://doi.org/10.1049/ipr2.70016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal diagnosis of ophthalmic disease is becoming increasingly important because combining multi-modal data allows for more accurate diagnosis. Color fundus photograph (CFP) and optical coherence tomography (OCT) are commonly used as two non-invasive modalities for ophthalmic examination. However, the diagnosis of each modality is not entirely accurate. Compounding the challenge is the difficulty in acquiring multi-modal data, with existing datasets frequently lacking paired multi-modal data. To solve these problems, we propose multi-modal distribution fusion diagnostic algorithm and cross-modal generation algorithm. The multi-modal distribution fusion diagnostic algorithm first calculates the mean and variance separately for each modality, and then generates multi-modal diagnostic results in a distribution fusion manner. In order to generate the absent modality (mainly OCT data), three sub-networks are designed in the cross-modal generation algorithm: cross-modal alignment network, conditional deformable autoencoder and latent consistency diffusion model (LCDM). Finally, we propose multi-task collaboration strategy where diagnosis and generation tasks are mutually reinforcing to achieve optimal performance. Experimental results demonstrate that our proposed method yield superior results compared to state-of-the-arts.},
  archive      = {J_IETIP},
  author       = {Yang Yu and Hongqing Zhu and Tianwei Qian and Tong Hou and Bingcang Huang},
  doi          = {10.1049/ipr2.70016},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70016},
  shortjournal = {IET Image Process.},
  title        = {Multi-task collaboration for cross-modal generation and multi-modal ophthalmic diseases diagnosis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TLBP: Tomography-aided local binary patterns with high discrimination for image classification. <em>IETIP</em>, <em>19</em>(1), e70015. (<a href='https://doi.org/10.1049/ipr2.70015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local binary patterns (LBP) play a vital role in image classification as a computationally efficient feature descriptor. A crucial reason for its limitation of discriminability is the lack of neighbourhood information description from a global perspective. Previous research has attempted to improve its performance by introducing global thresholds, but such threshold selection is not optimal. To address this issue, we propose a novel tomography-aided local binary patterns (TLBP), inspired by the tomographic process of sample separation. TLBP considers constructing visual feature representations under multi-level non-local information to compensate for the lack of LBP possessing only a single shallow feature. In addition to the basic LBP features from local visual context, TLBP captures refined neighbourhood greyscale information through multi-quantile thresholds from a global visual perspective, thereby greatly enhancing discriminability. Experimental results in texture classification, face recognition, and hyperspectral pixel-wise classification demonstrate that the proposed TLBP descriptor outperforms the competitors, achieving 94.39% (KTH-TIPS), 81.22% (KTH-TIPS-ROT), 93.81% (Indian Pines), 99.85% (Salinas), and 99.50% (ORL) accuracy. Furthermore, the performance of the T-variants that apply the tomographic idea to classic LBP descriptors improve significantly, especially for their rotation-invariant versions.},
  archive      = {J_IETIP},
  author       = {Yichen Liu and Xin Zhang and Yanan Jiang and Chunlei Zhang and Hanlin Feng},
  doi          = {10.1049/ipr2.70015},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70015},
  shortjournal = {IET Image Process.},
  title        = {TLBP: Tomography-aided local binary patterns with high discrimination for image classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified you only look once network model for enhanced traffic scene detection performance for small targets. <em>IETIP</em>, <em>19</em>(1), e70014. (<a href='https://doi.org/10.1049/ipr2.70014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to address the challenge of small target recognition in traffic scenes, we propose a model based on you only look once version 8X (Yolov8X) network model, which has been combined with receptive fields block (RFB) and multidimensional collaborative attention (MCA). First, the model employs the RFB to extract reliable and distinctive features, thereby enhancing the precision of small target identification. Furthermore, the MCA structure is introduced to simulate multidimensional attention through three parallel branches, thereby enhancing the feature expression ability of the model. This fragment describes a compression transformation and an excitation transformation that captures the differentiated feature representation of the command. These transformations facilitate the network's ability to locate and predict the location of small objects more accurately. Utilizing these transformations enhances the expressiveness and diversity of features, thereby improving the detection performance of small objects. Furthermore, data augmentation and hyperparameter optimization techniques are employed to enhance the model's generalisability. The validation results on the Argoverse 1.1 autonomous driving dataset demonstrate that the enhanced network model outperforms the prevailing detectors, achieving an F1 score of 78.6, an average precision of 55.1, and an average recall of 72.4. The algorithm's excellent performance for small target detection was demonstrated through visual analysis, proving its high application value and potential for promotion in fields such as autonomous driving.},
  archive      = {J_IETIP},
  author       = {Lei Shi and Shuai Ren and Xing Fan and Ke Wang and Shan Lin and Zhanwen Liu},
  doi          = {10.1049/ipr2.70014},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70014},
  shortjournal = {IET Image Process.},
  title        = {Modified you only look once network model for enhanced traffic scene detection performance for small targets},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight multi-stage holistic attention-based network for image super-resolution. <em>IETIP</em>, <em>19</em>(1), e70013. (<a href='https://doi.org/10.1049/ipr2.70013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution images are crucial for many applications, but factors such as environmental conditions can reduce image quality. Super-resolution (SR) techniques address this by generating high-resolution images from low-resolution inputs. While deep learning SR models have made significant progress, they can be computationally expensive and struggle with differentiating between various image scales. Lightweight SR methods, suitable for resource-constrained devices, often compromise image quality. This study introduces a multi-stage holistic attention-based network, using Gaussian Laplacian pyramids to decompose images and apply holistic attention modules at each level. This approach reduces parameters and computational costs while maintaining image quality, achieving a PSNR score of 28 and SSIM of 0.91 with only 29,000 parameters. The model demonstrates the potential for efficient and high-quality image reconstruction. Future work will focus on improving quality while minimizing costs and exploring other advanced techniques. The code will be made available upon request},
  archive      = {J_IETIP},
  author       = {Aatiqa Bint E Ghazali and Ahsan Fiaz and Muhammad Islam},
  doi          = {10.1049/ipr2.70013},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70013},
  shortjournal = {IET Image Process.},
  title        = {Lightweight multi-stage holistic attention-based network for image super-resolution},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient method for detecting targets from remote sensing images based on global attention mechanism. <em>IETIP</em>, <em>19</em>(1), e70012. (<a href='https://doi.org/10.1049/ipr2.70012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image target detection provides an effective and accurate data analysis tool for many application areas. Due to complex backgrounds, large differences in target scales, and missed detection of small targets, remote sensing image target detection is challenging. In order to enhance the model's understanding of the global information of remote sensing images, this paper proposes the GFA module. This module can establish the global contextual connection of remote sensing images to provide rich context to help understand the complex scene and background in which the target is located, without being limited to local information. Additionally, it focuses on channel information for enhanced target feature extraction. For the purpose of alleviating the serious imbalance in foreground–background samples that is present in single-level target detection models. The loss function is reconstructed based on focal loss by redefining the balance factor α and focus factor γ , so that it can be dynamically adjusted during network training. Meanwhile, EIoU is used to further enhance the bounding box regression capability. Affine transformations were also used to augment the dataset in order to assist the model in adjusting to real-world situations. The proposed method is experimentally validated on the publicly available HRRSD dataset. In comparison with YOLO v5, the mAP of the detection results improved by 2.7%. Compared with YOLO v8 and YOLO v10, the mAP improved by 3.2% and 3.3%. The model achieves an FPS of 40.1, an optimal balance between speed and accuracy. Further, experiments are conducted using the NWPU VHR-10 dataset and the RSOD dataset, both of which demonstrated that the proposed method outperforms other target detection methods and improves remote sensing target detection performance.},
  archive      = {J_IETIP},
  author       = {Zijun Gao and Jingwen Su and Bo Li and Jue Wang and Zhankui Song},
  doi          = {10.1049/ipr2.70012},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70012},
  shortjournal = {IET Image Process.},
  title        = {Efficient method for detecting targets from remote sensing images based on global attention mechanism},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NewTalker: Exploring frequency domain for speech-driven 3D facial animation with mamba. <em>IETIP</em>, <em>19</em>(1), e70011. (<a href='https://doi.org/10.1049/ipr2.70011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current field of speech-driven 3D facial animation, transformer-based methods are limited in practical applications due to their high computational complexity. A new model—NewTalker—is proposed, which has core modules consisting of the residual bidirectional Mamba (RBM) and the time–frequency domain Kolmogorov–Arnold networks (TFK). The RBM module incorporates the philosophy of Mamba, enhancing the model's predictive ability for sequence data by utilizing both past and future contextual information, thereby reducing the computational complexity. The TFK module integrates the temporal and frequency domain information of audio data through Kolmogorov–Arnold networks, allowing the model to generate 3D facial animations smoothly while learning more detailed features. Extensive experiments and user studies have shown that the proposed NewTalker significantly surpasses current mainstream algorithms in terms of animation quality and inference speed, achieving the state-of-the-art level in this domain.},
  archive      = {J_IETIP},
  author       = {Weiran Niu and Zan Wang and Yi Li and Tangtang Lou},
  doi          = {10.1049/ipr2.70011},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70011},
  shortjournal = {IET Image Process.},
  title        = {NewTalker: Exploring frequency domain for speech-driven 3D facial animation with mamba},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSA-net: Global spatial structure-aware attention network for liver segmentation in MR images with respiratory artifacts. <em>IETIP</em>, <em>19</em>(1), e70010. (<a href='https://doi.org/10.1049/ipr2.70010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic liver segmentation is of great significance for computer-aided treatment and surgery of liver diseases. However, respiratory motion often affects the liver, leading to image artifacts in liver magnetic resonance imaging (MRI) and increasing segmentation difficulty. To overcome this issue, we propose a global spatial structure-aware attention model (GSA-Net), a robust segmentation network developed to overcome the difficulties caused by respiratory motion. The GSA-Net is an encoder-decoder architecture, which extracts spatial structure information from images and identifies different objects using the minimum spanning tree algorithm. The network's encoder extracts multi-scale image features with the help of an effective and lightweight channel attention module. The decoder then transforms these features bottom-up using tree filter modules. Combined with the boundary detection module, the segmentation performance can be further improved. We evaluate the effectiveness of our method on two liver MRI benchmarks: one with respiratory artifacts and the other without. Numerical evaluations on different benchmarks demonstrate that GSA-Net consistently outperforms previous state-of-the-art models in terms of segmentation precision on our respiratory artifact dataset, and also achieves notable results on high-quality datasets.},
  archive      = {J_IETIP},
  author       = {Jiahuan Jiang and Dongsheng Zhou and Muzhen He and Xiaohan Yue and Shu Zhang},
  doi          = {10.1049/ipr2.70010},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70010},
  shortjournal = {IET Image Process.},
  title        = {GSA-net: Global spatial structure-aware attention network for liver segmentation in MR images with respiratory artifacts},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FMR-YOLO: An improved YOLOv8 algorithm for steel surface defect detection. <em>IETIP</em>, <em>19</em>(1), e70009. (<a href='https://doi.org/10.1049/ipr2.70009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the insufficient feature extraction capability for steel surface defects in industrial production, as well as issues such as low detection speed and poor accuracy caused by large model parameters, a metal surface defect detection algorithm named FMR-YOLO, based on an improved YOLOv8n, is proposed. The algorithm incorporates a fast lightweight feature extraction structure, the number of parameters and computation of the model are reduced while preserving the spatial information, thus improving the target detection performance. A multi-scale feature fusion module is introduced, enabling the extraction of more comprehensive and richer features compared to traditional single-scale methods, to better support defect detection tasks. Additionally, a receptive field attention structure, Receptive Field Attention Neck, is designed in the Neck part to expand the model's receptive field and reduce computational complexity, significantly improving detection accuracy for small defects. This allows the model to effectively capture both global and local features in complex industrial scenarios. The effectiveness of the improved FMR-YOLO algorithm is validated on two industrial surface defect datasets: GC10-DET and NEU-DET. Experimental results show that the mAP@0.5 detection accuracy has increased by 4.5% and 5.1% on the GC10-DET and NEU-DET datasets, respectively, with a parameter size of merely 2.7 M.},
  archive      = {J_IETIP},
  author       = {Yongjing Ni and Qi Wu and Xiuqing Zhang},
  doi          = {10.1049/ipr2.70009},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70009},
  shortjournal = {IET Image Process.},
  title        = {FMR-YOLO: An improved YOLOv8 algorithm for steel surface defect detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OMA-SSR: Optical-guided multi-kernel attention based SAR image super-resolution reconstruction network. <em>IETIP</em>, <em>19</em>(1), e70008. (<a href='https://doi.org/10.1049/ipr2.70008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar (SAR) has been widely studied and applied in many fields. Although image super-resolution technology has been successfully applied to SAR imaging in recent years, there is less research on large-scale factor SAR image super-resolution methods. A more effective method is to obtain comprehensive information to guide the reconstruction of SAR images. In fact, the co-registered characteristics of high-resolution optical images have been successfully applied to improve the quality of SAR images. Inspired by this, an optical-guided multi-kernel attention based SAR image super-resolution reconstruction network (OMA-SSR) is proposed. The proposed multi-modal mutual attention (MMA) module in this network can effectively establish the dependency between SAR image features and optical image features. This network also designs a deep feature extraction module for SAR images, which includes a channel-splitted multi-kernel attention (CSMA) module and residual connections. CSMA module splits SAR image channels, extracts features in different ranges through multi-kernel convolution, and finally fuses the extracted features between different channels. Experimental results on the Sen1-2 and QXS datasets show that the proposed OMA-SSR performs well in evaluation indicators and visual effects of SAR image super-resolution reconstruction.},
  archive      = {J_IETIP},
  author       = {Yanshan Li and Fan Xu},
  doi          = {10.1049/ipr2.70008},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70008},
  shortjournal = {IET Image Process.},
  title        = {OMA-SSR: Optical-guided multi-kernel attention based SAR image super-resolution reconstruction network},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiclassification tampering detection algorithm based on spatial-frequency fusion and swin-T. <em>IETIP</em>, <em>19</em>(1), e70007. (<a href='https://doi.org/10.1049/ipr2.70007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods for image forgery detection often struggle with compression attack robustness. This paper proposes a novel multi-class forgery detection framework combining spatial-frequency fusion with Swin-Transformer, outperforming existing methods in compression attack scenarios. Our approach integrates a frequency domain perception module with quantization tables, a spatial domain perception module through multi-strategy convolutions, and a dual-attention mechanism combining spatial and channel attention for feature fusion. Experimental results demonstrate superior performance with an F 1 score of 87% under JPEG compression ( q = 75), significantly surpassing current state-of-the-art methods by an average of 15% in compression resistance while maintaining high detection accuracy.},
  archive      = {J_IETIP},
  author       = {Li Li and Kejia Zhang and Jianfeng Lu and ShanQing Zhang and Ning Chu},
  doi          = {10.1049/ipr2.70007},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70007},
  shortjournal = {IET Image Process.},
  title        = {Multiclassification tampering detection algorithm based on spatial-frequency fusion and swin-T},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-cropping contrastive learning and domain consistency for unsupervised image-to-image translation. <em>IETIP</em>, <em>19</em>(1), e70006. (<a href='https://doi.org/10.1049/ipr2.70006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unsupervised image-to-image (i2i) translation methods based on contrastive learning have achieved state-of-the-art results. However, in previous works, the negatives are sampled from the input image itself, which inspires us to design a data augmentation method to improve the quality of the selected negatives. Moreover, the previous methods only preserve the content consistency via patch-wise contrastive learning, which ignores the domain consistency between the generated images and the real images of the target domain. This paper proposes a novel unsupervised i2i translation framework based on multi-cropping contrastive learning and domain consistency, called MCDUT. Specifically, the multi-cropping views are obtained with the aim of further generating high-quality negative examples. To constrain the embeddings in the deep feature space, a new domain consistency loss is formulated, which encourages the generated images to be close to the real images. In many i2i translation tasks, this method achieves state-of-the-art results, and the advantages of this method have been proven through extensive comparison experiments and ablation research. The code of MCDUT is available at https://github.com/zhihefang/MCDUT .},
  archive      = {J_IETIP},
  author       = {Chen Zhao and Wei-Ling Cai and Zheng Yuan and Cheng-Wei Hu},
  doi          = {10.1049/ipr2.70006},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70006},
  shortjournal = {IET Image Process.},
  title        = {Multi-cropping contrastive learning and domain consistency for unsupervised image-to-image translation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LGS-net: A lightweight convolutional neural network based on global feature capture for spatial image steganalysis. <em>IETIP</em>, <em>19</em>(1), e70005. (<a href='https://doi.org/10.1049/ipr2.70005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of image steganalysis is to detect whether the transmitted images in network communication contain secret messages. Current image steganalysis networks still have some problems such as inappropriate feature selection and easy overfitting. Therefore, this paper proposed a new spatial image steganalysis method based on convolutional neural networks. To extract richer features while reducing useless parameters in the network, this paper introduced the Im SRM filtering kernel into the image preprocessing module. To extract effective steganography noise from images, this paper combined depthwise separable convolution and residual networks for the first time and introduces them into the steganography noise extraction module. In addition, to focus network attention on the image regions where steganography information exists, this paper integrated the coordinate attention mechanism. This module will make the network pay attention to the overall structure and local details of the image during network training, improving the network's recognition ability for steganography information. Finally, the extracted steganography features are classified through a classification module. This paper conducted a series of experiments on the BOSSBase 1.01 and BOWS2 datasets. The improvement in detection accuracy is between 1.2% and 18.2% compared to classic and recent steganalysis networks.},
  archive      = {J_IETIP},
  author       = {Yuanyuan Ma and Jian Wang and Xinyu Zhang and Guifang Wang and Xianwei Xin and Qianqian Zhang},
  doi          = {10.1049/ipr2.70005},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70005},
  shortjournal = {IET Image Process.},
  title        = {LGS-net: A lightweight convolutional neural network based on global feature capture for spatial image steganalysis},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse representation for restoring images by exploiting topological structure of graph of patches. <em>IETIP</em>, <em>19</em>(1), e70004. (<a href='https://doi.org/10.1049/ipr2.70004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration poses a significant challenge, aiming to accurately recover damaged images by delving into their inherent characteristics. Various models and algorithms have been explored by researchers to address different types of image distortions, including sparse representation, grouped sparse representation, and low-rank self-representation. The grouped sparse representation algorithm leverages the prior knowledge of non-local self-similarity and imposes sparsity constraints to maintain texture information within images. To further exploit the intrinsic properties of images, this study proposes a novel low-rank representation-guided grouped sparse representation image restoration algorithm. This algorithm integrates self-representation models and trace optimization techniques to effectively preserve the original image structure, thereby enhancing image restoration performance while retaining the original texture and structural information. The proposed method was evaluated on image denoising and deblocking tasks across several datasets, demonstrating promising results.},
  archive      = {J_IETIP},
  author       = {Yaxian Gao and Zhaoyuan Cai and Xianghua Xie and Jingjing Deng and Zengfa Dou and Xiaoke Ma},
  doi          = {10.1049/ipr2.70004},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70004},
  shortjournal = {IET Image Process.},
  title        = {Sparse representation for restoring images by exploiting topological structure of graph of patches},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter efficient face frontalization in image sequences via GAN inversion. <em>IETIP</em>, <em>19</em>(1), e70003. (<a href='https://doi.org/10.1049/ipr2.70003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing facial images with varying poses is a significant challenge. Most existing face frontalization methods rely on heavy architectures that struggle with small datasets and produce low-quality images. Additionally, although video frames provide richer information, these methods typically use single images due to the lack of suitable multi-image datasets. To address these issues, a parameter-efficient framework for high-quality face frontalization in both single and multi-frame scenarios is proposed. First, a high-quality, diverse dataset is created for single and multi-image face frontalization tasks. Second, a novel single-image face frontalization method is introduced by combining GAN inversion with transfer learning. This approach reduces the number of trainable parameters by over 91% compared to existing GAN inversion methods while achieving far more photorealistic results than GAN-based methods. Finally, this method is extended to sequences of images, using attention mechanisms to merge information from multiple frames. This multi-frame approach reduces artefacts like eye blinks and improves reconstruction quality. Experiments demonstrate that this single-image method outperforms pSp, a state-of-the-art GAN inversion method, with a 0.15 LPIPS improvement and a 0.10 increase in ID similarity. This multi-frame approach further improves identity preservation to 0.87, showcasing its effectiveness for high-quality frontal-view reconstructions.},
  archive      = {J_IETIP},
  author       = {Mohammadhossein Ahmadi and Nima Kambarani and Mohammad Reza Mohammadi},
  doi          = {10.1049/ipr2.70003},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70003},
  shortjournal = {IET Image Process.},
  title        = {Parameter efficient face frontalization in image sequences via GAN inversion},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moving target detection based on improved gaussian mixture model in dynamic and complex environments. <em>IETIP</em>, <em>19</em>(1), e70001. (<a href='https://doi.org/10.1049/ipr2.70001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, background modeling has garnered significant attention for motion target detection in vision and image applications. However, most methods do not achieve satisfactory results because of the influence of background dynamics and other factors. The Gaussian mixture model (GMM) background modeling method is a popular and powerful motion background modeling technology owing to its ability to balance robustness and real-time constraints in various practical environments. However, when the background is complex and the target moves slowly, the traditional GMM cannot accurately detect the target and is prone to misjudging the moving background as a moving target. To address the interference from complex backgrounds, this study proposes a target detection method that combines an adaptive GMM with an improved three-frame difference method, along with an algorithm that combines grayscale statistics with an improved Phong illumination model for illumination compensation and shadow removal. The experimental results demonstrate that the improved method has better robustness, improves target detection accuracy, and reduces noise and background interference.},
  archive      = {J_IETIP},
  author       = {Jiaxin Li and Fajie Duan and Xiao Fu and Guangyue Niu and Rui Wang and Hao Zheng},
  doi          = {10.1049/ipr2.70001},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70001},
  shortjournal = {IET Image Process.},
  title        = {Moving target detection based on improved gaussian mixture model in dynamic and complex environments},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrative survey on indian sign language recognition and translation. <em>IETIP</em>, <em>19</em>(1), e70000. (<a href='https://doi.org/10.1049/ipr2.70000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hard of hearing (HoH) people commonly use sign languages (SLs) to communicate. They face major impediments in communicating with hearing individuals, mostly because hearing people are unaware of SLs. Therefore, it is important to promote tools that enable communication between users of sign language and users of spoken languages. The study of sign language recognition and translation (SLRT) is a step forward in this direction, as it tries to create a spoken-language translation of a sign-language video or vice versa. This study aims to survey the Indian sign language (ISL) interpretation literature and gives pertinent information about ISL recognition and translation (ISLRT). It provides an overview of recent advances in ISLRT, including the use of machine learning based, deep learning based, and gesture-based techniques. This work also summarizes the development of ISL datasets and dictionaries. It highlights the gaps in the literature and provides recommendations for future research opportunities for ISLRT development.},
  archive      = {J_IETIP},
  author       = {Rina Damdoo and Praveen Kumar},
  doi          = {10.1049/ipr2.70000},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e70000},
  shortjournal = {IET Image Process.},
  title        = {An integrative survey on indian sign language recognition and translation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of crowd density estimation and counting. <em>IETIP</em>, <em>19</em>(1), e13328. (<a href='https://doi.org/10.1049/ipr2.13328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is one of the important and challenging research topics in computer vision. In recent years, with the rapid development of deep learning, the model architectures, learning paradigms, and counting accuracy have undergone significant changes. To help researchers quickly understand the research progress in this area, this paper presents a comprehensive survey of crowd density estimation and counting approaches. Initially, the technical challenges and commonly used datasets are intoroduced for crowd counting. Crowd counting approaches is them categorized into two groups based on the feature extraction methods employed: traditional approaches and deep learning-based approaches. A systematic and focused analysis of deep learning-based approaches is proposed. Subsequently, some training and evaluation details are introduced, including labels generation, loss functions, supervised training methods, and evaluation metrics. The accuracy and robustness of selected classical models are further compared. Finally, future prospects, strategies, and challenges are discussed for crowd counting. This review is comprehensive and timely, stemming from the selection of prominent and unique works.},
  archive      = {J_IETIP},
  author       = {Mingtao Wang and Xin Zhou and Yuanyuan Chen},
  doi          = {10.1049/ipr2.13328},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13328},
  shortjournal = {IET Image Process.},
  title        = {A comprehensive survey of crowd density estimation and counting},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optical gas imaging and deep learning for quantifying enteric methane emissions from rumen fermentation in vitro. <em>IETIP</em>, <em>19</em>(1), e13327. (<a href='https://doi.org/10.1049/ipr2.13327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigated the possibility of using a laser methane detector (LMD) and optical gas imaging (OGI) to detect and quantify enteric methane ( CH 4 ${\rm CH}_4$ ) produced by ruminants in vitro. Four single-flow continuous fermenters were used for rumen culture incubation with four different treatment diets: Control (50:50 forage to concentrate [F:C] ratio), Control + Bromoform (CBR), Low Forage (LF; 20:80), and High Forage (HF; 80:20). After 10 days of incubation, all fermenter contents were transferred and used in a 24 h ANKOM batch culture to measure gas production with LMD and OGI. The authors introduce the Controlled Diet (CD) dataset, a large-scale collection of 4,885 plume images captured using an FLIR GF77 OGI camera under varying dietary conditions. The performance of six semantic segmentation models (FCN, U-Net, Vision Transformer, Swin Transformer, DeepLabv3+, and Gasformer) on the CD dataset is compared. Results showed that LMD data for followed a similar pattern to the gas chromatography (GC) instrument results. The in vitro results showed that different diets and F:C ratios had an impact on gas production and rumen fermentation characteristics. Adding bromoform to the control diet fully inhibited emission. The HF diet produced more compared to all treatments ( ) when measured with GC and LMD. CBR produced the lowest values when measured with GC and LMD. The Gasformer architecture achieved the highest performance with mean IoU of 85.1% and mean F-score of 91.72%. These findings demonstrate that OGI technology combined with advanced semantic segmentation models offers a promising solution for predicting and quantifying emissions in the livestock sector, potentially aiding in the development of mitigation strategies to combat climate change.},
  archive      = {J_IETIP},
  author       = {Mohamed G. Embaby and Toqi Tahamid Sarker and Amer AbuGhazaleh and Khaled R. Ahmed},
  doi          = {10.1049/ipr2.13327},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13327},
  shortjournal = {IET Image Process.},
  title        = {Optical gas imaging and deep learning for quantifying enteric methane emissions from rumen fermentation in vitro},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-learning weight network based on label distribution training for facial expression recognition. <em>IETIP</em>, <em>19</em>(1), e13326. (<a href='https://doi.org/10.1049/ipr2.13326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent widespread utilization of facial expression recognition (FER) has garnered significant attention in the affective computing field. To address the issue of dominant features being suppressed during feature fusion in FER, this study proposes a self-learning weight network based on label distribution training (SLW-LDT). First, based on the ShuffleNet-V2 backbone model, SLW-LDT introduced a local feature extraction branch that highlights specific expression-related features by cropping the facial image into four local regions. Subsequently, the SLW algorithm is devised to allocate learnable weights to global and local features from different branches before their fusion. Moreover, considering the challenge associated with accessing emotional distribution in facial images directly, a label distribution training module (LDT) is introduced during the training phase to generate label distributions for effective training purposes. Experimental results demonstrate that the proposed method achieves accuracies of 89.77% and 64.21% on two in-the-wild datasets (RAF-DB and AffectNet-7), and 98.90% on the lab-controlled CK+ dataset. Comparative analysis against state-of-the-art methods reveals slight improvements in recognition accuracy along with robust performance exhibited by the model.},
  archive      = {J_IETIP},
  author       = {Yangbo Chen and Chunyan Peng and Xuan Wang and Yuhui Zheng},
  doi          = {10.1049/ipr2.13326},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13326},
  shortjournal = {IET Image Process.},
  title        = {Self-learning weight network based on label distribution training for facial expression recognition},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OLODN: An efficient lightweight people detection method for occlusion and crowding scenarios. <em>IETIP</em>, <em>19</em>(1), e13325. (<a href='https://doi.org/10.1049/ipr2.13325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the real-time object detection problem for devices with limited computing resources in densely populated and occluded scenarios, a novel occlusion-aware lightweight object detection network (OLODN) is proposed. This network integrates innovative components to significantly enhance detection efficiency while maintaining high accuracy. Firstly, OLODN employs FasterNet blocks and reparameterised generalised-FPN to reduce computational complexity and preserve feature extraction and fusion capabilities. Secondly, a reinforced coordination attention mechanism is designed to strengthen the network's ability to capture occlusion boundary information. Additionally, a spatial pyramid pooling feature concatenation module is introduced to integrate multi-scale features and enhance the algorithm's robustness to occlusions. Lastly, OLODN adopts a task-aligned one-stage object detection strategy, optimising the anchor alignment of classification and localisation tasks, effectively improving detection accuracy under occluded conditions. Experiments demonstrate the superiority of the proposed method. On the WiderPerson dataset, OLODN achieved a recall rate of 68.0%, which is 2.8% higher than YOLOv11's 65.2%, while running at 35.3 frames per second (f/s) on CPU and 76.6 f/s on GPU, faster than YOLOv11's 34.1 f/s and 74.5 f/s by 1.2 f/s and 2.1 f/s, respectively.},
  archive      = {J_IETIP},
  author       = {Wei Sheng and Mingjian Liu and Xiang Li and Mingbao Zhang},
  doi          = {10.1049/ipr2.13325},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13325},
  shortjournal = {IET Image Process.},
  title        = {OLODN: An efficient lightweight people detection method for occlusion and crowding scenarios},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Face de-morphing based on identity feature transfer. <em>IETIP</em>, <em>19</em>(1), e13324. (<a href='https://doi.org/10.1049/ipr2.13324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face morphing attacks have emerged as a significant security threat, compromising the reliability of facial recognition systems. Despite extensive research on morphing detection, limited attention has been given to restoring accomplice face images, which is critical for forensic applications. This study aims to address this gap by proposing a novel face de-morphing (FD) method based on identity feature transfer for restoring accomplice face images. The method encodes facial attribute and identity features separately and employs cross-attention mechanisms to extract identity features from morphed faces relative to reference images. This process isolates and enhances the accomplice's identity features. Additionally, inverse linear interpolation is applied to transfer identity features to attribute features, further refining the restoration process. The enhanced identity features are then integrated with the StyleGAN generator to reconstruct high-quality accomplice facial images. Experimental evaluations on two morphed face datasets demonstrate the effectiveness of the proposed approach, improving the average restoration accuracy by at least 5% compared with other methods. These findings highlight the potential of this approach for advancing forensic and security applications.},
  archive      = {J_IETIP},
  author       = {Le-Bing Zhang and Song Chen and Min Long and Juan Cai},
  doi          = {10.1049/ipr2.13324},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13324},
  shortjournal = {IET Image Process.},
  title        = {Face de-morphing based on identity feature transfer},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning-based method for detecting and identifying surface defects in polyimide foam. <em>IETIP</em>, <em>19</em>(1), e13323. (<a href='https://doi.org/10.1049/ipr2.13323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the detection and identification of surface defects in polyimide foam products mainly rely on on-site work experience, which has issues such as low detection accuracy, strong subjectivity, and low efficiency. Existing research on foam product defect detection primarily targets internal defects, lacking studies on the detection, identification, and classification of surface defects. Therefore, this article proposes a method for identifying and classifying surface defects in polyimide foam based on an improved GoogLeNet, aiming to quickly and accurately detect and identify surface defects in foam products. By optimizing the Inception blocks, introducing the ECA attention mechanism, and adding an LSTM network module, the model's recognition accuracy and generalization ability are effectively improved. In experiments, the model proposed in this article performed excellently on the foam surface defect dataset, showing a significant advantage in detection accuracy compared to other convolutional neural network models. The detection accuracy for pits and cracks reached 98.24% and 98.25%, respectively, providing a reliable reference for the detection of surface defects in industrial foam production.},
  archive      = {J_IETIP},
  author       = {Xianhui Song and Guangzhong Hu and Jing Lu and Xianguo Tuo and Yuedong Li},
  doi          = {10.1049/ipr2.13323},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13323},
  shortjournal = {IET Image Process.},
  title        = {A deep learning-based method for detecting and identifying surface defects in polyimide foam},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning optical flow from spiking camera with direction disassembly. <em>IETIP</em>, <em>19</em>(1), e13322. (<a href='https://doi.org/10.1049/ipr2.13322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional optical flow estimation methods typically recover two-dimensional motion from RGB image sequences. Recently, due to the rise and widespread use of spike cameras, learning optical flow from spiking cameras has become a hot topic in the field of two-dimensional motion estimation. Although existing methods have been designed to learn optical flow by designing feature processing methods for spike streams, there is still insufficient consideration for flow field post-processing, resulting in limited accuracy of optical flow estimation. To address this problem, an optical flow estimation method based on directional disassembly is proposed. Specifically, the estimated flow fields along the horizontal and vertical directions are disassembled and the motion vectors along the two directions are denoised separately to reduce the burden of post-processing for complex two-dimensional motion information. In addition, contextual information is introduced in the post-processing so that the scene information can effectively contribute to the results of the flow post-processing. Experimental results show that this proposed method is capable of achieving comparable performance on spike-based public datasets.},
  archive      = {J_IETIP},
  author       = {Mingliang Zhai and Mingming Jiang and Xuezhi Xiang and Kang Ni and Hao Gao},
  doi          = {10.1049/ipr2.13322},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13322},
  shortjournal = {IET Image Process.},
  title        = {Learning optical flow from spiking camera with direction disassembly},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv7-SFWC: A detection algorithm for illegal manned trucks. <em>IETIP</em>, <em>19</em>(1), e13321. (<a href='https://doi.org/10.1049/ipr2.13321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic analysis and evidence collection of obvious traffic violations, such as illegal manned trucks, is one of the critical operational challenges of the traffic police department's business. For the enormous volume of road surveillance images generated daily, traditional manual screening is highly time-intensive and resource-draining. Therefore, this article proposes an improved detection model YOLOv7-SFWC for illegally manned trucks. First of all, the pictures of illegal manned vehicles obtained by relevant departments are expanded and labeled, and the dataset of illegal manned vehicles is created. Building upon the foundational YOLOv7 model, this study replaces the traditional convolution module with the FasterNet convolution module and SCConv module, and introduces the Wise-IoU (WIoU) loss function algorithm and Coordinate Attention (CA) mechanism. The results show that the mAP value of the YOLOv7-SFWC model is improved by 4.15% and FPS by 7.6 compared with the original YOLOv7 model, and the computational complexity is reduced to adapt to the deployment. Moreover, the model's effectiveness is validated through extensive comparison experiments. Finally, the visual results show the accurate performance of the model and verify the progress of YOLOv7-SFWC. This advancement has the potential to transform traffic violation enforcement by reducing reliance on manual screening, effectively combating traffic violations, and purifying traffic order.},
  archive      = {J_IETIP},
  author       = {Xuan Wu and Yanan Wang and Tengtao Nie and Wenlin Pan},
  doi          = {10.1049/ipr2.13321},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13321},
  shortjournal = {IET Image Process.},
  title        = {YOLOv7-SFWC: A detection algorithm for illegal manned trucks},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCFNet: Research on small target detection of RGB-infrared under UAV perspective with multi-scale complementary feature fusion. <em>IETIP</em>, <em>19</em>(1), e13320. (<a href='https://doi.org/10.1049/ipr2.13320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of single-modal image target detection from a drone's perspective faces multiple challenges such as large image sizes, small and dense targets, insufficient lighting conditions, and hardware resource constraints, all of which affect the accuracy and real-time performance of algorithms. In response, a small target detection algorithm that fuses multi-scale complementary features, named MCFNet is proposed. Firstly, in order to independently extract the unimodal features of visible light and infrared images and effectively enhance the complementary information between images, a dual-stream backbone network with a terminal fusion mechanism is proposed. Secondly, the complementary feature information residual module is utilized to optimize the integration process of residual features. Subsequently, by designing a multi-scale feature enhancement module the network's capability to capture multi-scale features is enhanced. Finally, for targets of varying sizes, a lightweight Transformer feature extraction module is proposed to improve the detection accuracy of small targets from a drone's perspective. Test results on the drone-vehicle dataset show that this method achieved an average detection accuracy of 67.92%, while the detection accuracy on the self-constructed UAV-data dataset reached 96.4%. Additionally, a series of ablation experiments validated the effectiveness of the different modules.},
  archive      = {J_IETIP},
  author       = {Jing Jing and Jian Feng Hu and Zuo Peng Zhao and Ying Liu},
  doi          = {10.1049/ipr2.13320},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13320},
  shortjournal = {IET Image Process.},
  title        = {MCFNet: Research on small target detection of RGB-infrared under UAV perspective with multi-scale complementary feature fusion},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on tea buds detection based on optimized YOLOv5s. <em>IETIP</em>, <em>19</em>(1), e13319. (<a href='https://doi.org/10.1049/ipr2.13319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the world's most popular beverages, tea plays a significant role in improving tea production efficiency and quality through the identification of tea shoots during the tea manufacturing process. However, due to the complex morphology, small size, and susceptibility to factors like lighting and obstruction, traditional identification methods suffer from low accuracy and efficiency. In this study, image enhancement techniques such as HSV transformation, horizontal flipping, and vertical flipping were applied to the training dataset to improve model robustness and enhance generalization across varying lighting and angles. To address these challenges in the context of tea buds detection, deep-learning-based object detection methods have emerged as promising solutions. Nevertheless, current object detection technologies still face limitations when detecting tea buds under these conditions. To enhance identification performance, this article proposed an improved YOLOv5s (You Only Look Once version 5 small model) algorithm. In the improved YOLOv5s algorithm, CBAM, SE, and CA attention mechanisms were incorporated into the backbone network to augment feature extraction, and a weighted Bidirectional Feature Pyramid Network (BiFPN) is employed in the neck network to boost performance, resulting in the YOLOv5s_teabuds model. Experimental results indicated that the improved model significantly outperformed the original in terms of precision, recall, mAP and F1-score, with the CA attention mechanism providing the most notable improvement—enhancing precision, recall, mAP and F1-score by 18.119%, 9.633%, 16.496% and 13.524%, respectively. After integrating BiFPN, the YOLOv5s_teabuds model further strengthened performance and robustness, with precision, recall, mAP and F1-score increased by 19.346%, 11.388%, 18.620%, and 15.059%, respectively. Experimental results prove that the optimized YOLOv5s model can provide a real-time, high-precision tea buds detection method for robotic harvesting.},
  archive      = {J_IETIP},
  author       = {Guanli Li and Jianqiang Lu and Dong Zhang and Zhongyi Guo},
  doi          = {10.1049/ipr2.13319},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13319},
  shortjournal = {IET Image Process.},
  title        = {Research on tea buds detection based on optimized YOLOv5s},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image inpainting with aggregated convolution progressive network. <em>IETIP</em>, <em>19</em>(1), e13318. (<a href='https://doi.org/10.1049/ipr2.13318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images can be corrupted during capture or transmission due to clouds, overlaps, and other interferences, deviating from their original state. Image inpainting techniques restore such images, but different types—Synthetic Aperture Radar (SAR), RGB, and infrared—require varying field-of-view sizes. SAR and infrared images, with less information, need a larger field of view, leading to uncorrelated interference in distant areas. RGB images, richer in information, are constrained by a limited local field of view, hindering access to full semantic details. To address these challenges, an aggregated convolution progressive network is proposed. This model employs a coarse-grained inpainting module for initial restoration, enhanced by an aggregated convolution module to capture contextual information. Local and global details are then used to refine the output, improving restoration quality. Additionally, existing datasets predominantly focus on RGB images, lacking diversity. To bridge this gap, a comprehensive dataset covering SAR, RGB, and infrared images under cloud, overlap, and corruption conditions is constructed. This method achieves superior performance, with MAE of 0.05, SSIM of 0.95, and PSNR of 36.68 within a 20–30% mask size range, outperforming state-of-the-art techniques across diverse image types and size ranges. Experimental results validate its effectiveness in advancing image inpainting.},
  archive      = {J_IETIP},
  author       = {Yang Li and Jia Zhai and Wen Lu and Haipeng Guo and JiaZheng Wen and Huanyu Liu and Junbao Li},
  doi          = {10.1049/ipr2.13318},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13318},
  shortjournal = {IET Image Process.},
  title        = {Image inpainting with aggregated convolution progressive network},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the registration of infrared and visible images based on phase consistency and edge extreme points. <em>IETIP</em>, <em>19</em>(1), e13317. (<a href='https://doi.org/10.1049/ipr2.13317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the significant differences in the imaging principles of infrared images and visible images, the feature points and descriptors between the two cannot be effectively matched directly by traditional feature extraction methods such as SIFT. To solve this problem, this study proposes a registration algorithm for infrared and visible images based on phase information and edge information. The algorithm extracts the feature points of the infrared image and the visible image through the principle of phase agreement and the edge binary map of the image and then calculates the descriptors of the gradient images of the infrared image and the visible image, and the descriptor calculation draws on some SIFT principles. Finally, the cosine similarity was used to match the feature points, and the improved random sample consensus algorithm was used to screen out the correct registration points. Experiments show that this method can effectively register between infrared and visible images and is also suitable for the registration of infrared and visible images with different rotation angles and similar structures.},
  archive      = {J_IETIP},
  author       = {Jie Li and Rougang Zhou and Zhenchao Ruan and Chou Jay Tsai Chien and Junjie Zhu},
  doi          = {10.1049/ipr2.13317},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13317},
  shortjournal = {IET Image Process.},
  title        = {Research on the registration of infrared and visible images based on phase consistency and edge extreme points},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-teacher-guided pseudo label supervision: A semi-supervised learning framework for muscle and adipose tissue segmentation on chest CT scans. <em>IETIP</em>, <em>19</em>(1), e13316. (<a href='https://doi.org/10.1049/ipr2.13316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of muscle and adipose tissue in chest CT scans is essential for the diagnosis, treatment, and prognosis of various diseases. However, this task is hindered by the limited availability of annotated data. This study proposes a novel semi-supervised learning framework, co-teacher-guided pseudo-label supervision (CTGP), to address this challenge. CTGP combines co-training and the Mean-Teacher strategy, where predictions generated by teacher models are filtered and utilized as high-quality pseudo-labels to train the other student models, thus facilitating co-training. Additionally, a medical image-specific augmentation method, MIAugment, is introduced to better adapt to the unique characteristics of medical data. Experiments on a real-life clinical dataset demonstrate that CTGP achieves high segmentation accuracy with minimal labelled data. Using only 10% of labelled data, the framework achieves a mean Dice Similarity Coefficient of 90.03% for four tissue types, a decrease of just 2.85% compared to fully supervised learning. This approach provides a promising solution for automated muscle and adipose tissue segmentation with limited annotations.},
  archive      = {J_IETIP},
  author       = {Jie Yang and Yanli Liu and Xiaoyan Chen and Tianle Chen and Qi Liu},
  doi          = {10.1049/ipr2.13316},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13316},
  shortjournal = {IET Image Process.},
  title        = {Co-teacher-guided pseudo label supervision: A semi-supervised learning framework for muscle and adipose tissue segmentation on chest CT scans},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bootstrapping vision–language transformer for monocular 3D visual grounding. <em>IETIP</em>, <em>19</em>(1), e13315. (<a href='https://doi.org/10.1049/ipr2.13315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of 3D visual grounding using monocular RGB images, it is a challenging problem to perceive visual features and accurately predict the localization of 3D objects based on given geometric and appearances descriptions. Traditional text-guided attention-based methods have achieved better results than baselines, but it is argued that there is still potential for improvement in the area of multi-modal fusion. Thus, Mono3DVG-TRv2, an end-to-end transformer-based architecture that employs a visual-text multi-modal encoder for the alignment and fusion of multi-modal features, incorporating an enhanced transformer module proven in 2D detection, is introduced. The depth features predicted by the multi-modal features and the visual-text features are associated with the learnable queries in the decoder, facilitating more efficient and effective acquisition of geometric information in intricate scenes. Following a comprehensive comparison and ablation study on the Mono3DRefer dataset, this method achieves state-of-the-art performance, markedly surpassing the prior approach. The code will be released at https://github.com/Jade-Ray/Mono3DVGv2 .},
  archive      = {J_IETIP},
  author       = {Qi Lei and Shijie Sun and Xiangyu Song and Huansheng Song and Mingtao Feng and Chengzhong Wu},
  doi          = {10.1049/ipr2.13315},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13315},
  shortjournal = {IET Image Process.},
  title        = {Bootstrapping vision–language transformer for monocular 3D visual grounding},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-tiny: A lightweight small object detection algorithm for UAV aerial imagery. <em>IETIP</em>, <em>19</em>(1), e13314. (<a href='https://doi.org/10.1049/ipr2.13314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unmanned aerial vehicle (UAV) aerial target detection tasks, two main challenges exist: the limited computational resources of UAV terminals, which are not conducive to running complex models, and the prevalence of small targets, which can easily lead to missed detections and false positives. To address these issues, this study proposes a lightweight and high-accuracy small-object detection algorithm for UAV aerial imagery that is based on YOLOv5s. First, the network structure is optimized by removing the layers in YOLOv5s primarily used for detecting large targets (P4 and P5) and adding layers primarily used for detecting small targets (P2 and P3). This enables the model to focus more on extracting small-object features. A lightweight dynamic convolution is subsequently introduced in the C3 module, and the lightweight LW_C3 and LW_downsampling modules are designed for feature extraction and downsampling operations. This enhances the model's feature extraction capability while achieving a lightweight design. Finally, the adaptive multi-scale spatial feature fusion (AMSFF) module is designed to adaptively learn the spatial weights of the feature maps at different levels, thereby further strengthening the effective fusion of multi-scale features. Experimental results show that the improved YOLO-Tiny model has higher accuracy and lower complexity, hence validating its excellent performance.},
  archive      = {J_IETIP},
  author       = {Fei Feng and Lu Yang and Quanxing Zhou and Weipeng Li},
  doi          = {10.1049/ipr2.13314},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13314},
  shortjournal = {IET Image Process.},
  title        = {YOLO-tiny: A lightweight small object detection algorithm for UAV aerial imagery},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved conditional diffusion model for image super-resolution. <em>IETIP</em>, <em>19</em>(1), e13313. (<a href='https://doi.org/10.1049/ipr2.13313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have achieved remarkable success in image super-resolution by addressing issues such as transition smoothing, insufficient high-frequency information, and training instability encountered in regression-based and GAN-based models. However, challenges persist when applying diffusion models to image super-resolution, including randomness, inadequate conditional information, high computational costs, and network architecture complexities. In this article, the authors introduce a diffusion model based on Mean-Reverting Stochastic Differential Equations (SDE), and propose the use of ENAFBlocks instead of traditional ResBlocks to enhance model performance in noise prediction. The Mean-Reverting SDE effectively mitigates the randomness of the diffusion model by leveraging low-resolution images as means. Additionally, an LR Encoder is introduced to capture hidden information from LR images, providing a more robust condition for stable result generation by the noise predictor. To efficiently handle high-resolution images within limited GPU memory, the method employs adaptive aggregate sampling, which merges overlapping regions smoothly using weighted averaging. Furthermore, color variations are addressed during diffusion model sampling through color correction. Extensive experiments on CelebA, DIV2K, and Urban100 demonstrate that the method outperforms state-of-the-art diffusion models like IDM, with a PSNR improvement of 0.22 dB, FID reduction of 2.35, and LPIPS reduction of 0.05 on the DIV2K dataset, along with a reduced parameter count and faster inference time.},
  archive      = {J_IETIP},
  author       = {Rui Wang and Ningning Zhou},
  doi          = {10.1049/ipr2.13313},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13313},
  shortjournal = {IET Image Process.},
  title        = {Improved conditional diffusion model for image super-resolution},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive lucky imaging method for turbulence-degraded image restoration. <em>IETIP</em>, <em>19</em>(1), e13312. (<a href='https://doi.org/10.1049/ipr2.13312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When capturing distant targets, the video sequence images are affected by atmospheric turbulence, resulting in distortion and blur. In order to restore the degraded images due to atmospheric turbulence in video sequences, this article combines lucky imaging with generative adversarial networks for the first time. The idea of lucky imaging is employed to eliminate geometric distortions, followed by the use of generative adversarial networks to address the blur issue. Additionally, an adaptive restoration method targeting turbulence intensity is proposed to improve the computational efficiency of the proposed approach. Experimental results demonstrate that the combined restoration method of lucky imaging and generative adversarial networks outperforms classical lucky imaging. Specifically, compared to classical lucky imaging, the Brenner gradient function, Laplacian gradient function, Spatial Median Difference (SMD), Entropy, Energy gradient function, PIQE, and Brisque indicators improve by 7.7%, 13.1%, 3.6%, 4.1%, 2.1%, 26.6% and 21.54% (all evaluation indicators in the above improvement rates have undergone logarithmic transformation), respectively. Meanwhile, the proposed adaptive restoration method can improve efficiency by 28%, with greater efficiency gains observed with larger datasets.},
  archive      = {J_IETIP},
  author       = {Pin Lv and Tiezhu Shi and Dongping Den and Mengdi Wang and Qian Liu and Guofeng Wu},
  doi          = {10.1049/ipr2.13312},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13312},
  shortjournal = {IET Image Process.},
  title        = {An adaptive lucky imaging method for turbulence-degraded image restoration},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-powered automated analysis of bone scans: A survey. <em>IETIP</em>, <em>19</em>(1), e13311. (<a href='https://doi.org/10.1049/ipr2.13311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the key techniques of artificial intelligence, deep learning has emerged as an effective approach for analysing medical images. Various imaging techniques including the planar bone scintigraphy, single photon emission computed tomography and PET can be used to evaluate, in vivo, bone conditions. The introduction of deep learning techniques especially the convolutional neural networks can significantly improve diagnosis accuracy and efficiency of nuclear medicine physicians. Focusing on bone scans acquired by various nuclear medicine imaging techniques, his paper reviews existing work on deep learning-based classification, segmentation and object detection of bone scans. Specifically, an overview of existing work about research objective is presented, deep learning models are adopted, and main results are achieved. Research challenges and directions for developing automated analysis of bone scans with deep learning techniques are then discussed.},
  archive      = {J_IETIP},
  author       = {Qiang Lin and Yang He and Sihan Guo},
  doi          = {10.1049/ipr2.13311},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13311},
  shortjournal = {IET Image Process.},
  title        = {AI-powered automated analysis of bone scans: A survey},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal sentiment analysis based on multi-stage graph fusion networks under random missing modality conditions. <em>IETIP</em>, <em>19</em>(1), e13310. (<a href='https://doi.org/10.1049/ipr2.13310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary challenge of the multimodal sentiment analysis (MSA) task is the modal fusion, and the lack of modalities may exist in the fusion process, which leads to poor prediction results. Most of the previous research on multimodal fusion is single-stage fusion, disregarding how various modality subsets interact, as well as rarely considering relative position relationship of modality sequences, causing the fragmentation of context info. Considering the aforementioned issues, this study introduces an MSA method based on the multi-stage graph fusion network (MSGFN) under random missing modality conditions to improve the robustness of the model to MSA under the random missing modality conditions. Firstly, for each modality, its inter-modal and intra-modal multi-head attention are used to learn robust representation of the modality sequence. Meanwhile, the relative position encoding (RPE) is introduced into mechanism of attention that enables model to perceive and learn the relative position before and after the modality sequence when calculating attention, thereby better understanding the contextual info of the sequence. After that, the transformer encoder receives the learned modality features and uses the pre-trained model to supervise the reconstruction of the missing modality information. Finally, the feature representations of different modalities have effectively fused using multi-stage graph fusion network, and the output is used for the ultimate sentiment classification. Wide experiments are conducted on two publicly available datasets, CMU-MOSI and IEMOCAP, and the findings indicate that the proposed method can better handle the challenges caused by modality fusion and modality missing compared with several baseline methods.},
  archive      = {J_IETIP},
  author       = {Ting Zhang and Bin Song and Zhiyong Zhang and Yajuan Zhang},
  doi          = {10.1049/ipr2.13310},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13310},
  shortjournal = {IET Image Process.},
  title        = {Multimodal sentiment analysis based on multi-stage graph fusion networks under random missing modality conditions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified multi-class anomaly detection model based on reverse distillation. <em>IETIP</em>, <em>19</em>(1), e13309. (<a href='https://doi.org/10.1049/ipr2.13309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-class single-model detection paradigm is a prevalent design for industrial anomaly detection, exhibiting suitability for varying industrial classes and dynamic, flexible production scenarios. This approach not only enhances model adaptability but also minimizes maintenance costs. However, the current popular methods are susceptible to the ‘copying shortcut’ phenomenon, which constrains their performance on benchmark datasets. To overcome this limitation, this article proposes a multi-class anomaly detection model: UniRD, based on the reverse distillation method. This model creates an image corruptor that expands the dataset and generates ‘normal-corrupt’ image pairs. During the training process, their correspondence is used to optimize the reverse distillation. This process greatly exploits and releases the potential of the student decoder. Furthermore, a teacher feature adaptation module is devised to enhance the compatibility between the pre-trained model and the anomaly detection task. This has the effect of reducing the discrepancy between teacher and student features while ensuring the consistency of normal sample features. The comprehensive evaluation results in two mainstream datasets, MVTec and VisA, and demonstrates that the proposed method exhibits improvement in all indicators compared to benchmark methods. The proposed method attains the state-of-the-art, substantiating its effectiveness and advancement.},
  archive      = {J_IETIP},
  author       = {Maoli Fu and Zhongliang Fu},
  doi          = {10.1049/ipr2.13309},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13309},
  shortjournal = {IET Image Process.},
  title        = {A unified multi-class anomaly detection model based on reverse distillation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-rigid point cloud registration method based on scene flow estimation. <em>IETIP</em>, <em>19</em>(1), e13308. (<a href='https://doi.org/10.1049/ipr2.13308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is an important aspect of computer vision, encompassing various applications. However, many existing algorithms neglect the registration of non-rigid point cloud. Recent studies have focused on scene flow, which can achieve non-rigid point cloud registration by estimating the scene flow of two point clouds. The presence of occlusion in a scene is primarily attributed to variations in viewpoint and access time, thereby posing a significant challenge in accurately predicting scene flow. In the endeavour to mitigate this issue, the focus is on the propagation of scene flow originating from non-occluded points towards occlusion points while concurrently estimating the occlusion map. The proposed network, a non-rigid point cloud registration method based on scene flow estimation, achieves exceptional performance for the EPE3D metric on the FlyingThings3D and KITTI scene flow datasets, and it demonstrates strong generalization on the railroad dataset.},
  archive      = {J_IETIP},
  author       = {Xiaopeng Deng and Kai Yang and Yong Wang and Jinlong Li and Liming Xie},
  doi          = {10.1049/ipr2.13308},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13308},
  shortjournal = {IET Image Process.},
  title        = {A non-rigid point cloud registration method based on scene flow estimation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Salt-and-pepper denoising based on lightweight convolutional neural networks for flexible AMOLED. <em>IETIP</em>, <em>19</em>(1), e13307. (<a href='https://doi.org/10.1049/ipr2.13307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the performance of the image preprocessing module in consumer electronics using an active-matrix organic light-emitting diode display panel, the concept of judging before processing for salt-and-pepper denoising is originally proposed. Firstly, a dataset for salt-and-pepper noise image classification is constructed, and a convolutional neural network (CNN) for judging noise image (CNN-J) is trained. Image classified as normal by CNN-J is not processed, while the classified noisy image is denoised. In the denoising process, a marking image and a rough denoised image are generated by CNN for noise mask (CNN-M) and CNN for denoising (CNN-D), respectively. Subsequently, the refined denoised image is output using the proposed refining mechanism. The middle layers of CNN-M and CNN-D are constructed by depth-separable CNN to reduce the network complexity. Experimental results show that the misjudging rate of CNN-M marking is reduced by 19.94% compared with the best existing marking method. Compared with the traditional methods, the peak signal to noise ratio of the proposed method is increased by 2.95% and the information loss is reduced by 21.46%. In addition, the computational complexity is at least 11.18% lower than that of the traditional CNN. Finally, the display of salt-and-pepper denoised images on the flexible AMOLED is realized.},
  archive      = {J_IETIP},
  author       = {Chengqiang Huang and Yanjun Yang and Yinghu He},
  doi          = {10.1049/ipr2.13307},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13307},
  shortjournal = {IET Image Process.},
  title        = {Salt-and-pepper denoising based on lightweight convolutional neural networks for flexible AMOLED},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying the retinal layers from optical coherence tomography images using a 3D segmentation method. <em>IETIP</em>, <em>19</em>(1), e13306. (<a href='https://doi.org/10.1049/ipr2.13306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel automated method for segmenting retinal layers in three-dimensional (3D) space from spectral domain optical coherence tomography (SD-OCT) images. Compared to 2D segmentation, 3D segmentation uses more data and produces findings that are more accurate and reliable. The class-specific area of interest (ROI) choice and three important reference class approximations make the suggested technique precise, effective, and reliable. In the first step, contours are detected based on gradient intensity. To choose a smaller region of interest (ROI), the second stage entails acquiring the identified boundary neighbour B scan data for the selected ROI by categorising the problem as a graph problem. The third stage involves locating edge pixels using Canny Edge Detection from nodes. In order to calculate the edge weight of a histogram, slope similarity to the reference line and node characteristics are considered. The fourth phase boundary is precisely found by Dijkstra's shortest path algorithm. The accuracy of the method was tested based on 288 B scans of 12 patients (ten normal macular degeneration (AMD) subjects and 2 age-related subjects from two different institutions). Five recent automated procedures are compared with the results to further validate the findings of the fifth phase. The outcomes demonstrate a mean original mean square error (RMSE) for each of the cut-off values, which are 2.82, 4.88, 2.03, 3.77, and 0.64 pixels, respectively. As can be seen, the suggested strategy outperforms the existing models' significantly with a return on investment of 0.26 pixels.},
  archive      = {J_IETIP},
  author       = {Akter Hossain and Andrea Giani and Victor Chong and Sobha Sivaprasad and Tasin R. Bhuiyan and Theodore Smith and Manaswini Pradhan and Alauddin Bhuiyan},
  doi          = {10.1049/ipr2.13306},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13306},
  shortjournal = {IET Image Process.},
  title        = {Identifying the retinal layers from optical coherence tomography images using a 3D segmentation method},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the object detection in complex scenes using diagonal distance. <em>IETIP</em>, <em>19</em>(1), e13305. (<a href='https://doi.org/10.1049/ipr2.13305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection method based on IoU boundary box is an important method of image recognition. This paper proposes a diagonal distance loss function, L DDIoU ${\mathcal {L}_{\text{DDIoU}}}$ , and a combined diagonal distance loss function, L CDIoU ${\mathcal {L}_{\text{CDIoU}}}$ , based IoU. These functions use the distance between diagonals as a penalty, overcoming the degradation phenomenon of prior functions based on traditional IoU. The altered loss functions are also integrated into faster R-CNN and SSD models, and their effectiveness is assessed on PASCAL VOC, MS COCO and SSDD data sets, yielding favourable results. This paper also extends the IoU calculation between two boxes when performing non-maximum suppression to advance position precision. The experiments have demonstrated that the enhanced distance calculation method improves both loss function and non-maximum suppression calculations by around .},
  archive      = {J_IETIP},
  author       = {Dan Guo and Guoliang He},
  doi          = {10.1049/ipr2.13305},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13305},
  shortjournal = {IET Image Process.},
  title        = {Research on the object detection in complex scenes using diagonal distance},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv7-PSAFP: Crop pest and disease detection based on improved YOLOv7. <em>IETIP</em>, <em>19</em>(1), e13304. (<a href='https://doi.org/10.1049/ipr2.13304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of pests and diseases in crops is currently a hot topic. The complexity of pest and disease object in the field, combined with inconsistent features across different levels, poses challenges for network detection. Additionally, the complex agricultural production environment tends to generate many interfering negative samples, which significantly complicates pest and disease differentiation. To address these two issues, the YOLOv7-PSAFP network structure was first proposed. Based on YOLOV7, the progressive Spatial Adaptive Feature Pyramid (PSAFP) was introduced. Second, a combination of the Varifocal Loss and Loss Rank Mining loss functions was used for calculating the object loss, which reduces the interference of useless negative examples during training. On the filtered-plant-village-dataset and rice-corn pest dataset, the mAP results of YOLOv7-PSAFP were 84.7 and 93.3 , which are 2.9 and 2.1 higher than the baseline model (YOLOv7), respectively. The code for this paper is located at https://github.com/DuLJ72/PSAFP .},
  archive      = {J_IETIP},
  author       = {Lujia Du and Junlong Zhu and Muhua Liu and Lin Wang},
  doi          = {10.1049/ipr2.13304},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13304},
  shortjournal = {IET Image Process.},
  title        = {YOLOv7-PSAFP: Crop pest and disease detection based on improved YOLOv7},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swin2-MoSE: A new single image supersolution model for remote sensing. <em>IETIP</em>, <em>19</em>(1), e13303. (<a href='https://doi.org/10.1049/ipr2.13303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations of current optical and sensor technologies and the high cost of updating them, the spectral and spatial resolution of satellites may not always meet desired requirements. For these reasons, Remote-Sensing Single-Image Super-Resolution (RS-SISR) techniques have gained significant interest. In this paper, Swin2-MoSE model is proposed, an enhanced version of Swin2SR. The model introduces MoE-SM, an enhanced Mixture-of-Experts (MoE) to replace the Feed-Forward inside all Transformer block. MoE-SM is designed with Smart-Merger, and new layer for merging the output of individual experts, and with a new way to split the work between experts, defining a new per-example strategy instead of the commonly used per-token one. Furthermore, it is analyzed how positional encodings interact with each other, demonstrating that per-channel bias and per-head bias can positively cooperate. Finally, the authors propose to use a combination of Normalized-Cross-Correlation (NCC) and Structural Similarity Index Measure (SSIM) losses, to avoid typical MSE loss limitations. Experimental results demonstrate that Swin2-MoSE outperforms any Swin derived models by up to 0.377–0.958 dB (PSNR) on task of , and resolution-upscaling ( and OLI2MSI datasets). It also outperforms SOTA models by a good margin, proving to be competitive and with excellent potential, especially for complex tasks. Additionally, an analysis of computational costs is also performed. Finally, the efficacy of Swin2-MoSE is shown, applying it to a semantic segmentation task (SeasoNet dataset). Code and pretrained are available on https://github.com/IMPLabUniPr/swin2-mose/tree/official_code},
  archive      = {J_IETIP},
  author       = {Leonardo Rossi and Vittorio Bernuzzi and Tomaso Fontanini and Massimo Bertozzi and Andrea Prati},
  doi          = {10.1049/ipr2.13303},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13303},
  shortjournal = {IET Image Process.},
  title        = {Swin2-MoSE: A new single image supersolution model for remote sensing},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient no-reference image quality analysis based on statistical perceptual features. <em>IETIP</em>, <em>19</em>(1), e13302. (<a href='https://doi.org/10.1049/ipr2.13302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that image quality needs to be measured with human perception in many computer vision applications. However, these approaches are expensive and require more time for image quality analysis. Therefore, this paper proposes a robust and computationally efficient objective-mathematical model based on statistical perceptual features. The structural and textural features are computed using the modified regularized heaviside local binary pattern (RH-LBP) approach and the concept of entropy. The higher-order probability coefficients of images are considered to extract features that are highly correlated to the human visual system features. Further, the additivity property of Renyi entropy is used to show the randomness of the information combining two terms: One extracts the images spatial intensity changes, and therefore their texture qualities, and the other attain structural details. The features in the proposed approach are jointly optimized to achieve better robustness, monotonicity and match human assessments on image quality, while minimizing the computational complexity and run time. Experiments are conducted with three synthetically distorted datasets, KonIQ-10K, BIQ2021, and LIVE (wild), and two intentionally distorted datasets, TID2013 and CSIQ and are used to evaluate performance index. The proposed method offers competitive performance compared with state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {B. N. Al Sameera and Vilas H. Gaidhane},
  doi          = {10.1049/ipr2.13302},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13302},
  shortjournal = {IET Image Process.},
  title        = {An efficient no-reference image quality analysis based on statistical perceptual features},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic detection of rice disease in images of various leaf sizes. <em>IETIP</em>, <em>19</em>(1), e13301. (<a href='https://doi.org/10.1049/ipr2.13301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To help farmers manage limited resources, rice disease diagnosis must be accurate, timely, and affordable. This study addresses challenges in rice field images, such as environmental variability and differences in rice leaf sizes. The proposed technique combines convolutional neural network object detection with image tiling, using estimated rice leaf width as a reference for image division. An 18-layer ResNet model was trained using ground truth leaf width values for regression in leaf width estimation. Experiments used a dataset of 4960 images representing 8 rice diseases. The leaf width prediction model achieved a mean absolute percentage error of 11.18% and was used to generate a tiled dataset for training advanced rice disease detection models. The tiling technique was evaluated using YOLOv4, YOLOv8n, YOLOv8l, DINO-5scale Swin-L, and Co-DINO-5scale Swin-L models by comparing detection performance on original and tiled datasets. Mean average precision improved significantly: YOLOv4 increased from 87.56% to91.14%, YOLOv8n from 89.80% to 91.70%, and YOLOv8l from 89.80% to 93.20%. More advanced models, such as DINO5scale Swin-L and Co-DINO-5scale Swin-L, achieved even higher precision, at 93.40% and 94.20%, respectively. In conclusion, the tiling technique improved detection efficiency and addressed object size variability, enhancing rice disease detection accuracy inreal-world scenarios.},
  archive      = {J_IETIP},
  author       = {Kantip Kiratiratanapruk and Pitchayagan Temniranrat and Wasin Sinthupinyo and Sanparith Marukatat and Sujin Patarapuwadol},
  doi          = {10.1049/ipr2.13301},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13301},
  shortjournal = {IET Image Process.},
  title        = {Automatic detection of rice disease in images of various leaf sizes},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pothole detection-you only look once: Deformable convolution based road pothole detection. <em>IETIP</em>, <em>19</em>(1), e13300. (<a href='https://doi.org/10.1049/ipr2.13300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of road potholes plays a crucial role in ensuring passenger comfort and the structural safety of vehicles. To address the challenges of pothole detection in complex road environments, this paper proposes a model focusing on shape features (pothole detection you only look once, PD-YOLO). The model aims to overcome the limitations of multi-scale feature learning caused by the use of fixed convolutional kernels in the baseline model, by constructing a feature extraction module that better adapts to variations in the shape of potholes. Subsequently, a cross-stage partial network was designed using a one-time aggregation method, simplifying the model while enabling the network to fuse information between feature maps at different stages. Additionally, a dynamic sparse attention mechanism is introduced to select relevant features, reducing redundancy and suppressing background noise. Experiments conducted on the VOC2007 and GRDDC2020_Pothole datasets reveal that compared to the baseline model YOLOv8, PD-YOLO achieves improvements of 3.9% and 2.8% in mean average precision, with a frame rate of approximately 290 frames per second, effectively meeting the accuracy and real-time requirements for pothole detection. The code and dataset for this paper are located at: https://github.com/woyijiankou/PD-YOLO .},
  archive      = {J_IETIP},
  author       = {Pei Tang and Mao Lv and Zhenyu Ding and Weikai Xu and Minnan Jiang},
  doi          = {10.1049/ipr2.13300},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13300},
  shortjournal = {IET Image Process.},
  title        = {Pothole detection-you only look once: Deformable convolution based road pothole detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot real scenes convolutional anchor clustering twin network for hyperspectral image classification. <em>IETIP</em>, <em>19</em>(1), e13299. (<a href='https://doi.org/10.1049/ipr2.13299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the performance of hyperspectral open set recognition processing with insufficient sample size, the authors propose a new supervised contrastive learning (SCL) framework (FSSCL-OSC) that can achieve open-set classification of hyperspectral images in scenarios with very few sample sizes, which consists of three steps. First, supervised contrastive learning (SCL) with geometric transformations is designed, which uses rotated labels as supervision to obtain lower-level characteristics that more accurately capture various orientations and then makes use of a spectral-spatial characteristic extraction network to maximize the utilization of HSI's spectrum and spatial information. Next, a class anchor open-set classification module based on a twin clustering network is designed, which uses SCL and FSL to extract more specific personal information by mining the category-invariant characteristics present in the known versus unknown class data. Finally, multiple convolution and open-set recognition (OSC) is performed on the feature blocks. Experimental results on three classical HSI datasets show that FSSCL-OSC provides a significant improvement over existing methods, under a sample size of only 10%, the overall accuracy reached 82.38%, 90.76% and 84.70%, respectively.},
  archive      = {J_IETIP},
  author       = {Xiangshan Zhou and Xiaoyi Tong and Xuchuan Zhou and Lin Wang and Lei Zhang and Jie Zhou and Su Qin},
  doi          = {10.1049/ipr2.13299},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13299},
  shortjournal = {IET Image Process.},
  title        = {Few-shot real scenes convolutional anchor clustering twin network for hyperspectral image classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel selection and local attention transformer model for semantic segmentation on UAV remote sensing scene. <em>IETIP</em>, <em>19</em>(1), e13298. (<a href='https://doi.org/10.1049/ipr2.13298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with common urban landscape semantic segmentation, unmanned aerial vehicle (UAV) image semantic segmentation is more challenging because small targets have very low pixel percentages and multi-scale features due to the influence of flight altitude. Yet, the commonly used successive grid downsampling strategy in the current transformer-based methods omits some important features of small targets. Furthermore, due to the complex background interference, it can lead to even worse results. In reaction to this, existing strategies aim to maintain superior resolution. Nevertheless, the application of this method incurs considerable computational costs, which brings challenges for the practical applications of UAVs. So it is significant to design a novel framework to balance retaining more pixels representing small objects during downsampling and reducing computational costs. For this, the Channel Selection and the Local Attention Transformer Model (CSLFormer) are proposed. During the overlap patch embedding process of feature maps, the model allocates half of the important channels to global attention and local attention. These two types of attention focus on different aspects: one learns the relationships and importance among various patches, while the other emphasizes the features of individual patches. The method shows superior performance on two public datasets: AeroScapes and Vaihingen, achieving mean intersection over union (mIoU) of 75.57% and 78.93%, respectively. The proposed CSLFormer has been released on GitHub: https://github.com/leoda1/CSLFormer .},
  archive      = {J_IETIP},
  author       = {Da Liu and Hao Long and Zhenbao Liu},
  doi          = {10.1049/ipr2.13298},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13298},
  shortjournal = {IET Image Process.},
  title        = {Channel selection and local attention transformer model for semantic segmentation on UAV remote sensing scene},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent retrievable object-tracking system with real-time edge inference capability. <em>IETIP</em>, <em>19</em>(1), e13297. (<a href='https://doi.org/10.1049/ipr2.13297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intelligent retrievable object-tracking system assists users in quickly and accurately locating lost objects. However, challenges such as real-time processing on edge devices, low image resolution, and small-object detection significantly impact the accuracy and efficiency of video-stream-based systems, especially in indoor home environments. To overcome these limitations, a novel real-time intelligent retrievable object-tracking system is designed. The system incorporates a retrievable object-tracking algorithm that combines DeepSORT and sliding window techniques to enhance tracking capabilities. Additionally, the YOLOv7-small-scale model is proposed for small-object detection, integrating a specialized detection layer and the convolutional batch normalization LeakyReLU spatial-depth convolution module to enhance feature capture for small objects. TensorRT and INT8 quantization are used for inference acceleration on edge devices, doubling the frames per second. Experiments on a Jetson Nano (4 GB) using YOLOv7-small-scale show an 8.9% improvement in recognition accuracy over YOLOv7-tiny in video stream processing. This advancement significantly boosts the system's performance in efficiently and accurately locating lost objects in indoor home settings.},
  archive      = {J_IETIP},
  author       = {Yujie Li and Yifu Wang and Zihang Ma and Xinghe Wang and Benying Tan and Shuxue Ding},
  doi          = {10.1049/ipr2.13297},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13297},
  shortjournal = {IET Image Process.},
  title        = {An intelligent retrievable object-tracking system with real-time edge inference capability},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multispectral image fusion technique based on decoupling of information. <em>IETIP</em>, <em>19</em>(1), e13296. (<a href='https://doi.org/10.1049/ipr2.13296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issue of unremarkable targets and the serious loss of background details that occurs in multispectral image fusion, a multispectral image fusion method based on information decoupling is proposed. In accordance with the U-shaped network structure, multi-scale feature fusion is employed to obtain comprehensive contextual features. Subsequently, taking into account the differences between base and texture information in the images, the recovered image is reconstructed by fusing feature information through the information decoupling branches. Finally, the information decoupling is supervised by a loss function. In addition, a series of comparative experiments are conducted on a self-constructed multispectral dataset and a publicly available dataset. Experimental results demonstrate that our method not only achieves higher fusion accuracy but also has superior visualization results.},
  archive      = {J_IETIP},
  author       = {Wenqi Yu and Huilin Wang and Jilong Liu and Guan Wang},
  doi          = {10.1049/ipr2.13296},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13296},
  shortjournal = {IET Image Process.},
  title        = {Multispectral image fusion technique based on decoupling of information},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fisheye image rectification and restoration based on swin transformer. <em>IETIP</em>, <em>19</em>(1), e13294. (<a href='https://doi.org/10.1049/ipr2.13294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisheye cameras are widely used in surveillance, automotive systems, virtual reality, and panoramic photography due to their wide-angle perspective. However, images captured by fisheye cameras suffer from significant geometric distortions, affecting image analysis and understanding. This distortion bends straight lines into curves, resulting in a barrel-shaped appearance of the image. To mitigate these effects and transform fisheye images into a regular perspective, fisheye image correction is necessary, enabling more accurate and reliable performance in applications like object recognition, 3D reconstruction, and visual navigation. While convolutional neural networks based fisheye image correction has progressed, it has not fully utilized the global distribution and local symmetry of distortions due to the limitations of fixed receptive fields. This paper introduces a new model based on the Swin Transformer that effectively utilizes both global and local distortion features to adapt automatically to fisheye lens distortions. It also incorporates image restoration functionality to enhance texture details in the corrected images. A novel approach to synthetic dataset generation is proposed to improve the network's generalization capabilities.},
  archive      = {J_IETIP},
  author       = {Jian Xu and Dewei Han and Kang Li and Junjie Li and Zhaoyuan Ma},
  doi          = {10.1049/ipr2.13294},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13294},
  shortjournal = {IET Image Process.},
  title        = {Fisheye image rectification and restoration based on swin transformer},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCFA-net: A U-shaped cross-fusion network with attention mechanism for enhanced polyp segmentation. <em>IETIP</em>, <em>19</em>(1), e13293. (<a href='https://doi.org/10.1049/ipr2.13293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing the precision of computer-assisted polyp segmentation and delineation during colonoscopies assists in the removal of potentially precancerous tissue, thus reducing the risk of malignant transformation. Most of the current medical segmentation models use the traditional U-shaped network structure, but they suffer from the problem of information loss during the encoding and decoding of images. To advance towards an autonomous model for detailed polyp segmentation, the authors propose a new framework for polyp segmentation called U-shaped cross-fusion network with attention mechanism (UCFA-Net), which employs a pyramid vision transformer as encoder to extract image features at multiple scales. Furthermore, the multi-scale cross-fusion module cross-fuses the different scale features and then goes through the multi-scale convolutional parallel feedforward transformer module for modelling the global and local information. Finally, progressive attentional up-sampling module acts as a decoder for up-sampling with progressive attention to get the final polyp segmentation result. The authors comprehensive testing demonstrates that their network achieves superior average scores across the five datasets and exhibits greater robustness in the face of diverse and demanding scenarios, when compared to current state-of-the-art approaches.},
  archive      = {J_IETIP},
  author       = {Shuai Wang and Tiejun Zhao and Guocun Wang and Ye Han and Fan Wu},
  doi          = {10.1049/ipr2.13293},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13293},
  shortjournal = {IET Image Process.},
  title        = {UCFA-net: A U-shaped cross-fusion network with attention mechanism for enhanced polyp segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gallbladder cancer detection via ultrasound image analysis: An end-to-end hierarchical feature-fused model. <em>IETIP</em>, <em>19</em>(1), e13292. (<a href='https://doi.org/10.1049/ipr2.13292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gallbladder cancer is a fatal disease, and its early diagnosis can significantly impact patient treatment. Ultrasound imaging is often the initial diagnostic test for gallbladder cancer, making the enhancement of cancer detection accuracy from these images crucial. Despite the promising results of artificial intelligence techniques in disease diagnosis, their black-box nature hinders the reliability of their results and their practical application. Therefore, it is essential not to rely solely on a single model's output and to further investigate for more reliable outcomes. This study presents a step-by-step structural investigation of forming an end-to-end model, a conjunction of two convolutional neural network based methods, for detecting gallbladder conditions. The final model, leveraging feature fusions and hierarchical classification, achieved a high accuracy of 92.62% for detecting normal, benign, and malignant gallbladders. It also achieved a remarkable accuracy of 98.36% for classifying normal and non-normal instances and 92.22% for classifying benign and malignant cases. Finally, comprehensive post-processing investigations, including cross-validation, temperature scaling, and uncertainty estimation, along with error analysis, are conducted to gain more insights into the model's output. Among these insights, the model demonstrated resilience of its results to active dropout and augmentation at the inference phase. Furthermore, when applied with test-time data augmentation, uncertainty estimation methods have better distinguishability between the uncertainties of correctly and incorrectly classified instances, which provides additional information about the model's output. The source code of experiments conducted in this study is available at https://github.com/SaraDadjouy/GBCRet .},
  archive      = {J_IETIP},
  author       = {Sara Dadjouy and Hedieh Sajedi},
  doi          = {10.1049/ipr2.13292},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13292},
  shortjournal = {IET Image Process.},
  title        = {Gallbladder cancer detection via ultrasound image analysis: An end-to-end hierarchical feature-fused model},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer morphological features for segmentation with few labels on fluorescent mitochondria images. <em>IETIP</em>, <em>19</em>(1), e13290. (<a href='https://doi.org/10.1049/ipr2.13290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated segmentation of mitochondria is crucial for statistical analysis in biological research. Existing segmentation techniques often face challenges with fluorescence images. Handcrafted methods have poor segmentation results while deep learning-based methods lack the labeled mitochondrial data. However, although the number of labeled mitochondrial images is limited, the unlabeled fluorescent data is easy to obtain. The authors aim to leverage a large amount of unlabeled data to learn mitochondrial morphological features. The approach begins with self-supervised learning from a vast set of unlabeled images through masked image modeling. This technique involves presenting images with randomly masked patches, prompting the model to predict the content of these masked areas. By doing so, the model learns the distinctive features of mitochondria. In the subsequent phase, the trained encoder is transferred to the segmentation task, replacing the original reconstruction decoder with the Segformer segmentation decoder. The model is then fine-tuned using a small labeled dataset. By reconstructing mitochondria in the masked regions, the model learns features more effectively on unlabeled samples, and improves segmentation performance even with limited labeled data. Empirical results validate the effectiveness of the approach, showing an 11.8% improvement in Intersection over Union metrics compared to existing fluorescence mitochondrial segmentation techniques.},
  archive      = {J_IETIP},
  author       = {Tianyi Zhang and Junchao Fan and Xiuli Bi and Weisheng Li and Bin Xiao and Xiaoshuai Huang},
  doi          = {10.1049/ipr2.13290},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13290},
  shortjournal = {IET Image Process.},
  title        = {Transfer morphological features for segmentation with few labels on fluorescent mitochondria images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised brain tumour segmentation with label propagation and level set loss. <em>IETIP</em>, <em>19</em>(1), e13289. (<a href='https://doi.org/10.1049/ipr2.13289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis of brain tumors significantly enhances treatment success. However, accurate detection and segmentation of tumors, essential for diagnosis, rely heavily on costly manual annotation by experts. To mitigate these costs, weakly supervised methods have gained traction. This paper introduces a novel weakly supervised brain tumor segmentation approach utilizing point and scribble supervision. Experts annotate only the slice with the largest tumor area by marking a single point near the tumor center or drawing a scribble within the tumor region. The method operates in two phases. First, labels are propagated to unlabelled pixels, generating a pseudo-ground-truth with three labels: tumor, non-tumor, and marginal pixels (unlabelled pixels surrounding the initial segmentation). Second, a segmentation model is trained using the pseudo-ground-truth and a loss function combining level-set and binary cross-entropy losses. Marginal pixels contribute to level-set loss computation, refining the segmentation process. The approach is validated on 3D magnetic resonance imaging (MRI) volumes from BraTS2020, BraTS2021, and BraTS2023 benchmark datasets. Experimental results show Dice scores comparable to fully supervised methods for whole tumor segmentation, demonstrating the effectiveness of the proposed weakly supervised strategy. This method reduces annotation effort while maintaining competitive segmentation performance, making it valuable for clinical applications.},
  archive      = {J_IETIP},
  author       = {Fatemeh-Sadat Abadian-Zadeh and Mohammad Reza Mohammadi and Mohsen Soryani},
  doi          = {10.1049/ipr2.13289},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13289},
  shortjournal = {IET Image Process.},
  title        = {Weakly supervised brain tumour segmentation with label propagation and level set loss},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corn leaf disease recognition based on improved EfficientNet. <em>IETIP</em>, <em>19</em>(1), e13288. (<a href='https://doi.org/10.1049/ipr2.13288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maize leaf disease seriously affects maize yield, a maize leaf disease identification model with an improved lightweight network EfficientNet was proposed in this study. First, the model replaces the SENet module in the MBConv module with the CBAM module, so that the model not only focuses on the correlation between the channels but also adaptively learns the attentional weight of each spatial location. Furthermore, a multi-scale feature fusion layer based on residual connection is introduced to extract more comprehensive and richer disease features at different scales. Finally, by introducing the double pooling method, the overall feature distribution is smoothed while highlighting the important disease features. After the three improvements, the model's recognition accuracy on the test set increased by 2.34%, 2.16%, and 0.97%, respectively, and the improved model achieved an average recognition accuracy of 98.32%, an average precision of 98.29%, and an average recall of 98.25%. The experimental results compared with other models show that the average recognition accuracy of the proposed model is 5.23%, 3.68%, 1.99%, 1.79%, and 3.2% higher than ResNet34, DenseNet121, MobileNet V2, SqueezeNet, and EfficientNet B0, respectively. Activation heat maps show that the improved model can effectively suppress background interference.},
  archive      = {J_IETIP},
  author       = {Xiaowei Sun and Hua Huo},
  doi          = {10.1049/ipr2.13288},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13288},
  shortjournal = {IET Image Process.},
  title        = {Corn leaf disease recognition based on improved EfficientNet},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDRN: Multi-domain representation network for unsupervised domain generalization. <em>IETIP</em>, <em>19</em>(1), e13283. (<a href='https://doi.org/10.1049/ipr2.13283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep neural networks, performance can degrade when test data distributions differ from training data. Unsupervised Domain Generalization (UDG) aims to improve generalization across unseen domains by leveraging multiple source domains without supervision. Traditional methods focus on extracting domain-invariant features, potentially at the expense of feature space integrity and generalization potential. We presents a Multi-Domain Representation Network (MDRN) for unsupervised multi-domain learning. MDRN innovates by disentangling and preserving both domain-invariant and domain-specific features through an unsupervised cross-domain reconstruction task. It employs content encoders for domain-invariant features and multi-domain style encoders for domain-specific characteristics. By merging these features based on domain similarity, MDRN constructs a comprehensive feature space that enhances image reconstruction across domains. Additionally, MDRN integrates domain-specific classifiers, which learn domain classification and provide weighted fusion of domain-specific features. This design facilitates effective inter-domain distance measurement and feature integration. Experiments on PACS and DomainNet show MDRN's superior performance over existing state-of-the-art UDG approaches, highlighting its effectiveness in handling distribution shifts between source and target domains.},
  archive      = {J_IETIP},
  author       = {Yangyang Zhong and YunFeng Yan and Pengxin Luo and Weizhen He and Yiheng Deng and Donglian Qi},
  doi          = {10.1049/ipr2.13283},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13283},
  shortjournal = {IET Image Process.},
  title        = {MDRN: Multi-domain representation network for unsupervised domain generalization},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From pixels to prognosis: Attention-CNN model for COVID-19 diagnosis using chest CT images. <em>IETIP</em>, <em>19</em>(1), e13249. (<a href='https://doi.org/10.1049/ipr2.13249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning assisted diagnosis for assessing the severity of various respiratory infections using chest computed tomography (CT) scan images has gained much attention after the COVID-19 pandemic. Major tasks while building such models require an understanding of the characteristic features associated with the disease, patient-to-patient variations and changes associated with disease severity. In this work, an attention-based convolutional neural network (CNN) model with customized bottleneck residual module (Attn-CNN) is proposed for classifying CT images into three classes: COVID-19, normal, and other pneumonia. The efficacy of the model is evaluated by carrying out various experiments, such as effect of class imbalance, impact of attention module, generalizability of the model and providing visualization of model's prediction for the interpretability of results. Comparative performance evaluation with five state-of-the-art deep architectures such as MobileNet, EfficientNet-B7, Inceptionv3, ResNet-50 and VGG-16, and with published models such as COVIDNet-CT, COVNet, COVID-Net CT2, etc. is discussed.},
  archive      = {J_IETIP},
  author       = {Suba Suseela and Nita Parekh},
  doi          = {10.1049/ipr2.13249},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e13249},
  shortjournal = {IET Image Process.},
  title        = {From pixels to prognosis: Attention-CNN model for COVID-19 diagnosis using chest CT images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast intensive crowd counting model of internet of things based on multi-scale attention mechanism. <em>IETIP</em>, <em>19</em>(1), e12686. (<a href='https://doi.org/10.1049/ipr2.12686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection based on deep learning plays an important role in the application of the Internet of Things (IoT). Traditional methods consume a lot of computing resources and cannot be well deployed in the IoT environment. A lightweight object detection method based on attention mechanism is proposed and applied to crowd counting. In view of the low accuracy and poor real-time performance of multi-scale crowd detection, we design a crowd counting model based on YOLO v5, and apply it to the IoT environment. It is proposed to insert the transformer into the YOLO v5 backbone network. Based on the multi-head attention mechanism in the transformer encoder, the global dependency is modelled to make full use of the context information. The CNN is used to realize the fusion of multi-scale feature maps, and the feature enhancement modules concerned by the attention network are further counted. Experiments show that it can not only detect multi-scale targets, but also achieve real-time performance in video surveillance scenes.},
  archive      = {J_IETIP},
  author       = {Dong Liu and Zhiyong Wang and Xiangjia Meng},
  doi          = {10.1049/ipr2.12686},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e12686},
  shortjournal = {IET Image Process.},
  title        = {Fast intensive crowd counting model of internet of things based on multi-scale attention mechanism},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A single-model multi-task method for face recognition and face attribute recognition in internet of things and visual computing. <em>IETIP</em>, <em>19</em>(1), e12611. (<a href='https://doi.org/10.1049/ipr2.12611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of the internet of things (IoT) is steadily increasing in a wide range of applications. Integration of IoT, computer vision, and artificial intelligence can improve people's daily life in various domains such as smart homes, smart cities, and smart industries. There are a large number of face recognition and face attribute recognition scenarios in reality, and the industry commonly decomposes these tasks, with three models responsible for handling face detection, face recognition, and face attribute recognition. The multi-model approach requires a lot of computational resources for context switching, while training one model with one dataset is not only complex, but also leads to overfitting of the multi-model approach. The authors propose a single-model multi-task approach, which can complete all tasks using only one model, and thus obtains a great improvement in inference speed, especially in scenes with high face density. After an experimental comparison, our approach saves a maximum of 96% of inference time, 49.5% of memory usage, and 59.7% of CPU time, avoids frequent context switching, and simplifies the training steps while improving the generalization performance of the model.},
  archive      = {J_IETIP},
  author       = {Jin Lu and Bo Wu},
  doi          = {10.1049/ipr2.12611},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {e12611},
  shortjournal = {IET Image Process.},
  title        = {A single-model multi-task method for face recognition and face attribute recognition in internet of things and visual computing},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified convolutional neural networks architecture for hyperspectral image classification (Extra-convolutional neural networks). <em>IETIP</em>, <em>19</em>(1), e12169. (<a href='https://doi.org/10.1049/ipr2.12169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of Hyperspectral Satellite Images (HSI) is a very important technology for object detection and cartography. Several problems can be detected, which make classification difficult (large size of the images, fusion between the classes, small amount of samples, etc.). Recently, several Convolutional Neural Networks (CNN-HSI) have been proposed for the classification of hyperspectral images. In this article, an improvement to CNN-HSI is proposed, aiming to reduce the number of erroneous pixels during classification (due to the limited number of samples). Thus, an extra-convolution technique (ExCNN) is proposed, where we add layers of global convolutions on the classified images, outgoing from classical CNN. The addition of 1 to 10 layers, on three real hyperspectral images, is tested. The results obtained are compared with other similar methods of the state of art, and show the effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Maissa HAMOUDA and Med Salim BOUHLEL},
  doi          = {10.1049/ipr2.12169},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e12169},
  shortjournal = {IET Image Process.},
  title        = {Modified convolutional neural networks architecture for hyperspectral image classification (Extra-convolutional neural networks)},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bibliometric analysis of human factors in aviation accident using MKD. <em>IETIP</em>, <em>19</em>(1), e12167. (<a href='https://doi.org/10.1049/ipr2.12167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to provide a better understanding of human factors and human performance mechanism in aviation accident analysis by visualisation and taxonomy analysis using mapping knowledge domain (MKD). An overview of aviation accident analysis involving human factors is first conducted, followed by an introduction of several conceptual models and human reliability analysis methods. Finally, a specific framework for risk assessment of human factors in aviation is proposed. The human factors analysis and classification system structure combined with the quantitative method is the most frequently used mode in aviation accident analysis. From a macro ergonomic perspective, systematic consideration has been emphasised frequently in existing research. Pilots or individuals are thought to be associated with the organisations and environment they located in. On the other hand, the micro ergonomic aspect prefers a quantitative and probabilistic way to reveal the individual's failure mode or error rate. Based on previous studies, the proposed framework provides a systematic view to help with the evaluation of human performance in aviation, taking into account human issues as well as management and environmental factors.},
  archive      = {J_IETIP},
  author       = {Ming Wan and Ying Liang and Lixin Yan and Tuqiang Zhou},
  doi          = {10.1049/ipr2.12167},
  journal      = {IET Image Processing},
  month        = {1-12},
  number       = {1},
  pages        = {e12167},
  shortjournal = {IET Image Process.},
  title        = {Bibliometric analysis of human factors in aviation accident using MKD},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image compression model based on dynamic convolution and vision mamba. <em>IETIP</em>, <em>19</em>(1), e70080. (<a href='https://doi.org/10.1049/ipr2.70080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an efficient image compression scheme leveraging Vision Mamba and dynamic convolution, addressing the limitations of existing methods, such as failure to capture long-range pixel dependencies and high computational complexity. Our approach improves both global and local information learning with reduced computational cost. Experimental results on the Kodak, Tecnick and CLIC datasets show that our model achieves competitive performance with lower algorithm complexity. Our code is available on: https://github.com/Lynxsx/ICVM .},
  archive      = {J_IETIP},
  author       = {Lingchen Qiu and Enjian Bai and Yun Wu and Yuwen Cao and Xue-qin Jiang},
  doi          = {10.1049/ipr2.70080},
  journal      = {IET Image Processing},
  number       = {1},
  pages        = {e70080},
  shortjournal = {IET Image Process.},
  title        = {Image compression model based on dynamic convolution and vision mamba},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-enhanced feature pyramid SwinUNet: Advanced segmentation of lung nodules in CT images. <em>IETIP</em>, <em>19</em>(1), e70072. (<a href='https://doi.org/10.1049/ipr2.70072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of oncology, lung cancer is a leading contributor to cancer-related mortality, highlighting the need for early detection of lung nodules for effective intervention. However, accurate segmentation of lung nodules in Computed Tomography (CT) images remains a significant challenge due to issues such as heterogeneous nodule dimensions, low contrast, and their visual similarity with surrounding tissues. To address these challenges, this study proposes the Edge-Enhanced Feature Pyramid SwinUNet (EE-FPS-UNet), an advanced segmentation model that integrates a modified Swin Transformer with a feature pyramid network (FPN). The research objective is to enhance boundary delineation and multi-scale feature aggregation for improved segmentation performance. The proposed model uses the Swin Transformer to capture long-range dependencies and integrates an FPN for robust multi-scale feature aggregation. Its window-based self-attention mechanism also reduces computational complexity, making it well-suited for high-resolution CT images. Additionally, an edge detection module enhances segmentation by providing edge-related features to the decoder, improving boundary precision. A comparative analysis evaluates the EE-FPS-UNet against leading models, including PSPNet, U-Net, Attention U-Net, and DeepLabV3. The results demonstrate that the proposed model outperforms these models, achieving a Dice Similarity of 0.91 and a sensitivity of 0.89, establishing its efficacy for lung nodule segmentation.},
  archive      = {J_IETIP},
  author       = {Akila Agnes S and Arun Solomon A and K Karthick and Mejdl Safran and Sultan Alfarhood},
  doi          = {10.1049/ipr2.70072},
  journal      = {IET Image Processing},
  number       = {1},
  pages        = {e70072},
  shortjournal = {IET Image Process.},
  title        = {Edge-enhanced feature pyramid SwinUNet: Advanced segmentation of lung nodules in CT images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating linear skip-attention with transformer-based network of multi-level features extraction for partial point cloud registration. <em>IETIP</em>, <em>19</em>(1), e70055. (<a href='https://doi.org/10.1049/ipr2.70055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate point correspondences is critical for rigid point cloud registration in correspondence-based methods. Many previous learning-based methods employ encoder-decoder backbone for point feature extraction, while applying attention mechanism for sparse superpoints to deal with the partial overlap situation. However, few of these methods focus on the intermediate layers yet mainly pay attention on the top-most patch features, thus neglecting multi-faceted feature perspectives leading to potential overlap areas estimation inaccuracy. Meanwhile, obtaining correct correspondences is usually interfered with the one-to-many case and outliers. To address these issues, we propose a multi-level features extraction network with integrating linear dual attention mechanism into skip-connection stage of encoder-decoder backbone, both efficiently suppressing irrelevant information and guiding residual features to learn the common regions on which the network should focus to tackle the overlap estimation inaccuracy issue, combined with a parallel-structured decoder forming distinguishable features and potential overlapping regions. Additionally, a two-stage correspondences pruning process is designed to tackle the mismatch issue, which mainly depends on the rigid geometric constraint. Extensive experiments conducted on indoor and outdoor scene datasets demonstrate our method's accuracy and stability, by outperforming state-of-the-art methods on registration recall.},
  archive      = {J_IETIP},
  author       = {Qinyu He and Tao Sun},
  doi          = {10.1049/ipr2.70055},
  journal      = {IET Image Processing},
  number       = {1},
  pages        = {e70055},
  shortjournal = {IET Image Process.},
  title        = {Integrating linear skip-attention with transformer-based network of multi-level features extraction for partial point cloud registration},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of U-net and its variants: Advances and applications in medical image segmentation. <em>IETIP</em>, <em>19</em>(1), e70019. (<a href='https://doi.org/10.1049/ipr2.70019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images often exhibit low and blurred contrast between lesions and surrounding tissues, with considerable variation in lesion edges and shapes even within the same disease, leading to significant challenges in segmentation. Therefore, precise segmentation of lesions has become an essential prerequisite for patient condition assessment and formulation of treatment plans. Significant achievements have been made in research related to the U-Net model in recent years. It improves segmentation performance and is extensively applied in the semantic segmentation of medical images to offer technical support for consistent quantitative lesion analysis methods. First, this paper classifies medical image datasets on the basis of their imaging modalities and then examines U-Net and its various improvement models from the perspective of structural modifications. The research objectives, innovative designs, and limitations of each approach are discussed in detail. Second, we summarise the four central improvement mechanisms of the U-Net and U-Net variant algorithms: the jump-connection mechanism, the residual-connection mechanism, 3D-UNet, and the transformer mechanism. Finally, we examine the relationships among the four core enhancement mechanisms and commonly utilized medical datasets and propose potential avenues and strategies for future advancements. This paper provides a systematic summary and reference for researchers in related fields, and we look forward to designing more efficient and stable medical image segmentation network models based on the U-Net network.},
  archive      = {J_IETIP},
  author       = {Wang Jiangtao and Nur Intan Raihana Ruhaiyem and Fu Panpan},
  doi          = {10.1049/ipr2.70019},
  journal      = {IET Image Processing},
  number       = {1},
  pages        = {e70019},
  shortjournal = {IET Image Process.},
  title        = {A comprehensive review of U-net and its variants: Advances and applications in medical image segmentation},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
