<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>INSR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="insr">INSR - 15</h2>
<ul>
<li><details>
<summary>
(2025). Statistical depth meets machine learning: Kernel mean embeddings and depth in functional data analysis. <em>INSR</em>, <em>93</em>(2), 317-348. (<a href='https://doi.org/10.1111/insr.12611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical depth is the act of gauging how representative a point is compared with a reference probability measure. The depth allows introducing rankings and orderings to data living in multivariate, or function spaces. Though widely applied and with much experimental success, little theoretical progress has been made in analysing functional depths. This article highlights how the common h -depth and related depths from functional data analysis can be viewed as a kernel mean embedding, widely used in statistical machine learning. This facilitates answers to several open questions regarding the statistical properties of functional depths. We show that (i) h -depth has the interpretation of a kernel-based method; (ii) several h -depths possess explicit expressions, without the need to estimate them using Monte Carlo procedures; (iii) under minimal assumptions, -depths and their maximisers are uniformly strongly consistent and asymptotically Gaussian (also in infinite-dimensional spaces and for imperfectly observed functional data); and (iv) several -depths uniquely characterise probability distributions in separable Hilbert spaces. In addition, we also provide a link between the depth and empirical characteristic function based procedures for functional data. Finally, the unveiled connections enable to design an extension of the -depth towards regression problems.},
  archive      = {J_INSR},
  author       = {George Wynne and Stanislav Nagy},
  doi          = {10.1111/insr.12611},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {317-348},
  shortjournal = {Int. Stat. Rev.},
  title        = {Statistical depth meets machine learning: Kernel mean embeddings and depth in functional data analysis},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How do applied researchers use the causal forest? a methodological review. <em>INSR</em>, <em>93</em>(2), 288-316. (<a href='https://doi.org/10.1111/insr.12610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This methodological review examines the use of the causal forest method by applied researchers across 133 peer-reviewed papers. It shows that the emerging best practice relies heavily on the approach and tools created by the original authors of the causal forest such as their grf package and the approaches given by them in examples. Generally, researchers use the causal forest on a relatively low-dimensional dataset relying on observed controls or in some cases experiments to identify effects. There are several common ways to then communicate results–by mapping out the univariate distribution of individual-level treatment effect estimates, displaying variable importance results for the forest and graphing the distribution of treatment effects across covariates that are important either for theoretical reasons or because they have high variable importance. Some deviations from this common practice are interesting and deserve further development and use. Others are unnecessary or even harmful. The paper concludes by reflecting on the emerging best practice for causal forest use and paths for future research.},
  archive      = {J_INSR},
  author       = {Patrick Rehill},
  doi          = {10.1111/insr.12610},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {288-316},
  shortjournal = {Int. Stat. Rev.},
  title        = {How do applied researchers use the causal forest? a methodological review},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature screening for ultrahigh dimensional mixed data via wasserstein distance. <em>INSR</em>, <em>93</em>(2), 267-287. (<a href='https://doi.org/10.1111/insr.12609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a novel feature screening procedure for ultrahigh dimensional mixed data based on Wasserstein distance, termed as Wasserstein-SIS. To handle the mixture of continuous and discrete data, we use Wasserstein distance as a new marginal utility to measure the difference between the joint distribution and the product of marginal distributions. In theory, we establish the sure screening property under less restrictive assumptions on data types. The proposed procedure does not require model specification, gives a more effective geometric measure to compare the discrepancy between distributions and avoids introducing biases caused by the choice of slicing rules for continuous data. Numerical comparison indicates that the proposed Wasserstein-SIS method performs better than existing methods in various models. A real data application also validates the better practicability of Wasserstein-SIS.},
  archive      = {J_INSR},
  author       = {Bing Tian and Hong Wang},
  doi          = {10.1111/insr.12609},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {267-287},
  shortjournal = {Int. Stat. Rev.},
  title        = {Feature screening for ultrahigh dimensional mixed data via wasserstein distance},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting estimation of number of trials in binomial distribution. <em>INSR</em>, <em>93</em>(2), 246-266. (<a href='https://doi.org/10.1111/insr.12608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the parameter n when p is known or simultaneous estimation of n and p of the binomial distribution based on k ≥ 1 independent observations has been considered by many authors over the last several decades. A range of estimators have been proposed, and questions regarding asymptotic and small sample properties received adequate treatment. In this paper, we provide an extensive review and a comprehensive performance comparison of the estimators from the literature. We propose a conceptually simple estimator of n that uses the marginal likelihood when p is integrated out by simultaneous optimisation w.r.t. n and the hyperparameters. We compare the proposed estimator with various existing estimators and find its performance competitive and, in some scenarios, superior.},
  archive      = {J_INSR},
  author       = {Mina Georgieva and Brani Vidakovic},
  doi          = {10.1111/insr.12608},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {246-266},
  shortjournal = {Int. Stat. Rev.},
  title        = {Revisiting estimation of number of trials in binomial distribution},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the number of components for matrix-variate mixtures: A comparison among information criteria. <em>INSR</em>, <em>93</em>(2), 222-245. (<a href='https://doi.org/10.1111/insr.12607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the crucial task of determining the optimal number of components in mixture models, known as mixture order, when considering matrix-variate data. Despite the growing interest in this data type among practitioners and researchers, the effectiveness of information criteria in selecting the mixture order remains largely unexplored in this branch of the literature. Although the Bayesian information criterion (BIC) is commonly utilised, its effectiveness is only marginally tested in this context, and several other potentially valuable criteria exist. An extensive simulation study evaluates the performance of 10 information criteria across various data structures, specifically focusing on matrix-variate normal mixtures.},
  archive      = {J_INSR},
  author       = {Salvatore D. Tomarchio and Antonio Punzo},
  doi          = {10.1111/insr.12607},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {222-245},
  shortjournal = {Int. Stat. Rev.},
  title        = {On the number of components for matrix-variate mixtures: A comparison among information criteria},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chance-corrected interrater agreement statistics for two-rater dichotomous responses: A method review with comparative assessment under possibly correlated decisions. <em>INSR</em>, <em>93</em>(2), 199-221. (<a href='https://doi.org/10.1111/insr.12606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement of the interrater agreement (IRA) is critical for assessing the reliability and validity of ratings in various disciplines. While numerous IRA statistics have been developed, there is a lack of guidance on selecting appropriate measures especially when raters' decisions could be correlated. To address this gap, we review a family of chance-corrected IRA statistics for two-rater dichotomous-response cases, a fundamental setting that not only serves as the theoretical foundation for categorical-response or multirater IRA methods but is also practically dominant in most empirical studies, and we propose a novel data-generating framework to simulate correlated decision processes between raters. Subsequently, a new estimand, which calibrates the ‘true’ chance-corrected IRA, is introduced while accounting for the potential ‘probabilistic certainty’. Extensive simulations were conducted to evaluate the performance of the reviewed IRA methods under various practical scenarios and were summarised by an agglomerative hierarchical clustering analysis. Finally, we provide recommendations for selecting appropriate IRA statistics based on outcome prevalence and rater characteristics and highlight the need for further advancements in IRA estimation methodologies.},
  archive      = {J_INSR},
  author       = {Zizhong Tian and Vernon M. Chinchilli and Chan Shen and Shouhao Zhou},
  doi          = {10.1111/insr.12606},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {199-221},
  shortjournal = {Int. Stat. Rev.},
  title        = {Chance-corrected interrater agreement statistics for two-rater dichotomous responses: A method review with comparative assessment under possibly correlated decisions},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A conversation with amy racine-poon. <em>INSR</em>, <em>93</em>(2), 183-198. (<a href='https://doi.org/10.1111/insr.12605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Professor Dr. Amy Racine-Poon is best known for her interdisciplinary contributions as an applied Bayesian statistician in the pharmaceutical industry and healthcare. She was born in Hong Kong and obtained a BA with upper honors in Mathematics (1970) from the Chinese University of Hong Kong. She earned a PhD in statistics from the University of California, Berkeley, under the supervision of Erich L. Lehmann. She worked as a Lecturer at the Department of Statistics at UC Berkeley (1975–1977) and as a Statistician at the Biometry Branch of the National Institute of Environmental Health in Research Triangle Park, North Carolina (1977–1980). Amy moved to Basel, Switzerland, in 1981 to join Ciba-Geiby/Novartis AG, where she worked for 42 years (1981–2023) across different therapeutic areas and stages of drug development, applying her skills in advanced statistical and pharmacometric methodologies that led to the development of large number of new drugs. During her career, she was also a Visiting Professor at the Department of Mathematics, Imperial College London (1995–1997) and a Volunteer Statistical Expert at Bill & Melinda Gates Foundation, Seattle, Washington (2015–2019). Amy Racine-Poon's numerous honors include the Royal Statistical Society Greenfield Industrial Medal for Innovative Use of Statistics in the Industries (1995), Fellow of the American Statistical Association (1997), Novartis Distinguished Scientist Award (1999), American Statistical Association Youden Interlaboratory Research Award (2020) and the Sheiner-Beal Pharmacometrics Award (2024) from the American Society of Clinical Pharmacology and Therapeutics. The following conversation took place between Oleksandr Sverdlov (Alex) and Amy Racine-Poon (Amy) in October 2024.},
  archive      = {J_INSR},
  author       = {Oleksandr Sverdlov},
  doi          = {10.1111/insr.12605},
  journal      = {International Statistical Review},
  month        = {8},
  number       = {2},
  pages        = {183-198},
  shortjournal = {Int. Stat. Rev.},
  title        = {A conversation with amy racine-poon},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data analytics and machine learning: Navigating the big data landscape edited by pushpa singh, asha rani mishra, and payal garg springer, 2024, 366 pages, $169.99, hardcover ISBN: 978-9819704477. <em>INSR</em>, <em>93</em>(1), 179-182. (<a href='https://doi.org/10.1111/insr.12603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_INSR},
  author       = {Brian W. Sloboda},
  doi          = {10.1111/insr.12603},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {179-182},
  shortjournal = {Int. Stat. Rev.},
  title        = {Data analytics and machine learning: Navigating the big data landscape edited by pushpa singh, asha rani mishra, and payal garg springer, 2024, 366 pages, $169.99, hardcover ISBN: 978-9819704477},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Summary characteristics for multivariate function-valued spatial point process attributes. <em>INSR</em>, <em>93</em>(1), 150-178. (<a href='https://doi.org/10.1111/insr.12582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompted by modern technologies in data acquisition, the statistical analysis of spatially distributed function-valued quantities has attracted a lot of attention in recent years. In particular, combinations of functional variables and spatial point processes yield a highly challenging instance of such modern spatial data applications. Indeed, the analysis of spatial random point configurations, where the point attributes themselves are functions rather than scalar-valued quantities, is just in its infancy, and extensions to function-valued quantities still remain limited. In this view, we extend current existing first- and second-order summary characteristics for real-valued point attributes to the case where, in addition to every spatial point location, a set of distinct function-valued quantities are available. Providing a flexible treatment of more complex point process scenarios, we build a framework to consider points with multivariate function-valued marks, and develop sets of different cross-function (cross-type and also multi-function cross-type) versions of summary characteristics that allow for the analysis of highly demanding modern spatial point process scenarios. We consider estimators of the theoretical tools and analyse their behaviour through a simulation study and two real data applications.},
  archive      = {J_INSR},
  author       = {Matthias Eckardt and Carles Comas and Jorge Mateu},
  doi          = {10.1111/insr.12582},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {150-178},
  shortjournal = {Int. Stat. Rev.},
  title        = {Summary characteristics for multivariate function-valued spatial point process attributes},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimised optional randomised response technique. <em>INSR</em>, <em>93</em>(1), 130-149. (<a href='https://doi.org/10.1111/insr.12581'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we begin by reviewing the optional randomised response technique estimator (ORRTE) developed by Chaudhuri and Mukerjee for estimating the proportion of a sensitive characteristic in a population. We show that their estimator is unbiased and has smaller variance than the Warner's estimator. Then we make an attempt at developing an optimised optional randomised response technique estimator (OORRTE). The proposed OORRTE is shown to be more efficient than the ORRTE. Findings from simulation studies are discussed and interpreted for various situations. Sample sizes for the Warner's estimator, the ORRTE and the OORRTE are computed based on power analysis introduced by Ulrich, Schroter, Striegel and Simon. Finally, we include an application to real data on COVID-19 by considering it to be partially sensitive variable; that is, sensitive to some but not to others. The data used are included in the paper and the R-codes used in the simulation study are documented in online material.},
  archive      = {J_INSR},
  author       = {Kavya Pushadapu and Sarjinder Singh and Stephen A. Sedory},
  doi          = {10.1111/insr.12581},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {130-149},
  shortjournal = {Int. Stat. Rev.},
  title        = {An optimised optional randomised response technique},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint robust variable selection of mean and covariance model via shrinkage methods. <em>INSR</em>, <em>93</em>(1), 102-129. (<a href='https://doi.org/10.1111/insr.12577'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A valuable and robust extension of the traditional joint mean and the covariance models when data subject to outliers and/or heavy-tailed outcomes can be achieved using the joint modelling of location and scatter matrix of the multivariate t-distribution. This model encompasses three models in itself, and the number of unknown parameters in the covariance model increases quadratically with the matrix size. As a result, selecting the important variables becomes a crucial aspect to consider. In this context, the variable selection combined with the parameter estimation is considered under the normality assumption. However, because of the non-robustness of the normal distribution, the resulting estimators will be sensitive to outliers and/or heavy taildness in the data. This paper has two objectives to overcome these problems. The first is to obtain the maximum likelihood estimates of the parameters and propose an expectation-maximisation type algorithm as an alternative to the Fisher scoring algorithm in the literature. We also consider simultaneous parameter estimation and variable selection in the multivariate t-joint location and scatter matrix models. The consistency and oracle properties of the regularised estimators are also established. Simulation studies and real data analysis are provided to assess the performance of the proposed methods.},
  archive      = {J_INSR},
  author       = {Yeşim Güney and Fulya Gokalp Yavuz and Olcay Arslan},
  doi          = {10.1111/insr.12577},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {102-129},
  shortjournal = {Int. Stat. Rev.},
  title        = {Joint robust variable selection of mean and covariance model via shrinkage methods},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The raise regression: Justification, properties and application. <em>INSR</em>, <em>93</em>(1), 73-101. (<a href='https://doi.org/10.1111/insr.12575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicollinearity results in inflation in the variance of the ordinary least squares estimators due to the correlation between two or more independent variables (including the constant term). A widely applied solution is to estimate with penalised estimators such as the ridge estimator, which trade off some bias in the estimators to gain a reduction in the variance of these estimators. Although the variance diminishes with these procedures, all seem to indicate that the inference and goodness of fit are controversial. Alternatively, the raise regression allows mitigation of the problems associated with multicollinearity without the loss of inference or the coefficient of determination. This paper completely formalises the raise estimator. For the first time, the norm of the estimator, the behaviour of the individual and joint significance, the behaviour of the mean squared error and the coefficient of variation are analysed. We also present the generalisation of the estimation and the relation between the raise and the residualisation estimators. To have a better understanding of raise regression, previous contributions are also summarised: its mean squared error, the variance inflation factor, the condition number, adequate selection of the variable to be raised, the successive raising, and the relation between the raise and the ridge estimator. The usefulness of the raise regression as an alternative to mitigate multicollinearity is illustrated with two empirical applications.},
  archive      = {J_INSR},
  author       = {Román Salmerón-Gómez and Catalina B. García-García and José García-Pérez},
  doi          = {10.1111/insr.12575},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {73-101},
  shortjournal = {Int. Stat. Rev.},
  title        = {The raise regression: Justification, properties and application},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small sample inference for two-way capture-recapture experiments. <em>INSR</em>, <em>93</em>(1), 62-72. (<a href='https://doi.org/10.1111/insr.12574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The properties of the generalised Waring distribution defined on the non-negative integers are reviewed. Formulas for its moments and its mode are given. A construction as a mixture of negative binomial distributions is also presented. Then we turn to the Petersen model for estimating the population size N in a two-way capture-recapture experiment. We construct a Bayesian model for N by combining a Waring prior with the hypergeometric distribution for the number of units caught twice in the experiment. Credible intervals for N are obtained using quantiles of the posterior, a generalised Waring distribution. The standard confidence interval for the population size constructed using the asymptotic variance of Petersen estimator and 0.5 logit transformed interval are shown to be special cases of the generalised Waring credible interval. The true coverage of this interval is shown to be bigger than or equal to its nominal converage in small populations, regardless of the capture probabilities. In addition, its length is substantially smaller than that of the 0.5 logit transformed interval. Thus, the proposed generalised Waring credible interval appears to be the best way to quantify the uncertainty of the Petersen estimator for populations size.},
  archive      = {J_INSR},
  author       = {Louis-Paul Rivest and Mamadou Yauck},
  doi          = {10.1111/insr.12574},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {62-72},
  shortjournal = {Int. Stat. Rev.},
  title        = {Small sample inference for two-way capture-recapture experiments},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of monte carlo methods for noisy and costly densities with application to reinforcement learning and ABC. <em>INSR</em>, <em>93</em>(1), 18-61. (<a href='https://doi.org/10.1111/insr.12573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey gives an overview of Monte Carlo methodologies using surrogate models, for dealing with densities that are intractable, costly, and/or noisy. This type of problem can be found in numerous real-world scenarios, including stochastic optimisation and reinforcement learning, where each evaluation of a density function may incur some computationally-expensive or even physical (real-world activity) cost, likely to give different results each time. The surrogate model does not incur this cost, but there are important trade-offs and considerations involved in the choice and design of such methodologies. We classify the different methodologies into three main classes and describe specific instances of algorithms under a unified notation. A modular scheme that encompasses the considered methods is also presented. A range of application scenarios is discussed, with special attention to the likelihood-free setting and reinforcement learning. Several numerical comparisons are also provided.},
  archive      = {J_INSR},
  author       = {Fernando Llorente and Luca Martino and Jesse Read and David Delgado-Gómez},
  doi          = {10.1111/insr.12573},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {18-61},
  shortjournal = {Int. Stat. Rev.},
  title        = {A survey of monte carlo methods for noisy and costly densities with application to reinforcement learning and ABC},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interview with peter rousseeuw. <em>INSR</em>, <em>93</em>(1), 1-17. (<a href='https://doi.org/10.1111/insr.12587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peter J. Rousseeuw is a statistician known mainly for his work on robust statistics and cluster analysis. Among his creations are least trimmed squares regression, the minimum covariance determinant estimator, the partitioning around medoids clustering method and the silhouettes graphical display. Peter obtained his PhD in 1981 following research carried out at the ETH in Zürich, Switzerland, which led to a book on influence functions. Later, he was a professor at Delft University of Technology, The Netherlands, and at the University of Antwerp, Belgium. Next, he was a researcher at Renaissance Technologies in New York for over a decade. He then returned to Belgium as a full professor at KU Leuven, until becoming emeritus in 2022. He is an elected member of the International Statistical Institute and a fellow of the Institute of Mathematical Statistics and the American Statistical Association. In the course of his career, Peter published three books and over 200 papers, together receiving over 100 000 citations. He was awarded the George Box Medal for Business and Industrial Statistics, the Research Medal of the International Federation of Classification Societies, the Frank Wilcoxon Prize, and twice the Jack Youden Prize. Recently, Peter received the 2024 ASA Noether Distinguished Scholar Award for nonparametric statistics. His former PhD students include Annick Leroy, Rik Lopuhaä, Geert Molenberghs, Christophe Croux, Mia Hubert, Stefan Van Aelst, Tim Verdonck and Jakob Raymaekers. He is the creator and sole sponsor of the Rousseeuw Prize for Statistics, which was first handed out by the King of Belgium in 2022.},
  archive      = {J_INSR},
  author       = {Mia Hubert},
  doi          = {10.1111/insr.12587},
  journal      = {International Statistical Review},
  month        = {4},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Int. Stat. Rev.},
  title        = {An interview with peter rousseeuw},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
