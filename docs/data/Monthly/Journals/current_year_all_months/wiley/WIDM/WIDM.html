<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>WIDM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="widm">WIDM - 54</h2>
<ul>
<li><details>
<summary>
(2025). Automated detection of non-alcoholic fatty liver disease using histopathological images: A systematic review. <em>WIDM</em>, <em>15</em>(3), e70044. (<a href='https://doi.org/10.1002/widm.70044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global rise in fatty liver diseases is alarming. Traditional diagnostic methods include ultrasound, CT scans, MRI, and liver biopsies, the latter being the gold standard for diagnosis and treatment. Recent advancements in artificial intelligence (AI) have enhanced liver biopsy accuracy, improving treatment outcomes. This study investigates how various AI techniques aid histopathologists, gastroenterologists, and liver specialists in diagnosing and assessing liver damage due to abnormal fat accumulation. We conducted a systematic review of AI applications in evaluating fatty liver diseases, particularly through histopathological image analysis. Our search encompassed five scientific databases: PubMed Central, ACM Digital Library, IEEE Xplore, Scopus, and Google Scholar. We focused on peer-reviewed articles, conference papers, theses, and book chapters, adhering to specific terminology. The data synthesis followed the PRISMA guidelines, comparing literature based on four key indices and their annual distribution. We evaluated 37 studies utilizing histopathological imaging for the diagnosis of non-alcoholic fatty liver disease and non-alcoholic steatohepatitis, including related conditions, metabolic dysfunction-associated fatty liver disease and metabolic dysfunction-associated steatohepatitis. The review summarized the performance of various algorithms and explored the distribution of machine learning efforts. Given the complexity of histopathological images, AI algorithms can effectively stratify liver samples affected by fat. Our findings indicate that AI's diagnostic performance closely matches traditional pathological interpretations, offering reliable results for clinical applications. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Hamed Zamanian and Ahmad Shalbaf and Maryam Parvizi and Roohallah Alizadehsani and Ru-San Tan and U. Rajendra Acharya},
  doi          = {10.1002/widm.70044},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70044},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Automated detection of non-alcoholic fatty liver disease using histopathological images: A systematic review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of unlabeled and imbalanced data challenges in machine learning: Strategies and solutions. <em>WIDM</em>, <em>15</em>(3), e70043. (<a href='https://doi.org/10.1002/widm.70043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models often face significant challenges while dealing with imbalanced and unlabeled datasets. Addressing these issues is resource-intensive, requiring comprehensive strategies to navigate their individual complexities and compounded effects. This article explores the dual challenges imposed by class imbalance and the absence of labeled data, along with their individual complexities and combined effects on the performance of the model. This study addresses approaches for handling the imbalance problem in datasets, such as data-level, algorithm-level, and deep learning methods. The survey also examines hybrid methodologies that integrate these strategies to tackle the compounded issues effectively. Emerging techniques like Bayesian graph-based learning, uncertainty-guided semi-supervised learning, and self-supervised approaches are also considered for their potential to address the scalability, noise filtering, and generalization challenges associated with imbalanced and unlabeled datasets. It identified persistent gaps, such as the lack of robust evaluation metrics and the underutilization of dynamic feature extraction techniques, suggesting solutions with advanced machine learning approaches. Additionally, the need for adaptive techniques, such as dynamic class weighting and data-driven filtering mechanisms, is highlighted to address limitations and improve the scalability of machine learning models in real-world applications. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Neethu M S and Vinod Chandra S S},
  doi          = {10.1002/widm.70043},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70043},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review of unlabeled and imbalanced data challenges in machine learning: Strategies and solutions},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the application of the internet of things in precision machining by comparative text mining. <em>WIDM</em>, <em>15</em>(3), e70042. (<a href='https://doi.org/10.1002/widm.70042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision machining, manufacturing components with superior surface quality and dimensional accuracy, increasingly leverages Internet of Things (IoT) technologies. This study employs a novel comparative text mining approach by systematically integrating tree maps, word clouds, keyword network analysis, and Pearson correlation to identify critical linkages between IoT and precision machining. By analyzing a scientific research database (2019–2023), this study highlights IoT's core competencies in enhancing precision machining, including real-time monitoring, predictive maintenance, and data-driven optimization. Furthermore, this study proposes actionable strategies, including neural network-based cyber production systems, blockchain-integrated IIoT platforms, and machine learning-driven predictive models, for precision machining. These recommendations empower academia and industry to harness IoT to improve product quality and reduce costs in precision machining. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Edward Hengzhou Yan and Feng Guo and Baolong Zhang and Muhammad Rehan and Delei Wang and Zhicheng Xu and Chi Ho Wong and Long Teng and Wai Sze Yip and Suet To},
  doi          = {10.1002/widm.70042},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70042},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Exploring the application of the internet of things in precision machining by comparative text mining},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of argument mining in the educational domain: Techniques, applications, and future directions. <em>WIDM</em>, <em>15</em>(3), e70041. (<a href='https://doi.org/10.1002/widm.70041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of argument mining (AM) in the educational domain is a tool for identifying text structures that express an argument. AM can help evaluate the quality of students' assignments, generate insights into their perspectives, and understand their stance on certain topics. This article examines various aspects of AM in education, including techniques, models, approaches, data representation, language resources, and target artifacts. The findings suggest that AM can enhance learning and teaching processes. However, the study highlights gaps in the literature, particularly in exploring educational artifacts like debates and a lack of research on AM in languages other than English. This paper calls for further research to improve educational outcomes through AM in the educational domain. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {David Eduardo Pereira and Daniela Thuaslar Simão Gomes and Larissa Lucena Vasconcelos and Claudio Elizio Calazans Campelo},
  doi          = {10.1002/widm.70041},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70041},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A comprehensive survey of argument mining in the educational domain: Techniques, applications, and future directions},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the evolution of feature extraction methods in Brain–Computer interfaces (BCIs): A systematic review of research progress and future trends. <em>WIDM</em>, <em>15</em>(3), e70040. (<a href='https://doi.org/10.1002/widm.70040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–computer interfaces (BCIs) have emerged as transformative tools, enabling direct communication between the brain and external devices, particularly for individuals with neuromuscular disabilities. This paper provides a comprehensive analysis of feature extraction (FE) methods across all major signal processing domains and various types of BCIs, addressing a significant gap in existing reviews and surveys that often focus exclusively on EEG-based systems. Also, a detailed comparative analysis of FE techniques, highlighting their formulas, advantages, limitations, and practical applications, is provided. The study not only reviews state-of-the-art methods but also evaluates recent research, identifying trends and gaps in the field. Key insights reveal a growing foundation for invasive BCI research, which, while currently limited, shows promise for future advancements. Based on this analysis, we identify and discuss open challenges such as inter-subject variability, real-time processing demands, integration of multiple modalities, and user training and adaptation. Additionally, we examine pressing concerns related to security, privacy, and the transferability of models. By addressing these challenges, this paper aims to guide the development of robust, efficient, and inclusive BCI systems, paving the way for cutting-edge innovations and real-world applications. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Shweta Thakur and Samriti Thakur and Aryan Rana and Pankaj Kumar and Kranti Kumar and Chien-Ming Chen},
  doi          = {10.1002/widm.70040},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70040},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Exploring the evolution of feature extraction methods in Brain–Computer interfaces (BCIs): A systematic review of research progress and future trends},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A state-of-the-art survey of remote photoplethysmography for contactless health parameters sensing. <em>WIDM</em>, <em>15</em>(3), e70039. (<a href='https://doi.org/10.1002/widm.70039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) has emerged as a vital technology for remote healthcare, offering non-invasive and accessible health monitoring through off-the-shelf standard video cameras. rPPG facilitates the assessment of key health indicators like heart rate (HR), respiratory rate (RR), and blood oxygen saturation (SpO 2 ) from video data, providing advantages in early disease diagnosis and routine health assessments. Recognizing its potential, researchers from multiple fields have substantially progressed rPPG by establishing a strong theoretical basis for signal acquisition and developing signal processing and data-driven algorithms for rPPG extraction. While most rPPG reviews primarily focus on HR signal extraction methods, our research provides an overview of the potential scope of rPPG. We systematically organize research on rPPG signal acquisition and extraction techniques and provide a critical review of recent rPPG advancements in diverse health parameter estimation. Besides providing a thorough HR estimation review, we incorporate the extraction of derivative signals such as RR and SpO 2 from rPPG data, including their applications and limitations. We also highlight the adaptation of Machine Learning (ML), Deep Learning (DL), and Computer Vision (CV) techniques with rPPG technologies, and accumulate available critical rPPG resources like datasets, codes, and tutorials. Finally, we identify challenges and research gaps, such as motion artifacts, varying lighting conditions, and differences in skin tone. We aim to uplift advancements in rPPG systems by outlining future research directions. Our comprehensive review aims to support the development of robust and safe applications by advancing the field of contactless health parameter sensing. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Shadman Sakib and Zahid Hasan and Nirmalya Roy},
  doi          = {10.1002/widm.70039},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70039},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A state-of-the-art survey of remote photoplethysmography for contactless health parameters sensing},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A guide to machine learning epistemic ignorance, hidden paradoxes, and other tensions. <em>WIDM</em>, <em>15</em>(3), e70038. (<a href='https://doi.org/10.1002/widm.70038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has rapidly scaled in capacity and complexity, yet blind spots persist beneath its high performance façade. In order to shed more light on this argument, this paper presents a curated catalogue of 175 unconventional concepts, each capturing a paradox, tension, or overlooked risk in modern ML practice. Through nine themes spanning data quality, model architecture and training, interpretability and explainability, fairness and bias, model behavior and limitations, evaluation and metrics, multimodal and system integration, practical and societal implications, and causal reasoning, we provide conceptual definitions, illustrative examples, and actionable mitigation strategies. This review equips practitioners and researchers with a structured taxonomy for diagnosing and preempting the brittle edges of modern ML systems and offers a paradox detection and remediation framework (PDRF) to anticipate limitations, design more thoughtful evaluation protocols, and develop ML systems that balance predictive power with epistemic transparency. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {M. Z. Naser},
  doi          = {10.1002/widm.70038},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70038},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A guide to machine learning epistemic ignorance, hidden paradoxes, and other tensions},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature survey of crowdsourcing: Current status and future perspectives. <em>WIDM</em>, <em>15</em>(3), e70037. (<a href='https://doi.org/10.1002/widm.70037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing has recently evolved as a distributed human problem-solving method and has received considerable interest from academics and practitioners in various domains. The proliferation of crowdsourcing has made it much simpler to utilize the intelligence and adaptability of many people to learn new knowledge to solve the problem of acquiring new knowledge. In the past, numerous crowdsourcing works have highlighted multiple aspects; however, no surveys have been conducted that focus on the entire crowdsourcing process. This concentrated survey provides a comprehensive review of the technical advances from a systematic perspective. This survey systematically reviews technical advances for a crowdsourcing process that contains four dimensions: task modeling, crowdsourcing data acquisition, the learning process, and predictive model learning, and proposes a comprehensive and scalable framework from CROWD4AI (Crowdsourcing Framework with 4 Dimensions for Artificial Intelligence). In addition, this paper focuses on each dimension's potential challenges and future direction, encouraging researchers to participate in crowdsourcing. To bridge theory with practice, we also include a detailed case study that demonstrates the real-world application of our proposed framework in the context of annotating cultural heritage damages using crowdsourced input. The case study illustrates how the framework supports effective task design, label collection, robust learning strategies, and accurate predictive modeling in a practical setting. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Himanshu Suyal and Avtar Singh},
  doi          = {10.1002/widm.70037},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70037},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A systematic literature survey of crowdsourcing: Current status and future perspectives},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on efficient vision-language models. <em>WIDM</em>, <em>15</em>(3), e70036. (<a href='https://doi.org/10.1002/widm.70036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-language models (VLMs) integrate visual and textual information, enabling a wide range of applications such as image captioning and visual question answering, making them crucial for modern AI systems. However, their high computational demands pose challenges for real-time applications. This has led to a growing focus on developing efficient vision-language models. In this survey, we review key techniques for optimizing VLMs on edge and resource-constrained devices. We also explore compact VLM architectures, frameworks, and provide detailed insights into the performance–memory trade-offs of efficient VLMs. Furthermore, we establish a GitHub repository at MPSC-GitHub to compile all surveyed papers, which we will actively update. Our objective is to foster deeper research in this area. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Gaurav Shinde and Anuradha Ravi and Emon Dey and Shadman Sakib and Milind Rampure and Nirmalya Roy},
  doi          = {10.1002/widm.70036},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70036},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey on efficient vision-language models},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning and deep learning techniques to detect mental stress using various physiological signals: A critical insight. <em>WIDM</em>, <em>15</em>(3), e70035. (<a href='https://doi.org/10.1002/widm.70035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive review on the various techniques and methodologies employed to detect stress among individuals. The review encompasses a broad spectrum of methods, including physiological measurements, wearable technology, machine learning and deep learning algorithms, and contactless image-based techniques. The paper outlines the physiological markers commonly associated with stress, such as Electrocardiogram (ECG), Electroencephalography (EEG), Photoplethysmography (PPG), and Skin Galvanic response. It examines the various wearable and contactless techniques to acquire data. Furthermore, it explores the integration of machine learning and deep learning techniques for the development of predictive stress detection models, highlighting their accuracy. It also addresses the potential of multispectral and hyperspectral imaging in this area. Some of the publicly available datasets are also discussed in this paper. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Megha Khandelwal and Arun Sharma},
  doi          = {10.1002/widm.70035},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70035},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Machine learning and deep learning techniques to detect mental stress using various physiological signals: A critical insight},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware security in the connected world. <em>WIDM</em>, <em>15</em>(3), e70034. (<a href='https://doi.org/10.1002/widm.70034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of the Internet of Things (IoT) has integrated billions of smart devices into our daily lives, generating and exchanging vast amounts of critical data. While this connectivity offers significant benefits, it also introduces numerous security vulnerabilities. Addressing these vulnerabilities requires a comprehensive approach to hardware security, one that evaluates the interplay of various attacks and countermeasures to protect these systems. This article provides an extensive overview of hardware security strategies and explores contemporary attacks threatening connected systems. We begin by presenting state-of-the-art side-channel and fault attacks targeting embedded systems, emphasizing the wide range of IoT targets such as smart home devices, medical implants, industrial control systems, and automotive components. Next, we examine hardware-based security primitives such as physically unclonable functions (PUFs) and physically related functions (PReFs), which have emerged as promising solutions for establishing a hardware root-of-trust in lightweight, resource-constrained devices. These primitives provide robust alternatives to secure storage of cryptographic keys, essential for protecting the diverse array of IoT devices. Further, we discuss trusted architectures, hardware Trojans, and physical assurance mechanisms, highlighting their roles in enhancing security across different IoT environments. We conclude by exploring the expanse of machine learning-assisted attacks, which present new and intriguing challenges across all the aforementioned security domains. This article aims to offer valuable insights into the current challenges and future directions of research in hardware security, particularly pertaining to the varied and expanding landscape of IoT devices. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Durba Chatterjee and Shuvodip Maitra and Nimish Mishra and Shubhi Shukla and Debdeep Mukhopadhyay},
  doi          = {10.1002/widm.70034},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70034},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Hardware security in the connected world},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical and machine learning approaches for electrical energy forecasting. <em>WIDM</em>, <em>15</em>(3), e70033. (<a href='https://doi.org/10.1002/widm.70033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With renewable energy being aggressively integrated into the grid, energy supplies are becoming vulnerable to weather and the environment, and are often incapable of meeting population demands at a large scale if not accurately predicted for energy planning. Understanding consumers' power demands ahead of time and the influences of weather on consumption and generation can help producers generate effective power management plans to support the target demand. In addition to the high correlation with the environment, consumers' behaviors also cause non-stationary characteristics of energy data, which is the main challenge for energy prediction. In this survey, we perform a review of the literature on prediction methods in the energy field. So far, most of the available research encompasses one type of generation or consumption. There is no research approaching prediction in the energy sector as a whole and its correlated features. We propose to address the energy prediction challenges from both consumption and generation sides, encompassing techniques from statistical to machine learning techniques. We also summarize the work related to energy prediction, electricity measurements, challenges related to energy consumption and generation, energy forecasting methods, and real-world energy forecasting resources, such as datasets and software solutions for energy prediction. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Solange Machado and Xingquan Zhu},
  doi          = {10.1002/widm.70033},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70033},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Statistical and machine learning approaches for electrical energy forecasting},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence techniques enabled soil moisture estimation frameworks using remote sensing satellite images: Challenges and future directions- review. <em>WIDM</em>, <em>15</em>(3), e70032. (<a href='https://doi.org/10.1002/widm.70032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting soil moisture is critical for keeping groundwater levels stable, monitoring droughts, and assisting agricultural productivity. Surface soil moisture has a tremendous impact on both the environment and society. To provide proper soil moisture, the right tools are required. Gravimetric, physical, and empirical models produce reliable results, but they are generally context-dependent and inappropriate for large-scale investigations. Remote sensing has developed as a credible technology for estimating large-scale soil moisture levels. However, various obstacles exist when getting soil moisture data using remote sensing, including the availability and precision of data sources. The spatial and temporal limits of many remote sensing sources, such as microwave and optical sensors, combined with environmental conditions, provide considerable feasibility issues. As a result, a robust model capable of accurately capturing both linear and nonlinear connections between multiple surface soil variables is critical. Recently, AI approaches have been identified as promising options for managing complicated factors in this domain. This review paper investigates the use of several AI algorithms for estimating soil moisture content (SMC). It focusses on AI-enabled frameworks built with remote sensing satellite imagery. In addition to including in situ observations, the study discusses the advantages of AI approaches, the issues they solve, and provides a detailed description of the integration of microwave, optical, and combination (synergistic) data sources. This paper also addresses the most common AI approaches applied with various types of remote sensing data and the results they produced. By exploring the strengths and technical problems associated with diverse data sources, this work hopes to help researchers make wise choices about data selection and model construction. Finally, the proposed future research directions are likely to assist emerging researchers in broadening the scope of this critical topic in a way that corresponds with future demands. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mangayarkarasi Ramaiah and Prabhavathy Settu and Vinayakumar Ravi},
  doi          = {10.1002/widm.70032},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70032},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Artificial intelligence techniques enabled soil moisture estimation frameworks using remote sensing satellite images: Challenges and future directions- review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of explainable artificial intelligence (XAI) techniques in patients with intracranial hemorrhage: A systematic review. <em>WIDM</em>, <em>15</em>(3), e70031. (<a href='https://doi.org/10.1002/widm.70031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intracranial hemorrhage (IH) is a critical condition requiring rapid and accurate diagnosis to ensure effective treatment and reduce mortality rates. Recently, artificial intelligence (AI) models have demonstrated significant potential in automating the detection and analysis of brain injuries in IH patients. However, the “black-box” nature of many AI systems raises concerns about transparency, reliability, and clinical applicability. Explainable AI (XAI) addresses these challenges by making AI models more interpretable, allowing healthcare professionals to understand and trust the decision-making processes. This review paper explores various XAI techniques—such as SHapley Additive exPlanations (SHAP), Local Interpretable Model-Agnostic Explanations (LIME), Randomized Input Sampling for Explanation (RISE), Class Activation Mapping (CAM), and its variants—and their specific applications in IH clinical tasks. We systematically examine studies incorporating XAI for curing IH patients, highlighting how these methods enhance model transparency and support clinical decision-making. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology was employed to select the papers. Studies are categorized into those using tabular data and those using image data. The literature indicates a rapidly growing number of XAI publications in this field. SHAP is the most commonly used XAI method for tabular data, while CAM-based methods, such as Grad-CAM, dominate in image-based applications. Furthermore, we discuss current limitations of XAI methods and future research directions. This review aims to provide researchers and clinicians with valuable insights into the role of XAI in improving the reliability and practical integration of AI-driven tools for IH patient care. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ali Kohan and Amir Zahedi and Roohallah Alizadehsani and Ru-San Tan and U. Rajendra Acharya},
  doi          = {10.1002/widm.70031},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70031},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Application of explainable artificial intelligence (XAI) techniques in patients with intracranial hemorrhage: A systematic review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-heuristic optimization for the multi-classification of chronic disease: A review with machine learning perspectives. <em>WIDM</em>, <em>15</em>(3), e70030. (<a href='https://doi.org/10.1002/widm.70030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic diseases (CDs) present a global health challenge due to their complex, overlapping symptoms and the limitations of traditional diagnostic methods. Artificial intelligence (AI)-based techniques, particularly Machine Learning (ML) and Meta-Heuristic Optimization (MHO) algorithms, have emerged as powerful tools for addressing these challenges. This review examines ML and MHO-based approaches for the multi-classification of CDs, highlighting how MHO enhances ML frameworks by addressing key limitations such as class imbalance and suboptimal feature selection. Despite these advancements, MHO-based methods face challenges, including computational complexity and algorithmic biases, which require further research. By critically analyzing existing studies and identifying gaps, this paper provides a foundation for developing more robust and efficient diagnostic models for CDs. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Akansha Singh and Nupur Prakash and Anurag Jain},
  doi          = {10.1002/widm.70030},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70030},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Meta-heuristic optimization for the multi-classification of chronic disease: A review with machine learning perspectives},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A literature review of textual cyber abuse detection using cutting-edge natural language processing techniques: Language models and large language models. <em>WIDM</em>, <em>15</em>(3), e70029. (<a href='https://doi.org/10.1002/widm.70029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of social media platforms has facilitated the emergence of various forms of online abuse within digital communities. This abuse manifests in multiple ways, including hate speech, cyberbullying, emotional abuse, grooming, and shame sexting or sextortion. In this paper, we present a comprehensive analysis of the different forms of abuse prevalent in social media, with a particular focus on how emerging technologies, such as Language Models (LMs) and Large Language Models (LLMs), are reshaping both the detection and generation of abusive content within these networks. We delve into the mechanisms through which social media abuse is perpetuated, exploring the psychological and social impact. To achieve this, we conducted a literature review based on PRISMA methodology, deriving key insights in the field of cyber abuse detection. Additionally, we examine the dual role of advanced language models—highlighting their potential to enhance automated detection systems for abusive behavior while also acknowledging their capacity to generate harmful content. This paper contributes to the ongoing discourse on online safety and ethics by offering both theoretical and practical insights into the evolving landscape of cyber abuse, as well as the technological innovations that simultaneously mitigate and exacerbate it. The findings support platform administrators and policymakers in developing more effective moderation strategies, conducting comprehensive risk assessments, and integrating AI responsibly to create safer digital environments. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {J. Angel Diaz-Garcia and Joao Paulo Carvalho},
  doi          = {10.1002/widm.70029},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {3},
  pages        = {e70029},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A literature review of textual cyber abuse detection using cutting-edge natural language processing techniques: Language models and large language models},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An overview of heterogeneous social network analysis. <em>WIDM</em>, <em>15</em>(2), e70028. (<a href='https://doi.org/10.1002/widm.70028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous Social Networks (HSNs) represent complex structures where diverse entities, such as users, items, and interactions, coexist and interact within a unified framework. This paper offers a systematic review of HSN Analysis, addressing the theoretical and practical challenges associated with investigating the interplay between varied node types and diverse relationships within HSNs. The paper begins by defining HSNs and outlining their characteristics, highlighting the existence of diverse entity kinds and a range of relationship types. It explores the significance of HSNs in modeling real-world systems, including online social platforms, biological networks, e-commerce networks, and recommendation systems, where diverse entities play distinct roles. The analysis of HSNs extends beyond traditional homogeneous networks, incorporating various types of nodes and edges, and introduces novel considerations for effective analysis. The difficulties in modeling, representing, and analyzing HSNs will be covered in this work. Several reviews of social network analysis have been published in the past, but they often focus on simple networks, not HSN analysis specifically. This paper aims to fill that gap by comprehensively reviewing different aspects of HSN and its analysis. We start with the fundamentals of HSNs, explore its major types-multi-relational networks and multi-modal networks and further their impact on popular data mining tasks. Then, we explore various applications of heterogeneous information network analysis, like recommender systems, text mining, fraud detection, and e-commerce. Finally, we look at recent research and suggest promising future directions in the field of HSN analysis.},
  archive      = {J_WIDM},
  author       = {Deepti Singh and Ankita Verma},
  doi          = {10.1002/widm.70028},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70028},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {An overview of heterogeneous social network analysis},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle damage detection using artificial intelligence: A systematic literature review. <em>WIDM</em>, <em>15</em>(2), e70027. (<a href='https://doi.org/10.1002/widm.70027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating vehicle damage detection is essential for automotive industry applications like insurance claims, online sales, and repair cost estimates, addressing the labor-intensive, time-consuming, and error-prone nature of current manual inspections. This systematic literature review explores the use of artificial intelligence (AI), particularly deep learning-based algorithms, to improve the accuracy and efficiency of damage detection under dynamic and challenging conditions specific to the requirements of our industry partners. The review is structured around five key research questions and includes extensive empirical evaluations to identify gaps and challenges in existing methods. Findings reveal significant potential for AI to automate and enhance the damage detection process but also highlight areas requiring further research and development. The review discusses these gaps in detail, providing a comprehensive foundation for future work in this field. Furthermore, the review findings are intended to guide both our research and the broader research community in advancing the practical application of AI for vehicle damage assessment. The insights gained from this review are crucial for developing robust AI solutions that can operate effectively in real-world scenarios, ultimately improving operational efficiency and customer experience in the automotive industry.},
  archive      = {J_WIDM},
  author       = {Md Jahid Hasan and Cong Kha Nguyen and Yee Ling Boo and Hamed Jahani and Kok-Leong Ong},
  doi          = {10.1002/widm.70027},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70027},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Vehicle damage detection using artificial intelligence: A systematic literature review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in feature selection using memetic algorithms: A comprehensive review. <em>WIDM</em>, <em>15</em>(2), e70026. (<a href='https://doi.org/10.1002/widm.70026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review paper presents a comprehensive analysis of the memetic algorithms (MAs) for feature selection (FS), particularly in high-dimensional datasets. MAs effectively address the challenges of feature selection by combining the global exploration capabilities of evolutionary algorithms with the local optimization of search techniques. Their hybrid nature makes them well suited for tackling the complexity, scalability, and computational demands of FS problems across various domains, including bioinformatics, image processing, and financial forecasting. This review highlights the recent advancements, customized variants, and practical applications of MA-based FS methods while providing critical insights into their limitations, such as computational overhead and overfitting. Additionally, the paper outlines future research directions to further enhance the efficacy of MAs in feature selection, offering a balanced perspective on their contributions to the field.},
  archive      = {J_WIDM},
  author       = {Keerthi Gabbi Reddy and Deepasikha Mishra},
  doi          = {10.1002/widm.70026},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70026},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Advances in feature selection using memetic algorithms: A comprehensive review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-based waste management: A review of classification, techniques, issues, and challenges. <em>WIDM</em>, <em>15</em>(2), e70025. (<a href='https://doi.org/10.1002/widm.70025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is emerging as a transforming force in waste management practices, enabling new ways of bringing efficiency and effectiveness. This survey presents methods related to waste management, which are categorized systematically for understanding the effectiveness of various AI-based techniques. The study undertakes a critical review of relevant research works that epitomize major advances and methodologies of AI-driven waste management. The manuscript provides an exhaustive taxonomy, dividing AI methods into Supervised Learning, Unsupervised Learning, and Reinforcement Learning, and then subdividing Supervised Learning into four broad categories: Machine Learning-based Classification, CNNs, Transfer Learning, and Hybrid or Ensemble Learning. We further evaluate different datasets applied in performance benchmarking and the efficacy of the various AI models. We also discuss some critical issues, such as the problem of available data quality, poor generalization of models, and integration of systems. Future research directions, which would go a long way toward helping to surmount these challenges, are also discussed. This survey aims to present a structured framework for understanding current AI applications in waste management, therefore guiding ongoing and future research in the field.},
  archive      = {J_WIDM},
  author       = {Dhanashree Vipul Yevle and Palvinder Singh Mann},
  doi          = {10.1002/widm.70025},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70025},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Artificial intelligence-based waste management: A review of classification, techniques, issues, and challenges},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring causal learning through graph neural networks: An in-depth review. <em>WIDM</em>, <em>15</em>(2), e70024. (<a href='https://doi.org/10.1002/widm.70024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, exploring data correlations to predict outcomes is a fundamental task. Recognizing causal relationships embedded within data is pivotal for a comprehensive understanding of system dynamics, the significance of which is paramount in data-driven decision-making processes. Beyond traditional methods, there has been a shift toward using graph neural networks (GNNs) for causal learning, given their capabilities as universal data approximators. Thus, a thorough review of the advancements in causal learning using GNNs is both relevant and timely. To structure this review, we introduce a novel taxonomy that encompasses various state-of-the-art GNN methods used in studying causality. GNNs are further categorized based on their applications in the causality domain. We further provide an exhaustive compilation of datasets integral to causal learning with GNNs to serve as a resource for practical study. This review also touches upon the application of causal learning across diverse sectors. We conclude the review with insights into potential challenges and promising avenues for future exploration in this rapidly evolving field of machine learning.},
  archive      = {J_WIDM},
  author       = {Simi Job and Xiaohui Tao and Taotao Cai and Haoran Xie and Lin Li and Qing Li and Jianming Yong},
  doi          = {10.1002/widm.70024},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70024},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Exploring causal learning through graph neural networks: An in-depth review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geospatial data clustering in network space: A survey. <em>WIDM</em>, <em>15</em>(2), e70023. (<a href='https://doi.org/10.1002/widm.70023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geospatial data enhances traditional datasets by integrating spatial and temporal dimensions, facilitating advanced visualizations and comprehensive analytical insights. As a fundamental aspect of geospatial analytics, geospatial data clustering (GDC) has become a prominent area of academic research, playing a critical role in theoretical exploration and applied domains. GDC seeks to group geospatial objects based on inherent similarities, a necessity driven by modern datasets' increasing scale and complexity, particularly those within geographic information systems (GIS). This paper highlights key challenges and advancements in GDC, including spatial data clustering (SDC), clustering techniques within GIS, and algorithms designed for geospatial data clustering in network spaces (GDC in NS). Practical implementations of these methodologies encompass diverse applications such as hotspot analysis, infectious disease monitoring, transportation optimization, urban traffic management, and emergency response planning. These contributions are foundational for advancing scholarly research and addressing domain-specific challenges in this field.},
  archive      = {J_WIDM},
  author       = {Loan T. T. Nguyen and Trang T. D. Nguyen and Quang-Thinh Bui and Bay Vo},
  doi          = {10.1002/widm.70023},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70023},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Geospatial data clustering in network space: A survey},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weak supervision: A survey on predictive maintenance. <em>WIDM</em>, <em>15</em>(2), e70022. (<a href='https://doi.org/10.1002/widm.70022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maintenance advancements achieved in Industry 4.0 generate large amounts of data, necessitating complete, accurate, and precise labels for training datasets to align with corresponding ground truth. These labels serve as annotations for early anomaly detection. Delivering high-quality annotations derived from weak labels and striking a balance between annotation efforts and accuracy are critical tasks. Consequently, researchers have focused their attention on Weakly Supervised Learning methods, which have shown effectiveness in handling datasets characterized by incomplete, imprecise, and erroneous labels across various maintenance applications. In this survey, the authors aim to address a gap in the existing literature by conducting a comprehensive examination of Weakly Supervised Learning for Predictive Maintenance, categorizing related works. Furthermore, the survey discusses challenges and identifies open research lines.},
  archive      = {J_WIDM},
  author       = {Antonio M. Martínez-Heredia and Sebastián Ventura},
  doi          = {10.1002/widm.70022},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70022},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Weak supervision: A survey on predictive maintenance},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The “Curious case of contexts” in retrieval-augmented generation with a combination of labeled and unlabeled data. <em>WIDM</em>, <em>15</em>(2), e70021. (<a href='https://doi.org/10.1002/widm.70021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing reliance on LLMs for a wide range of NLP tasks, optimizing the use of labeled and unlabeled data for effective context generation has become critical. This work explores the interplay between two prominent methodologies in few-shot learning: in-context learning (ICL), which utilizes labeled task-specific data, and retrieval-augmented generation (RAG), which leverages unlabeled external knowledge to augment generative models. Since each has its individual limitations, we propose a novel hybrid approach to obtain “the best of both worlds” by dynamically integrating both labeled and unlabeled data towards improving the downstream performance of LLMs. Our methodology, which we call LU-RAG (labeled and unlabeled RAG), recomputes the scores of top- k labeled instances and top- m unlabeled passages to refine context selection. Our experimental results demonstrate that LU-RAG consistently outperforms both standalone ICL and RAG across multiple benchmarks, showing significant gains in downstream performance. Furthermore, we show that LU-RAG performs better with a semantic neighborhood as compared to a lexical one, highlighting its ability to generalize effectively.},
  archive      = {J_WIDM},
  author       = {Payel Santra and Madhusudan Ghosh and Debasis Ganguly and Partha Basuchowdhuri and Sudip Kumar Naskar},
  doi          = {10.1002/widm.70021},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70021},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The “Curious case of contexts” in retrieval-augmented generation with a combination of labeled and unlabeled data},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on causal inference-driven data bias optimization in recommendation systems: Principles, opportunities and challenges. <em>WIDM</em>, <em>15</em>(2), e70020. (<a href='https://doi.org/10.1002/widm.70020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems predict user interests and recommend items for online platforms including e-commerce, social networks, and decision systems. However, data bias has become a significant obstacle, severely impacting the accuracy, fairness, and reliability of recommendation results. This survey examines causal inference for optimizing recommendation systems and mitigating data bias, addressing three questions: (1) Bias types and performance impacts; (2) Causal inference mitigation methods; (3) Approach advantages, limitations, and research opportunities. The motivation for this survey stems from the limitations of traditional debiasing methods, which often fail to account for causal relationships and struggle in dynamic, real-world scenarios. Causal inference provides a robust framework for identifying and addressing the underlying causes of bias, enabling more transparent and accurate recommendation systems. Therefore, we define three critical stages of bias: bias in the data stage, model selection stage, and model evaluation stage. For each stage, causal inference-based optimization methods are introduced and critically analyzed. Unlike traditional debiasing methods, this study analyzes data augmentation and regularization techniques as potential strategies for future research. The whole research might highlight the ability of causal inference to uncover and control confounding factors, offering deeper insights into the mechanisms driving biases.},
  archive      = {J_WIDM},
  author       = {Yongkang Li and Xingyu Zhu and Yuheng Wu and Wenxu Zhao and Xiaona Xia},
  doi          = {10.1002/widm.70020},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70020},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey on causal inference-driven data bias optimization in recommendation systems: Principles, opportunities and challenges},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long document classification in the transformer era: A survey on challenges, advances, and open issues. <em>WIDM</em>, <em>15</em>(2), e70019. (<a href='https://doi.org/10.1002/widm.70019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Document Classification (ADC) refers to the process of automatically categorizing or labeling documents into predefined classes or categories. Its effectiveness may depend on various factors, including the models used for the formal representation of documents, the classification techniques applied, or a combination of both. Recently, Transformer models have gained popularity due to their pre-training on large corpora, allowing for flexible knowledge transfer to downstream tasks, such as ADC. However, such models can face challenges when handling “long” documents, particularly due to input sequence length constraints, which can have knock-on effects on the task we refer to as Automatic Long Document Classification (ALDC). Distinct models for tackling this limitation of Transformers have been proposed over the past few years, and employed to perform ALDC; however, their application to this task has resulted in some inconsistent outcomes, struggles to surpass simple baselines, and difficulties in generalizing across diverse datasets and scenarios. That is why this survey aims to illustrate these limitations, by: (i) presenting current long document representation issues and solutions proposed in the literature; (ii) based on such solutions, illustrating a comprehensive analysis of their application in ALDC and their effectiveness; and (iii) discussing current evaluation strategies in ALDC with particular reference to suitable baselines and actual long-document benchmark datasets.},
  archive      = {J_WIDM},
  author       = {Renzo Alva Principe and Nicola Chiarini and Marco Viviani},
  doi          = {10.1002/widm.70019},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70019},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Long document classification in the transformer era: A survey on challenges, advances, and open issues},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling explainable AI in healthcare: Current trends, challenges, and future directions. <em>WIDM</em>, <em>15</em>(2), e70018. (<a href='https://doi.org/10.1002/widm.70018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This overview investigates the evolution and current landscape of eXplainable Artificial Intelligence (XAI) in healthcare, highlighting its implications for researchers, technology developers, and policymakers. Following the PRISMA protocol, we analyzed 89 publications from January 2000 to June 2024, spanning 19 medical domains, with a focus on Neurology and Cancer as the most studied areas. Various data types are reviewed, including tabular data, medical imaging, and clinical text, offering a comprehensive perspective on XAI applications. Key findings identify significant gaps, such as the limited availability of public datasets, suboptimal data preprocessing techniques, insufficient feature selection and engineering, and the limited utilization of multiple XAI methods. Additionally, the lack of standardized XAI evaluation metrics and practical obstacles in integrating XAI systems into clinical workflows are emphasized. We provide actionable recommendations, including the design of explainability-centric models, the application of diverse and multiple XAI methods, and the fostering of interdisciplinary collaboration. These strategies aim to guide researchers in building robust AI models, assist technology developers in creating intuitive and user-friendly AI tools, and inform policymakers in establishing effective regulations. Addressing these gaps will promote the development of transparent, reliable, and user-centred AI systems in healthcare, ultimately improving decision-making and patient outcomes.},
  archive      = {J_WIDM},
  author       = {Abdul Aziz Noor and Awais Manzoor and Muhammad Deedahwar Mazhar Qureshi and M. Atif Qureshi and Wael Rashwan},
  doi          = {10.1002/widm.70018},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70018},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Unveiling explainable AI in healthcare: Current trends, challenges, and future directions},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Review on information fusion-based data mining for improving complex anomaly detection. <em>WIDM</em>, <em>15</em>(2), e70017. (<a href='https://doi.org/10.1002/widm.70017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly predicated upon multiple distributed hybrid sensors frequently uses hybrid approaches, integrating techniques derived from statistical analysis, probability, data mining, machine learning, deep learning, and signal denoising. Many of these methods are based on the analysis of irregularities, data continuity, correlation, and data consistency, aiming to discern anomalous patterns from normal behavior. By leveraging these techniques information fusion aims to enhance situational awareness, detect potential threats or abnormalities, and improve decision-making processes in complex environments. It addresses uncertainties by integrating data from diverse sources, thereby enhancing performance, and reducing dependency on individual sensors. This study examines applications based on single and multiple sensor data, revealing common strategies, identifying strengths and weaknesses, and potential solutions for detecting and diagnosing anomalies by analyzing low, large, and complex data derived from the context of homogeneous or heterogeneous systems. Information fusion techniques are evaluated for their performance on various levels of algorithm complexity. This in-depth bibliographic study involved searching top indexing databases such as Web of Science and Scopus. The inclusion criteria were articles published between 2012 and 2024. The search capitalized on specific keywords as follows: “sensor malfunction,” “sensor anomaly,” “sensor failure,” “sensor fusion,” and “anomaly data mining.” Publications that did not strictly focus on analytical processing for anomaly detection, diagnosis, and prognosis in sensor data were excluded. In conclusion, the practice of information fusion promotes transparency by elucidating the process of combining information, thereby enabling the inclusion of multitude of perspectives, and aligning with established best practices in the field. Data deviation remains the primary criterion for detecting anomalies using mostly deep learning and extensively hybrid techniques. Nevertheless, state-of-the-art algorithms based on neural networks still require further contextual interpretation and analysis. Functional safety and safety of intended functionality breaching can lead to decision-making errors, physical harm, and erosion of trust in autonomous systems. This is due to the lack of interpretability in AI approaches, making it challenging to predict and understand the system's behavior under various conditions.},
  archive      = {J_WIDM},
  author       = {Sorin-Claudiu Moldovan and Laszlo Barna Iantovics},
  doi          = {10.1002/widm.70017},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70017},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Review on information fusion-based data mining for improving complex anomaly detection},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-assisted literature review: Integrating visualization and geometric features for insightful analysis. <em>WIDM</em>, <em>15</em>(2), e70016. (<a href='https://doi.org/10.1002/widm.70016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advancements in technology and Artificial Intelligence have increased the volume of scientific research, making it challenging for researchers and scholars to keep pace with the evolving literature and state-of-the-art techniques and methods. Traditional review papers offer a way to mitigate these difficulties but are often time-consuming and labor-intensive. This article introduces a novel AI-assisted narrative review methodology that integrates advanced text retrieval and visualization techniques, enhanced with geometric features, to address this. The proposed approach relies on the automatic identification of research topics/clusters within a large different document corpus of different time periods. This approach not only facilitates the systematic exploration of trends over time but also serves as a valuable adjunct, enabling experts to focus on specific, homogeneous areas within scientific fields/clusters. Initially, the methodology in its generality and mapping of the evolution of emerging topics are described, revealing the temporal dynamics and interconnections within the literature of time series anomalies. Subsequently, the proposed method is applied to time series data and an in-depth exploration of the identified dominant cluster is presented. The cluster involves advanced techniques and models for anomaly detection in time series analysis. Focusing on such a homogeneous subfield enables the derivation of a wealth of characteristics and outcomes regarding the evolution of this topic, revealing its temporal dynamics and trends. The review process demonstrates the effectiveness of the proposed AI-driven approach in literature reviews and provides researchers with a powerful tool to synthesize and interpret complex, dynamically changing, advanced scientific fields.},
  archive      = {J_WIDM},
  author       = {Grigorios Papageorgiou and Ekaterini Skamnia and Polychronis Economou},
  doi          = {10.1002/widm.70016},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70016},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {AI-assisted literature review: Integrating visualization and geometric features for insightful analysis},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of causality in explainable artificial intelligence. <em>WIDM</em>, <em>15</em>(2), e70015. (<a href='https://doi.org/10.1002/widm.70015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality and eXplainable Artificial Intelligence (XAI) have developed as separate fields in computer science, even though the underlying concepts of causation and explanation share common ancient roots. This is further enforced by the lack of review works jointly covering these two fields. In this paper, we investigate the literature to try to understand how and to what extent causality and XAI are intertwined. More precisely, we seek to uncover what kinds of relationships exist between the two concepts and how one can benefit from them, for instance, in building trust in AI systems. As a result, three main perspectives are identified. In the first one, the lack of causality is seen as one of the major limitations of current AI and XAI approaches, and the “optimal” form of explanations is investigated. The second is a pragmatic perspective and considers XAI as a tool to foster scientific exploration for causal inquiry, via the identification of pursue-worthy experimental manipulations. Finally, the third perspective supports the idea that causality is propaedeutic to XAI in three possible manners: exploiting concepts borrowed from causality to support or improve XAI, utilizing counterfactuals for explainability, and considering accessing a causal model as explaining itself. To complement our analysis, we also provide relevant software solutions used to automate causal tasks. We believe our work provides a unified view of the two fields of causality and XAI by highlighting potential domain bridges and uncovering possible limitations.},
  archive      = {J_WIDM},
  author       = {Gianluca Carloni and Andrea Berti and Sara Colantonio},
  doi          = {10.1002/widm.70015},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70015},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The role of causality in explainable artificial intelligence},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neuromorphic computing and applications: A topical review. <em>WIDM</em>, <em>15</em>(2), e70014. (<a href='https://doi.org/10.1002/widm.70014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic computers achieve energy efficiency by emulating brain structure and event-driven processing that reduces energy consumption significantly. An increasing interest in this technology started in the initial years of this millennium, sparked by the awareness and concern on the ever-increasing power demands of modern-day computing. In current times, there are several neuromorphic computers and sensors that continue to be developed in both industry and academic research. The focus of this survey is on the neuromorphic computing applications of these devices that include brain-inspired neural networks, brain-inspired artificial neural networks, and Hybrid circuits comprising both artificial and brain-inspired units of computation. Many of these applications use neuromorphic sensors as input devices. We have surveyed three specific neuromorphic computers viz. SpiNNaker, TrueNorth, Loihi, and one neuromorphic sensor viz. Dynamic vision sensor (DVS)-based electronic retina; the demonstration of neuromorphic computing and applications using these devices far outnumbers those on the others that are currently available, which forms the basis of our choice. The applications include low-power cognitive machine intelligence as well as neuropathological understanding and knowledge discovery. Overall, our survey identifies the potential for neuromorphic computing to provide low power, low cost, and dynamic solutions for societal and scientific problems in the not-too-distant future.},
  archive      = {J_WIDM},
  author       = {Pavan Kumar Enuganti and Basabdatta Sen Bhattacharya and Teresa Serrano Gotarredona and Oliver Rhodes},
  doi          = {10.1002/widm.70014},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70014},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Neuromorphic computing and applications: A topical review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Algorithmic profiling and facial recognition in EU border control: Examining ETIAS decision-making, privacy and law. <em>WIDM</em>, <em>15</em>(2), e70013. (<a href='https://doi.org/10.1002/widm.70013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing use of algorithmic and biometric technologies in border control is part of a larger trend in global security governance that has significant legal and ethical implications for their effect on individual rights and procedural justice. As central features in the EU's shifting security regime, ETIAS and facial recognition technologies deploy algorithmic profiling and biometric risk assessment to screen visa-exempt third-country nationals. The research systematically examines the decision-making processes of ETIAS and the overall facial recognition system, demonstrating the interplay between algorithmic risk assessments and discretionary human discretion by national authorities. It contends that the algorithmic profiling lack of transparency, combined with sweeping national security exceptions, produces a procedural void, in which the right to reasoned decisions and effective remedies is compromised. Second, the use of interoperable databases and risk indicators puts core data protection principles into jeopardy, notably purpose limitation and the right to be forgotten. This paper also argues that ETIAS and the application of facial recognition technologies represent a larger trend toward “techno-regulatory assemblages” in EU governance, where technological infrastructures increasingly influence legal and administrative decisions. It critically assesses whether the human oversight mechanisms incorporated within ETIAS National Units are adequate to prevent the risks involved in automated decision-making, especially in the face of strict time pressures and security requirements. The study detects a latent paradox: though these systems aim to strengthen a “Security Union,” they might inadvertently lead to an “Insecurity Union” by undermining transparency, procedural protections, and citizen rights.},
  archive      = {J_WIDM},
  author       = {Abhishek Thommandru and Varda Mone and Fayzulloyev Shokhijakhon and Giyosbek Mirzayev},
  doi          = {10.1002/widm.70013},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70013},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Algorithmic profiling and facial recognition in EU border control: Examining ETIAS decision-making, privacy and law},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic survey of graph convolutional networks for artificial intelligence applications. <em>WIDM</em>, <em>15</em>(2), e70012. (<a href='https://doi.org/10.1002/widm.70012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have become an essential tool for handling graph-structured data, enhancing the functionality of conventional convolutional neural networks (CNNs) in non-Euclidean contexts. GCNs are particularly proficient in tasks such as node classification, link prediction, and graph clustering by collecting information from neighboring nodes. These models are utilized in a range of domains, including recommendation systems, social network analysis, bioinformatics, and computer vision. GCNs demonstrate significant effectiveness in challenges like citation prediction and knowledge graph completion, where both the structure of the graph and the information from the nodes are crucial. Emerging from the field of graph signal processing, GCNs have been enhanced by a variety of models that combine spectral and spatial convolution methods. Despite these improvements, there remain obstacles to fully harnessing the structural information of graphs, which is a vital component of network science. This survey presents an extensive review of GCNs and introduces a new taxonomy that classifies models into five categories: supervised, unsupervised, semi-supervised, weakly-supervised, and self-supervised GCNs. We emphasize recent innovations, discuss present challenges, and propose promising avenues for future investigations.},
  archive      = {J_WIDM},
  author       = {Amutha Sadasivan and Kavipriya Gananathan and Joe Dhanith Pal Nesamony Rose Mary and Surendiran Balasubramanian},
  doi          = {10.1002/widm.70012},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70012},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A systematic survey of graph convolutional networks for artificial intelligence applications},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A brief review on benchmarking for large language models evaluation in healthcare. <em>WIDM</em>, <em>15</em>(2), e70010. (<a href='https://doi.org/10.1002/widm.70010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reviews benchmarking methods for evaluating large language models (LLMs) in healthcare settings. It highlights the importance of rigorous benchmarking to ensure LLMs' safety, accuracy, and effectiveness in clinical applications. The review also discusses the challenges of developing standardized benchmarks and metrics tailored to healthcare-specific tasks such as medical text generation, disease diagnosis, and patient management. Ethical considerations, including privacy, data security, and bias, are also addressed, underscoring the need for multidisciplinary collaboration to establish robust benchmarking frameworks that facilitate LLMs' reliable and ethical use in healthcare. Evaluation of LLMs remains challenging due to the lack of standardized healthcare-specific benchmarks and comprehensive datasets. Key concerns include patient safety, data privacy, model bias, and better explainability, all of which impact the overall trustworthiness of LLMs in clinical settings.},
  archive      = {J_WIDM},
  author       = {Leona Cilar Budler and Hongyu Chen and Aokun Chen and Maxim Topaz and Wilson Tam and Jiang Bian and Gregor Stiglic},
  doi          = {10.1002/widm.70010},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70010},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A brief review on benchmarking for large language models evaluation in healthcare},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review on data-driven methods of lithium-ion batteries state-of-health forecasting. <em>WIDM</em>, <em>15</em>(2), e70009. (<a href='https://doi.org/10.1002/widm.70009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lithium-ion batteries are widely used in moving devices due to their many advantages compared to other battery types. The prevalence of Lithium-ion batteries is evident, playing its clear role in the operation of small devices as well as large systems such as electric vehicles, flying devices, mobile devices, and more. Monitoring lithium-ion battery health is crucial for assessing, minimizing degradation, preventing explosions, and enabling timely replacements. Assessing health often involves predicting state-of-health (SoH) or remaining useful life (RUL), with numerous studies dedicated to this field. Hence, many research studies have been conducted on predicting SoH, with a primary focus on data-driven methods based on machine learning, owing to the recent advancements in artificial intelligence (AI) techniques. To provide a systematic overview of the trends in this emerging problem, we present a comprehensive survey of classified SoH forecasting methods, with a primary focus on data-driven approaches. The paper also offers an in-depth focus on recent advancements in deep learning (DL) models, an area that has not been thoroughly discussed previously. Furthermore, we highlight the importance of input features and emphasize the critical role of temporal attributes incorporated into the models. The insights provided in this paper offer readers a comprehensive understanding of the field, equipping them to effectively advance related future work.},
  archive      = {J_WIDM},
  author       = {Thien Pham and Hung Bui and Mao Nguyen and Quang Pham and Vinh Vu and Triet Le and Tho Quan},
  doi          = {10.1002/widm.70009},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70009},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A comprehensive review on data-driven methods of lithium-ion batteries state-of-health forecasting},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing intrusion detection for IoT: A systematic review of machine learning and deep learning approaches with feature selection and data balancing. <em>WIDM</em>, <em>15</em>(2), e70008. (<a href='https://doi.org/10.1002/widm.70008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Internet of Things (IoT) continues expanding its footprint across various sectors, robust security systems to mitigate associated risks are more critical than ever. Intrusion Detection Systems (IDS) are fundamental in safeguarding IoT infrastructures against malicious activities. This systematic review aims to guide future research by addressing six pivotal research questions that underscore the development of advanced IDS tailored for IoT environments. Specifically, the review concentrates on applying machine learning (ML) and deep learning (DL) technologies to enhance IDS capabilities. It explores various feature selection methodologies aimed at developing lightweight IDS solutions that are both effective and efficient for IoT scenarios. Additionally, the review assesses different datasets and balancing techniques, which are crucial for training IDS models to perform accurately and reliably. Through a comprehensive analysis of existing literature, this review highlights significant trends, identifies current research gaps, and suggests future studies to optimize IDS frameworks for the ever-evolving IoT landscape.},
  archive      = {J_WIDM},
  author       = {S. Kumar Reddy Mallidi and Rajeswara Rao Ramisetty},
  doi          = {10.1002/widm.70008},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70008},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Optimizing intrusion detection for IoT: A systematic review of machine learning and deep learning approaches with feature selection and data balancing},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of wavelet transformation and artificial intelligence techniques in healthcare: A systemic review. <em>WIDM</em>, <em>15</em>(2), e70007. (<a href='https://doi.org/10.1002/widm.70007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of wavelet transformation and artificial intelligence techniques has demonstrated significant potential in healthcare applications. Wavelet analysis enables multi-scale signal decomposition and feature extraction that, when combined with machine and deep learning approaches, enhance the accuracy and efficiency of medical data analysis. This systematic review synthesizes 112 relevant studies from 2013 to 2023 exploring wavelet-based artificial intelligence in healthcare. Our analysis reveals that the discrete wavelet transform dominates (43% of studies), primarily used for feature extraction from biosignals (82%) and medical images. Major applications include cardiac abnormality detection (29%), neurological disorder diagnosis (27%), and mental health assessment (16%), with classification accuracies frequently exceeding 95%. Key findings indicate a shift from traditional machine learning to deep learning approaches after 2020, with emerging trends in hybrid architectures. The review identifies critical challenges in computational efficiency, optimal wavelet selection, and clinical validation. Future developments should focus on real-time processing optimization, interpretable deep learning models, multi-modal data fusion, and validation on larger clinical datasets, advancing the translation of these systems into practical clinical tools.},
  archive      = {J_WIDM},
  author       = {Samiul Based Shuvo and Syed Samiul Alam and Syeda Umme Ayman and Arbil Chakma and Massimo Salvi and Silvia Seoni and Prabal Datta Barua and Filippo Molinari and U. Rajendra Acharya},
  doi          = {10.1002/widm.70007},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70007},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Application of wavelet transformation and artificial intelligence techniques in healthcare: A systemic review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mapping the landscape of personalization: A comprehensive review of prediction and trends in recommendation systems. <em>WIDM</em>, <em>15</em>(2), e70006. (<a href='https://doi.org/10.1002/widm.70006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems (RSs) have become indispensable features in nearly all web applications. Sifting through data and alleviating information overload, these systems offer more streamlined and personalized recommendations. E-commerce giants such as Amazon, Netflix, and YouTube are offering recommendations to users based on their interests, past experiences, demographic information, etc. hence, increasing the user's engagement on these applications. This study offers a comprehensive review of recommendation systems, covering their types, fundamental techniques, and emerging trends, with a focus on the predictive models and algorithms that power personalization. This study shows how in comparison to traditional collaborative and content-based recommendation systems-building techniques, the novel approaches of deep learning, graph-based techniques, meta-learning, few-shot learning, exploration, and federated learning offer promising prospects to improve recommendation systems' scalability, privacy-preserving abilities, and accuracy. These advanced methods deliver more diverse, context-aware, and personalized recommendations by leveraging large-scale data and complex predictive algorithms. Furthermore, this paper depicts forthcoming trajectories in the field of recommendation systems, including the adoption of graph-based approaches, federated learning, and the exploration of ethical considerations. By mapping the current landscape of prediction-driven personalization and identifying emerging trends, this review serves as a valuable resource for scholars and practitioners seeking to deepen their understanding of the field and drive innovation in recommendation systems. Readers can expect to gain insights into both foundational and cutting-edge techniques and how these can shape the future of personalized recommendations.},
  archive      = {J_WIDM},
  author       = {Tamanna Sachdeva and Lalit Mohan Goyal and Mamta Mittal},
  doi          = {10.1002/widm.70006},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70006},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Mapping the landscape of personalization: A comprehensive review of prediction and trends in recommendation systems},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on process mining for curricular analysis. <em>WIDM</em>, <em>15</em>(2), e70005. (<a href='https://doi.org/10.1002/widm.70005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Educational Process Mining (EPM) is a data analysis technique that is used to improve educational processes. It is based on Process Mining (PM), which involves gathering records (logs) of events to discover process models and analyze the data from a process-centric perspective. One specific application of EPM is curriculum mining, which focuses on understanding the learning program students follow to achieve educational goals. This is important for institutional curriculum decision-making and quality improvement. Therefore, academic institutions can benefit from organizing the existing techniques, capabilities, and limitations. We conducted a systematic literature review to identify works on applying PM to curricular analysis and provide insights for further research. We reviewed 27 primary studies published across seven major databases. Our analysis classified these studies into five main research objectives: discovery of educational trajectories, identification of deviations in student behavior, bottleneck analysis, dropout/stopout analysis, and generation of recommendations. Key findings highlight challenges such as standardization to facilitate cross-university analysis, better integration of process and data mining techniques, and improved tools for educational stakeholders. This review provides a comprehensive overview of the current landscape in curricular process mining and outlines specific research opportunities to support more robust and actionable curricular analyses in educational settings.},
  archive      = {J_WIDM},
  author       = {Daniel Calegari and Andrea Delgado},
  doi          = {10.1002/widm.70005},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70005},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A systematic review on process mining for curricular analysis},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How AI contributes to poverty alleviation: A systematic literature review. <em>WIDM</em>, <em>15</em>(2), e70003. (<a href='https://doi.org/10.1002/widm.70003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence and smart city initiatives are pivotal to achieving sustainable development goals, particularly in poverty alleviation. Artificial intelligence has high potential in controlling, monitoring, and alleviating poverty, offering innovative solutions that can improve living conditions and welfare. AI-assisted poverty alleviation requires a comprehensive approach to creating supportive institutions, appropriate regulations, comprehensive training programs, and resource allocation. This article systematically reviews the AI-poverty literature based on problem-oriented innovation system functions. By screening existing articles, it analyzes 30 sources specifically focused on AI-enhanced poverty control to highlight the articles' focus and identify the neglected functions. The findings can help governments, policymakers, and scholars guide decisions to address poverty through AI and fill the existing gaps and shortcomings.},
  archive      = {J_WIDM},
  author       = {Sepehr Ghazinoory and Mercedeh Pahlavanian and Mehdi Fatemi and Fatemeh Parvin and Sayna Ahad Bhat},
  doi          = {10.1002/widm.70003},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {6},
  number       = {2},
  pages        = {e70003},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {How AI contributes to poverty alleviation: A systematic literature review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming disaster risk reduction with AI and big data: Legal and interdisciplinary perspectives. <em>WIDM</em>, <em>15</em>(2), e70011. (<a href='https://doi.org/10.1002/widm.70011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Managing complex disaster risks requires interdisciplinary efforts. Breaking down silos between law, social sciences, and natural sciences is critical for all processes of disaster risk reduction. It is essential to explore how AI enhances understanding of legal frameworks and environmental management, while also examining how legal and environmental factors may limit AI's role in society. From a co-production review perspective, drawing on insights from lawyers, social scientists, and environmental scientists, principles for responsible data mining are proposed based on safety, transparency, fairness, accountability, and contestability. This discussion offers a blueprint for interdisciplinary collaboration to create adaptive law systems based on AI integration of knowledge from environmental and social sciences. When social networks are useful for mitigating disaster risks based on AI, the legal implications related to privacy and liability of the outcomes of disaster management must be considered. Fair and accountable principles emphasize environmental considerations and foster socioeconomic discussions related to public engagement. AI also has an important role to play in education, bringing together the next generations of law, social sciences, and natural sciences to work on interdisciplinary solutions in harmony. Although emerging AI approaches can be powerful tools for disaster management, they must be implemented with ethical considerations and safeguards to address concerns about bias, transparency, and privacy. The responsible execution of AI approaches, based on the dynamic interplay between AI, law, and environmental risk, promotes sustainable and equitable practices in data mining.},
  archive      = {J_WIDM},
  author       = {Kwok P. Chun and Thanti Octavianti and Nilay Dogulu and Hristos Tyralis and Georgia Papacharalampous and Ryan Rowberry and Pingyu Fan and Mark Everard and Maria Francesch-Huidobro and Wellington Migliari and David M. Hannah and John Travis Marshall and Rafael Tolosana Calasanz and Chad Staddon and Ida Ansharyani and Bastien Dieppois and Todd R. Lewis and Juli Ponce and Silvia Ibrean and Tiago Miguel Ferreira and Chinkie Peliño-Golle and Ye Mu and Manuel Davila Delgado and Elizabeth Silvestre Espinoza and Martin Keulertz and Deepak Gopinath and Cheng Li},
  doi          = {10.1002/widm.70011},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  number       = {2},
  pages        = {e70011},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Transforming disaster risk reduction with AI and big data: Legal and interdisciplinary perspectives},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey on latest advances in natural language processing applications of generative adversarial networks. <em>WIDM</em>, <em>15</em>(1), e70004. (<a href='https://doi.org/10.1002/widm.70004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data mining and natural language processing (NLP) are fundamental fields that interact in many ways. Text mining shares many topics, such as sentiment analysis and content understanding. Combining these two fields enables more efficient mining of text data and the extraction of valuable information. In particular, the GAN (Generative Adversarial Network) architecture has achieved success in image generation and has started to be used on text data. However, training GANs is fraught with difficulties due to the complexity of text data. Linguistic studies show important differences between languages. Language is characterized by fluidity, ambiguity, and context-sensitive interpretations, and text-generating GAN models can struggle to deal with these complexities. The interaction between data quality, language structure, and complex interpretation can lead to inconsistency and ambiguity in the text production of GAN models. These problems are particularly pronounced when complexities such as semantic subtleties, idiomatic expressions, and context-dependent usages come into play. Text generation is an area of GAN models used in NLP to generate language and enrich text-based applications. Work in this area can contribute to analyzing, classifying, and processing text data. Many methods and techniques have been proposed to improve the performance of text GANs. However, some problems may be encountered in the optimization of these methods. Therefore, it is essential to use optimized methods. In conclusion, GANs can be an important tool to improve text generation in NLP. Still, they require continuous research and innovation to deal with factors such as language complexity and data quality.},
  archive      = {J_WIDM},
  author       = {Canan Koç and Fatih Özyurt and Laszlo Barna Iantovics},
  doi          = {10.1002/widm.70004},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e70004},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Survey on latest advances in natural language processing applications of generative adversarial networks},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated detection of neurological and mental health disorders using EEG signals and artificial intelligence: A systematic review. <em>WIDM</em>, <em>15</em>(1), e70002. (<a href='https://doi.org/10.1002/widm.70002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental and neurological disorders significantly impact global health. This systematic review examines the use of artificial intelligence (AI) techniques to automatically detect these conditions using electroencephalography (EEG) signals. Guided by Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA), we reviewed 74 carefully selected studies published between 2013 and August 2024 that used machine learning (ML), deep learning (DL), or both of these two methods to detect neurological and mental health disorders automatically using EEG signals. The most common and most prevalent neurological and mental health disorder types were sourced from major databases, including Scopus, Web of Science, Science Direct, PubMed, and IEEE Xplore. Epilepsy, depression, and Alzheimer's disease are the most studied conditions that meet our evaluation criteria, 32, 12, and 10 studies were identified on these topics, respectively. Conversely, the number of studies meeting our criteria regarding stress, schizophrenia, Parkinson's disease, and autism spectrum disorders was relatively more average: 6, 4, 3, and 3, respectively. The diseases that least met our evaluation conditions were one study each of seizure, stroke, anxiety diseases, and one study examining Alzheimer's disease and epilepsy together. Support Vector Machines (SVM) were most widely used in ML methods, while Convolutional Neural Networks (CNNs) dominated DL approaches. DL methods generally outperformed traditional ML, as they yielded higher performance using huge EEG data. We observed that the complex decision process during feature extraction from EEG signals in ML-based models significantly impacted results, while DL-based models handled this more efficiently. AI-based EEG analysis shows promise for automated detection of neurological and mental health conditions. Future research should focus on multi-disease studies, standardizing datasets, improving model interpretability, and developing clinical decision support systems to assist in the diagnosis and treatment of these disorders.},
  archive      = {J_WIDM},
  author       = {Hakan Uyanik and Abdulkadir Sengur and Massimo Salvi and Ru-San Tan and Jen Hong Tan and U. Rajendra Acharya},
  doi          = {10.1002/widm.70002},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e70002},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Automated detection of neurological and mental health disorders using EEG signals and artificial intelligence: A systematic review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of approaches to early rumor detection on microblogging platforms: Computational and socio-psychological insights. <em>WIDM</em>, <em>15</em>(1), e70001. (<a href='https://doi.org/10.1002/widm.70001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media, particularly microblogging platforms, are essential for rapid information sharing and public discussion but often allow rumors, that is, unverified information, to spread rapidly during events or persist over time. These platforms also offer opportunities to study the dynamics of rumors and develop computational methods to assess their veracity. In this paper, we provide a comprehensive review of existing theoretical foundations, interdisciplinary challenges, and emerging advancements in rumor detection research, with a focus on integrating theoretical and computational approaches. Drawing on insights from computer science, cognitive psychology, and sociology, we explore methodologies, such as multimodal fusion, graph-based models, and attention mechanisms, while highlighting gaps in real-world scalability, ethical transparency, and cross-platform adaptability. Using a systematic literature review and bibliometric analysis, we identify trends, methods, and gaps in current research. Our findings emphasize interdisciplinary collaboration to develop adaptable, efficient, and ethical rumor detection strategies. We also highlight the critical role of combining socio-psychological insights with advanced computational techniques to address the human factors in rumor spread. Furthermore, we emphasize the importance of designing systems that remain effective across diverse cultural and linguistic contexts, enhancing their global applicability. We propose a conceptual framework integrating diverse theories and computational techniques, offering a roadmap for improving detection systems and addressing misinformation challenges on microblogging platforms.},
  archive      = {J_WIDM},
  author       = {Lazarus Kwao and Yang Yang and Jie Zou and Jing Ma},
  doi          = {10.1002/widm.70001},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e70001},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey of approaches to early rumor detection on microblogging platforms: Computational and socio-psychological insights},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ICT-driven data mining analysis in civil engineering: A scientometric review. <em>WIDM</em>, <em>15</em>(1), e70000. (<a href='https://doi.org/10.1002/widm.70000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the contemporary landscape, the remarkable evolution of civil engineering is being driven by the pervasive integration of Information and Communication Technology (ICT). ICT-driven innovations are playing a crucial role in advancing sustainable development goals by promoting energy efficiency, minimizing resource consumption, and fostering resilient infrastructure. Solutions such as smart grids, intelligent transportation systems, and sustainable urban planning are integral to this progress to address global challenges. The goal of the current study is to conduct a scientometric analysis of scholarly literature published in the recent decade within the domain of ICT-assisted civil engineering. To achieve this, the study categorizes the civil engineering field into seven major subfields. It includes structural engineering, geotechnical engineering, transportation engineering, water resources engineering, environmental engineering, construction management, and urban planning and design. Employing CiteSpace as the analytical tool, the research offers insights into the intellectual foundations of the civil engineering. This is accomplished through reference co-citation analysis, cluster analysis, and burst reference analysis. The results demonstrate the adoption of advanced technologies such as Internet of Things (IoT), Machine Learning (ML), Extreme Gradient Boosting (XGBoost), and artificial neural networks in resolving complex civil engineering challenges that reflect the dynamism and diversity of the field. Moreover, it addresses current research challenges within this knowledge domain and explores potential research prospects. The findings emphasize the importance of collaborative efforts among academia, industry stakeholders, and government entities.},
  archive      = {J_WIDM},
  author       = {Kashvi Sood},
  doi          = {10.1002/widm.70000},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e70000},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {ICT-driven data mining analysis in civil engineering: A scientometric review},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on medical image segmentation: Datasets, technical models, challenges and solutions. <em>WIDM</em>, <em>15</em>(1), e1574. (<a href='https://doi.org/10.1002/widm.1574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is prerequisite in computer-aided diagnosis. As the field experiences tremendous paradigm changes since the introduction of foundation models, technicality of deep medical segmentation model is no longer a privilege limited to computer science researchers. A comprehensive educational resource suitable for researchers of broad, different backgrounds such as biomedical and medicine, is needed. This review strategically covers the evolving trends that happens to different fundamental components of medical image segmentation such as the emerging of multimodal medical image datasets, updates on deep learning libraries, classical-to-contemporary development in deep segmentation models and latest challenges with focus on enhancing the interpretability and generalizability of model. Last, the conclusion section highlights on future trends in deep medical segmentation that worth further attention and investigations.},
  archive      = {J_WIDM},
  author       = {Hong-Seng Gan and Muhammad Hanif Ramlee and Zimu Wang and Akinobu Shimizu},
  doi          = {10.1002/widm.1574},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1574},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review on medical image segmentation: Datasets, technical models, challenges and solutions},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trace encoding techniques for multi-perspective process mining: A comparative study. <em>WIDM</em>, <em>15</em>(1), e1573. (<a href='https://doi.org/10.1002/widm.1573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining (PM) comprises a variety of methods for discovering information about processes from their execution logs. Some of them, such as trace clustering, trace classification, and anomalous trace detection require a preliminary preprocessing step in which the raw data is encoded into a numerical feature space. To this end, encoding techniques are used to generate vectorial representations of process traces. Most of the PM literature provides trace encoding techniques that look at the control flow, that is, only encode the sequence of activities that characterize a process trace disregarding other process data that is fundamental for effectively describing the process behavior. To fill this gap, in this article we show 19 trace encoding methods that work in a multi-perspective manner, that is, by embedding events and trace attributes in addition to activity names into the vectorial representations of process traces. We also provide an extensive experimental study where these techniques are applied to real-life datasets and compared to each other.},
  archive      = {J_WIDM},
  author       = {Antonino Rullo and Farhana Alam and Edoardo Serra},
  doi          = {10.1002/widm.1573},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1573},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Trace encoding techniques for multi-perspective process mining: A comparative study},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyper-parameter optimization of kernel functions on multi-class text categorization: A comparative evaluation. <em>WIDM</em>, <em>15</em>(1), e1572. (<a href='https://doi.org/10.1002/widm.1572'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, machine learning (ML) has witnessed a paradigm shift in kernel function selection, which is pivotal in optimizing various ML models. Despite multiple studies about its significance, a comprehensive understanding of kernel function selection, particularly about model performance, still needs to be explored. Challenges remain in selecting and optimizing kernel functions to improve model performance and efficiency. The study investigates how gamma parameter and cost parameter influence performance metrics in multi-class classification tasks using various kernel-based algorithms. Through sensitivity analysis, the impact of these parameters on classification performance and computational efficiency is assessed. The experimental setup involves deploying ML models using four kernel-based algorithms: Support Vector Machine, Radial Basis Function, Polynomial Kernel, and Sigmoid Kernel. Data preparation includes text processing, categorization, and feature extraction using TfidfVectorizer, followed by model training and validation. Results indicate that Support Vector Machine with default settings and Radial Basis Function kernel consistently outperforms polynomial and sigmoid kernels. Adjusting gamma improves model accuracy and precision, highlighting its role in capturing complex relationships. Regularization cost parameters, however, show minimal impact on performance. The study also reveals that configurations with moderate gamma values achieve better balance between performance and computational time compared to higher gamma values or no gamma adjustment. The findings underscore the delicate balance between model performance and computational efficiency by highlighting the trade-offs between model complexity and efficiency.},
  archive      = {J_WIDM},
  author       = {Michael Loki and Agnes Mindila and Wilson Cheruiyot},
  doi          = {10.1002/widm.1572},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1572},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Hyper-parameter optimization of kernel functions on multi-class text categorization: A comparative evaluation},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Business analytics in customer lifetime value: An overview analysis. <em>WIDM</em>, <em>15</em>(1), e1571. (<a href='https://doi.org/10.1002/widm.1571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In customer-oriented systems, customer lifetime value (CLV) has been of significant importance for academia and marketing practitioners, especially within the scope of analytical modeling. CLV is a critical approach to managing and organizing a company's profitability. With the vast availability of consumer data, business analytics (BA) tools and approaches, alongside CLV models, have been applied to gain deeper insights into customer behaviors and decision-making processes. Despite the recognized importance of CLV, there is a noticeable gap in comprehensive analyses and reviews of BA techniques applied to CLV. This study aims to fill this gap by conducting a thorough survey of the state-of-the-art investigations on CLV models integrated with BA approaches, thereby contributing to a research agenda in this field. The review methodology consists of three main steps: identification of relevant studies, creating a coding plan, and ensuring coding reliability. First, relevant studies were identified using predefined keywords. Next, a coding plan—one of the study's significant contributions—was developed to evaluate these studies comprehensively. Finally, the coding plan's reliability was tested by three experts before being applied to the selected studies. Additionally, specific evaluation criteria in the coding plan were implemented to introduce new insights. This study presents exciting and valuable results from various perspectives, providing a crucial reference for academic researchers and marketing practitioners interested in the intersection of BA and CLV.},
  archive      = {J_WIDM},
  author       = {Onur Dogan and Abdulkadir Hiziroglu and Ali Pisirgen and Omer Faruk Seymen},
  doi          = {10.1002/widm.1571},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1571},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Business analytics in customer lifetime value: An overview analysis},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge graph for solubility big data: Construction and applications. <em>WIDM</em>, <em>15</em>(1), e1570. (<a href='https://doi.org/10.1002/widm.1570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dissolution refers to the process in which solvent molecules and solute molecules attract and combine with each other. The extensive solubility data generated from the dissolution of various compounds under different conditions, is distributed across structured or semi-structured formats in various media, such as text, web pages, tables, images, and databases. These data exhibit multi-source and unstructured features, aligning with the typical 5 V characteristics of big data. A solubility big data technology system has emerged under the fusion of solubility data and big data technologies. However, the acquisition, fusion, storage, representation, and utilization of solubility big data are encountering new challenges. Knowledge Graphs, known as extensive systems for representing and applying knowledge, can effectively describe entities, concepts, and relations across diverse domains. The construction of solubility big data knowledge graph holds substantial value in the retrieval, analysis, utilization, and visualization of solubility knowledge. Throwing out a brick to attract a jade, this paper focuses on the solubility big data knowledge graph and, firstly, summarizes the architecture of solubility knowledge graph construction. Secondly, the key technologies such as knowledge extraction, knowledge fusion, and knowledge reasoning of solubility big data are emphasized, along with summarizing the common machine learning methods in knowledge graph construction. Furthermore, this paper explores application scenarios, such as knowledge question answering and recommender systems for solubility big data. Finally, it presents a prospective view of the shortcomings, challenges, and future directions related to the construction of solubility big data knowledge graph. This article proposes the research direction of solubility big data knowledge graph, which can provide technical references for constructing a solubility knowledge graph. At the same time, it serves as a comprehensive medium for describing data, resources, and their applications across diverse fields such as chemistry, materials, biology, energy, medicine, and so on. It further aids in knowledge retrieval and mining, analysis and utilization, and visualization across various disciplines.},
  archive      = {J_WIDM},
  author       = {Xiao Haiyang and Yan Ruomei and Wu Yan and Guan Lixin and Li Mengshan},
  doi          = {10.1002/widm.1570},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1570},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Knowledge graph for solubility big data: Construction and applications},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using machine learning for systematic literature review case in point: Agile software development. <em>WIDM</em>, <em>15</em>(1), e1569. (<a href='https://doi.org/10.1002/widm.1569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systematic literature reviews (SLRs) are essential for researchers to keep up with past and recent research in their domains. However, the rapid growth in knowledge creation and the rising number of publications have made this task increasingly complex and challenging. Moreover, most systematic literature reviews are performed manually, which requires significant effort and creates potential bias. The risk of bias is particularly relevant in the data synthesis task, where researchers interpret each study's evidence and summarize the results. This study uses an experimental approach to explore using machine learning (ML) techniques in the SLR process. Specifically, this study replicates a study that manually performed sentiment analysis for the data synthesis step to determine the polarity (negative or positive) of evidence extracted from studies in the field of agile methodology. This study employs a lexicon-based approach to sentiment analysis and achieves an accuracy rate of approximately 86.5% in identifying study evidence polarity.},
  archive      = {J_WIDM},
  author       = {Itzik David and Roy Gelbard},
  doi          = {10.1002/widm.1569},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1569},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Using machine learning for systematic literature review case in point: Agile software development},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dimensionality reduction for data analysis with quantum feature learning. <em>WIDM</em>, <em>15</em>(1), e1568. (<a href='https://doi.org/10.1002/widm.1568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve data analysis and feature learning, this study compares the effectiveness of quantum dimensionality reduction (qDR) techniques to classical ones. In this study, we investigate several qDR techniques on a variety of datasets such as quantum Gaussian distribution adaptation (qGDA), quantum principal component analysis (qPCA), quantum linear discriminant analysis (qLDA), and quantum t-SNE (qt-SNE). The Olivetti Faces, Wine, Breast Cancer, Digits, and Iris are among the datasets used in this investigation. Through comparison evaluations against well-established classical approaches, such as classical PCA (cPCA), classical LDA (cLDA), and classical GDA (cGDA), and using well-established metrics like loss, fidelity, and processing time, the effectiveness of these techniques is assessed. The findings show that cPCA produced positive results with the lowest loss and highest fidelity when used on the Iris dataset. On the other hand, quantum uniform manifold approximation and projection (qUMAP) performs well and shows strong fidelity when tested against the Wine dataset, but ct-SNE shows mediocre performance against the Digits dataset. Isomap and locally linear embedding (LLE) function differently depending on the dataset. Notably, LLE showed the largest loss and lowest fidelity on the Olivetti Faces dataset. The hypothesis testing findings showed that the qDR strategies did not significantly outperform the classical techniques in terms of maintaining pertinent information from quantum datasets. More specifically, the outcomes of paired t -tests show that when it comes to the ability to capture complex patterns, there are no statistically significant differences between the cPCA and qPCA, the cLDA and qLDA, and the cGDA and qGDA. According to the findings of the assessments of mutual information (MI) and clustering accuracy, qPCA may be able to recognize patterns more clearly than standardized cPCA. Nevertheless, there is no discernible improvement between the qLDA and qGDA approaches and their classical counterparts.},
  archive      = {J_WIDM},
  author       = {Shyam R. Sihare},
  doi          = {10.1002/widm.1568},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1568},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Dimensionality reduction for data analysis with quantum feature learning},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial attacks in explainable machine learning: A survey of threats against models and humans. <em>WIDM</em>, <em>15</em>(1), e1567. (<a href='https://doi.org/10.1002/widm.1567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable deployment of machine learning models such as neural networks continues to be challenging due to several limitations. Some of the main shortcomings are the lack of interpretability and the lack of robustness against adversarial examples or out-of-distribution inputs. In this paper, we comprehensively review the possibilities and limits of adversarial attacks for explainable machine learning models. First, we extend the notion of adversarial examples to fit in explainable machine learning scenarios where a human assesses not only the input and the output classification, but also the explanation of the model's decision. Next, we propose a comprehensive framework to study whether (and how) adversarial examples can be generated for explainable models under human assessment. Based on this framework, we provide a structured review of the diverse attack paradigms existing in this domain, identify current gaps and future research directions, and illustrate the main attack paradigms discussed. Furthermore, our framework considers a wide range of relevant yet often ignored factors such as the type of problem, the user expertise or the objective of the explanations, in order to identify the attack strategies that should be adopted in each scenario to successfully deceive the model (and the human). The intention of these contributions is to serve as a basis for a more rigorous and realistic study of adversarial examples in the field of explainable machine learning.},
  archive      = {J_WIDM},
  author       = {Jon Vadillo and Roberto Santana and Jose A. Lozano},
  doi          = {10.1002/widm.1567},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1567},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Adversarial attacks in explainable machine learning: A survey of threats against models and humans},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application-based review of soft computational methods to enhance industrial practices abetted by the patent landscape analysis. <em>WIDM</em>, <em>15</em>(1), e1564. (<a href='https://doi.org/10.1002/widm.1564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft computing is a collective methodology that touches all engineering and technology fields owing to its easiness in solving various problems while comparing the conventional methods. Many analytical methods are taken over by this soft computing technique and resolve it accurately and the soft computing has given a paradigm shift. The flexibility in soft computing results in swift knowledge acquisition processing and the information supply renders versatile and affordable technological system. Besides, the accuracy with which the soft computing technique predicts the parameters has transformed the industrial productivity to a whole new level. The interest of this article focuses on versatile applications of SC methods to forecast the technological changes which intend to reorient the progress of various industries, and this is ascertained by a patent landscape analysis. The patent landscape revealed the players who are in the segment consistently and this also provides how this field moves on in the future and who could be a dominant country for a specific technology. Alongside, the accuracy of the soft computing method for a particular practice has also been mentioned indicating the feasibility of the technique. The novel part of this article lies in patent landscape analysis compared with the other data while the other part is the discussion of application of computational techniques to various industrial practices. The progress of various engineering applications integrating them with the patent landscape analysis must be envisaged for a better understanding of the future of all these applications resulting in an improved productivity.},
  archive      = {J_WIDM},
  author       = {S. Tamilselvan and G. Dhanalakshmi and D. Balaji and L. Rajeshkumar},
  doi          = {10.1002/widm.1564},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {1},
  pages        = {e1564},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Application-based review of soft computational methods to enhance industrial practices abetted by the patent landscape analysis},
  volume       = {15},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
