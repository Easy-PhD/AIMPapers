<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AAIL</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aail">AAIL - 3</h2>
<ul>
<li><details>
<summary>
(2026). Automated AI-based lung disease classification using point-of-care ultrasound. <em>AAIL</em>, <em>7</em>(1), e70012. (<a href='https://doi.org/10.1002/ail2.70012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely and accurate diagnosis of lung diseases is critical for reducing related morbidity and mortality. Lung ultrasound (LUS) has emerged as a useful point-of-care tool for evaluating various lung conditions. However, interpreting LUS images remains challenging due to operator-dependent variability, low image quality, and limited availability of experts in many regions. In this study, we present a lightweight and efficient deep learning model, ParSE-CNN, alongside fine-tuned versions of VGG-16, InceptionV3, Xception, and Vision Transformer architectures, to classify LUS images into three categories: COVID-19, other lung pathology, and healthy lung. Models were trained using data from public sources and Ugandan healthcare facilities, and evaluated on a held-out Ugandan dataset. Fine-tuned VGG-16 achieved the highest classification performance with 98% accuracy, 97% precision, 98% recall, and a 97% F1-score. ParSE-CNN yielded a competitive accuracy of 95%, precision of 94%, recall of 95%, and F1-score of 97% while offering a 58.3% faster inference time (0.006 s vs. 0.014 s) and a lower parameter count (5.18 M vs. 10.30 M) than VGG-16. To enhance input quality, we developed a preprocessing pipeline, and to improve interpretability, we employed Grad-CAM heatmaps, which showed high alignment with radiologically relevant features. Finally, ParSE-CNN was integrated into a mobile LUS workflow with a PC backend, enabling real-time AI-assisted diagnosis at the point of care in low-resource settings.},
  archive      = {J_AAIL},
  author       = {Nixson Okila and Andrew Katumba and Joyce Nakatumba-Nabende and Sudi Murindanyi and Jonathan Serugunda and Cosmas Mwikirize and Samuel Bugeza and Anthony Oriekot and Juliet Bosa and Eva Nabawanuka},
  doi          = {10.1002/ail2.70012},
  journal      = {Applied AI Letters},
  month        = {2},
  number       = {1},
  pages        = {e70012},
  shortjournal = {Appl. AI Lett.},
  title        = {Automated AI-based lung disease classification using point-of-care ultrasound},
  volume       = {7},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An explainable and lightweight CNN framework for robust potato leaf disease classification using grad-CAM visualization. <em>AAIL</em>, <em>7</em>(1), e70011. (<a href='https://doi.org/10.1002/ail2.70011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For identifying foliar diseases in crops at an early stage, accurate detection is necessary in maintaining food security, minimizing economic losses, and cultivating sustainable agriculture. In staple crops, potato is highly vulnerable to lethal diseases like Early Blight and Late Blight that can drastically affect both the quality and the quantity of the yield. Conventional diagnostic procedures using visual observation and/or laboratory examinations are frequently tedious, time-consuming, and susceptible to error. To address these problems, in this research, we propose a novel deep learning architecture using a customized convolutional neural network (CNN) for classifying potato leaf images into three distinct classes, namely Early Blight, Late Blight and Healthy. The model is trained on a selective and heavily augmented subset of the PlantVillage dataset containing 11,593 images and further optimized using regularization techniques like dropout and batch normalization. The system architecture is intended to keep the tradeoff between performance and computational efficiency, so as to fit real-world agricultural scenarios. To increase interpretability and improve trust, we use the Gradient-weighted Class Activation Mapping (Grad-CAM) to visualize the regions in space of the leaves that most contribute to the prediction of the model. The experimental results show superior performance and the proposed model reaches 99.14% accuracy and close-to-perfect precision, recall and F1-scores in all of the classes. Grad-CAM visualizations validate that the model is robust in attending to biologically meaningful regions for the disease symptoms. In addition, we perform comparative analyses against recent state-of-the-art models, and demonstrate that the proposed approach outperforms the others in accuracy and interpretability.},
  archive      = {J_AAIL},
  author       = {MD Jiabul Hoque and Md. Saiful Islam},
  doi          = {10.1002/ail2.70011},
  journal      = {Applied AI Letters},
  month        = {2},
  number       = {1},
  pages        = {e70011},
  shortjournal = {Appl. AI Lett.},
  title        = {An explainable and lightweight CNN framework for robust potato leaf disease classification using grad-CAM visualization},
  volume       = {7},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient few-shot learning in remote sensing: Fusing vision and vision-language models. <em>AAIL</em>, <em>7</em>(1), e70010. (<a href='https://doi.org/10.1002/ail2.70010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing has become a vital tool across sectors such as urban planning, environmental monitoring, and disaster response. Although the volume of data generated has increased significantly, traditional vision models are often constrained by the requirement for extensive domain-specific labelled data and their limited ability to understand the context within complex environments. Vision Language Models offer a complementary approach by integrating visual and textual data; however, their application to remote sensing remains underexplored, particularly given their generalist nature. This work investigates the combination of vision models and VLMs to enhance image analysis in remote sensing, with a focus on aircraft detection and scene understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and Gemini aims to achieve more accurate and contextually aware image interpretation. Performance is evaluated on both labelled and unlabelled remote sensing data, as well as degraded image scenarios that are crucial for remote sensing. The findings show an average MAE improvement of 48.46% across models in the accuracy of aircraft detection and counting, especially in challenging conditions, in both raw and degraded scenarios. A 6.17% improvement in CLIPScore for comprehensive understanding of remote sensing images is obtained. The proposed approach combining traditional vision models and VLMs paves the way for more advanced and efficient remote sensing image analysis, especially in few-shot learning scenarios.},
  archive      = {J_AAIL},
  author       = {Jia Yun Chua and Argyrios Zolotas and Miguel Arana-Catania},
  doi          = {10.1002/ail2.70010},
  journal      = {Applied AI Letters},
  month        = {2},
  number       = {1},
  pages        = {e70010},
  shortjournal = {Appl. AI Lett.},
  title        = {Efficient few-shot learning in remote sensing: Fusing vision and vision-language models},
  volume       = {7},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
