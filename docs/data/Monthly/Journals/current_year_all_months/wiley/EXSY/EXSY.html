<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EXSY</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="exsy">EXSY - 418</h2>
<ul>
<li><details>
<summary>
(2025). Twitter and sentiment analysis for wildfire heat mapping. <em>EXSY</em>, <em>42</em>(11), e70147. (<a href='https://doi.org/10.1111/exsy.70147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, automated intelligent systems play an increasingly vital role in aiding decision-making processes across various fields. Firefighting represents a crucial area where accurate information gathering is paramount for efficient resource allocation. Social media platforms as Twitter (or X) have emerged as valuable sources of real-time data, often referred to as ‘citizen science’, offering additional insights alongside traditional data sources. In this work, we introduce a novel pipeline that leverages Natural Language Processing (NLP) techniques and Twitter data, utilising transformer models to identify and monitor wildfire incidents. Expanding on this approach, we incorporate sentiment analysis to provide deeper insights into public perceptions and emotions related to fire events. Additionally, we present visual representations of geographic data through heat mapping, potentially aiding firefighters in making informed decisions. By integrating advanced NLP techniques with social media data, our approach presents a promising strategy for enhancing wildfire management efforts.},
  archive      = {J_EXSY},
  author       = {Catarina Silva and Isabel Carvalho and Bruno Ferreira and João Cabral Pinto and Alberto Cardoso and Hugo Gonçalo Oliveira},
  doi          = {10.1111/exsy.70147},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70147},
  shortjournal = {Expert Syst.},
  title        = {Twitter and sentiment analysis for wildfire heat mapping},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotional climate recognition in speech-based conversations: Leveraging deep bispectral image analysis and affect dynamics. <em>EXSY</em>, <em>42</em>(11), e70146. (<a href='https://doi.org/10.1111/exsy.70146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing availability of conversational data across multiple platforms has intensified interest in dynamic emotion recognition. Speech plays a pivotal role in shaping the emotional climate (EC) of peer conversations. We propose DeepBispec, the first framework to integrate deep bispectral image analysis with affect dynamics (AD) for speech-based EC recognition. Bispectrum representations capture nonlinear and non-Gaussian speech characteristics, while AD descriptors model temporal emotion fluctuations. Evaluated on K-EmoCon, IEMOCAP and SEWA datasets, DeepBispec consistently improved EC classification performance. For example, on K-EmoCon, arousal accuracy increased from 79.0% (bispectrum only) to 81.4% (with AD), while valence accuracy improved from 76.8% to 77.5%; similar trends were observed for IEMOCAP and SEWA. DeepBispec outperformed strong CNN, LSTM, and Transformer baselines, demonstrating robust cross-lingual performance across seven languages. These findings highlight its potential for real-world applications such as mental health monitoring, affect-aware learning platforms and empathetic dialogue systems.},
  archive      = {J_EXSY},
  author       = {Ghada Alhussein and Mohanad Alkhodari and Shiza Saleem and Ahsan H. Khandoker and Leontios J. Hadjileontiadis},
  doi          = {10.1111/exsy.70146},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70146},
  shortjournal = {Expert Syst.},
  title        = {Emotional climate recognition in speech-based conversations: Leveraging deep bispectral image analysis and affect dynamics},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous time markov chain for smartwatch sensors. <em>EXSY</em>, <em>42</em>(11), e70144. (<a href='https://doi.org/10.1111/exsy.70144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series forecasting is essential for predicting events in the future and for tracking objects. The conventional recurrent neural network model needs to pad the target with zeros when handling long inputs, resulting in a loss in accuracy. Recently, it was proposed to divide a time series input into patches and merge the learned weights. However, such a model is difficult to interpret. In this article, we consider a mixture of continuous and discrete Markov states to model long-range time dependencies. For example, in a vehicle, each gear level can be a discrete state and the throttle input is continuously controlled to maximise the efficiency of the engine. Data collected from the sensor is prone to noise due to component faults or external disturbances. Hence, we apply a stability constraint to select samples for training. We validate our algorithm on three datasets: (1) Apple Watch, (2) Car engine and (3) Election tweets. On all datasets, we achieve an improvement in the range of 5%–20% in the F-measure. Furthermore, the features learned are easy to explain in terms of real-world scenarios.},
  archive      = {J_EXSY},
  author       = {Iti Chaturvedi and Wei Liang Seow and Amber Hogarth and Luca Adornetto and Erik Cambria},
  doi          = {10.1111/exsy.70144},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70144},
  shortjournal = {Expert Syst.},
  title        = {Continuous time markov chain for smartwatch sensors},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An overview of Q-learning and deep Q-learning for an autonomous multi-UAV wireless network. <em>EXSY</em>, <em>42</em>(11), e70143. (<a href='https://doi.org/10.1111/exsy.70143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various fields, including environmental monitoring, communication, surveillance, and disaster response, unmanned aerial vehicles (UAVs) are increasingly essential. Autonomous multi-UAV wireless networks (MUWNs), where multiple UAVs work together, offer powerful solutions in these fields. However, making these networks fully autonomous is challenging, especially concerning real-time decision-making, coordination, and resource management. In the field of literature, there is a demand for a comprehensive survey of recent developments in Q-learning (QL) and deep Q-learning (DQL) within MUWNs. To address this gap, we provide a thorough evaluation of QL and DQL approaches, focusing on their application in autonomous MUWNs. We highlight their roles in route planning, communication relays, and network optimization, discussing advantages and limitations. The survey also explores key challenges, including scalability, energy efficiency, and real-time adaptability, and reviews how QL/DQL enhancements address them. In particular, this paper provides an overview of some QL and DQL applications in MUWNs, such as data retrieval, monitoring, aggregation, resource allocation, and task scheduling to support wireless connectivity, UAV-assisted autonomous trajectory planning, navigation, security, and jamming avoidance. The use of these technologies enables the effective use of UAVs in smart cities, monitoring of industrial complexes, agricultural surveys, and border security. Additionally, using the knowledge gathered from our review, we identify and discuss several open challenges.},
  archive      = {J_EXSY},
  author       = {Vikash Chandra Sharma and Saugata Roy},
  doi          = {10.1111/exsy.70143},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70143},
  shortjournal = {Expert Syst.},
  title        = {An overview of Q-learning and deep Q-learning for an autonomous multi-UAV wireless network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traversal learning coordination for lossless and efficient distributed learning. <em>EXSY</em>, <em>42</em>(11), e70141. (<a href='https://doi.org/10.1111/exsy.70141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL) and SplitFed Learning (SFL). Traditional FL often suffers an accuracy drop during aggregation due to its averaging function, while SL and SFL face increased loss due to the independent gradient updates on each split network. TL adopts a unique strategy where the model traverses the nodes during forward propagation (FP) and performs backward propagation (BP) at the orchestrator, effectively implementing centralised learning (CL) principles within a distributed environment. The orchestrator is tasked with generating virtual batches and planning the model's sequential node visits during FP, aligning them with the ordered index of the data within these batches. We conducted experiments on six datasets representing diverse characteristics across various domains. Our evaluation demonstrates that TL is on par with classic CL approaches in terms of accurate inference, thereby offering a viable and robust solution for DL tasks. TL outperformed other DL methods and improved accuracy by 7.85% for independent and identically distributed (IID) datasets, macro F1-score by 1.06% for non-IID datasets, accuracy by 2.05% for text classification and AUC by 1.41% and 2.82% for medical and financial datasets, respectively. By effectively preserving data privacy while maintaining performance, TL represents a significant advancement in DL methodologies.},
  archive      = {J_EXSY},
  author       = {Erdenebileg Batbaatar and Jeonggeol Kim and Yongcheol Kim and Young Yoon},
  doi          = {10.1111/exsy.70141},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70141},
  shortjournal = {Expert Syst.},
  title        = {Traversal learning coordination for lossless and efficient distributed learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The economic globalization for sustainable management of overseas trade enterprise logistics. <em>EXSY</em>, <em>42</em>(11), e70139. (<a href='https://doi.org/10.1111/exsy.70139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: W. Zuo , X. Zhang , and Y. Ge , “ The Economic Globalization for Sustainable Management of Overseas Trade Enterprise Logistics ”, Expert Systems 41 , no. 5 ( 2024 ): e12887. https://doi.org/10.1111/exsy.12887 . The above article, published online on 16 December 2021, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors did not respond to our notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70139},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70139},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The economic globalization for sustainable management of overseas trade enterprise logistics},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Linking AI supply chain strength to sustainable development and innovation: A country-level analysis. <em>EXSY</em>, <em>42</em>(11), e70138. (<a href='https://doi.org/10.1111/exsy.70138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: H. Wang , “ Linking AI Supply Chain Strength to Sustainable Development and Innovation: A Country-Level Analysis ”, Expert Systems 41 , no. 5 ( 2024 ): e12973. https://doi.org/10.1111/exsy.12973 . The above article, published online on 03 March 2022, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The author did not respond to our notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70138},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70138},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Linking AI supply chain strength to sustainable development and innovation: A country-level analysis},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The model of tibetan thangka sales under blockchain technology. <em>EXSY</em>, <em>42</em>(11), e70137. (<a href='https://doi.org/10.1111/exsy.70137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: Y. Fang , “ The Model of Tibetan Thangka Sales under Blockchain Technology ”, Expert Systems 41 , no. 5 ( 2024 ): e12989. https://doi.org/10.1111/exsy.12989 . The above article, published online on 09 March 2022, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. In addition, further investigation by the editors found that the article's topic was not within the scope of the journal. In view of the clear evidence of compromised peer review, the parties agreed that the paper must be retracted. The author did not respond to our notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70137},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70137},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The model of tibetan thangka sales under blockchain technology},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Analysis and design of financial data mining system based on fuzzy clustering. <em>EXSY</em>, <em>42</em>(11), e70136. (<a href='https://doi.org/10.1111/exsy.70136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: H. Li , “ Analysis and Design of Financial Data Mining System Based on Fuzzy Clustering ”, Expert Systems 41 , no. 5 ( 2024 ): e13031. https://doi.org/10.1111/exsy.13031 . The above article, published online on 20 March 2022, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The author did not respond to our notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70136},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70136},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Analysis and design of financial data mining system based on fuzzy clustering},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Dynamic relationship network and international management of enterprise supply chain by particle swarm optimization algorithm under deep learning. <em>EXSY</em>, <em>42</em>(11), e70135. (<a href='https://doi.org/10.1111/exsy.70135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: M. Chen and W. Du , “ Dynamic Relationship Network and International Management of Enterprise Supply Chain by Particle Swarm Optimization Algorithm under Deep Learning ”, Expert Systems 41 , no. 5 ( 2024 ): e13081. https://doi.org/10.1111/exsy.13081 . The above article, published online on 18 June 2022, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors disagree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70135},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70135},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Dynamic relationship network and international management of enterprise supply chain by particle swarm optimization algorithm under deep learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Adaptive control and supply chain management of intelligent agricultural greenhouse by intelligent fuzzy auxiliary cognitive system. <em>EXSY</em>, <em>42</em>(11), e70134. (<a href='https://doi.org/10.1111/exsy.70134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: Y. Tian , “ Adaptive Control and Supply Chain Management of Intelligent Agricultural Greenhouse by Intelligent Fuzzy Auxiliary Cognitive System ”, Expert Systems 41 , no. 5 ( 2024 ): e13117. https://doi.org/10.1111/exsy.13117 . The above article, published online on 27 July 2022, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The author did not respond to our notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70134},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70134},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Adaptive control and supply chain management of intelligent agricultural greenhouse by intelligent fuzzy auxiliary cognitive system},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment analysis on memes: A review. <em>EXSY</em>, <em>42</em>(11), e70133. (<a href='https://doi.org/10.1111/exsy.70133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review explores the field of Sentiment Analysis on Memes, examining the methodologies employed to analyse the emotions expressed in widely shared online images. We discuss the various architectures used in sentiment analysis, review existing datasets and highlight shared tasks that facilitate model evaluation. The review also addresses the challenges specific to this domain, such as the interpretation of humour and sarcasm, which add complexity to sentiment analysis in the context of memes. A key focus of this review is the need for novel datasets that better capture the unique nature of memes, particularly those that blend text and images with cultural and emotional nuances. Existing benchmark datasets often fall short in representing the diversity of meme formats and regional variations, highlighting the necessity for more comprehensive datasets. Looking forward, we anticipate advancements in analytical methodologies and the development of such specialised datasets, which would significantly enhance the accuracy and depth of sentiment analysis models. This review serves as a comprehensive resource for researchers and practitioners interested in advancing the study of sentiment analysis in the evolving field of memes.},
  archive      = {J_EXSY},
  author       = {Ravi Kumar Routhu and Ujwala Baruah},
  doi          = {10.1111/exsy.70133},
  journal      = {Expert Systems},
  month        = {11},
  number       = {11},
  pages        = {e70133},
  shortjournal = {Expert Syst.},
  title        = {Sentiment analysis on memes: A review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dimensionality reduction strategies for classification: ML versus DL approaches and their combinations. <em>EXSY</em>, <em>42</em>(10), e70140. (<a href='https://doi.org/10.1111/exsy.70140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction plays a vital role in enhancing the performance of data classification tasks by reducing the complexity of the feature space. This study examines the effectiveness of integrating dimensionality reduction techniques with classification algorithms across four strategic configurations: (1) machine learning (ML)-based dimensionality reduction with ML classifiers, (2) deep learning (DL)-based dimensionality reduction with DL classifiers, and two heterogeneous combinations that mix ML and DL methods. Using 20 benchmark datasets from diverse domains, with feature dimensions ranging from 44 to 19,993, we systematically evaluate and compare these configurations. The dimensionality reduction methods include three ML-based feature selection techniques, Genetic Algorithm (GA), Information Gain (IG), and the C4.5 decision tree, and four DL-based feature extraction approaches, Autoencoder (AE), Sparse Autoencoder (SAE), Denoising Autoencoder (DAE), and Variational Autoencoder (VAE). For classification, Support Vector Machine (SVM) and k-Nearest Neighbours (KNN) are used as ML classifiers, while Multilayer Perceptron (MLP) and Deep Belief Network (DBN) serve as DL classifiers. Experimental results show that SAE consistently produces the most compact feature sets and improves classification performance, with the SAE + MLP combination achieving the best overall results. Furthermore, we explore ensemble dimensionality reduction strategies that integrate multiple algorithms. Although the best ensemble approach slightly outperforms the SAE + MLP model, the observed performance improvements are not statistically significant, that is, 0.839 versus 0.836 for AUC rates. In addition, SAE achieves a significantly higher dimensionality reduction rate compared to the best ensemble method (63% vs. 18%).},
  archive      = {J_EXSY},
  author       = {Chihli Hung and Chih-Fong Tsai and Ming-Hui Wu},
  doi          = {10.1111/exsy.70140},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70140},
  shortjournal = {Expert Syst.},
  title        = {Dimensionality reduction strategies for classification: ML versus DL approaches and their combinations},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the antecedents and consequences of privacy concerns: A comparison of humanoid robot to tablet. <em>EXSY</em>, <em>42</em>(10), e70132. (<a href='https://doi.org/10.1111/exsy.70132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of AI-driven technologies often necessitates the collection of private user information to deliver personalised services and enhance the overall user experience. Given the recurring incidents of data breaches, awareness of privacy risks and concerns about disclosing personal information to AI-driven applications has significantly increased. Privacy concerns have become a critical issue, heavily influencing users' intentions to interact with such systems. To appropriately investigate the antecedents and consequences of disclosing private information, this study examines the influence of social presence (humanlike vs. non-humanlike media) on privacy concerns and information disclosure across different types of data sensitivities (including retail, financial, and medical data). An online survey ( N = 282) and a lab experiment ( N = 70) were conducted, incorporating multiple experimental tasks under various conditions. The results reveal that both social presence and data sensitivity significantly impact privacy concerns and information disclosure. Additionally, a privacy paradox is observed: while participants express concern about privacy, their attitudinal and behavioural intentions shift, indicating a willingness to trade sensitive information for enhanced services. The findings also show that individual personality traits strongly influence one's intention to disclose personal information when interacting with humanlike media. Furthermore, when investigating privacy concerns, it is essential to move beyond task-driven assessments. Instead, identifying the specific types of private information involved and adopting a data-driven perspective provides a more accurate understanding of privacy-related behaviours.},
  archive      = {J_EXSY},
  author       = {Shih-Yi Chien and Yi-Ling Lin and Jing-Ting Luo and Yao-Cheng Chan},
  doi          = {10.1111/exsy.70132},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70132},
  shortjournal = {Expert Syst.},
  title        = {Exploring the antecedents and consequences of privacy concerns: A comparison of humanoid robot to tablet},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The quality of a scientific manuscript given by a peer-reviewed. report in three dimensions: Accessibility, contribution and experimentation (AccConExp). <em>EXSY</em>, <em>42</em>(10), e70131. (<a href='https://doi.org/10.1111/exsy.70131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new model (AccConExp) to help the editor evaluate the reviewer's report in the peer-review process. The model provides information about accessibility, contribution and experimentation and analyses the sentiment of these characteristics. Accessibility pertains to the clarity and coherence of the manuscript; Contribution assesses whether the work is original or well justified; and Experimentation reflects the presence of significant comparisons or substance within the paper. For example, with this information, a journal editor can establish whether the paper meets the journal's standards given the polarity of accessibility, contribution or experimentation. The AccConExp model provides a strong and flexible framework for the analysis of reports that emphasise accessibility, contribution and experimentation. Its computational efficiency and scalability with emerging categories render it an essential resource for journal editors and various stakeholders within the academic and research communities. Furthermore, the AccConExp model introduces a novel method for improving the peer-review process by offering a more organised and insightful analysis of reviewers' reports, ultimately resulting in more consistent and high-quality assessments of scientific research. For this, the AccConExp model integrates a theoretical model based on partial least squares-structural equation modelling (PLS-SEM) to acquire new knowledge and a multi-task deep machine learning to explore the knowledge learning with the model PLS-SEM. The PLS-SEM part of the AccConExp model obtains a causal prediction from a set of aspect categories assigned to the reviewer's report to build new knowledge of the report based on accessibility, contribution and experimentation. The causal-exploratory capabilities of the multi-task deep learning model allow the labelling of new report's sentences based on accessibility, contribution, experimentation constructs and sentiment. Once we discover a sentence's construct, a second deep learning machine allows us to obtain its aspect category (clarity, soundness, originality, motivation, substance and meaningful comparison). The AccConExp model has been tested using reviewer reports from ICLR and NeurIPS papers (conferences with high impact in machine learning). The AccConExp model is compared with a multi-task architecture that assigns aspect categories to the report's sentences. The results obtained with the AccConExp model are competitive and allow us to give new information to the reviewer's reports without the effort to generate a new dataset labelled with these new constructs. Also, the AccConExp model's computational efficiency and capacity to adapt to new categories render it an invaluable resource for journal editors and various stakeholders within the academic and research community. The methodology used in this paper can be extended to other research fields to define its constructs, even if the aspects considered in the review's reports area differ from those used in this proposal. We release our codes for more study.},
  archive      = {J_EXSY},
  author       = {J. J. Montero-Parodi and Rosa Rodriguez-Sánchez and J. A. García and J. Fdez-Valdivia},
  doi          = {10.1111/exsy.70131},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70131},
  shortjournal = {Expert Syst.},
  title        = {The quality of a scientific manuscript given by a peer-reviewed. report in three dimensions: Accessibility, contribution and experimentation (AccConExp)},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperbolic space feature enhancement and interaction mechanism for classification on small multimodal medical datasets. <em>EXSY</em>, <em>42</em>(10), e70130. (<a href='https://doi.org/10.1111/exsy.70130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is one of the leading causes of death worldwide, and early diagnosis of the disease is one of the most important factors in reducing mortality or increasing lifespan. Traditionally, healthcare experts use various sources of information to determine a diagnosis, often including some form of imaging along with clinical and demographic data. In this work, we propose a method to improve fusion of medical images and multi-field complementary data for classification in small datasets using deep learning models. To achieve that, we introduce a novel complementary data extraction block using hyperbolic space feature enhancement by Poincaré transformation and a mechanism for multi-field feature interactions. We evaluate it using datasets for the diagnosis of skin cancer (PAD-UFES-20) and oral cavity cancer (NDB-UFES). The experimental results show statistically significant improvement in performance on PAD-UFES-20 dataset of the proposed model over baseline. The SingleS Poincaré Conv–Concat with MetaBlock fusion model using PiT image backbone achieved performance of in balanced accuracy metric. For NBD-UFES dataset, the ViT architecture with SingleDS Poincaré Conv–Concat using MetaBlock fusion ( ) obtained the best mean among all the results evaluated. The results indicate the feasibility of the proposed approach. The source code is public available at GitHub at https://github.com/lmlima/Hyperbolic-space-feature-enhancement-and-interaction-mechanism-for-classification .},
  archive      = {J_EXSY},
  author       = {Leandro Muniz de Lima and Renato Antonio Krohling},
  doi          = {10.1111/exsy.70130},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70130},
  shortjournal = {Expert Syst.},
  title        = {Hyperbolic space feature enhancement and interaction mechanism for classification on small multimodal medical datasets},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demosaicking algorithm using swin transformer and long-range attention network. <em>EXSY</em>, <em>42</em>(10), e70129. (<a href='https://doi.org/10.1111/exsy.70129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many mobile devices—including digital cameras, smartphones, and personal digital assistants (PDAs)—rely on single image sensors to capture scenes for real-time processing. Convolutional neural networks (CNNs) have shown outstanding performance in various image processing tasks. In this paper, we propose a novel demosaicking method based on a transformer and long-range attention network (TLAN). The approach begins by initializing the mosaicked image using a bicubic interpolation algorithm, which provides a coarse reconstruction. TLAN is then applied to refine the output and accurately reconstruct the three colour channels. Our TLAN architecture combines the Swin Transformer (ST) with a dedicated long-range attention (LA) mechanism. The overall framework consists of both shallow and deep feature extraction modules. The deep extraction module is built from multiple residual swin transformer blocks (RSTBs), each composed of several Swin Transformer layers and augmented with a long-range attention block (LAB) to capture extended spatial dependencies. Experimental results demonstrate that the proposed method achieves superior performance in both PSNR and visual quality compared to existing demosaicking techniques.},
  archive      = {J_EXSY},
  author       = {Jin Wang and Ohjung Kwon and Gwanggil Jeon},
  doi          = {10.1111/exsy.70129},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70129},
  shortjournal = {Expert Syst.},
  title        = {Demosaicking algorithm using swin transformer and long-range attention network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning-based automatic sentiment annotation of a twitter-based arabic mental illness (AMI) dataset. <em>EXSY</em>, <em>42</em>(10), e70128. (<a href='https://doi.org/10.1111/exsy.70128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis, crucial for discerning emotional tones in text, relies on manual annotation to train machine learning models and is considered the gold standard for creating annotated corpora. However, this process is time-consuming, labour-intensive, and prone to biases. This paper proposes an automatic annotation approach for the Twitter-based Arabic Mental Illness (AMI) dataset, which encompasses both Modern Standard Arabic and Dialectal Arabic. The approach leverages transfer learning with existing manually annotated datasets and three advanced Arabic language models to automate annotation, thereby enriching Arabic as a low-resource language with labelled sentiment data. Validation was conducted by comparing the automatically generated annotations to manual annotation on the same dataset, achieving strong inter-annotator agreement with a Cohen's Kappa statistic of k = 0.8457. Additionally, various baseline models were evaluated on the AMI dataset, identifying AraBERT as the top performer with the highest F1 score and accuracy.},
  archive      = {J_EXSY},
  author       = {Arwa Diwali and Kawther Saeedi and Kia Dashtipour and Mandar Gogate and Zain Hussain and Adam Howard and Amir Hussain},
  doi          = {10.1111/exsy.70128},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70128},
  shortjournal = {Expert Syst.},
  title        = {Transfer learning-based automatic sentiment annotation of a twitter-based arabic mental illness (AMI) dataset},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards incremental learning in large language models: A critical review. <em>EXSY</em>, <em>42</em>(10), e70127. (<a href='https://doi.org/10.1111/exsy.70127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental learning is the ability of systems to acquire knowledge over time, enabling their adaptation and generalisation to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data changes frequently or is limited. This review provides a comprehensive analysis of incremental learning in large language models. It synthesises the state-of-the-art incremental learning paradigms, including continual learning, meta-learning, parameter-efficient learning and mixture-of-experts learning. We demonstrate their utility for incremental learning by describing specific achievements from these related topics and their critical factors. An important finding is that many of these approaches do not update the core model, and none of them update incrementally in real time. The paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of incremental learning and its implications for designing and developing LLM-based learning systems.},
  archive      = {J_EXSY},
  author       = {Mladjan Jovanovic and Peter Voss},
  doi          = {10.1111/exsy.70127},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70127},
  shortjournal = {Expert Syst.},
  title        = {Towards incremental learning in large language models: A critical review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GNN-EnKF fusion: A novel framework for cotton canopy nitrogen inversion using multi-source remote sensing fusion and crop growth model assimilation. <em>EXSY</em>, <em>42</em>(10), e70126. (<a href='https://doi.org/10.1111/exsy.70126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the dual pressures of rapid global population growth and escalating climate change, there is a growing demand for real-time monitoring of crop nitrogen levels to support precision agriculture. This necessity has catalysed the integration of crop modelling techniques with remote sensing technologies. Addressing challenges such as multi-source remote sensing data heterogeneity and limited generalisation in nitrogen inversion models for cotton canopies, this paper designs a novel inversion framework based on the assimilation of diverse remote sensing sources and mechanistic crop models. Firstly, this paper employed spectral resampling techniques, fuzzy logic for uncertainty quantification, and Pearson correlation analysis to harmonise differences in spectral characteristics and spatial resolution between Sentinel-2A and Landsat 8 imagery, ultimately identifying eight nitrogen-sensitive features. Subsequently, a multi-scale feature enhancement module was developed to improve representational richness. Additionally, the paper employed a satellite image fusion module, which effectively reduced data heterogeneity errors by 12.7% across sources. Building on this, a hybrid GNN-EnKF model was proposed. GNN was used to establish spatial neighbourhood dependencies, while EnKF dynamically adjusted the parameters within the WOFOST crop model. This approach successfully fuses data-driven learning with physically based modelling. Experimental evaluations revealed that the proposed architecture attained a mAP of 95.83%, outperforming baseline models such as ResNet18 (83.92%) and Transformer (92.84%), demonstrating robust adaptability in complex agricultural settings. In conclusion, the framework presented in this paper offers a high-accuracy nitrogen monitoring solution tailored for precision farming, and provides strong data support for cotton nitrogen deficiency and additional fertilisation.},
  archive      = {J_EXSY},
  author       = {Ke Wu and Yang Li and Jing Nie and Jingbin Li and Sezai Ercisli},
  doi          = {10.1111/exsy.70126},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70126},
  shortjournal = {Expert Syst.},
  title        = {GNN-EnKF fusion: A novel framework for cotton canopy nitrogen inversion using multi-source remote sensing fusion and crop growth model assimilation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-positive-neighbours guide contrastive graph clustering network. <em>EXSY</em>, <em>42</em>(10), e70125. (<a href='https://doi.org/10.1111/exsy.70125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of deep learning has introduced promising techniques for attribute graph clustering. However, existing deep attributed graph clustering methods face two key limitations: (1) insufficient exploration of multi-scale neighbourhood structural information during training, and (2) inappropriate graph data augmentation strategies, which often lead to semantic drift and indistinguishable positive samples. To address these issues, this paper proposes a novel Re al-positive-neighbours Guided Co ntrastive G raph Clustering Network (ReCogNet) for attribute graph clustering. ReCogNet employs a dynamic attention-weighted fusion mechanism to refine shallow semantic information derived from the multi-scale GCN network, enabling the model to capture subtle yet critical node relationships. Additionally, it dynamically identifies real-positive-neighbour nodes and adopts a negative-free contrastive learning objective. This objective maximises the similarity between a query node and its real-positive-neighbours in the latent embedding space, thereby improving clustering performance by leveraging meaningful local relationships. Extensive experiments on six benchmark datasets demonstrate that the proposed ReCogNet method consistently outperforms state-of-the-art approaches.},
  archive      = {J_EXSY},
  author       = {Jing Yang and Chulei Xiang and Wenjun Xu and Zihao Zhao and Jinrui Zhang and Huaming Wu},
  doi          = {10.1111/exsy.70125},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70125},
  shortjournal = {Expert Syst.},
  title        = {Real-positive-neighbours guide contrastive graph clustering network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on model-driven engineering and domain-specific languages for chatbot development: Requirements, challenges and solutions. <em>EXSY</em>, <em>42</em>(10), e70124. (<a href='https://doi.org/10.1111/exsy.70124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chatbots have become widely adopted tools for improving user interactions across multiple platforms. They are advanced software applications designed to emulate human conversation across various platforms. Moreover, developing chatbots using existing platforms and frameworks presents challenges, such as the lock-in of NLP services, and incurs substantial costs. Recently, research has introduced solutions to ease chatbot development. Many of these approaches utilise Model-Driven Engineering (MDE) and Domain-Specific Languages (DSLs) to automate processes and simplify implementation. Through the use of MDE and DSLs, these solutions enhance efficiency and make chatbot creation more accessible. This study aims to provide a comprehensive survey on MDE and DSLs in chatbot development, highlighting key research topics, opportunities, and challenges. The first contribution explores the primary application domains of DSLs in chatbot development and the associated challenges in their adoption. Second, this work examines the various ways in which DSLs are employed to model and develop chatbots, assessing their impact on automation and efficiency. Additionally, this study identifies the challenges and limitations of using DSLs in chatbot development. Atlast, it investigates the influence of DSL utilisation on user experience, both from the perspective of chatbot developers and end-users, to determine how DSLs enhance the chatbot development process and interaction quality. To achieve this, a comprehensive search will be conducted across Scopus, Web of Science, and ScienceDirect for studies published between 2014 and 2024. A total of 306 publications were reviewed, of which 15 were identified as primary studies.},
  archive      = {J_EXSY},
  author       = {Lamya Benaddi and Charaf Ouaddi and Abdeslam Jakimi and Hasna Chaibi and Abdellah Chehri and Gwanggil Jeon and Brahim Ouchao},
  doi          = {10.1111/exsy.70124},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70124},
  shortjournal = {Expert Syst.},
  title        = {A survey on model-driven engineering and domain-specific languages for chatbot development: Requirements, challenges and solutions},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). State-of-the-art on ensemble of real coded genetic algorithm operators. <em>EXSY</em>, <em>42</em>(10), e70123. (<a href='https://doi.org/10.1111/exsy.70123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real coded genetic algorithms (RCGAs) are among the most versatile metaheuristic algorithms used to determine the global optimal solutions for nonlinear optimization problems, aimed at solving real-life complex optimization problems. After the selection operator, the crossover and mutation operators are two crucial strategies upon which the performance of a genetic algorithm (GA) depends. At present, various types of crossover and mutation operators are available in the literature. Therefore, this study attempts to present a state-of-the-art review on real coded crossover operators and real coded mutation operators. Each operator is explained with the help of examples. The objective of this state-of-the-art paper is to serve as a foundation for researchers who wish to design new real coded crossover operators or mutation operators. Moreover, to evaluate the effectiveness of various variants of RCGA, 23 classical benchmark problems are utilised, offering insights into their performance across different optimization problems.},
  archive      = {J_EXSY},
  author       = {Yogesh Kumar and Kusum Deep},
  doi          = {10.1111/exsy.70123},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70123},
  shortjournal = {Expert Syst.},
  title        = {State-of-the-art on ensemble of real coded genetic algorithm operators},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Deep reinforcement learning-based precise prediction model for smart M-health system. <em>EXSY</em>, <em>42</em>(10), e70122. (<a href='https://doi.org/10.1111/exsy.70122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : D. J. Jagannath , R. J. Dolly , J. D. Peter , “ Deep reinforcement learning-based precise prediction model for Smart M-Health system ”, Expert Systems (Early View) : https://doi.org/10.1111/exsy.13450 . The above article, published online on 8 September 2023 on Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho and John Wiley & Sons Ltd. The retraction has been agreed as the research is not reproducible based on the information provided. The origin of the patients’ data investigated in the study is also not explained. The authors have been informed of the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70122},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70122},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Deep reinforcement learning-based precise prediction model for smart M-health system},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: 6G-enabled internet of medical things. <em>EXSY</em>, <em>42</em>(10), e70121. (<a href='https://doi.org/10.1111/exsy.70121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : S.S. Dhanda , B. Singh , P. Jindal , T. K. Sharma , D. Panwar , “ 6G-enabled internet of medical things ”, Expert Systems no. 41, Issue 1 ( 2024 ): e13472. https://doi.org/10.1111/exsy.13472 . The above article, published online on 18 October 2023 on Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho and John Wiley & Sons Ltd. The retraction has been agreed as the conclusions do not appear to be supported by the research data. Further investigation raised concerns regarding the manipulation of the peer review process. The authors have been informed of the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70121},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70121},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: 6G-enabled internet of medical things},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Sensitivity analysis of physical and mental health factors affecting polycystic ovary syndrome in women. <em>EXSY</em>, <em>42</em>(10), e70120. (<a href='https://doi.org/10.1111/exsy.70120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : S. Guha , A. Kodipalli , “ Sensitivity analysis of physical and mental health factors affecting Polycystic ovary syndrome in women ”, Expert Systems (Early View): https://doi.org/10.1111/exsy.13413 . The above article, published online on 26 July 2023 on Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho and John Wiley & Sons Ltd. The retraction has been agreed after several flaws and inconsistencies between results presented and experimental methods described were found. The study design also appears to be flawed. Accordingly, the editors consider the conclusions unreliable. The authors do not agree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70120},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70120},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Sensitivity analysis of physical and mental health factors affecting polycystic ovary syndrome in women},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Frequency aware task scheduling using DVFS for energy efficiency in cloud data centre. <em>EXSY</em>, <em>42</em>(10), e70119. (<a href='https://doi.org/10.1111/exsy.70119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION: Samual, J. , M. Hussin , N. A. W. A. Hamid , and A. Abdullah . 2025 . “ Frequency aware task scheduling using DVFS for energy efficiency in Cloud data centre ”, Expert Systems 41 , no. 1 : e13276 . https://doi.org/10.1111/exsy.13276 . The above article, published online on 26 April 2023 on Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho and John Wiley & Sons Ltd. The retraction has been agreed after several flaws and inconsistencies between results presented and experimental methods described were found. Concerns about conceptual flaws in the study design and reproducibility of the research were also identified. Further investigation raised concerns regarding the manipulation of the peer review process. The conclusions of this article are considered invalid. The authors have been informed of the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70119},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70119},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Frequency aware task scheduling using DVFS for energy efficiency in cloud data centre},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Optimizing time delay in cloud using enhanced multi-hold inherited maximization algorithm to reduce cost and improve bandwidth. <em>EXSY</em>, <em>42</em>(10), e70118. (<a href='https://doi.org/10.1111/exsy.70118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : D. S. M. Kumar , P. Sriramya , “ Optimizing time delay in cloud using enhanced multi-hold inherited maximization algorithm to reduce cost and improve bandwidth ”, Expert Systems no. 40, Issue 4 ( 2022 ): e13073. https://doi.org/10.1111/exsy.13073 . The above article, published online on 23rd June 2022 on Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the Editor in Chief David Camacho and John Wiley and Sons Ltd. The article was published as part of a guest-edited issue. Following an investigation by the publisher, all parties have concluded that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors do not agree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70118},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70118},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Optimizing time delay in cloud using enhanced multi-hold inherited maximization algorithm to reduce cost and improve bandwidth},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Hindi podcast genre prediction using support vector classifier. <em>EXSY</em>, <em>42</em>(10), e70117. (<a href='https://doi.org/10.1111/exsy.70117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : M. Jain , M. Mahrishi , G. Sharma , S. Hosseini , “ Hindi podcast genre prediction using support vector classifier ”, Expert Systems (Early View): https://doi.org/10.1111/exsy.13391 . The above article, published online on 27 June 2023 on Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho and John Wiley & Sons Ltd. The retraction has been agreed after several flaws and inconsistencies between results presented and experimental methods described were found. The study design also appears to be flawed. The conclusions of this article are considered invalid. The authors do not agree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70117},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70117},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Hindi podcast genre prediction using support vector classifier},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Node localization and data aggregation scheme using cuckoo search and neural network. <em>EXSY</em>, <em>42</em>(10), e70116. (<a href='https://doi.org/10.1111/exsy.70116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : S. Kaur , N. Kaur , K. S. Bhatia , M. A. R. Khan , M. Gupta , N. K. Sharma , S. K. Sharma , “ Node localization and data aggregation scheme using cuckoo search and neural network ”, Expert Systems no. 40, Issue 4 ( 2022 ): e13033. https://doi.org/10.1111/exsy.13033 . The above article, published online on 22nd July 2022 on Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the Editor in Chief David Camacho and John Wiley and Sons Ltd. The article was published as part of a guest-edited issue. Following an investigation by the publisher, all parties have concluded that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors have been informed of the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70116},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70116},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Node localization and data aggregation scheme using cuckoo search and neural network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Trajectory clustering and query processing analysis framework for knowledge discovery in cloud environment. <em>EXSY</em>, <em>42</em>(10), e70115. (<a href='https://doi.org/10.1111/exsy.70115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : S.S.L. P. J. Shabu , K. Yadav , E. Kariri , K. K. Gola , M. AnulHaq , A. Kumar , “ Trajectory clustering and query processing analysis framework for knowledge discovery in cloud environment ”, Expert Systems no. 40 , Issue 4 ( 2023 ): e12968. https://doi.org/10.1111/exsy.12968 . The above article, published online on 1 March 2022 on Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho and John Wiley & Sons Ltd. The retraction has been agreed after several flaws and inconsistencies between results presented and experimental methods described were found. The conclusions of this article are considered invalid. Further investigation raised concerns regarding the manipulation of the peer review process. The authors have been informed of the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70115},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70115},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Trajectory clustering and query processing analysis framework for knowledge discovery in cloud environment},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing link prediction through graph attention network and linear discriminant analysis. <em>EXSY</em>, <em>42</em>(10), e70114. (<a href='https://doi.org/10.1111/exsy.70114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction (LP) is a prominent research topic in network science and complex network analysis, focused on predicting future connections between unconnected node pairs by examining network topology and related characteristics. This paper introduces an advanced LP model combining the graph attention network (GAT) with Jaccard similarity and node centrality measures, such as local interaction density (LID) and hubs and authority centrality (HAC). Initially, a weight matrix incorporating structural and topological information is created using a feature matrix derived from similarity measures and node centrality. Then, node embeddings are generated using GAT, allowing the model to learn detailed representations that consider local and global contexts. GAT's multi-head attention mechanism enables the model to focus on various aspects of the node neighbourhood, capturing diverse structural information. A well-defined dataset is created from these embeddings, representing nodes at the endpoints of edges labelled as positive or negative. Linear discriminant analysis (LDA) is applied to this well-balanced and labelled dataset to perform classification tasks, leading to accurate link prediction. The model's performance is evaluated across eight different datasets, and the obtained results reveal the proposed model's superiority over several baseline and recently proposed LP models.},
  archive      = {J_EXSY},
  author       = {Fatima Ziya and Sanjay Kumar},
  doi          = {10.1111/exsy.70114},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70114},
  shortjournal = {Expert Syst.},
  title        = {Enhancing link prediction through graph attention network and linear discriminant analysis},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of training healthcare robots with extended reality and digital twins. <em>EXSY</em>, <em>42</em>(10), e70113. (<a href='https://doi.org/10.1111/exsy.70113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare robots are cyberphysical systems designed to assist older adults and reduce the burden on healthcare professionals and family caregivers. These robots can perform various tasks, including delivering medications on time, promoting physical activity, and cultivating social connections by contacting family and friends. As the global population ages, healthcare robots are emerging as critical technology to support older adults and alleviate the burden on healthcare systems. However, their widespread adoption is hindered by significant challenges, including inflexible hard-coded functionalities, high development and training costs, and a common lack of cultural adaptability. Extended reality (XR) and digital twin (DT) technologies offer a transformative approach to overcome these hurdles by enabling safe, scalable, and cost-effective virtual training environments. This paper presents a systematic review of the literature at the intersection of XR, DT, and healthcare robotics, adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We not only present a thematic analysis of current research but also identify key fundamental challenges in existing robot training methodologies and analyse the field's evolutionary trends through visualisations. Based on our synthesis, we propose a conceptual framework and guidelines for developing future healthcare robots that are not only technologically advanced but also personalised, empathetic, and culturally sensitive. The purpose of this survey is to provide a roadmap for researchers and practitioners to take advantage of these convergent technologies, with the intention of creating a more effective and humane healthcare ecosystem for ageing populations around the world.},
  archive      = {J_EXSY},
  author       = {Khusrav Badalov and Hao-An Tseng and Young Yoon},
  doi          = {10.1111/exsy.70113},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70113},
  shortjournal = {Expert Syst.},
  title        = {A survey of training healthcare robots with extended reality and digital twins},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimised multilevel image thresholding leveraging enhanced elephant herding and symbiotic organisms search. <em>EXSY</em>, <em>42</em>(10), e70110. (<a href='https://doi.org/10.1111/exsy.70110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study sheds light on a fundamental problem in image segmentation known as multilevel image thresholding. With the rapid growth of artificial intelligence applications that rely on image processing such as medical imaging, remote sensing, and pattern recognition the demand for more effective techniques has become increasingly urgent. Traditional methods suffer from significant limitations, including slow convergence and premature convergence to local optima, particularly when applied to complex or high-dimensional images. To address these challenges, this study proposes a novel approach based on metaheuristic algorithms, specifically elephant herding optimization (EHO) and symbiotic organism search (SOS). Although these algorithms have shown promising results due to their adaptability and exploratory capabilities, they still face performance bottlenecks resulting from insufficient diversity in the search process. To overcome these limitations, enhanced variants of EHO and SOS are introduced by integrating opposition-based learning (OBL) and chaos theory to achieve a better balance between exploration and exploitation. These improved algorithms, OCEHO and OCSOS, are applied to the multilevel thresholding problem using Otsu's variance, Kapur's entropy and Masi's entropy as objective functions. The proposed methods are evaluated on 75 standard benchmark images, with segmentation quality evaluated using PSNR, SSIM, and FSIM metrics. Experimental results on 75 standard benchmark images show that the proposed OCEHO algorithm achieves PSNR values up to 37.51 dB, SSIM scores of 0.972, and FSIM values of 0.986, significantly outperforming baseline and hybrid variants. Furthermore, statistical analyzes, including the Wilcoxon rank sum test, confirm the superior stability and convergence speed of OCEHO over its counterparts. These results validate the effectiveness and robustness of the proposed approach for high-quality image segmentation.},
  archive      = {J_EXSY},
  author       = {Falguni Chakraborty and Ruba Abu Khurma and David Camacho and Miguel Angel Diaz},
  doi          = {10.1111/exsy.70110},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e70110},
  shortjournal = {Expert Syst.},
  title        = {Optimised multilevel image thresholding leveraging enhanced elephant herding and symbiotic organisms search},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED: Deep reinforcement learning-based precise prediction model for smart M-health system. <em>EXSY</em>, <em>42</em>(10), e13450. (<a href='https://doi.org/10.1111/exsy.13450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of Android mobile phones and other wireless technology in the field of healthcare infers to mHealth (mobile health). Modern Science and technological developments have paved way for better and more sophisticated solutions towards Smart mHealth systems and preventive Smart Healthcare services for disease—treatment, surveillance, management of chronic disease and tracking epidemic outbreaks. Thus, the mHealth data can be collected from various end users, deposited in repository, perform analysis and suitable Smart Healthcare services can be made accessible to end users, anytime, anywhere, over the internet. This can be achieved by incorporating the methodologies of advanced Soft Computing methodologies, data communication, cloud storage and cloud computing, big data analysis, artificial intelligence, computer communication/networking and other engineering techniques. However, the analysis of such huge volumes of data and to provide precise Smart Healthcare services is a million-dollar question. This research article exposes a Deep Reinforcement Learning model for precisely predicting the disease and offer precise Smart m-Healthcare services to the end users. This research provides an intensive experimental analysis and investigation using synthetic health parameters that were simulated using various mHealth sensors. The dataset includes 15 varieties of mHealth metrics with a total dataset size of 285.},
  archive      = {J_EXSY},
  author       = {Duraiswamy Jothinath Jagannath and Raveena Judie Dolly and James Dinesh Peter},
  doi          = {10.1111/exsy.13450},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13450},
  shortjournal = {Expert Syst.},
  title        = {RETRACTED: Deep reinforcement learning-based precise prediction model for smart M-health system},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED: Sensitivity analysis of physical and mental health factors affecting polycystic ovary syndrome in women. <em>EXSY</em>, <em>42</em>(10), e13413. (<a href='https://doi.org/10.1111/exsy.13413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polycystic ovary syndrome (PCOS) is a common condition affecting women worldwide. In this paper, we analyse physical and mental health factors affecting PCOS in women. We collected data in the form of questionnaires from women belonging to diverse backgrounds and performed a sensitivity analysis to find which factors are significant in determining the presence or absence of PCOS in women and further predict the severity of the symptoms. We implemented four types of methods for determining significant physical and mental health factors affecting the presence and degree of PCOS in women. Our analysis showed that menstrual period cycle length, menstrual period duration, regularity of menstrual periods, menstrual period flow, hair growth, eating pattern, and sleep pattern are the most significant physical health factors and feeling of worthlessness, depression, feeling of hopelessness, not able to be calm, sad and can't cheer up, can't sit still and restlessness are the most significant mental health factors in determining the presence or absence of PCOS in an individual, as well as the degree of PCOS. Furthermore, physical factors are more significant than mental health factors in determining PCOS presence and degree in the cohort of female individuals considered for the analysis.},
  archive      = {J_EXSY},
  author       = {Srirupa Guha and Ashwini Kodipalli},
  doi          = {10.1111/exsy.13413},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13413},
  shortjournal = {Expert Syst.},
  title        = {RETRACTED: Sensitivity analysis of physical and mental health factors affecting polycystic ovary syndrome in women},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED: Hindi podcast genre prediction using support vector classifier. <em>EXSY</em>, <em>42</em>(10), e13391. (<a href='https://doi.org/10.1111/exsy.13391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {India experienced a 23% rise in podcast listening after the Covid-19 pandemic. The pandemic and screen fatigue led people to seek their favourite old audio podcasts. Podcast genre classification allows listeners to compile a playlist of their favourite tracks; it also helps podcast streaming services provide recommendations to users based on the genre of the podcasts they enjoy. Since the COVID-19 pandemic, the need for educational content in all forms, including podcasts, has skyrocketed, making it even more crucial to anticipate the genre of educational podcasts. Educational podcasts are a sub-genre of the broader education genre and typically involve audio recordings of discussions, lectures, or interviews on educational topics. Education podcast genre prediction is required to efficiently classify and arrange educational content and make it simpler for listeners to access and absorb pertinent information. This study focuses on Podcast Genre Prediction, specifically for the Hindi language. In this study, our developed PodGen dataset was used, which consists of 550 five-minute podcasts with 26,867 sentences, where every podcast was manually annotated into one of the four genre categories (Horror, Motivational, Crime, and Romance). The performance comparison of state-of-the-art machine learning techniques on the PodGen dataset was used to demonstrate accuracy. The best performance on testing data was observed in the Support Vector Classifier model with balanced accuracy: 82.42%, precision (weighted): 83.09%, recall (weighted): 82.42%, and F1 score (weighted): 82.39%.},
  archive      = {J_EXSY},
  author       = {Mudeet Jain and Mehul Mahrishi and Girish Sharma and Samira Hosseini},
  doi          = {10.1111/exsy.13391},
  journal      = {Expert Systems},
  month        = {10},
  number       = {10},
  pages        = {e13391},
  shortjournal = {Expert Syst.},
  title        = {RETRACTED: Hindi podcast genre prediction using support vector classifier},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated social robot and virtual assistant solution to support medical management for older adults. <em>EXSY</em>, <em>42</em>(9), e70112. (<a href='https://doi.org/10.1111/exsy.70112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Matheus Ancelmo Bonfim Pita and Marcelo Fantinato and Patrick C. K. Hung},
  doi          = {10.1111/exsy.70112},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70112},
  shortjournal = {Expert Syst.},
  title        = {An integrated social robot and virtual assistant solution to support medical management for older adults},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method-oriented review of explainable artificial intelligence for neurological medical imaging. <em>EXSY</em>, <em>42</em>(9), e70111. (<a href='https://doi.org/10.1111/exsy.70111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of artificial intelligence (AI) techniques in medical imaging has led to significant improvements in diagnostic performance, particularly in neurological disorders. However, the limited interpretability of deep learning models, often referred to as the “black box” issue, poses substantial challenges in clinical trust, transparency, and regulatory acceptance. Explainable artificial intelligence (XAI) aims to address these limitations by enhancing model transparency and interpretability. This review systematically analysed 77 eligible studies, selected from an initial pool of 108 publications, focusing on XAI applications in neurological medical imaging. The included approaches were categorised into four primary groups: (1) feature visualisation techniques, (2) hierarchical and causal interpretability methods, (3) self-supervised and federated learning strategies, and (4) dynamic and multimodal interpretability frameworks. Each category was evaluated in terms of technical methodology, clinical applicability, and associated limitations. Feature visualisation methods such as Grad-CAM offer intuitive visual outputs for imaging data but often lack robustness and reproducibility, while attribution methods such as SHAP provide global or local feature importance—mainly for tabular or structured data—and are less frequently applied to medical images. Hierarchical models, including Layer-wise Relevance Propagation, provide more detailed insights but face barriers to clinical integration. Federated and self-supervised learning approaches are increasingly explored for privacy preservation and model generalisation in medical imaging; however, the integration of explainability mechanisms into these frameworks is still at an early stage, and standardised methods for interpretable federated/self-supervised models remain underdeveloped. Dynamic and multimodal frameworks represent a promising direction for comprehensive model explanation but are still in the early stages of exploration. Despite progress, key challenges persist, including the lack of standardised evaluation metrics, limited clinical validation, and unresolved ethical concerns. Future research should focus on integrating interpretability into model development, establishing benchmark evaluation protocols, and promoting effective human–AI collaboration in clinical workflows.},
  archive      = {J_EXSY},
  author       = {Changyu Peng and Lifeng Li and Dechang Peng},
  doi          = {10.1111/exsy.70111},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70111},
  shortjournal = {Expert Syst.},
  title        = {A method-oriented review of explainable artificial intelligence for neurological medical imaging},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: An improved model for sentiment analysis on luxury hotel review. <em>EXSY</em>, <em>42</em>(9), e70109. (<a href='https://doi.org/10.1111/exsy.70109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION: V. Chang , L. Liu , Q. Xu , T. Li , and C-H Hsu , “ An Improved Model for Sentiment Analysis on Luxury Hotel Review ,” Expert Systems 40 no. 2 ( 2023 ): e12580, https://doi.org/10.1111/exsy.12580 . The above article, published online on 14 June 2020 in Wiley Online Library ( wileyonlinelibrary.com ) has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was published as part of a guest-edited issue. Following an investigation by the publisher, all parties have concluded that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors have been informed of the retraction but did not reply.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70109},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70109},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: An improved model for sentiment analysis on luxury hotel review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skeleton-based posture recognition for home care from virtual unmanned aerial vehicle. <em>EXSY</em>, <em>42</em>(9), e70108. (<a href='https://doi.org/10.1111/exsy.70108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel approach for real-time posture recognition in monitoring scenarios, utilising a virtual camera simulated on a UAV within virtual environments. Leveraging the MediaPipe Pose library, key points of the body skeleton are extracted, focusing on a subset of 8 key points for computational efficiency. Through the integration of heuristic algorithms based on physical proportions of the human body, the proposed methodology provides accurate estimations of three distinct postures: lying, standing, and sitting. This heuristic-based approach offers a computationally efficient alternative to traditional machine learning and deep learning methods, ensuring real-time performance and scalability. The efficiency of the framework is demonstrated through experiments that show its potential applications in various fields, including healthcare, virtual reality, and human-computer interaction. This approach achieved an average precision of 98.08% for virtual images. Success rates were 100%, 95.8%, and 98.9% for standing, sitting, and lying postures, respectively. Furthermore, the original classification model, which was tuned for virtual images, was tested on real images without any alteration to the parameter values. Its good performance demonstrates its potential for generalisation and application in diverse environments. Overall, this work contributes to the advancement of posture recognition technology, offering a versatile and accessible solution for posture analysis in dynamic monitoring environments.},
  archive      = {J_EXSY},
  author       = {Andrés Bustamante and Lidia M. Belmonte and António Pereira and Rafael Morales and Antonio Fernández-Caballero},
  doi          = {10.1111/exsy.70108},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70108},
  shortjournal = {Expert Syst.},
  title        = {Skeleton-based posture recognition for home care from virtual unmanned aerial vehicle},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to “RETRACTION: Using electroencephalogram classification in a convolutional neural network, infer privacy on healthcare internet of things 5.0”. <em>EXSY</em>, <em>42</em>(9), e70107. (<a href='https://doi.org/10.1111/exsy.70107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70107},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70107},
  shortjournal = {Expert Syst.},
  title        = {Correction to “RETRACTION: Using electroencephalogram classification in a convolutional neural network, infer privacy on healthcare internet of things 5.0”},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive based detection of anomalous sequences using bayesian surprise. <em>EXSY</em>, <em>42</em>(9), e70106. (<a href='https://doi.org/10.1111/exsy.70106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we implement Bayesian surprise as a method to sift through sequences of discrete patterns and identify any unusual or interesting patterns that deviate from known sequences. Surprise is a biological trait inherent in humans and animals and is essential for many creative acts and efforts of discovery. Numerous technical domains are comprised of discrete elements in sequences such as e-commerce transactions, genome data searching, online financial transactions of many types, criminal cyber-attacks and life-course data from sociology. In addition to the complexity and computational burden of this type of problem is the issue of their rarity. Many anomalies are infrequent and may defy categorisation; therefore, they are not suited to classification solutions. We test our methods on four discrete datasets (Hospital Sepsis patients, Chess Moves, the Wisconsin Card Sorting Task and BioFamilies) consisting of discrete sequences. Probabilistic Suffix Trees are trained on this data which maintain each discrete symbol's location and position in a given sequence. The trained models are exposed to “new” data where any deviations from learned patterns either in location on the sequence or frequency of occurrence will denote patterns that are unusual compared with the original training data. To assist in the identification of new patterns and to avoid confusing old patterns as new or novel we use Bayesian surprise to detect the discrepancies between what we are expecting and actual results. We can assign the degree of surprise or unexpectedness to any new pattern and provide an indication of why certain patterns are deemed novel or surprising and why others are not.},
  archive      = {J_EXSY},
  author       = {Ken McGarry and David Nelson},
  doi          = {10.1111/exsy.70106},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70106},
  shortjournal = {Expert Syst.},
  title        = {Cognitive based detection of anomalous sequences using bayesian surprise},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligent application in project management: An algorithm comparison for solar plants planning construction. <em>EXSY</em>, <em>42</em>(9), e70105. (<a href='https://doi.org/10.1111/exsy.70105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction planning is a critical and complex phase in the deployment of large-scale renewable energy infrastructure. This study applies artificial intelligence techniques to a domain-specific problem that has traditionally relied on expert judgement: the generation of detailed construction schedules for photovoltaic power plants. As renewable generation is a key part to meet the challenges of energy transition, the implementation of large projects has increased in recent years and this trend is expected to continue in the future. The main difficulty in meeting construction deadlines is the elaboration of an adequate planning. A tool that automatically generates schedules can be of great help to set up an initial baseline planning. To this end, this work compares five artificial intelligence techniques, on a data set consisting of real examples of successfully completed projects. The evaluation of the results obtained on test data shows that Adaptive Neuro-Fuzzy Inference System (ANFIS) is the technique that obtains the best performance in all error metrics, although it entails a high computational cost. The model thus obtained manages to generate a complete construction schedule with an error of 8% of the total duration. The use of metrics as MAE, MSE and provides a robust understanding of prediction accuracy, variability, and fit. These metrics are commonly used in project planning evaluations and help interpret model behaviour under different error profiles. Additionally, the resulting 8% total duration error implies a deviation of around 24 days in a 300-day project, which is highly actionable in real-world solar project management. The findings not only demonstrate the feasibility of using AI for solar construction planning, but also lay the groundwork for the development of intelligent software tools or platforms that could support planners in the renewable energy sector. While this study focuses on photovoltaic plants, the approach is extendable to other power plants as wind farms, combined-cycle or nuclear plants, or even to other construction projects.},
  archive      = {J_EXSY},
  author       = {Manuel Ángel López Ferreiro and Jesús Gil Ruiz and Óscar García and Luis De La Fuente Valentín},
  doi          = {10.1111/exsy.70105},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70105},
  shortjournal = {Expert Syst.},
  title        = {Artificial intelligent application in project management: An algorithm comparison for solar plants planning construction},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy decision-making support model for traffic safety analysis. <em>EXSY</em>, <em>42</em>(9), e70104. (<a href='https://doi.org/10.1111/exsy.70104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our study delves into the crucial issue of road safety by examining the intricate dynamics of driver behaviour, often resulting in tragic accidents. The importance of comprehending these behaviours is acknowledged, leading us to propose an innovative decision-making support model that integrates the analytic hierarchy process (AHP) with the best worst method (BWM) in a fuzzy context. Our objective is to effectively evaluate the overall influence of driver behaviour on road safety while reducing ambiguity in assessments. In a practical case study involving skilled drivers in Budapest, Hungary, a thorough survey was conducted to prioritise key driving behaviour factors that impact road safety. Our findings reveal ‘errors’ as the most vital aspect, followed by specific behaviours like ‘colliding when reversing without observation’ and ‘driving under the influence of alcohol’. By simplifying the survey procedure and offering practical insights, our unified model improves decision-making for policymakers striving to tackle road safety issues efficiently. To conclude, our research showcases the effectiveness of merging AHP and BWM methodologies in a fuzzy setting to obtain valuable perspectives on road safety concerns, ultimately aiding in the advancement of sustainable transportation systems.},
  archive      = {J_EXSY},
  author       = {Sarbast Moslem and Danish Farooq and Gülay Demir and Rana Faisal Tufail and Páraic Carroll and Domokos Esztergár-Kiss and Francesco Pilla},
  doi          = {10.1111/exsy.70104},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70104},
  shortjournal = {Expert Syst.},
  title        = {A fuzzy decision-making support model for traffic safety analysis},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of unimodal and multimodal emotion detection: Datasets, approaches, and limitations. <em>EXSY</em>, <em>42</em>(9), e70103. (<a href='https://doi.org/10.1111/exsy.70103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion detection from face and speech is inherent for human–computer interaction, mental health assessment, social robotics, and emotional intelligence. Traditional machine learning methods typically depend on handcrafted features and are primarily centred on unimodal systems. However, the unique characteristics of facial expressions and the variability in speech features present challenges in capturing complex emotional states. Accordingly, deep learning models have been substantial in automatically extracting intrinsic emotional features with greater accuracy across multiple modalities. The proposed article presents a comprehensive review of recent progress in emotion detection, spanning from unimodal to multimodal systems, with a focus on facial and speech modalities. It examines state-of-the-art machine learning, deep learning, and the latest transformer-based approaches for emotion detection. The review aims to provide an in-depth analysis of both unimodal and multimodal emotion detection techniques, highlighting their limitations, popular datasets, challenges, and the best-performing models. Such analysis aids researchers in judicious selection of the most appropriate dataset and audio-visual emotion detection models. Key findings suggest that integrating multimodal data significantly improves emotion recognition, particularly when utilising deep learning methods trained on synchronised audio and video datasets. By assessing recent advancements and current challenges, this article serves as a fundamental resource for researchers and practitioners in the field of emotional AI, thereby aiding in the creation of more intuitive and empathetic technologies.},
  archive      = {J_EXSY},
  author       = {Priyanka Thakur and Nirmal Kaur and Naveen Aggarwal and Sarbjeet Singh},
  doi          = {10.1111/exsy.70103},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70103},
  shortjournal = {Expert Syst.},
  title        = {A comprehensive review of unimodal and multimodal emotion detection: Datasets, approaches, and limitations},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving crowd counting via quantum-enhanced federated learning. <em>EXSY</em>, <em>42</em>(9), e70098. (<a href='https://doi.org/10.1111/exsy.70098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting plays a crucial role in analyzing group behavior in smart cities. Traditional crowd-counting models rely on large datasets gathered from diverse individuals for training while ignoring the privacy protection for each training client. Meanwhile, the scale variation has long been a difficult problem in crowd counting and has greatly reduced model accuracy. Therefore, it is essential to achieve privacy-aware crowd counting and to solve the problem of scale variation in dense scenes. To this end, we propose a Privacy-preserving Quantum-enhanced Network (PQNet). The PQNet uses federated learning to share parameters rather than data, which ensures the privacy of each client. Subsequently, a multi-scale quantum-driven calibration module is designed to capture multi-scale information via quantum states. It enhances counting accuracy in dense crowd environments where scale varies. Experiments on four crowd counting and two vehicle counting benchmarks demonstrate that PQNet outperforms state-of-the-art methods subjectively and objectively. The code will be available at: https://github.com/sdutzhangchen/PQNet .},
  archive      = {J_EXSY},
  author       = {Chen Zhang and Jing-an Cheng and Qiang Zhou and Wenzhe Zhai and Mingliang Gao},
  doi          = {10.1111/exsy.70098},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70098},
  shortjournal = {Expert Syst.},
  title        = {Privacy-preserving crowd counting via quantum-enhanced federated learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large-scale dataset and robust multifeature representation with maximum correlation-based feature fusion and matching for apparel image retrieval. <em>EXSY</em>, <em>42</em>(9), e70097. (<a href='https://doi.org/10.1111/exsy.70097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the correct match to a probe image from a vast amount of data is critical for the online retrieval of apparel images. These images are captured under an uncontrolled environment (e.g., viewpoint and illumination changes); therefore, such type of data is extremely challenging in Content-Based Image Retrieval (CBIR) research. Even in Google searches, most of the time the query results are provided with inaccurate results or duplicate results due to the minor variations between apparel. Another major challenge is that the extracted feature vector dimensions are too high and difficult to handle. In this paper, a method named Multifeature Representation with Maximum Correlation-based Feature Fusion, and Matching (MFR-MCF 2 M) is proposed for apparel retrieval. This method consists of three modules: (1) Multifeature Representation Module (MFR-M), (2) Maximum Correlation-based Feature Fusion Module (MCF 2 -M) and (3) Multifeature Matching Module (MFM-M). In the MFR module, the shape, texture and deep features of apparel images are extracted using a Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP) and a pretrained deep CNN model, respectively. Also, the dimensionality of extracted features is reduced using the proposed Feature Subselection (FSS) method. The MCF module is implemented to measure the maximum correlation between reduced feature vectors. Finally, MCF 2 is performed using Euclidean distance and a generated Feature Correlation Vector (FCV) to improve the retrieval accuracy and as the benchmark to assess the efficacy of the proposed method. In addition, a new large-scale dataset named Apparel Images Gallery (AIG), which consists of 130,000 images, has been provided to the community. The performance of the proposed MFR-MCF 2 M method is evaluated on three datasets, including two publicly available datasets and the proposed AIG dataset. The retrieval results are obtained after passing through the threshold function of both the Euclidean distance and the computed FCV. The proposed method achieved an accuracy of 78.3% on the clothing dataset, 94.8% on the CR dataset and 89.1% on the proposed AIG dataset. Consequently, the MFR-MCF 2 M outperformed state-of-the-art (SOTA) apparel retrieval methods.},
  archive      = {J_EXSY},
  author       = {Marryam Murtaza and Muhammad Fayyaz and Mussarat Yasmin and Muhammad Anwar and Kashif Naseer Qureshi and Usman Ahmed Raza},
  doi          = {10.1111/exsy.70097},
  journal      = {Expert Systems},
  month        = {9},
  number       = {9},
  pages        = {e70097},
  shortjournal = {Expert Syst.},
  title        = {A large-scale dataset and robust multifeature representation with maximum correlation-based feature fusion and matching for apparel image retrieval},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based contrastive learning with dynamic masking and adaptive pathways for time series anomaly detection. <em>EXSY</em>, <em>42</em>(8), e70102. (<a href='https://doi.org/10.1111/exsy.70102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Anomaly Detection (TSAD) has demonstrated broad applicability across various industries, including manufacturing, healthcare, and finance. Its primary objective is to identify unusual deviations in the test set by capturing the typical behavioral patterns of timing data. Despite their strong detection capabilities when labeled data is not available, current reconstruction-based approaches still struggle with anomalous interference and inadequate semantic information extraction at higher time series levels. To tackle these problems, we provide a multi-scale dual-domain patch attention contrast learning model (DMAP-DDCL) that incorporates adaptive path selection and adaptive dynamic context-aware masking. Dynamic context-aware masks are specifically used by DMAP-DDCL to improve the model's generalization ability and mitigate bias resulting from the influence of anomalous data during training. Multi-scale patch segmentation and dual attention to the segmented patches are introduced to capture local details and global correlations as time dependencies. By enlarging the contrast between the two data perspectives, global and local, DMAP-DDCL improves the capacity to differentiate between normal and abnormal patterns. In addition, we enhance the adaptive path of the multi-scale bi-domain attention network, which adapts the multi-scale modeling process to the temporal dynamics of the inputs and enhances the model's accuracy. According to experimental results, DMAP-DDCL performs better on five real datasets from various domains than eight state-of-the-art baselines. Specifically, our model enhances F1 and R_AUC_ROC by an average of 7.5% and 16.67%.},
  archive      = {J_EXSY},
  author       = {Qian Liang and Xiang Yin},
  doi          = {10.1111/exsy.70102},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70102},
  shortjournal = {Expert Syst.},
  title        = {Transformer-based contrastive learning with dynamic masking and adaptive pathways for time series anomaly detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming healthcare with artificial intelligence and blockchain: A secure, transparent and energy-efficient approach. <em>EXSY</em>, <em>42</em>(8), e70101. (<a href='https://doi.org/10.1111/exsy.70101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The healthcare industry is undergoing a transformative shift with the integration of blockchain technology and Artificial Intelligence (AI). Traditional healthcare systems struggle with data security, lack of transparency, and inefficiencies in resource allocation, leading to increased risks and operational challenges. AI-driven models provide intelligent solutions by enabling predictive diagnostics, fraud detection, and personalised treatment plans, while blockchain ensures data integrity, security, and decentralised access control. The synergy between AI and blockchain enhances decision-making, optimises resource utilisation and fosters trust in healthcare systems by automating processes with greater transparency and security. AI-powered analytics can extract meaningful insights from vast healthcare datasets, improving patient outcomes and streamlining supply chains. Meanwhile, blockchain's immutable ledger safeguards medical data, preventing breaches and ensuring regulatory compliance. This paper presents a comprehensive review of blockchain solutions in healthcare, exploring their impact on enhancing security, promoting transparency and improving energy efficiency. Additionally, this paper also presents insights on the integration of AI and blockchain in healthcare. It categorises existing blockchain-based frameworks and highlights emerging trends, challenges, and future research directions. This review aims to serve as a foundational reference for researchers and practitioners developing secure, transparent, and intelligent healthcare systems.},
  archive      = {J_EXSY},
  author       = {Sagnik Datta and Suyel Namasudra and Nageswara Rao Moparthi and Suchi Kumari and Ruben Gonzalez Crespo},
  doi          = {10.1111/exsy.70101},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70101},
  shortjournal = {Expert Syst.},
  title        = {Transforming healthcare with artificial intelligence and blockchain: A secure, transparent and energy-efficient approach},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing autonomous vehicles: An in-depth review of cyber attacks and anomaly detection challenges. <em>EXSY</em>, <em>42</em>(8), e70100. (<a href='https://doi.org/10.1111/exsy.70100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Vehicle (AV) initiatives have rapidly grown in recent years, significantly impacting daily life and enhancing transportation safety and efficiency. Autonomous driving technology promises a future with fully self-driving vehicles while presenting new challenges in safety assurance. In this review, the evolution, obstacles, and methodologies of the statistical approaches for defining cyber threats and discovering anomalies in AVs, specifically under negative conditions and different datasets, are discussed. More critically, this survey assesses the strengths and weaknesses of these methods for their current and future directions. Discussing anomaly detection in AVs under adverse conditions through Federated Learning (FL) and Deep Learning (DL) techniques enhances threat detection capabilities. Additionally, the review explores security vulnerabilities of intra-vehicle and inter-vehicle communication systems concerning various sensors and perception systems, and examines possible attacks on AV software and hardware, emphasising their effects. In addition, the research proposes defensive schemes, founded on statistical methods, DL, optimisations, FL, and blockchain, for strengthening AVs' security. The review aims to improve AVs' resilience against cyberattacks in reconstructing the weaknesses in sensor and perception systems, thereby resulting in the growth and safety of self-driving technology. Furthermore, the review covers anomaly detection in various scenarios, examining advancements in methodologies for better detection performance. Performance evaluations using publicly available datasets are thoroughly analysed, offering a comprehensive overview of current research trends and suggesting pathways for future improvements in AV detection technology.},
  archive      = {J_EXSY},
  author       = {Ratnapal Kumarswami Mane and Poonam Sharma},
  doi          = {10.1111/exsy.70100},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70100},
  shortjournal = {Expert Syst.},
  title        = {Securing autonomous vehicles: An in-depth review of cyber attacks and anomaly detection challenges},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud layer and precipitation forecasting via multi-scale gated temporal and spatial attention network. <em>EXSY</em>, <em>42</em>(8), e70099. (<a href='https://doi.org/10.1111/exsy.70099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud layer and precipitation forecasting play a crucial role in daily life and decision-making. Most existing deep learning models extract features at a single scale and ignore the correlation between features at different scales in the cloud layer and precipitation data. This hinders the ability to extract multi-scale cloud layer features and precipitation features and further constrains the predictive accuracy of the model. To address these challenges, we propose the multi-scale gated temporal and spatial attention network (MGTSA-Net). This network is designed to capture multi-scale spatiotemporal features in the cloud layer and precipitation data more effectively. As a result, it can improve the accuracy of cloud layer and precipitation forecasting. The core component is the multi-scale temporal gated (MTG) module, which integrates multi-scale convolutions and gated recurrent unit (GRU). To further enhance the model's capability of spatial feature extraction, we integrate a multi-scale spatial attention (MSA) module into the encoder. Experimental evaluations on the WeatherBench dataset demonstrate that the MGTSA-Net outperforms state-of-the-art models in predictive accuracy and computational efficiency.},
  archive      = {J_EXSY},
  author       = {Jiabing Liu and Jianhao Sun and Haiwen Wei and Junzhi Shi and Mingliang Gao},
  doi          = {10.1111/exsy.70099},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70099},
  shortjournal = {Expert Syst.},
  title        = {Cloud layer and precipitation forecasting via multi-scale gated temporal and spatial attention network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning: Concepts, challenges and implementation. <em>EXSY</em>, <em>42</em>(8), e70096. (<a href='https://doi.org/10.1111/exsy.70096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as an innovative approach for distributed neural networks, allowing multiple clients to collaboratively train a model without centralising their data, thus preserving decentralisation and data privacy. This review provides a comprehensive discussion of FL's core concepts, including its components, key challenges, and distinctions from traditional machine learning. The paper outlines the various types of FL, highlighting applications in privacy-sensitive fields like healthcare and finance. It also addresses recent advancements in self-supervised learning, personalisation, and multi-modal applications within FL, as well as the integration of blockchain technology for enhanced privacy. Key advantages of FL are discussed, such as reduced communication overhead through the transmission of model parameters instead of raw data, which minimises network load and enhances privacy protection. Furthermore, the paper explores emerging questions for FL development, including scalability, fairness, and system standardisation. Real-world examples, such as Google Gboard and brain tumour segmentation, are presented to illustrate FL's practical impact. Finally, the paper discusses future directions, including potential integration with other AI techniques like reinforcement learning and transfer learning. This review provides valuable insights for researchers and professionals who are new to FL or seek a broader understanding of its ecosystem. While there are few studies that explore limited aspect of FL, this review adopts a holistic approach and covers all aspects of FL including foundational concepts, implementation, challenges faced by FL, and real-world implementation. The broader scope, which spans FL from concepts to practical implementation, makes it particularly distinctive and a valuable contribution.},
  archive      = {J_EXSY},
  author       = {Naeem Khan and Shibli Nisar and Muhammad Asghar Khan and Muhammad Attique Khan and David Camacho and Yasar Abbas Ur Rehman and Amir Hussain},
  doi          = {10.1111/exsy.70096},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70096},
  shortjournal = {Expert Syst.},
  title        = {Federated learning: Concepts, challenges and implementation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active object detection using a novel network and partial prior information. <em>EXSY</em>, <em>42</em>(8), e70095. (<a href='https://doi.org/10.1111/exsy.70095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active object detection (AOD) enables a system to actively adjust camera parameters or plan the next viewpoint to improve detection accuracy when the current visual input is insufficient. However, most existing AOD methods assume that the target object is visible from the initial viewpoint, which is often unrealistic and reduces task efficiency. To address this limitation, we propose a novel AOD framework that leverages partial prior information to enhance detection performance and task efficiency. Specifically, we construct an extensible prior information library that describes large and easily identifiable adjacent objects (Adj-objects) that are spatially related to the target. This allows the system to initiate AOD based on the presence of an Adj-object, even when the target is initially out of view. Our approach incorporates a duelling deep Q-learning network (Duelling-DQN) with a newly designed reward function to effectively utilise prior information. Additionally, we introduce a viewpoint storage scheme to support fast retrieval and transition between viewpoints. We evaluate the proposed method on the Active Vision Dataset (AVD) and compare it with several state-of-the-art (SOTA) approaches. The experimental results show that our method achieves a superior average success rate of 81.3%, demonstrating its effectiveness in overcoming the initial state limitations of traditional AOD tasks.},
  archive      = {J_EXSY},
  author       = {Jianyu Wang and Feng Zhu and Qun Wang and Pengfei Zhao},
  doi          = {10.1111/exsy.70095},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70095},
  shortjournal = {Expert Syst.},
  title        = {Active object detection using a novel network and partial prior information},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security in metaverse markets: Challenges and Solutions—A comprehensive review. <em>EXSY</em>, <em>42</em>(8), e70094. (<a href='https://doi.org/10.1111/exsy.70094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review paper provides a systematic overview of the metaverse markets security problems and solutions. The metaverse is an emerging digital space, bridging virtual, augmented and mixed reality environments. As the metaverse evolves, issues related to customer security have emerged, which include breaches of privacy, thefts of identity and cybercrimes, all of this compounded by insecurities in the decentralised structures. Through this systematic literature review, we analyse these challenges and assess current approaches to mitigate them, including encryption, decentralised identity and related regulatory frameworks, but highlight their limited capacity for dealing with immersive virtual spaces' unique risks. The review also explores some advanced solutions adopting artificial intelligence, blockchain and privacy enhancing technologies (PETs) for securing the metaverse and enhancing its privacy. Furthermore, the review also points out the gaps in the current literature, particularly a lack of customised customer protection structures and a poor analysis of the psychological effects. It also proposes future solutions such as quantum-resistant security and zero-trust architecture to fortify the security. This review highlights the necessity of partnership between the industries, as well as the setting of security protocols so as to safeguard customer trust and engagement in the growing digital space.},
  archive      = {J_EXSY},
  author       = {Mohammad Z. Aloudat and Mahmoud Barhamgi and Elias Yaacoub and Dani Aoun},
  doi          = {10.1111/exsy.70094},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70094},
  shortjournal = {Expert Syst.},
  title        = {Security in metaverse markets: Challenges and Solutions—A comprehensive review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The performance of distances between time series: An in-depth comparison. <em>EXSY</em>, <em>42</em>(8), e70093. (<a href='https://doi.org/10.1111/exsy.70093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of distance measures between time series has been discussed in diverse studies. Most identified performance as the accuracy resulting from the use of a specific distance in 1-Nearest Neighbour. Few studies have addressed the related computation time, and no systematic analyses of the associations between the distances' performance (1-NN-based accuracy and computation time) and the time series' characteristics have been presented yet. We propose to fill this research gap by analysing these relationships considering the following features: the training and test sets' dimensions, the time series' length, the number of classes, and the classes' separability as measured by the Average Silhouette index. This last characteristic was not mentioned in previous studies. A methodological approach is devised to compare nine distance measures, including three recently proposed combined distances (COMB and two variants). We resort to a stepwise method for multiple comparisons and deal with the experiment-wise error rate to obtain homogeneous groups of distances with indistinct performances. The CART algorithm is used to explore the relationships between accuracy values corresponding to each distance measure under study (target) and the time series characteristics (predictors). Our analyses are based on datasets from the UCR time series classification archive. We concluded that the combined distance (COMB), dynamic time warping distance (DTW), and complexity invariance distance (CID) are consistently included in the subset of best-performing distances in all experimental scenarios. The latter (CID) has a significantly lower computational cost. We determined that the classes' separability is the time series' attribute most associated with the distances' performance.},
  archive      = {J_EXSY},
  author       = {Margarida G. M. S. Cardoso and Ana A. Martins},
  doi          = {10.1111/exsy.70093},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70093},
  shortjournal = {Expert Syst.},
  title        = {The performance of distances between time series: An in-depth comparison},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective manifold representation for opinion mining. <em>EXSY</em>, <em>42</em>(8), e70092. (<a href='https://doi.org/10.1111/exsy.70092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis plays a crucial role across various domains, requiring advanced methods for effective dimensionality reduction and feature extraction. This study introduces a novel framework, multi-objective manifold representation (MOMR) for opinion mining, which uniquely integrates deep global features with local manifold representations to capture comprehensive data patterns efficiently. Unlike existing methods, MOMR employs advanced dimensionality reduction techniques combined with a self-attention mechanism, enabling the model to focus on contextually relevant textual elements. This dual approach not only enhances interpretability but also improves the performance of sentiment analysis. The proposed method was rigorously evaluated against both classical techniques such as long short-term memory (LSTM), naive Bayes (NB) and support vector machines (SVMs), and modern state-of-the-art models including recurrent neural networks (RNN) and convolutional neural networks (CNN). Experiments on diverse datasets: IMDB, Fake News, Twitter and Yelp demonstrated the superior accuracy and robustness of MOMR. By outperforming competing methods in terms of generalizability and effectiveness, MOMR establishes itself as a significant advancement in sentiment analysis, with broad applicability in real-world opinion mining tasks ( https://github.com/pshtirahman/Sentiment-Analysis.git ).},
  archive      = {J_EXSY},
  author       = {Pshtiwan Rahman and Fatemeh Daneshfar and Hashem Parvin},
  doi          = {10.1111/exsy.70092},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70092},
  shortjournal = {Expert Syst.},
  title        = {Multi-objective manifold representation for opinion mining},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive review of graph neural networks: Challenges, classification, architectures, applications, and potential utility in bioinformatics. <em>EXSY</em>, <em>42</em>(8), e70091. (<a href='https://doi.org/10.1111/exsy.70091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are data structures that represent complex interactions in artificial and natural systems. While deep learning has revolutionised tasks like image processing, audio/video analysis, and natural language processing, these tasks can be viewed as special cases of graph representation learning. Real-world data is often graph-structured, representing complex dependencies in physical systems, molecular signatures, and disease prediction. Graph neural networks (GNNs) excel at processing such non-Euclidean data by capturing dependencies through message passing between graph nodes. This review provides an organised in-depth overview of existing GNN models, emphasising their applications in bioinformatics apart from most structured and unstructured GNN data utility. We provide formal mathematical foundations, compare key model variants, and evaluate their performance across real-world tasks. To enable systematic analysis, we propose a unified taxonomy based on three core axes: learning settings, expressive capacity, and aggregation mechanisms. The taxonomy defines four main GNN types: structure-agnostic, structure-aware, sparsity-optimized, and advanced learning-based models. Regarding applications, we studied them under a proposed taxonomy in detail. Additionally, we provide resources for evaluating and implementing GNN models, including open-source code, bioinformatics databases, and general GNN benchmark datasets. Finally, we propose eight GNN challenges along with corresponding research directions to advance the field. Our survey aims to establish a common reference point for researchers, empowering them to harness the full potential of GNNs in tackling the complexities of both natural and artificial systems.},
  archive      = {J_EXSY},
  author       = {Adil Mudasir Malla and Asif Ali Banka},
  doi          = {10.1111/exsy.70091},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70091},
  shortjournal = {Expert Syst.},
  title        = {Comprehensive review of graph neural networks: Challenges, classification, architectures, applications, and potential utility in bioinformatics},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLAMES—Federated learning for advanced MEdical segmentation. <em>EXSY</em>, <em>42</em>(8), e70090. (<a href='https://doi.org/10.1111/exsy.70090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is gaining traction across numerous fields for its ability to foster collaboration among multiple participants while preserving data privacy. In the medical domain, FL enables institutions to share knowledge while maintaining control over their data, which often vary in modality, source, and quantity. Institutions are often specialised in treating one or a few types of tumours, typically focusing on a specific organ. Hence, different institutions may contribute with distinct types of medical imaging data of various organs, originating from diverse machines. Collaboration among these institutions enhances performance on shared tasks across different areas of the body. The framework employs modality-specific models hosted on the server, each designed for a particular imaging modality and designed to predict the presence of tumours in scans from its respective modality, regardless of the organ being imaged. Clients focus on their specific imaging modality, utilising knowledge derived from images contributed by institutions employing the same modality. This approach facilitates broader collaboration, extending beyond institutions specialising in the same organ to include those working within the same imaging modality. This approach also helps avoid the introduction of potential noise from clients with images of different modalities, which might hinder the model's ability to effectively specialise and adapt to the data specific to each institution. Experiments showed that FLAMES achieves strong performance on server data, even when tested across different organs, demonstrating its ability to generalise effectively across diverse medical imaging datasets. Our code is available at https://github.com/MODAL-UNINA/FLAMES .},
  archive      = {J_EXSY},
  author       = {Martina Savoia and Edoardo Prezioso and Francesco Piccialli},
  doi          = {10.1111/exsy.70090},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70090},
  shortjournal = {Expert Syst.},
  title        = {FLAMES—Federated learning for advanced MEdical segmentation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CasText: Fusion of text information flow and global perspective for predicting the size of information dissemination. <em>EXSY</em>, <em>42</em>(8), e70089. (<a href='https://doi.org/10.1111/exsy.70089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting the size of information dissemination has important theoretical and practical significance for formulating content distribution strategies, optimising network resource allocation and conducting effective public opinion management. Current research on the cascade growth size of social media information dissemination mostly focuses on network structure and user behaviour analysis. Still, it neglects the crucial role of textual information in driving information dissemination. We propose a deep learning framework called CasText, which integrates multisource features such as text information, global propagation graphs and local propagation structures to more accurately predict the size of information propagation. Using Sentence-BERT to extract deep semantic features of text and combining it with GNN, precise capture of the interaction between text information and cascading structures has been achieved; using DeepWalk to view the entire social network as a complex graphic structure, high-dimensional feature representations of each social media user can be automatically learned. This global perspective helps to reveal broader dissemination patterns and potential influence paths, thereby improving the accuracy of predicting the size of future information dissemination. In multiple comparative experiments based on a real Weibo cascaded text retweeting dataset, the CasText model improved the MSLE index by 3.1% compared to the baseline model, significantly demonstrating the effectiveness of multisource feature fusion in predicting information dissemination size. We further confirmed the importance of text information, global propagation graphs and local propagation embeddings in improving model performance through ablation experiments.},
  archive      = {J_EXSY},
  author       = {Hao Luo and Guixiang Cheng and Zhongying Deng and Haiyang Chi and Xin Yan},
  doi          = {10.1111/exsy.70089},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70089},
  shortjournal = {Expert Syst.},
  title        = {CasText: Fusion of text information flow and global perspective for predicting the size of information dissemination},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling the application of graph neural networks on graphs with unknown connectivity. <em>EXSY</em>, <em>42</em>(8), e70088. (<a href='https://doi.org/10.1111/exsy.70088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have proven to be reliable methods for working with graph-structured data. However, it is common to find graphs with partially or fully inaccessible connectivity patterns, hindering the direct application of GNNs to the task at hand. To tackle this problem, several Graph Structure Learning (GSL) methods have been proposed, with the objective of jointly optimizing both the graph structure and the GNN model by adding loss terms that enforce desired graph properties. These properties, such as sparseness and connectivity of similar nodes, can have a drastic impact on the performance of a GNN. However, current methods offer little control on the desired degree of sparseness, which may lead to non-optimal connectivity and reduced efficiency. In this paper, we propose a new method called Adaptative Sparsification Graph Learning (ASGL), which enables fine-grained, linear control over the total number of edges in the resulting learned graph via a novel perturbation-based loss term. ASGL not only provides flexibility in sparsity control but also improves both accuracy and computational efficiency, outperforming state-of-the-art methods in most benchmarks. We demonstrate its robustness through extensive experiments and highlight how adjusting sparsity enables optimizing the trade-off between accuracy, complexity, and interpretability.},
  archive      = {J_EXSY},
  author       = {Jorge García-Carrasco and Alejandro Maté and Juan Trujillo},
  doi          = {10.1111/exsy.70088},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70088},
  shortjournal = {Expert Syst.},
  title        = {Enabling the application of graph neural networks on graphs with unknown connectivity},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated cross-domain recommendation framework with graph neural network. <em>EXSY</em>, <em>42</em>(8), e70087. (<a href='https://doi.org/10.1111/exsy.70087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain recommendation (CDR) leverages more abundant source-domain information to improve target-domain recommendation accuracy. However, traditional centralized CDR approaches face two critical limitations: (1) centralized data storage causes privacy vulnerabilities against malicious servers, and (2) gradient leakage during uploading enables recovery of source data. To address these challenges, in this work, we propose FedGraphCDR, a federated learning-based cross-domain recommendation framework that integrates local differential privacy (LDP) with pseudo item injection during gradient aggregation to prevent gradient leakage attacks, while utilizing graph neural networks to identify comparable users and mitigate cold-start problems. Evaluation on a real-life Douban dataset spanning three domains demonstrates that our framework successfully combines LDP with pseudo items to enhance privacy protection while achieving superior recommendation accuracy over benchmark methods. The results confirm that FedGraphCDR effectively resolves privacy concerns and improves recommendation quality, particularly for cold-start users, and establishes a practical solution for privacy-preserving cross-domain recommendation.},
  archive      = {J_EXSY},
  author       = {Deling Huang and Qilong Feng},
  doi          = {10.1111/exsy.70087},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70087},
  shortjournal = {Expert Syst.},
  title        = {Federated cross-domain recommendation framework with graph neural network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid spiking model for anomaly detection in multivariate time series. <em>EXSY</em>, <em>42</em>(8), e70086. (<a href='https://doi.org/10.1111/exsy.70086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have exhibited preeminent performance in anomaly detection, but they struggle to effectively capture changes over time in multivariate time-series data and suffer from resource consumption issues. Spiking neural networks address these limitations by capturing the change in time-varying signals and decreasing resource consumption, but they sacrifice performance. This paper develops a novel spiking-based hybrid model incorporated a temporal prediction network and a reconstruction network. It integrates a unique first-spike frequency encoding scheme and a firing rate based anomaly score method. The encoding scheme enhances the event representation ability, while the anomaly score enables efficient anomaly identification. Our proposed model not only maintains low resource consumption but also improves the ability of anomaly detection. Experiments on publicly real-world datasets confirmed that the proposed model acquires state-of-the-art performance superior to existing approaches. Remarkably, it costs 5.04× lower energy consumption compared with the artificial neural network version.},
  archive      = {J_EXSY},
  author       = {Wei Zhang and Ping He and Shengrui Wang and Fan Yang and Ying Liu},
  doi          = {10.1111/exsy.70086},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70086},
  shortjournal = {Expert Syst.},
  title        = {A hybrid spiking model for anomaly detection in multivariate time series},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based prediction in HTTP Request–Response cycles: Impacts on webpage quality metrics. <em>EXSY</em>, <em>42</em>(8), e70085. (<a href='https://doi.org/10.1111/exsy.70085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hypertext transfer protocol (HTTP) request–response cycles during webpage access and content posting exhibit recognisable patterns; however, no unified standard currently streamlines both activities, despite the existence of independent specifications for each. Previous research has leveraged cycles of client–server requests and responses to predict outcomes such as user behaviour (UB) analysis, anomaly detection (AD), performance optimisation (PE), predictive maintenance (PM) and user authentication and security (UA), often without explicitly associating these activities. Addressing this gap, the present study focuses on the combined modelling of HTTP request–response cycles for both webpage access and personal information submission. An experimental study was conducted, where HTTP sessions were generated and analysed for both access and posting activities. Six machine learning models—Decision Tree, Random Forest, Gradient Boosting, k-Nearest Neighbours (kNNs), Logistic Regression and Support Vector Machine—were applied to both the CSIC 2010 HTTP dataset and lab-generated HTTP transmission datasets across the UB, AD, PE-PM and UA tasks. Results indicate that the Random Forest classifier achieved the highest accuracy of 97.53% in predicting AD-based HTTP request–response cycles during webpage access, and 85.93% accuracy in predicting PE-PM tasks during content posting. Gradient Boosting, kNNs and Support Vector Machine models also demonstrated strong versatility and robustness across different HTTP cycle prediction tasks. Furthermore, the analysis concluded that HTTP request–response cycles for webpage access exhibit greater structural consistency compared to those associated with content posting activities.},
  archive      = {J_EXSY},
  author       = {Ala Alarood},
  doi          = {10.1111/exsy.70085},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70085},
  shortjournal = {Expert Syst.},
  title        = {Machine learning-based prediction in HTTP Request–Response cycles: Impacts on webpage quality metrics},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single word change is all you need: Using LLMs to create synthetic training examples for text classifiers. <em>EXSY</em>, <em>42</em>(8), e70079. (<a href='https://doi.org/10.1111/exsy.70079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In text classification, creating an adversarial example means subtly perturbing a few words in a sentence without changing its meaning, causing it to be misclassified by a classifier. A concerning observation is that a significant portion of adversarial examples generated by existing methods change only one word. This single-word perturbation vulnerability represents a significant weakness in classifiers, which malicious users can exploit to efficiently create a multitude of adversarial examples. This paper studies this problem and makes the following key contributions: (1) We introduce a novel metric to quantitatively assess a classifier's robustness against single-word perturbation . (2) We present the SP-Attack, designed to exploit the single-word perturbation vulnerability, achieving a higher attack success rate, better preserving sentence meaning, while reducing computation costs compared to state-of-the-art adversarial methods. (3) We propose SP-Defence, which aims to improve by applying data augmentation in learning. Experimental results on 4 datasets and 2 masked language models show that SP-Defence improves by 14.6% and 13.9% and decreases the attack success rate of SP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the attack success rate of existing attack methods that involve multiple-word perturbations.},
  archive      = {J_EXSY},
  author       = {Lei Xu and Sarah Alnegheimish and Laure Berti-Equille and Alfredo Cuesta-Infante and Kalyan Veeramachaneni},
  doi          = {10.1111/exsy.70079},
  journal      = {Expert Systems},
  month        = {8},
  number       = {8},
  pages        = {e70079},
  shortjournal = {Expert Syst.},
  title        = {Single word change is all you need: Using LLMs to create synthetic training examples for text classifiers},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for extracting black box models based on interpretable attention. <em>EXSY</em>, <em>42</em>(7), e70084. (<a href='https://doi.org/10.1111/exsy.70084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have achieved remarkable success in face recognition. However, their vulnerability has attracted considerable attention. Researchers can analyse the weaknesses of face recognition models by extracting their functionality, aiming to enhance the security performance of these models. The findings of the study reveal that current model extraction methods are afflicted with notable drawbacks, namely low similarity in capturing model functionality and insufficient availability of samples. These limitations significantly impede the analysis of model security performance. We propose an interpretable attention-based method for black-box model extraction, enhancing the similarity between substitute and victim model functionality. Our main contributions are summarized as follows: (i) This study addresses the issue of limited sample training caused by the restricted number of black-box hard label queries. (ii) By applying input perturbations, we obtain feedback from deep black-box models, enabling us to identify facial local regions and the distribution of feature weights that positively influence predictions. (iii) By normalizing the feature weight distribution matrix and associating it with the attention weight matrix, the construction of an attention mask for the dataset is achieved, enabling differential attention to features in different regions. (iv) Leveraging a pre-trained base model, we extract relevant knowledge and features, facilitating cross-domain knowledge transfer. Experiments on Emore, PubFig and CASIA-WebFace show that our method outperforms traditional methods by 10%–20% in model consistency for the same query budget. Also, our method achieves the highest model stealing consistency on the three datasets: 94.51%, 93.27% and 91.74%, respectively.},
  archive      = {J_EXSY},
  author       = {Lijun Gao and Huibin Tian and Kai Liu},
  doi          = {10.1111/exsy.70084},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70084},
  shortjournal = {Expert Syst.},
  title        = {A method for extracting black box models based on interpretable attention},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing smart tourism with chatbots: Focus on the metamodel of domain-specific language and emerging technologies. <em>EXSY</em>, <em>42</em>(7), e70083. (<a href='https://doi.org/10.1111/exsy.70083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tourism sector is adopting smart solutions to offer visitors more personalised and sustainable experiences. By leveraging urban infrastructure and new technologies, tourist destinations aim to enhance the interaction between travellers and their environment. Artificial intelligence (AI) and natural language processing (NLP) play a key role in this transformation, particularly through chatbots. They are AI-driven applications designed to simulate human-like conversations, enabling users to interact with digital services through text or voice interfaces. In the tourism sector, they facilitate real-time access to information and services, improving the visitors' experience. These applications typically rely on intent recognition APIs, which may be proprietary, requiring access fees and potentially leading to high implementation costs. This study explores the use of a domain-specific language (DSL) dedicated to chatbot development for smart tourism. The first contribution comprises various research topics and emerging technologies used to improve smart tourism experiences and their impact on key tourism components such as attractions, accessibility, amenities, activities, available packages, and ancillary services. Second, this work aims to present the key concepts of model-driven engineering involved in constructing a DSL and to introduce our approach to building a DSL, with a focus on presenting the DSL metamodel. Third, this study identifies the challenges and limitations of using DSLs in chatbot development.},
  archive      = {J_EXSY},
  author       = {Lamya Benaddi and Adnane Souha and Charaf Ouaddi and Abdellah Chehri and Abdeslam Jakimi and Brahim Ouchao},
  doi          = {10.1111/exsy.70083},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70083},
  shortjournal = {Expert Syst.},
  title        = {Enhancing smart tourism with chatbots: Focus on the metamodel of domain-specific language and emerging technologies},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming earth observation: An extensive evaluation of vision transformers for satellite images-based land cover classification. <em>EXSY</em>, <em>42</em>(7), e70082. (<a href='https://doi.org/10.1111/exsy.70082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite imagery offers rich information for land cover classification, but choosing an effective yet efficient feature extractor or backbone architecture remains challenging. In this study, I benchmark 25 vision-transformers across 10 public land cover datasets to guide backbone selection for downstream classification tasks. The proposed approach encodes each satellite image into a fixed-length feature vector via a pre-trained transformer, then trains and tests a linear support-vector classifier on these encodings to isolate the impact of the backbone alone. I report average classification accuracy and F1-score over three random stratified splits per dataset, and I also measure training time to assess the computational cost. Results show that the image encoding performed using large-receptive-field transformers with advanced self-attention—particularly deit3_base_patch16_224 and twins_svt_large —achieve the highest accuracies without incurring prohibitive training times. In contrast, encodings of the compact variants achieve faster training but incur notable performance drops around 7%–8%. These findings reveal a clear trade-off between representational power and efficiency. Practitioners can leverage such rankings to select a transformer backbone that best balances accuracy and computational efficiency for satellite image-based land cover classification tasks, accelerating the development of robust and resource-aware systems.},
  archive      = {J_EXSY},
  author       = {Fakhri Alam Khan},
  doi          = {10.1111/exsy.70082},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70082},
  shortjournal = {Expert Syst.},
  title        = {Transforming earth observation: An extensive evaluation of vision transformers for satellite images-based land cover classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toxic discourse in the digital battlefield: Analysing telegram channels during the Russia–Ukraine ‘Conflict’. <em>EXSY</em>, <em>42</em>(7), e70081. (<a href='https://doi.org/10.1111/exsy.70081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instant messenger Telegram has emerged as a favoured platform for far-right activism, conspiracy theories, political propaganda, and misinformation, which has its own target audience. This study explores the application of multilingual pre-trained language models to detect and measure toxicity in political content on Telegram channels. The proposed techniques have shown notable advancements in identifying toxic information using a fine-tuned RoBERTa model. Through the combination of data analysis, time-series analysis, and BERTopic modelling, the research demonstrates how toxicity varies by topic, country, and time period, using metadata. The study identified key topics in the dataset, which includes 23.6 million messages from 1491 Telegram channels, including the Russian–Ukrainian conflict and political tensions in Europe and the United States from 2016 to 1 July 2023. Despite these achievements, challenges such as the dominance of Russian language content and a focus on specific topics were highlighted. This research advances the understanding of how toxic language and propaganda are disseminated across different languages and political narratives, contributing to the study of digital communication and information warfare.},
  archive      = {J_EXSY},
  author       = {Arsenii Tretiakov and Sergio D'Antonio-Maceiras and Áurea Anguera de Sojo Hernández and Alejandro Martín},
  doi          = {10.1111/exsy.70081},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70081},
  shortjournal = {Expert Syst.},
  title        = {Toxic discourse in the digital battlefield: Analysing telegram channels during the Russia–Ukraine ‘Conflict’},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of artificial intelligence in rock tunnel engineering: A survey on where and how. <em>EXSY</em>, <em>42</em>(7), e70080. (<a href='https://doi.org/10.1111/exsy.70080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rock tunnel engineering (RTE) plays a crucial role in modern infrastructure development. The development of artificial intelligence (AI) is able to drive transformative advances in RTE. This review provides an in-depth analysis of the AI application in RTE. Through a comprehensive examination of existing literature, we explore how AI technologies have revolutionised various aspects of RTE, including construction methodology, rock parameter estimation, hazard disaster management during construction, and tunnel operation. In addition, we provide an in-depth study of the synergies between various AI algorithms and related open datasets. This work also outlines promising future research directions for the AI application in RTE, aiming to inspire further advancements in this emerging field. In conclusion, this review underscores the positive influence of AI on RTE, emphasising its capacity to elevate efficiency, accuracy, and safety standards throughout various phases of tunnel projects. The convergence of AI with RTE holds immense promise for advancing the field and ensuring the success and sustainability of future tunnel infrastructure endeavours.},
  archive      = {J_EXSY},
  author       = {Xiaojie Yu and Ben-Guo He and Xu Xu and Yicong Zhou and Miguel A. Diaz and Junxin Chen and David Camacho},
  doi          = {10.1111/exsy.70080},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70080},
  shortjournal = {Expert Syst.},
  title        = {Application of artificial intelligence in rock tunnel engineering: A survey on where and how},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). T-VAE: Transformer-based variational AutoEncoder for perceiving anomalies in multivariate time series data. <em>EXSY</em>, <em>42</em>(7), e70078. (<a href='https://doi.org/10.1111/exsy.70078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly perception in multivariate time series data has crucial applications in various domains such as industrial control and intrusion detection. In real-world scenarios, the sequence information in multivariate time series data, which encompasses the temporal order and dependencies among high-dimensional samples and features, can be complex and nonlinear. Additionally, the time series data often exhibit high volatility and are interspersed with noise data. These factors make anomaly perception in multivariate time series challenging. Despite the recent development of deep learning methods, only a few are able to address all of these challenges. In this paper, we propose a Transformer-based Variational AutoEncoder (T-VAE) for anomaly perception in multivariate time series data. The T-VAE consists of two sub-networks, the Representation Network and the Memory Network, and achieves end-to-end jointly optimisation. The Representation Network leverages self-attention mechanisms and residual network structures to capture sequence information and metaphorical patterns from multivariate time series data. The Memory Network employs a Variational AutoEncoder to learn the distribution of normal data. It employs Maximum Mean Discrepancy to approximate the distribution of high-volatility and noisy data to the distribution of the normal data. We evaluate T-VAE on five datasets, showing superior performance and validating its effectiveness and robustness through comprehensive ablation studies and sensitivity analyses.},
  archive      = {J_EXSY},
  author       = {Chang Li and Yeo Chai Kiat and Jiwu Jing and Chun Long},
  doi          = {10.1111/exsy.70078},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70078},
  shortjournal = {Expert Syst.},
  title        = {T-VAE: Transformer-based variational AutoEncoder for perceiving anomalies in multivariate time series data},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linearformer: Tri-net multi-layer DVF medical image registration. <em>EXSY</em>, <em>42</em>(7), e70077. (<a href='https://doi.org/10.1111/exsy.70077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical imaging, accurate registration is crucial for reliable analysis. While transformer models demonstrate potential, their application to large datasets like OASIS is constrained by substantial memory requirements, quadratic complexity and the challenge of managing complex deformations. To overcome these challenges, Linearformer is introduced, an efficient transformer-based model with Linear-ProbSparse self-attention for optimised time and memory, along with TNM DVF, a Pyramid-based framework for unsupervised non-rigid registration. Evaluated on OASIS and LPBA40 brain MRI datasets, the model outperforms state-of-the-art methods in Dice score and Jacobian metrics, surpassing TransMatch by 0.6% and 1.9% on the two datasets while maintaining a comparable voxel folding percentage.},
  archive      = {J_EXSY},
  author       = {Muhammad Anwar and Zhiyue Yan and Wenming Cao},
  doi          = {10.1111/exsy.70077},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70077},
  shortjournal = {Expert Syst.},
  title        = {Linearformer: Tri-net multi-layer DVF medical image registration},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single and ensemble based filters in environmental data. <em>EXSY</em>, <em>42</em>(7), e70076. (<a href='https://doi.org/10.1111/exsy.70076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers rely on species distribution models (SDMs) to establish a correlation between species occurrence records and environmental data. These models offer insights into the ecological and evolutionary aspects of the subject. Feature selection (FS) aims to choose useful interlinked features or remove unnecessary and redundant ones and make the induced model easier to understand. Although feature selection plays a crucial role in SDMs, only a limited number of studies in the literature have addressed it with several key shortcomings such as lack of the use of multivariate techniques, lack of comparison between the univariate and the multivariate filters, and absence of a comparison between the ensemble univariate and multivariate filters. Therefore, this study presents a rigorous empirical evaluation consisting of assessing and comparing six filter-based univariate feature selection methods using two thresholds with two multivariate techniques, as well as four classifiers: Extreme Gradient boosting (XGB), Random Forest (RF), Decision Tree (DT), and Light gradient-boosting machine (LGBM). Furthermore, the current study proposes a novel approach for ensemble construction consisting of evaluating the applications of ensemble learning using 40% of features ranked by means of Borda Count and Reciprocal Rank (univariate filter ensembles) as well as the fusion-based and the intersection-based ensembles (multivariate filter ensembles). Moreover, we evaluated and compared the performances of univariate and multivariate techniques with their ensembles. Similarly, we evaluated and compared the performances of the best ensemble techniques across datasets. The empirical evaluations involve several techniques, such as the 5-fold cross-validation method, the Scott Knott (SK) test, and Borda Count. In addition, we used three performance metrics (accuracy, Kappa, and F 1-score). Experiments showed that Consistency-based subset selection in conjunction with RF outperformed all other univariate and multivariate FS techniques with an accuracy value of 91.63% across all datasets. However, Fisher score trained with RF was the best choice when considering the number of features. Moreover, the univariate or multivariate based ensembles, in general, outperformed their singles. In addition, when comparing the univariate and multivariate ensembles, the fusion-based ensemble outperformed all other ensembles achieving an accuracy of 91.77% when using RF across datasets. Nevertheless, in terms of performance and number of features, the ensemble constructed using Reciprocal Rank performed better than all other FS techniques regardless of the classifier used. It achieved an accuracy of 91.61% across datasets when using RF.},
  archive      = {J_EXSY},
  author       = {Yousra Cherif and Ali Idri},
  doi          = {10.1111/exsy.70076},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70076},
  shortjournal = {Expert Syst.},
  title        = {Single and ensemble based filters in environmental data},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimising performance curves for ensemble models through pareto front analysis of the decision space. <em>EXSY</em>, <em>42</em>(7), e70075. (<a href='https://doi.org/10.1111/exsy.70075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Receiver operating characteristic curves are commonly used to evaluate the performance of machine learning ensemble classification models that combine multiple classifiers through a voting procedure. Although these models have many parameters, standard ROC analyses typically vary only the voting threshold, limiting their potential for improvement. In this paper, we propose Performance Curve Mapping , a new method that redefines the ROC curve as the Pareto front of a multi-objective optimisation problem. The method maps the multidimensional space of all ensemble parameters (Decision space) into a two-dimensional Objective space defined by classification performance metrics. We employ an algorithm based on NSGA-II to explore the Decision space and validate the proposal on two different classification problems: (1) predicting car insurance claims in a highly imbalanced dataset ( Insurance dataset), and (2) predicting obesity risk in a balanced clinical dataset ( GenObIA dataset). We compare our method with alternative ensemble optimisation approaches, using visual assessment, the area under the curve and the Youden index as performance measures. In the Insurance dataset, Performance Curve Mapping achieves an average improvement of 46.4% in AUC-ROC and 26.1% in the Youden index. In the GenObIA dataset, it achieves an average improvement of 29.7% in AUC-ROC and 11.9% in the Youden index. All improvements are calculated relative to the maximum achievable improvement.},
  archive      = {J_EXSY},
  author       = {Alberto Gutierrez-Gallego and Oscar Garnica and Daniel Parra and J. Manuel Velasco and J. Ignacio Hidalgo},
  doi          = {10.1111/exsy.70075},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70075},
  shortjournal = {Expert Syst.},
  title        = {Optimising performance curves for ensemble models through pareto front analysis of the decision space},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards clustering of incomplete mixed-attribute data. <em>EXSY</em>, <em>42</em>(7), e70074. (<a href='https://doi.org/10.1111/exsy.70074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering analysis is one of the most important data mining and knowledge discovery tools in real applications. Since the widespread presence of missing values hampers clustering performance, missing values imputation becomes necessary for data pre-processing. However, for the common datasets composed of both numerical and categorical attributes (also known as mixed-attribute datasets), most existing imputation methods suffer from the following three limitations: (1) Only feasible for a certain type of attribute; (2) Encounter difficulties in considering the interdependence between different types of attributes; (3) Short in exploiting the information provided by the incomplete mix-valued objects. As a result, the original data distribution can be ill-restored, misleading the downstream clustering tasks. This paper therefore proposes a clustering-imputation co-learning method for incomplete mixed-attribute datasets to address these issues. This method integrates imputation and clustering into one learning process, emphasising the interrelationships between mixed attributes during the imputation process and exploiting the information of incomplete objectsduring clustering. It turns out that appropriate recovery of the dataset and accurate clustering can be better achieved through a cross-coupling manner. Experiments on various datasets validate the promising efficacy of the proposed method.},
  archive      = {J_EXSY},
  author       = {Chuyao Zhang and Xinxi Chen and Zexi Tan and Fangqing Gu and Yuzhu Ji and Yiqun Zhang},
  doi          = {10.1111/exsy.70074},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70074},
  shortjournal = {Expert Syst.},
  title        = {Towards clustering of incomplete mixed-attribute data},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the significance of graph neural networks with pretrained transformers in content-based recommender systems for academic article classification. <em>EXSY</em>, <em>42</em>(7), e70073. (<a href='https://doi.org/10.1111/exsy.70073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are tools for interacting with large and complex information spaces by providing a personalised view of such spaces, prioritising items that are likely to be of interest to the user. In addition, they serve as a significant tool in academic research, helping authors select the most appropriate journals for their academic articles. This paper presents a comprehensive study of various journal recommender systems, focusing on the synergy of graph neural networks (GNNs) with pretrained transformers for enhanced text classification. Furthermore, we propose a content-based journal recommender system that combines a pretrained Transformer with a Graph Attention Network (GAT) using title, abstract and keywords as input data. The proposed architecture enhances text representation by forming graphs from the Transformers' hidden states and attention matrices, excluding padding tokens. Our findings highlight that this integration improves the accuracy of the journal recommendations and reduces the transformer oversmoothing problem, with RoBERTa outperforming BERT models. Furthermore, excluding padding tokens from graph construction reduces training time by 8%–15%. Furthermore, we offer a publicly available dataset comprising 830,978 articles.},
  archive      = {J_EXSY},
  author       = {Jiayun Liu and Manuel Castillo-Cara and Raúl García-Castro},
  doi          = {10.1111/exsy.70073},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70073},
  shortjournal = {Expert Syst.},
  title        = {On the significance of graph neural networks with pretrained transformers in content-based recommender systems for academic article classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging ethical narratives to enhance LLM-AutoML generated machine learning models. <em>EXSY</em>, <em>42</em>(7), e70072. (<a href='https://doi.org/10.1111/exsy.70072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing popularity of generative AI and large language models (LLMs) has sparked innovation alongside debate, particularly around issues of plagiarism and intellectual property law. However, a less-discussed concern is the quality of code generated by these models, which often contains errors and encourages poor programming practices. This paper proposes a novel solution by integrating LLMs with automated machine learning (AutoML). By leveraging AutoML's strengths in hyperparameter tuning and model selection, we present a framework for generating robust and reliable machine learning (ML) algorithms. Our approach incorporates natural language processing (NLP) and natural language understanding (NLU) techniques to interpret chatbot prompts, enabling more accurate and customisable ML model generation through AutoML. To ensure ethical AI practices, we have also introduced a filtering mechanism to address potential biases and enhance accountability. The proposed methodology not only demonstrates practical implementation but also achieves high predictive accuracy, offering a viable solution to current challenges in LLM-based code generation. In summary, this paper introduces a new application of NLP and NLU to extract features from chatbot prompts, feeding them into an AutoML system to generate ML algorithms. This approach is framed within a rigorous ethical framework, addressing concerns of bias and accountability while enhancing the reliability of code generation.},
  archive      = {J_EXSY},
  author       = {Jordan Nelson and Michalis Pavlidis and Andrew Fish and Nikolaos Polatidis and Yannis Manolopoulos},
  doi          = {10.1111/exsy.70072},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70072},
  shortjournal = {Expert Syst.},
  title        = {Leveraging ethical narratives to enhance LLM-AutoML generated machine learning models},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-guided diffusion model for high-fidelity ECG synthesis and classification. <em>EXSY</em>, <em>42</em>(7), e70070. (<a href='https://doi.org/10.1111/exsy.70070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram (ECG) synthesis plays a crucial role in medical research, education and device development. However, achieving high-fidelity ECG signal synthesis remains challenging, particularly in accurately reproducing specific waveform patterns at the sample level. In this paper, we propose an uncertainty-guided diffusion model that integrates uncertainty estimation into the ECG synthesis process. The uncertainty guidance preserves meaningful waveform characteristics. The model combines diffusion models, known for generating high-quality samples from complex distributions, with uncertainty guidance that captures and propagates uncertainty throughout the pipeline. Extensive experiments demonstrate that our approach outperforms existing methods in terms of both distribution-level and sample-level evaluation.},
  archive      = {J_EXSY},
  author       = {Qi Zhang and Hongyan Li},
  doi          = {10.1111/exsy.70070},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70070},
  shortjournal = {Expert Syst.},
  title        = {Uncertainty-guided diffusion model for high-fidelity ECG synthesis and classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainability of highly associated fuzzy churn patterns in binary classification. <em>EXSY</em>, <em>42</em>(7), e70066. (<a href='https://doi.org/10.1111/exsy.70066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer churn, particularly in the telecommunications sector, influences both costs and profits. As the explainability of models becomes increasingly important, this study emphasises not only the explainability of customer churn through machine learning models, but also the importance of identifying multivariate patterns and setting soft bounds for intuitive interpretation. The main objective is to use a machine learning model and fuzzy-set theory with top- k HUIM to identify highly associated patterns of customer churn with intuitive identification, referred to as Highly Associated Fuzzy Churn Patterns (HAFCP). Moreover, this method aids in uncovering association rules among multiple features across low, medium, and high distributions. Such discoveries are instrumental in enhancing the explainability of findings. Experiments show that when the top-5 HAFCPs are included in five datasets, a mixture of performance results is observed, with some showing notable improvements. It becomes clear that high importance features enhance explanatory power through their distribution and patterns associated with other features. As a result, the study introduces an innovative approach that improves the explainability and effectiveness of customer churn prediction models.},
  archive      = {J_EXSY},
  author       = {D. Y. C. Wang and Lars Arne Jordanger and Jerry Chun-Wei Lin},
  doi          = {10.1111/exsy.70066},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70066},
  shortjournal = {Expert Syst.},
  title        = {Explainability of highly associated fuzzy churn patterns in binary classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A critical analysis of generative adversarial networks in anomaly detection for time series. <em>EXSY</em>, <em>42</em>(7), e70065. (<a href='https://doi.org/10.1111/exsy.70065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection has applications across different knowledge domains and is intricately linked to numerous problems, such as fault detection for industrial and measurement systems. However, the usual completely unsupervised nature of the problem complicates and restricts the application of various intelligent models. In this context, solutions based on GANs for modelling distributions and arbitrary processes with unsupervised data show potential in anomaly detection. This work addresses a solution based on the TadGAN architecture in the unsupervised detection of anomalies in time series. Initially, a brief review of the state of the art on essential concepts about anomalies in time series is provided, as well as the main works involving GANs in this respective area. Subsequently, the TadGAN architecture is assessed utilising the proposed methodology, wherein its principles and primary limitations are discussed, such as the absence of standardisation in performance evaluation metrics. As an innovation, we assess TadGAN using experimental data and propose new metrics to quantify the anomalous state from both the model and the data. The obtained results confirm the significant potential of GANs in detecting anomalies in time series.},
  archive      = {J_EXSY},
  author       = {Marcelo Bozzetto and Maurício Cagliari Tosin and Tiago Oliveira Weber and Alexandre Balbinot},
  doi          = {10.1111/exsy.70065},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70065},
  shortjournal = {Expert Syst.},
  title        = {A critical analysis of generative adversarial networks in anomaly detection for time series},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nested named entity recognition: A survey of latest research. <em>EXSY</em>, <em>42</em>(7), e70052. (<a href='https://doi.org/10.1111/exsy.70052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on nested named entity recognition (NER) is conducive to providing richer semantic representations and capturing the nested structure among entities, which is crucial for the execution of downstream tasks. This paper aims to summarise the nested NER methods that have been combined with emerging technologies in recent years. We summarise the nested NER methods that are integrated with emerging technologies from three dimensions: model, framework, and data. Additionally, we explore the research progress of nested NER in two scenarios: cross-lingual modality and multi-modal in different modalities. Furthermore, we discuss the practical applications of NER technology in five fields: biomedicine, justice, finance, media, and e-commerce. Through this review, we can clearly see the development trends of nested NER technology under emerging technologies and different modalities, as well as its broad application prospects in various fields. This provides a reference for future exploration directions in nested NER.},
  archive      = {J_EXSY},
  author       = {Lixia Ji and Yiping Dang and Yunlong Du and Wenzhao Gao and Han Zhang},
  doi          = {10.1111/exsy.70052},
  journal      = {Expert Systems},
  month        = {7},
  number       = {7},
  pages        = {e70052},
  shortjournal = {Expert Syst.},
  title        = {Nested named entity recognition: A survey of latest research},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Linear pricing game based power control with resource allocation and interference management in device-to-device communication for IoT applications. <em>EXSY</em>, <em>42</em>(6), e70071. (<a href='https://doi.org/10.1111/exsy.70071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70071},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70071},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Linear pricing game based power control with resource allocation and interference management in device-to-device communication for IoT applications},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approaches to automatic classification, detection and segmentation of breast arterial calcification using deep learning. <em>EXSY</em>, <em>42</em>(6), e70069. (<a href='https://doi.org/10.1111/exsy.70069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Dominic Maguire and John D. Thompson and Sunil Vadera and Katy Szczepura},
  doi          = {10.1111/exsy.70069},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70069},
  shortjournal = {Expert Syst.},
  title        = {Approaches to automatic classification, detection and segmentation of breast arterial calcification using deep learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smartphone sensor-based physiological parameter monitoring: Advances, apps, and discussions. <em>EXSY</em>, <em>42</em>(6), e70068. (<a href='https://doi.org/10.1111/exsy.70068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing prevalence of smartphones and advancements in sensors, smartphone-based solutions for physiological parameter monitoring appear to offer notable advantages over traditional methods, potentially enhancing safety, convenience and efficiency. This paper aims to present a systematic survey of smartphone sensor-based physiological parameter monitoring apps, with particular discussions of gaps between their current functional capabilities and recent advances. We conducted a systematic analysis of relevant apps available on the App Store and Google Play, mainly focusing on four vital signs: heart rate (HR), blood pressure(BP), body temperature (BT) and respiratory rate (RR), as well as oxygen saturation ( ), blood glucose (BG) and haemoglobin (Hb). The analysis revealed that HR measurement apps were the most prevalent, while BP, BT and RR measurement apps were comparatively fewer, and no smartphone sensor-based BG measurement apps were identified. The contact photoplethysmography method is widely adopted by current apps, while non-contact approach holds potential. Novel techniques require further investigation beyond laboratory settings to enhance robustness. Smartphone-based measurement of physiological parameters shows promise, though further research and development are needed to bridge the gap between current capabilities and the demands of accurate, real-world health monitoring.},
  archive      = {J_EXSY},
  author       = {Shuni Li and Mingzhi Wang and Xiong Jiang and Xingyao Li and Jiawei Du and Nan Ji and Junxin Chen},
  doi          = {10.1111/exsy.70068},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70068},
  shortjournal = {Expert Syst.},
  title        = {Smartphone sensor-based physiological parameter monitoring: Advances, apps, and discussions},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards optimal guidance of autonomous swarm drones in dynamic constrained environments. <em>EXSY</em>, <em>42</em>(6), e70067. (<a href='https://doi.org/10.1111/exsy.70067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As autonomous drone swarms become increasingly important for complex missions, there remains a critical need for integrated approaches that can simultaneously handle task allocation and safe navigation in dynamic environments. This paper addresses the challenge of optimally allocating tasks and generating collision-free trajectories for drone swarms operating in obstacle-rich settings. Our proposed Swarm Allocation and Route Generation (SARG) framework integrates optimal task assignment with dynamically feasible trajectory planning, enabling efficient mission completion while ensuring safe navigation through complex 3D workspaces. Using quadrotors as our experimental platform, the framework incorporates both Drone-to-Obstacle and Drone-to-Drone collision avoidance algorithms, alongside a modified path planning algorithm that enhances simultaneous graph search efficiency. Our extensive experiments demonstrate that the SARG framework significantly improves performance over existing approaches. The SARG framework, while maintaining a 100% collision avoidance rate in dense environments, achieves a 21.6% reduction in the computation time of the simultaneous graph searching phase compared to conventional methods, contributing to overall system efficiency. These results establish SARG as a viable solution for real-world autonomous drone swarm applications in complex, dynamic settings. Supporting Information, including animated simulations, are available at https://youtu.be/56oabPTUz4g .},
  archive      = {J_EXSY},
  author       = {Yunes Alqudsi and Murat Makaraci},
  doi          = {10.1111/exsy.70067},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70067},
  shortjournal = {Expert Syst.},
  title        = {Towards optimal guidance of autonomous swarm drones in dynamic constrained environments},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guide in designing an asynchronous performance-centric framework for heterogeneous microservices in time-critical cybersecurity applications. the BIECO use case. <em>EXSY</em>, <em>42</em>(6), e70064. (<a href='https://doi.org/10.1111/exsy.70064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the architecture, design and validation of a microservice orchestration approach that improves the flexibility of heterogeneous microservice-based platforms. Improving user experience and interaction for time-critical applications are aspects that were primary objectives for the design of the architecture. Each microservice can provide its own embedded user interface component, also decentralising it and, in consequence, improving the loosely coupled approach to the architecture. Obtained results are promising, with high throughput and low response times. Also, a key finding was the introduction of benchmarking as a new step in the development lifecycle of performance-critical software components, with an example of how it can be applied within an Agile methodology. Further research is proposed to improve the results and raise the final technology readiness level of the system. Obtained results already make the approach a candidate and viable alternative to classical service composers.},
  archive      = {J_EXSY},
  author       = {Daniela Delinschi and Rudolf Erdei and Emil Marian Pasca and Iulia Bărăian and Oliviu Matei},
  doi          = {10.1111/exsy.70064},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70064},
  shortjournal = {Expert Syst.},
  title        = {Guide in designing an asynchronous performance-centric framework for heterogeneous microservices in time-critical cybersecurity applications. the BIECO use case},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SoK: Federated learning and unlearning for medical image analysis. <em>EXSY</em>, <em>42</em>(6), e70063. (<a href='https://doi.org/10.1111/exsy.70063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image analysis is a critical component of modern healthcare, enabling accurate disease diagnosis and effective patient treatment. However, the process is fraught with challenges, including inter- and intra-observer variability, time constraints, and data-related issues such as privacy, heterogeneity and accessibility. Within this framework, Federated Learning (FL) has emerged as a promising solution, allowing collaborative model training across distributed healthcare entities without sharing sensitive patient data. This study provides a comprehensive Systematization of Knowledge (SoK) review of FL and its extension, Federated Unlearning (FU), within the context of medical image analysis. FL enables privacy-preserving, decentralised model training, while FU addresses the ‘Right To Be Forgotten’, ensuring compliance with data protection regulations like GDPR and HIPAA. We explore the opportunities and challenges of FL and FU, detailing their methodologies, frameworks, datasets, and evaluation metrics. The review highlights the potential of FL and FU to enhance diagnostic accuracy, improve patient care, and foster trust in AI-driven healthcare systems. We also identify research gaps and propose future directions for advancing FL and FU in medical imaging, emphasising the need for interdisciplinary collaboration and the development of dedicated frameworks. Thus, this study aims to bridge the gap between theoretical advancements and practical applications, paving the way for more robust and privacy-compliant AI models in healthcare.},
  archive      = {J_EXSY},
  author       = {Khaoula ElBedoui and Walid Barhoumi and Jungwon Cho},
  doi          = {10.1111/exsy.70063},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70063},
  shortjournal = {Expert Syst.},
  title        = {SoK: Federated learning and unlearning for medical image analysis},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the robustness of transformer-based models to different linguistic perturbations: A case of study in irony detection. <em>EXSY</em>, <em>42</em>(6), e70062. (<a href='https://doi.org/10.1111/exsy.70062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the robustness of Transformer models in irony detection addressing various textual perturbations, revealing potential biases in training data concerning ironic and non-ironic classes. The perturbations involve three distinct approaches, each progressively increasing in complexity. The first approach is word masking, which employs wild-card characters or utilises BERT-specific masking through the mask token provided by BERT models. The second approach is word substitution, replacing the bias word with a contextually appropriate alternative. Lastly, paraphrasing generates a new phrase while preserving the original semantic meaning. We leverage Large Language Models (GPT 3.5 Turbo) and human inspection to ensure linguistic correctness and contextual coherence for word substitutions and paraphrasing. The results indicate that models are susceptible to these perturbations, and paraphrasing and word substitution demonstrate the most significant impact on model predictions. The irony class appears to be particularly challenging for models when subjected to these perturbations. The SHAP and LIME methods are used to correlate variations in attribution scores with prediction errors. A notable difference in the Total Variation of attribution scores is observed between original examples and cases involving bias word substitution or masking. Among the corpora used, TwSemEval2018 emerges as the most challenging. Regarding model performance, Transformer-based models such as RoBERTa and BERTweet demonstrate superior overall performance addressing these perturbations. This research contributes to understanding the robustness and limitations of irony detection models, highlighting areas for improvement in model design and training data curation.},
  archive      = {J_EXSY},
  author       = {Reynier Ortega-Bueno and Elisabetta Fersini and Paolo Rosso},
  doi          = {10.1111/exsy.70062},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70062},
  shortjournal = {Expert Syst.},
  title        = {On the robustness of transformer-based models to different linguistic perturbations: A case of study in irony detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parameter control strategy for parallel island-based metaheuristics. <em>EXSY</em>, <em>42</em>(6), e70061. (<a href='https://doi.org/10.1111/exsy.70061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of optimisation, the accurate configuration of parameters in metaheuristic algorithms is a critical yet often arduous task that significantly impacts the efficiency and efficacy of the search process. This study was motivated by the need to address the inefficiencies and limitations associated with conventional methods of parameter configuration, which typically involve manual, trial-and-error approaches. These traditional methods can lead to suboptimal performance and increased computational overhead. To tackle these challenges, this study introduces a novel adaptive parameter control strategy for parallel island-based metaheuristics, with a particular emphasis on the ant colony optimisation (ACO) algorithm. Our research process involved extensive experimentation to evaluate the effectiveness of this adaptive strategy. We conducted a series of tests to enable real-time adjustment of key parameters based on the performance of ACO colonies, thereby enhancing both exploration and exploitation capabilities. The results indicate that the adaptive strategy consistently outperforms offline manual and automated tuning configurations, particularly in larger and more complex problem instances, providing a more efficient solution for parameter optimisation in metaheuristics. These findings highlight the potential of dynamic parameter control to reduce dependency on expert knowledge and manual tuning while improving algorithmic performance.},
  archive      = {J_EXSY},
  author       = {Roberto Prado-Rodríguez and Patricia González and Julio R. Banga},
  doi          = {10.1111/exsy.70061},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70061},
  shortjournal = {Expert Syst.},
  title        = {A parameter control strategy for parallel island-based metaheuristics},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From AI to the era of explainable AI in healthcare 5.0: Current state and future outlook. <em>EXSY</em>, <em>42</em>(6), e70060. (<a href='https://doi.org/10.1111/exsy.70060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) and explainable artificial intelligence (XAI) are advancing rapidly, with the potential to deliver significant benefits to modern society. The healthcare sector, in particular, has experienced transformative changes; overall, these technologies are helping to address numerous challenges, such as cancer cell detection, tumour zone identification in animal bodies, predictions of major and minor diseases, diagnosis, and more. This article provides an in-depth and detailed overview of AI and XAI, focusing on recent trends and their implications for advancing Healthcare 5.0 applications. Initially, the study examines the key concepts and exceptional features of AI, XAI, and Healthcare 5.0. Additional emphasis is placed on state-of-the-art practices currently being implemented in healthcare, particularly those involving AI and XAI. Subsequently, it establishes a coherent link between AI and XAI in Healthcare 5.0, grounded in contemporary advancements. Based on the findings, algorithms are recommended to address initial obstacles to integrating AI into the Healthcare 5.0 framework. Proposals for further enhancing Healthcare 5.0 performance through the integration of XAI and its unique features are discussed in detail. The work also provides in-depth implementation strategies and highlights model-specific trends within AI and XAI frameworks in Healthcare 5.0. Particular attention is given to AI model predictions in healthcare settings, emphasising their contributions to improved patient feedback and the delivery of more sophisticated care. Most importantly, this research highlights the potential for AI and XAI to support sustainable advancements in Healthcare 5.0 applications. Finally, significant issues are analysed, and an open discussion is presented on future guidelines for the blending of AI with XAI, and Healthcare 5.0 applications.},
  archive      = {J_EXSY},
  author       = {Anichur Rahman and Dipanjali Kundu and Tanoy Debnath and Muaz Rahman and Utpol Kanti Das and Abu Saleh Musa Miah and Ghulam Muhammad},
  doi          = {10.1111/exsy.70060},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70060},
  shortjournal = {Expert Syst.},
  title        = {From AI to the era of explainable AI in healthcare 5.0: Current state and future outlook},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight plant disease detection with adaptive multi-scale model and relationship-based knowledge distillation. <em>EXSY</em>, <em>42</em>(6), e70059. (<a href='https://doi.org/10.1111/exsy.70059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant disease detection is able to control disease spread and help prevent significant food production losses. However, existing detection methods are still limited to different target scales and high model parameters. To this end, we develop a novel framework, that is, FPDD-Net, for lightweight plant disease detection. It is based on YOLOv8 with an adaptive multi-scale model (AMSM) and relationship-based knowledge distillation (RKD). More specifically, the original cross stage partial (CSP) bottleneck is replaced by an AMSM to effectively fuse the multi-scale features. Next, an Alpha-IoU loss optimization is adopted for aligning predicted boxes more precisely with ground truth, leading to fewer localization errors. Finally, RKD is introduced to assist the training and further improve the performance of target detection. To evaluate our network, the FPDD-Net is trained and tested on two typical datasets, that is, the plant village dataset and the plant-doc dataset. Experimental results indicated that our FPDD-Net is lightweight and has advantages over peer methods.},
  archive      = {J_EXSY},
  author       = {Wei Li and Xu Xu and Wei Wang and Junxin Chen},
  doi          = {10.1111/exsy.70059},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70059},
  shortjournal = {Expert Syst.},
  title        = {Lightweight plant disease detection with adaptive multi-scale model and relationship-based knowledge distillation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous graph distillation for stance prediction. <em>EXSY</em>, <em>42</em>(6), e70058. (<a href='https://doi.org/10.1111/exsy.70058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stance prediction is a critical task in public opinion analysis, aiming to identify users' viewpoints on specific events. Existing research often relies on user interactions for stance inference but generally underutilizes multi-source heterogeneous information such as user entities, opinion text, issues and topics. To address this limitation, this study proposes a stance prediction approach based on heterogeneous entity modeling. By integrating four types of heterogeneous entities to capture similarity in users' participation in issues, the proposed method improves stance inference accuracy. Specifically, we design a heterogeneous graph knowledge extraction framework that fully incorporates both content features and structural semantic information of various entities. First, we construct a heterogeneous information network to capture different types of social media entities and their interactions, learning rich feature representations in the process. Next, we employ matrix factorization to assess users' preferences toward specific issues. Finally, by introducing a knowledge distillation mechanism, the approach significantly enhances prediction accuracy with only a modest increase in computational cost. Experimental results on public datasets demonstrate that our method outperforms existing baselines, verifying its effectiveness.},
  archive      = {J_EXSY},
  author       = {Yibing Lu and Jingyun Sun and Yang Li},
  doi          = {10.1111/exsy.70058},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70058},
  shortjournal = {Expert Syst.},
  title        = {Heterogeneous graph distillation for stance prediction},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customs commodity classification method based on the fusion of text sequence and graph information. <em>EXSY</em>, <em>42</em>(6), e70057. (<a href='https://doi.org/10.1111/exsy.70057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today's prevalent international trade, the customs clearance and flow of massive import and export commodities bring enormous audit and regulatory pressure to ports of entry. With the rise of artificial intelligence, many researchers have explored deep learning technology to assist import and export commodity classification and audit. However, the text of the commodity declaration needs to be structured and arranged according to the customs audit rules, resulting in its lack of continuous context, and the elements in the text present complex joint discriminative relationships; it is difficult for existing algorithms to classify commodities accurately based on the unprocessed commodity declaration text. In order to solve the above problems, this paper proposes a fusing text sequence and graph information (FTSGI) neural network. The model comprises the following components: (a) The sequence learning module identifies sequential features and filters out irrelevant details. (b) The key element identification mechanism (KEIM) distinguishes between ordinary and key declaration elements. (c) The graph learning module introduces graph features by modeling the relationships between crucial declaration elements, capturing the interdependencies between textual elements. Compared to other models that have achieved state-of-the-art performance on text classification tasks, FTSGI demonstrates superior performance on real customs datasets.},
  archive      = {J_EXSY},
  author       = {Haichao Sun and Chengjie Zhou and Chao Che},
  doi          = {10.1111/exsy.70057},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70057},
  shortjournal = {Expert Syst.},
  title        = {Customs commodity classification method based on the fusion of text sequence and graph information},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using deep neural networks architectures to identify narcissistic personality traits. <em>EXSY</em>, <em>42</em>(6), e70056. (<a href='https://doi.org/10.1111/exsy.70056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personality is the characteristics of a person represented by thoughts, feelings and behaviours in a certain way. Knowing the personality characteristics of an individual can help improve interpersonal relationships, regardless of their type. Virtual media of social interaction is a rich source of information where online users share and post comments, and express their feelings of likes or dislikes. This information reveals traits about the personality and behaviour of users. In this sense, it is possible to identify personality traits of the dark triad through computational models. In this area, research has found correlations between personality traits and users' online behaviour. In this study, we propose a computational model that uses Neural Network Architectures and Transformer models to identify narcissistic personality traits in Spanish-language text based on the Narcissistic Personality Inventory (NPI) test. Specifically, we leverage the ability of the pre-trained Transformers models BERT, RoBERTa and DistilBERT, to capture the semantic context and structural features of text using sentence-level embeddings. These attributes make them suitable for multi-class classification tasks, such as identifying personality traits from reviews. Furthermore, the model utilises the algorithms Glove, FastText, and Word2Vec to generate embedding, which are used to represent vectors of semantic and syntactic features of words in narcissistic expressions. The semantic information is then used by several neural network architectures—namely SimpleRNN, LSTM, GRU, BiLSTM, CNN + BiLSTM, and CNN + GRU—to construct a multi-class model for automatically identifying narcissistic personality traits. The model's performance is assessed using a Twitter dataset that has been annotated by psychology experts and increased using augmentation techniques such as Back Translation, Paraphrasing, and substituting words with their synonyms. Ultimately, the results indicate that BERT and RoBERTa Transformers yield better accuracy and precision compared to Neural Network Architectures.},
  archive      = {J_EXSY},
  author       = {Lidice Haz and Miguel Ángel Rodríguez-García and Alberto Fernández},
  doi          = {10.1111/exsy.70056},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70056},
  shortjournal = {Expert Syst.},
  title        = {Using deep neural networks architectures to identify narcissistic personality traits},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ELWARD: Empowering language model with world insights and human-aligned reward design. <em>EXSY</em>, <em>42</em>(6), e70055. (<a href='https://doi.org/10.1111/exsy.70055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have made significant progress in many tasks, but they may also generate biased or misleading outputs. Alignment techniques address this issue by refining models to reflect human values, but high-quality preference datasets are limited. This study introduces a method to train a high-performance reward model (RM) by integrating open knowledge with human feedback. We construct the Open Knowledge and Human Feedback (OK-HF) dataset, comprising 39.8 million open preference data entries and 30,000 human feedback entries. The dual-stage aligning strategy is proposed to combine preference pre-training with domain adaptation, leveraging multi-objective optimization to enhance learning from both preference data and fine-grained human feedback. The Open Knowledge and Human-feedback Reward Model (OKH-RM), designed with the dual-stage aligning strategy on the OK-HF dataset, demonstrates exceptional performance in aligning LLMs with human preferences. The experimental results show that OKH-RM outperforms Llama2-RM, Qwen-RM and Ultra-RM, particularly achieving an accuracy of 85.93% on the Stanford SHP dataset. The model has shown advanced capabilities in detecting low-quality repetitive responses and mitigating biases related to response length.},
  archive      = {J_EXSY},
  author       = {Yongping Du and Siyuan Li and Rui Yan and Ying Hou and Honggui Han},
  doi          = {10.1111/exsy.70055},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70055},
  shortjournal = {Expert Syst.},
  title        = {ELWARD: Empowering language model with world insights and human-aligned reward design},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing deepfake audio detection: A ResNet framework based on hybrid features and self-attention mechanism. <em>EXSY</em>, <em>42</em>(6), e70054. (<a href='https://doi.org/10.1111/exsy.70054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the successful application of deep learning, audio spoofing detection has made significant progress. Spoofed audio with speech synthesis or voice conversion can be detected by many countermeasures well. However, an automatic speaker verification system is still vulnerable to spoofing attacks such as replay or deepfake audio. Deepfake audio, generated using text-to-speech (TTS) and voice conversion (VC) algorithms, poses a particularly significant challenge. To address this vulnerability, we propose a novel framework incorporating hybrid features and a self-attention mechanism for enhanced spoofing detection. Our approach is distinguished by the following key contributions: (1) A novel dual-path feature extraction architecture, leveraging parallel convolutional neural networks (CNNs) and Short-Time Fourier Transform (STFT) with Mel-frequency filtering to capture complementary deep learning and Mel-spectrogram features, respectively; (2) A max-pooling-based feature fusion strategy, concatenating the extracted features to preserve crucial discriminative information; (3) The integration of a self-attention mechanism to dynamically weight and focus on salient temporal-spectral patterns within the fused feature representation; (4) A ResNet-based classifier, augmented with linear layers, for robust spoofing classification. Rigorous evaluation on the ASVspoof 2021 dataset demonstrates the efficacy of our proposed framework. We achieve state-of-the-art performance, attaining Equal Error Rate (EER) of 9.67% in the physical access (PA) scenario and 8.94% in the deepfake task. These results correspond to substantial relative improvements of 74.60% and 60.05%, respectively, compared to the best-performing baseline systems. These findings underscore the superior discriminative power of our hybrid feature approach, highlighting its ability to capture richer utterance details compared to conventional single-modality feature representations. This work offers a promising new direction for developing robust ASV systems resilient to increasingly sophisticated spoofing attacks.},
  archive      = {J_EXSY},
  author       = {Lian Huang and Jixiang Yang and Jinhong Zhao and Chunxiang Wu},
  doi          = {10.1111/exsy.70054},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70054},
  shortjournal = {Expert Syst.},
  title        = {Enhancing deepfake audio detection: A ResNet framework based on hybrid features and self-attention mechanism},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based privacy preserving multimodal biometrics recognition for cross-silo datasets. <em>EXSY</em>, <em>42</em>(6), e70053. (<a href='https://doi.org/10.1111/exsy.70053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different biometric modalities, such as fingerprints and left and right eye irises, contain physiological characteristics that offer high accuracy in identification processes. These modalities complement each other; for example, fingerprints provide intricate ridge patterns, while irises exhibit stable, precise features that perform well in challenging environments. A new proposed framework based on federated learning with optimised features, pre-trained deep learning models, linear discriminant analysis and dense neural networks ensures privacy protection for multi-modal biometric recognition across diverse biometric datasets. The system obtains better accuracy levels alongside increased robustness through the combination of fingerprint and iris scan technology that functions across independent and identically distributed (IID) and non-independent and non-identically distributed (non-IID) conditions. Privacy protection functions as a key asset of federated learning because it allows distributed training operations through non-raw data sharing, supporting high classification results. The system's performance is enhanced by implementing feature fusion alongside dimensionality reduction methods, which enhance both the efficiency and resistance to noise and variabilities. The system establishes an essential reference point for distributed and heterogeneous real-world biometric recognition because it implements accurate computation with enhanced efficiency together with privacy protection. The IID data experiments demonstrated 98.86% training accuracy while achieving precision and recall at precise levels of 98.86% and 96.59%. All metrics achieved 100% on the validation data set while keeping loss at zero. The system's performance slightly decreased under non-IID training data conditions, which resulted in 95.01% training accuracy and 0.18 training loss. The reported precision levels matched recall values since both measurements reached 97.99% and 95.01%. The system maintained perfect validation results through all metrics, which demonstrated a strong ability to generalise beyond data distribution impediments. The integration of multimodal biometric systems with federated learning enables the optimisation of large-scale solutions because it establishes efficient but accurate and secure applications across domains that include surveillance and security together with healthcare.},
  archive      = {J_EXSY},
  author       = {Isha Kansal and Vikas Khuallar and Gifty Gupta and Deepali Gupta and Sapna Juneja and Ali Nauman and Ghulam Muhammad},
  doi          = {10.1111/exsy.70053},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70053},
  shortjournal = {Expert Syst.},
  title        = {Deep learning-based privacy preserving multimodal biometrics recognition for cross-silo datasets},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing analytic hierarchy process modelling under uncertainty with fine-tuning LLM. <em>EXSY</em>, <em>42</em>(6), e70051. (<a href='https://doi.org/10.1111/exsy.70051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given that decision-making typically encompasses stages such as problem recognition, the generation of alternatives, and the selection of the optimal choice, Large Language Models (LLMs) are progressively being integrated into tasks requiring the enumeration and comparative evaluation of alternatives, thereby promoting more rational decision-making frameworks. Analysing the extent to which LLMs exhibit meaningful performance at each stage of the decision-making process has thus become a critical area of inquiry. In particular, LLMs hold the potential to identify latent relationships within contextual information and data related to the problem domain. This capability enables them to propose novel evaluation criteria or alternatives that may otherwise be overlooked by human designers. This study seeks to advance the modelling and evaluation of the analytical hierarchy process (AHP), a widely utilised multiple criteria decision making (MCDM) method, by leveraging LLMs. To achieve this, a methodology was developed for constructing AHP models using LLMs fine-tuned with domain-specific documents. The performance of the proposed methodology was assessed by evaluating the extent to which its outputs aligned with reference hierarchies and criteria created by human experts under predefined AHP frameworks. Additionally, the study examined the model's efficacy in generating complete AHP hierarchies and criteria in scenarios where these were not predefined. For empirical validation, the proposed methodology was applied to assess and improve the management performance of six-sector agricultural enterprises. Comparative analysis of the LLM-based AHP results with human expert evaluations was conducted to determine the validity and robustness of the approach. The findings provide insights into the potential of LLMs to contribute to structured decision-making and enhance the application of MCDM methods.},
  archive      = {J_EXSY},
  author       = {Haeun Park and Hyunjoo Oh and Feng Gao and Ohbyung Kwon},
  doi          = {10.1111/exsy.70051},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70051},
  shortjournal = {Expert Syst.},
  title        = {Enhancing analytic hierarchy process modelling under uncertainty with fine-tuning LLM},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design, implementation and validation of a level 2 automated driving vehicle reference architecture. <em>EXSY</em>, <em>42</em>(6), e70050. (<a href='https://doi.org/10.1111/exsy.70050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated vehicles represent a rapidly expanding global market, drawing significant attention from both industry and academia. However, existing solutions often lack transparency, particularly in the disclosure of architectural designs, resulting in fragmented development approaches. To address these gaps, this paper introduces a novel, modular reference architecture tailored for Level 2 Automated Driving Systems (ADS). The proposed architecture ensures safety, scalability, and adaptability across diverse vehicle platforms. A comprehensive validation is conducted using OpenPilot, an open-source Level 2 ADS implementation, demonstrating the architecture's practical feasibility in achieving reliable control tasks under real-time constraints. This work bridges the gap between industrial and academic contributions, offering actionable insights and a robust foundation for future advancements in ADS development.},
  archive      = {J_EXSY},
  author       = {Javier Saez-Perez and Julio Diez-Tomillo and David Tena-Gago and Jose M. Alcaraz-Calero and Qi Wang},
  doi          = {10.1111/exsy.70050},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70050},
  shortjournal = {Expert Syst.},
  title        = {Design, implementation and validation of a level 2 automated driving vehicle reference architecture},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A particle swarm optimization-based approach coupled with large language models for prompt optimization. <em>EXSY</em>, <em>42</em>(6), e70049. (<a href='https://doi.org/10.1111/exsy.70049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have been developing rapidly to attract significant attention these days. These models have exhibited remarkable abilities in achieving various natural language processing (NLP) tasks, but the performance depends highly on the quality of prompting. Prompt engineering methods have been promoted for further extending the models' abilities to perform different applications. However, prompt engineering involves crafting input prompts for better accuracy and efficiency, demanding substantial expertise with trial-and-error effort. Automating the prompting process is important and can largely reduce human efforts in building suitable prompts. In this work, we develop a new metaheuristic algorithm to couple the Particle Swarm Optimization (PSO) technique and LLMs for prompt optimization. Our approach has some unique features: it can converge within only a small number of iterations (i.e., typically 10–20 iterations) to vastly reduce the expensive LLM usage cost; it can easily be applied to conduct many kinds of tasks owing to its simplicity and efficiency; and most importantly, it does not need to depend so much on the quality of initial prompts, because it can improve the prompts through learning more effectively based on enormous existing data. To evaluate the proposed approach, we conducted a series of experiments with several types of NLP datasets and compared them to others. The results highlight the importance of coupling metaheuristic search algorithms and LLMs for prompt optimization, proving that the presented approach can be adopted to enhance the performance of LLMs.},
  archive      = {J_EXSY},
  author       = {Po-Cheng Hsieh and Wei-Po Lee},
  doi          = {10.1111/exsy.70049},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70049},
  shortjournal = {Expert Syst.},
  title        = {A particle swarm optimization-based approach coupled with large language models for prompt optimization},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Consumer sentiment analysis with aspect fusion and GAN-BERT aided adversarial learning. <em>EXSY</em>, <em>42</em>(6), e70048. (<a href='https://doi.org/10.1111/exsy.70048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : P. K. Jain , W. Quamer , and R. Pamula , “ Consumer Sentiment Analysis With Aspect Fusion and GAN-BERT Aided Adversarial Learning ,” Expert Systems 40 , no. 4 ( 2023 ): e13247, https://doi.org/10.1111/exsy.13247 . The above article, published online on 14 February 2023 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Experiments and analysis do not support the research objectives sufficiently. Accordingly, the conclusions of this manuscript are considered unreliable.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70048},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70048},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Consumer sentiment analysis with aspect fusion and GAN-BERT aided adversarial learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Investigations on optimization techniques for stabilized clinical images. <em>EXSY</em>, <em>42</em>(6), e70047. (<a href='https://doi.org/10.1111/exsy.70047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : D. R. J. Dolly , J. D. Peter , and D. J. Jagannath , “ Investigations on Optimization Techniques for Stabilized Clinical Images ,” Expert Systems (Early View): https://doi.org/10.1111/exsy.12901 . The above article, published online on 29 November 2021 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Discussion, analysis, and research conducted in this manuscript are insufficiently described. Accordingly, the results are considered insufficiently supported and not reproducible.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.70047},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70047},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Investigations on optimization techniques for stabilized clinical images},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition by multi-scale local binary patterns (MLBP) and convolutional neural network (CNN) features. <em>EXSY</em>, <em>42</em>(6), e70044. (<a href='https://doi.org/10.1111/exsy.70044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of human-computer interactions (HCI) has increased recently because of developments in artificial intelligence (AI) and machine learning methods, but there are still numerous obstacles to overcome. One of these difficulties that has been taken into account by several academics in recent years is the recognition of emotions via the processing of facial pictures. Most of the previously suggested solutions have drawbacks like poor accuracy and restrictions on the amount of emotions detected. On the other hand, researchers need to focus more on identifying the ideal feature set that results in maximum detection accuracy. This work addresses these issues by outlining a novel method for extracting the best face characteristics and their improved categorisation. Pre-processing, feature extraction, feature selection and classification are the four phases of the suggested technique. Image normalisation and face recognition are steps in the pre-processing stage. The ideal features are chosen using a black hole optimisation approach in the proposed method, which combines a Convolutional Neural Network (CNN) and Multi-scale Local Binary Patterns (MLBP) to extract the feature. The next step is to categorise certain characteristics and identify facial emotions in the photos using Error Correcting Output Codes (ECOC). To lessen the issue's complexity, the suggested ECOC model combines a number of Support Vector Machine (SVM) classifiers. Results reveal that the proposed model has average accuracies of 98.9% and 79.82%, respectively, for the Yale and FER-2013 datasets in recognising facial expressions, which shows an increase of at least 1% over the prior approaches.},
  archive      = {J_EXSY},
  author       = {Haoyu Yang and Entesar Gemeay and Abdullah Alqahtani and Abed Alanazi and Shtwai Alsubai and Sangkeum Lee},
  doi          = {10.1111/exsy.70044},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70044},
  shortjournal = {Expert Syst.},
  title        = {Facial expression recognition by multi-scale local binary patterns (MLBP) and convolutional neural network (CNN) features},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a no code deployment of social robotics use cases. <em>EXSY</em>, <em>42</em>(6), e70038. (<a href='https://doi.org/10.1111/exsy.70038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social Autonomous Robotics aims to deploy robots in scenarios that involve intensive and continuous interaction with humans. To control the behaviour of robotic platforms in such environments, the use of automated planning (AP) within a control architecture has been proposed as an effective mechanism. However, the design of AP models is time-consuming and typically carried out by domain experts and engineers. A significant amount of knowledge must be acquired in order to properly define the use case description by specifying the different tasks performed by the robot. In this paper, we present DeVPlan , a framework for graphically designing robotic use cases and configuring the platform for the desired execution. DeVPlan provides an interface that allows domain experts, in collaboration with knowledge engineers, to use state transition diagrams to specify the tasks a robot can perform and define recovery strategies for exogenous events that disrupt normal execution. This graphical design is automatically translated into the standard Planning Domain Definition Language (PDDL). Additionally, to facilitate the integration of the AP model with the robot's control architecture, DeVPlan includes a module for generating the configuration files required to set up the control system. The proposed framework has been successfully used to design and deploy two different use cases in a real environment in a retirement home.},
  archive      = {J_EXSY},
  author       = {Alba Gragera and Carmen Díaz-de-Mera and Juan Pedro Bandera and Ángel García-Olaya and Fernando Fernández},
  doi          = {10.1111/exsy.70038},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e70038},
  shortjournal = {Expert Syst.},
  title        = {Towards a no code deployment of social robotics use cases},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED: Investigations on optimization techniques for stabilized clinical images. <em>EXSY</em>, <em>42</em>(6), e12901. (<a href='https://doi.org/10.1111/exsy.12901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical imaging is the most vital part for precise medical analysis by physicians. Huge number of researcher's explore various possibilities to come up with an optimal solution. Innumerable innovative progressions aligned to medical images are radically growing. This article emphasizes on the various optimization techniques which helps the clinicians to have a clear picture on the region of interest. Unique modalities provide detailed information on a specific area, such sequences are made to stabilize to reduce the artefacts and they are allowed to pass through the processing stage for fusing so that the doctors can envisage the common features of CT and MRI. Further, Registration of the images is initialized to effectively align and overlay for appropriate determination by the physicians. To ensure the quality, essential optimization is performed and the assessments are performed subjectively and objectively. The proposed method is instigated with appropriate optimization procedures and is found that it outperforms the existing approaches subjectively and objectively. The proposed approach is also statistically investigated to prove its predominance. Quantum computation is being extensively used for research in medical applications. The computation complexity becomes less while adopting quantum processing.},
  archive      = {J_EXSY},
  author       = {D. Raveena Judie Dolly and J. Dinesh Peter and D. J. Jagannath},
  doi          = {10.1111/exsy.12901},
  journal      = {Expert Systems},
  month        = {6},
  number       = {6},
  pages        = {e12901},
  shortjournal = {Expert Syst.},
  title        = {RETRACTED: Investigations on optimization techniques for stabilized clinical images},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual indicator ranking method for complexly constrained multi-objective optimization. <em>EXSY</em>, <em>42</em>(5), e70046. (<a href='https://doi.org/10.1111/exsy.70046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing multi-objective optimization problems (MOPs) with complex constraints presents a significant challenge due to their diverse nature. While existing algorithms can effectively handle specific types of complex constraints, they often struggle with a variety of such constraints. To address this issue, we propose an innovative evolutionary algorithm for constrained multi-objective optimization. A key feature is the integration of a novel differential operator that generates offspring based on the presence of feasible solutions within the main population. This strategy is particularly effective for handling complex constraints characterised by small feasible spaces and deceptive infeasible regions. Additionally, the algorithm employs a dual-indicator ranking mechanism to evaluate and select individuals from the auxiliary population based on the quality and quantity of feasible solutions generated by the main population. Promising individuals are then migrated back to the main population, thereby enhancing the exploration of the solution space. This approach demonstrates significant superiority in solving MOPs with discontinuous feasible regions or extensive infeasible areas. Empirical comparisons across a range of benchmark problems show that the proposed algorithm outperforms current state-of-the-art methods in evolutionary constrained multi-objective optimization, underscoring its potential as a robust tool for handling MOPs with complex constraints.},
  archive      = {J_EXSY},
  author       = {Qian Zeng and Hai-Lin Liu},
  doi          = {10.1111/exsy.70046},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70046},
  shortjournal = {Expert Syst.},
  title        = {A dual indicator ranking method for complexly constrained multi-objective optimization},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated artificial bee colony optimization for cost-sensitive neural networks in multi-class problems. <em>EXSY</em>, <em>42</em>(5), e70045. (<a href='https://doi.org/10.1111/exsy.70045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristics are advanced problem-solving techniques that develop efficient algorithms to address complex challenges, while neural networks are algorithms inspired by the structure and function of the human brain. Combining these approaches enables the resolution of complex optimization problems that traditional methods struggle to solve. This study presents a novel approach integrating the ABC algorithm with ANNs for weight optimization. The method is further enhanced by vectorization and parallelization techniques on both CPU and GPU to improve computational efficiency. Additionally, this study introduces a cost-sensitive fitness function tailored for multi-class classification to optimize results by considering relationships between target class levels. It validates these advancements in two critical applications: network intrusion detection and earthquake damage estimation. Notably, this study makes a significant contribution to earthquake damage assessment by leveraging machine learning algorithms and metaheuristics to enhance predictive models and decision-making in disaster response. By addressing the dynamic nature of earthquake damage, this research fills a critical gap in existing models and broadens the understanding of how machine learning and metaheuristics can improve disaster response strategies. In both domains, the ABC-ANN implementation yields promising results, particularly in earthquake damage estimation, where the cost-sensitive approach demonstrates satisfactory outcomes in macro-F1 and accuracy. The best results for macro-F1, weighted-F1, and overall accuracy provides best results with the UNSW-NB15 and earthquake datasets, showing values of 64%, 72%, 68%, and 60%, 80%, and 79%, respectively. Comparative performance evaluations reveal that the proposed parallel ABC-ANN model, incorporating the novel cost-sensitive fitness function and enhanced by vectorization and parallelization techniques, significantly reduces training time and outperforms state-of-the-art methods in terms of macro-F1 and accuracy in both network intrusion detection and earthquake damage estimation.},
  archive      = {J_EXSY},
  author       = {Hilal Hacilar and Bilge Kagan Dedeturk and Mihrimah Ozmen and Mehlika Eraslan Celik and Vehbi Cagri Gungor},
  doi          = {10.1111/exsy.70045},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70045},
  shortjournal = {Expert Syst.},
  title        = {Accelerated artificial bee colony optimization for cost-sensitive neural networks in multi-class problems},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HiSum: Hierarchical topic-driven approach for role-oriented dialogue summarisation. <em>EXSY</em>, <em>42</em>(5), e70043. (<a href='https://doi.org/10.1111/exsy.70043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the volume of information on online communication platforms continues to grow, the task of dialogue summarisation becomes increasingly critical for understanding and extracting key information from diverse conversations. Traditional approaches often struggle to cope with the dynamic nature of dialogues, such as managing perspectives from multiple speakers and seamlessly transitioning between different topics. We propose a novel hierarchical topic-driven approach to generate role-oriented dialogue summarisation (HiSum) to address these challenges. First, we utilise VarGMM clustering technology for in-depth topic segmentation, which enables the model to capture the key topics in a dialogue. Second, we employ a LayerAttn hierarchical attention mechanism to dynamically adjust the focus of dialogue content based on participants' importance and the topics' relevance. Experimental results on three public dialogue summarisation data sets (CSDS, MC and SAMSUM) demonstrate that our method significantly outperforms most existing strong baseline methods across various evaluation metrics and surpasses the current state-of-the-art methods in certain metrics. Detailed analysis demonstrates that HiSum can perform more precise topic segmentation and effectively identify critical information. Our code is publicly available at: https://github.com/kjin0119/HiSum .},
  archive      = {J_EXSY},
  author       = {Keyan Jin and Yapeng Wang and Xu Yang and Sio Kei Im},
  doi          = {10.1111/exsy.70043},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70043},
  shortjournal = {Expert Syst.},
  title        = {HiSum: Hierarchical topic-driven approach for role-oriented dialogue summarisation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved grid clustering algorithm for geographic data mining. <em>EXSY</em>, <em>42</em>(5), e70042. (<a href='https://doi.org/10.1111/exsy.70042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid clustering is a classical clustering algorithm with the advantage of lower time complexity, which is suitable for the analysis of large geographic data. However, it is sensitive to the grid division parameter M and density threshold R , and the clustering accuracy is poor. The article proposes a hybrid clustering algorithm HCA-BGP based on grid and division. the algorithm first uses grid clustering to obtain the core part of the class family, and then uses the division-based method to obtain the edge part of the class family. Through experiments on simulated datasets and real geographic datasets, it is proved to have better results than the existing grid clustering as well as some other classical algorithms. In terms of clustering accuracy, compared with the classical grid clustering algorithm Clique, the clustering F-value of this paper's algorithm is improved by 20.3% on dataset S1, 81.8% on dataset R15, and 7.6% on average on the eight geographic datasets. In terms of the sensitivity of parameters M and R , compared with Clique, the variance of the clustered F-value of this paper's algorithm is reduced by 89.3% on dataset S1; the variance of the clustered ARI is reduced by 99.9% on the real geographic dataset Data8. Compared to another grid-based clustering algorithm, GDB, HCA-BGP also demonstrates significant advantages.},
  archive      = {J_EXSY},
  author       = {Honglei He},
  doi          = {10.1111/exsy.70042},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70042},
  shortjournal = {Expert Syst.},
  title        = {An improved grid clustering algorithm for geographic data mining},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A BERT-based multi-embedding fusion method using review text for recommendation. <em>EXSY</em>, <em>42</em>(5), e70041. (<a href='https://doi.org/10.1111/exsy.70041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering is a widely used method in recommender systems research. However, contrary to the assumption that it relies solely on rating data, many contemporary models incorporate review information to address issues such as data sparsity. Although previous recommender systems utilised review texts to capture user preferences and item features, they often rely on a single-embedding model to represent these features, which may limit the richness of the extracted information. Recent advancements suggest that combining multiple pre-trained embedding models can enhance text representation by leveraging the strengths of different encoding methods. In this study, we propose a novel recommender system model, the Multi-embedding Fusion Network for Recommendation (MFNR), which employs a multi-embedding approach to effectively capture and represent user and item features in review texts. Specifically, the proposed model integrates Bidirectional Encoder Representations from Transformers (BERT) and its optimised variant, RoBERTa, both of which are pre-trained transformer-based models designed for natural language understanding. By leveraging their contextual embeddings, our model extracts enriched feature representations from review texts. Extensive experiments conducted on real-world review datasets from Amazon.com and Goodreads.com demonstrate that MFNR significantly outperforms existing baseline models, achieving an average improvement of 9.18% in RMSE and 14.81% in MAE. These results highlight the efficacy of the multi-embedding approach, indicating its potential for broader application in complex recommendation scenarios.},
  archive      = {J_EXSY},
  author       = {Haebin Lim and Qinglong Li and Sigeon Yang and Jaekyeong Kim},
  doi          = {10.1111/exsy.70041},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70041},
  shortjournal = {Expert Syst.},
  title        = {A BERT-based multi-embedding fusion method using review text for recommendation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EFNet: An effective facial expression recognition network for infants. <em>EXSY</em>, <em>42</em>(5), e70040. (<a href='https://doi.org/10.1111/exsy.70040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression plays a crucial role during interactions with people. Previous studies on facial expression recognition (FER) have mainly focused on adults, while there are few studies on FER for infants. Due to the apparent differences in facial proportions and facial contours between infants and adults, the FER studies for infants could not be conducted on existing expression datasets. In order to study infant facial expressions in-depth, we create the infant facial expression recognition (IFER) dataset by collecting 10,240 infant images. Since infants' faces have smooth facial lines and weak sharpness, the inter-class similarity of facial expressions is higher than adults, and the existing networks for facial expression recognition lack attention to inter-class similarity. To address the above problems, we propose an effective infant facial expression recognition network named EFNet. In the first stage, the convolutional neural network (CNN) branch and the self-attention branch extract the overall features of infants' faces. In the second stage, we propose the self-adaptive attentional centre loss (SACL). The SACL uses the extracted feature maps as contexts to estimate the weights by an attention mechanism and then applies the attentional weights to guide the centre loss. Overall, the SACL facilitates inter-class separateness and intra-class compressiveness of related information in an embedding space. The state-of-the-art results on the IFER dataset confirm the remarkable effectiveness of the EFNet.},
  archive      = {J_EXSY},
  author       = {Lei Geng and Tingting Qi and Zhitao Xiao and Yuelong Li and Wei Wang and Mei Wei},
  doi          = {10.1111/exsy.70040},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70040},
  shortjournal = {Expert Syst.},
  title        = {EFNet: An effective facial expression recognition network for infants},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CancerFusionPrompt: A novel framework for multimodal cancer subtype classification using vision-language model. <em>EXSY</em>, <em>42</em>(5), e70039. (<a href='https://doi.org/10.1111/exsy.70039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Ruonan Liu and Muhammad Ayoub and Junaid Abdul Wahid},
  doi          = {10.1111/exsy.70039},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70039},
  shortjournal = {Expert Syst.},
  title        = {CancerFusionPrompt: A novel framework for multimodal cancer subtype classification using vision-language model},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel continual learning and adaptive sensing state response-based target recognition and long-term tracking framework for smart industrial applications. <em>EXSY</em>, <em>42</em>(5), e70037. (<a href='https://doi.org/10.1111/exsy.70037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Lu Chen and Gun Li and Jie Tan and Yang Li and Shenbing Fu and Haoyuan Ma and Yu Liu and Yuhao Yang and Weizhong Qian and Qinsheng Zhu and Amir Hussain},
  doi          = {10.1111/exsy.70037},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70037},
  shortjournal = {Expert Syst.},
  title        = {A novel continual learning and adaptive sensing state response-based target recognition and long-term tracking framework for smart industrial applications},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). G-WVDTW: A generalised weighted variance dynamic time warping algorithm for subsequence matching in multivariate time series. <em>EXSY</em>, <em>42</em>(5), e70036. (<a href='https://doi.org/10.1111/exsy.70036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic time warping (DTW) is an algorithm used to measure the similarity between sequences, with widespread applications in domains such as speech recognition, image processing and video synchronisation. However, when matching a shorter multivariate time subsequence to a longer time series containing a similar subsequence, existing DTW variants struggle to accurately determine the matching path. To address this issue, we propose an improved algorithm, generalised weighted variance DTW (G-WVDTW). We extend the DTW algorithm to multivariate time series and introduce a weighted variance-based approach to calculate local distances. This allows the algorithm to better assess the distance between different time points in multivariate time series. Additionally, we modify the algorithm's boundary conditions, enabling it to handle subsequence matching tasks in multivariate time series. We conducted similarity retrieval experiments using public datasets and evaluated the algorithm's performance with the AUC metric, achieving up to a 19% improvement on certain datasets. Furthermore, we performed alignment experiments on industrial data, where we artificially generated aligned sequences and quantitatively assessed the alignment errors, which were lower than those produced by other DTW variants. Finally, we validated the algorithm's superior performance in multivariate time series subsequence matching tasks using a synthetic dataset and showcased its use in motif detection using a wind power generation dataset. The algorithm can be applied in fields such as industrial, meteorological and electrocardiogram (ECG) signal analysis for tasks like time series retrieval, matching and data labelling.},
  archive      = {J_EXSY},
  author       = {Danyang Cao and ZiFeng Lin and Di Liu and Xiaoyuan Chai},
  doi          = {10.1111/exsy.70036},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70036},
  shortjournal = {Expert Syst.},
  title        = {G-WVDTW: A generalised weighted variance dynamic time warping algorithm for subsequence matching in multivariate time series},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommendation of learning resources for MOOCs based on historical sequential behaviours. <em>EXSY</em>, <em>42</em>(5), e70034. (<a href='https://doi.org/10.1111/exsy.70034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning path recommendation is crucial for guiding learners through a series of courses in a logical sequence based on their previous learning experiences. This is particularly important for improving learning outcomes in massive open online courses (MOOCs) for diverse learners. Because both the historical learning courses and recommended learning paths can be represented as sequential patterns (SPs); it is reasonable to approach this problem through SP mining (SPM). In addition to support, we incorporate three factors, that is, course learning days, grades and engagement, to model frequent high-utility SPs (FHUSPs). When recommending a learning path, FHUSPs that align with the target user's learning history and are common among successful learners, while rare among less successful ones, are prioritised. If there are insufficient matching FHUSPs, we address this by recommending additional courses based on the joint competency and complementarity of learners similar to the target learner. Experimental results on a real-world dataset demonstrate that our method provides highly accurate and relevant recommendations.},
  archive      = {J_EXSY},
  author       = {Wei Song and Qihao Zhang and Simon Fong and Tengyue Li},
  doi          = {10.1111/exsy.70034},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70034},
  shortjournal = {Expert Syst.},
  title        = {Recommendation of learning resources for MOOCs based on historical sequential behaviours},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GRIMP: A genetic algorithm for compression-based descriptive pattern mining. <em>EXSY</em>, <em>42</em>(5), e70033. (<a href='https://doi.org/10.1111/exsy.70033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional frequent pattern mining algorithms often report an overwhelming number of patterns in large datasets, many of which are redundant. To address this issue, Minimum Description Length (MDL)-based methods have been employed, which use data compression to capture a smaller yet significant set of patterns. However, finding a good set of patterns according to MDL involves a very large search space, and current MDL-based techniques often suffer from long runtimes and find suboptimal solutions. To discover better sets of patterns in less time, this paper introduces GRIMP (a Genetic algoRIthm for coMpression-based descriptive Pattern mining), a novel framework that combines a genetic algorithm with MDL-based pattern selection. Multiple genetic algorithm variants are explored within the GRIMP framework, and their effectiveness is compared using a large number of datasets. Experimental results demonstrate that GRIMP consistently outperforms previous methods by achieving higher compression ratios, generating more representative itemsets, and requiring less time. Additionally, the extracted patterns improve downstream classification tasks, highlighting the ability of GRIMP to find more representative patterns within the data.},
  archive      = {J_EXSY},
  author       = {M. Zohaib Nawaz and M. Saqib Nawaz and Philippe Fournier-Viger and Nazha Selmaoui-Folcher},
  doi          = {10.1111/exsy.70033},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70033},
  shortjournal = {Expert Syst.},
  title        = {GRIMP: A genetic algorithm for compression-based descriptive pattern mining},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic speech recognition: Comparisons between convolutional neural networks, hidden markov model and hybrid architecture. <em>EXSY</em>, <em>42</em>(5), e70032. (<a href='https://doi.org/10.1111/exsy.70032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Speech Recognition (ASR) systems have been widely used as a practical method of interaction between humans and devices. They are typically employed to enhance the accessibility of devices and to improve the security of systems, among other purposes. However, the design of speech-based systems imposes many challenges due to their particularities. Currently, the majority of ASR systems is based on the Hidden Markov Model (HMM), and, more recently, on Convolutional Neural Networks (CNN). The present research evaluates the performance of Hidden Markov Model (HMM) and Convolutional Neural Network (CNN) algorithms in speech recognition and proposes a novel hybrid approach that combines both methods. The study assesses various performance metrics, including accuracy, precision, recall, F1-score, response time, and computational cost. The experimental tests show that the integration between HMM and CNN increased the accuracy by 6% and 8% when compared to HMM and CNN isolated, respectively, in accordance with results presented in previous papers. However, the results of the ANOVA test revealed that the difference in question is not statistically significant, and the HMM-only approach still being an interesting option for embedded systems due to its lesser demanded computational effort.},
  archive      = {J_EXSY},
  author       = {Lyndainês Santos and Nícolas de Araújo Moreira and Robson Sampaio and Raizielle Lima and Francisco Carlos Mattos Brito Oliveira},
  doi          = {10.1111/exsy.70032},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70032},
  shortjournal = {Expert Syst.},
  title        = {Automatic speech recognition: Comparisons between convolutional neural networks, hidden markov model and hybrid architecture},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arabic short-text dataset for sentiment analysis of tourism and leisure events. <em>EXSY</em>, <em>42</em>(5), e70030. (<a href='https://doi.org/10.1111/exsy.70030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focus of this study is to present the detailed process of collecting a dataset of Arabic short-text in the tourism context and annotating this dataset for the task of sentiment analysis using an automatic zero-shot labelling technique utilising transformer-based models. This is benchmarked against a baseline manual annotation approach utilising native Arab human annotators. This study also introduces an approach exploiting both manual/handcrafted and automatically generated annotations of the dataset tweets for the task of sentiment analysis as part of a cross-domain approach using a model trained on sarcasm labels and vice versa. The total collected corpus size is 2293 tweets; after annotation, these tweets were labelled in a three-way classification approach as either positive, negative or neutral. We run different experiments to provide benchmark results of Arabic sentiment classification. Comparative results on our dataset show that the highest performing baseline model when utilising manual labels was MARBERT, with an accuracy of up to 87%, which was pre-trained for Arabic on a massive amount of data. It should be noted that this model enhanced its performance additionally after pre-training on a dialectical Arabic and modern standard Arabic corpus. On the other hand, zero-shot automatically generated labels achieved an 84% accuracy rate in predicting sarcasm classes from sentiment labels.},
  archive      = {J_EXSY},
  author       = {Seham Basabain and Ahmed Al-Dubai and Erik Cambria and Khalid Alomar and Amir Hussain},
  doi          = {10.1111/exsy.70030},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70030},
  shortjournal = {Expert Syst.},
  title        = {Arabic short-text dataset for sentiment analysis of tourism and leisure events},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ST-IDS: Spatio-temporal feature-based multi-tier intrusion detection system for artificial intelligence-powered connected autonomous vehicles. <em>EXSY</em>, <em>42</em>(5), e70026. (<a href='https://doi.org/10.1111/exsy.70026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in 3GPP specifications and the extensive deployment of 5G networks have driven significant growth in the Internet of Vehicles (IoVs). This development has led to an increase in Connected and Autonomous Vehicles (CAVs), which provide capabilities such as automated navigation, ADAS, cruise control, and environmentally sustainable transportation in real-time. Additionally, the widespread adoption of CAVs has also escalated vulnerabilities within the IoV ecosystem, exposing it to potential cyberattacks. The integration of various functional interfaces has enlarged its attack surface, thereby increasing the risk of vehicle infiltration. Researchers have proposed various Intrusion Detection Systems (IDS) to address the ongoing risk of vehicle attacks, without applying encryption and related authentication methods for intra-and inter-vehicular communications. However, a significant limitation of many IDSs is their dependency on characteristics specific to a particular category of vehicles, which limits their adaptability. Additionally, current IDSs frequently rely on one-dimensional features such as traffic, time, etc., which limits their capability of detecting attacks in adverse scenarios. Moreover, incorporating machine learning algorithms into IDSs deployed in automated automobiles causes an increase in computational demands. We propose to develop a collaborative IDS specifically designed for cloud-based vehicle environments. We aim to improve our capabilities of identifying intrusion detection and differentiate which are malicious by using multidimensional features. A customised Convolutional Neural Network (CNN), optimised through hyperparameter tuning, is also developed for detecting the malicious vehicles and enhancing the overall IDS. To address the challenge of data diversity, we integrate various vehicular datasets into a unified feature space. This integration allows a single model to efficiently perform multi-classification tasks without frequent adjustments. Our feature space integrates dimensions such as traffic, time and so forth, levels, thereby expanding the spectrum of detectable attack scenarios. By identifying abnormal data points within this comprehensive feature framework, our system effectively identifies intrusions across a diverse range of vehicle types. As a result, our methodology supports robust intrusion detection through comprehensive multiclass vehicle classification.},
  archive      = {J_EXSY},
  author       = {Amol Ghanshyam Bhatkar and Shashank Gupta and Priyansh Patel},
  doi          = {10.1111/exsy.70026},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70026},
  shortjournal = {Expert Syst.},
  title        = {ST-IDS: Spatio-temporal feature-based multi-tier intrusion detection system for artificial intelligence-powered connected autonomous vehicles},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on internet of medical things (IoMT): Enabling technologies, security and explainability issues, challenges, and future directions. <em>EXSY</em>, <em>42</em>(5), e70010. (<a href='https://doi.org/10.1111/exsy.70010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Medical Things (IoMT) paradigm refers to the process of collection, transmission and analysis of healthcare data using communication and information systems over the internet. IoMT consist of medical devices that can link to the internet or other networks, including wearables, sensors, monitoring tools and other medical appliances. IoMT data can be utilised to lower costs, increase the effectiveness of healthcare delivery and improve the patient health status. In addition to the potential benefits IoMT may provide, the impact of COVID19 pandemic has also strengthened the desire to collect patient data remotely and pushed a lot of medical professionals to utilise IoMT applications such as telemedicine, telehealth, remote patient monitoring, remote patient diagnostics and distant consultations etc. The expectation is that IoMT market size and the usage will increase dramatically and IoMT will change the conventional healthcare systems significantly in the upcoming years. Motivated with that growth expectation, this study aims to analyse the IoMT, its components, enabling technologies and applications by emphasising the fundamental pillars (sensing, communication, data analytics, and security) essential for developing a reliable, dependable, and secure IoMT ecosystem. Furthermore, this study conducts a detailed analysis of recent major cyberattacks targeting the healthcare industry, evaluating their impact and discussing the key lessons derived from these incidents by employing DOTMLPFI approach. Additionally, this survey offers a concise overview of the emerging technologies that complement IoMT in the development of smart healthcare systems and explores potential future directions within this evolving field.},
  archive      = {J_EXSY},
  author       = {Mert Melih Ozcelik and Ibrahim Kok and Suat Ozdemir},
  doi          = {10.1111/exsy.70010},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70010},
  shortjournal = {Expert Syst.},
  title        = {A survey on internet of medical things (IoMT): Enabling technologies, security and explainability issues, challenges, and future directions},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRAD: Autonomous decision-making method for UAV based on safety reinforcement learning. <em>EXSY</em>, <em>42</em>(5), e70004. (<a href='https://doi.org/10.1111/exsy.70004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) are increasingly vital across numerous sectors, from logistics and rescue operations to military endeavours and beyond. However, ensuring safety in the decision-making processes surrounding UAV operations in real-world settings has become an urgent and complex challenge. At present, the main methods to minimise the risk of drone decision-making include utilising pre-established control rules, expert prior knowledge and regularisation constraints. However, these methodologies require UAVs to meet demanding prerequisites, including the acquisition of extensive decision-making experience and the establishment of comprehensive rules. Regrettably, these strict requirements often lead to frequent UAV crashes in uncertain environments and subsequent mission failures. In order to tackle these issues, we propose a self-decision-making method for quadcopter UAVs based on safe reinforcement learning. Our method utilises a multilevel cascading feature semantic space for reinforcement learning, integrating depth images, greyscale images, semantic segmentation images and object detection results as inputs. This approach aims to facilitate safe autonomous learning. Moreover, we integrate real offline labelled data to enhance the safety policy. Depending on the varying levels of risk encountered during the UAV's decision-making process, we dynamically select different safety policies. Through this iterative process, the UAV progressively eliminates extreme actions and reverts to the UAV learning policy module. Experimental results indicate that our method not only ensures safe decision-making for UAVs in uncertain environments but also exhibits superior safety decision-making efficacy compared to certain baseline methods.},
  archive      = {J_EXSY},
  author       = {Wenwen Xiao and Xiangfeng Luo and Shaorong Xie},
  doi          = {10.1111/exsy.70004},
  journal      = {Expert Systems},
  month        = {5},
  number       = {5},
  pages        = {e70004},
  shortjournal = {Expert Syst.},
  title        = {SRAD: Autonomous decision-making method for UAV based on safety reinforcement learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalised entropies for decision trees in classification under monotonicity constraints. <em>EXSY</em>, <em>42</em>(4), e70035. (<a href='https://doi.org/10.1111/exsy.70035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several decision-making approaches involve ordinal labelling between feature values and decision outcomes. These issues refer to ordinal classification under monotonicity constraints. Recently, some machine learning approaches have been designed to deal with these kinds of problems. Indeed, numerous experiments have shown that these algorithms are widely used in real-life applications because of their flexibility and efficiency in terms of interpretation and predictions. In this paper, we introduce novel approaches for measuring feature quality and information quantity, called Rényi-Tsallis Monotonic Tree (RTMT), which uses the advantages of Rényi and Tsallis entropies while incorporating monotonicity constraints through an optimisation framework. Moreover, we introduce Mono-CART, a variant of the CART approach adapted for monotonic classification. New decision tree algorithms are designed on the basis of aforementioned entropies while considering the monotonicity constraints within an optimisation system. The experiments conducted using some benchmark datasets demonstrate the superiority of the proposed approaches compared to existing methods.},
  archive      = {J_EXSY},
  author       = {Oumaima Khalaf and Salvador Garcia and Anis Ben Ishak},
  doi          = {10.1111/exsy.70035},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70035},
  shortjournal = {Expert Syst.},
  title        = {Generalised entropies for decision trees in classification under monotonicity constraints},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Helping to choose a robust alternative: A sensitivity analysis and a software tool for multi-criteria decision-making. <em>EXSY</em>, <em>42</em>(4), e70031. (<a href='https://doi.org/10.1111/exsy.70031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicriteria decision-making (MCDM) often involves evaluating or ranking alternatives on multiple attributes, a process that is far from trivial due to flexible preferences and uncertainty in the criteria importance. The recently proposed W eightless, I nterval- B ased A pproach (WIBA) tackles these issues by relying on an ordering of the criteria (according to their relevance) instead of explicit weights and using interval scores to evaluate alternatives. Although originally proposed for selecting solutions of interest in the context of multi-objective and many objective optimization problems, it can be adapted to rank such solutions. However, the robustness of WIBA rankings has not been studied, and sensitivity analysis approaches based on perturbations of the weights cannot be applied. Furthermore, there is no friendly environment for exploring WIBA properties. This paper addresses these gaps by (1) introducing a novel local sensitivity analysis technique to explore how small perturbations in the order of criteria affect rankings, and (2) presenting WIBApp, a freely available visual software tool that implements WIBA features, including the proposed sensitivity analysis. Using a case study on the selection of technical universities, the paper first illustrates WIBA's flexibility and utility in real-world decision scenarios, enabling decision makers to effectively deal with uncertainty and complexity, and second shows how WIBApp simplifies data management, enhances analysis and facilitates comparisons among rankings. By advancing the theoretical foundations of WIBA and providing a practical implementation, this work contributes to providing decision makers with a robust framework for handling multi-criteria problems, enhancing the reliability of rankings and supporting informed decisions.},
  archive      = {J_EXSY},
  author       = {Pavel Novoa-Hernández and David A. Pelta and Carlos Cruz Corona},
  doi          = {10.1111/exsy.70031},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70031},
  shortjournal = {Expert Syst.},
  title        = {Helping to choose a robust alternative: A sensitivity analysis and a software tool for multi-criteria decision-making},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lifecycle models in machine learning development. <em>EXSY</em>, <em>42</em>(4), e70029. (<a href='https://doi.org/10.1111/exsy.70029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) development introduces challenges that traditional software processes often struggle to address. As ML applications grow in complexity and adoption, various lifecycle models have been proposed to address the unique stages of ML development. This study systematically synthesises these models, mapping their stages and activities to provide an understanding of the ML development landscape. The findings highlight research gaps and opportunities, offering insights for advancing academic research and practical implementation.},
  archive      = {J_EXSY},
  author       = {Antonio Crespí and Antoni-Lluís Mesquida and Maria Monserrat and Antonia Mas},
  doi          = {10.1111/exsy.70029},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70029},
  shortjournal = {Expert Syst.},
  title        = {Lifecycle models in machine learning development},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New technologies of artificial intelligence in convergence ICT. <em>EXSY</em>, <em>42</em>(4), e70028. (<a href='https://doi.org/10.1111/exsy.70028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A total of 12 papers were accepted for the special issue on the topic of ‘New Technologies of Artificial Intelligence in Convergence ICT’. Recently, as the convergence of artificial intelligence continues to occur in various fields, various technologies are emerging. In this paper, 12 papers introduce AI utilisation technologies in various fields such as smart city, security, medical, economy, healthcare and electricity.},
  archive      = {J_EXSY},
  author       = {Ji Su Park and Laurence T. Yang and Jong Hyuk Park},
  doi          = {10.1111/exsy.70028},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70028},
  shortjournal = {Expert Syst.},
  title        = {New technologies of artificial intelligence in convergence ICT},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy particle filtering based approach for battery RUL prediction with uncertainty reduction strategies. <em>EXSY</em>, <em>42</em>(4), e70027. (<a href='https://doi.org/10.1111/exsy.70027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a two-stage framework that combines uncertainty reduction and predictive modelling to enhance the accuracy of battery Remaining Useful Life (RUL) prediction. In the first stage, a simplified fuzzy optimization learning model is introduced to mitigate uncertainty caused by abnormal capacity fluctuations in battery data. The proposed fuzzy model reconstructs degradation data into a consistent downward trend based on mid- and short-term tendencies of the battery, alleviating abnormal variability and improving suitability for predictive modelling. In the second stage, uncertainty arising during the recursive prediction process of a standalone Transformer model was mitigated through the integration of a particle filter. This approach dynamically manages prediction errors using particles, effectively controlling cumulative errors and enhancing the stability and reliability of long-term predictions. This methodology can lead to extended battery life and increased operational reliability through accurate RUL prediction. The proposed methodology is validated through experiments using NASA and CALCE battery datasets, demonstrating superior prediction accuracy and stability compared to conventional approaches by systematically reducing uncertainties.},
  archive      = {J_EXSY},
  author       = {Gwanpil Kim and Jason J. Jung and Dong Kyu Kim and Min Koo and Grzegorz J. Nalepa and Slawomir Nowaczyk},
  doi          = {10.1111/exsy.70027},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70027},
  shortjournal = {Expert Syst.},
  title        = {Fuzzy particle filtering based approach for battery RUL prediction with uncertainty reduction strategies},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A strategic data-driven roadmap for enhancing energy security in taiwan under industry 5.0. <em>EXSY</em>, <em>42</em>(4), e70025. (<a href='https://doi.org/10.1111/exsy.70025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy security performs a decisive position in the economic sustainability and societal development. As Taiwan attempts for sustainable expansion, decoupling for energy security is fundamental and requires advanced information technologies and infrastructure application, especially in connection to the Industry 5.0 era. However, the two concepts proxy manifest the multi-dimensional nature with vast literature; there is an absence of a strategic roadmap for the implementation tactics. This study presents a systematic data-driven analysis combining text mining, the fuzzy Delphi method, interpretive structural modelling, fuzzy decision-making trial and evaluation laboratory, and analytic network process to outline a distinct energy security roadmap and unveil Industry 5.0 contributions. There are 22 valid indicators generated and allocated into five aspects. The causal interrelation model and strategic roadmap are obtained. The technological advancement and integration, environmental and climate actions, and public demand and perception are categorised as the causative aspects. The top causal indicators are indicated as climate change mitigation, cyber-physical systems, energy investment, public perception, and supply–demand side technologies. This study enriches the theoretical literature and serves as a valuable practical locus to improve energy security in the Industry 5.0.},
  archive      = {J_EXSY},
  author       = {Tat-Dat Bui and Jiun-Wei Tseng and Anthony S. F. Chiu and Kanchana Sethanan and Ming-Lang Tseng},
  doi          = {10.1111/exsy.70025},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70025},
  shortjournal = {Expert Syst.},
  title        = {A strategic data-driven roadmap for enhancing energy security in taiwan under industry 5.0},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of damage on wind turbine blades using automatic machine learning and pressure coefficient. <em>EXSY</em>, <em>42</em>(4), e70024. (<a href='https://doi.org/10.1111/exsy.70024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind turbine blades (WTB) are critical components of wind energy systems. Operating in harsh environments WTBs face significant challenges, since damage to their leading edge caused by erosion or additive surface roughness can reduce performance, and increase maintenance costs and operational downtime. One approach to detect WTB damage is to use machine learning, but properly designing a predictive system is not trivial. Auto machine learning (AutoML) can be used to simplify the design and implementation of machine learning pipelines. This work presents the first comparison of state-of-the-art AutoML methods, Auto-Sklearn, H2O-DAI and TPOT, to detect erosion and additive roughness in WTBs. The Leading-Edge Erosion Study database is used, which provides measurements of the pressure coefficient along the airfoil under different conditions. This is the first work to combine the pressure coefficient and AutoML systems to detect these types of damage. Results show the viability of using AutoML in this task, with H2O-DAI producing the best results, achieving an accuracy above in many cases. However, statistical analysis shows that a standard classifier can achieve similar performance across all problems considered, based on the Friedman test and the Wilcoxon-Holm post hoc analysis with an significance level. However, AutoML systems perform better as the complexity and difficulty of the problem increases.},
  archive      = {J_EXSY},
  author       = {Javier A. Carmona-Troyo and Leonardo Trujillo and Josué Enríquez-Zárate and Daniel E. Hernandez and Luis A. Cárdenas-Florido},
  doi          = {10.1111/exsy.70024},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70024},
  shortjournal = {Expert Syst.},
  title        = {Classification of damage on wind turbine blades using automatic machine learning and pressure coefficient},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial orca optimiser: Theory and applications for global optimisation problems. <em>EXSY</em>, <em>42</em>(4), e70023. (<a href='https://doi.org/10.1111/exsy.70023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing complexity of real-world engineering optimisation problems, interest in meta-heuristic algorithms is increasing. However, existing meta-heuristic algorithms still suffer from several shortcomings, including a poor balance between global and local search, a tendency to converge toward the centre of the solution space, and susceptibility to getting trapped in local optima. To overcome these shortcomings, a novel meta-heuristic algorithm, called artificial orca optimiser (AOO), is proposed based on the unique behaviours of orcas in nature. Within the framework of AOO, the switching factor, guidance phase, and iterative formulas that do not converge toward the centre of the solution space, are designed to enhance the equilibrium between exploration and exploitation, ensure agents the ability to escape from the local optimum, and comprehensively explore the solution space without being limited to the centre of the solution space, thereby increasing the likelihood of finding the global optimal solution. Qualitative, quantitative, scalability, sensitivity, and practical application analyses of the experimental results demonstrate that AOO overcomes the issue of converging to the centre of the solution space, alleviates the problems of poor balance and susceptibility to the local optimum, and exhibits outstanding optimising performance, fast convergence, great scalability, high robustness, and excellent practicality.},
  archive      = {J_EXSY},
  author       = {Lin Wang and Xuerui Wang and Yingying Pi},
  doi          = {10.1111/exsy.70023},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70023},
  shortjournal = {Expert Syst.},
  title        = {Artificial orca optimiser: Theory and applications for global optimisation problems},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guideline for novel fine-grained sentiment annotation and data curation: A case study. <em>EXSY</em>, <em>42</em>(4), e70022. (<a href='https://doi.org/10.1111/exsy.70022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the rise of the internet, recent years have witnessed the gradual manifestation of commercial values of online reviews. In movie industry, sentiment analysis serves as the foundation for mining user preferences among diverse and multi-layered audiences, providing insight into the market value of movies. As a representative task, aspect-based sentiment analysis (ABSA) aims to analyse and extract fine-grained sentiment elements and their relations in terms of discussed aspects. Relevant studies, particularly in the realm of deep learning research, face challenges due to insufficient annotated data. To alleviate this problem, we propose a guideline for fine-grained sentiment annotations that defines aspect categories, describes the method for annotating aspect sentiment triplets, either simple or complex and designs a scheme to represent hierarchical labels. Based on this, an ABSA dataset tailored for the movie domain is curated by annotating on 1100 Chinese short reviews acquired from Douban. Applicability of both the annotation guideline and curated data is evaluated through inter-annotator consistency and self-consistency checks, and domain adaptation assessment of e-commerce and healthcare cases. Predictive performance of machine learning models on this dataset shed light on possible applications in more fine-grained sentiment analysis in the movie domain, for example, figuring out the aspects from which to stimulate viewership and influence public opinions, thereby providing substantial support for the movie's box office performance. Finally, we extended our fine-grained sentiment annotation guideline to the e-commerce and healthcare. Through empirical experimentation, we demonstrated the universality of these guideline across diverse domains.},
  archive      = {J_EXSY},
  author       = {Wei Dai and Wanqiu Kong and Tao Shang and Jianhong Feng and Jiaji Wu and Tan Qu},
  doi          = {10.1111/exsy.70022},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70022},
  shortjournal = {Expert Syst.},
  title        = {Guideline for novel fine-grained sentiment annotation and data curation: A case study},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on multi-facet fog-computing resource management techniques, trends, applications and future directions. <em>EXSY</em>, <em>42</em>(4), e70019. (<a href='https://doi.org/10.1111/exsy.70019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the recent advancements in high-speed networks, underlying hardware computing resources and resource scheduling algorithms, Cloud computing has emerged as a popular computing paradigm globally providing end-user services such as infrastructure, hardware platforms and application tools. Subsequently, the researchers across various domains have integrated different services to facilitate the end users. However, the real issue faced by the cloud infrastructure is the network latency due to the physical dispersion between clients and cloud data centers. According to an estimate, billions of internet of things (IoT) devices are sharing approximately two exabytes of data daily. Such a huge amount of data can affect network performance if the underlying physical system does not expand up to the required levels, leading to performance degradation. To overcome these issues, a new computing paradigm called Fog Computing has emerged in recent years. In this paper, we discuss the recent developments in fog computing with the integration of real-time Healthcare 5.0 technology. Furthermore, we describe the proposed layered architecture and taxonomy of resource management (RM) techniques in fog computing, which consists of energy awareness, scheduling, reliability and scalability. Besides that, our survey covers the three-tier layered architecture, evaluation metrics, real-time application aspects of fog computing and tools providing the implementation of RM techniques in fog computing. Furthermore, the proposed layered architecture of the standard fog framework and different state-of-the-art techniques for utilising the computing resources of fog networks have been covered in this study. Moreover, we include various sensors to demonstrate the fog data offloading example in healthcare 5.0 applications. We also present a thorough discussion on various current and future real-time applications of fog computing. Finally, open challenges and promising future research directions have been identified and discussed in the area of fog-based real-time applications.},
  archive      = {J_EXSY},
  author       = {Salman Khan and Ibrar Ali Shah and Shabir Ahmad and Javed Ali Khan and Muhammad Shahid Anwar and Khursheed Aurangzeb},
  doi          = {10.1111/exsy.70019},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70019},
  shortjournal = {Expert Syst.},
  title        = {A comprehensive survey on multi-facet fog-computing resource management techniques, trends, applications and future directions},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lionfish search algorithm: A novel nature-inspired metaheuristic. <em>EXSY</em>, <em>42</em>(4), e70016. (<a href='https://doi.org/10.1111/exsy.70016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an innovative optimization algorithm called Lionfish Search (LFS) technique, which is inspired by the visual predator Lionfish, in which it is specifically imitating their hunting tactics. The suggested algorithm considers several parameters that influence the hunting behaviour of lionfish, such as visual acuity, mobility, striking success, and prey swallowing potential. Furthermore, this study examines the influence of the physiological traits of the lionfish and their relationship with environmental factors. The novel search algorithm has shown enhanced performance and efficiency, particularly in scenarios where the integration of visual cues and intricate hunting strategies is vital. The suggested LFS method was evaluated using 20 well-known single-modal and multi-modal mathematical functions to analyse its different characteristics. The LFS method has shown remarkable efficacy in both exploration and exploitation, effectively reducing the likelihood of being trapped in local optima. Additionally, it has a rapid convergence capacity, particularly in the realm of large-scale global optimization. Comparisons were made between the LFS algorithm, and 10 other prominent algorithms mentioned in the literature. The proposed LFS metaheuristic algorithm outperformed the others on almost all of the examined functions, demonstrating a statistically significant advantage. Moreover, the positive results found in three practical optimization situations demonstrate the effectiveness of the LFS in accomplishing problem-solving tasks that have limited and unknown search areas.},
  archive      = {J_EXSY},
  author       = {Saif Mohanad Kadhim and Johnny Koh Siaw Paw and Yaw Chong Tak and Shahad Thamear Abd Al-Latief and Ahmed Alkhayyat and Deepak Gupta},
  doi          = {10.1111/exsy.70016},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70016},
  shortjournal = {Expert Syst.},
  title        = {Lionfish search algorithm: A novel nature-inspired metaheuristic},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Water-energy-carbon nexus within the urban eco-transformation of the beijing-tianjin-hebei region. <em>EXSY</em>, <em>42</em>(4), e70015. (<a href='https://doi.org/10.1111/exsy.70015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by rapid urbanisation, the Beijing-Tianjin-Hebei region (BTH) has experienced a dramatic increase in resource consumption and environmental strain. Investigating the relationships among water, energy and carbon can help balance efficient resource utilisation, environmental conservation and economic growth, while promoting sustainable urban development. This study develops an analytical framework for the water-energy-carbon nexus within the urban eco-transformation. Specifically, this study first illustrates the conceptual model for the interaction among water use, energy consumption and carbon emissions theoretically, then examines the water-energy-carbon dynamics in urbanisation and ecological transition of the BTH region. Furthermore, an empirical analysis was conducted taking Beijing city as the case study area to explore the water-energy-carbon nexus and its decoupling with socio-economic development. Results show rapid urbanisation has significantly increased population and economic scale, exerting substantial pressure on water resources, energy supply and the environment. The study reveals a significant positive interaction between water consumption, electricity consumption and carbon emissions in Beijing, with an inverted U-shaped parabolic relationship between GDP and population. Beijing is expected to decouple economic growth from carbon emissions after 2030 and from water consumption after 2037, reducing resource consumption and carbon emissions while sustaining economic growth. To achieve sustainable development, it is recommended that the Beijing-Tianjin-Hebei region accelerate industrial transformation, enhance water resource efficiency, develop clean energy and improve power system efficiency. This paper provides a theoretical foundation and practical insights for decision-making and facilitates ecological urbanisation in the Beijing-Tianjin-Hebei region.},
  archive      = {J_EXSY},
  author       = {Yifei Wang and Ze Han and Xiangzheng Deng},
  doi          = {10.1111/exsy.70015},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70015},
  shortjournal = {Expert Syst.},
  title        = {Water-energy-carbon nexus within the urban eco-transformation of the beijing-tianjin-hebei region},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of abnormal behaviour detection in crowd for video surveillance: Advances and trends, datasets, opportunities and prospects. <em>EXSY</em>, <em>42</em>(4), e70013. (<a href='https://doi.org/10.1111/exsy.70013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of abnormal behaviours with fast and automatic recognising is significant in crowded areas to provide higher security to the public. The adoption of deep learning and machine learning-based abnormal behaviour detection models enhances the influential detection and real-time security monitoring in crowds. The researchers have remotely evaluated the heart rate based on physiological information to detect abnormal activities in various years. Over the past few years, several progress have been made, and there are still some issues concerning processing time, accuracy, and computational complexity. The developed approaches detects the activities of anomalies like traffic rule violations, riots, fighting, and stampede, in addition, several anomalous entities such as abandoned luggage and weapons at the sensitive place automatically in time. However, the identification of video anomalies methods poses several challenges because of various environmental conditions, the ambiguous nature of the anomaly, lack of proper datasets, and the complex nature of human characteristics. In recent days, there have been only a few devoted surveys associated with deep learning related video anomaly identification as the research domain is in its initial stages. In this review work, the abnormal behaviour analysis models using deep learning are reviewed in depth in for security applications. Based on the traditionally used abnormal behaviour analysis models in crowded scenes, we widely categorised the methods into classification using object tracking, classification using handcrafted extracted features, classification using non-contact heart rate variability and blood pressure, analysing motion patterns from the visual frames, and classification using face images. We also discuss the comparative analysis of the previous methods with respect to datasets, computational infrastructure, and performance measures for both qualitative and quantitative analysis. In addition, the open and trending research challenges are analysed for future research.},
  archive      = {J_EXSY},
  author       = {A. Jency and K. Ramar},
  doi          = {10.1111/exsy.70013},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70013},
  shortjournal = {Expert Syst.},
  title        = {A review of abnormal behaviour detection in crowd for video surveillance: Advances and trends, datasets, opportunities and prospects},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Q-GEV based novel trainable clustering scheme for reducing complexity of data clustering. <em>EXSY</em>, <em>42</em>(4), e70011. (<a href='https://doi.org/10.1111/exsy.70011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new data clustering technique aimed at enhancing the performance of the trainable path-cost algorithm and reducing the computational complexity of data clustering models. The proposed method facilitates the discovery of natural groupings and behaviours, which is crucial for effective coordination in complex environments. It identifies natural groupings within a set of features and detects the best clusters with similar behaviour in the data, overcoming the limitations of traditional state-of-the-art methods. The algorithm utilises a density peak clustering method to determine cluster centers and then extracts features from paths passing through these peak points (centers). These features are used to train the support vector machine (SVM) to predict the labels of other points. The proposed algorithm is enhanced using two key concepts: first, it employs Q-Generalised Extreme Value (Q-GEV) under power normalisation instead of traditional generalised extreme value distributions, thereby increasing modelling flexibility; second, it utilises the random vector functional link (RVFL) network rather than the SVM, which helps avoid overfitting and improves label prediction accuracy. The effectiveness of the proposed clustering algorithm is evaluated through various experiments, including those on UCI benchmark datasets and real-world data, demonstrating significant improvements across multiple performance metrics, including F1 measure, Jaccard index, purity, and accuracy, highlighting its capability in accurately identifying paths between similar clusters. Its average F1 measure, Jaccard index, purity, and accuracy is measured 76.87%, 56.29%, 80.29%, and 79.64%, respectively.},
  archive      = {J_EXSY},
  author       = {Mohamed Abd Elaziz and Esraa Osama Abo Zaid and Mohammed A. A. Al-qaness and Amjad Ali and Ali Kashif Bashir and Ahmed A. Ewees and Yasser D. Al-Otaibi and Ala Al-Fuqaha},
  doi          = {10.1111/exsy.70011},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e70011},
  shortjournal = {Expert Syst.},
  title        = {Q-GEV based novel trainable clustering scheme for reducing complexity of data clustering},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic role labelling: A systematic review of approaches, challenges, and trends for english and indian languages. <em>EXSY</em>, <em>42</em>(4), e13838. (<a href='https://doi.org/10.1111/exsy.13838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This systematic review looks at the advances, trends, and challenges within semantic role labelling (SRL) for both English and Indian languages. SRL stands as a pivotal undertaking in the realm of natural language processing (NLP), entailing the identification of semantic connections between predicates and their corresponding arguments in a given sentence. The synthesis of findings from publicly available NLP repositories in this review sheds light on the progression of SRL methodologies and their use across various linguistic contexts. The investigation examines the distinct hurdles presented by Indian languages, which are characterised by their morphological complexity and syntactic variability, juxtaposed with the more widely studied English language. Furthermore, we perform an analysis of the impact of sophisticated machine learning algorithms, particularly deep learning, on enhancing SRL efficacy across these languages. The review identifies key research gaps and proposes future research pathways to address the complex nature of SRL in multilingual environments. By offering a comprehensive overview of the evolutionary trajectory of SRL research, the primary objective of this article is to contribute to the advancement of more resilient and adaptable NLP systems capable of accommodating a myriad of languages.},
  archive      = {J_EXSY},
  author       = {Kunal Chakma and Sima Datta and Anupam Jamatia and Dwijen Rudrapal},
  doi          = {10.1111/exsy.13838},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13838},
  shortjournal = {Expert Syst.},
  title        = {Semantic role labelling: A systematic review of approaches, challenges, and trends for english and indian languages},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on text summarization: Techniques, challenges, opportunities. <em>EXSY</em>, <em>42</em>(4), e13833. (<a href='https://doi.org/10.1111/exsy.13833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text Summarization (TS) is a technique for condensing lengthy text passages. The objective of text summarization is to make concise and coherent summaries that contain the main ideas from a document. When thinking about a page or watching a video, researchers or readers might imagine an abbreviated version which will just catch important parts only. This paper provides an overview of research work done by different authors on this field. There are numerous machine learning and deep learning-based approaches and methods for implementing text summarization in practice because of several factors like time saving, increased productivity, effective comparative analysis, among others. In this article we explore the concept of text summarization as well as techniques, general framework, applications, evaluation measures within both Indic and Non-Indic scripts. Additionally, the article brings out some related issues between text summarization and other intelligent systems such as script nature datasets architectures latest works, and so forth. Finally, the authors presented the challenges of text summarization, as well as analytical ideas, conclusions, and future directions for text summarization.},
  archive      = {J_EXSY},
  author       = {Kanta Prasad Sharma and Mohd Shukri Ab Yajid and J. Gowrishankar and Rohini Mahajan and Anas Ratib Alsoud and Abhilasha Jadhav and Devendra Singh},
  doi          = {10.1111/exsy.13833},
  journal      = {Expert Systems},
  month        = {4},
  number       = {4},
  pages        = {e13833},
  shortjournal = {Expert Syst.},
  title        = {A systematic review on text summarization: Techniques, challenges, opportunities},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable machine learning for age-at-death estimation from the pubic symphysis. <em>EXSY</em>, <em>42</em>(3), e70021. (<a href='https://doi.org/10.1111/exsy.70021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age-at-death estimation is an arduous task in human identification based on characteristics such as appearance, morphology or ossification patterns in skeletal remains. This process is performed manually, although in recent years there have been several studies that attempt to automate it. One of the most recent approaches involves considering interpretable machine learning methods, obtaining simple and easily understandable models. The ultimate goal is not to fully automate the task but to obtain an accurate model supporting the forensic anthropologists in the age-at-death estimation process. We propose a semi-automatic method for age-at-death estimation based on nine pubic symphysis traits identified from Todd's pioneering method. Genetic programming is used to learn simple mathematical expressions following a symbolic regression process, also developing feature selection. Our method follows a component-scoring approach where the values of the different traits are evaluated by the expert and aggregated by the corresponding mathematical expression to directly estimate the numeric age-at-death value. Oversampling methods are considered to deal with the strongly imbalanced nature of the problem. State-of-the-art performance is achieved thanks to an interpretable model structure that allows us to both validate existing knowledge and extract some new insights in the discipline.},
  archive      = {J_EXSY},
  author       = {Enrique Bermejo and Antonio David Villegas and Javier Irurita and Sergio Damas and Oscar Cordón},
  doi          = {10.1111/exsy.70021},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70021},
  shortjournal = {Expert Syst.},
  title        = {Interpretable machine learning for age-at-death estimation from the pubic symphysis},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI for finance: Applications, case studies and challenges. <em>EXSY</em>, <em>42</em>(3), e70018. (<a href='https://doi.org/10.1111/exsy.70018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative AI (GAI), which has become increasingly popular nowadays, can be considered a brilliant computational machine that can not only assist with simple searching and organising tasks but also possesses the capability to propose new ideas, make decisions on its own and derive better conclusions from complex inputs. Finance comprises various difficult and time-consuming tasks that require significant human effort and are highly prone to errors, such as creating and managing financial documents and reports. Hence, incorporating GAI to simplify processes and make them hassle-free will be consequential. Integrating GAI with finance can open new doors of possibility. With its capacity to enhance decision-making and provide more effective personalised insights, it has the power to optimise financial procedures. In this paper, we address the research gap of the lack of a detailed study exploring the possibilities and advancements of the integration of GAI with finance. We discuss applications that include providing financial consultations to customers, making predictions about the stock market, identifying and addressing fraudulent activities, evaluating risks, and organising unstructured data. We explore real-world examples of GAI, including Finance generative pre-trained transformer (GPT), Bloomberg GPT, and so forth. We look closer at how finance professionals work with AI-integrated systems and tools and how this affects the overall process. We address the challenges presented by comprehensibility, bias, resource demands, and security issues while at the same time emphasising solutions such as GPTs specialised in financial contexts. To the best of our knowledge, this is the first comprehensive paper dealing with GAI for finance.},
  archive      = {J_EXSY},
  author       = {Siva Sai and Keya Arunakar and Vinay Chamola and Amir Hussain and Pranav Bisht and Sanjeev Kumar},
  doi          = {10.1111/exsy.70018},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70018},
  shortjournal = {Expert Syst.},
  title        = {Generative AI for finance: Applications, case studies and challenges},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-driven passenger demand forecasting for autonomous taxi transportation systems in smart cities. <em>EXSY</em>, <em>42</em>(3), e70014. (<a href='https://doi.org/10.1111/exsy.70014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Taxis (ATs) have seen remarkable global proliferation in recent years owing to the widespread adoption and advancements in Artificial Intelligence (AI) across various domains. ATs play a crucial role in Intelligent Transportation Systems (ITS) in smart cities. However, the effectiveness of ITS relies heavily on accurately forecasting the passenger demand for ATs, which poses a significant challenge. Precise prediction of passenger demand is essential for minimising waiting times and unnecessary cruising of ATs in metropolitan areas, which helps conserve energy. To address this issue, this study proposed an adaptive Bayesian Regularisation Backpropagation Neural Network (BRBNN) augmented with a Machine Learning (ML) model to predict passenger demand in different regions of metropolitan cities specifically for ATs. The study conducted extensive simulations using a real-world dataset collected from 4781 taxis in Bangkok, Thailand. Using MATLAB2022b, the proposed model compared various state of art methods and existing research. The results indicate that proposed model outperforms existing methods in terms of performance metrics such as Root Mean Square Error (RMSE) and R -squared ( ) for passenger demand forecasting. These findings validated the effectiveness of the prediction model and its ability to accurately forecast passenger demand for ATs, thereby contributing to the advancement of efficient transportation systems in smart cities.},
  archive      = {J_EXSY},
  author       = {Adeel Munawar and Mongkut Piantanakulchai},
  doi          = {10.1111/exsy.70014},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70014},
  shortjournal = {Expert Syst.},
  title        = {Machine learning-driven passenger demand forecasting for autonomous taxi transportation systems in smart cities},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimising resource use through low-precision feature selection: A performance analysis of logarithmic division and stochastic rounding. <em>EXSY</em>, <em>42</em>(3), e70012. (<a href='https://doi.org/10.1111/exsy.70012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth in the number of wearable devices has increased the amount of data produced daily. Simultaneously, the limitations of such devices has also led to a growing interest in the implementation of machine learning algorithms with low-precision computation. We propose green and efficient modifications of state-of-the-art feature selection methods based on information theory and fixed-point representation. We tested two potential improvements: stochastic rounding to prevent information loss, and logarithmic division to improve computational and energy efficiency. Experiments with several datasets showed comparable results to baseline methods, with minimal information loss in both feature selection and subsequent classification steps. Our low-precision approach proved viable even for complex datasets like microarrays, making it suitable for energy-efficient internet-of-things (IoT) devices. While further investigation into stochastic rounding did not yield significant improvements, the use of logarithmic division for probability approximation showed promising results without compromising classification performance. Our findings offer valuable insights into resource-efficient feature selection that contribute to IoT device performance and sustainability.},
  archive      = {J_EXSY},
  author       = {Samuel Suárez-Marcote and Laura Morán-Fernández and Verónica Bolón-Canedo},
  doi          = {10.1111/exsy.70012},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70012},
  shortjournal = {Expert Syst.},
  title        = {Optimising resource use through low-precision feature selection: A performance analysis of logarithmic division and stochastic rounding},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAM-net: Robust deepfake detection through hybrid attention into scalable convolutional network. <em>EXSY</em>, <em>42</em>(3), e70009. (<a href='https://doi.org/10.1111/exsy.70009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancements in computer vision have transformed data manipulation detection into a significantly challenging task. Deepfakes are advanced manipulation methods for generating highly convincing synthetic media wherein one digitally forges an individual's visuals. Therefore, safeguarding the authenticity and integrity of digital content against such forgeries and developing robust detection methods is essential. Identifying manipulated regions and channels within deepfake images is especially critical in countering these forgeries. Introducing attention features into the classification pipeline enhances the detection of subtle manipulations. Such subtle manipulations are typical of deepfake content. This study presents a novel feature selection approach, a Unified Attention Mechanism into convolutional networks—the ‘UAM-Net’ . The UAM-Net framework concurrently integrates spatial and channel attention features into the data-driven scalable convolutional features. The UAM-Net was trained and evaluated on the DeepFake Detection Challenge Preview (DFDC-P) data set. It was then cross-validated on combined FaceForensics++ and CelebA-DF data sets. UAM-Net has achieved outstanding results, including an accuracy of 98.07%, precision of 97.91%, recall of 98.23%, F1 score of 98.07% and an AUC-ROC score of 99.82%. The UAM-Net model maintained strong performance on the combined data set and achieved 89.7% accuracy, 85.4% precision, 95.8% recall, 90.3% F1 score, and AUC ROC score of 96.8%. The UAM-Net also demonstrated robustness to degraded input quality with 96.98% accuracy and 97% AUC-ROC on the spatially compressed DFDC-P data set. Thus, the model would adapt to real-world conditions, as evidenced by a 97% AUC-ROC on randomly blurred data sets.},
  archive      = {J_EXSY},
  author       = {Kerenalli Sudarshana and Yendapalli Vamsidhar},
  doi          = {10.1111/exsy.70009},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70009},
  shortjournal = {Expert Syst.},
  title        = {UAM-net: Robust deepfake detection through hybrid attention into scalable convolutional network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilising explainable AI to enhance real-time student performance prediction in educational serious games. <em>EXSY</em>, <em>42</em>(3), e70008. (<a href='https://doi.org/10.1111/exsy.70008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, serious games (SGs) have emerged as a powerful tool in education by combining pedagogy and entertainment, facilitating the acquisition of knowledge and skills in engaging environments. SGs enable the collection of valuable interaction data from students, allowing for the analysis of student performance, with artificial intelligence (AI) playing a key role in processing this data to make informed inferences about their knowledge and skills. However, the lack of explainability in AI models represents a significant challenge. This research aims to develop an interpretable model for predicting students' performance in real-time while playing an SG by: (1) calculating the performance of an interpretable prediction model of task completion in an SG and (2) demonstrating the application of the interpretable model for just-in-time (JIT) classroom interventions. Our results show that we are able to predict students' task completion in real-time with a balanced accuracy result of 77.21% after a short playtime has elapsed. In addition, an explainable artificial intelligence (XAI) approach has been applied to ensure the interpretability of the developed models. This approach supports personalised learning experiences, unlocks AI benefits for non-technical users, and maintains transparency in education.},
  archive      = {J_EXSY},
  author       = {Manuel J. Gomez and Álvaro Armada Sánchez and Mariano Albaladejo-González and Félix J. García Clemente and José A. Ruipérez-Valiente},
  doi          = {10.1111/exsy.70008},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70008},
  shortjournal = {Expert Syst.},
  title        = {Utilising explainable AI to enhance real-time student performance prediction in educational serious games},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical data classification using genetic programming: A systematic literature review. <em>EXSY</em>, <em>42</em>(3), e70007. (<a href='https://doi.org/10.1111/exsy.70007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Pratibha Maurya and Arati Kushwaha and Om Prakash},
  doi          = {10.1111/exsy.70007},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70007},
  shortjournal = {Expert Syst.},
  title        = {Medical data classification using genetic programming: A systematic literature review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to fire detection with enhanced target localisation and recognition. <em>EXSY</em>, <em>42</em>(3), e70006. (<a href='https://doi.org/10.1111/exsy.70006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time monitoring of fires is crucial for safeguarding lives and property. However, current fire detection methods still suffer from issues such as redundant feature information, poor network generalisation capabilities and low perception of target location information. To address these challenges, a novel fire detection method called YOLO-FDI has been proposed. This method utilises partial convolution and coordinate convolution with attention mechanisms and Alpha loss at different stages. Specifically, to enhance target localisation accuracy, an attention mechanism is integrated into the model to autonomously focus on fire-affected areas. In terms of feature extraction, partial convolution is employed to reduce computational redundancy and memory access, improving performance and effectively extracting spatial features. During the feature fusion stage, coordinate convolution embeds feature information into coordinate data, further enhancing the coordinate perception capabilities of pixels on the feature map, thereby improving adaptability and accuracy in detecting fire targets. Additionally, the model utilises Alpha loss to enhance flexibility and robustness in fire object detection and recognition. Experimental results demonstrate the effectiveness of the proposed model based on three self-constructed datasets. Compared to the baseline YOLOv7 model, its mAP has improved by 4.5 percentage points, 1.7 percentage points and 2.6 percentage points, respectively. This method demonstrates the capability to accurately represent fire targets and exhibits better stability and reliability in fire target detection, effectively reducing false positives and missed detections.},
  archive      = {J_EXSY},
  author       = {Le Zou and Qiang Sun and Fengling Jiang and Zhize Wu and Lingma Sun and Xiaofeng Wang and Mandar Gogate and Kia Dashtipour and Amir Hussain},
  doi          = {10.1111/exsy.70006},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70006},
  shortjournal = {Expert Syst.},
  title        = {A novel approach to fire detection with enhanced target localisation and recognition},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy decision-making framework for evaluating hybrid detection models of trauma patients. <em>EXSY</em>, <em>42</em>(3), e70005. (<a href='https://doi.org/10.1111/exsy.70005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a new multi-criteria decision-making (MCDM) framework to evaluate trauma injury detection models in intensive care units (ICUs). This research addresses the challenges associated with diverse machine learning (ML) models, inconsistencies, conflicting priorities, and the importance of metrics. The developed methodology consists of three phases: dataset identification and pre-processing, hybrid model development, and an evaluation/benchmarking framework. Through meticulous pre-processing, the dataset is tailored to focus on adult trauma patients. Forty hybrid models were developed by combining eight ML algorithms with four filter-based feature-selection methods and principal component analysis (PCA) as a dimensionality reduction method, and these models were evaluated using seven metrics. The weight coefficients for these metrics are determined using the 2-tuple Linguistic Fermatean Fuzzy-Weighted Zero-Inconsistency (2TLF-FWZIC) method. The Vlsekriterijumska Optimizcija I Kompromisno Resenje (VIKOR) approach is applied to rank the developed models. According to 2TLF-FWZIC, classification accuracy (CA) and precision obtained the highest importance weights of 0.2439 and 0.1805, respectively, while F1, training time, and test time obtained the lowest weights of 0.1055, 0.0886, and 0.1111, respectively. The benchmarking results revealed the following top-performing models: the Gini index with logistic regression (GI-LR), the Gini index with a decision tree (GI_DT), and the information gain with a decision tree (IG_DT), with VIKOR Q score values of 0.016435, 0.023804, and 0.042077, respectively. The proposed MCDM framework is assessed and examined using systematic ranking, sensitivity analysis, validation of the best-selected model using two unseen trauma datasets, and mode explainability using the SHapley Additive exPlanations (SHAP) method. We benchmarked the proposed methodology against three other benchmark studies and achieved a score of 100% across six key areas. The proposed methodology provides several insights into the empirical synthesis of this study. It contributes to advancing medical informatics by enhancing the understanding and selection of trauma injury detection models for ICUs.},
  archive      = {J_EXSY},
  author       = {Rula A. Hamid and Idrees A. Zahid and A. S. Albahri and O. S. Albahri and A. H. Alamoodi and Laith Alzubaidi and Iman Mohamad Sharaf and Shahad Sabbar Joudar and YuanTong Gu and Z. T. Al-qaysi},
  doi          = {10.1111/exsy.70005},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70005},
  shortjournal = {Expert Syst.},
  title        = {Fuzzy decision-making framework for evaluating hybrid detection models of trauma patients},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving paragraph similarity by sentence interaction with BERT. <em>EXSY</em>, <em>42</em>(3), e70003. (<a href='https://doi.org/10.1111/exsy.70003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on semantic similarity between relatively short texts, for example, at word- and sentence-level, has progressed significantly in recent years. However, paragraph-level similarity has not been researched in as much detail owing to the challenges associated with embedding representations, despite its utility in numerous applications. A rudimentary approach to paragraph-level similarity involves treating each paragraph as an elongated sentence, thereby encoding the entire paragraph into a single vector. However, this results in the loss of long-distance dependency information, ignoring interactions between sentences belonging to different paragraphs. In this paper, we propose a simple yet efficient method for estimating paragraph similarity. Given two paragraphs, it first obtains a vector for each sentence by leveraging advanced sentence-embedding techniques. Next, the similarity between each sentence in the first paragraph and the second paragraph is estimated as the maximum cosine similarity value between the sentence and each sentence in the second paragraph. This process is repeated for all sentences in the first paragraph to determine the maximum similarity of each sentence with the second paragraph. Finally, overall paragraph similarity is computed by averaging the maximum cosine similarity values. This method alleviates long-range dependency by embedding sentences individually. In addition, it accounts for sentence-level interactions between the two paragraphs. Experiments conducted on two benchmark data sets demonstrate that the proposed method outperforms the baseline approach that encodes entire paragraphs into single vectors.},
  archive      = {J_EXSY},
  author       = {Xi Jin},
  doi          = {10.1111/exsy.70003},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70003},
  shortjournal = {Expert Syst.},
  title        = {Improving paragraph similarity by sentence interaction with BERT},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature identification using hypotheses of relevance and a 2D-cascade of SEQENS ensembles. <em>EXSY</em>, <em>42</em>(3), e70002. (<a href='https://doi.org/10.1111/exsy.70002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SEQENS is an ensemble method aimed at feature identification that has demonstrated strong performance in identifying relevant genes in high-dimensional spaces, across different synthetic tasks. In this paper, we first introduce the differences between feature importance , feature selection (FS) and feature identification concepts. Following this, we present a framework based on SEQENS covering the following contributions: (1) computing the hypergeometric p- value of the features of a SEQENS output ranking in order to be able to establish a threshold between relevant and non-relevant features; (2) extending SEQENS by introducing the use of preselected features as hypotheses of relevance in the sequential FS, which may help to attract other features that might exhibit weak correlation with the target on their own, but gain relevance when combined with the preselected ones and; (3) designing an automated process based on a 2D-cascade of SEQENS ensembles to obtain a purged feature set , or PFS, that is, having as many relevant features, and as few non-relevant, as possible. The framework presented, named pc–SEQENS, integrates the former techniques so that the PFS is used as a hypothesis of relevance in a SEQENS ensemble. Performance is analysed in a gene expression identification task using the E-MTAB-3732 public database and synthetic targets. pc–SEQENS is compared to other state-of-the-art methods, including SEQENS to check the effect of using hypotheses of relevance. On average, the proposed framework identifies better the relevant genes, especially in unfavourable sample-to-dimension rates, and exhibits a stronger stability.},
  archive      = {J_EXSY},
  author       = {Joaquim Arlandis and Rafael Llobet and J. Ramón Navarro Cerdán and Laura Arnal and François Signol and Juan-Carlos Perez-Cortes},
  doi          = {10.1111/exsy.70002},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70002},
  shortjournal = {Expert Syst.},
  title        = {Feature identification using hypotheses of relevance and a 2D-cascade of SEQENS ensembles},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced electricity forecasting for smart buildings using a TCN-bi-LSTM deep learning model. <em>EXSY</em>, <em>42</em>(3), e70000. (<a href='https://doi.org/10.1111/exsy.70000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integration of sensor technology and advanced software empowers consumers to manage energy usage proactively. This proactive approach yields positive impacts at both micro and macro levels, benefiting individuals and contributing to broader environmental conservation efforts. By leveraging predictive models, consumers can make informed decisions that serve their interests and promote a greener and more sustainable future for all. Thus, energy consumption (EC) prediction is crucial for effective resource management. In this study, we propose an innovative deep-learning approach to predict EC, focusing specifically on smart buildings. Our model utilises a hybrid deep learning architecture to effectively capture low and high information patterns present in multivariate time series data of various sensors deployed in smart buildings and numerous influencing factors. To address the nonlinear and dynamic nature of this data, our model combines a deep neural network (DNN) with a deep learning sequential model (DLS). Specifically, temporal convolutional networks (TCN) within the DNN family are employed to extract various trends from the data, while the DLS model, which consists of Bi-directional Long Short-term Memory Networks (Bi-LSTM), is employed to learn and capture these trends effectively. Consequently, we present a hybrid deep learning framework that leverages for learning multivariate time series data related to EC with shared feature representation. To validate our approach, we extensively evaluate our model using a dataset from an office building in Berkeley, California. Experimental results demonstrate that our model achieves satisfactory accuracy in EC prediction. For the 7-h horizon and on multivariate TS data, an R 2 of 0.97 is realised for the proposed model. This is confirmed by the 1.65% improvement in transiting from univariate to multivariate data, which supports using multiple modalities.},
  archive      = {J_EXSY},
  author       = {Sandeep Kumar Gautam and Vinayak Shrivastava and Sandeep S. Udmale},
  doi          = {10.1111/exsy.70000},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e70000},
  shortjournal = {Expert Syst.},
  title        = {Enhanced electricity forecasting for smart buildings using a TCN-bi-LSTM deep learning model},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). No-reference image quality assessment: Past, present, and future. <em>EXSY</em>, <em>42</em>(3), e13842. (<a href='https://doi.org/10.1111/exsy.13842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {No-reference image quality assessment (NR-IQA) has garnered significant attention due to its critical role in various image processing applications. This survey provides a comprehensive and systematic review of NR-IQA methods, datasets, and challenges, offering new perspectives and insights for the field. Specifically, we propose a novel taxonomy for NR-IQA methods based on distortion scenarios and design principles, which distinguishes this work from previous surveys. Representative methods within each category are thoroughly examined, with a focus on their strengths, limitations, and performance characteristics. Additionally, we review 20 widely used NR-IQA datasets that serve as benchmarks for evaluating these methods, providing detailed information on the number of images, distortion types, and distortion levels for each dataset. Furthermore, we identify and discuss key challenges currently faced by NR-IQA methods, such as handling diverse and complex distortions, ensuring generalisation across datasets and devices, and achieving real-time performance. We also suggest potential future research directions to address these issues. In summary, this survey offers a comprehensive and systematic examination of NR-IQA methods, datasets, and challenges, offering valuable insights and guidance for researchers and practitioners working in the NR-IQA domain.},
  archive      = {J_EXSY},
  author       = {Qingyu Mao and Shuai Liu and Qilei Li and Gwanggil Jeon and Hyunbum Kim and David Camacho},
  doi          = {10.1111/exsy.13842},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13842},
  shortjournal = {Expert Syst.},
  title        = {No-reference image quality assessment: Past, present, and future},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChartNet: Reducing subjectivity in stock prediction through unified technical chart representation. <em>EXSY</em>, <em>42</em>(3), e13841. (<a href='https://doi.org/10.1111/exsy.13841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technical analysis, which includes technical indicators and charts derived from specific rules, has proven effective and widely used for stock movement prediction. However, technical chart evaluation is often limited by subjectivity, arising from sparse chart types and substantial information loss due to rigid rules. While pattern recognition algorithms have been developed to address this issue, they still rely on manual chart labelling and primarily focus on closing prices, leaving much of the chart's broader information untapped. To overcome these limitations, we propose a novel framework called ChartNet, designed to extract general information from technical charts and reduce subjectivity in chart analysis. ChartNet employs a unified representation for charts across financial series with varying simplification levels and leverages a chart triplet loss function for unsupervised training, eliminating the need for labelled data. Compared with several state-of-the-art baselines, our framework has reached the best prediction accuracy on CSI-300, SZ-50 components and Dow Jones Index in 2022: 65.91%, 63.70% and 64.96% respectively. In backtesting using actual stock data, our framework achieves the highest average return of 1.12 and 1.15. Furthermore, we highlight the interpretability of ChartNet through two case studies, some important charts and failure cases, illustrating its capability to uncover meaningful insights from charts. This research contributes to advancing the objective evaluation of technical charts and promoting a more comprehensive understanding of chart-based stock prediction performance.},
  archive      = {J_EXSY},
  author       = {Shangzhe Li and Yingke Liu and Fanglei Cheng and Junran Wu and Ke Xu},
  doi          = {10.1111/exsy.13841},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13841},
  shortjournal = {Expert Syst.},
  title        = {ChartNet: Reducing subjectivity in stock prediction through unified technical chart representation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantification of data imbalance. <em>EXSY</em>, <em>42</em>(3), e13840. (<a href='https://doi.org/10.1111/exsy.13840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel approach to quantify the imbalance in data, addressing a significant gap in the field of regression analysis. Real-world datasets often exhibit an inherent imbalance in their data distribution, which adversely affects learning algorithms such as those used in neural networks. This results in less accurate learning of rare occurrences and a model bias towards more frequent cases, posing challenges in scenarios where rare events are crucial, like energy load prediction. While many solutions exist for classification problems with imbalanced data, regression problems lack adequate research. To address this, we introduce a method to quantify data imbalance by defining it as the disparity between the probability distribution of the data and a relevance-associated distribution. Our approach includes various metrics that can handle multivariate data, allowing for the identification of imbalanced samples and the abstract quantification of imbalance through the mean imbalance ratio. This method facilitates the comparison of regression datasets based on their imbalance, providing insights into dataset quality and evaluating data resampling techniques. We validate our approach using synthetic data and compare it to established metrics such as the Kullback–Leibler divergence and the Wasserstein metric. Furthermore, analysis of real datasets shows a moderate correlation between sample rarity and the approximation error of neural networks, extreme gradient boosting trees and random forests, indicating that underrepresented samples are linked to higher approximation errors.},
  archive      = {J_EXSY},
  author       = {Jelke Wibbeke and Sebastian Rohjans and Andreas Rauh},
  doi          = {10.1111/exsy.13840},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13840},
  shortjournal = {Expert Syst.},
  title        = {Quantification of data imbalance},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling context and content features for fake news detection. <em>EXSY</em>, <em>42</em>(3), e13839. (<a href='https://doi.org/10.1111/exsy.13839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence and rapid development of social networks, an increasing amount of news has been spreading. In addition to the benefits of factual information, there are always risks associated with the dissemination of fake news and preventing the spread of fake news has been a concern for researchers. Many methods have been proposed to detect fake news, but they do not fully extract important information related to news content and context, and rarely consider modelling the simultaneous exploitation of the news context and content in fake news detection. This study proposes a method to improve the performance of fake news detection by modelling features related to news context and content. First, we combine contextualised embeddings (e.g., BERT) and dependency-based embeddings (e.g., dependency-based GCN) to enhance the performance of the content representations of news and reviews posting them. Second, we combine all available review texts related to news belonging to the user. Third, we explore all the reviews that other users had posted about current news by clearly creating review representations posted by the same user about the same news. This leads the model to quickly memorise all reviews related to news from one user. Finally, we model the news content features and the modelled news context features to enhance the richness of the news feature representations. Experimental results on the PolitiFact and GossipCop datasets show improvement to the state-of-the-art method of more than three percentage points in the best case.},
  archive      = {J_EXSY},
  author       = {Huyen Trang Phan and Dosam Hwang and Yeong-Seok Seo and Ngoc Thanh Nguyen},
  doi          = {10.1111/exsy.13839},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13839},
  shortjournal = {Expert Syst.},
  title        = {Modelling context and content features for fake news detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling the differences in early performance prediction between online social sciences and STEM courses using educational data mining. <em>EXSY</em>, <em>42</em>(3), e13837. (<a href='https://doi.org/10.1111/exsy.13837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Educational Data Mining and Learning Analytics in virtual environments can be used to diagnose student performance problems at an early stage. Information that is useful for guiding the decisions of teachers managing academic training, so that students can successfully complete their course. However, student interaction patterns may vary depending on the knowledge domain. Our aim is to design a framework applicable to online Social Sciences and STEM courses, recommending methods for building accurate early performance prediction models. A large-scale comparative study of the accuracy of multiple classifiers applied to classify the interaction logs of 32,593 students from 9 Social Sciences and 13 STEM courses is presented. Corroborating the results of other works, it was observed that high early performance prediction accuracy was obtained based on nothing other than student logs: accuracies of 0.75 in the 10th week, 0.80 in the 20th week, 0.85 in the 30th week and 0.90 in the 40th week. However, accuracy rates were observed to vary significantly, in relation to the classification algorithm and the knowledge domain (Social Sciences vs. STEM). These predictions are generally less accurate for Social Sciences compared to STEM courses, especially at the beginning of the course, with fewer differences observed in the final weeks. Additionally, this research identifies instances of low-accuracy outliers in the prediction of Social Sciences courses over time. These findings highlight the complex challenges and variations in early performance prediction across different domains in online education.},
  archive      = {J_EXSY},
  author       = {Raúl Marticorena-Sánchez and Antonio Canepa-Oneto and Carlos López-Nozal and José A. Barbero-Aparicio},
  doi          = {10.1111/exsy.13837},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13837},
  shortjournal = {Expert Syst.},
  title        = {Unveiling the differences in early performance prediction between online social sciences and STEM courses using educational data mining},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving safety and efficiency of industrial vehicles by bio-inspired algorithms. <em>EXSY</em>, <em>42</em>(3), e13836. (<a href='https://doi.org/10.1111/exsy.13836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of industrial automation, optimising automated guided vehicle (AGV) trajectories is crucial for enhancing operational efficiency and safety. They must travel in crowded work areas and cross narrow corridors with strict safety and time requirements. Bio-inspired optimization algorithms have emerged as a promising approach to deal with complex optimization scenarios. Thus, this paper explores the ability of three novel bio-inspired algorithms: the Bat Algorithm (BA), the Whale Optimization Algorithm (WOA) and the Gazelle Optimization Algorithm (GOA); to optimise the AGV path planning in complex environments. To do it, a new optimization strategy is described: the AGV trajectory is based on clothoid curves and a specialised piece-wise fitness function which prioritises safety and efficiency is designed. Simulation experiments were conducted across different occupancy maps to evaluate the performance of each algorithm. WOA demonstrates faster optimization providing suitable safety solutions 4 times faster than GOA. Meanwhile, GOA gives solutions with better safety metrics but demands more computational time. The study highlights the potential of bio-inspired approaches for AGV trajectory optimisation and suggests avenues for future research, including hybrid algorithm development.},
  archive      = {J_EXSY},
  author       = {Eduardo Bayona and J. Enrique Sierra-García and Matilde Santos Peñas},
  doi          = {10.1111/exsy.13836},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13836},
  shortjournal = {Expert Syst.},
  title        = {Improving safety and efficiency of industrial vehicles by bio-inspired algorithms},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WoodAD: A new dataset and a comparison of deep learning approaches for wood anomaly detection. <em>EXSY</em>, <em>42</em>(3), e13834. (<a href='https://doi.org/10.1111/exsy.13834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a crucial task in computer vision, with applications ranging from quality control to security monitoring, among many others. Recent technological advancements have enabled near-perfect solutions on benchmark datasets like MVTec, raising the need for novel datasets that pose new challenges for this modelling task. This work presents a novel Wood Anomaly Detection (WoodAD) dataset, which includes defects in wooden pieces that result in challenges for the most advanced techniques applied to other established datasets. This article evaluates such challenges posed by WoodAD with one-class and few-shot supervised learning approaches. Our experiments herein reveal that EfficientAD, a state-of-the-art method previously excelling on the MVTec dataset, outperforms all other one-class learning approaches. Nevertheless, there is room for improvement, as EfficientAD achieves a 0.535 pixel/segmentation average precision (AP) over the complete test set. UNet, a well-known pixel-level classification architecture, leveraged few-shot supervised learning to enhance the pixel AP score, achieving 0.862 pixel/segmentation AP over the entire test set. Our WoodAD dataset represents a valuable contribution to the field of anomaly detection, offering complex image textures and challenging defects. Researchers and practitioners are encouraged to leverage this dataset to push the boundaries of anomaly detection and develop more robust and effective solutions for more complex real-world applications. The WoodAD dataset has been made publicly available in Kaggle ( https://www.kaggle.com/datasets/itiresearch/wood-anomaly-detection-one-class-classification ).},
  archive      = {J_EXSY},
  author       = {Omar del-Tejo-Catala and Javier Perez and Nicolas Garcia and Juan-Carlos Perez-Cortes and Javier Del Ser},
  doi          = {10.1111/exsy.13834},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13834},
  shortjournal = {Expert Syst.},
  title        = {WoodAD: A new dataset and a comparison of deep learning approaches for wood anomaly detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phishing website detection: An in-depth investigation of feature selection and deep learning. <em>EXSY</em>, <em>42</em>(3), e13824. (<a href='https://doi.org/10.1111/exsy.13824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud and fog computing technologies benefit from integrating AI-driven phishing detection as it enhances security, scalability, real-time reaction, and privacy. Nowadays, there is a noticeable rise in illegal activity taking place online. One of the illicit cybersecurity practices is phishing, in which hackers trick consumers by pretending to be authentic websites and spoofing them to obtain sensitive user information. Phishing attacks, regrettably, have increased dramatically in recent years, according to research. Machine learning (ML) and deep learning (DL) techniques have shown encouraging progress in thwarting these attacks. Consequently, we employed DL and ML techniques to identify phishing websites in this study. This article presents four scenarios in both ML and DL models. Two are proposed in ML, while the others are employed in DL. The outcomes of four scenarios were contrasted to determine which algorithm performed better at distinguishing between legal and illicit websites. Many popular ML techniques were used, including K-nearest neighbour, random forest (RF), decision trees, and SVMs. PCA and Importance Features are implemented in both ML scenarios to find the best features. RF successfully reached an accuracy of 97.82% using the Importance Feature technique. However, the PCA method failed to improve the performance of ML algorithms. As a result of ML-based scenarios, 98 features are selected for the final deep learning scenarios. In DL-based scenarios, algorithm architectures are essential to avoid overfitting and bias due to various hyperparameters. Thus, in the third scenario, our aim focuses on DL architecture design. Multilayer perceptron and convolutional neural networks (CNNs) are employed to detect phishing websites. Finally, our proposed 1D CNN model, using stratified k-fold cross-validation, outperformed the classical ML algorithm, achieving 98.94% accuracy and 0.99 AUC-ROC score in detecting phishing websites.},
  archive      = {J_EXSY},
  author       = {Soudabe Mousavi and Mahdi Bahaghighat},
  doi          = {10.1111/exsy.13824},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13824},
  shortjournal = {Expert Syst.},
  title        = {Phishing website detection: An in-depth investigation of feature selection and deep learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic code summarization using abbreviation expansion and subword segmentation. <em>EXSY</em>, <em>42</em>(2), e13835. (<a href='https://doi.org/10.1111/exsy.13835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic code summarization refers to generating concise natural language descriptions for code snippets. It is vital for improving the efficiency of program understanding among software developers and maintainers. Despite the impressive strides made by deep learning-based methods, limitations still exist in their ability to understand and model semantic information due to the unique nature of programming languages. We propose two methods to boost code summarization models: context-based abbreviation expansion and unigram language model-based subword segmentation. We use heuristics to expand abbreviations within identifiers, reducing semantic ambiguity and improving the language alignment of code summarization models. Furthermore, we leverage subword segmentation to tokenize code into finer subword sequences, providing more semantic information during training and inference, thereby enhancing program understanding. These methods are model-agnostic and can be readily integrated into existing automatic code summarization approaches. Experiments conducted on two widely used Java code summarization datasets demonstrated the effectiveness of our approach. Specifically, by fusing original and modified code representations into the Transformer model, our Semantic Enhanced Transformer for Code Summarizsation (SETCS) serves as a robust semantic-level baseline. By simply modifying the datasets, our methods achieved performance improvements of up to 7.3%, 10.0%, 6.7%, and 3.2% for representative code summarization models in terms of BLEU-4 , METEOR , ROUGE-L and SIDE , respectively.},
  archive      = {J_EXSY},
  author       = {Yu-Guo Liang and Gui-Sheng Fan and Hui-Qun Yu and Ming-Chen Li and Zi-Jie Huang},
  doi          = {10.1111/exsy.13835},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13835},
  shortjournal = {Expert Syst.},
  title        = {Automatic code summarization using abbreviation expansion and subword segmentation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning algorithms to address the polarity and stigma of mental health disclosures on instagram. <em>EXSY</em>, <em>42</em>(2), e13832. (<a href='https://doi.org/10.1111/exsy.13832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research explores the social response to disclosures and conversations about mental health on social media, which is a pioneering and innovative approach. Unlike previous studies, which focused predominantly on psychopathological aspects, this study explores how communities react to conversations about mental health on Instagram, one of the favourite social media platforms among young people, breaking new ground not only in the Spanish context, but also on a global scale, filling a gap in international research. The study created a novel corpus by collecting and labelling comments on Instagram posts related to celebrity mental health disclosures, categorising them by polarity (positive, negative, neutral) and stigma. Additionally, the research implements machine learning algorithms to detect stigma and polarity in mental health disclosures on Instagram. While traditional techniques like Support Vector Machine (SVM) and RF (Random Forest) displayed decent performance with lower computational loads, advanced deep learning and BERT (Bidirectional Encoder Representation from Transformers) algorithms achieved outstanding results. In fact, BERT models achieve around 96% accuracy in polarity and stigma detection, while deep learning models achieve 80% for polarity and 87% for stigma, very high accuracy metrics. This research contributes significantly to understanding the impact of mental health discussions on social media, offering insights that can reduce stigma and raise awareness. Artificial intelligence can be used for more responsible use of social media and effective management of mental health problems in digital environments.},
  archive      = {J_EXSY},
  author       = {Noemí Merayo and Alba Ayuso-Lanchares and Clara González-Sanguino},
  doi          = {10.1111/exsy.13832},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13832},
  shortjournal = {Expert Syst.},
  title        = {Machine learning algorithms to address the polarity and stigma of mental health disclosures on instagram},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Point class-adaptive transformer (PCaT): A novel approach for efficient point cloud classification and segmentation. <em>EXSY</em>, <em>42</em>(2), e13831. (<a href='https://doi.org/10.1111/exsy.13831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent 3D point cloud classification has predominantly focused on local spatial attention, neglecting distant contextual relationships due to the inherent sparsity of LiDAR-generated data over longer distances. Existing 3D object detection methods prioritize local features, hindering the extraction of semantic information. Despite attempts with transformers, methods often reduce computations through local spatial attention, neglecting content class and scarcely establishing connections among distant global points. Our proposed point class-adaptive transformer (PCaT) addresses these limitations by establishing long-range feature dependencies while significantly reducing computations. PCaT includes three key modules: the class-adaptive transformer (CaT), which utilizes local self-attention and global self-attention based on class similarity to facilitate an efficient trade-off between capturing extended-global dependencies and managing computational challenges; nested binary clustering (NbC), which dynamically partitions queries into multiple clusters based on content features in each Transformer block; and the AfA, which aggregates high-dimensional features using max-pooling alongside a residual MLP component and low-dimensional features using average pooling and a CaT block. Additionally, PCaT incorporates point cloud segmentation via local–global feature aggregation (PcSeg) to facilitate effective point cloud segmentation. Extensive experimentation on the ModelNet40, ScanObjectNN, and S3DIS datasets demonstrates the superior performance and reasonable stability of PCaT compared with existing methods. PCaT achieves 94.2% overall accuracy (OA) and mIoU scores of 89.2% and 86.2% for the ScanObjectNN and S3DIS datasets, respectively.},
  archive      = {J_EXSY},
  author       = {Husnain Mushtaq and Xiaoheng Deng and Ping Jinag and Shaohua Wan and Rawal Javed and Irshad Ullah},
  doi          = {10.1111/exsy.13831},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13831},
  shortjournal = {Expert Syst.},
  title        = {Point class-adaptive transformer (PCaT): A novel approach for efficient point cloud classification and segmentation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybridizing machine learning algorithms with numerical models for accurate wind power forecasting. <em>EXSY</em>, <em>42</em>(2), e13830. (<a href='https://doi.org/10.1111/exsy.13830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate prediction of wind power generation is crucial for optimizing the integration of wind energy into the power grid, ensuring energy reliability. This research focuses on enhancing the accuracy of wind power generation forecasts by combining data from mesoscale and reanalysis models with Machine Learning (ML) approaches. We utilized WRF forecast data alongside ERA5 reanalysis data to estimate wind power generation for a wind farm located at Valladolid, Spain. The study evaluated the performance of ML models based on WRF and ERA5 data individually, as well as a combined model using inputs from both datasets. The hybrid model combining WRF and ERA5 data with ML resulted in a 15% improvement in root mean square error (RMSE) and a 10% increase in compared with standalone models, providing a more reliable 1-h forecast of wind power generation. Additionally, the availability of data over time was addressed: WRF provides the advantage of projecting data into the future, whereas ERA5 offers retrospective data.},
  archive      = {J_EXSY},
  author       = {Álvaro Abad-Santjago and C. Peláez-Rodríguez and J. Pérez-Aracil and J. Sanz-Justo and C. Casanova-Mateo and S. Salcedo-Sanz},
  doi          = {10.1111/exsy.13830},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13830},
  shortjournal = {Expert Syst.},
  title        = {Hybridizing machine learning algorithms with numerical models for accurate wind power forecasting},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-LORA: A vision-language approach for synthetic image detection. <em>EXSY</em>, <em>42</em>(2), e13829. (<a href='https://doi.org/10.1111/exsy.13829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the high challenge in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP)2. Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalisation capabilities to GANs. The experiments show that Bi-LORA outperforms state of the art models in cross-generator tasks because it leverages multi-modal learning, open-world visual knowledge, and benefits from robust, high-level semantic understanding. By combining visual and textual knowledge, it can handle variations in the data distribution (such as those caused by different generators) and maintain strong performance across different domains. Its ability to transfer knowledge, robustly extract features and perform zero-shot learning also contributes to its generalisation capabilities, making it more adaptable to new generators. The experimental results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT .},
  archive      = {J_EXSY},
  author       = {Mamadou Keita and Wassim Hamidouche and Hessen Bougueffa Eutamene and Abdelmalik Taleb-Ahmed and David Camacho and Abdenour Hadid},
  doi          = {10.1111/exsy.13829},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13829},
  shortjournal = {Expert Syst.},
  title        = {Bi-LORA: A vision-language approach for synthetic image detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hyper-tuned multilayer perceptron with effective stochastic learning strategies for missing values imputation. <em>EXSY</em>, <em>42</em>(2), e13828. (<a href='https://doi.org/10.1111/exsy.13828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vast amount of data in many different formats is produced and stored daily, offering machine learning a valuable resource to enhance its predictive capabilities. However, the pervasiveness of inaccuracies in real-world data presents a significant barrier that can seriously limit the effectiveness of learning algorithms. The ensemble models and hyper-tuned multi-layer perceptron (MLP) with need-based hidden neuron layers are effective frameworks for data imputation. Addressing the issue of missing data is a complex and demanding task, and much remains to be explored in developing effective and precise methods for predicting and imputing missing values across different datasets. The study offers important perspectives on using algorithms in machine learning to predict and impute gaps in data in recently updated datasets. The findings indicate that finely tuned MLP classifiers notably improve prediction accuracy and dependability compared to models with a static or reduced number of neurons. Furthermore, the study highlights the promising potential of ensemble models within the error-correcting output code (ECOC) framework as an effective approach for this task. It also suggests future research directions to refine further and strengthen machine learning-based imputation methods regarding precision and stability. ECOC framework includes all kinds of MLP classifiers and regressors such as binary classifiers, multi-class classifiers, or regression models. MLP models predict complex relationships in modern datasets. Hugging Face, COSMIC, SKlearn, and Kaggle have relevant and updated datasets. The weighted average recognition (96%) shows that the proposed MLP-based stochastic learning strategies achieved better results.},
  archive      = {J_EXSY},
  author       = {Muhammad Hameed Siddiqi and Madallah Alruwaili and Yousef Alhwaiti and Saad Alanazi and Faheem Khan},
  doi          = {10.1111/exsy.13828},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13828},
  shortjournal = {Expert Syst.},
  title        = {A novel hyper-tuned multilayer perceptron with effective stochastic learning strategies for missing values imputation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging transfer learning domain adaptation model with federated learning to revolutionise healthcare. <em>EXSY</em>, <em>42</em>(2), e13827. (<a href='https://doi.org/10.1111/exsy.13827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of artificial intelligence (AI) in healthcare has been witnessing an increasing interest. Particularly, federated learning (FL) has become favourable due to its potential for enhancing model quality whilst maintaining data privacy and security. However, the effectiveness of present FL methodologies could underperform under non-IID conditions, characterised by divergent data distributions across clients. The globally constructed FL model may suffer potent issues by allowing the least-performing models to equal participation. Thus, we propose a new accuracy-based FL approach (FedAcc) which only takes into account the clients' validation accuracy to consider their participation during global aggregation, also called Smart Healthcare Amplified (SHA). However, with limited supervised data it is challenging to increase the model performance thus concept of transfer learning (TL) is used. TL enables the global model to integrate knowledge from precomputed systems, resulting in an efficient model. However, the complexity of the global system is amplified by these TL models, leading to challenges related to vanishing gradients, particularly when dealing with a substantial number of layers. To mitigate this, we present a Transfer Learning Domain Adaptation Model (TLDAM). TLDAM employs a two-layered sequentially trained TL model, which contains approximately 50% fewer layers compared to traditional TL models. TLDAM is trained on multiple datasets such as MNIST and CIFAR10, to enhance its knowledge and make it domain-adaptive. Moreover, experimental results conducted on the UCI-HAR dataset reveal the supremacy of our proposed framework with an accuracy of 94.2990%, F-score of 94.2820%, precision of 94.3058%, and recall of 94.2993% over traditional FL techniques and state-of-the-art techniques.},
  archive      = {J_EXSY},
  author       = {Priyanka Verma and Nitesh Bharot and John G. Breslin and Donna O'Shea and Anand Kumar Mishra and Ankit Vidyarthi and Deepak Gupta},
  doi          = {10.1111/exsy.13827},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13827},
  shortjournal = {Expert Syst.},
  title        = {Leveraging transfer learning domain adaptation model with federated learning to revolutionise healthcare},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring metaheuristic optimization algorithms in the context of textual cyberharassment: A systematic review. <em>EXSY</em>, <em>42</em>(2), e13826. (<a href='https://doi.org/10.1111/exsy.13826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital landscape and rapid advancement of Information and Communication Technology have significantly increased social interactions, but it has also led to a rise in harmful behaviours such as offensive language, cyberbullying, and HS. Addressing online harassment is critical due to its severe consequences. This study offers a comprehensive evaluation of existing studies that employed metaheuristic optimization algorithms for detecting textual harassment content across social media platforms, highlighting their strengths and limitations. Using the PRISMA methodology, we reviewed and analysed 271 research papers, ultimately narrowing down the selection to 36 papers based on specific inclusion and exclusion criteria. By analysing key factors such as optimization techniques, feature engineering strategies, and dataset characteristics, we identify crucial trends and challenges in the field. Finally, we offer practical recommendations to improve the accuracy of predictive models, including adopting hybrid approaches, enhancing multilingual capabilities, and expanding models to operate effectively across various social media platforms.},
  archive      = {J_EXSY},
  author       = {Fatima Shannaq and Mohammad Shehab and Areej Alshorman and Mahmoud Hammad and Bassam Hammo and Wala'a Al-Omari},
  doi          = {10.1111/exsy.13826},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13826},
  shortjournal = {Expert Syst.},
  title        = {Exploring metaheuristic optimization algorithms in the context of textual cyberharassment: A systematic review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel reciprocal domain adaptation neural network for enhanced diagnosis of chronic kidney disease. <em>EXSY</em>, <em>42</em>(2), e13825. (<a href='https://doi.org/10.1111/exsy.13825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic kidney disease (CKD) is a major global health concern caused mostly by high blood pressure and glucose levels. Detecting CKD early is critical for reducing its negative consequences since it can lead to increased mortality rates. With CKD's rising incidence expected to make it the fifth biggest cause of death by 2040, rapid advances in diagnostic approaches are required. This study presents the Reciprocal Domain Adaptation Network (RDAN) as a potential approach to the various issues of CKD diagnosis. RDAN is a neural network model that will help to traverse the complexity of CKD diagnosis by smoothly combining diverse data sets. RDAN consists of two critical units at its foundation: Mutual Model Adaptation (MMA) and Domain Model Learning. The MMA unit uses a powerful Global and Local Pyramid Pooling technique to extract rich features from a variety of data domains. Meanwhile, the DML unit uses semi-supervised domain-independent features combined with MMA features to improve representation learning. RDAN includes a reciprocal regularizer to promote cross-domain knowledge transfer, maximising feature representation for accurate CKD identification. An analysis of RDAN's performance on a variety of real-world datasets showed remarkable results in terms of accuracy (96.94%), precision (98.81%), recall (98.73%), F1-Score (98.88%), and area under the curve (AUC—99.35%). These results highlight the unmatched expertise of RDAN in managing data bias, domain changes, and privacy issues related to CKD diagnosis. Beyond statistical measures, RDAN's implications promise revolutionary breakthroughs in early CKD identification and subsequent therapeutic therapies. RDAN stands out as a groundbreaking method for diagnosing CKD. It delivers exceptional accuracy and can be seamlessly applied in various clinical environments.},
  archive      = {J_EXSY},
  author       = {Saeed Iqbal and Adnan N. Qureshi and Musaed Alhussein and Khursheed Aurangzeb and Muhammad Zubair and Amir Hussain},
  doi          = {10.1111/exsy.13825},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13825},
  shortjournal = {Expert Syst.},
  title        = {A novel reciprocal domain adaptation neural network for enhanced diagnosis of chronic kidney disease},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing security and performance in live VM migration: A machine learning-driven framework with selective encryption for enhanced security and performance in cloud computing environments. <em>EXSY</em>, <em>42</em>(2), e13823. (<a href='https://doi.org/10.1111/exsy.13823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live virtual machine (LVM) migration is pivotal in cloud computing for its ability to seamlessly transfer virtual machines (VMs) between physical hosts, optimise resource utilisation, and enable uninterrupted service. However, concerns persist regarding safeguarding sensitive data during migration, particularly in critical sectors like healthcare, banking and military operations. Existing migration methods often compromise between performance and data security, prompting the need for a balanced solution. To address this, we propose a novel framework merging machine learning with selective encryption to fortify the pre-copy live migration process. Our approach intelligently predicts optimal migration times while selectively encrypting sensitive data, ensuring confidentiality and integrity without compromising performance. Rigorous experiments demonstrate its effectiveness, showcasing an average 51.82% reduction in downtime and an average 72.73% decrease in total migration time across diverse workloads. This integration of selective encryption not only bolsters security but also optimises migration metrics, presenting a robust solution for uninterrupted service delivery in critical cloud computing domains.},
  archive      = {J_EXSY},
  author       = {Raseena M. Haris and Mahmoud Barhamgi and Ahmed Badawy and Armstrong Nhlabatsi and Khaled M. Khan},
  doi          = {10.1111/exsy.13823},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13823},
  shortjournal = {Expert Syst.},
  title        = {Enhancing security and performance in live VM migration: A machine learning-driven framework with selective encryption for enhanced security and performance in cloud computing environments},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The energy storage and optimal dispatch supply chain for new energy grids using edge computing and the internet of things. <em>EXSY</em>, <em>42</em>(2), e13822. (<a href='https://doi.org/10.1111/exsy.13822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: J. Liu , “ The Energy Storage and Optimal Dispatch Supply Chain for New Energy Grids Using Edge Computing and the Internet of Things ,” Expert Systems 41 , no. 5 ( 2024 ): e13266. https://doi.org/10.1111/exsy.13266 . The above article, published online on 01 March 2023, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. In addition, the investigation found there was significant unattributed overlap between this article and a previously-published article (Dunnan et al. 2021 [ https://doi.org/10.1088/1755-1315/687/1/012140 ]), including the re-use of the charts between Figure 5 in this article and Figures 2 and 3 in the previously-published article. Therefore, the article must be retracted. The authors did not respond to the notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13822},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13822},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The energy storage and optimal dispatch supply chain for new energy grids using edge computing and the internet of things},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The design of intelligent fuzzy cognitive system of music emotion by product supply chain management. <em>EXSY</em>, <em>42</em>(2), e13821. (<a href='https://doi.org/10.1111/exsy.13821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: F. Li , R. Jiang , and J. Li , “ The Design of Intelligent Fuzzy Cognitive System of Music Emotion by Product Supply Chain Management ,” Expert Systems 41 , no. 5 ( 2024 ): e13265. https://doi.org/10.1111/exsy.13265 . The above article, published online on 23 February 2023, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. In addition, the investigation found logical inconsistencies between the topic of the article, the model design, and the conclusions reached in the article. Therefore, the article must be retracted. The authors did not respond to the notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13821},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13821},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The design of intelligent fuzzy cognitive system of music emotion by product supply chain management},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The evaluation of enterprise supply chain intelligent manufacturing system for agricultural interconnection data based on machine learning. <em>EXSY</em>, <em>42</em>(2), e13820. (<a href='https://doi.org/10.1111/exsy.13820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: C. Yu , L. Li , J. Li , P. Qin , and B. Zhang , “ The Evaluation of Enterprise Supply Chain Intelligent Manufacturing System for Agricultural Interconnection Data Based on Machine Learning ,” Expert Systems 41 , no. 5 ( 2024 ): e13259. https://doi.org/10.1111/exsy.13259 . The above article, published online on 23 February 2023, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. In addition, the investigation found a number of irrelevant or missing citations included in the published article which leaves some statements insufficiently supported. Therefore, the article must be retracted. The authors did not respond to the notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13820},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13820},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The evaluation of enterprise supply chain intelligent manufacturing system for agricultural interconnection data based on machine learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The analysis of green advertisement communication strategy based on deep factorization machine deep learning model under supply chain management. <em>EXSY</em>, <em>42</em>(2), e13819. (<a href='https://doi.org/10.1111/exsy.13819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: X. Yu , Y. Zhu , C. Jia , W. Lu , and H. Xu , “ The Analysis of Green Advertisement Communication Strategy Based on Deep Factorization Machine Deep Learning Model Under Supply Chain Management ,” Expert Systems 41 , no. 5 ( 2024 ): e13258. https://doi.org/10.1111/exsy.13258 . The above article, published online on 22 February 2023, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. In addition, the investigation found logical inconsistencies between the topic of the article and the conclusions reached in this article. Therefore, the article must be retracted. The authors did not respond to the notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13819},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13819},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The analysis of green advertisement communication strategy based on deep factorization machine deep learning model under supply chain management},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Intelligent cognitive evaluation of ice and snow sports training by fuzzy comprehensive evaluation from the perspective of supply chain management. <em>EXSY</em>, <em>42</em>(2), e13818. (<a href='https://doi.org/10.1111/exsy.13818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: P. Hu and P. Zhang , “ Intelligent Cognitive Evaluation of Ice and Snow Sports Training by Fuzzy Comprehensive Evaluation from the Perspective of Supply Chain Management ,” Expert Systems 41 , no. 5 ( 2024 ): e13212. https://doi.org/10.1111/exsy.13212 . The above article, published online on 08 December 2022, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. In addition, the investigation found logical inconsistencies between the topic of the article and the conclusions reached in this article. Therefore, the article must be retracted. The authors did not respond to the notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13818},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13818},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Intelligent cognitive evaluation of ice and snow sports training by fuzzy comprehensive evaluation from the perspective of supply chain management},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The analysis of optimized path selection for management mode of coastal regional circular economy based on fuzzy decision algorithm. <em>EXSY</em>, <em>42</em>(2), e13817. (<a href='https://doi.org/10.1111/exsy.13817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: W. Wei , D. Xiao , and W. Liu , “ The Analysis of Optimized Path Selection for Management Mode of Coastal Regional Circular Economy Based on Fuzzy Decision Algorithm ,” Expert Systems 41 , no. 5 ( 2024 ): e12985. https://doi.org/10.1111/exsy.12985 . The above article, published online on 04 March 2022, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. In addition, the investigation found significant flaws and logical inconsistencies in the article. Therefore, the article must be retracted. The authors did not respond to the notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13817},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13817},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The analysis of optimized path selection for management mode of coastal regional circular economy based on fuzzy decision algorithm},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Measuring systemic and systematic risk in the financial markets using artificial intelligence. <em>EXSY</em>, <em>42</em>(2), e13816. (<a href='https://doi.org/10.1111/exsy.13816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: M. M. Kamruzzaman , O. Alruwaili , and D. Aldaghmani , “ Measuring Systemic and Systematic Risk in the Financial Markets Using Artificial Intelligence ,” Expert Systems 41 , no. 5 ( 2024 ): e12971. https://doi.org/10.1111/exsy.12971 . The above article, published online on 10 March 2022, in Wiley Online Library ( http://onlinelibrary.wiley.com/ ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. Following an investigation by the publisher, the parties have concluded that this article was accepted solely on the basis of a compromised peer review process. In addition, the investigation found there was significant unattributed textual overlap between this article and online course documents available on an online learning platform. Therefore, the article must be retracted. The authors did not respond to the notice regarding the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13816},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13816},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Measuring systemic and systematic risk in the financial markets using artificial intelligence},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Segmentation and classification of lymphoblastic leukaemia using quantum neural network. <em>EXSY</em>, <em>42</em>(2), e13815. (<a href='https://doi.org/10.1111/exsy.13815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : J. Amin , M. A. Anjum , S. Krivic , M. I. Sharif , “ Segmentation and Classification of Lymphoblastic Leukaemia Using Quantum Neural Network ,” Expert Systems (Early View): e13225, https://doi.org/10.1111/exsy.13225 . The above article, published online on 23 December 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to our attention that the article was not reviewed in line with the journal's peer review standards. Furthermore, the use of quantum computing in this manuscript is insufficiently described, and the experimental methods and its supporting information lack sufficient detail to reproduce the findings. The authors disagree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13815},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13815},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Segmentation and classification of lymphoblastic leukaemia using quantum neural network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Model innovation of students' mental health education from the perspective of big data. <em>EXSY</em>, <em>42</em>(2), e13814. (<a href='https://doi.org/10.1111/exsy.13814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : B. Yang , “ Model Innovation of Students' Mental Health Education from the Perspective of Big Data ,” Expert Systems 40 , no. 4 ( 2023 ): e12948, https://doi.org/10.1111/exsy.12948 . The above article, published online on 11 February 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to our attention that the article was not reviewed in line with the journal's peer review standards. Moreover, multiple inconsistencies and flaws were identified in this article that affect the validity of the conclusions. Relevant information is missing so that the research described is not comprehensible.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13814},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13814},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Model innovation of students' mental health education from the perspective of big data},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Using electroencephalogram classification in a convolutional neural network, infer privacy on healthcare internet of things 5.0. <em>EXSY</em>, <em>42</em>(2), e13813. (<a href='https://doi.org/10.1111/exsy.13813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : U. S. B. K. Mahalaxmi , K. B. Sahay , R. Sabitha , S. L. A. Haleem , P. Kaur , P. Vijayakumar , “ Using Electroencephalogram Classification in a Convolutional Neural Network, Infer Privacy on Healthcare Internet of Things 5.0 ,” Expert Systems 40 , no. 4 ( 2023 ): e12942, https://doi.org/10.1111/exsy.12942 . The above article, published online on 03 March 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to our attention that the article was not reviewed in line with the journal's peer review standards and solely accepted on the basis of a compromised peer review process. Furthermore, relevant information about the research concept, the authentication method, and the source or nature of the patients' data are missing. As a result, the conclusions presented are considered invalid.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13813},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13813},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Using electroencephalogram classification in a convolutional neural network, infer privacy on healthcare internet of things 5.0},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Diagnosis of depression level using multimodal approaches using deep learning techniques with multiple selective features. <em>EXSY</em>, <em>42</em>(2), e13812. (<a href='https://doi.org/10.1111/exsy.13812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : P. Meshram , R. K. Rambola , “ Diagnosis of Depression Level Using Multimodal Approaches Using Deep Learning Techniques with Multiple Selective Features ,” Expert Systems 40 , no. 4 ( 2023 ): e12933, https://doi.org/10.1111/exsy.12933 . The above article, published online on 13 January 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to our attention that the article was not reviewed in line with the journal's peer review standards. Moreover, multiple inconsistencies and flaws were identified in this article that affect the validity of the conclusions. The underlying dataset and its processing are described insufficiently and explanation of information in several figures and tables is not appropriately provided.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13812},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13812},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Diagnosis of depression level using multimodal approaches using deep learning techniques with multiple selective features},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: An efficient deep learning-based video captioning framework using multi-modal features. <em>EXSY</em>, <em>42</em>(2), e13811. (<a href='https://doi.org/10.1111/exsy.13811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : S. Varma , D. P. James , “ An Efficient Deep Learning-based Video Captioning Framework Using Multi-modal Features ,” Expert Systems (Early View): e12920, https://doi.org/10.1111/exsy.12920 . The above article, published online on 13 December 2021 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. The retraction has been agreed on as the peer review and publishing process was found to be manipulated. Furthermore, the origin of the corresponding videos and datasets described remains uncertain. Accordingly, the research cannot be considered reproducible. The authors disagree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13811},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13811},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: An efficient deep learning-based video captioning framework using multi-modal features},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Data-driven decision-making model based on artificial intelligence in higher education system of colleges and universities. <em>EXSY</em>, <em>42</em>(2), e13810. (<a href='https://doi.org/10.1111/exsy.13810'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : Y. Teng , J. Zhang , T. Sun , “ Data-driven Decision-making Model Based on Artificial Intelligence in Higher Education System of Colleges and Universities ,” Expert Systems 40 , no. 4 ( 2023 ): e12820, https://doi.org/10.1111/exsy.12820 . The above article, published online on 31 January 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. The retraction has been agreed on as the predictive models and algorithms in this manuscript are insufficiently described, and the dataset used is not detailed. Furthermore, the article was not reviewed in line with the journal's peer review standards.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13810},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13810},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Data-driven decision-making model based on artificial intelligence in higher education system of colleges and universities},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: DAE-GAN: An autoencoder based adversarial network for gaussian denoising. <em>EXSY</em>, <em>42</em>(2), e13809. (<a href='https://doi.org/10.1111/exsy.13809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : A. Samanta , A. Saha , S. C. Satapathy , H. Lin , “ DAE-GAN: An Autoencoder Based Adversarial Network for Gaussian Denoising ,” Expert Systems (Early View): e12709, https://doi.org/10.1111/exsy.12709 . The above article, published online on 06 May 2021 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. The retraction has been agreed on as the article was not reviewed in line with the journal's peer review standards. Furthermore, the methodology and model in this manuscript are insufficiently described. Accordingly, the results are not considered reliable. The authors disagree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13809},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13809},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: DAE-GAN: An autoencoder based adversarial network for gaussian denoising},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PC-based user continuous authentication in the artificial intelligence method and system using the user's finger stroke characteristics. <em>EXSY</em>, <em>42</em>(2), e13806. (<a href='https://doi.org/10.1111/exsy.13806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric technology, which performs continuous authentication based on user behaviour, has been developed in various ways depending on the type of device, input device, and sensor. Research on continuous authentication technology in PC-based systems with few sensors installed is based on data from physical devices that extract and analyse features from keyboard and mouse input patterns. Among these, previous studies on continuous authentication through keyboard input performed continuous authentication using the key hold delay time that occurs when one key is pressed, the key interval delay time that occurs due to the interaction between fingers, and the key press delay time. However, the keyboard-based continuous authentication model has limitations in increasing accuracy due to a small number of features. Therefore, in this paper, when a user inputs a sentence using a QWERTY keyboard in a PC system, the function is subdivided by reflecting the characteristics of each finger and used for continuous authentication. The features extracted by reflecting the characteristics of the finger were subdivided into a total of 151 latencies, and Support Vector Data Description (SVDD), decision tree and CNN were used as continuous authentication models. Experimental data was collected through the user's input of randomly displayed sentences, and features were created based on this. User keystroke behaviour was used to validate the continuous authentication in the artificial intelligence model. Validation metrics included thresholds for classification accuracy (ACC), ROC curves, false rejection rate (FRR), equal error rate (EER), and false acceptance rate (FAR). As a result of the experiment, it was found that continuous authentication including the user's finger input pattern was superior to the existing method.},
  archive      = {J_EXSY},
  author       = {Heewoong Lee and Deok Gyu Lee and Kihyo Nam and Mun-Kweon Jeong},
  doi          = {10.1111/exsy.13806},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13806},
  shortjournal = {Expert Syst.},
  title        = {PC-based user continuous authentication in the artificial intelligence method and system using the user's finger stroke characteristics},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced semantic natural scenery retrieval system through novel dominant colour and multi-resolution texture feature learning model. <em>EXSY</em>, <em>42</em>(2), e13805. (<a href='https://doi.org/10.1111/exsy.13805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A conventional content-based image retrieval system (CBIR) extracts image features from every pixel of the images, and its depiction of the feature is entirely different from human perception. Additionally, it takes a significant amount of time for retrieval. An optimal combination of appropriate image features is necessary to bridge the semantic gap between user queries and retrieval responses. Furthermore, users should require minimal interactions with the CBIR system to obtain accurate responses. Therefore, the proposed work focuses on extracting highly relevant feature information from a set of images in various natural image databases. Subsequently, a feature-based learning/classification model is introduced before similarity measure calculations, aiming to minimise retrieval time and the number of comparisons. The proposed work analyses the learning models based on the retrieval system's performance separately for the following features: (i) dominant colour, (ii) multi-resolution radial difference texture patterns, and a combination of both. The developed work is assessed with other techniques, and the results are reported. The results demonstrate that the implemented ensemble learning model-based CBIR outperforms the recent CBIR techniques.},
  archive      = {J_EXSY},
  author       = {L. K. Pavithra and P. Subbulakshmi and Nirmala Paramanandham and S. Vimal and Norah Saleh Alghamdi and Gaurav Dhiman},
  doi          = {10.1111/exsy.13805},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13805},
  shortjournal = {Expert Syst.},
  title        = {Enhanced semantic natural scenery retrieval system through novel dominant colour and multi-resolution texture feature learning model},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolutionary game study of consumers' low-carbon travel behavior under carbon-inclusive policy. <em>EXSY</em>, <em>42</em>(2), e13804. (<a href='https://doi.org/10.1111/exsy.13804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carbon-inclusive policy is regarded as an incentive measure to personal low-carbon actions. However, its impacts are various for different parties under the government-led (including government and consumer) mode and the enterprise-led (including government, consumer and the enterprise) mode, while few studies reveal their difference and give reasonable implications. To fill these gaps, taking consumer's low-carbon travel as an example, this study develops two evolutionary game models—a two-party model (based on government-led adoption) and a tripartite model (based on enterprise-led adoption)—to investigate the effects of carbon-inclusive policy. The findings show that (1) the policy benefits all parties in both models, but the participation of the enterprise enhances the effectiveness of the policy; (2) the enterprise-led mode, that is, the operation of the carbon-inclusive platform by the enterprise is preferred because all parties have higher payoff, compared with the government-led mode; and (3) subsidies from the government has a greater impact for consumers' low-carbon behaviours. However, it has a less impact for the enterprise, which indicates the strategic action of the government is to establish a reasonable consumer subsidy system while reducing subsidies for the enterprise. This study offers a novel perspective on the effects of the carbon-inclusive policy on consumers' low-carbon behaviour, and enriches the practice of personal carbon trading.},
  archive      = {J_EXSY},
  author       = {Yaqin Liu and Xi Chen and Mengya Zhang and Ke Li and Daniel S. da Silva and Victor Hugo C. de Albuquerque},
  doi          = {10.1111/exsy.13804},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13804},
  shortjournal = {Expert Syst.},
  title        = {An evolutionary game study of consumers' low-carbon travel behavior under carbon-inclusive policy},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zone oriented binary multi-objective charged system search based feature selection approach for multi-label classification. <em>EXSY</em>, <em>42</em>(2), e13803. (<a href='https://doi.org/10.1111/exsy.13803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning is used in situations when each instance has many labels. Due to the high-dimensional feature space and noise in multi-label datasets, multi-label learning algorithms face substantial problems. Researchers have researched multi-label FS techniques to minimise data dimensionality in multi-label classification (MLC) problems. Global optimization approaches, such as evolutionary algorithm (EA) optimizers, scale well to high-dimensional problems. This paper proposes a hybrid multi-objective FS approach based on the charged system search (CSS) and grey wolf optimization (GWO) methods for the MLC problem. The first objective is to minimise the hamming loss (HLoss) value, and the second objective is to minimise the features from the feature set. A novel concept feature zone based on informative and non-informative features has been added here. Here, we have added the Preference Ranking Organisation METHod for Enrichment of Evaluations (PROMETHEE) approach to the objective function in the FS approach. Here, we have added the new velocity equation for the updated charge particles in the CSS algorithm. The GWO property has been added to the new velocity equation to improve the exploration and exploiting property in the CSS algorithm. For experimental verification, we have utilised six publically accessible multi-label datasets: CAL500 , Emotions , Medical , Enron , Scene , and the Yeast . The findings show that the proposed approach gets the best value regarding various performance metrics. The proposed method achieves optimal Jaccard Score (JC) and HLoss values of 0.4408 and 0.0645 for CAL500 , 0.8169 and 0.0719 for Emotions , 0.9486 and 0.0019 for Medical , 0.5950 and 0.0205 for Enron , 0.7391 and 0.0495 for Scene , and 0.6452 and 0.0766 for Yeast datasets. In particular, according to empirical data on a popular six-label benchmark multi-label datasets, the proposed method obtains competitive performance when labels are constrained.},
  archive      = {J_EXSY},
  author       = {Pradip Dhal and Chandrashekhar Azad},
  doi          = {10.1111/exsy.13803},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13803},
  shortjournal = {Expert Syst.},
  title        = {Zone oriented binary multi-objective charged system search based feature selection approach for multi-label classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective evolutionary algorithm based on decomposition with orthogonal experimental design. <em>EXSY</em>, <em>42</em>(2), e13802. (<a href='https://doi.org/10.1111/exsy.13802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary optimisation algorithms (MOEAs) have become a widely adopted way of solving the multi-objective optimisation problems (MOPs). The decomposition-based MOEAs demonstrate a promising performance for solving regular MOPs. However, when handling the irregular MOPs, the decomposition-based MOEAs cannot offer a convincing performance because no intersection between weight vector and the Pareto Front (PF) may lead to the same optimal solution assigned to the different weight vectors. To solve this problem, this paper proposes an MOEA based on decomposition with the orthogonal experimental design (MOEA/D-OED) that involves the selection operation, Orthogonal Experimental Design (OED) operation, and adjustment operation. The selection operation is to judge the unpromising weight vectors based on the history data of relative reduction values and convergence degree. The OED method based on the relative reduction function could make an explicit guidance for removing the worthless weight vectors. The adjustment operation brings in an estimation indicator of both diversity and convergence for adding new weight vectors into the interesting regions. To verify the versatility of the proposed MOEA/D-OED, 26 test problems with various PFs are evaluated in this paper. Empirical results have demonstrated that the proposed MOEA/D-OED outperforms eight representative MOEAs on MOPs with various types of PFs, showing promising versatility. The proposed algorithm shows highly competitive performance on all the various MOPs.},
  archive      = {J_EXSY},
  author       = {Maowei He and Zhixue Wang and Hanning Chen and Yang Cao and Lianbo Ma},
  doi          = {10.1111/exsy.13802},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13802},
  shortjournal = {Expert Syst.},
  title        = {Multi-objective evolutionary algorithm based on decomposition with orthogonal experimental design},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fog computing for artificial intelligence digital textbooks: Educational scaffolding and security and privacy challenges. <em>EXSY</em>, <em>42</em>(2), e13801. (<a href='https://doi.org/10.1111/exsy.13801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital textbooks (DTs) have evolved from DT 1.0, which simply converted paper textbooks to PDF format, to DT 2.0, which provides various multimedia content, for example, video and audio content. DTs have now advanced to DT 3.0, which enhances learner engagement through gamification and simulations. Recently, with the advancement of cloud computing technology and digital devices, for example, tablets, DT 4.0, which supports personalised learning through artificial intelligence (AI) tutors and chatbots, has been realised. South Korea is actively implementing a policy to distribute artificial intelligence–based DTs, equivalent to DT 4.0, to all schools under national leadership. For artificial intelligence–based DTs (AIDTs) in South Korea to develop into a sustainable education system, reliance on cloud computing alone is insufficient. It is also necessary to build layers of fog computing and edge computing from the initial stage. There are concerns that AIDTs may exacerbate the learning gap because they are more likely to be utilised actively by high-performing students with established self-directed learning habits rather than struggling students. Thus, it is essential to enhance usage monitoring and explore strategies that provide educational scaffolding to prevent differences in the level of AIDT utilisation from leading to a widening learning gap.},
  archive      = {J_EXSY},
  author       = {Pyoung Won Kim},
  doi          = {10.1111/exsy.13801},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13801},
  shortjournal = {Expert Syst.},
  title        = {Fog computing for artificial intelligence digital textbooks: Educational scaffolding and security and privacy challenges},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal task allocation and sequencing for flight test based on a memetic algorithm with lexicographic optimisation. <em>EXSY</em>, <em>42</em>(2), e13800. (<a href='https://doi.org/10.1111/exsy.13800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The flight test plays an important role in the development of an aircraft. Currently, with the increasing complexity and higher validation requirements for aircraft, there is a crucial need to generate high-quality flight test task schedules in an efficient way. This paper proposes a flight test task scheduling problem (FTTSP), which involves assigning suitable aircraft and executing the flight test tasks in a given order. Generally, the flight test duration (FTD) is the primary optimisation objective for the flight test task schedule, as it has a direct impact on aircraft development costs and the time to enter the market. In this study, the FTTSP not only considers FTD but also takes into account task transfer consumption (TTC). A mixed-integer linear programming mathematical model is first formulated to describe the FTTSP characteristics with the optimisation of the FTD and the TTC in a sequential manner. Then, a memetic algorithm with lexicographic optimisation (MALO) is proposed, which can efficiently obtain a high-quality solution and ensure that the most critical metric can be fully optimised. In MALO, a two-vector encoding and a task logic relationship repair mechanism based on the binary tree are established. An idle time insertion decoding method is designed to improve the aircraft utilisation rate. In addition to the selection, crossover and mutation operators, a local search operator is designed to enhance the solution quality. Finally, the full-scale test instances are generated for the FTTSP to evaluate the algorithm's performance. The numerical results demonstrate the effectiveness and competitiveness of the MALO in generating a high-quality schedule for flight test tasks.},
  archive      = {J_EXSY},
  author       = {Bei Tian and Gang Xiao and Yu Shen and Xingwei Jiang},
  doi          = {10.1111/exsy.13800},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13800},
  shortjournal = {Expert Syst.},
  title        = {Optimal task allocation and sequencing for flight test based on a memetic algorithm with lexicographic optimisation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling lung diseases in CT scan images with a hybrid bio-inspired mutated spider-monkey and crow search algorithm. <em>EXSY</em>, <em>42</em>(2), e13799. (<a href='https://doi.org/10.1111/exsy.13799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bio-inspired computer-aided diagnosis (CAD) has garnered significant attention in recent years due to the inherent advantages of bio-inspired evolutionary algorithms (EAs) in handling small datasets with elevated precision and reduced computational complexity. Traditional CAD models face limitations as they can only be developed post-outbreak, relying on datasets that become available during such events such as the COVID-19 pandemic. The scarcity of data for emerging diseases poses a substantial challenge to achieving elevated precision with conventional deep-learning algorithms. Furthermore, even when datasets are available, employing deep learning for class-based classification is arduous, necessitating model retraining, in this paper, we propose a novel hybrid algorithm that leverages the strengths of the crow search algorithm (CSA) and the spider monkey optimization (SMO) algorithm to create an optimised spider monkey crow search (OSM-CS) algorithm. We developed a CAD tool that maps each input CT image to a high-dimensional vector by extracting four categories of features: high contrast, polynomial decomposition, textural, and pixel statistics. The proposed OSM-CS algorithm is employed as a feature selection method. Our experimental results demonstrate the effectiveness of the OSM-CS algorithm, achieving an impressive accuracy of 98.2% when coupled with an AdaBoost classifier for multi-class classification and 99.93% for binary classification. This performance surpasses that of state-of-the-art (SOTA) deep learning models and recently published hybrid algorithms, underscoring the potential of the OSM-CS algorithm as a powerful tool in the realm of CAD.},
  archive      = {J_EXSY},
  author       = {Anupam Kumar and Faiyaz Ahmad and Bashir Alam},
  doi          = {10.1111/exsy.13799},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13799},
  shortjournal = {Expert Syst.},
  title        = {Unveiling lung diseases in CT scan images with a hybrid bio-inspired mutated spider-monkey and crow search algorithm},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The application of artificial intelligence planning and scheduling in photovoltaic plant construction projects. <em>EXSY</em>, <em>42</em>(2), e13798. (<a href='https://doi.org/10.1111/exsy.13798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning is one of the most critical areas within Project Management, with adequate task scheduling and resource management being of vital importance, especially at the project's outset. This paper introduces an Artificial Intelligence designed for the automatic planning of photovoltaic plant (PV) construction projects, encompassing various tasks such as engineering, procurement, logistics, construction and commissioning, and including the substation and transmission line, scheduling a total of 100 tasks, which constitute a basic Engineering, Procurement and Construction project planning. The model is trained using a total of 50 real-case project plans for PVs. The results demonstrate that the model successfully and effectively carries out photovoltaic project planning, marking a significant step towards digital transformation.},
  archive      = {J_EXSY},
  author       = {Jesús Gil Ruiz and Hernán Díaz and Rubén González Crespo},
  doi          = {10.1111/exsy.13798},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13798},
  shortjournal = {Expert Syst.},
  title        = {The application of artificial intelligence planning and scheduling in photovoltaic plant construction projects},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: An improved differential bond energy algorithm with fuzzy merging method to improve the document clustering for information mining. <em>EXSY</em>, <em>42</em>(2), e13797. (<a href='https://doi.org/10.1111/exsy.13797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : S. Tejasree and B. Chandra Mohan , “ An Improved Differential Bond Energy Algorithm With Fuzzy Merging Method to Improve the Document Clustering for Information Mining ,” Expert Systems 41 , no. 6 ( 2024 ): e13261, https://doi.org/10.1111/exsy.13261 . The above article, published online on 16 April 2023 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted on the basis of a compromised peer review process. The editors have therefore decided to retract this article. The authors disagree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13797},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13797},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: An improved differential bond energy algorithm with fuzzy merging method to improve the document clustering for information mining},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: An information extraction method based on improved mixed text density web pages. <em>EXSY</em>, <em>42</em>(2), e13796. (<a href='https://doi.org/10.1111/exsy.13796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : Y. Zhou , X. Yin and J. Yan , “ An Information Extraction Method Based on Improved Mixed Text Density Web Pages ,” Expert Systems 41 , no. 6 ( 2024 ): e13267, https://doi.org/10.1111/exsy.13267 . The above article, published online on 03 March 2023 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted on the basis of a compromised peer review process. Furthermore, unreliable references have been used as a basis for the research. The editors have therefore decided to retract this article. The authors were informed of the decision to retract but were unavailable for comment.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13796},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13796},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: An information extraction method based on improved mixed text density web pages},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Elstm: An improved long short-term memory network language model for sequence learning. <em>EXSY</em>, <em>42</em>(2), e13795. (<a href='https://doi.org/10.1111/exsy.13795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : Z. Li , Q. Wang , J.-Q. Wang , H.-B. Qu , J. Dong , and Z. Dong , " Elstm: An Improved Long Short-term Memory Network Language Model for Sequence Learning ,” Expert Systems 41 , no. 6 ( 2024 ): e13211, https://doi.org/10.1111/exsy.13211 . The above article, published online on 28 December 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted on the basis of a compromised peer review process. The editors have therefore decided to retract this article. The authors were informed of the decision to retract but were unavailable for comment.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13795},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13795},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Elstm: An improved long short-term memory network language model for sequence learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Cyber security for federated learning environment using AI technique. <em>EXSY</em>, <em>42</em>(2), e13794. (<a href='https://doi.org/10.1111/exsy.13794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : H. J. Alyamani , “ Cyber Security for Federated Learning Environment Using AI Technique ,” Expert Systems 40 , no. 5 ( 2023 ): e13080, https://doi.org/10.1111/exsy.13080 . The above article, published online on 26 September 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was published on the basis of a compromised peer review process. Furthermore, the research as described is not comprehensible for readers and unreliable references have been cited leaving some statements insufficiently supported. The editors have therefore decided to retract this article. The authors were informed of the decision to retract but were unavailable for comment.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13794},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13794},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Cyber security for federated learning environment using AI technique},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrusion detection using CTGAN and lightweight neural network for internet of things. <em>EXSY</em>, <em>42</em>(2), e13793. (<a href='https://doi.org/10.1111/exsy.13793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based intrusion detection systems have high accuracy and low false alarm rates. However, there are challenges to deploy deep learning models in the vulnerable, resource-constrained Internet of Things. Therefore, two deep learning models are proposed: Lightweight Intrusion Detection System using Feedforward Neural Network (LIDSuFNN) and Lightweight Intrusion Detection System using Convolutional Neural Network (LIDSuCNN). In the models, the feedforward neural network is compressed using neuron pruning and the convolutional neural network is compressed using filter pruning. Then, quantization has been applied to the models. The models are trained and tested on standard datasets and synthetic datasets. A generative artificial intelligence model, Conditional Tabular Generative Adversarial Network (CTGAN), has been used to generate synthetic data. The models have been compared with the baselines and results are analyzed. Experimental results show that the proposed models require less training time and memory than the baselines, with approximately similar performance. The reduction of various parameters is due to the fact that pruning and quantization have removed unnecessary calculations from the networks. Statistical analysis has also been done to show the superiority of the proposed techniques.},
  archive      = {J_EXSY},
  author       = {Sudeshna Das and Abhishek Majumder and Suyel Namasudra and Ashish Singh},
  doi          = {10.1111/exsy.13793},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13793},
  shortjournal = {Expert Syst.},
  title        = {Intrusion detection using CTGAN and lightweight neural network for internet of things},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ontology for in-depth description of user situations in connected environments. <em>EXSY</em>, <em>42</em>(2), e13792. (<a href='https://doi.org/10.1111/exsy.13792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context-awareness is increasingly recognised as a fundamental principle in the development of ubiquitous computing and ambient intelligence. By leveraging contextual data about users and their environments, systems can gain a deeper understanding of the evolving user situation. This empowers them to dynamically adapt their operations, leading to optimised resource utilisation, enhanced decision-making, and ultimately, greater user satisfaction. However, a critical challenge lies in effectively representing user situations with a high degree of expressiveness. While ontology-based data models have emerged as a promising approach due to their ability to handle the inherent heterogeneity of context information, existing ontologies have limitations in terms of information coverage, data heterogeneity and uncertainties consideration, and reusability across various application domains. This paper addresses these limitations by proposing uCSN, an ontology that builds upon and extends the Data Privacy Vocabulary (DPV), Semantic Sensor Network (SSN) and W3C Uncertainty ontologies, to provide a rich and expressive vocabulary for representing diverse user situations. We evaluate uCSN based on its consistency, accuracy, clarity and performance.},
  archive      = {J_EXSY},
  author       = {Karam Bou-Chaaya and Richard Chbeir and Mahmoud Barhamgi and Philippe Arnould and Benslimane Djamal},
  doi          = {10.1111/exsy.13792},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13792},
  shortjournal = {Expert Syst.},
  title        = {An ontology for in-depth description of user situations in connected environments},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An attention-driven hybrid deep neural network for enhanced heart disease classification. <em>EXSY</em>, <em>42</em>(2), e13791. (<a href='https://doi.org/10.1111/exsy.13791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart disease continues to be a primary cause of mortality globally, highlighting the critical necessity for efficient early prediction and classification techniques. This study presents a new hybrid model attention-based CNN-Bi-LSTM that integrates the SMOTE with an attention-driven improved convolutional neural network-recurrent neural network architecture to improve the classification of heart sounds, especially from imbalanced datasets. Heart sounds are difficult to classify because of their complex acoustic properties and the variability of their characteristics across frequency and temporal domains. The proposed model utilises an advanced CNN to effectively extract global and local features, in conjunction with a bidirectional long short-term memory network to improve the architecture by capturing contextual information from both preceding and subsequent time sequences. The incorporation of spatial attention within the CNN and temporal attention in the RNN enables the model to concentrate on the most pertinent audio segments. To address the challenges presented by imbalanced and noisy datasets that may impede the efficacy of deep learning algorithms, our model employs SMOTE to improve data representation. The hybrid model outperformed popular models such as CNN, LSTM and CNN-LSTM, achieving a classification accuracy of more than 97% on the PCG and PASCAL heart sound datasets. The findings demonstrate the model's reliability as an initial evaluation tool in clinical settings, thereby improving support for cardiovascular disease diagnosis.},
  archive      = {J_EXSY},
  author       = {Umesh Kumar Lilhore and Sarita Simaiya and Musaed Alhussein and Surjeet Dalal and Khursheed Aurangzeb and Amir Hussain},
  doi          = {10.1111/exsy.13791},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13791},
  shortjournal = {Expert Syst.},
  title        = {An attention-driven hybrid deep neural network for enhanced heart disease classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of different modality of data to diagnose parkinson's disease using machine learning and deep learning approaches: A review. <em>EXSY</em>, <em>42</em>(2), e13790. (<a href='https://doi.org/10.1111/exsy.13790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic nature of Parkinson's disease (PD) is that it gradually impacts regions of the brain that are responsible for the production of the dopamine hormone. Despite continuous efforts, no effective treatment or preventative approach exists for PD. Nonetheless, the disease can be detected. Our goal is to create a Machine Learning and Deep Learning-based system that can detect Parkinson's disease from a variety of data sources with high accuracy, sensitivity, specificity and interpretability. However, there have been significant advancements in the field of research, especially the use of artificial intelligence in the Parkinson's disease diagnostic process. We reviewed articles that were released between 2018 and 2024, concentrating on the most current studies that had been published. We chose 70 research articles for our review paper based on a set of criteria from a variety of online databases, including IEEExpress, medical databases like PubMed, Google Scholar, ResearchGate and ScienceDirect, and various publishers, including Elsevier, Taylor & Francis, Springer, MDPI, Plos One and so forth. According to our review, the majority of works make use of voice data. Our review study found that the highest accuracy level of most papers was above 90%, and the most commonly used algorithms were CNN and SVM. The main goal of this review study is to look into and put together information about the different ways that artificial intelligence, especially Machine Learning, can be used to find Parkinson's disease. Using diverse data gathered from multiple public and private datasets, we can infer that the application of artificial intelligence, particularly Machine Learning algorithms, for identifying Parkinson's disease plays a crucial role in the medical field.},
  archive      = {J_EXSY},
  author       = {Sheikh Bahauddin Arnab and Md Istakiak Adnan Palash and Rakibul Islam and Hemal Hossain Ovi and Mohammad Abu Yousuf and Md Zia Uddin},
  doi          = {10.1111/exsy.13790},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13790},
  shortjournal = {Expert Syst.},
  title        = {Analysis of different modality of data to diagnose parkinson's disease using machine learning and deep learning approaches: A review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible distribution approaches to enhance regression and deep topic modelling techniques. <em>EXSY</em>, <em>42</em>(2), e13789. (<a href='https://doi.org/10.1111/exsy.13789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an extension of the Dirichlet multinomial regression (DMR) and deep Dirichlet multinomial regression (dDMR) topic modelling approaches by incorporating the generalised Dirichlet (GD) and Beta-Liouville (BL) distributions using collapsed Gibbs sampling for parameter inference. The DMR and dDMR approaches have been shown to be effective in discovering latent topics in text corpora. However, these approaches have limitations when it comes to handling complex data structures and overfitting issues. To address these limitations, we introduce the GD and BL distributions, which have more flexibility in modelling complex data structures and handling sparse data. Additionally, we use collapsed Gibbs sampling to estimate the model parameters, which provides a computationally efficient method for inference. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed approach in improving topic modelling performance, particularly in handling complex data structures and reducing overfitting. The proposed models also exhibit good interpretability of the learned topics, making them suitable for various applications in natural language processing and machine learning.},
  archive      = {J_EXSY},
  author       = {Pantea Koochemeshkian and Nizar Bouguila},
  doi          = {10.1111/exsy.13789},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13789},
  shortjournal = {Expert Syst.},
  title        = {Flexible distribution approaches to enhance regression and deep topic modelling techniques},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing psychologists' understanding through explainable deep learning framework for ADHD diagnosis. <em>EXSY</em>, <em>42</em>(2), e13788. (<a href='https://doi.org/10.1111/exsy.13788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental disorder that is challenging to diagnose and requires advanced approaches for reliable and transparent identification and classification. It is characterised by a pattern of inattention, hyperactivity and impulsivity that is more severe and more frequent than in individuals with a comparable level of development. In this paper, an explainable framework based on a fine-tuned hybrid Deep Neural Network (DNN) and Recurrent Neural Network (RNN) called HyExDNN-RNN model is proposed for ADHD detection, multi-class categorization and decision interpretation. This framework not only detects ADHD but also provides interpretable insights into the diagnostic process so that psychologists can better understand and trust the results of the diagnosis. We use the Pearson correlation coefficient for optimal feature selection and machine and deep learning models for experimental analysis and comparison. We use a standardised technique for feature reduction, model selection and interpretation to accurately determine the diagnosis rate and ensure the interpretability of the proposed framework. Our framework provided excellent results on binary classification, with HyExDNN-RNN achieving an F1-score of 99% and 94.2% on multi-class categorization. XAI approaches, in particular SHapley Additive exPlanations (SHAP) and Permutation Feature Importance (PFI), provided important insights into the importance of features and the decision logic of models. By combining AI with human expertise, we aim to bridge the gap between advanced computational techniques and practical psychological applications. These results demonstrate the potential of our framework to assist in ADHD diagnosis and interpretation.},
  archive      = {J_EXSY},
  author       = {Abdul Rehman and Jerry Chun-Wei Lin and Ilona Heldal},
  doi          = {10.1111/exsy.13788},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13788},
  shortjournal = {Expert Syst.},
  title        = {Enhancing psychologists' understanding through explainable deep learning framework for ADHD diagnosis},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSL-driven approaches and metamodels for chatbot development: A systematic literature review. <em>EXSY</em>, <em>42</em>(2), e13787. (<a href='https://doi.org/10.1111/exsy.13787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chatbots have emerged as ubiquitous tools for enhancing user interaction across various platforms, from customer service to personal assistance. They are computer programs that simulate and process human conversation, either written, spoken or both. However, developing efficient chatbots remains a challenge, primarily due to the intricate nature of critical components of chatbots like natural language understanding (NLU) requiring a subscription from intent recognition providers like Dialogflow and Amazon Lex. This makes chatbots closely linked to NLP services and can be locked in. Recently, various research studies have provided solutions to reduce the workload of developers and designers. These approaches have proposed model-driven development via domain-specific languages (DSLs), which make the chatbot development process more accessible and more automated. This advancement aims to enhance effectiveness in chatbot development by leveraging DSLs. This study aims to provide a comprehensive overview of DSLs for developing chatbots, with the first contribution comprising various research topics, tools, approaches, and technologies employed to implement DSLs. Second, this work aims to assess and contrast the primary DSLs currently available for chatbot development, focusing on presenting the key elements used in constructing these DSLs. Third, this study identifies the challenges and limitations of using DSLs in chatbot development.},
  archive      = {J_EXSY},
  author       = {Charaf Ouaddi and Lamya Benaddi and El Mahi Bouziane and Abdeslam Jakimi and Abdellah Chehri and Rachid Saadane},
  doi          = {10.1111/exsy.13787},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13787},
  shortjournal = {Expert Syst.},
  title        = {DSL-driven approaches and metamodels for chatbot development: A systematic literature review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent computing for crop monitoring in CIoT: Leveraging AI and big data technologies. <em>EXSY</em>, <em>42</em>(2), e13786. (<a href='https://doi.org/10.1111/exsy.13786'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumer Internet of Things (CIoT) has revolutionised agriculture by integrating intelligent computing, artificial intelligence and big data technologies in crop monitoring. This paper explores the application of intelligent computing and deep learning methodologies in crop monitoring within the CIoT framework. In CIoT-based crop monitoring, a vision sensor collects real-time data from crop leaf images. The image dataset is processed using state-of-the-art deep learning models and intelligent computing algorithms. This integration enables the early detection of crop diseases by leveraging computer vision and deep learning. Intelligent computing systems provide accurate disease classification, real-time alerts, and actionable recommendations for optimised crop management practises. This advanced system empowers farmers to make data-driven decisions, such as irrigation optimization, targeted pesticide application and nutrient supplementation, to maximise crop productivity and minimise losses. A benchmark dataset of leaf images is used, and a deep learning based model is presented for classifying healthy and diseased leaves. Experimental results demonstrate an accuracy rate of 0.98, with detailed validation, including dataset size and model parameters. Key benefits of intelligent computing in CIoT-based crop monitoring include enhanced resource efficiency, reduced environmental impact, and improved sustainability. The paper also addresses the challenges of implementing AI and big data technologies, such as data privacy, security, interoperability and resource management in agricultural settings.},
  archive      = {J_EXSY},
  author       = {Imran Ahmed and Misbah Ahmad and Haythem Ghazouani and Walid Barhoumi and Gwanggil Jeon},
  doi          = {10.1111/exsy.13786},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13786},
  shortjournal = {Expert Syst.},
  title        = {Intelligent computing for crop monitoring in CIoT: Leveraging AI and big data technologies},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dragon boat optimization: A meta-heuristic for intelligent systems. <em>EXSY</em>, <em>42</em>(2), e13785. (<a href='https://doi.org/10.1111/exsy.13785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dragon boat racing, a popular aquatic folklore team sport, is traditionally held during the Dragon Boat Festival. Inspired by this event, we propose a novel human-based meta-heuristic algorithm called dragon boat optimization (DBO) in this paper. It models the unique behaviours of each crew member on the dragon boat during the race by introducing social psychology mechanisms (social loafing, social incentive). Throughout this process, the focus is on the interaction and collaboration among the crew members, as well as their decision-making in various situations. During each iteration, DBO implements different state updating strategies. By accurately modelling the crew's behaviour and employing adaptive state update strategies, DBO consistently achieves high optimization performance, as validated by comprehensive testing on 29 benchmark functions and 2 structural design problems. Experimental results indicate that DBO outperforms 7 and 16 state-of-the-art meta-heuristic algorithms across these test functions and problems, respectively.},
  archive      = {J_EXSY},
  author       = {Xiang Li and Long Lan and Husam Lahza and Shaowu Yang and Shuihua Wang and Wenjing Yang and Hengzhu Liu and Yudong Zhang},
  doi          = {10.1111/exsy.13785},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13785},
  shortjournal = {Expert Syst.},
  title        = {Dragon boat optimization: A meta-heuristic for intelligent systems},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent crack detection in infrastructure using computer vision at the edge. <em>EXSY</em>, <em>42</em>(2), e13784. (<a href='https://doi.org/10.1111/exsy.13784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To fulfil the demands of the industry in autonomous intelligent inspection, innovative frameworks that allow Convolutional Neural Networks to run at the edge in real-time are required. This paper proposes an end-to-end approach and system to enable crack detection onboard a customised embedded system. In order to make possible the deployment and execution on edge, this work develops a dataset by combining new and existing images, it introduces a quantization approach that includes inference optimization, memory reuse, and freezing layers. Real-time, onsite results from aerial and hand-held setup images of industrial environments show that the system is capable of identifying and localiszing cracks within the field of view of the camera with a mean average precision (mAP) of 98.44% and at ~2.5 frames per second with real-time inference. Therefore, it is evidenced that, despite using a full model, the introduced model customization improved the mAP by ~8% with respect to lighter state-of-the-art models, and the quantization technique led to a model inference two times faster. The proposed intelligent and autonomous approach advances common offline inspection techniques to enable on-site, artificial intelligence-based inspection systems, which also aid in reducing human errors and enhance safety conditions by automatically performing defect-recognition in tight and difficult-to-reach spots.},
  archive      = {J_EXSY},
  author       = {Mst. Mousumi Rizia and Julio A. Reyes-Munoz and Angel G. Ortega and Ahsan Choudhuri and Angel Flores-Abad},
  doi          = {10.1111/exsy.13784},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13784},
  shortjournal = {Expert Syst.},
  title        = {Intelligent crack detection in infrastructure using computer vision at the edge},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval time series forecasting: An innovative approach transforming interval to single time series. <em>EXSY</em>, <em>42</em>(2), e13783. (<a href='https://doi.org/10.1111/exsy.13783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval Time Series are present in everyday life. An example is the opening and closing value of some stock in a market for certain time intervals. The forecasting plays an essential role in many financial organisations. The development of new mathematical tools or improving the existing ones will lead to more accurate forecasting techniques. Interval Arithmetic is a mathematical field that uses intervals by nature and algorithms that use it are involved in the solution of Interval Time Series. Another classical algorithm is VAR models. In this paper, a method that comes from the insurance sector is used to forecast Brent Oil monthly values. The innovation of what we propose is that it converts the Interval Time Series system into a single time series and can propagate the results back to each time series of the system. This way the researcher works with only one time series instead of two (or more). The forecasting algorithm is a choice of the researcher, expediting the development of forecasting (even ARIMA can be applied). We demonstrate our methodology in forecasting the Brent Oil monthly prices by applying the ANFIS algorithm.},
  archive      = {J_EXSY},
  author       = {George Varelas and Giannis Tzimas},
  doi          = {10.1111/exsy.13783},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13783},
  shortjournal = {Expert Syst.},
  title        = {Interval time series forecasting: An innovative approach transforming interval to single time series},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The construction of enterprise's financial supply chain management under blockchain technology. <em>EXSY</em>, <em>42</em>(2), e13782. (<a href='https://doi.org/10.1111/exsy.13782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction : W. Ke , “ The Construction of Enterprise's Financial Supply Chain Management Under Blockchain Technology ,” Expert Systems 41 , no. 5 ( 2024 ): e13297. https://doi.org/10.1111/exsy.13297 . The above article, published online on 30 March 2023, in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors did not respond to the notice of retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13782},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13782},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The construction of enterprise's financial supply chain management under blockchain technology},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The risk evaluation and management of the sports service supply chain by introducing fuzzy comprehensive appraisal and artificial intelligence technology. <em>EXSY</em>, <em>42</em>(2), e13781. (<a href='https://doi.org/10.1111/exsy.13781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction : Y. Teng , Y. Wang , and H. You , “ The Risk Evaluation and Management of the Sports Service Supply Chain by Introducing Fuzzy Comprehensive Appraisal and Artificial Intelligence Technology ,” Expert Systems 41 , no. 5 ( 2024 ): e13279. https://doi.org/10.1111/exsy.13279 . The above article, published online on 13 March 2023, in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors did not respond to the notice of retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13781},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13781},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The risk evaluation and management of the sports service supply chain by introducing fuzzy comprehensive appraisal and artificial intelligence technology},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Supply chain risk management of badminton supplies company using decision tree model assisted by fuzzy comprehensive evaluation. <em>EXSY</em>, <em>42</em>(2), e13780. (<a href='https://doi.org/10.1111/exsy.13780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction : D. Zhang , Y. Tang , and X. Yan , “ Supply Chain Risk Management of Badminton Supplies Company Using Decision Tree Model Assisted by Fuzzy Comprehensive Evaluation ,” Expert Systems 41 , no. 5 ( 2024 ): e13275. https://doi.org/10.1111/exsy.13275 . The above article, published online on 05 March 2023, in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors did not respond to the notice of retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13780},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13780},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Supply chain risk management of badminton supplies company using decision tree model assisted by fuzzy comprehensive evaluation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The genetic algorithm and BP neural network in financial supply chain management under information sharing. <em>EXSY</em>, <em>42</em>(2), e13779. (<a href='https://doi.org/10.1111/exsy.13779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction : C. Li , Z. Li , and M. Wu , “ The Genetic Algorithm and BP Neural Network in Financial Supply Chain Management Under Information Sharing ,” Expert Systems 41 , no. 5 ( 2024 ): e13273. https://doi.org/10.1111/exsy.13273 . The above article, published online on 05 March 2023, in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors did not respond to the notice of retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13779},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13779},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The genetic algorithm and BP neural network in financial supply chain management under information sharing},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: The design for supply chain management of intelligent logistics system using cloud computing and the internet of things. <em>EXSY</em>, <em>42</em>(2), e13778. (<a href='https://doi.org/10.1111/exsy.13778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: H. Wang , Y. Yin , and X. Wang , “ The Design for Supply Chain Management of Intelligent Logistics System Using Cloud Computing and the Internet of Things ,” Expert Systems 41 , no. 5 ( 2024 ): e13271. https://doi.org/10.1111/exsy.13271 . The above article, published online on 02 March 2023, in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted solely on the basis of a compromised peer review process. In addition, the authors did not provide a statement regarding individual consent for the images used in Figure 13, which violates the journal's and the publisher's guidelines. The editors have therefore decided to retract the article. The authors did not respond to the notice of retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13778},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13778},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: The design for supply chain management of intelligent logistics system using cloud computing and the internet of things},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Adoption of blockchain + internet of things in demand forecasting of farm supply chain. <em>EXSY</em>, <em>42</em>(2), e13777. (<a href='https://doi.org/10.1111/exsy.13777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: H. Wang , T. Liu , and Z. Yu , “ Adoption of Blockchain + Internet of Things in Demand Forecasting of Farm Supply Chain ,” Expert Systems 41 , no. 5 ( 2024 ): e13187. https://doi.org/10.1111/exsy.13187 . The above article, published online on 02 November 2022, in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors did not respond to the notice of retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13777},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13777},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Adoption of blockchain + internet of things in demand forecasting of farm supply chain},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Supply chain operation evaluation and management decision by fuzzy cognitive map model. <em>EXSY</em>, <em>42</em>(2), e13776. (<a href='https://doi.org/10.1111/exsy.13776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction: P. Zheng , “ Supply Chain Operation Evaluation and Management Decision by Fuzzy Cognitive Map Model ,” Expert Systems 41 , no. 5 ( 2024 ): e13022. https://doi.org/10.1111/exsy.13022 . The above article, published online on 15 May 2022, in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors did not respond to the notice of retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13776},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13776},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Supply chain operation evaluation and management decision by fuzzy cognitive map model},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Auxiliary cognition system-based management strategy optimization of supply chain of new energy in Oil–Gas enterprises. <em>EXSY</em>, <em>42</em>(2), e13775. (<a href='https://doi.org/10.1111/exsy.13775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retraction : Q. Sun and S. He , “ Auxiliary Cognition System-Based Management Strategy Optimization of Supply Chain of New Energy in Oil–Gas Enterprises ,” Expert Systems 41 , no. 5 ( 2024 ): e12974. https://doi.org/10.1111/exsy.12974 . The above article, published online on 04 March 2022, in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was accepted solely on the basis of a compromised peer review process. The editors have therefore decided to retract the article. The authors did not respond to the notice of retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13775},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13775},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Auxiliary cognition system-based management strategy optimization of supply chain of new energy in Oil–Gas enterprises},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: Challenges and vulnerability evaluation of smart cities in IoT device based on cybersecurity mechanism. <em>EXSY</em>, <em>42</em>(2), e13774. (<a href='https://doi.org/10.1111/exsy.13774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : A. O. Almagrabi , “ Challenges and Vulnerability Evaluation of Smart Cities in IoT Device Based on Cybersecurity Mechanism ,” Expert Systems 40 , no. 4 ( 2023 ): e13113, https://doi.org/10.1111/exsy.13113 . The above article, published online on 01 September 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that this article was not reviewed in line with the journal's peer review standards. Furthermore, some of the figures included in this article are inadequately presented and insufficiently discussed. As a result, the research described cannot be reproduced. The editors have therefore decided to retract this article. The author was informed of the decision to retract but was unavailable for comment.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13774},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13774},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: Challenges and vulnerability evaluation of smart cities in IoT device based on cybersecurity mechanism},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MeDi-TODER: Medical domain-incremental task-oriented dialogue generator using experience replay. <em>EXSY</em>, <em>42</em>(2), e13773. (<a href='https://doi.org/10.1111/exsy.13773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) technology has brought groundbreaking changes to the healthcare domain. Specifically, the integration of a medical dialogue system (MDS) has facilitated interactions with patients, identifying meaningful information such as symptoms and medications from their dialogue history to generate appropriate responses. However, shortcomings arise when MDS lacks access to the patient's cumulative history or prior domain knowledge, resulting in the generation of inaccurate responses. To address this challenge, we propose a medical domain-incremental task-oriented dialogue generator using experience replay (MeDi-TODER) that applies the continual learning technique to the medical task-oriented dialogue generator. By strategically sampling and storing exemplars from previous domains and rehearsing it as it learns, the model effectively retains knowledge and can respond to the novel domains. Extensive experiments demonstrated that MeDi-TODER significantly outperforms other models that lack continual learning in both natural language generation and natural language understanding. Notably, our proposed method achieves the highest scores, surpassing the upper-class benchmarks.},
  archive      = {J_EXSY},
  author       = {Minji Kim and Joon Yoo and OkRan Jeong},
  doi          = {10.1111/exsy.13773},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13773},
  shortjournal = {Expert Syst.},
  title        = {MeDi-TODER: Medical domain-incremental task-oriented dialogue generator using experience replay},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crude oil markets volatility forecasting: A novel deep learning hybrid model. <em>EXSY</em>, <em>42</em>(2), e13772. (<a href='https://doi.org/10.1111/exsy.13772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To the national economy, increasing the forecasting accuracy of realised volatility (RV) on crude oil futures markets is of critical strategic importance. However, the RV of crude oil futures cannot be accurately predicted with a single model. For this study, we adopt a hybrid model which combines gated recurrent unit (GRU) and complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) to forecast the RV of crude oil futures. Moreover, back propagation neural networks (BP), Elman neural networks (Elman), support vector regression machine (SVR), autoregressive model (AR), heterogeneous autoregressive model (HAR), and their hybrid models with CEEMDAN are adopted as comparisons. In general, this article demonstrates the superiority of the CEEMDAN-GRU model in RV forecasting from several aspects: for both evaluation criteria, CEEMDAN-GRU achieves the highest RV forecasting accuracy in emerging and developed crude oil futures markets; furthermore, the empirical results are robust to alternative realised measures and training sets of different lengths.},
  archive      = {J_EXSY},
  author       = {Zixiao Lin and Bin Tan and Yu Lin and Qin Lu},
  doi          = {10.1111/exsy.13772},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13772},
  shortjournal = {Expert Syst.},
  title        = {Crude oil markets volatility forecasting: A novel deep learning hybrid model},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot contrastive learning-based multi-round dialogue intent classification method. <em>EXSY</em>, <em>42</em>(2), e13771. (<a href='https://doi.org/10.1111/exsy.13771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional text classification models face challenges in handling long texts and understanding topic transitions in dialogue scenarios, leading to suboptimal performance in automatic speech recognition (ASR)-based multi-round dialogue intent classification. In this article, we propose a few-shot contrastive learning-based multi-round dialogue intent classification method. First, the ASR texts are partitioned, and role-based features are extracted using a Transformer encoder. Second, refined sample pairs are forward-propagated, adversarial samples are generated by perturbing word embedding matrices and contrastive loss is applied to positive sample pairs. Then, positive sample pairs are input into a multi-round reasoning module to learn semantic clues from the entire scenario through multiple dialogues, obtain reasoning features, input them into a classifier to obtain classification results, and calculate multi-task loss. Finally, a prototype update module (PUM) is introduced to rectify the biased prototypes by using gated recurrent unit (GRU) to update the prototypes stored in the memory bank and few-shot learning (FSL) task. Experimental evaluations demonstrate that the proposed method outperforms state-of-the-art methods on two public datasets (DailyDialog and CM) and a private real-world dataset.},
  archive      = {J_EXSY},
  author       = {Feng Wei and Xu Zhang},
  doi          = {10.1111/exsy.13771},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13771},
  shortjournal = {Expert Syst.},
  title        = {Few-shot contrastive learning-based multi-round dialogue intent classification method},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and compressed deep learning model for brain tumour classification with explainable AI for smart healthcare and information communication systems. <em>EXSY</em>, <em>42</em>(2), e13770. (<a href='https://doi.org/10.1111/exsy.13770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of brain tumours presents a significant challenge in the medical domain, where prompt and precise diagnosis is crucial as patient outcomes depend on it. Conventional deep neural networks perform well in carrying out various imaging tasks within the healthcare sector; however, their effectiveness often falls short of expectations in practical applications due to the substantial computational resources required and issues with reliability. In this research, an optimised and effective deep learning model founded on the DenseNet-169 architecture is introduced for the classification of magnetic resonance imaging brain tumours, which is particularly advantageous for smart healthcare systems and information and communication technology (ICT) settings with limited computational capabilities. The model compression methodologies, including pruning and quantization, have been employed to significantly diminish the dimensions and intricacy of the model while achieving a classification accuracy of 97.07%. Furthermore, this endeavour necessitates the enhancement of the model's interpretability through the utilisation of explainable artificial intelligence methodologies such as Gradient-weighted Class Activation Mapping (Grad-CAM) and SHapley Additive exPlanations (SHAP), which will aid clinicians in highlighting crucial areas of the images and validating feature importance concerning the decisions rendered by the model. A comparative performance evaluation is conducted against DenseNet-169, ResNet-50 and various other models to delineate the superior efficacy of our model, rendering it exceptionally adept for knowledge-driven, real-time brain tumour diagnosis within smart healthcare and ICT systems where resources are constrained.},
  archive      = {J_EXSY},
  author       = {Amar Singh and Rajesh Kumar Shrivastava and Ashutosh Srivastava},
  doi          = {10.1111/exsy.13770},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13770},
  shortjournal = {Expert Syst.},
  title        = {Efficient and compressed deep learning model for brain tumour classification with explainable AI for smart healthcare and information communication systems},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). General industrial process optimization method to leverage machine learning applied to injection moulding. <em>EXSY</em>, <em>42</em>(2), e13769. (<a href='https://doi.org/10.1111/exsy.13769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of machine learning (ML) technologies is driving changes across many sectors. In industrial settings, this is called the fourth industrial revolution and encompasses several technologies pushing the boundaries of industrial automation. In this study, a general industrial process optimization (GIPO) methodology is formulated in the context of Industry 4.0 and tested on an industrial injection moulding machine (IMM). GIPO aims to encourage the practical inclusion of industrial artificial intelligence at all levels of the manufacturing process, while enabling industrial equipment to adapt to a changing processing environment. Special attention is given to the generality of the methodology so that it can be extended to other applications. In the example case study presented here, GIPO combines K-nearest neighbours classification and nearest neighbours optimization methods to optimize an injection moulding process effectively. Practical implementation conducted on the IMM demonstrates a novel methodology to leverage data mining and ML methods in a real-world setting to improve production quality, production time and energy cost.},
  archive      = {J_EXSY},
  author       = {Meaghan Charest-Finn and Rickey Dubay},
  doi          = {10.1111/exsy.13769},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13769},
  shortjournal = {Expert Syst.},
  title        = {General industrial process optimization method to leverage machine learning applied to injection moulding},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAPTURE—Computational analysis and predictive techniques for urban resource efficiency. <em>EXSY</em>, <em>42</em>(2), e13768. (<a href='https://doi.org/10.1111/exsy.13768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Municipal waste management (MWM) poses significant challenges in the context of rapid urbanisation and population growth. Accurate forecasting of waste production is crucial for designing sustainable waste management strategies. However, traditional forecasting methods often struggle to capture the complexities of waste generation dynamics. This paper proposes a novel methodology leveraging deep learning techniques to forecast municipal waste production. By harnessing the power of deep neural networks, our approach transcends the limitations of conventional models, providing more accurate and impactful predictions. We integrate heterogeneous data sources, including demographic and territorial information, into a comprehensive graph representation of municipalities. Graph Neural Networks are then employed to extract intricate spatial and temporal patterns from the graph structure. Empirical validation through a case study in the Apulia region demonstrates the effectiveness of our methodology in furnishing accurate forecasts for waste production. Our framework is adaptable and scalable, making it suitable for application across diverse geographical areas. This research contributes to advancing waste management practices by providing stakeholders with actionable insights for informed decision-making.},
  archive      = {J_EXSY},
  author       = {Marzia Canzaniello and Stefano Izzo and Diletta Chiaro and Antonella Longo and Francesco Piccialli},
  doi          = {10.1111/exsy.13768},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13768},
  shortjournal = {Expert Syst.},
  title        = {CAPTURE—Computational analysis and predictive techniques for urban resource efficiency},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benchmarking anomaly detection methods: Insights from the UCR time series anomaly archive. <em>EXSY</em>, <em>42</em>(2), e13767. (<a href='https://doi.org/10.1111/exsy.13767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection, vital for identifying deviations from normative data patterns, is particularly crucial in sensor-driven real-world applications, which predominantly involve temporal data in the form of time series. Traditional evaluation of anomaly detection methods has relied on public benchmark datasets. Yet, recent revelations have uncovered inherent flaws and inadequacies in these datasets, casting doubt on the perceived progress in the field. To address this challenge, the UCR Time Series Anomaly Archive has been recently proposed—a meticulously curated database comprising 250 time series—designed to provide a robust and error-free benchmark for anomaly detection research. This paper comprehensively evaluates state-of-the-art anomaly detection techniques using the UCR Time Series Anomaly Archive. Our findings demonstrate the efficacy of current methods in accurately detecting anomalies across an important portion of datasets without additional optimization, underscoring the archive's utility as a foundational baseline for future research and development in anomaly detection methodologies.},
  archive      = {J_EXSY},
  author       = {Francisco J. Baldán and Diego García-Gil},
  doi          = {10.1111/exsy.13767},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13767},
  shortjournal = {Expert Syst.},
  title        = {Benchmarking anomaly detection methods: Insights from the UCR time series anomaly archive},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A social group chatbot system by multiple topics tracking and atkinson-shiffrin memory model using AI agents collaboration. <em>EXSY</em>, <em>42</em>(2), e13766. (<a href='https://doi.org/10.1111/exsy.13766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of Internet has accelerated the explosive growth of data, which in turn leads to information overload and information confusion. This makes it difficult for us to communicate effectively in social groups, thereby intensifying the demands for emotional companionship. Therefore, we propose a novel social group chatting framework based on Large Language Model (LLM) powered multiple autonomous agents collaboration in this article. Specifically, BERTopic is used to extract topics from history chatting content for each social group everyday, and then multiple topics tracking is realised through multi-level association by adaptive time sliding-window mechanism and optimal matching. Furthermore, we use topic tracking architecture and prompts to design and implement an AI Chatbot system with different characters that can conduct natural language conversations with users in online social group. LLM, as the controller and coordinator of the whole AI Chatbot for sub-tasks, allows different AI Agents to autonomously decide whether to participate in current topic, how to generate response, and whether to propose a new topic. Each AI Agent has their own multi-store memory system based on the Atkinson-Shiffrin model. Finally, we construct a verification environment based on online game that is consistent with real society. Subjective and objective evaluation methods were deployed to perform qualitative and quantitative analyses to demonstrate the performance of our AI Chatbot system.},
  archive      = {J_EXSY},
  author       = {Guoshuai Zhang and Jiaji Wu and Gwanggil Jeon and Penghui Wang},
  doi          = {10.1111/exsy.13766},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13766},
  shortjournal = {Expert Syst.},
  title        = {A social group chatbot system by multiple topics tracking and atkinson-shiffrin memory model using AI agents collaboration},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TOPS: A framework for trusted opinion analysis of product reviews using hybrid deep learning based D2CL filter. <em>EXSY</em>, <em>42</em>(2), e13765. (<a href='https://doi.org/10.1111/exsy.13765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of online product reviews has made it increasingly challenging for consumers to make informed purchase decisions. However, the abundance of reviews, including fake or augmented and sarcastic reviews, poses a challenge for consumers. To address this challenge, this paper introduces the TOPS (Trusted Opinion analysis of Product reviewS) framework, a novel approach that leverages a hybrid deep learning-based D2CL (Dual Deep leaning based cleaning) filter to enhance the reliability of online reviews. The proposed methodology employs the D2CL filter to identify and eliminate fake and sarcastic reviews, ensuring that the consolidated sentiment analysis provides users with trustworthy opinions. The framework is equipped with the R-mGRU, a hybrid deep learning model specifically designed to tackle the nuances of product reviews. This model has demonstrated impressive accuracy rates, achieving 89%, 91%, and 94% for fake, sarcasm, and sentiment analysis tasks, respectively. The TOPS framework makes a significant contribution to improving the overall quality and authenticity of product reviews, empowering consumers with more reliable information for informed decision-making in online shopping scenarios.},
  archive      = {J_EXSY},
  author       = {T. K. Balaji and Annushree Bablani and S. R. Sreeja and Hemant Misra},
  doi          = {10.1111/exsy.13765},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13765},
  shortjournal = {Expert Syst.},
  title        = {TOPS: A framework for trusted opinion analysis of product reviews using hybrid deep learning based D2CL filter},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving teacher training through emotion recognition and data fusion. <em>EXSY</em>, <em>42</em>(2), e13764. (<a href='https://doi.org/10.1111/exsy.13764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of education hinges on the proficiency and training of educators. Due to the importance of teacher training, the innovative platform Teacher Moments creates simulated classroom scenarios. In this scenario-based learning, confusion is an important indicator to detect users who struggle with the simulations. Through Teacher Moments, we gathered 7975 audio recording responses from participants who self-labelled their recordings according to whether they sounded confused. Our dataset stands out for its size, for not including actor-generated audio, and for measuring confusion, a neglected emotion in artificial intelligence (AI). Our experiments tested unimodal approaches and feature-level, model-level and decision-level fusion. Feature-level fusion demonstrated superior performance to unimodal methods, achieving a balanced accuracy of 0.6607 on the test set. This outcome highlights the necessity for further investigation in the overlooked area of confusion detection, particularly employing realistic datasets like the one used in this study and exploring new methods. Beyond teacher training, the insights of this research also extend to other directions, such as other professionals making critical decisions, user interface design or adaptive learning systems.},
  archive      = {J_EXSY},
  author       = {Mariano Albaladejo-González and Rubén Gaspar-Marco and Félix Gómez Mármol and Justin Reich and José A. Ruipérez-Valiente},
  doi          = {10.1111/exsy.13764},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13764},
  shortjournal = {Expert Syst.},
  title        = {Improving teacher training through emotion recognition and data fusion},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging unsupervised task adaptation and semi-supervised learning with semantic-enriched representations for online sexism detection. <em>EXSY</em>, <em>42</em>(2), e13763. (<a href='https://doi.org/10.1111/exsy.13763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, the proliferation of hateful and sexist content targeting women on social media has become a concerning issue, adversely affecting women's lives and freedom of expression. Previous efforts to detect online sexism have utilized monolingual ensemble transformers combined with data augmentation techniques that incorporate related-domain data, such as hate speech. However, these approaches often struggle to capture the full diversity and complexity of sexism due to limitations in the size and quality of training data. In this study, we introduce a novel sexism detection system that employs in-domain unlabeled data through unsupervised task-adaptation techniques and semi-supervised learning, using an efficient single multilingual transformer model. Additionally, we incorporate a Sentence-BERT layer to enhance our system with semantically meaningful sentence embeddings. Our proposed system outperforms existing state-of-the-art methods across all tasks and datasets, demonstrating its effectiveness in detecting and addressing sexism in social media text. These results underscore the potential of our approach, providing a foundation for further research and practical applications.},
  archive      = {J_EXSY},
  author       = {Francisco Rodríguez-Sánchez and Jorge Carrillo-de-Albornoz and Laura Plaza},
  doi          = {10.1111/exsy.13763},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13763},
  shortjournal = {Expert Syst.},
  title        = {Leveraging unsupervised task adaptation and semi-supervised learning with semantic-enriched representations for online sexism detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTION: SVM-based generative adverserial networks for federated learning and edge computing attack model and outpoising. <em>EXSY</em>, <em>42</em>(2), e13762. (<a href='https://doi.org/10.1111/exsy.13762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RETRACTION : P. Manoharan , R. Walia , C. Iwendi , T. A. Ahanger , S. T. Suganthi , M. M. Kamruzzaman , S. Bourouis , W. Alhakami and M. Hamdi , “ SVM-based Generative Adverserial Networks for Federated Learning and Edge Computing Attack Model and Outpoising ,” Expert Systems 40 , no. 5 ( 2023 ): e13072, https://doi.org/10.1111/exsy.13072 . The above article, published online on 09 August 2022 in Wiley Online Library ( wileyonlinelibrary.com ), has been retracted by agreement between the journal Editor-in-Chief, David Camacho; and John Wiley & Sons Ltd. The article was submitted as part of a guest-edited special issue. Following publication, it has come to the attention of the journal that the experimental methods in this manuscript are insufficiently described. Accordingly, the results cannot be reproduced and the research is not comprehensible for readers. The editors have therefore decided to retract this article. The authors disagree with the retraction.},
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.13762},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13762},
  shortjournal = {Expert Syst.},
  title        = {RETRACTION: SVM-based generative adverserial networks for federated learning and edge computing attack model and outpoising},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective federated averaging algorithm. <em>EXSY</em>, <em>42</em>(2), e13761. (<a href='https://doi.org/10.1111/exsy.13761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent global trend is the convergence of information and communications technology (ICT). By applying ICT in various fields such as the humanities, new types of products and services are created, and new values that help people's lives can be created. AI can be selected as a representative technology in such convergence ICT. However, applying AI technology to actual production requires ensuring data security. Federated learning (FL) can achieve secure sharing of data, where all parties participate in model training locally and upload it to the server for aggregation. The data never leaves the parties involved, thus solving the problems of data privacy and data silos. However, FL faces issues such as high communication cost, imbalanced performance distribution among participants, and low privacy protection. To achieve a balance between model accuracy, communication cost, fairness, and privacy, this paper proposes a multi-objective optimization-based FL algorithm (M-FedAvg). The multi-objective optimization problem of maximising the accuracy of the global model, minimising the communication cost, minimising the variance of the accuracy, and minimising the privacy budget is solved by NSGA-III. The experimental results show that the algorithm proposed can effectively reduce the communication cost of FL and achieve privacy protection for participants without affecting the accuracy of the global model.},
  archive      = {J_EXSY},
  author       = {Daoqu Geng and Shouzheng Wang and Yihang Zhang},
  doi          = {10.1111/exsy.13761},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13761},
  shortjournal = {Expert Syst.},
  title        = {Multi-objective federated averaging algorithm},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient biomedical text summarization with quantized LLaMA 2: Enhancing memory usage and inference on low powered devices. <em>EXSY</em>, <em>42</em>(2), e13760. (<a href='https://doi.org/10.1111/exsy.13760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of large language models (LLMs) on edge devices and non-server environments presents significant challenges, primarily due to constraints in memory usage, computational power, and inference time. This article investigates the feasibility of running LLMs across such devices by focusing on optimising memory usage, employing quantization techniques, and reducing inference time. Specifically, we utilise LLaMA 2 for biomedical text summarization and implement low-rank adaptation (LoRA) quantization to compress the model size to compress the model size and fine-tune it using limited resources. Our study systematically evaluates memory consumption during both training and inference phases, demonstrating substantial reductions through efficient LoRA quantization. Our results indicate that with careful optimization, it is feasible to deploy sophisticated LLMs like LLaMA 2 on low powered devices, thereby broadening the scope of their application in resource-constrained environments.},
  archive      = {J_EXSY},
  author       = {Sanjeev Kumar and Vikas Ranjan and Arjab Chakrabarti and Tridib Kumar Das and Anushka Singh},
  doi          = {10.1111/exsy.13760},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13760},
  shortjournal = {Expert Syst.},
  title        = {Efficient biomedical text summarization with quantized LLaMA 2: Enhancing memory usage and inference on low powered devices},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in emotion classification via facial and body gesture analysis: A survey. <em>EXSY</em>, <em>42</em>(2), e13759. (<a href='https://doi.org/10.1111/exsy.13759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine vision has had a substantial impact on human-computer interaction and psychological research. The ability of robots to generate realistic facial expressions has been enhanced by advancements in artificial intelligence, neural networks, and deep learning, thereby nurturing more profound emotive connections between humans and machines. This paper offers an in-depth look of the most recent developments in emotion classification technologies, with a specific focus on the period from 2019 to 2023. This research addresses the integration of facial and body gesture analysis into emotion detection, emphasising the development of methods such as partial transfer learning, sign-based measurement systems, and lightweight convolutional neural networks. By analysing the most recent research, the paper not only provides a comprehensive overview of the most advanced methods, but also emphasises their superiority over previous approaches in terms of efficiency, and applicability. The integration of gestures into emotion analysis is underscored as a critical area for further investigation, providing novel opportunities for comprehending and interpreting the intricate layers of nonverbal communication. The objective of this survey is to provide a resource for researchers and practitioners who are interested in advancing the field of emotion classification by utilising innovative methodologies.},
  archive      = {J_EXSY},
  author       = {Anjali Diwan and Reshma Sunil and Parita Mer and Rajesh Mahadeva and Shashikant P. Patole},
  doi          = {10.1111/exsy.13759},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13759},
  shortjournal = {Expert Syst.},
  title        = {Advancements in emotion classification via facial and body gesture analysis: A survey},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal knowledge graph reasoning based on dynamic fusion representation learning. <em>EXSY</em>, <em>42</em>(2), e13758. (<a href='https://doi.org/10.1111/exsy.13758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, significant progress has been made in completing static knowledge graphs. However, knowledge tends to evolve with time, and static knowledge graph completion (KGC) methods struggle to capture the changes. Therefore, temporal knowledge graph (TKG) reasoning has become a focus of research. Most existing TKG methods incorporate temporal information into triplets and transform them into KGC tasks, ignoring the important influence of time information and implicit relationships between entities. In this paper, we propose a new method called TD-RKG, which addresses the challenges of temporal variability and implicit entity correlations based on a dynamic fusion representation learning approach. The method consists of four modules: dynamic local recurrent encoding layer, dynamic implicit encoding layer, dynamic global information attention layer and decoding layer. Experimental results on three benchmark datasets demonstrate substantial improvements in TD-RKG across multiple evaluation metrics.},
  archive      = {J_EXSY},
  author       = {Hongwei Chen and Man Zhang and Zexi Chen},
  doi          = {10.1111/exsy.13758},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13758},
  shortjournal = {Expert Syst.},
  title        = {Temporal knowledge graph reasoning based on dynamic fusion representation learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep feature driven expert system to estimate the postmortem interval from corneal opacity development. <em>EXSY</em>, <em>42</em>(2), e13757. (<a href='https://doi.org/10.1111/exsy.13757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Postmortem interval (PMI) estimation remains an unresolved challenge in forensic science, necessitating practical, reliable and more accurate tools. This study aimed to develop a quantitative PMI estimation tool that effectively meets these needs. Focusing on the postmortem opacity development of the eye as a key marker for determining time since death, we propose an artificial intelligence-based clinical PMI prediction system utilising computer vision, deep learning and machine learning methods. The AlexNet algorithm was utilised to extract deep features from the postmortem eye images. Extracted features were then processed by machine learning algorithms. For feature selection, Lasso and Relief techniques were employed, while SVM and KNN were applied for classifications. The results were validated using the leave-one-subject-out method. The system was tested across different postmortem ranges, providing multi-label predictions. The performance was evaluated using various metrics. The deep features exhibited effective performance in grading postmortem opacity development, achieving state-of-the-art results. The accuracy scores were 0.96 and 0.97 for 3-h intervals (i.e., 5-class) and 5-h intervals (i.e., 3-class) experiments, respectively. The experimental results indicate that the proposed system represents a promising tool for PMI estimation.},
  archive      = {J_EXSY},
  author       = {İsmail Cantürk and Lale Özyılmaz},
  doi          = {10.1111/exsy.13757},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13757},
  shortjournal = {Expert Syst.},
  title        = {A deep feature driven expert system to estimate the postmortem interval from corneal opacity development},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LightAuth: A lightweight sensor nodes authentication framework for smart health system. <em>EXSY</em>, <em>42</em>(2), e13756. (<a href='https://doi.org/10.1111/exsy.13756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfeit medical devices pose a threat to patient safety, necessitating a secure device authentication system for medical applications. Resource-constrained sensory nodes are vulnerable to hacking, prompting the need for robust security measures. Token-based authentication schemes, such as one-time passwords (OTPs), smart cards, key fobs, and mobile authentication apps, along with certificate-based authentication methods, such as client and code-signing, employ cryptographic frameworks like elliptical curve cryptography (ECC) and physical unclonable functions (PUF). However, these methods face challenges, including block sequence issues and susceptibility to side-channel attacks. To address these issues, we propose a framework for mutual authentication using private Ethereum. This framework integrates private Ethereum and cryptographic techniques for encrypting and decrypting data using mathematical algorithms to overcome block sequence issues and side-channel attacks. Similarly, fog nodes are utilised to enhance local computing, storage, and networking capabilities for sensors. The framework is evaluated using metrics such as communication costs, execution costs, and computation costs based on Ethereum gas consumption. The performance of the LightAuth framework is compared with that of the Smart Contracts Against Counterfeit IoMT (SCACIoMT) framework, designed for Internet of Medical Things (IoMT) devices. The effectiveness of LightAuth is verified through formal security analysis using BAN logic.},
  archive      = {J_EXSY},
  author       = {Zain Ul Islam Adil and Majid Iqbal Khan and Kahkishan Sanam and Saif U. R. Malik and Syed Atif Moqurrab and Gautam Srivastava},
  doi          = {10.1111/exsy.13756},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13756},
  shortjournal = {Expert Syst.},
  title        = {LightAuth: A lightweight sensor nodes authentication framework for smart health system},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal evolution pathway and forecasting of non-fossil energy consumption and carbon emission under china's carbon peak target: A markov switching AR and RNN approach. <em>EXSY</em>, <em>42</em>(2), e13755. (<a href='https://doi.org/10.1111/exsy.13755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To fulfil the commitments of the Paris Agreement, China will strive to achieve carbon peak (CP) by 2030. It is necessary to identify the evolution characteristics of China's carbon emissions and provide a scientific path prediction for the formulation of reasonable emission reduction policies and measures. This study summarises and predicts the pathway of China's carbon peak (CP) using carbon emission intensity (CEI) and the percentage of non-fossil energy consumption (NEC) as indicators, and combining MSIH(3)-AR(2) model and recurrent neural network. The results show that: (1) China's CEI experiences a ‘low decline regime’ (LDR), a ‘medium decline regime’ (MDR) and a ‘high decline regime’ (HDR), while the share of NEC goes through a ‘low fluctuation regime’ (LFR), a ‘medium growth regime’ (MGR) and a ‘high growth regime’ (HGR). (2) For CEI, the switching probability from MDR to the HDR is 74.88%, illustrating a substantial improvement. For NEC, the switching probability from MGR to HGR is 28.92%, but the probability of returning to MGR is 61.76%, indicating an adjustment. (3) By 2030, CEI will reach 0.9896 tons/100 million CNY, decreased by 66.35% compared with 2005. While the percentage of NEC will rise to 26.61%. Based on these, policy suggestions such as strengthening the top-level design, upgrading energy mix and accelerating green technological changes are proposed to break the bottlenecks of reaching CP and further zero carbon goal. This study is expected to provide theoretical support and empirical evidence for the achievement CP in China, and to provide empirical references for promoting the ‘dual carbon’ process in other countries.},
  archive      = {J_EXSY},
  author       = {Bei Liu and Zhaoxuan Qiu and Jiachao Peng and Yaoyin Hu},
  doi          = {10.1111/exsy.13755},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13755},
  shortjournal = {Expert Syst.},
  title        = {Temporal evolution pathway and forecasting of non-fossil energy consumption and carbon emission under china's carbon peak target: A markov switching AR and RNN approach},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BENN: Balanced ensemble neural network for handling class imbalance in big data. <em>EXSY</em>, <em>42</em>(2), e13754. (<a href='https://doi.org/10.1111/exsy.13754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a critical challenge in big data analytics, often leading to biased predictive models. This imbalance can lead to biased models that perform well on the majority class but poorly on the minority class. Many machine learning models tend to be biased towards the majority class because they aim to minimise overall error, often leading to poor performance on the minority class. This paper presents the balanced ensemble neural network, a novel solution to effectively address class imbalance in big data. Balanced ensemble neural network combines the robust capabilities of neural networks with the power of ensemble learning, incorporating class balancing strategies to ensure fair representation of minority classes. The methodology involves integrating multiple neural networks, each trained on balanced subsets of data using techniques like Synthetic Minority Over-sampling Technique and Random Undersampling. This integration aims to leverage the strengths of individual networks while reducing their inherent biases. Our extensive experiments across various datasets reveal that BENN achieves an AUC-ROC score of 0.94, surpassing other models such as random forest (0.88), support vector (0.84) and single neural net (0.80). It was also observed that BENN's performance is better compared to traditional neural network models and standard ensemble methods in key metrics like accuracy, precision, recall, F1-score and AUC-ROC. The results specifically highlight BENN's effectiveness in accurately classifying instances of minority classes, a notable challenge in many existing models. These findings underscore BENN's potential as a substantial advancement in handling class imbalance within big data environments, offering a promising direction for future research and application in machine learning.},
  archive      = {J_EXSY},
  author       = {Sneha Halebeedu Ramesh and Annappa Basava and Sankar Pariserum Perumal},
  doi          = {10.1111/exsy.13754},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13754},
  shortjournal = {Expert Syst.},
  title        = {BENN: Balanced ensemble neural network for handling class imbalance in big data},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transforming lung disease diagnosis with transfer learning using chest X-ray images on cloud computing. <em>EXSY</em>, <em>42</em>(2), e13750. (<a href='https://doi.org/10.1111/exsy.13750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of Cloud and Fog computing settings, recent developments in deep learning techniques show great potential for changing several fields, including healthcare. In this study, we make a contribution to this changing field by proposing an enhanced deep learning-based strategy for classifying chest X-ray images, using pre-trained models such as RetinaNet, EfficientNet and Faster-R-CNN, which we use through transfer learning. Our strategy outperforms single models and traditional techniques by leveraging critical data gleaned from multiple models, demonstrating the ability of deep learning to improve diagnostic precision. Our approach presents a novel dual-check system in the context of worries about security, privacy and trust in Cloud and Fog-based Smart Systems. In this case, a decision support system uses chest X-ray images to make an initial diagnosis that is then confirmed by a medical practitioner. This cooperative strategy not only reduces diagnostic errors that come from machine and human sources but also emphasises how crucial it is to incorporate AI-driven solutions into safe and reliable healthcare ecosystems. Our approach raises the bar for the quality of patient care and healthcare outcomes by overcoming the drawbacks of traditional diagnostic methods that depend on the subjective opinions of physicians. Our work brings out how deep learning might transform clinical diagnostics by distinguishing inflammatory regions in chest X-ray images. Research is needed to fully grasp the transformative potential of deep learning in medical image processing, especially as the healthcare industry continues to embrace AI-driven solutions. Further research endeavours have to dig into tactics like broadening the scope of datasets, executing data augmentation methodologies and incorporating bespoken features to augment the elasticity and effectiveness of AI-driven diagnostic systems.},
  archive      = {J_EXSY},
  author       = {Imran Arshad Choudhry and Saeed Iqbal and Musaed Alhussein and Adnan N. Qureshi and Khursheed Aurangzeb and Rizwan Ali Naqvi},
  doi          = {10.1111/exsy.13750},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13750},
  shortjournal = {Expert Syst.},
  title        = {Transforming lung disease diagnosis with transfer learning using chest X-ray images on cloud computing},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Filling the holes on 3D heritage object surface based on automatic segmentation algorithm. <em>EXSY</em>, <em>42</em>(2), e13749. (<a href='https://doi.org/10.1111/exsy.13749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing and processing 3D objects are activities in the research field of computer graphics, image processing and computer vision. The 3D objects are processed based on geometric modelling (a branch of applied mathematics and computational geometry) or machine learning algorithms based on image processing. The computation of geometrical objects includes processing the curves and surfaces, subdivision, simplification, meshing, holes filling or reconstructing the 3D surface's objects on both point cloud data and triangular mesh. While the machine learning methods are developed using deep learning models. With the support of 3D laser scan devices and LiDAR techniques, the obtained dataset is close to the original shape of the real objects. Besides, photography and its application based on modern techniques in recent years help us collect data and process the 3D models more precisely. This article proposes a new method for filling holes on the 3D object's surface based on automatic segmentation. Instead of filling the hole directly as the existing methods, we now subdivide the hole before filling it. The hole is first determined and segmented automatically based on the computation of its local curvature. It is then filled on each part of the hole to match its local curvature shape. The method can work on both 3D point cloud surfaces and triangular mesh surfaces. Compared to the state-of-the-art (SOTA) methods, our proposed method obtained higher accuracy of the reconstructed 3D objects.},
  archive      = {J_EXSY},
  author       = {Sinh Van Nguyen and Son Thanh Le and Minh Khai Tran and Sach Thanh Le},
  doi          = {10.1111/exsy.13749},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13749},
  shortjournal = {Expert Syst.},
  title        = {Filling the holes on 3D heritage object surface based on automatic segmentation algorithm},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy security, intelligence and innovation quality: Evidence from china's new energy demonstration city policy. <em>EXSY</em>, <em>42</em>(2), e13748. (<a href='https://doi.org/10.1111/exsy.13748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of new energy and the application of intelligent technologies are crucial for ensuring energy security and promoting high-quality economic growth. This article utilises a sample of A-share listed enterprises in China from 2008 to 2020, employing a difference-in-differences (DID) approach to explore the impact of new energy demonstration cities (NEDC) on innovation quality and focuses on the role of intelligent development. It is found that the construction of NEDC contributes to the improvement of enterprise innovation quality. Corporate intelligence transformation further enhances the effect of the NEDC policy, demonstrating a synergistic relationship with new energy transition. Additionally, compared with collaborative innovation, the improvement effect of NEDC policy on enterprise self-innovation quality is more significant. Non-state-owned enterprises, those in the growth stage, with a solid technological foundation and located in non-resource-based cities are more likely to achieve improvements in innovation quality during the energy transition. Our work provides empirical evidence and valuable insights for ensuring energy security and facilitating intelligent innovation development.},
  archive      = {J_EXSY},
  author       = {Minghui Liu and Sasa Yang and Xin Yao},
  doi          = {10.1111/exsy.13748},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13748},
  shortjournal = {Expert Syst.},
  title        = {Energy security, intelligence and innovation quality: Evidence from china's new energy demonstration city policy},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Support vector machines with uncertainty option and incremental sampling for kriging. <em>EXSY</em>, <em>42</em>(2), e13747. (<a href='https://doi.org/10.1111/exsy.13747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach to pollution assessment by investigating support vector machines (SVM) with an uncertainty option to overcome the limitations of traditional kriging. While kriging is a major tool for geostatistical modelling, allowing to estimate the distribution of contaminants in a region from a small set of samples, it does not allow to extract also the uncertainty map. An uncertainty map is of great interest, as it allows to identify regions of high uncertainty where one should sample in order to reduce high level of uncertainties. In this paper, we propose two variants of the SVM with an uncertainty option, each using a different hinge loss to improve the accuracy and efficiency. These losses allow to estimate different levels of contaminations, as well as uncertainty, such as the three levels: positive, uncertain and negative, namely for pollution estimation: high-pollution, uncertain and low-pollution. In addition to the exploration of SVM variants, we propose an innovative active sample selection strategy based on the uncertainty criterion. This strategy is designed to systematically reduce uncertainties in pollution assessment, thus providing adaptability to dynamic environmental changes. An incremental SVM with an uncertainty option is introduced to further optimise the sample selection process. Furthermore, the decision-making process is refined through the introduction of a novel three-hinge loss. The corresponding optimization problem and its resolution allow for a more nuanced contamination assessment with multiple levels of estimation, providing a valuable tool for characterising contamination levels with increased granularity. Extensive experiments on synthetic and real data validate the proposed methodology. Synthetic data simulations assess the quality of the approach, while real data from a two-dimensional porosity measurement demonstrate practical applicability. This research contributes to the advancement of pollution assessment methodologies, providing an adaptable solution for environmental monitoring.},
  archive      = {J_EXSY},
  author       = {Chen Xiong and Paul Honeine and Maxime Berar and Antonin van Exem},
  doi          = {10.1111/exsy.13747},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13747},
  shortjournal = {Expert Syst.},
  title        = {Support vector machines with uncertainty option and incremental sampling for kriging},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KDBI special issue: Time-series pattern verification in CNC turning—A comparative study of one-class and binary classification. <em>EXSY</em>, <em>42</em>(2), e13745. (<a href='https://doi.org/10.1111/exsy.13745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Industry 4.0 and Quality 4.0 optimises manufacturing through IoT and ML, improving processes and product quality. The primary challenge involves identifying patterns in computer numerical control (CNC) machining time-series data to boost manufacturing quality control. The proposed solution involves an experimental study comparing one-class and binary classification algorithms. This study aims to classify time-series data from CNC turning machines, offering insight into monitoring and adjusting tool wear to maintain product quality. The methodology entails extracting spectral features from time-series data to train both one-class and binary classification algorithms, assessing their effectiveness and computational efficiency. Although certain models consistently outperform others, determining the best performing is not possible, as a trade-off between classification and computational performance is observed, with gradient boosting standing out for effectively balancing both aspects. Thus, the choice between one-class and binary classification ultimately relies on dataset's features and task objectives.},
  archive      = {J_EXSY},
  author       = {João Pinto da Silva and Ana Rita Nogueira and José Pinto and Manuel Curral and António Correia Alves and Ricardo Sousa},
  doi          = {10.1111/exsy.13745},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13745},
  shortjournal = {Expert Syst.},
  title        = {KDBI special issue: Time-series pattern verification in CNC turning—A comparative study of one-class and binary classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous graph contrastive learning with adaptive data augmentation for semi-supervised short text classification. <em>EXSY</em>, <em>42</em>(2), e13744. (<a href='https://doi.org/10.1111/exsy.13744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text classification has been widely used in many fields. Due to the scarcity of labelled data, implementing short text classification under semi-supervised learning setting has become increasingly popular. Semi-supervised short text classification methods based on graph neural networks can achieve state-of-the-art classification performance by utilizing the expressive power of graph neural networks. However, these methods usually fail to mine the hidden patterns of a large amount of short text node data in the graph to optimize the short text node embeddings, which limits the semantic representation power of the short texts, thus leading to suboptimal classification performance. To overcome the limitation, this paper proposes a novel semi-supervised short text classification method called the Heterogeneous Graph Contrastive Learning with Adaptive Data Augmentation (HGCLADA). In the knowledge bases guided soft prompt-based data augmentation component, the related words of the tag words are used to optimize the soft prompts for generating diverse augmented samples. In the heterogeneous graph contrastive learning framework component, a heterogeneous graph that is constructed using short texts and keywords and an effective edge augmentation scheme based on a short text clustering algorithm are proposed. The optimized short text embeddings can be obtained to achieve the effective semi-supervised short text classification. Extensive experiments on six benchmark datasets show that our HGCLADA method outperforms four classes of state-of-the-art methods in terms of classification accuracy, especially with significant performance improvements of 8.74% on the TagMyNews dataset when each class only contains 20 labelled data.},
  archive      = {J_EXSY},
  author       = {Mingqiang Wu and Zhuoming Xu and Lei Zheng},
  doi          = {10.1111/exsy.13744},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13744},
  shortjournal = {Expert Syst.},
  title        = {Heterogeneous graph contrastive learning with adaptive data augmentation for semi-supervised short text classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing healthcare systems: A tri-tier architecture by using data communication, AI data generative and regulation and compliance standards. <em>EXSY</em>, <em>42</em>(2), e13742. (<a href='https://doi.org/10.1111/exsy.13742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional healthcare systems have suffered from different data communication, security, data processing, and compliance issues. The traditional systems are also not well equipped to handle the new technologies like Artificial Intelligence (AI) by enabling more accurate diagnostics, personalized treatment plans, and improved patient outcomes. The existing data communication and security protocols and compliance are also not fully implemented to tackle the system's challenges. This article proposes a Tri-Tier architecture by using data communication, AI data generative, and regulation and compliance tiers. The data communication tier is based on advanced sensing and monitoring technologies like cloud and edge-based systems integrated with security detection mechanisms. The edge and cloud layer provides the all functions of the perception layer like smart sensing, visual sensing, and monitoring services, and can control the device's perception and behaviour. The second tier provides the AI data generative functionalities to handle real-time synthetic medical images for predictive analytics to enhance patient care. This tier also automates routine tasks, such as administrative work and data analysis, which can free up healthcare professionals to focus on more complex tasks. The last regulation and compliance tier is responsible for handling the standards and compliance for healthcare systems. Experiments are conducted to test the data communication and security level of the proposed architecture. The results showed the suitability of existing solutions and synchronization with the proposed architecture.},
  archive      = {J_EXSY},
  author       = {Kashif Naseer Qureshi and Hanaa Nafea and Pyoungwon Kim},
  doi          = {10.1111/exsy.13742},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13742},
  shortjournal = {Expert Syst.},
  title        = {Advancing healthcare systems: A tri-tier architecture by using data communication, AI data generative and regulation and compliance standards},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep learning models with data fusion approach for electricity load forecasting. <em>EXSY</em>, <em>42</em>(2), e13741. (<a href='https://doi.org/10.1111/exsy.13741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the application of deep learning in forecasting electricity consumption. Initially, we assess the performance of standard neural networks, such as convolutional neural networks (CNN) and long short-term memory (LSTM), along with basic methods like ARIMA and random forest, on a univariate electricity consumption data set. Subsequently, we develop hybrid models for a comprehensive multivariate data set created by merging weather and electricity data. These hybrid models demonstrate superior performance compared to individual models on the univariate data set. Our main contribution is the introduction of a novel hybrid data fusion model. This model integrates a single-model approach for univariate data, a hybrid model for multivariate data, and a linear regression model that processes the outputs from both. Our hybrid fusion model achieved an RMSE value of 0.0871 on the Chicago data set, outperforming other models such as Random Forest (0.2351), ARIMA (0.2184), CNN (0.1802), LSTM + LSTM (0.1496), and CNN + LSTM (0.1587). Additionally, our model surpassed the performance of our base transformer model. Furthermore, combining the best-performing transformer model, with a Gaussian Process model resulted in further improvement in performance. The Transformer + Gaussian model achieved an RMSE of 0.0768, compared with 0.0781 for the single transformer model. Similar trends were observed in the Pittsburgh and IHEC data sets.},
  archive      = {J_EXSY},
  author       = {Serkan Özen and Adnan Yazıcı and Volkan Atalay},
  doi          = {10.1111/exsy.13741},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13741},
  shortjournal = {Expert Syst.},
  title        = {Hybrid deep learning models with data fusion approach for electricity load forecasting},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative artificial intelligence and adversarial network for fraud detections in current evolutional systems. <em>EXSY</em>, <em>42</em>(2), e13740. (<a href='https://doi.org/10.1111/exsy.13740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article examines the impact of utilizing generative artificial intelligence optimizations in automating the content generation process. This instance involves the identification of fraudulent content, which is often characterized by dynamic patterns, in addition to content production. The generated contents are constrained, which limits their dimensionality. In this scenario, duplicated contents are eliminated from the automatic creations. Furthermore, the generated ratios are utilized to discover current patterns with minimized losses and errors, hence enhancing the accuracy of generative contents. Furthermore, while analysing the created patterns, we detect a significant discrepancy in lead durations, resulting in the generation of high scores for relevant information. In order to test the results using generative tools, the adversarial network codes are employed in four scenarios. These scenarios involve generating large patterns and reducing the dynamic patterns with an enhanced accuracy of 97% in the projected model. This is in contrast to the existing approach, which only provides a content accuracy of 77% after detecting fraud.},
  archive      = {J_EXSY},
  author       = {Shitharth Selvarajan and Hariprasath Manoharan and Adil O. Khadidos and Alaa O. Khadidos and Achyut Shankar and Carsten Maple and Suresh Singh},
  doi          = {10.1111/exsy.13740},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13740},
  shortjournal = {Expert Syst.},
  title        = {Generative artificial intelligence and adversarial network for fraud detections in current evolutional systems},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robotic frameless brain biopsy system enhanced by facial mesh registration. <em>EXSY</em>, <em>42</em>(2), e13739. (<a href='https://doi.org/10.1111/exsy.13739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a new approach is presented that eliminates stereotactic frameworks and the use of markers, offering an alternative to traditional brain biopsy systems. The classical biopsy operation involves the registration of magnetic resonance (MR) and computed tomography (CT) information taken from the patient at different times. Typically, the surgeon's planning information, which takes an average of 4 h on MR, is transferred to CT, and the surgical operation commences. However, this approach necessitates two separate acquisitions (MR and CT), adversely affecting patient comfort and increasing the workload. In the proposed system, it is recommended to register MR-Depth camera data instead of MR-CT registration. To achieve this, a 3D face pattern is obtained from the data received from the depth camera attached to the robot arm and overlapped with the mesh obtained by segmentation of the MR. It was observed that registration with sub-millimeter precision was achieved using the CMFreg surface registration technique.},
  archive      = {J_EXSY},
  author       = {Omur Aydogmus and Muhammed Fatih Talu},
  doi          = {10.1111/exsy.13739},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13739},
  shortjournal = {Expert Syst.},
  title        = {Robotic frameless brain biopsy system enhanced by facial mesh registration},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid random convolutional kernels model for price volatlity forecasting of precious metals. <em>EXSY</em>, <em>42</em>(2), e13738. (<a href='https://doi.org/10.1111/exsy.13738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precious metals are rare metals with high economic value. Forecasting the price volatility of precious metals is essential for investment purposes. In this work, we propose a novel hybrid model of random convolutional kernels-based neural network model (RCK) and generalized autoregressive conditional heteroscedasticity (GARCH) model for forecasting the metal price volatilities of gold, silver, and platinum. Realized volatility calculated on logarithmic returns is used as an estimate for the volatility of prices, and data standardization is performed before feeding the price volatility data to the RCK model. RCK model applies multiple carefully designed random convolution kernels on the time series input to extract robust features for forecasting. The proportion of positive values (PPV) is extracted as features from the output of convolving convolutional kernels with time-series inputs, which are then passed through a regressor to forecast volatility. Compared to the existing methods, the proposed method has the advantage that the weights of the random convolutional kernels need not be trained, unlike other neural network models. Further, no other work has made use of random convolutional kernels for precious metal forecasting, to the best of our knowledge. We incorporated novel learning and data augmentation strategies to achieve better performance. In particular, we used the cosine annealing learning rate strategy and Mixup data augmentation technique to improve the proposed model's performance. We have used MSE (Mean Squared Error), RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), and MAPE (Mean Absolute Percentage Error) as metrics to compare the proposed models' performance. The proposed model decreases the MSE by 53% compared to the GARCH-LSTM model, which is the current state-of-the-art hybrid model for volatility forecasting.},
  archive      = {J_EXSY},
  author       = {Siva Sai and Arun Kumar Giri and Vinay Chamola},
  doi          = {10.1111/exsy.13738},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13738},
  shortjournal = {Expert Syst.},
  title        = {A novel hybrid random convolutional kernels model for price volatlity forecasting of precious metals},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing healthcare data quality with optimal features driven mutual entropy gain. <em>EXSY</em>, <em>42</em>(2), e13737. (<a href='https://doi.org/10.1111/exsy.13737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the dynamic domain of healthcare data management, safeguarding sensitive information while ensuring data efficiency is always of the highest priority. Healthcare data are frequently mishandled, posing significant risks. This research offers a new network that assesses the quality of visual data using robust features-driven Mutual Entropy Gain (MEG). The proposed network addresses a critical gap in healthcare data management, significantly enhancing patient data security and operational efficiency in medical institutions. Our method begins with a thorough empirical investigation to find the optimal intermediate features for network input. We incorporate both distance entropy and probability entropy adopted and normalized in MEG, resulting in a comprehensive healthcare data quality evaluation. The results show that the network can distinguish between high-quality and low-quality data based on information content. Furthermore, our assessment reveals a large performance discrepancy between high and low-quality data, even with variable datasets. Notably, using only half of the data achieves commendable accuracy when compared with using the complete dataset, demonstrating possible efficiency gains. This breakthrough has far-reaching implications for healthcare providers, potentially reducing data storage costs, accelerating data processing times, and minimizing the risk of data breaches. In essence, our proposed network enhances efficiency and security in healthcare data and adapts to the evolving landscape of convergence ICT, paving the way for more robust, cost-effective, and secure healthcare information systems that can significantly improve patient care and operational outcomes.},
  archive      = {J_EXSY},
  author       = {Sushil Kumar Singh and Shailendrasinh Chauhan and Abdulrahman Alsafrani and Muhammad Islam and Hammad I. Sherazi and Inam Ullah},
  doi          = {10.1111/exsy.13737},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13737},
  shortjournal = {Expert Syst.},
  title        = {Optimizing healthcare data quality with optimal features driven mutual entropy gain},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the gravitation-based classification: A novel algorithm using equilibrium points for enhanced learning and dimensionality reduction. <em>EXSY</em>, <em>42</em>(2), e13736. (<a href='https://doi.org/10.1111/exsy.13736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept and effects of gravitation have been effectively utilized to design various data classification algorithms. Generally, there are two primary approaches to gravitation-based classification: one that relies on gravitational force and another that is based on gravitational potential energy. In this paper, we examine these two approaches and introduce a novel classification algorithm grounded in gravitational potential energy. The core idea of our approach is to identify an equilibrium point for a line mass (serving as the classifier line) situated between two groups of fixed point masses (representing two data classes). The equilibrium point of the classifier line is determined by minimizing the total gravitational potential energy resulting from the two groups of point masses. Notably, our method demonstrates the following: (i) it acts as a dimensionality reduction technique that seeks a new feature space with lower dimensionality for improved class discrimination by maximizing the sum of the logarithms of the projections, (ii) it leads to an information-theoretic learning strategy that minimizes the overall uncertainty of the classifier, and (iii) it offers a convex formulation that guarantees convergence to a global optimum solution. We also present experimental results that indicate the superior performance of the proposed method compared to existing techniques.},
  archive      = {J_EXSY},
  author       = {Mostafa Monemizadeh and Seyed Rouhollah Samareh Hashemi and Morteza Monemizadeh},
  doi          = {10.1111/exsy.13736},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13736},
  shortjournal = {Expert Syst.},
  title        = {On the gravitation-based classification: A novel algorithm using equilibrium points for enhanced learning and dimensionality reduction},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel triage framework for emergency department based on machine learning paradigm. <em>EXSY</em>, <em>42</em>(2), e13735. (<a href='https://doi.org/10.1111/exsy.13735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency departments, crucial in managing patient emergencies, are often challenged by overcrowding and diagnostic errors during triage—the process that assesses the urgency of patients' conditions. Traditional triage systems, heavily dependent on human judgement, are prone to errors like under-triage, where severe conditions are missed, delaying treatment, and over-triage, where less severe conditions are overly prioritized, causing unnecessary resource use and morbidity. This thesis presents a novel multi-model machine-learning framework designed to enhance triage accuracy by evaluating multiple medical conditions concurrently, including chronic illnesses like heart disease. The framework employs various machine-learning techniques—such as logistic regression, support vector machines, random forests, deep neural networks, and decision trees—to analyze different medical conditions in two comprehensive phases. In the first phase, patients' conditions are categorized, and a multi-tiered analysis using multiple classifiers refines the assessment by considering probabilistic outcomes from each classifier. The second phase synthesizes these insights into a unified and precise triage decision, distinguishing between patients who require critical care and those needing less urgent hospitalization. This integration of diverse machine learning models allows for a fused and precise triage decision, overcoming traditional triage systems' limitations that usually focus on single conditions and rely on isolated model predictions. A hybrid feature selection method is also utilized to identify critical predictors, enhancing the decision-making process. The framework is validated through a specially curated dataset that simulates multiple triage scenarios, evaluated by medical experts for efficacy. Comparative analysis with traditional triage methods demonstrates significant improvements in decision accuracy, as evidenced by higher area under curve (AUC) values—0.95 for critical care and 0.90 for hospitalization. The implementation of this data-driven approach substantially reduces human error, boosts operational efficiency, and aids medical staff in making rapid, informed decisions. This research represents a significant advancement in emergency medical care, illustrating the benefits of integrating sophisticated machine learning techniques to improve patient outcomes and resource management.},
  archive      = {J_EXSY},
  author       = {Alaa Mohammad Menshawi and Mohammad Mehedi Hassan},
  doi          = {10.1111/exsy.13735},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13735},
  shortjournal = {Expert Syst.},
  title        = {A novel triage framework for emergency department based on machine learning paradigm},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bias mitigation for fair automation of classification tasks. <em>EXSY</em>, <em>42</em>(2), e13734. (<a href='https://doi.org/10.1111/exsy.13734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incorporation of machine learning algorithms into high-risk decision-making tasks has raised some alarms in the scientific community. Research shows that machine learning-based technologies can contain biases that cause unfair decisions for certain population groups. The fundamental danger of ignoring this problem is that machine learning methods can not only reflect the biases present in our society but could also amplify them. This article presents the design and validation of a technology to assist the fair automation of classification problems. In essence, the proposal is based on taking advantage of the intermediate solutions generated during the resolution of classification problems through using Auto-ML tools, in particular, AutoGOAL, to create unbiased/fair classifiers. The technology employs a multi-objective optimization search to find the collection of models with the best trade-offs between performance and fairness. To solve the optimization problem, we introduce a combination of Probabilistic Grammatical Evolution Search and NSGA-II. The technology was evaluated using the Adult dataset from the UCI repository, a common benchmark in related research. Results were compared with other published results in scenarios with single and multiple fairness definitions. Our experiments demonstrate the technology's ability to automate classification tasks while incorporating fairness constraints. Additionally, our method achieves competitive results against other bias mitigation techniques. A notable advantage of our approach is its minimal requirement for machine learning expertise, thanks to its Auto-ML foundation. This makes the technology accessible and valuable for advancing fairness in machine learning applications. The source code is available online for the research community.},
  archive      = {J_EXSY},
  author       = {Juan Pablo Consuegra-Ayala and Yoan Gutiérrez and Yudivian Almeida-Cruz and Manuel Palomar},
  doi          = {10.1111/exsy.13734},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13734},
  shortjournal = {Expert Syst.},
  title        = {Bias mitigation for fair automation of classification tasks},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual embedded text summarizer system: A hybrid approach. <em>EXSY</em>, <em>42</em>(2), e13733. (<a href='https://doi.org/10.1111/exsy.13733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting crucial sentences from a document is a pivotal task in automatic text summarization systems. Abstractive summarization involves rephrasing key content through advanced natural language techniques, generating a concise, new text conveying critical information. Conversely, extractive summarization reproduces important material from the original text. In the proposed method, a hybrid ensemble approach combines BERTsum for extractive summarization and Longformer2Roberta for abstractive summarization for generating a contextual semantic rich summary for a huge collection of text. These proposed system-generated summaries were evaluated against reference summaries using the ROUGE package at three rouge levels (Rouge-1, Rouge-2, and Rouge-L). The Proposed contextual embedded hybrid text summarization model has shown significant performance improvement in multiple levels of Rouge score and word mover distance (WMD) of generated summary with a reference summary. The proposed hybrid model demonstrates superior performance over existing state-of-the-art summarizing models on three distinct datasets CNN dataset, WikiSum, and Gigaword dataset. The proposed hybrid model as a text summarizer involves leveraging its capabilities to process longer sequences of text with domain-specific contextual summaries. This transformers-based text summarization model has great potential in developing expert systems in various research domains such as health decision support systems, the education sector, customer support chatbots, financial analysis investment recommendations, and financial assistance.},
  archive      = {J_EXSY},
  author       = {Pooja Kherwa and Jyoti Arora and Tripti Sharma and Deepali Gupta and Sapna Juneja and Ghulam Muhammad and Ali Nauman},
  doi          = {10.1111/exsy.13733},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13733},
  shortjournal = {Expert Syst.},
  title        = {Contextual embedded text summarizer system: A hybrid approach},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TS-mixer: A lightweight text representation model based on context awareness. <em>EXSY</em>, <em>42</em>(2), e13732. (<a href='https://doi.org/10.1111/exsy.13732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pre-trained models (PTMs) have shown their powerful ability in multiple natural language processing tasks. However, using them in practical application remains a challenge due to the significant computational cost and memory requirements. In order to achieve the balance of computational cost and accuracy, MLP architecture can be used as an alternative to the self-attention module, such as pNLP-Mixer and Hyper-Mixer. Experiments indicate that, MLP-based models can attain competitive performance with low cost. They maintain the balance of computation cost and accuracy successfully, yet, this is at the expense of not being able to capture short-range dependencies. In this paper, a novel MLP-based model, termed TS-Mixer, is proposed which can capture local dependencies by shifting operation. Compared with other MLP-based models, the parameters of TS-Mixer are decoupled from the sequence length, hence it has a smaller model size in long sequence tasks. In addition, TS-Mixer has linear computational complexity, therefore it can be used as a lightweight alternative to the self-attention model. Experiments show that the TS-Mixer outperforms other MLP-based models, which achieves higher accuracy with fewer parameters in multiple downstream tasks. Notably, compared with pre-trained models, TS-Mixer can reach more than 90% of their accuracy with 1% or even one thousandth of the parameters (0.174 ~ 1.2 M). These results demonstrate that TS-Mixer can achieve a better balance between the computing resources and accuracy. Code is available at: https://github.com/wyl-privacy-project/TS-Mixer .},
  archive      = {J_EXSY},
  author       = {Huanling Tang and Yulin Wang and Yu Zhang and Quansheng Dou and Mingyu Lu},
  doi          = {10.1111/exsy.13732},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13732},
  shortjournal = {Expert Syst.},
  title        = {TS-mixer: A lightweight text representation model based on context awareness},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An agent-based model to simulate the public acceptability of social innovations. <em>EXSY</em>, <em>42</em>(2), e13731. (<a href='https://doi.org/10.1111/exsy.13731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The successful adoption of social innovations, such as renewable energy systems or pollution reduction plans in cities, depends, to a large extent, on the willingness and participation of the population in their development and implementation. We present an agent-based model (ABM) to analyze the process of citizen acceptability of a social innovation that uses a variety of agents to represent individual citizens and relevant groups of citizens. Citizen agents make use of the HUMAT cognitive decision-making model, based on psychosocial theories, to decide on their support for the social innovation considering how their needs will be satisfied if they decide to support (or not) the innovation project, and the influence exerted by the agents in their environment. The ABM was initially developed to represent the urban and transport planning superblock project in the city of Vitoria-Gasteiz (Spain). The ABM simulations make it possible to study the evolution of public acceptance of social innovation, with the results providing insights to the social dynamics and individual factors that affect the acceptance of the project, enabling an evaluation of how to devise new policies that increase public acceptance. Sufficiently generic to be easily adaptable to different types of social innovations, the ABM is a powerful tool to explore different scenarios and design strategies that foster the acceptance and sustainable adoption of social innovations.},
  archive      = {J_EXSY},
  author       = {Alejandro Rodríguez-Arias and Noelia Sánchez-Maroño and Bertha Guijarro-Berdiñas and Amparo Alonso-Betanzos and Isabel Lema-Blanco and Adina Dumitru},
  doi          = {10.1111/exsy.13731},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13731},
  shortjournal = {Expert Syst.},
  title        = {An agent-based model to simulate the public acceptability of social innovations},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sliding window based adaptative fuzzy measure for edge detection. <em>EXSY</em>, <em>42</em>(2), e13730. (<a href='https://doi.org/10.1111/exsy.13730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we explore the impact of adaptive fuzzy measures on edge detection, aiming to enhance how computers interpret images by identifying edges more accurately. Traditional methods rely on analysing changes in image brightness to locate edges, but they often use fixed rules that do not account for the unique characteristics of each image. Our approach differs by adjusting fuzzy measures based on the information within specific areas of an image under a sliding window approach, utilizing a variety of fusion functions and generalizations of the Choquet integral to analyse and combine pixel data. The proposed method is flexible, allowing for the adaptation of measures in response to the image's local features. We put our method to the test against the well-established Canny edge detector to evaluate its effectiveness. Our experimental results suggest that by adapting fuzzy measures for each image section, we can improve edge detection results.},
  archive      = {J_EXSY},
  author       = {Cedric Marco-Detchart and Giancarlo Lucca and Miquéias Amorim Santos Silva and Jaime A. Rincon and Vicente Julian and Graçaliz Dimuro},
  doi          = {10.1111/exsy.13730},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13730},
  shortjournal = {Expert Syst.},
  title        = {Sliding window based adaptative fuzzy measure for edge detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MTFDN: An image copy-move forgery detection method based on multi-task learning. <em>EXSY</em>, <em>42</em>(2), e13729. (<a href='https://doi.org/10.1111/exsy.13729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image copy-move forgery, where an image region is copied and pasted within the same image, is a simple yet widely employed manipulation. In this paper, we rethink copy-move forgery detection from the perspective of multi-task learning and summarize two characteristics of this problem: (1) Homology and (2) Manipulated traces. Consequently, we propose a multi-task forgery detection network (MTFDN) for image copy-move forgery localization and source/target distinguishment. The network consists of a hard-parameter sharing feature extractor, global forged homology detection (GFHD) and local manipulated trace detection (LMTD) modules. The difference of feature distribution between the GFHD module and the LMTD module is significantly reduced by sharing parameters. Experimental results on several benchmark copy-move forgery datasets demonstrate the effectiveness of our proposed MTFDN.},
  archive      = {J_EXSY},
  author       = {Peng Liang and Hang Tu and Amir Hussain and Ziyuan Li},
  doi          = {10.1111/exsy.13729},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13729},
  shortjournal = {Expert Syst.},
  title        = {MTFDN: An image copy-move forgery detection method based on multi-task learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STP-CNN: Selection of transfer parameters in convolutional neural networks. <em>EXSY</em>, <em>42</em>(2), e13728. (<a href='https://doi.org/10.1111/exsy.13728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, transfer learning has shown promising results in many applications. However, most deep transfer learning methods such as parameter sharing and fine-tuning are still suffering from the lack of parameters transmission strategy. In this paper, we propose a new optimization model for parameter-based transfer learning in convolutional neural networks named STP-CNN. Indeed, we propose a Lasso transfer model supported by a regularization term that controls transferability. Moreover, we opt for the proximal gradient descent method to solve the proposed model. The suggested technique allows, under certain conditions, to control exactly which parameters, in each convolutional layer of the source network, which will be used directly or adjusted in the target network. Several experiments prove the performance of our model in locating the transferable parameters as well as improving the data classification.},
  archive      = {J_EXSY},
  author       = {Otmane Mallouk and Nour-Eddine Joudar and Mohamed Ettaouil},
  doi          = {10.1111/exsy.13728},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13728},
  shortjournal = {Expert Syst.},
  title        = {STP-CNN: Selection of transfer parameters in convolutional neural networks},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced monte-carlo tree search for dynamic flexible job shop scheduling with transportation time constraints. <em>EXSY</em>, <em>42</em>(2), e13727. (<a href='https://doi.org/10.1111/exsy.13727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing uncertainty and complexity of industrial production, dynamic events are inevitably happened during the production process. This paper deals with the transportation time constrained dynamic flexible job shop scheduling problem with machine breakdown, new jobs arrival, jobs cancellation, and processing time change of operations. Considering the actual production situation, a makespan rescheduling model based on transportation time is established. A Monte-Carlo tree search based algorithm is proposed to solve the dynamic flexible job shop scheduling problem such that the makespan is minimized. In order to improve the accuracy of the developed algorithm and the quality of the obtained scheduling schemes, subtree keeping policy, multi-branching simulation, reinforcement learning, greedy strategy, and expectation-best evaluation method are employed. Moreover, a subtree pruning policy is embedded to improve the search efficiency of the developed algorithm. Simulation experiments are conducted on a series of instances of different sizes to compare the developed approach with the GA-based scheduling approach and the Monte-Carlo tree search based approach. The simulation results indicate that the developed approach is superior to the existing scheduling approaches in terms of the qualities of the obtained solutions.},
  archive      = {J_EXSY},
  author       = {Zhou He and Biao Tang and Chan Gu and Ning Ran},
  doi          = {10.1111/exsy.13727},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13727},
  shortjournal = {Expert Syst.},
  title        = {Enhanced monte-carlo tree search for dynamic flexible job shop scheduling with transportation time constraints},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on deep learning-based intrusion detection systems in internet of things (IoT). <em>EXSY</em>, <em>42</em>(2), e13726. (<a href='https://doi.org/10.1111/exsy.13726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferating popularity of Internet of Things (IoT) devices has led to wide-scale networked system implementations across multiple disciplines, including transportation, medicine, smart homes, and many others. This unprecedented level of interconnectivity has introduced new security vulnerabilities and threats. Ensuring security in these IoT settings is crucial for protecting against malicious activities and safeguarding data. Real-time identification and response to potential intrusions and attacks are essential, and intrusion detection systems (IDS) are pivotal in this process. However, the dynamic and diverse nature of the IoT environment presents significant challenges to existing IDS solutions, which are often based on rule-based or statistical approaches. Deep learning, a subset of artificial intelligence, has shown great potential to enhance IDS in IoT. Deep learning models can identify complex patterns and characteristics by utilizing artificial neural networks, automatically building hierarchical representations from data. This capability results in more precise and efficient intrusion detection in IoT-based systems. The primary aim of this survey is to present an extensive overview of the current research on deep learning and IDS in the IoT domain. By examining existing literature, discussing mainstream datasets, and highlighting current challenges and potential prospects, this survey provides valuable insights into the prevailing scenario and future directions for using deep learning in IDS for IoT. The findings from this research aim to enhance intrusion detection techniques in IoT environments and promote the development of more effective antimalware solutions against cyber threats targeting IoT device systems.},
  archive      = {J_EXSY},
  author       = {Qasem Abu Al-Haija and Ayat Droos},
  doi          = {10.1111/exsy.13726},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13726},
  shortjournal = {Expert Syst.},
  title        = {A comprehensive survey on deep learning-based intrusion detection systems in internet of things (IoT)},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of stock prices with automated reinforced learning algorithms. <em>EXSY</em>, <em>42</em>(2), e13725. (<a href='https://doi.org/10.1111/exsy.13725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting stock price movements remains a major challenge in time series analysis. Despite extensive research on various machine learning techniques, few models have consistently achieved success in automated stock trading. One of the main challenges in stock price forecasting is that the optimal model changes over time due to market dynamics. This paper aims to predict stock prices using automated reinforcement learning algorithms and to analyse their efficiency compared with conventional methods. We automate DQN models and their variants, known for their adaptability, by continuously retraining them using recent data to capture market dynamics. We demonstrate that our dynamic models improve the accuracy of predicting the directions of various DAX stocks from 50.00% to approximately 60.00%, compared with conventional methods. Additionally, we conclude that dynamic models should be updated in response to shifts rather than at fixed intervals.},
  archive      = {J_EXSY},
  author       = {Said Yasin and Adrian Paschke and Jamal Al Qundus},
  doi          = {10.1111/exsy.13725},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13725},
  shortjournal = {Expert Syst.},
  title        = {Prediction of stock prices with automated reinforced learning algorithms},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label distribution learning for compound facial expression recognition in-the-wild: A comparative study. <em>EXSY</em>, <em>42</em>(2), e13724. (<a href='https://doi.org/10.1111/exsy.13724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human emotional states encompass both basic and compound facial expressions. However, current works primarily focus on basic expressions, consequently neglecting the broad spectrum of human emotions encountered in practical scenarios. Compound facial expressions involve the simultaneous manifestation of multiple emotions on an individual's face. This phenomenon reflects the complexity and richness of human states, where facial features dynamically convey a combination of feelings. This study embarks on a pioneering exploration of Compound Facial Expression Recognition (CFER), with a distinctive emphasis on leveraging the Label Distribution Learning (LDL) paradigm. This strategic application of LDL aims to address the ambiguity and complexity inherent in compound expressions, marking a significant departure from the dominant Single Label Learning (SLL) and Multi-Label Learning (MLL) paradigms. Within this framework, we rigorously investigate the potential of LDL for a critical challenge in Facial Expression Recognition (FER): recognizing compound facial expressions in uncontrolled environments. We utilize the recently introduced RAF-CE dataset, meticulously designed for compound expression assessment. By conducting a comprehensive comparative analysis pitting LDL against conventional SLL and MLL approaches on RAF-CE, we aim to definitively establish LDL's superiority in handling this complex task. Furthermore, we assess the generalizability of LDL models trained on RAF-CE by evaluating their performance on the EmotioNet and RAF-DB Compound datasets. This demonstrates their effectiveness without domain adaptation. To solidify these findings, we conduct a comprehensive comparative analysis of 12 cutting-edge LDL algorithms on RAF-CE, S-BU3DFE, and S-JAFFE datasets, providing valuable insights into the most effective LDL techniques for FER in-the-wild.},
  archive      = {J_EXSY},
  author       = {Afifa Khelifa and Haythem Ghazouani and Walid Barhoumi},
  doi          = {10.1111/exsy.13724},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13724},
  shortjournal = {Expert Syst.},
  title        = {Label distribution learning for compound facial expression recognition in-the-wild: A comparative study},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing anomaly detection in cloud environments with cutting-edge generative AI for expert systems. <em>EXSY</em>, <em>42</em>(2), e13722. (<a href='https://doi.org/10.1111/exsy.13722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) continues to advance, Generative AI emerges as a transformative force, capable of generating novel content and revolutionizing anomaly detection methodologies. This paper presents CloudGEN, a pioneering approach to anomaly detection in cloud environments by leveraging the potential of Generative Adversarial Networks (GANs) and Convolutional Neural Network (CNN). Our research focuses on developing a state-of-the-art Generative AI-based anomaly detection system, integrating GANs, deep learning techniques, and adversarial training. We explore unsupervised generative modelling, multi-modal architectures, and transfer learning to enhance expert systems' anomaly detection systems. We illustrate our approach by dissecting anomalies regarding job performance, network behaviour, and resource utilization in cloud computing environments. The experimental results underscore a notable surge in anomaly detection accuracy with significant development of approximately 11%.},
  archive      = {J_EXSY},
  author       = {Umit Demirbaga},
  doi          = {10.1111/exsy.13722},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13722},
  shortjournal = {Expert Syst.},
  title        = {Advancing anomaly detection in cloud environments with cutting-edge generative AI for expert systems},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How does energy transition improve energy utilization efficiency? a case study of china's coal-to-gas program. <em>EXSY</em>, <em>42</em>(2), e13721. (<a href='https://doi.org/10.1111/exsy.13721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving energy efficiency by adjusting the structure of energy consumption types is of great significance for reducing carbon emissions in the short term. The present paper constructs new data envelopment analysis models for evaluating energy utilization under different structural conditions and calculating potential emissions reductions. We conducted empirical research on 30 provinces in China from 2003 to 2019—a time frame that coincides with the instituting of China's “coal-to-gas” program. Our results show that technological progress is the main way for China to reduce carbon emissions and that it is possible to reduce the total amount of carbon emissions by 35%. Additionally, optimizing the energy consumption structure following the coal-to-gas program guidelines could reduce the country's carbon emissions by a further 25%. Finally, this paper provides specific policy recommendations based on the efficiency analysis results to guide each province in reducing carbon emissions under the conditions of energy demand growth.},
  archive      = {J_EXSY},
  author       = {Zhixiang Zhou and Yifei Zhu and Yannan Li and Huaqing Wu},
  doi          = {10.1111/exsy.13721},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13721},
  shortjournal = {Expert Syst.},
  title        = {How does energy transition improve energy utilization efficiency? a case study of china's coal-to-gas program},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative evaluation of large language models using key metrics and emerging tools. <em>EXSY</em>, <em>42</em>(2), e13719. (<a href='https://doi.org/10.1111/exsy.13719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research involved designing and building an interactive generative AI application to conduct a comparative analysis of two advanced Large Language Models (LLMs), GPT-4, and Claude 2, using Langsmith evaluation tools. The project was developed to explore the potential of LLMs in facilitating postgraduate course recommendations within a simulated environment at Munster Technological University (MTU). Designed for comparative analysis, the application enables testing of GPT-4 and Claude 2 and can be hosted flexibly on either Amazon Web Services (AWS) or Azure. It utilizes advanced natural language processing and retrieval-augmented generation (RAG) techniques to process proprietary data tailored to postgraduate needs. A key component of this research was the rigorous assessment of the LLMs using the Langsmith evaluation tool against both customized and standard benchmarks. The evaluation focused on metrics such as bias, safety, accuracy, cost, robustness, and latency. Additionally, adaptability covering critical features like language translation and internet access, was independently researched since the Langsmith tool does not evaluate this metric. This ensures a holistic assessment of the LLM's capabilities.},
  archive      = {J_EXSY},
  author       = {Sarah McAvinue and Kapal Dev},
  doi          = {10.1111/exsy.13719},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13719},
  shortjournal = {Expert Syst.},
  title        = {Comparative evaluation of large language models using key metrics and emerging tools},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EmoiPlanner: Human emotion and intention aware socially acceptable robot navigation in human-centric environments. <em>EXSY</em>, <em>42</em>(2), e13718. (<a href='https://doi.org/10.1111/exsy.13718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of robots in human-centric environments has significantly increased in recent years. It is crucial for robots to navigate human environments while understanding social norms and personal boundaries to ensure a harmonious coexistence between humans and robots. A socially aware robot should be capable of interpreting and responding appropriately to human cues, expressions, and intentions, thereby fostering trust and confidence among humans. However, prior studies were insufficient or unable to address the navigation challenges in human-populated environments, as they perceive people as obstacles rather than social agents. Recent studies have utilized proxemic areas that are present in interpersonal interactions for human-robot interaction scenarios, but they have enforced consistent proxemic areas for social robot navigation. This approach fails to fully capture the highly sophisticated behaviour and preferences of humans. Therefore, we propose a psychologically-based adaptive proxemic area that fluctuates based on the human's emotional state. Furthermore, we integrate this feature into a reinforcement learning-based social navigation framework, making our navigation framework robust to the unpredictable affections of humans. Additionally, our navigation framework includes human intention prediction to approximate the future proxemic area, thereby avoiding interference with the path to be taken by individuals. We have named our framework the Human Emotion and Intention Aware Path Planner (EmoiPlanner). Our framework has been subjected to case studies involving realistic crowd navigation scenarios, and the results indicate that it enables robots to navigate through crowds without causing discomfort to pedestrians who exhibit stochastic behaviours and emotional states, while also ensuring efficient path planning.},
  archive      = {J_EXSY},
  author       = {Weiwei Yu and Siong Yuen Kok and Gautam Srivastava},
  doi          = {10.1111/exsy.13718},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13718},
  shortjournal = {Expert Syst.},
  title        = {EmoiPlanner: Human emotion and intention aware socially acceptable robot navigation in human-centric environments},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing task allocation with temporal-spatial privacy protection in mobile crowdsensing. <em>EXSY</em>, <em>42</em>(2), e13717. (<a href='https://doi.org/10.1111/exsy.13717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Crowdsensing (MCS) is considered to be a key emerging example of a smart city, which combines the wisdom of dynamic people with mobile devices to provide distributed, ubiquitous services and applications. In MCS, each worker tends to complete as many tasks as possible within the limited idle time to obtain higher income, while completing a task may require the worker to move to the specific location of the task and perform continuous sensing. Thus the time and location information of each worker is necessary for an efficient task allocation mechanism. However, submitting the time and location information of the workers to the system raises several privacy concerns, making it significant to protect both the temporal and spatial privacy of workers in MCS. In this article, we propose the Task Allocation with Temporal-Spatial Privacy Protection (TASP) problem, aiming to maximize the total worker income to further improve the workers' motivation in executing tasks and the platform's utility, which is proved to be NP-hard. We adopt differential privacy technology to introduce Laplace noise into the location and time information of workers, after which we propose the Improved Genetic Algorithm (SPGA) and the Clone-Enhanced Genetic Algorithm (SPCGA), to solve the TASP problem. Experimental results on two real-world datasets verify the effectiveness of the proposed SPGA and SPCGA with the required personalized privacy protection.},
  archive      = {J_EXSY},
  author       = {Yuping Liu and Honglong Chen and Xiaolong Liu and Wentao Wei and Huansheng Xue and Osama Alfarraj and Zafer Almakhadmeh},
  doi          = {10.1111/exsy.13717},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13717},
  shortjournal = {Expert Syst.},
  title        = {Optimizing task allocation with temporal-spatial privacy protection in mobile crowdsensing},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning-driven dual blockchain for data sharing and reputation management in internet of medical things. <em>EXSY</em>, <em>42</em>(2), e13714. (<a href='https://doi.org/10.1111/exsy.13714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Medical Things (IoMT), the vulnerability of federated learning (FL) to single points of failure, low-quality nodes, and poisoning attacks necessitates innovative solutions. This article introduces a FL-driven dual-blockchain approach to address these challenges and improve data sharing and reputation management. Our approach comprises two blockchains: the Model Quality Blockchain (MQchain) and the Reputation Incentive Blockchain (RIchain). MQchain utilizes an enhanced Proof of Quality (PoQ) consensus algorithm to exclude low-quality nodes from participating in aggregation, effectively mitigating single points of failure and poisoning attacks by leveraging node reputation and quality thresholds. In parallel, RIchain incorporates a reputation evaluation, incentive mechanism, and index query mechanism, allowing for rapid and comprehensive node evaluation, thus identifying high-reputation nodes for MQchain. Security analysis confirms the theoretical soundness of the proposed method. Experimental evaluation using real medical datasets, specifically MedMNIST, demonstrates the remarkable resilience of our approach against attacks compared to three alternative methods.},
  archive      = {J_EXSY},
  author       = {Chenquan Gan and Xinghai Xiao and Qingyi Zhu and Deepak Kumar Jain and Akanksha Saini and Amir Hussain},
  doi          = {10.1111/exsy.13714},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13714},
  shortjournal = {Expert Syst.},
  title        = {Federated learning-driven dual blockchain for data sharing and reputation management in internet of medical things},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intent detection for task-oriented conversational agents: A comparative study of recurrent neural networks and transformer models. <em>EXSY</em>, <em>42</em>(2), e13712. (<a href='https://doi.org/10.1111/exsy.13712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational assistants (CAs) and Task-oriented ones, in particular, are designed to interact with users in a natural language manner, assisting them in completing specific tasks or providing relevant information. These systems employ advanced natural language understanding (NLU) and dialogue management techniques to comprehend user inputs, infer their intentions, and generate appropriate responses or actions. Over time, the CAs have gradually diversified to today touch various fields such as e-commerce, healthcare, tourism, fashion, travel, and many other sectors. NLU is fundamental in the natural language processing (NLP) field. Identifying user intents from natural language utterances is a sub-task of NLU that is crucial for conversational systems. The diversity in user utterances makes intent detection (ID) even a challenging problem. Recently, with the emergence of Deep Neural Networks. New State of the Art (SOA) results have been achieved for different NLP tasks. Recurrent neural networks (RNNs) and Transformer architectures are two major players in those improvements. RNNs have significantly contributed to sequence modelling across various application areas. Conversely, Transformer models represent a newer architecture leveraging attention mechanisms, extensive training data sets, and computational power. This review paper begins with a detailed exploration of RNN and Transformer models. Subsequently, it conducts a comparative analysis of their performance in intent recognition for Task-oriented (CAs). Finally, it concludes by addressing the main challenges and outlining future research directions.},
  archive      = {J_EXSY},
  author       = {Mourad Jbene and Abdellah Chehri and Rachid Saadane and Smail Tigani and Gwanggil Jeon},
  doi          = {10.1111/exsy.13712},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13712},
  shortjournal = {Expert Syst.},
  title        = {Intent detection for task-oriented conversational agents: A comparative study of recurrent neural networks and transformer models},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection preference and effectiveness quantification of provincial energy security policies in china. <em>EXSY</em>, <em>42</em>(2), e13711. (<a href='https://doi.org/10.1111/exsy.13711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy security constitutes a pivotal determinant in safeguarding the seamless functioning of economies. This research endeavours to shed light on the underlying predilections and potential scopes for enhancement within China's provincial energy security policies. By delving into an array of policy documents procured from the esteemed Legal Information Network of Peking University, it offers a meticulous exploration. Employing sophisticated text analysis methodologies, the study constructs a two-tier analytical framework, meticulously encapsulating both the policy instruments employed and the intricate processes of their execution. Leveraging the power of Nvivo 12 Plus software, pertinent policy contents are systematically coded, with those aligning with the defined analytical dimensions aggregated for frequency computations. Furthermore, a Policy Measurement and Categorization (PMC) index model is devised, harnessing word frequency statistical data to assign a quantitative assessment to the policies under scrutiny. The empirical results demonstrate a noteworthy disparity in the adoption of policy tools among various provinces, with command-and-control mechanisms, economic incentive structures, and societal engagement strategies emerging as the most recurrent policy types. Among the energy security policies scrutinized, approximately 84.85% were categorized as effective, while a smaller yet significant portion, 6.06%, was classified as outstanding. Despite the overall robustness of China's provincial energy security policies, the investigation identifies several avenues for further refinement. The study suggests that the government could bolster these measures through intensified focus on transformative adjustments to energy structures, augmentation of green loan guarantee systems, and fostering enhanced inter-sectoral collaboration. These strategic enhancements may serve as key levers to propel China's provincial energy security policies towards even greater effectiveness and resilience.},
  archive      = {J_EXSY},
  author       = {Liangpeng Wu and Yujing Tang and Qingyuan Zhu},
  doi          = {10.1111/exsy.13711},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13711},
  shortjournal = {Expert Syst.},
  title        = {Selection preference and effectiveness quantification of provincial energy security policies in china},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trajectory mapping and future charting of hydrogen energy policy: A systematic review. <em>EXSY</em>, <em>42</em>(2), e13696. (<a href='https://doi.org/10.1111/exsy.13696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article conducts a systematic mapping and inductive analysis of existing work related to hydrogen energy policy in the Web of Science Core Collection from 1996 to 7 July 2023. First, based on bibliometrics, the research reveals the publication volume trends, influential contributors (countries, authors, organizations and journals). Second, bibliometric and content analysis were applied to vividly demonstrate the evolution of the research topic, discuss leading research topics, and provide valuable insights into issues that deserving appropriate attention in the future. The unequivocal role of policies in supporting the development of hydrogen energy is undeniable. In the future, the focal point of hydrogen energy policies should be concentrated on stimulating the creation of demand for green hydrogen, necessitating the effective evaluation of policy outcomes and ensuring the safety of hydrogen energy. Furthermore, the application scope of hydrogen energy extends beyond the transportation sector, holding potential for expansion into other high carbon-emitting domains. With the strengthening of international collaboration in hydrogen energy, considerations of energy justice and fairness are poised to become pivotal factors in cooperation, exerting a profound influence on the attainment of long-term development and environmental sustainability. These critical research directions will shape the future landscape of hydrogen energy policy and serve as an essential resource for policymakers, researchers, and stakeholders in this domain.},
  archive      = {J_EXSY},
  author       = {Shuhan Meng and Xianhua Wu and Hua Li and You Wu},
  doi          = {10.1111/exsy.13696},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13696},
  shortjournal = {Expert Syst.},
  title        = {Trajectory mapping and future charting of hydrogen energy policy: A systematic review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DemocracyGuard: Blockchain-based secure voting framework for digital democracy. <em>EXSY</em>, <em>42</em>(2), e13694. (<a href='https://doi.org/10.1111/exsy.13694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online voting is gaining traction in contemporary society to reduce costs and boost voter turnout, allowing individuals to cast their ballots from anywhere with an internet connection. This innovation is cautiously met due to the inherent security risks, where a single vulnerability can lead to widespread vote manipulation. Blockchain technology has emerged as a promising solution to address these concerns and create a trustworthy electoral process. Blockchain offers a decentralized network of nodes that enhances transparency, security, and verifiability. Its distributed ledger and non-repudiation features make it a compelling alternative to traditional electronic voting systems, ensuring the integrity of elections. To further bolster the security of online voting, we propose DemocracyGuard platform on the Ethereum blockchain, which incorporates facial recognition technology to authenticate voters. By leveraging these advancements, DemocracyGuard aims to provide a secure and resilient platform for online voting, paving the way for its broader adoption and revolutionizing the electoral landscape.},
  archive      = {J_EXSY},
  author       = {Mritunjay Shall Peelam and Gaurav Kumar and Kunjan Shah and Vinay Chamola},
  doi          = {10.1111/exsy.13694},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13694},
  shortjournal = {Expert Syst.},
  title        = {DemocracyGuard: Blockchain-based secure voting framework for digital democracy},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient malware detection using hybrid approach of transfer learning and generative adversarial examples with image representation. <em>EXSY</em>, <em>42</em>(2), e13693. (<a href='https://doi.org/10.1111/exsy.13693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying malicious intent within a program, also known as malware, is a critical security task. Many detection systems remain ineffective due to the persistent emergence of zero-day variants, despite the pervasive use of antivirus tools for malware detection. The application of generative AI in the realm of malware visualization, particularly when binaries are depicted as colour visuals, represents a significant advancement over traditional machine-learning approaches. Generative AI generates various samples, minimizing the need for specialized knowledge and time-consuming analysis, hence boosting zero-day attack detection and mitigation. This paper introduces the Deep Convolutional Generative Adversarial Network for Zero-Shot Learning (DCGAN-ZSL), leveraging transfer learning and generative adversarial examples for efficient malware classification. First, a normalization method is proposed, resizing malicious images to 128 × 128 or 300 × 300 for standardized input, enhancing feature transformation for improved malware pattern recognition. Second, greyscale representations are converted into colour images to augment feature extraction, providing a richer input for enhanced model performance in malware classification. Third, a novel DCGAN with progressive training improves model stability, mode collapse, and image quality, thus advancing generative model training. We apply the Attention ResNet-based transfer learning method to extract texture features from generated samples, which increases security evaluation performance. Finally, the ZSL for zero-day malware presents a novel method for identifying previously unknown threats, indicating a significant advancement in cybersecurity. The proposed approach is evaluated using two standard datasets, namely dumpware and malimg, achieving malware classification accuracies of 96.21% and 98.91%, respectively.},
  archive      = {J_EXSY},
  author       = {Yue Zhao and Farhan Ullah and Chien-Ming Chen and Mohammed Amoon and Saru Kumari},
  doi          = {10.1111/exsy.13693},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13693},
  shortjournal = {Expert Syst.},
  title        = {Efficient malware detection using hybrid approach of transfer learning and generative adversarial examples with image representation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater image enhancement using contrast correction. <em>EXSY</em>, <em>42</em>(2), e13692. (<a href='https://doi.org/10.1111/exsy.13692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light-induced degeneration of underwater images occurs by physical features of seawater. According to the wavelength of the colour spectrum, light reduces intensity significantly when it moves through water. The greatest wavelength of light that is visible gets absorbed first. Red and blue absorb the most and least, respectively. Because of the reducing consequences of the light spectrum, underwater images having poor contrast can be obtained. As a result, the crucial data contained inside these images will not be effectively retrieved for later analysis. The recent research suggests a novel approach to enhance the contrast while decreasing noise in underwater images. The recommended approach involves image histogram transformation using two significant colour spaces, Red-Green-Blue (RGB) and Hue-Saturation-Value (HSV). The histogram of the dominant colour channel (blue channel) in the RGB colour model is extended towards the lower level, containing a maximum limitation of 95%, while the inferior red colour channel has been extended towards the upper side, containing a minimum limitation of 5%. During the entire dynamic range, the green colour channel having the dominant and inferior colour channels expands in each direction. The Rayleigh distribution has been utilized for developing various stretching actions within the RGB colour space. The image has been converted to the HSV colour space, having the S and V elements adjusted within 1% of their minimum and maximum values. The suggested approach is examined in both qualitative and quantitative analysis. According to qualitative analysis, the recommended approach substantially boosts image contrast, lowers its blue and green effect, and minimizes over-enhanced and under-enhanced sections in the final resultant underwater image. The quantitative examination of 500 large scale underwater images dataset reveals that the suggested technique generates better results. The dataset images are grouped into small fish images, blue coral images, stone wall images, and coral branch images. The quantitative examination of all these four groups have been evaluated and shown. The average mean square error, peak signal to noise ratio, underwater image quality measurement, and underwater colour image quality evaluation values of dataset images are 76.69, 31.25, 3.85, and 0.64, respectively. These values of our proposed work outperform six other previous methods.},
  archive      = {J_EXSY},
  author       = {Nishant Singh and Aruna Bhat},
  doi          = {10.1111/exsy.13692},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13692},
  shortjournal = {Expert Syst.},
  title        = {Underwater image enhancement using contrast correction},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing internet of things device data: An ABE approach using fog computing and generative AI. <em>EXSY</em>, <em>42</em>(2), e13691. (<a href='https://doi.org/10.1111/exsy.13691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence of fog computing, new paradigms for data processing and management for IoT devices have been established in the quickly changing world of teaching/learning. This study addresses the complex issues brought about by the infiltration of diverse data sources by investigating novel approaches to strengthen data security and enhance access control mechanisms in fog computing environments. The commonly used cryptographic technique known as CP-ABE is renowned for providing accurate access control. Unfortunately, current multi-authority CP-ABE methods have difficulties when implemented on low-resource IoT devices. These techniques are not appropriate for resource-constrained IoT devices since the sizes of the secret key and ciphertext grow in proportion to the number of attributes. In this paper, a novel multi-authority CP-ABE approach, called MA-based CP-ABE, efficiently tackles these issues by optimizing the length of secret keys and ciphertext. Users' secret keys are always the same size, no matter how many attributes they own. Moreover, MA-based CP-ABE ensures that the size of the ciphertext scales linearly with the number of authorities rather than characteristics, which makes it a sensible option for devices with restricted resources. A Generative AI approach has also been integrated along with CP-ABE to make sure that the IoT data is secure and privacy is maintained. As per the security and experimental analysis, the proposed approach is considered secure and suitable for IoT-based applications.},
  archive      = {J_EXSY},
  author       = {Shruti and Shalli Rani and Wadii Boulila},
  doi          = {10.1111/exsy.13691},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13691},
  shortjournal = {Expert Syst.},
  title        = {Securing internet of things device data: An ABE approach using fog computing and generative AI},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sampling approaches to reduce very frequent seasonal time series. <em>EXSY</em>, <em>42</em>(2), e13690. (<a href='https://doi.org/10.1111/exsy.13690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With technological advancements, much data is being captured by sensors, smartphones, wearable devices, and so forth. These vast datasets are stored in data centres and utilized to forge data-driven models for the condition monitoring of infrastructures and systems through future data mining tasks. However, these datasets often surpass the processing capabilities of traditional information systems and methodologies due to their significant size. Additionally, not all samples within these datasets contribute valuable information during the model training phase, leading to inefficiencies. The processing and training of Machine Learning algorithms become time-consuming, and storing all the data demands excessive space, contributing to the Big Data challenge. In this paper, we propose two novel techniques to reduce large time-series datasets into more compact versions without undermining the predictive performance of the resulting models. These methods also aim to decrease the time required for training the models and the storage space needed for the condensed datasets. We evaluated our techniques on five public datasets, employing three Machine Learning algorithms: Holt-Winters, SARIMA, and LSTM. The outcomes indicate that for most of the datasets examined, our techniques maintain, and in several instances enhance, the forecasting accuracy of the models. Moreover, we significantly reduced the time required to train the Machine Learning algorithms employed.},
  archive      = {J_EXSY},
  author       = {Afonso Baldo and Paulo J. S. Ferreira and João Mendes-Moreira},
  doi          = {10.1111/exsy.13690},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13690},
  shortjournal = {Expert Syst.},
  title        = {Sampling approaches to reduce very frequent seasonal time series},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in deep learning for alzheimer's disease diagnosis: A comprehensive exploration and critical analysis of neuroimaging approaches. <em>EXSY</em>, <em>42</em>(2), e13688. (<a href='https://doi.org/10.1111/exsy.13688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer's disease (AD) is a major global health concern that affects millions of people globally. This study investigates the technical challenges in AD analysis and provides a thorough analysis of AD, emphasizing the disease's worldwide effects as well as the predicted increase. It explores the technological difficulties associated with AD analysis, concentrating on the shift in automated clinical diagnosis using MRI data from conventional machine learning to deep learning techniques. This study advances our knowledge of the effects of AD and provides new developments in deep learning for precise diagnosis, providing insightful information for both clinical and future research. The research introduces an innovative deep learning model, leveraging YOLOv5 and variants of YOLOv8, to classify AD images into four (NC, EMCI, LMCI, AD) categories. This study evaluates the performance of YOLOv5 which achieved high accuracy (97%) in multi-class classification (classes 0 to 3) with precision, recall, and F1-score reported for each class. YOLOv8 (Small) and YOLOv8 (Medium) models are also assessed for Alzheimer's disease diagnosis, demonstrating accuracy of 97% and 98%, respectively. Precision, recall, and F1-score metrics provide detailed insights into the models' effectiveness across different classes. Comparative analysis against a transfer learning model reveals YOLOv5, YOLOv8 (Small), and YOLOv8 (Medium) consistently outperforming across six binary classifications related to cognitive impairment. These models show improved sensitivity and accuracy compared to baseline architectures from [32]. In AD/NC classification, YOLOv8 (Medium) achieves 98.43% accuracy and 97.45% sensitivity, for EMCI/LMCI classification, YOLOv8 (Medium) also excels with 92.12% accuracy and 90.12% sensitivity. The results highlight the effectiveness of YOLOv5 and YOLOv8 variants in neuroimaging tasks, showcasing their potential in clinical applications for cognitive impairment classification. The proposed models showcase superior performance, achieving high accuracy, sensitivity, and F1-scores, surpassing baseline architectures and previous methods. Comparative analyses highlight the robustness and effectiveness of the proposed models in AD classification tasks, providing valuable insights for future research and clinical applications.},
  archive      = {J_EXSY},
  author       = {Fakhri Alam Khan and Abdullah Khan and Muhammad Imran and Awais Ahmad and Gwanggil Jeon},
  doi          = {10.1111/exsy.13688},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13688},
  shortjournal = {Expert Syst.},
  title        = {Advancements in deep learning for alzheimer's disease diagnosis: A comprehensive exploration and critical analysis of neuroimaging approaches},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CATcAFSMs: Context-based adaptive trust calculation for attack detection in fog computing based smart medical systems. <em>EXSY</em>, <em>42</em>(2), e13687. (<a href='https://doi.org/10.1111/exsy.13687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog's basic distributed nature and ability to process data in transit—that is, to make decisions in real time—make it a good fit for scenarios involving several distributed devices that need to communicate, provide real-time data analysis, and carry out storage functions. The majority of fog computing applications are driven by the user's demands and/or their desire for functioning services, either neglecting or giving security considerations second attention. Fog computing security issues have not received enough attention. Fog computing could be exploitable due to the security difficulties associated with cloud computing. Due to its flexibility to function near the end user and independence from a centralized design, fog computing provides the dependability required by time-sensitive smart healthcare systems. There is a need for enhanced security and privacy solutions for fog computing, where trust is essential, due to the importance of healthcare data. This research aims to develop a context-based adaptive trust solution for the smart healthcare environment utilizing Bayesian approaches and similarity measures against bad mouthing and ballot stuffing, while context-dependent trust solutions for fogs remain an unexplored area of study. The proposed trust model has been simulated in Contiki-Cooja to evaluate our findings. In contrast to static weighting, adaptive weights are provided to direct and indirect trust using entropy values that ensure the least degree of trust bias, and context similarity calculations eliminate recommender nodes with malicious intent by leveraging server, colleague, and service similarities. The proposed model protects smart healthcare systems from attacks using similarity metrics, incorporates context, and also uses adaptive weighting for trust calculation. By eliminating trust bias and also detecting attacks, this solution enhances the trust calculation by 10% as compared to the previous solution. This paradigm is efficient due to its small trust computation overhead and linear complexity O ( n ).},
  archive      = {J_EXSY},
  author       = {Alishba Nawaz and Waseem Iqbal and Ayesha Altaf and Abrar Almjally and Hatoon AlSagri and Bayan Alabdullah},
  doi          = {10.1111/exsy.13687},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13687},
  shortjournal = {Expert Syst.},
  title        = {CATcAFSMs: Context-based adaptive trust calculation for attack detection in fog computing based smart medical systems},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GRDATFusion: A gradient residual dense and attention transformer infrared and visible image fusion network for smart city security systems in cloud and fog computing. <em>EXSY</em>, <em>42</em>(2), e13685. (<a href='https://doi.org/10.1111/exsy.13685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The infrared and visible fusion technology holds a pivotal position in smart city for cloud and fog computing, particularly in security system. By fusing infrared and visible image information, this technology enhances target identification, tracking and monitoring precision, bolstering overall system security. However, existing deep learning-based methods rely heavily on convolutional operations, which excel at extracting local features but have limited receptive fields, hampering global information capture. To overcome this difficulty, we introduce GRDATFusion, a novel end-to-end network comprising three key modules: transformer, gradient residual dense and attention residual. The gradient residual dense module extracts local complementary features, leveraging a dense-shaped network to retain potentially lost information. The attention residual module focuses on crucial input image details, while the transformer module captures global information and models long-range dependencies. Experiments on public datasets show that GRDATFusion outperforms state-of-the-art algorithms in qualitative and quantitative assessments. Ablation studies validate our approach's advantages, and efficiency comparisons demonstrate its computational efficiency. Therefore, our method makes the security systems in smart city with shorter delay and satisfies the real-time requirement.},
  archive      = {J_EXSY},
  author       = {Jian Zheng and Seunggil Jeon and Xiaomin Yang},
  doi          = {10.1111/exsy.13685},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13685},
  shortjournal = {Expert Syst.},
  title        = {GRDATFusion: A gradient residual dense and attention transformer infrared and visible image fusion network for smart city security systems in cloud and fog computing},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TMaD: Three-tier malware detection using multi-view feature for secure convergence ICT environments. <em>EXSY</em>, <em>42</em>(2), e13684. (<a href='https://doi.org/10.1111/exsy.13684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As digital transformation accelerates, data generated in a convergence information and communication technology (ICT) environment must be secured. This data includes confidential information such as personal and financial information, so attackers spread malware in convergence ICT environments to steal this information. To protect convergence ICT environments from diverse cyber threats, deep learning models have been utilized for malware detection. However, accurately detecting rapidly generated variants and obfuscated malware is challenging. This study proposes a three-tier malware detection (TMaD) scheme that utilizes a cloud-fog-edge collaborative architecture to analyse multi-view features of executable files and detect malware. TMaD performs signature-based malware detection at the edge device tier, then sends executables detected as unknown or benign to the fog tier. The fog tier conducts static analysis on non-obfuscated executables and those transferred from the previous tier to detect variant malware. Subsequently, TMaD sends executables detected as benign in the fog tier to the cloud tier, where dynamic analysis is performed on obfuscated executables and those detected as benign to identify obfuscated malware. An evaluation of TMaD's detection performance resulted in an accuracy of 94.78%, a recall of 0.9794, a precision of 0.9535, and an f1-score of 0.9663. This performance demonstrates that TMaD, by analysing executables across several tiers and minimizing false negatives, exhibits superior detection performance compared to existing malware detection models.},
  archive      = {J_EXSY},
  author       = {Jueun Jeon and Byeonghui Jeong and Seungyeon Baek and Young-Sik Jeong},
  doi          = {10.1111/exsy.13684},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13684},
  shortjournal = {Expert Syst.},
  title        = {TMaD: Three-tier malware detection using multi-view feature for secure convergence ICT environments},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing hybrid deep learning models for drug-target interaction prediction: A comparative analysis of evolutionary algorithms. <em>EXSY</em>, <em>42</em>(2), e13683. (<a href='https://doi.org/10.1111/exsy.13683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of Drug-Target Interaction (DTI) prediction, this research investigates and contrasts the efficacy of diverse evolutionary algorithms in fine-tuning a sophisticated hybrid deep learning model. Recognizing the critical role of DTI in drug discovery and repositioning, we tackle the challenges of binary classification by reframing the problem as a regression task. Our focus lies on the Convolution Self-Attention Network with Attention-based bidirectional Long Short-Term Memory Network (CSAN-BiLSTM-Att), a hybrid model combining convolutional neural network (CNN) blocks, self-attention mechanisms, and bidirectional LSTM layers. To optimize this complex model, we employ Differential Evolution (DE), Particle Swarm Optimization (PSO), Memetic Particle Swarm Optimization Algorithm (MPSOA), Fire Hawk Optimization (FHO), and Artificial Hummingbird Algorithm (AHA). Through thorough comparative analysis, we evaluate the performance of these evolutionary algorithms in enhancing the CSAN-BiLSTM-Att model's effectiveness. By examining the strengths and weaknesses of each algorithm, our study aims to provide valuable insights into DTI prediction, identifying the most effective evolutionary algorithm for hyperparameter tuning in advanced deep learning models. Notably, Fire-hawk optimization (FHO) emerges as particularly promising, achieving the highest Concordance Index (C-index) as 0.974 for KIBA datasets and 0.894 for DAVIS datasets and demonstrating exceptional accuracy in ranking continuous predictions across both the datasets.},
  archive      = {J_EXSY},
  author       = {Moolchand Sharma and Aryan Bhatia and Akhil and Ashit Kumar Dutta and Shtwai Alsubai},
  doi          = {10.1111/exsy.13683},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13683},
  shortjournal = {Expert Syst.},
  title        = {Optimizing hybrid deep learning models for drug-target interaction prediction: A comparative analysis of evolutionary algorithms},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing fraud detection in financial transactions with machine learning and imbalance mitigation. <em>EXSY</em>, <em>42</em>(2), e13682. (<a href='https://doi.org/10.1111/exsy.13682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of the Internet and digital payments has transformed the landscape of financial transactions, leading to both technological progress and an alarming rise in cybercrime. This study addresses the critical issue of financial fraud detection in the era of digital payments, focusing on enhancing operational risk frameworks to mitigate the increasing threats. The objective is to improve the predictive performance of fraud detection systems using machine learning techniques. The methodology involves a comprehensive data preprocessing and model creation process, including one-hot encoding, feature selection, sampling, standardization, and tokenization. Six machine learning models are employed for fraud detection, and their hyperparameters are optimized. Evaluation metrics such as accuracy, precision, recall, and F 1-score are used to assess model performance. Results reveal that XGBoost and Random Forest outperform other models, achieving a balance between false positives and false negatives. The study meets the requirements for fraud detection systems, ensuring accuracy, scalability, adaptability, and explainability. This paper provides valuable insights into the efficacy of machine learning models for financial fraud detection and emphasizes the importance of striking a balance between false positives and false negatives.},
  archive      = {J_EXSY},
  author       = {Ezaz Mohammed Al-dahasi and Rama Khaled Alsheikh and Fakhri Alam Khan and Gwanggil Jeon},
  doi          = {10.1111/exsy.13682},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13682},
  shortjournal = {Expert Syst.},
  title        = {Optimizing fraud detection in financial transactions with machine learning and imbalance mitigation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating spotted hyena optimization technique with generative artificial intelligence for time series forecasting. <em>EXSY</em>, <em>42</em>(2), e13681. (<a href='https://doi.org/10.1111/exsy.13681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative artificial intelligence (AI) has developed as an effective tool for time series predicting, revolutionizing the typical methods of prediction. Different classical approaches that depend on existing approaches and assumptions, generative AI controls advanced deep learning (DL) approaches like generative adversarial networks (GANs) and recurrent neural networks (RNNs), to identify designs and connections in time series data. DL has accomplished major success in optimizing performances connected with AI. In the financial area, it can be extremely utilized for the stock market predictive, trade implementation approaches, and set of optimizers. Stock market predictive is the most important use case in this field. GANs with advanced AI approaches have become more significant in recent times. However, it can be utilized in image-image-translation and other computer vision (CV) conditions. GANs could not utilized greatly for stock market prediction because of their effort to establish the proper set of hyperparameters. This study develops an integrated spotted hyena optimization algorithm with generative artificial intelligence for time series forecasting (SHOAGAI-TSF) technique. The purpose of the SHOAGAI-TSF technique is to accomplish a forecasting process for the utilization of stock price prediction. The SHOAGAI-TSF technique uses probabilistic forecasting with a conditional GAN (CGAN) approach for the prediction of stock prices. The CGAN model learns the data generation distribution and determines the probabilistic prediction from it. To boost the prediction results of the CGAN approach, the hyperparameter tuning can be performed by the use of the SHOA. The simulation result analysis of the SHOAGAI-TSF technique takes place on the stock market dataset. The experimental outcomes determine the significant solution of the SHOAGAI-TSF algorithm with other compared methods in terms of distinct metrics.},
  archive      = {J_EXSY},
  author       = {Reda Salama},
  doi          = {10.1111/exsy.13681},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13681},
  shortjournal = {Expert Syst.},
  title        = {Integrating spotted hyena optimization technique with generative artificial intelligence for time series forecasting},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing a digital therapeutic for obesity management through 3D human body reconstruction. <em>EXSY</em>, <em>42</em>(2), e13678. (<a href='https://doi.org/10.1111/exsy.13678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduced a groundbreaking approach to address the pressing public health challenges of obesity management and its associated health implications. By establishing a clear link between obesity and various health issues, this study underscores the critical need for effective interventions. Our team developed a pioneering digital therapeutic tool through the application of advanced 3D artificial intelligence technologies. This innovative solution offers a dynamic visual representation of weight loss and health enhancement journeys for individuals with obesity. By providing a platform for users to monitor their progress in real time, digital therapeutics (DTx) foster deeper engagement and strengthen motivation towards health goals. The experimental results showed that the digital therapeutic received high scores in terms of usability, effectiveness, predictiveness and personalization, user satisfaction, and continuous usage and adherence. These findings suggest that DTx can be a valuable tool for the management and treatment of obesity. The effectiveness of this digital approach was thoroughly assessed from multiple dimensions, showing its significant potential and effectiveness in obesity management. These findings advocate ongoing research in this area, projecting that the continuous evolution of DTx will have a profound positive impact on both personal and public health outcomes.},
  archive      = {J_EXSY},
  author       = {Hyunsook Lee and Sekyoung Youm},
  doi          = {10.1111/exsy.13678},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13678},
  shortjournal = {Expert Syst.},
  title        = {Developing a digital therapeutic for obesity management through 3D human body reconstruction},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of visual attribute transfer technology in analysing changes in emotional expression in picture books. <em>EXSY</em>, <em>42</em>(2), e13677. (<a href='https://doi.org/10.1111/exsy.13677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In picture books, readers can obtain different emotional perceptions according to different image style attributes. Artists often use different combinations of colours, textures, materials, and other style elements in images to convey different emotions in their creations. Especially in picture books for children, there is a strong correlation between the perceived effect of the work and the accuracy and degree of emotional expression. In the process of creating picture books, various factors will affect the efficiency of artists trying to transfer styles to meet their creative needs. With the development of image style transfer technology based on a deep convolutional neural network, artists can use this technology to create works with different styles of emotional changes efficiently. In this paper, we select illustrations of picture books and use deep convolutional neural networks to transfer image styles from three aspects: colour style transfer, texture style, and material style transfer. Through sampling survey experiments, we discuss the changes in image attributes, emotional expression, and emotional perception in picture books for children. The survey results found that the most direct and evident influence on the emotional changes of picture book images is the transfer of colour style attributes, material style attributes, and texture style attributes. The results of this study can provide a valuable reference for improving the accuracy of emotional expression, the depth of meaning extension, and the height of artistic value in picture books for children during the process of an artist's creation. This research stands out by systematically analysing the distinct impact of each style attribute transfer, offering a comprehensive framework that can be utilized by artists and technologists alike to enhance the emotional and artistic quality of children's picture books.},
  archive      = {J_EXSY},
  author       = {Yue Wang and Yin Wang and Yansu Qi and Sheng Miao and Weijun Gao},
  doi          = {10.1111/exsy.13677},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13677},
  shortjournal = {Expert Syst.},
  title        = {Application of visual attribute transfer technology in analysing changes in emotional expression in picture books},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generative adversarial network-based client-level handwriting forgery attack in federated learning scenario. <em>EXSY</em>, <em>42</em>(2), e13676. (<a href='https://doi.org/10.1111/exsy.13676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL), celebrated for its privacy-preserving features, has been revealed by recent studies to harbour security vulnerabilities that jeopardize client privacy, particularly through data reconstruction attacks that enable adversaries to recover original client data. This study introduces a client-level handwriting forgery attack method for FL based on generative adversarial networks (GANs), which reveals security vulnerabilities existing in FL systems. It should be stressed that this research is purely for academic purposes, aiming to raise concerns about privacy protection and data security, and does not encourage illegal activities. Our novel methodology assumes an adversarial scenario wherein adversaries intercept a fraction of parameter updates via victim clients’ wireless communication channels, then use this information to train GAN for data recovery. Finally, the purpose of handwriting imitation is achieved. To rigorously assess and validate our methodology, experiments were conducted using a bespoke Chinese digit dataset, facilitating in-depth analysis and robust verification of results. Our experimental findings demonstrated enhanced data recovery effectiveness, a client-level attack and greater versatility compared to prior art. Notably, our method maintained high attack performance even with a streamlined GAN design, yielding increased precision and significantly faster execution times compared to standard methods. Specifically, our experimental numerical results revealed a substantial boost in reconstruction accuracy by 16.7%, coupled with a 51.9% decrease in computational time compared to the latest similar techniques. Furthermore, tests on a simplified version of our GAN exhibited an average 10% enhancement in accuracy, alongside a remarkable 70% reduction in time consumption. By surmounting the limitations of previous work, this study fills crucial gaps and affirms the effectiveness of our approach in achieving high-accuracy client-level data reconstruction within the FL context, thereby stimulating further exploration into FL security measures.},
  archive      = {J_EXSY},
  author       = {Lei Shi and Han Wu and Xu Ding and Hao Xu and Sinan Pan},
  doi          = {10.1111/exsy.13676},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13676},
  shortjournal = {Expert Syst.},
  title        = {A generative adversarial network-based client-level handwriting forgery attack in federated learning scenario},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting early depression in WZT drawing image based on deep learning. <em>EXSY</em>, <em>42</em>(2), e13675. (<a href='https://doi.org/10.1111/exsy.13675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When stress causes negative behaviours to emerge in our daily lives, it is important to intervene quickly and appropriately to control the negative problem behaviours. Questionnaires, a common method of information gathering, have the disadvantage that it is difficult to get the exact information needed due to defensive or insincere responses from subjects. As an alternative to these drawbacks, projective testing using pictures can provide the necessary information more accurately than questionnaires because the subject responds subconsciously and the direct experience expressed through pictures can be more accurate than questionnaires. Analysing hand-drawn image data with the Wartegg Zeichen Test (WZT) is not easy. In this study, we used deep learning to analyse image data represented as pictures through WZT to predict early depression. We analyse the data of 54 people who were judged as early depression and 54 people without depression, and increase the number of people without depression to 100 and 500, and aim to study in unbalanced data. We use CNN and CNN-SVM to analyse the drawing images of WZT's initial depression with deep learning and predict the outcome of depression. The results show that the initial depression is predicted with 92%–98% accuracy on the image data directly drawn by WZT. This is the first study to automatically analyse and predict early depression in WZT based on hand-drawn image data using deep learning models. The extraction of features from WZT images by deep learning analysis is expected to create more research opportunities through the convergence of psychotherapy and Information and Communication Technology (ICT) technology, and is expected to have high growth potential.},
  archive      = {J_EXSY},
  author       = {Kyung-yeul Kim and Young-bo Yang and Mi-ra Kim and Jihie Kim and Ji Su Park},
  doi          = {10.1111/exsy.13675},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13675},
  shortjournal = {Expert Syst.},
  title        = {Predicting early depression in WZT drawing image based on deep learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KDBI special issue: Explainability feature selection framework application for LSTM multivariate time-series forecast self optimization. <em>EXSY</em>, <em>42</em>(2), e13674. (<a href='https://doi.org/10.1111/exsy.13674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models are widely used in multivariate time series forecasting, yet, they have high computational costs. One way to reduce this cost is by reducing data dimensionality, which involves removing unimportant or low importance information with the proper method. This work presents a study on an explainability feature selection framework composed of four methods (IMV-LSTM Tensor, LIME-LSTM, Average SHAP-LSTM, and Instance SHAP-LSTM) aimed at using the LSTM black-box model complexity to its favour, with the end goal of improving the error metrics and reducing the computational cost on a forecast task. To test the framework, three datasets with a total of 101 multivariate time series were used, with the explainability methods outperforming the baseline methods in most of the data, be it in error metrics or computation time for the LSTM model training.},
  archive      = {J_EXSY},
  author       = {Eduardo M. Rodrigues and Yassine Baghoussi and João Mendes-Moreira},
  doi          = {10.1111/exsy.13674},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13674},
  shortjournal = {Expert Syst.},
  title        = {KDBI special issue: Explainability feature selection framework application for LSTM multivariate time-series forecast self optimization},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unique morpho-feature extraction algorithm for medicinal plant identification. <em>EXSY</em>, <em>42</em>(2), e13663. (<a href='https://doi.org/10.1111/exsy.13663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An image is a set of numbers arranged in matrix form. The image feature extraction algorithm converts the input image into different numerical forms to extract the useful information from the input image and the selection of appropriate feature extraction algorithm is crucial for medicinal plant identification. In medicinal plants, the leaves are an available important resource of morphological features. Botanists use these morphological features of leaf images for medicinal plant identification. The existing leaf-based medicinal plant identification strategies include shape, colour and texture features. In these methods, environmental factors directly influence the features and hence, the impact can be observed in the accuracy of the result. To overcome these limitations, we have proposed a unique morpho-feature extraction algorithm (UMFEA) for accurate identification of medicinal plants. The UMFEA includes three sub-algorithms for shape, apex, base, and vein features extraction. The proposed UMFEA is tested over Flavia, Swedish, Leaf and our databases. The performance comparison of UMFEA is done on different databases and the results obtained were remarkably good.},
  archive      = {J_EXSY},
  author       = {Ashwani Kumar Dubey and Jibi G. Thanikkal and Puneet Sharma and Manoj Kumar Shukla},
  doi          = {10.1111/exsy.13663},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13663},
  shortjournal = {Expert Syst.},
  title        = {A unique morpho-feature extraction algorithm for medicinal plant identification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-focus image fusion network deployed in smart city target detection. <em>EXSY</em>, <em>42</em>(2), e13662. (<a href='https://doi.org/10.1111/exsy.13662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the global monitoring of smart cities, the demands of global object detection systems based on cloud and fog computing in intelligent systems can be satisfied by photographs with globally recognized properties. Nevertheless, conventional techniques are constrained by the imaging depth of field and can produce artefacts or indistinct borders, which can be disastrous for accurately detecting the object. In light of this, this paper proposes an artificial intelligence-based gradient learning network that gathers and enhances domain information at different sizes in order to produce globally focused fusion results. Gradient features, which provide a lot of boundary information, can eliminate the problem of border artefacts and blur in multi-focus fusion. The multiple-receptive module (MRM) facilitates effective information sharing and enables the capture of object properties at different scales. In addition, with the assistance of the global enhancement module (GEM), the network can effectively combine the scale features and gradient data from various receptive fields and reinforce the features to provide precise decision maps. Numerous experiments have demonstrated that our approach outperforms the seven most sophisticated algorithms currently in use.},
  archive      = {J_EXSY},
  author       = {Haojie Zhao and Shuang Guo and Gwanggil Jeon and Xiaomin Yang},
  doi          = {10.1111/exsy.13662},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13662},
  shortjournal = {Expert Syst.},
  title        = {A multi-focus image fusion network deployed in smart city target detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The hierarchical importance of patent's characteristics to licensing: An analysis through random forest. <em>EXSY</em>, <em>42</em>(2), e13661. (<a href='https://doi.org/10.1111/exsy.13661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to ascertain the hierarchical importance of a patent's characteristics to licensing. This research has a causal-exploratory purpose, in that it sought to establish relationships between variables. This research aims to identify which characteristics are influential in the licensing of Brazilian academic patents in the biotechnology and pharmaceutical technology fields, based on the mining of data contained in licensed and unlicensed patent documents. Which characteristics of Brazilian academic patents are most influential in their licensing potential? An analysis through Random Forest was performed. To the best of our knowledge, there are no studies in Brazil using machine learning to identify which characteristics are influential in licensing a particular academic patent, especially given the difficulty of gathering this information. We found that regardless of the measure used, the three most critical licensing characteristics for the Biotechnology and Pharmaceutical patents analysed are Patent Scope, Life Cycle, and Claims. At the same time, the least important is the Patent Cooperation Treaty. The relevance of this research is based on the fact that after identifying which intrinsic characteristics influence the final value and licensing probabilities of a given patent, it will be possible to develop mathematical models that provide accurate information for establishing technology transfer agreements. In practical terms, the results suggest that greater patent versatility, combined with lifecycle management and a technical effort to build strong claims, increases the licensing potential of academic biopharmaceutical patents.},
  archive      = {J_EXSY},
  author       = {Alexânder Araújo Reis and Rafael Ângelo Santos Leite and Cicero Eduardo Walter and Igor Bezerra Reis and Ramiro Gonçalves and José Martins and Frederico Branco and Manuel Au-Yong-Oliveira},
  doi          = {10.1111/exsy.13661},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13661},
  shortjournal = {Expert Syst.},
  title        = {The hierarchical importance of patent's characteristics to licensing: An analysis through random forest},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time series generative adversarial network for muscle force prognostication using statistical outlier detection. <em>EXSY</em>, <em>42</em>(2), e13653. (<a href='https://doi.org/10.1111/exsy.13653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning approaches, such as artificial neural networks (ANN), effectively perform various tasks and provide new predictive models for complicated physiological systems. Examples of Robotics applications involving direct human engagement, such as controlling prosthetic arms, athletic training, and investigating muscle physiology. It is now time for automated systems to take over modelling and monitoring tasks. However, there is a problem with the massive amount of time series data collected to build accurate forecasting systems. There may be inconsistencies in forecasting muscle forces due to the enormous amount of data. As a result, anomaly detection techniques play a significant role in detecting anomalous data. Detecting anomalies can help reduce redundancy and free up large storage space for storing relevant time-series data. This paper employs several anomaly detection techniques, including Isolation Forest (iforest), K-Nearest Neighbour (KNN), Open Support Vector Machine (OSVM), Histogram, and Local Outlier Factor (LOF). These techniques have been used by Long Short-Term Memory (LSTM), Auto-Regressive Integrated Moving Average (ARIMA), and Prophet models. The dataset used in this study contained raw measurements of body movements (kinematics) and the forces generated during walking (kinetics) of 57 healthy people (29 Female, 28 Male) without walking abnormalities or recent leg injuries. To increase the data samples, we used TimeGAN that generates synthetic time series data with temporal dependencies, aiding in training robust predictive models for muscle force prediction. The results are then compared with different evaluation metrics for five different samples. It is found that anomaly detection techniques with LSTM, ARIMA, and Prophet models provided better performance in forecasting muscle forces. The iforest method achieved the best Pearson's Correlation Coefficient ( r ) of 0.95, which is a competitive score with existing systems that perform between 0.7 and 0.9. The methodology provides a foundation for precision medicine, enhancing prognostic capability over relying solely on population averages.},
  archive      = {J_EXSY},
  author       = {Hunish Bansal and Basavraj Chinagundi and Prashant Singh Rana and Neeraj Kumar},
  doi          = {10.1111/exsy.13653},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13653},
  shortjournal = {Expert Syst.},
  title        = {Time series generative adversarial network for muscle force prognostication using statistical outlier detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random permutation-based linear regression for cancelable biometrics. <em>EXSY</em>, <em>42</em>(2), e13652. (<a href='https://doi.org/10.1111/exsy.13652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security of biometric data in biometric-based authentication systems is a significant concern. Cancellable biometrics aim to generate templates that can be replaced by new templates if compromised. We propose a new approach for generating cancellable biometric templates based on linear regression with random permutation. Our approach generates a virtual image for every biometric image by applying linear regression. In the next step, the cancellable biometric template is produced by randomly permuting each virtual image depending on a key assigned to each individual. If the template is compromised, it can be cancelled, and a new template can be generated by altering the key. Our method has shown superior performance compared to existing random permutation-based methods in terms of authentication accuracy across six databases, encompassing face, iris, and ear, even when dealing with low-resolution images. It performed well on challenging databases like UBIRIS and Georgia Tech, demonstrating the robustness of the proposed approach.},
  archive      = {J_EXSY},
  author       = {Onkar Singh and Ajay Jaiswal and Nitin Kumar and Naveen Kumar},
  doi          = {10.1111/exsy.13652},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13652},
  shortjournal = {Expert Syst.},
  title        = {Random permutation-based linear regression for cancelable biometrics},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient object tracking based on multi-head cross-attention transformer. <em>EXSY</em>, <em>42</em>(2), e13650. (<a href='https://doi.org/10.1111/exsy.13650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking is an essential component of computer vision and plays a significant role in various practical applications. Recently, transformer-based trackers have become the predominant method for tracking due to their robustness and efficiency. However, existing transformer-based trackers typically focus solely on the template features, neglecting the interactions between the search features and the template features during the tracking process. To address this issue, this article introduces a multi-head cross-attention transformer for visual tracking (MCTT), which effectively enhance the interaction between the template branch and the search branch, enabling the tracker to prioritize discriminative feature. Additionally, an auxiliary segmentation mask head has been designed to produce a pixel-level feature representation, enhancing and tracking accuracy by predicting a set of binary masks. Comprehensive experiments have been performed on benchmark datasets, such as LaSOT, GOT-10k, UAV123 and TrackingNet using various advanced methods, demonstrating that our approach achieves promising tracking performance. MCTT achieves an AO score of 72.8 on the GOT-10k.},
  archive      = {J_EXSY},
  author       = {Jiahai Dai and Huimin Li and Shan Jiang and Hongwei Yang},
  doi          = {10.1111/exsy.13650},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13650},
  shortjournal = {Expert Syst.},
  title        = {An efficient object tracking based on multi-head cross-attention transformer},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified M-RCNN approach for abandoned object detection in public places. <em>EXSY</em>, <em>42</em>(2), e13648. (<a href='https://doi.org/10.1111/exsy.13648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of abandoned and stationary objects like luggage, boxes, machinery, and so forth, in public places is one of the challenging and critical tasks in the video surveillance system. These objects may contain weapons, bombs, or other explosive materials that threaten the public. Though various applications have been developed to detect stationary objects, different challenges, like occlusions, changes in geometrical features of things, and so forth, are still to be addressed. Considering the complexity of scenarios in public places and the variety of objects, a context-aware model is developed based on mask region-based convolution network (M-RCNN) for detecting abandoned objects. A modified convolution operation is implemented in the Backbone network to understand features from geometric variations near objects. These modified operation layers can be adapted based on geometric interpretations to extract required features. Finally, a bounding box operation is performed to locate the abandoned object and mask the particular thing. Experiments have been performed on the benchmark dataset like ABODA and our dataset, which shows that an mAP of 0. 0.699 is achieved for model 1, 0.675 is achieved for model 2, and 0.734 mAP is completed for model 3. An ablation analysis has also been performed and compared with other state-of-the-art methods. Based on the results, the proposed model better detects abandoned objects than existing state-of-the-art methods.},
  archive      = {J_EXSY},
  author       = {Rahul Chiranjeevi Veluri and Shakir Khan and Senthil Pandi Sankareswaran and Mohammad Shabaz and Ahmed Farouk and Nisreen Innab},
  doi          = {10.1111/exsy.13648},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13648},
  shortjournal = {Expert Syst.},
  title        = {Modified M-RCNN approach for abandoned object detection in public places},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Al-based energy aware parent selection mechanism to enhance security and energy efficiency for smart homes in internet of things. <em>EXSY</em>, <em>42</em>(2), e13647. (<a href='https://doi.org/10.1111/exsy.13647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing ubiquity of Internet of Things (IoT) devices within smart homes demands the use of advanced strategies in IoT implementation, with an emphasis on energy efficiency and security. The incorporation of Artificial Intelligence (AI) within the IoT framework improves the overall efficiency of the network. An inefficient mechanism of parent selection at the network layer of IoT causes energy drain in the nodes, particularly near the sink node. As a result, nodes die earlier, causing network holes that further increase the control message overhead as well as the energy consumption of the network, compromising network security. This research introduces an AI-based approach to parent selection of the Routing Protocol for Low Power and Lossy networks (RPL) at the network layer of IoT to enhance security and energy efficiency. A novel objective function, named Energy and Parent Load Objective Function (EA-EPL), is also proposed that considers the composite metrics, including energy and parent load. Extensive experiments are conducted to assess EA-EPL against OF0 and MRHOF algorithms. Experimental results show that EA-EPL outperformed these algorithms in improving energy efficiency, network stability, and packet delivery ratio. The results also demonstrate a significant enhancement in the overall efficiency of IoT networks and increased security in smart home environments.},
  archive      = {J_EXSY},
  author       = {Habib Ur Rahman and Muhammad Asif Habib and Shahzad Sarwar and Awais Ahmad and Anand Paul and Yazeed Alkhrijah and Waeal J. Obidallah},
  doi          = {10.1111/exsy.13647},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13647},
  shortjournal = {Expert Syst.},
  title        = {Al-based energy aware parent selection mechanism to enhance security and energy efficiency for smart homes in internet of things},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy preserving security using multi-key homomorphic encryption for face recognition. <em>EXSY</em>, <em>42</em>(2), e13645. (<a href='https://doi.org/10.1111/exsy.13645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, face recognition based on homomorphic encryption for privacy preservation has garnered significant attention. However, there are two major challenges with homomorphic encryption methods: the security and efficiency of face recognition systems. We present a more efficient and secure PUM (Privacy preserving security Using Multi-key homomorphic encryption) mechanism for facial recognition. By integrating feature grouping with parallel computing, we enhance the efficiency of homomorphic operations. The use of multi-key encryption ensures the security of the facial recognition system. This approach improves the security and speed of facial recognition systems in cloud computing scenarios, increasing the original 128-bit security to a maximum of 1664-bit security. In terms of efficiency, comparing encrypted images takes only 0.302 s, with an accuracy rate of 99.425%. When applied to a campus scenario, the average search time for a facial template library containing 700 encrypted features is approximately 1.5 s. Consequently, our solution not only ensures user privacy but also demonstrates superior operational efficiency and practical value. In comparison to recently emerged ciphertext facial recognition systems, our solution has demonstrated notable enhancements in both security and time efficiency.},
  archive      = {J_EXSY},
  author       = {Jing Wang and Rundong Xin and Osama Alfarraj and Amr Tolba and Qitao Tang},
  doi          = {10.1111/exsy.13645},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13645},
  shortjournal = {Expert Syst.},
  title        = {Privacy preserving security using multi-key homomorphic encryption for face recognition},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based brain tumour architecture for weight sharing optimization in federated learning. <em>EXSY</em>, <em>42</em>(2), e13643. (<a href='https://doi.org/10.1111/exsy.13643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large amounts of data is necessary for deep learning models to semantically segment images. A major issue in the field of medical imaging is accumulating adequate data and then applying specialized skills to label those medical imaging data. Collaboration across institutions might be able to alleviate this problem, but sharing medical data to a centralized place is complicated due to legal, privacy, technical, and data ownership constraints, particularly among international institutions. By guaranteeing user privacy and preventing unauthorized access to raw data, Federated Learning plays a significant role especially in decentralized deep learning applications. Each client is given a unique learning process assignment. Clients first train a machine learning model locally using data from their area. Then, clients upload training data (local updates of model weights and biases) to a server. After that, the server compiles client-provided updates to build a global learning model. Due to the numerous parameters (weights and biases) employed by deep learning models, the constant transmission between clients and the server raises communication costs and is inefficient from the standpoint of resource use. When there are more contributing clients and communication rounds, the cost of communication becomes a bigger concern. In this paper, a novel federated learning with weight sharing optimization compression architecture FedWSOcomp is proposed for cross institutional collaboration. In FedWSOcomp, the weights from deep learning models between clients and servers help in considerably reducing the amount of updates. Top-z sparsification, quantization with clustering, and compression with three separate encoding techniques are all implemented by FedWSOcomp. Modern compression techniques are outperformed by FedWSOcomp, which achieves compression rates of up to 1085× while saving up to 99% of bandwidth and 99% of energy for clients during communication.},
  archive      = {J_EXSY},
  author       = {Ameer N. Onaizah and Yuanqing Xia and Ahmed J. Obaid and Khurram Hussain},
  doi          = {10.1111/exsy.13643},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13643},
  shortjournal = {Expert Syst.},
  title        = {Deep learning based brain tumour architecture for weight sharing optimization in federated learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soutcom: Real-time sentiment analysis of arabic text for football fan satisfaction using a bidirectional LSTM. <em>EXSY</em>, <em>42</em>(2), e13641. (<a href='https://doi.org/10.1111/exsy.13641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, various topics, including sports, have seen social media platforms emerge as significant sources of information and viewpoints. Football fans use social media to express their opinions and sentiments about their favourite teams and players. Analysing these opinions can provide valuable information on the satisfaction of football fans with their teams. In this article, we present Soutcom, a scalable real-time system that estimates the satisfaction of football fans with their teams. Our approach leverages the power of social media platforms to gather real-time opinions and emotions of football fans and applies state-of-the-art machine learning-based sentiment analysis techniques to accurately predict the sentiment of Arabic posts. Soutcom is designed as a cloud-based scalable system integrated with the X (formerly known as Twitter) API and a football data service to retrieve live posts and match data. The Arabic posts are analysed using our proposed bidirectional LSTM (biLSTM) model, which we trained on a custom dataset specifically tailored for the sports domain. Our evaluation shows that the proposed model outperforms other machine learning models such as Random Forest, XGBoost and Convolutional Neural Networks (CNNs) in terms of accuracy and F 1-score with values of 0.83 and 0.82, respectively. Furthermore, we analyse the inference time of our proposed model and suggest that there is a trade-off between performance and efficiency when selecting a model for sentiment analysis on Arabic posts.},
  archive      = {J_EXSY},
  author       = {Sultan Alfarhood},
  doi          = {10.1111/exsy.13641},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13641},
  shortjournal = {Expert Syst.},
  title        = {Soutcom: Real-time sentiment analysis of arabic text for football fan satisfaction using a bidirectional LSTM},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging YOLOv5s with optimization-based effective anomaly detection in pedestrian walkways. <em>EXSY</em>, <em>42</em>(2), e13640. (<a href='https://doi.org/10.1111/exsy.13640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, video surveillance is generally used to safeguard safety in public places like railway stations, traffic signals, malls, and so on. Video anomaly recognition and localization are the main components of the intelligent video surveillance method. Video anomaly recognition refers to the procedure of spatiotemporal localization of the abnormal design existing in the video. A main task in video surveillance is the classification of anomalies that occur in it like thefts, crimes, and so forth. Also, anomaly recognition in pedestrian walkways has enlarged major attention among the computer vision (CV) groups to improve pedestrian protection. The current developments in Deep Learning (DL) methods have great attention to dissimilar procedures like image classification, object recognition, and so forth. This study designs an Optimal Deep Learning for Effective Anomaly Detection in Pedestrian Walkways (ODL-EADPW) model. The ODL-EADPW technique employs a fine-tuned DL model for the identification of pedestrians and anomalies in the walkways. In the ODL-EADPW technique, the image pre-processing is primarily involved in two stages median filtering (MF) based noise removal and adaptive histogram equalization (AHE)-based contrast enhancement. For anomaly detection in pedestrian walkways, the ODL-EADPW technique uses the YOLOv5s model with EfficientRep as a backbone network. To enhance the detection results of the ODL-EADPW technique, a stochastic gradient descent (SGD) optimizer was employed to perfect the hyperparameters of the EfficientRep model. The performance evaluation of the ODL-EADPW methodology is implemented on the UCSD Anomaly detection dataset. An extensive comparison study stated that the ODL-EADPW technique gains effectual detection results over other DL models in terms of different measures.},
  archive      = {J_EXSY},
  author       = {Allabaksh Shaik and Shaik Mahaboob Basha},
  doi          = {10.1111/exsy.13640},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13640},
  shortjournal = {Expert Syst.},
  title        = {Leveraging YOLOv5s with optimization-based effective anomaly detection in pedestrian walkways},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autism spectrum disorder identification using multi-model deep ensemble classifier with transfer learning. <em>EXSY</em>, <em>42</em>(2), e13623. (<a href='https://doi.org/10.1111/exsy.13623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying autism spectrum disorder (ASD) symptoms accurately is a challenging task. The traditional subjective diagnostic process of ASD relies on time-consuming behavioural and psychological observations. In this study, we introduce an ensemble learning-based classification model using an open-access database focusing on functional magnetic resonance imaging (fMRI). We propose a novel multi-model ensemble classifier (MMEC) and multisite ensemble classifier (MSEC) with transfer learning (TL) for ASD classification to improve the prediction accuracy. The MMEC utilizes four base classifiers, Inception V3, ResNet50, MobileNet, and DenseNet to boost the performance of the individual convolutional neural network (CNN) models. The MSEC combined the base classifiers trained from different data sites. We evaluate the two models with ensemble averaging, weighted averaging, and stacking methods. The proposed MMEC with stacking shows the state of art performance compared to MSEC, improving the prediction accuracy by 3.25%. The obtained results have shown an accuracy of 97.82%, 97.82%, and 97.78% for ensemble averaging, weighted averaging, and stacking methods, respectively, on multi-site datasets. The ensemble classifier MMEC performed better than a single classifier on the multi-site dataset. The proposed MMEC opens a new paradigm to design a universal ASD classification framework.},
  archive      = {J_EXSY},
  author       = {Lakmini Herath and Dulani Meedeniya and Janaka Marasinghe and Vajira Weerasinghe and Tele Tan},
  doi          = {10.1111/exsy.13623},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13623},
  shortjournal = {Expert Syst.},
  title        = {Autism spectrum disorder identification using multi-model deep ensemble classifier with transfer learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED: The construction of enterprise's financial supply chain management under blockchain technology. <em>EXSY</em>, <em>42</em>(2), e13297. (<a href='https://doi.org/10.1111/exsy.13297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper uses the advantages of blockchain technology to help construction enterprises obtain low-interest bank loans, effectively reduce financing costs, thus improving the level of financial supply chain, innovating financial management (FM) of construction enterprises, solving the FM transformation problem of construction enterprises and promoting the sustainable development of construction enterprises, and thus doing the following work. First, this paper describes the concept and characteristics of BT and the motivation of FM of construction enterprises to use BT. Second, it analyses BT's three main application advantages in construction enterprise financing: reducing financial risks, improving the credit penetration of core enterprises, and reducing financing costs. Meanwhile, this paper summarizes the problems that should be paid attention to in implementing the current financial supply chain management strategy in enterprises. It provides a solid theoretical foundation for the research of FM transformation in construction enterprises. Finally, the overall design and functional analysis of the BT FM platform are carried out. The functions of the BT FM platform are divided into four modules: system management, credit management, contract management, and information management. Besides, their specific business processes are analysed. The research results show that BT is applicable in enterprises, and the value of each participant's financing platform can be clearly defined from the construction of BT-based financing platform. The income quota of the owner's individual operation is 33%, and the income quota of the tripartite cooperation is also 33%, which further illustrates the effectiveness of the financial financing model of construction enterprises based on BT and the feasibility of its actual implementation in the future. In this paper, the related issues of financial supply chain management are systematically and comprehensively summarized, which has theoretical guiding significance for the research of FM transformation of construction enterprises based on BT.},
  archive      = {J_EXSY},
  author       = {Wumin Ke},
  doi          = {10.1111/exsy.13297},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13297},
  shortjournal = {Expert Syst.},
  title        = {RETRACTED: The construction of enterprise's financial supply chain management under blockchain technology},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED: Segmentation and classification of lymphoblastic leukaemia using quantum neural network. <em>EXSY</em>, <em>42</em>(2), e13225. (<a href='https://doi.org/10.1111/exsy.13225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In lymphoblastic leukaemia (ALL), the bone marrow naturally produces immature cells. Each year ALL is diagnosed with over 6500 instances, and the trend is still going upward. Technological advancements in AI and big data analytics help doctors and radiologists make accurate and efficient clinical decisions. The proposed method consists of two core steps: segmentation and classification based on the quantum convolutional networks. A three-dimensional U-network is proposed having 70 layers that are trained on the optimal hyperparameters, which provides 0.98 dice scores. The four-qubit quantum transfer learning model is proposed for classifying different types of blood cells. The accuracies achieved are 0.99 on blast cells, 0.99 on Basophils, 0.98 on Eosinophils, 0.97 on Neutrophils, 0.99 on Lymphocytes, and 0.96 on Monocytes. The proposed classification model provides 0.99 average accuracy.},
  archive      = {J_EXSY},
  author       = {Javeria Amin and Muhammad Almas Anjum and Senka Krivic and Muhammad Irfan Sharif},
  doi          = {10.1111/exsy.13225},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13225},
  shortjournal = {Expert Syst.},
  title        = {RETRACTED: Segmentation and classification of lymphoblastic leukaemia using quantum neural network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED: An efficient deep learning-based video captioning framework using multi-modal features. <em>EXSY</em>, <em>42</em>(2), e12920. (<a href='https://doi.org/10.1111/exsy.12920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual understanding has become more significant in gathering information in many real-life applications. For a human, it is a trivial task to understand the content in a visual, however the same is a challenging task for a machine. Generating captions for images and videos for better understanding the situation is gaining more importance as they have wide application in assistive technologies, automatic video captioning, video summarizing, subtitling, blind navigation, and so on. The visual understanding framework will analyse the content present in the video to generate semantically accurate caption for the visual. Apart from the visual understanding of the situation, the gained semantics must be represented in a natural language like English, for which we require a language model. Hence, the semantics and grammar of the sentences being generated in English is yet another challenge. The captured description of the video is supposed to collect information of not just the objects contained in the scene, but it should also express how these objects are related to each other through the activity described in the scene, thus making the entire process a complex task for a machine. This work is an attempt to peep into the various methods for video captioning using deep learning methodologies, datasets that are widely used for these tasks and various evaluation metrics that are used for the performance comparison. The insights that we gained from our premiere work and the extensive literature review made us capable of proposing a practical, efficient video captioning architecture using deep learning which that will utilize the audio clues, external knowledge and attention context to improve the captioning process. Quantum deep learning architectures can bring about extraordinary results in object recognition tasks and feature extraction using convolutions.},
  archive      = {J_EXSY},
  author       = {Soumya Varma and Dinesh Peter James},
  doi          = {10.1111/exsy.12920},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e12920},
  shortjournal = {Expert Syst.},
  title        = {RETRACTED: An efficient deep learning-based video captioning framework using multi-modal features},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED: DAE-GAN: An autoencoder based adversarial network for gaussian denoising. <em>EXSY</em>, <em>42</em>(2), e12709. (<a href='https://doi.org/10.1111/exsy.12709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is one of the most classic problems in computer vision for restoring corrupted images. It has been approached by using various traditional state of the art architectures in convolutional neural network (CNN), which has demonstrated considerably better results than the prior methods. There has been recent advancements in approaching the problem using generative adversarial networks (GAN), which has shown considerable promise. In this paper, we propose a novel denoising adversarial architecture to generate denoised image samples from a noisy distribution. A denoising autoencoder has been employed as the Generator to learn image distributions and generate denoised images while the discriminator penalizes the generated output. We employ an additive loss comprising of root mean square and mean absolute error for the Generator function. The model is trained adversarially followed by extensive experiments. We achieved PSNR and SSIM values comparable to the state-of-the-art for a range of blind and non-blind Gaussian noise.},
  archive      = {J_EXSY},
  author       = {Abhishek Samanta and Aheli Saha and Suresh Chandra Satapathy and Hong Lin},
  doi          = {10.1111/exsy.12709},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e12709},
  shortjournal = {Expert Syst.},
  title        = {RETRACTED: DAE-GAN: An autoencoder based adversarial network for gaussian denoising},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive analysis of global terrorist attacks using lexical patterns across multiple datasets. <em>EXSY</em>, <em>42</em>(1), e13808. (<a href='https://doi.org/10.1111/exsy.13808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Worldwide terrorist activities continue to pose a significant threat to global security and stability. The unpredictable nature of these acts necessitates advanced analytical approaches to enhance prevention and response strategies. This study examines undetectable word extensions across multiple datasets, using terrorism-related datasets as a case study. This research aims to overcome constraints in current predictive models associated with terrorist attack prediction. While many studies have used the GTD for predicting global terrorist attacks, this study expands beyond GTD by evaluating a corpus of terrorism incidents to enhance predictive analysis through lexical usage. The study employs several machine learning algorithms including Decision Tree (DT), Bootstrap Aggregating (BA), Random Forest (RF), Extra Trees (ET) and XGBoost (XG) algorithms for evaluation. Our approach integrates multiple datasets to reduce dependence on GTD alone. Findings indicate that RF performs best on the GTD database, with 90.20% accuracy in predicting worldwide terrorist attacks. DT achieves 90.40% accuracy when applied to the TF–IDF dataset. XG demonstrates superior performance across various aggregation settings and feature sets, achieving 95.77% accuracy in forecasting worldwide terrorist acts. XG's consistent and effective performance across various contexts highlights its versatility. Its high adaptability and robust performance position it as the preferred algorithm for conducting predictive research on global terrorist acts using the available datasets. Our research findings underscore the importance of incorporating diverse datasets to enhance understanding of terrorist activities and improve predictive capabilities.},
  archive      = {J_EXSY},
  author       = {Mohammed Salem Atoum and Ala Abdulsalam Alarood and Eesa Abdullah Alsolmi and Areej Obeidat and Moutaz Alazab},
  doi          = {10.1111/exsy.13808},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13808},
  shortjournal = {Expert Syst.},
  title        = {Predictive analysis of global terrorist attacks using lexical patterns across multiple datasets},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing disability healthcare solutions through privacy-preserving federated learning with theme framework. <em>EXSY</em>, <em>42</em>(1), e13807. (<a href='https://doi.org/10.1111/exsy.13807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of machine learning, particularly federated learning, in collaborative model training, has demonstrated significant potential for enhancing diversity and efficiency in outcomes. In the healthcare domain, particularly healthcare with disabilities, the sensitive nature of data presents a significant challenge as sharing even the computation on these data can risk exposing personal health information. This research addresses the problem of enabling shared model training for healthcare data—particularly with disabilities decreasing the risk of leaking or compromising sensitive information. Technologies such as federated learning provide solution for decentralised model training but fall short in addressing concerns related to trust building, accountability and control over participation and data. We propose a framework that integrates federated learning with advanced identity management as well as privacy and trust management technologies. Our framework called Theme (Trusted Healthcare Machine Learning Environment) leverages digital identities (e.g., W3C decentralised identifiers and verified credentials) and policy enforcements to regulate participation. This is to ensure that only authorised and trusted entities can contribute to the model training. Additionally, we introduce the mechanisms to track contributions per participant and offer the flexibility for participants to opt out of model training at any point. Participants can choose to be either contributors (providers) or consumers (model users) or both, and they can also choose to participate in subset of activities. This is particularly important in healthcare settings, where individuals and healthcare institutions have the flexibility to control how their data are used without compromising the benefits. In summary, this research work contributes to privacy preserving shared model training leveraging federated learning without exposing sensitive data; trust and accountability mechanisms; contribution tracking per participant for accountability and back-tracking; and fine-grained control and autonomy per participant. By addressing the specific needs of healthcare data for people with disabilities or such institutions, the Theme framework offers a robust solution to balance the benefits of shared machine learning with critical need to protecting sensitive data.},
  archive      = {J_EXSY},
  author       = {Madallah Alruwaili and Muhammad Hameed Siddiqi and Muhammad Idris and Salman Alruwaili and Abdullah Saleh Alanazi and Faheem Khan},
  doi          = {10.1111/exsy.13807},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13807},
  shortjournal = {Expert Syst.},
  title        = {Advancing disability healthcare solutions through privacy-preserving federated learning with theme framework},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel transformer attention-based approach for sarcasm detection. <em>EXSY</em>, <em>42</em>(1), e13686. (<a href='https://doi.org/10.1111/exsy.13686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm detection is challenging in natural language processing (NLP) due to its implicit nature, particularly in low-resource languages. Despite limited linguistic resources, researchers have focused on detecting sarcasm on social media platforms, leading to the development of specialized algorithms and models tailored for Urdu text. Researchers have significantly improved sarcasm detection accuracy by analysing patterns and linguistic cues unique to the language, thereby advancing NLP capabilities in low-resource languages and facilitating better communication within diverse online communities. This work introduces UrduSarcasmNet, a novel architecture using cascaded group multi-head attention, which is an innovative deep-learning approach that employs cascaded group multi-head attention techniques to enhance effectiveness. By employing a series of attention heads in a cascading manner, our model captures both local and global contexts, facilitating a more comprehensive understanding of the text. Adding a group attention mechanism enables simultaneous consideration of various sub-topics within the content, thereby enriching the model's effectiveness. The proposed UrduSarcasmNet approach is validated with the Urdu-sarcastic-tweets-dataset (UST) dataset, which has been curated for this purpose. Our experimental results on the UST dataset show that the proposed UrduSarcasmNet framework outperforms the simple-attention mechanism and other state-of-the-art models. This research significantly enhances natural language processing (NLP) and provides valuable insights for improving sarcasm recognition tools in low-resource languages like Urdu.},
  archive      = {J_EXSY},
  author       = {Shumaila Khan and Iqbal Qasim and Wahab Khan and Khursheed Aurangzeb and Javed Ali Khan and Muhammad Shahid Anwar},
  doi          = {10.1111/exsy.13686},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13686},
  shortjournal = {Expert Syst.},
  title        = {A novel transformer attention-based approach for sarcasm detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enterprise violation risk deduction combining generative AI and event evolution graph. <em>EXSY</em>, <em>42</em>(1), e13622. (<a href='https://doi.org/10.1111/exsy.13622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current realms of scientific research and commercial applications, the risk inference of regulatory violations by publicly listed enterprises has attracted considerable attention. However, there are some problems in the existing research on the deduction and prediction of violation risk of listed enterprises, such as the lack of analysis of the causal logic association between violation events, the low interpretability and effectiveness of the deduction and the lack of training data. To solve these problems, we propose a framework for enterprise violation risk deduction based on generative AI and event evolution graphs. First, the generative AI technology was used to generate a new text summary of the lengthy and complex enterprise violation announcement to realize a concise overview of the violation matters. Second, by fine-tuning the generative AI model, an event entity and causality extraction framework based on automated data augmentation are proposed, and the UIE (Unified Structure Generation for Universal Information Extraction) event entity extraction model is used to create the event entity extraction for listed enterprises ‘violations. Then, a causality extraction model CDDP-GAT (Event Causality Extraction Based on Chinese Dictionary and Dependency Parsing of GAT) is proposed. This model aims to identify and analyse the causal links between corporate breaches, thereby deepening the understanding of the event logic. Then, the merger of similar events was realized, and the causal correlation weights between enterprise violation-related events were evaluated. Finally, the listed enterprise's violation risk event evolution graph was constructed, and the enterprise violation risk deduction was carried out to form an expert system of financial violations. The deduction results show that the method can effectively reveal signs of enterprise violations and adverse consequences.},
  archive      = {J_EXSY},
  author       = {Chao Zhong and Pengjun Li and Jinlong Wang and Xiaoyun Xiong and Zhihan Lv and Xiaochen Zhou and Qixin Zhao},
  doi          = {10.1111/exsy.13622},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13622},
  shortjournal = {Expert Syst.},
  title        = {Enterprise violation risk deduction combining generative AI and event evolution graph},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of liaoning province steel import and export trade based on deep learning models. <em>EXSY</em>, <em>42</em>(1), e13615. (<a href='https://doi.org/10.1111/exsy.13615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of deep learning, time series forecasting, particularly for economic and trade data, is a critical area of research. This study introduces a hybrid of auto regressive integrated moving average and gated recurrent unit (ARIMA-GRU) to enhance the prediction of steel import and export trade in Liaoning Province, addressing the limitations of traditional time series methods. Traditional models like ARIMA excel with linear data but often struggle with non-linear patterns and long-term dependencies. The ARIMA-GRU model combines ARIMA's linear data analysis with GRU's proficiency in non-linear pattern recognition, effectively capturing complex dynamics in economic datasets. Our experiments show that this hybrid approach surpasses traditional models in accuracy and reliability for forecasting steel trade, providing valuable insights for economic planning and strategic decision-making. This innovative approach not only advances the field of economic forecasting but also demonstrates the potential of integrating deep learning techniques in complex data analysis.},
  archive      = {J_EXSY},
  author       = {Limin Zhang},
  doi          = {10.1111/exsy.13615},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13615},
  shortjournal = {Expert Syst.},
  title        = {Prediction of liaoning province steel import and export trade based on deep learning models},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based aggregate analysis to identify cut-off points for decision-making in pancreatic cancer detection. <em>EXSY</em>, <em>42</em>(1), e13614. (<a href='https://doi.org/10.1111/exsy.13614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the problem of detecting pancreatic cancer by classifying computed tomography (CT) images into cancerous and non-cancerous classes using the proposed deep learning-based aggregate analysis framework. The application of deep learning, as a branch of machine learning and artificial intelligence, to specific medical challenges can lead to the early detection of diseases, thus accelerating the process towards timely and effective intervention. The concept of classification is to reasonably select an optimal cut-off point, which is used as a threshold for evaluating the model results. The choice of this point is key to ensure efficient evaluation of the classification results, which directly affects the diagnostic accuracy. A significant aspect of this research is the incorporation of private CT images from Vilnius University Hospital Santaros Klinikos, combined with publicly available data sets. To investigate the capabilities of the deep learning-based framework and to maximize pancreatic cancer diagnostic performance, experimental studies were carried out combining data from different sources. Classification accuracy metrics such as the Youden index, (0, 1)-criterion, Matthew's correlation coefficient, the F1 score, LR+, LR−, balanced accuracy, and g-mean were used to find the optimal cut-off point in order to balance sensitivity and specificity. By carefully analyzing and comparing the obtained results, we aim to develop a reliable system that will not only improve the accuracy of pancreatic cancer detection but also have wider application in the early diagnosis of other malignancies.},
  archive      = {J_EXSY},
  author       = {Gintautas Dzemyda and Olga Kurasova and Viktor Medvedev and Aušra Šubonienė and Aistė Gulla and Artūras Samuilis and Džiugas Jagminas and Kęstutis Strupas},
  doi          = {10.1111/exsy.13614},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13614},
  shortjournal = {Expert Syst.},
  title        = {Deep learning-based aggregate analysis to identify cut-off points for decision-making in pancreatic cancer detection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling of healthcare data analytics using optimal machine learning model in big data environment. <em>EXSY</em>, <em>42</em>(1), e13612. (<a href='https://doi.org/10.1111/exsy.13612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in wireless networking, big data technologies, namely Internet of Things (IoT) 5G networks, health care big data analytics, and other technologies in artificial intelligence (AI) and wearables, have supported the progression of intellectual disease diagnosis methods. Medical data covers all patient data such as pharmacy texts, electronic health reports (EHR), prescriptions, study data from medical journals, clinical photographs, and diagnostic reports. Big data is a renowned method in the healthcare sector, with beneficial datasets that are highly difficult, voluminous, and rapid for healthcare providers for interpreting and computing using prevailing tools. This study combines concepts like deep learning (DL) and big data analytics in medical field. This article develops a new healthcare data analytics using optimal machine learning model in big data environment (HDAOML-BDE) technique. The presented HDAOML-BDE technique mainly aims to examine the healthcare data for disease detection and classification in the big data environment. For handling big data, the HDAOML-BDE technique uses Hadoop MapReduce environment. In addition, the HDAOML-BDE technique uses manta ray foraging optimization-based feature selection (MRFO-FS) technique to reduce high dimensionality problems. Moreover, the HDAOML-BDE method uses relevance vector machine (RVM) model for the healthcare data environment. Furthermore, the arithmetic optimization algorithm (AOA) is utilized for the parameter tuning of the RVM classifier. The simulation results of the HDAOML-BDE technique are tested on a healthcare dataset, and the outcomes portray the improved performance of the HDAOML-BDE strategy over recent approaches in different measures.},
  archive      = {J_EXSY},
  author       = {Chelladurai Fancy and Nagappan Krishnaraj and K. Ishwarya and G. Raja and Shyamala Chandrasekaran},
  doi          = {10.1111/exsy.13612},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13612},
  shortjournal = {Expert Syst.},
  title        = {Modelling of healthcare data analytics using optimal machine learning model in big data environment},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison on resilience and energy efficiency of authentication schemes in IoT networks. <em>EXSY</em>, <em>42</em>(1), e13603. (<a href='https://doi.org/10.1111/exsy.13603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network security is one of the primary concerns when deploying IoT applications. A proper authentication scheme can strengthen network security and resilience against malicious attacks. The quantitative comparison among authentication schemes is rarely found. Thus, it is difficult to choose the appropriate authentication scheme objectively. This paper presents a quantitative comparison of three authentication schemes, including blockchain-based authentication, a widely discussed new approach for IoT security. This paper focuses on network-level simulation instead of the protocol, providing a different angle when evaluating an authentication scheme. The simulations include five common topologies in IoT networks under five different attack strategies. The results show that the blockchain-based scheme has the most substantial resilience compared to PKI-based and PSK methods, while the computational cost is 1% less than the PKI-based method. In addition, the PSK method is most energy efficient as its computational cost is only around 1% of PKI-based and blockchain-based methods.},
  archive      = {J_EXSY},
  author       = {Chi Ho Lau and Sammy Chan},
  doi          = {10.1111/exsy.13603},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13603},
  shortjournal = {Expert Syst.},
  title        = {Comparison on resilience and energy efficiency of authentication schemes in IoT networks},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-enabled decentralized service selection for QoS-aware cloud manufacturing. <em>EXSY</em>, <em>42</em>(1), e13602. (<a href='https://doi.org/10.1111/exsy.13602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cloud manufacturing has brought both opportunities and challenges to the manufacturing industry. Cloud manufacturing enables global manufacturing resources to be unified and shared, thus breaking down geographical constraints to enhance the level and efficiency of manufacturing. However, with the explosive growth of manufacturing resources and user demands, traditional cloud manufacturing platforms will face problems of insufficient computility, lack of real-time data and difficulties in securing user privacy during the service selection process. In this article, a blockchain-based decentralized cloud manufacturing service selection method is proposed, where the computility resource is deployed in multiple distributed nodes rather than the traditional centralized cloud manufacturing platform to solve the problem of insufficient computility. The credibility of the users is evaluated based on their performance on the contract and the PBFT consensus algorithm is improved based on the credibility of the users. In addition, a tri-chain blockchain data storage model is designed to ensure the security, real-time and transparency of data in the cloud manufacturing service selection process. The experimental results show that the method both speeds up service selection process and improves the quality of service selection results, and achieves a significant increase in manufacturing efficiency.},
  archive      = {J_EXSY},
  author       = {Ke Meng and Zhiyong Wu and Muhammad Bilal and Xiaoyu Xia and Xiaolong Xu},
  doi          = {10.1111/exsy.13602},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13602},
  shortjournal = {Expert Syst.},
  title        = {Blockchain-enabled decentralized service selection for QoS-aware cloud manufacturing},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FoodBlock: A secure and cost-optimal framework for online food ordering using blockchain. <em>EXSY</em>, <em>42</em>(1), e13601. (<a href='https://doi.org/10.1111/exsy.13601'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The internet has drastically changed trade and interpersonal interactions since the introduction of the pandemic. Food delivery services have expanded significantly in recent years. These services link customers with restaurants using the internet, enabling quick delivery of mobile meals. But as these businesses grow, dealing with problems like fraud and inefficiencies that may lower the quality of their services is more crucial than ever. This article lists the drawbacks of the existing food delivery services and recommends how to fix them. A blockchain-based architecture is also used to increase security and reliability, and an auction process encourages fairness and transparency for both riders and restaurants. We have developed an auction model where the riders can quote the price they estimate for a particular delivery. Additionally, a machine-predicted bid is also used to prevent the case where the riders would try to exploit the restaurants. This method leads to a better way of implementing online food delivery services, safeguarding the interests of customers, riders, and restaurants.},
  archive      = {J_EXSY},
  author       = {Uday Mittal and Shivam Goyal and Egna Praneeth Gummana and Vinay Chamola and Shivi Agarwal and Trilok Mathur},
  doi          = {10.1111/exsy.13601},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13601},
  shortjournal = {Expert Syst.},
  title        = {FoodBlock: A secure and cost-optimal framework for online food ordering using blockchain},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-based provenance framework for duplicates identification and data quality enhancement. <em>EXSY</em>, <em>42</em>(1), e13600. (<a href='https://doi.org/10.1111/exsy.13600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing revolutionizes data management by offering centralized repositories or services accessible over the Internet. These services, hosted by a single provider or distributed across multiple entities, facilitate seamless access for users and applications. Additionally, cloud technology enables federated search capabilities, allowing organizations to amalgamate data from diverse sources and perform comprehensive searches. However, such integration often leads to challenges in data quality and duplication due to structural disparities among datasets, including variations in metadata. This research presents a novel provenance-based search model designed to enhance data quality within cloud environments. The model expands the traditional concept of a single canonical URL by incorporating provenance data, thus providing users with diverse search options. Leveraging this model, the study conducts inferential analyses to improve data accuracy and identify duplicate entries effectively. To verify the proposed model, two research paper datasets from Kaggle and DBLP repositories are utilized, and the model effectively identifies duplicates, even with partial queries. Tests demonstrate the system's ability to remove duplicates based on title or author, in both single and distributed dataset scenarios. Traditional search engines struggle with duplicate content, resulting in biased results or inefficient crawling. In contrast, this research uses provenance data to improve search capabilities, overcoming these limitations.},
  archive      = {J_EXSY},
  author       = {Fakhri Alam Khan},
  doi          = {10.1111/exsy.13600},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13600},
  shortjournal = {Expert Syst.},
  title        = {Cloud-based provenance framework for duplicates identification and data quality enhancement},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating long short-term memory approach for extremist messages detection in kazakh language. <em>EXSY</em>, <em>42</em>(1), e13595. (<a href='https://doi.org/10.1111/exsy.13595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a noticeable increase in both individuals and organizations utilizing social networks for illicit purposes. This trend can be viewed as a potential threat to the national security of the country. In this article, the authors pay attention to how various extremist organizations use social networks in their activities, and offer LSTM-based models for classifying extremist texts in Kazakh on web resources. The main purpose of the article is to classify Kazakh texts in social networks into extremist and non-extremist classes. The authors employed techniques such as Tf-Idf, Word2Vec, Bag of Words (BoW), and n-grams in experiments. A list of extremist keywords in the Kazakh language and, accordingly, a corpus of extremist texts in the Kazakh language were created for training and testing machine learning methods. As a result, the authors introduced a model that demonstrated superior performance across all evaluation metrics in machine learning for detecting extremist texts in the Kazakh language. The theoretical significance of this study lies in its comprehensive exploration of methods and algorithms for detecting extremist activities and organizations. The foundational findings derived from this research can contribute valuable insights to the global scientific community. The practical implications, including the developed methodology can be utilized by authorized entities to enhance information security, safeguard critical infrastructure, and combat online extremism.},
  archive      = {J_EXSY},
  author       = {Mussiraliyeva Shynar Zhenisbekovna and Bolatbek Milana Aslanbekkyzy and Baispay Gulshat Bolatkyzy},
  doi          = {10.1111/exsy.13595},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13595},
  shortjournal = {Expert Syst.},
  title        = {Investigating long short-term memory approach for extremist messages detection in kazakh language},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven allocation of renewables quota among regional power industries under the policy of renewable electricity standard. <em>EXSY</em>, <em>42</em>(1), e13582. (<a href='https://doi.org/10.1111/exsy.13582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {China is struggling to facilitate the application of renewable portfolio standards to realize sustainable economic growth. As such, improving the current distribution mechanism is crucial. In this paper, the context-dependent data envelopment analysis and multi-objective linear programming are combined to allocate the renewables quota for each province. This integrated approach can maximize total electricity generation while minimizing the total CO 2 emission with considering the disparity of production technology level. Then, the extended Gini coefficient is employed to assess the fairness of new quota mechanism. We find that (1) the eastern region is the most efficient during the power generation process. During 2016–2019, the efficiency in the western region presents an upward trend. (2) The allocation results indicate that Inner Mongolia and Qinghai have the greatest pressure to absorb renewable energy electricity, while Guangdong and Guizhou can instead reduce the most. Shandong and Inner Mongolia face the greatest burden in conserving non-renewable electricity. (3) Compared to 2020, the newly allocated scheme can mitigate inequality, with the Gini coefficient changing from 0.264 in 2020 to 0.248 after the allocation. Meanwhile, the reallocation reduces the Gini coefficient related to renewable electricity, non-renewable electricity, and CO 2 emissions by 0.003, 0.028, and 0.073, respectively at the 2020 level.},
  archive      = {J_EXSY},
  author       = {Xiaohong Liu and Chengzhen Xu and Yinghao Pan and Xingchen Li and Qingyuan Zhu},
  doi          = {10.1111/exsy.13582},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13582},
  shortjournal = {Expert Syst.},
  title        = {Data-driven allocation of renewables quota among regional power industries under the policy of renewable electricity standard},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating singular value decomposition with deep learning for enhanced travel time estimation in multimodal freight transportation networks. <em>EXSY</em>, <em>42</em>(1), e13581. (<a href='https://doi.org/10.1111/exsy.13581'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal freight transport allows switching among various modes of transportation to efficiently utilize transport facilities. A multimodal transport system incorporates geographical scales from global to local. Travel time estimation in a multi-modal cargo transportation network is essential for enhancing supply chain (SC) and logistics operations. Accurate travel time prediction is of great importance for cargo transportation, as it enables SC participants to increase logistics efficiency and quality. It requires adequate input data, which can be generated. In recent times, the machine learning (ML) algorithm has been well-suited to resolve complex and nonlinear relationships in the collected tracking data. This study designs a deep learning-powered travel time estimation in multimodal freight transportation networks (DLTTE-MFTN) technique. The goal of the DLTTE-MFTN technique is to estimate the travel time using a hyperparameter-tuned ensemble learning approach. To achieve this, the DLTTE-MFTN method initially undergoes data pre-processing to convert the input raw data into a useful format. In addition, the singular value decomposition (SVD) model can be applied for feature dimensionality reduction in multimodal transport data, considerably improving travel time prediction. Besides, the DLTTE-MFTN method estimates travel time using an ensemble of three DL approaches including one-dimensional convolutional neural network (1D-CNN), stacked autoencoder (SAE) attention, and recurrent neural network (RNN). Finally, the hyperparameter tuning of the DL models takes place using the whale optimization algorithm (WOA). The performance analysis of the DLTTE-MFTN method takes place using the Kaggle dataset. The experimental results stated that the DLTTE-MFTN technique attains superior performance over other ML and DL models.},
  archive      = {J_EXSY},
  author       = {Mohanad R. Aljanabi and Keivan Borna and Shamsollah Ghanbari and Ahmed J. Obaid},
  doi          = {10.1111/exsy.13581},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13581},
  shortjournal = {Expert Syst.},
  title        = {Integrating singular value decomposition with deep learning for enhanced travel time estimation in multimodal freight transportation networks},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extracting the explore-exploit intelligence of physarum to manage the sustainability of an enterprise network. <em>EXSY</em>, <em>42</em>(1), e13580. (<a href='https://doi.org/10.1111/exsy.13580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we enhance the sustainability of an enterprise network (EN) by complementing it with an expert system that apprehends the explore-exploit behavioural intelligence of Physarum to survive against the attractive-adversarial nutritional environment. EN sustainability is dynamic since it depends on how well EN can react to an adversarial environment. We capture a reverse analogy to characterize EN's workload-environment with Physarum 's nutritive-environment, where the high volume of workloads at the backbone network corresponds to a poor-nutrient environment. The expert system explores EN to find out how to manage the workloads as Physarum handles its survivability, and exploits the users' workload patterns by grouping the highly communicating users together to redesign the network structure as Physarum 's intelligence to exploit energy from rich- and poor-nutrient food sources through redesigned tubular structures. We define two factors, such as nutrient-intensity and chemo-attractant to aid the redesign process. EN evolves through a set of redesigned clusters with an objective function to maximize its sustainability for a given set of explored workloads by minimizing the workloads through the backbone. EN evolution terminates when there is no change in the backbone utilization, resembling the organism's stay in a dormant state until it experiences a favourable environment. Our experimental results on an EN with a higher volume of workloads at the backbone producing 14.26 kWh energy consumption demonstrated that the developed expert system reduced the energy consumption to 11.27 kWh, thus enhanced the sustainability from 21% to 61%.},
  archive      = {J_EXSY},
  author       = {Sami J. Habib and Paulvanna Nayaki Marimuthu},
  doi          = {10.1111/exsy.13580},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13580},
  shortjournal = {Expert Syst.},
  title        = {Extracting the explore-exploit intelligence of physarum to manage the sustainability of an enterprise network},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security and privacy protection protocol based on edge computing in smart campus. <em>EXSY</em>, <em>42</em>(1), e13579. (<a href='https://doi.org/10.1111/exsy.13579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous emergence of modern Internet of Things, mobile Internet and other new generation of campus information technology and its extensive application in professional campus education, the development of college professional campus education information construction has entered a new stage of development. In view of higher requirements on data storage and network transmission delay of core business system in smart campus and more prominent wireless network security problems, this paper proposes a privacy protection protocol for smart campus based on chaotic mapping and edge computing. The protocol uses chaos mapping algorithm to protect user identity and negotiate session keys, and introduces edge computing to process authentication information on the edge server, which reduces the pressure of the core network and improves the authentication efficiency. This paper proves the security of the protocol by using BAN logic and Scyther, and verifies that the protocol can resist known attacks by analysing informal security. Through performance analysis and comparison, the proposed protocol has obvious advantages in computation and communication overhead.},
  archive      = {J_EXSY},
  author       = {Jing Liang and Yan Gong},
  doi          = {10.1111/exsy.13579},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13579},
  shortjournal = {Expert Syst.},
  title        = {Security and privacy protection protocol based on edge computing in smart campus},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing democratic processes in ecuador: A case study on neural network-driven OCR for election report verification. <em>EXSY</em>, <em>42</em>(1), e13578. (<a href='https://doi.org/10.1111/exsy.13578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {José Alejandro Mosquera Asimbaya and Gabriel M. Ramírez and Jaime Díaz-Arancibia},
  doi          = {10.1111/exsy.13578},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13578},
  shortjournal = {Expert Syst.},
  title        = {Advancing democratic processes in ecuador: A case study on neural network-driven OCR for election report verification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-enabled smart city management using multi-objective optimization strategies. <em>EXSY</em>, <em>42</em>(1), e13574. (<a href='https://doi.org/10.1111/exsy.13574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article outlines an integrated strategy that combines fuzzy multi-objective programming and a multi-criteria decision-making framework to achieve a number of transportation system management-related objectives. To rank fleet cars using various criteria enhancement, the Fuzzy technique for order of preference by resemblance to optimum solution are initially integrated. We then offer a novel Multi-Objective Possibilistic Linear Programming (MOPLP) model, based on the rankings of the vehicles, to determine the number of vehicles chosen for the work while taking into consideration the constraints placed on them. The search for optimal solutions to MOPs has benefited from the decades-long development of classical optimisation techniques. As a result of its potential for use in the real world, multi-objective optimisation (MOO) under uncertainty has gained traction in recent years. Recently, fuzzy set theory has been used to solve challenges in multi-objective linear programming. In this paper, we present a method for solving MOPs that makes use of both linear and non-linear membership functions to maximize user happiness. A hypothetical case study of transportation issue is taken here. This innovative approach improves management for the betterment of transportation networks in smart cities. The method is a more robust and versatile approach to the complex difficulties of contemporary urban transportation because it incorporates the TOPSIS method for vehicle ranking and then using Distance Operator and variable Membership Functions in fuzzy goal programming operation on the selected vehicles. The results provide valuable insights into the strengths and limitations of each technique, facilitating informed decision-making in real-world optimization scenarios.},
  archive      = {J_EXSY},
  author       = {Pinki and Rakesh Kumar and S. Vimal and Norah Saleh Alghamdi and Gaurav Dhiman and Subbulakshmi Pasupathi and Aarna Sood and Wattana Viriyasitavat and Assadaporn Sapsomboon and Amandeep Kaur},
  doi          = {10.1111/exsy.13574},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13574},
  shortjournal = {Expert Syst.},
  title        = {Artificial intelligence-enabled smart city management using multi-objective optimization strategies},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven IoT-fog analytics interactive smart system with data protection. <em>EXSY</em>, <em>42</em>(1), e13573. (<a href='https://doi.org/10.1111/exsy.13573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, fog computing has contributed significantly to the expansion of smart cities. It generated numerous real-time data and coped with time-constraint applications. They use sensors, physical objects, and network standards to monitor health imaging, traffic surveillance, industrial management, and so forth. Interactive applications have been proposed for the Internet of Things (IoT) to control wireless channels and improve communication. However, most of the existing lack of handing network interference and a reliable monitoring process. Moreover, many solutions are vulnerable to external threats, resulting in inconsistent and untrustworthy information for end users. Thus, this article proposes a framework that considers possible shortest paths to provide the most reliable and low-latency healthcare decision system using Q-learning. In addition, fog devices offer a trusted transmission interference system and are kept secure. The proposed framework is specially designed for rapid real-time medical data processing while enforcing robust security throughout the IoT-based transmission process. To identify the health sensors in pairwise objects with the initial computing cost, the proposed framework applies graph theory. It also extracts the most effective and least loaded communication edges by examining the behaviour of devices. Moreover, the identities of devices are verified using lightweight timestamps and secret information, accordingly, it decreases the privacy threats.},
  archive      = {J_EXSY},
  author       = {Khalid Haseeb and Tanzila Saba and Amjad Rehman and Naveed Abbas and Pyoung Won Kim},
  doi          = {10.1111/exsy.13573},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13573},
  shortjournal = {Expert Syst.},
  title        = {AI-driven IoT-fog analytics interactive smart system with data protection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI unveiled personalities: Profiling optimistic and pessimistic attitudes in hindi dataset using transformer-based models. <em>EXSY</em>, <em>42</em>(1), e13572. (<a href='https://doi.org/10.1111/exsy.13572'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both optimism and pessimism are intricately intertwined with an individual's inherent personality traits and people of all personality types can exhibit a wide range of attitudes and behaviours, including levels of optimism and pessimism. This paper undertakes a comprehensive analysis of optimistic and pessimistic tendencies present within Hindi textual data, employing transformer-based models. The research represents a pioneering effort to define and establish an interaction between the personality and attitude chakras within the realm of human psychology. Introducing an innovative “Chakra” system to illustrate complex interrelationships within human psychology, this work aligns the Myers-Briggs Type Indicator (MBTI) personality traits with optimistic and pessimistic attitudes, enriching our understanding of emotional projection in text. The study employs meticulously fine-tuned transformer models—specifically mBERT, XLM-RoBERTa, IndicBERT, mDeBERTa and a novel stacked mDeBERTa—trained on the novel Hindi dataset ‘मनोभाव’ (pronounced as Manobhav). Remarkably, the proposed Stacked mDeBERTa model outperforms others, recording an accuracy of 0.7785 along with elevated precision, recall, and F1 score values. Notably, its ROC AUC score of 0.7226 underlines its robustness in distinguishing between positive and negative emotional attitudes. The comparative analysis highlights the superiority of the Stacked mDeBERTa model in effectively capturing emotional attitudes in Hindi text.},
  archive      = {J_EXSY},
  author       = {Dipika Jain and Akshi Kumar},
  doi          = {10.1111/exsy.13572},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13572},
  shortjournal = {Expert Syst.},
  title        = {AI unveiled personalities: Profiling optimistic and pessimistic attitudes in hindi dataset using transformer-based models},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing COVID-19 misinformation detection through novel attention mechanisms in NLP. <em>EXSY</em>, <em>42</em>(1), e13571. (<a href='https://doi.org/10.1111/exsy.13571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of electronic media in recent decades has exponentially amplified the propagation of fake news, resulting in widespread confusion and misunderstanding among the masses, especially concerning critical topics like the COVID-19 pandemic. Consequently, detecting fake news on social media has emerged as a prominent area of research, attracting significant attention. This article introduces a novel cascaded group multi-head attention (CGMHA) model for COVID-19 fake news detection. Our research collected Twitter datasets with accurate and fake tweets in Urdu. The novel CGMHA model and depth-wise convolution capture local and global contextual information by employing multiple attention heads in a cascaded fashion, enabling a comprehensive understanding of fake news. While achieving state-of-the-art performance, we also highlight challenges such as language variations and misinformation nuances in the detection process, contributing to a more comprehensive understanding of the complexities involved in combatting fake news. Our proposed model surpasses the performance of state-of-the-art models in classifying fake news and achieves accuracy, F1 score, precision, and recall of 0.98, 0.96, 0.95, and 0.95, respectively.},
  archive      = {J_EXSY},
  author       = {Anbar Hussain and Wajid Ali and Awais Ahmad and Muhammad Shahid Iqbal and Syed Atif Moqurrab and Anand Paul and Sohail Jabbar and Sheeraz Akram},
  doi          = {10.1111/exsy.13571},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13571},
  shortjournal = {Expert Syst.},
  title        = {Enhancing COVID-19 misinformation detection through novel attention mechanisms in NLP},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced method for surface defect detection in workpieces based on improved MobileNetV2-SSD. <em>EXSY</em>, <em>42</em>(1), e13567. (<a href='https://doi.org/10.1111/exsy.13567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of workpieces production, surface defects are prone to occur, and these defects come in a wide variety and are often intermixed, making defect detection and classification exceptionally challenging. With the development of artificial intelligence and deep learning, to tackle this problem, this paper introduces an enhanced single shot multibox detector algorithm based on MobileNetV2 for the detection of surface defects. The method utilizes MobileNetv2 as the backbone of the feature extraction network to obtain six feature layers with different detection scales from the baseline network, that is, the original 1 × 1 feature prediction layer is deleted and a 75 × 75 feature prediction layer is added, which is closer to the specific features of the defects on the surface of the workpiece. In the additional feature layer, two parallel dilated convolution structures are connected, introducing depth-separable convolution and dilated convolution, combining skip connections and pixel-wise addition operations. A special feature fusion structure is proposed to perform feature fusion for small, medium and large target detection layers, which effectively solves the problem of missed and false detection. Moreover, it refines candidate bounding box aspect ratios within the training set through the utilization of the K-means clustering algorithm, ensuring a better match with real boxes. The experimental results demonstrate the effectiveness of the enhanced model, and the mean average precision value reaches 88.72%. Compared to other state-of-the-art detection methods, it exhibits superior capabilities.},
  archive      = {J_EXSY},
  author       = {Junlin Qiu and Yongshan Shen and Jianchu Lin and Yuxin Qin and Jian Yang and Hengdan Lei and Minghui Li},
  doi          = {10.1111/exsy.13567},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13567},
  shortjournal = {Expert Syst.},
  title        = {An enhanced method for surface defect detection in workpieces based on improved MobileNetV2-SSD},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy logic-based trusted routing protocol using vehicular cloud networks for smart cities. <em>EXSY</em>, <em>42</em>(1), e13561. (<a href='https://doi.org/10.1111/exsy.13561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the characteristics of vehicular ad hoc networks, the increased mobility of nodes and the inconsistency of wireless communication connections pose significant challenges for routing. As a result, researchers find it to be a fascinating topic to study. Furthermore, since these networks are vulnerable to various assaults, providing an authentication method between the source and destination nodes is crucial. How to route in such networks more efficiently, taking into account node mobility characteristics and accompanying massive historical data, is still a matter of discussion. Fuzzy logic-based Trusted Routing Protocol for vehicular cloud networks (FTRP) is proposed in this study that determines the secure path for data dissemination. Fuzzy Logic determines the node candidacy value and selects or rejects a path accordingly. The cloud assigns a confidence score to each vehicle based on the data it collects from nodes after each interaction. Our study identifies the secure path on the basis of trust along with factors such as speed, closeness to other nodes, signal strength and distance from the neighbouring nodes. Simulations of the novel protocol demonstrate that it can keep the packet delivery ratio high with little overhead and low delay. FTRP has significant implications for deploying Vehicular Cloud Networks using electric vehicle technologies in smart cities. The routing data is collected with the help of Internet of Technology (IOT) sensors. The information is transmitted between vehicles using IOT gateways.},
  archive      = {J_EXSY},
  author       = {Ramesh Kait and Sarbjit Kaur and Purushottam Sharma and Chhikara Ankita and Tajinder Kumar and Xiaochun Cheng},
  doi          = {10.1111/exsy.13561},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13561},
  shortjournal = {Expert Syst.},
  title        = {Fuzzy logic-based trusted routing protocol using vehicular cloud networks for smart cities},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new frontier in dashboard design: Evaluating an innovative meta-modelling approach through expert insights. <em>EXSY</em>, <em>42</em>(1), e13560. (<a href='https://doi.org/10.1111/exsy.13560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Andrea Vázquez-Ingelmo and Alicia García-Holgado and Francisco José García-Peñalvo and Roberto Therón and Ricardo Colomo-Palacios},
  doi          = {10.1111/exsy.13560},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13560},
  shortjournal = {Expert Syst.},
  title        = {A new frontier in dashboard design: Evaluating an innovative meta-modelling approach through expert insights},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring deep learning techniques for illuminance estimation. <em>EXSY</em>, <em>42</em>(1), e13559. (<a href='https://doi.org/10.1111/exsy.13559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning techniques had a revolutionary impact on several domains, including computer vision and image processing. This research paper focuses on exploring deep learning methods to achieve precise illuminance estimation, which holds significant importance in applications such as augmented reality, virtual reality, and photography. However, accurately estimating illuminance in complex scenes continues to pose challenges due to the intricate interplay between light sources, objects, and surfaces. The results of extensive experimentation demonstrate the immense potential of deep learning techniques in illuminance estimation. These techniques exhibit promising accuracy and robustness, enabling them to handle diverse scenarios effectively. The valuable insights derived from this study can serve as a guiding framework for future research endeavours and contribute to the development of efficient and precise methodologies for illuminance estimation across a wide range of practical applications.},
  archive      = {J_EXSY},
  author       = {Jairo Iván Vélez Bedoya and Manuel González Bedia and Luis Fernando Castillo Ossa and Jeferson Arango-López and Jaime Díaz-Arancibia},
  doi          = {10.1111/exsy.13559},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13559},
  shortjournal = {Expert Syst.},
  title        = {Exploring deep learning techniques for illuminance estimation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preventing and mitigating risks of rumours during major pandemics in the era of artificial intelligence: A perspective on vulnerability. <em>EXSY</em>, <em>42</em>(1), e13558. (<a href='https://doi.org/10.1111/exsy.13558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sudden outbreak of a major pandemic often leads to the widespread dissemination of rumours related to the event. The public serves as both disseminators and regulators of rumours. Enhancing the public's capability to defend against rumours and strengthening their resilience are crucial for turning the tide of the pandemic. This study focuses on the rumours surrounding the COVID-19 event and explores their impact on public vulnerability. Researching rumours during the pandemic reveals that in the era of artificial intelligence, the public's information needs, scepticism towards government resilience, and distrust in social relationships can deepen vulnerability, resulting in a proliferation of rumours. Therefore, it is proposed that governments should utilize new technologies, break away from traditional governance systems, and construct a rumour resolution system focusing on demand-oriented approaches, employing artificial intelligence techniques, and precision repairing of social trust. This approach aims to reduce public vulnerability during significant pandemics and enhance the government's capabilities in rumour prevention and emergency management.},
  archive      = {J_EXSY},
  author       = {Yuhuan Kong},
  doi          = {10.1111/exsy.13558},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13558},
  shortjournal = {Expert Syst.},
  title        = {Preventing and mitigating risks of rumours during major pandemics in the era of artificial intelligence: A perspective on vulnerability},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced slime mould optimization with convolutional BLSTM autoencoder based malware classification in intelligent systems. <em>EXSY</em>, <em>42</em>(1), e13557. (<a href='https://doi.org/10.1111/exsy.13557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous intelligent systems are artificial intelligence (AI) tools that act autonomously without direct human supervision. Cloud computing (CC) and Internet of Things (IoT) technologies find it challenging to deploy sufficient security defences because of the different structures, storage, and limited computing capabilities that make them more vulnerable to attacks. Security threats against IoT structures, devices, and applications are increasing with the demand for IoT technology. The training data available to AI models may be limited, which could impact their performance and generalizability. Adopting AI solutions in real-world situations may be impeded by compatibility concerns and the requirement for flawless integration. Malware classification errors can occur due to a lack of contextual knowledge, particularly in cases where benign files behave identically to malicious. Various studies were carried out on detecting IoT malware to evade the menaces posed by malicious code. However, prevailing techniques of IoT malware classification supported particular platforms or demanded complicated methods for attaining higher accuracy. This study introduces an enhanced slime mould optimization with a convolutional BLSTM autoencoder-based malware classification (ESMO-CBLSTMAE) system in the IoT cloud platform. The projected ESMO-CBLSTMAE system focuses on detecting and classifying malware in the IoT cloud platform. To achieve that, the ESMO-CBLSTMAE algorithm employs a min–max normalization technique for scaling the input dataset. The ESMO-CBLSTMAE method uses a convolutional bidirectional long short-term memory autoencoder (CBLSTM-AE) model for the malware detection process. Lastly, the ESMO method is executed for the optimum hyperparameter tuning of the CBLSTM-AE technique, which boosts the malware classification results. The experimental analysis of the ESMO-CBLSTMAE method is tested against a benchmark database, and the outcomes portray the greater efficacy of the ESMO-CBLSTMAE approach over other existing techniques. The proposed malware classification model achieved an accuracy of 98.57 and F Score of 80.77 and outperformed the existing models.},
  archive      = {J_EXSY},
  author       = {Shtwai Alsubai and Ashit Kumar Dutta and Abdul Rahaman Wahab Sait and Yasser Adnan Abu Jaish and Bader Hussain Alamer and Hussam Eldin Hussein Saad and Rashid Ayub},
  doi          = {10.1111/exsy.13557},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13557},
  shortjournal = {Expert Syst.},
  title        = {Enhanced slime mould optimization with convolutional BLSTM autoencoder based malware classification in intelligent systems},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of intrusion cyber-attacks in smart power grids using deep ensemble learning with metaheuristic-based optimization. <em>EXSY</em>, <em>42</em>(1), e13556. (<a href='https://doi.org/10.1111/exsy.13556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most advanced power grid design, known as a ‘smart power grid’, integrates information and communication technology (ICT) with a conventional grid system to enable remote management of electricity distribution. The intelligent cyber-physical architecture enables bidirectional, real-time data sharing between electricity suppliers and consumers through smart meters and advanced metering infrastructure (AMI). Data protection issues, such as data tampering, firmware exploitation, and the leakage of sensitive information arise due to the smart power grid's substantial reliance on ICT. To maintain reliable and efficient power distribution, these issues must be identified and resolved quickly. Intrusion detection is essential for providing secure services and alerting system administrators in the case of adversary attacks. This paper proposes an intrusion classification scheme that identifies several types of cyber attacks on modern smart power grids. Grey-Wolf metaheuristic optimization-based feature selection is used to learn non-linear, overlapping, and complex electrical grid properties. An extended deep-stacked ensemble technique is advanced by putting predictions from weak learners (CNNs) into a meta-learner (MLP). The outcomes of this approach are explained and confirmed using explainable AI (XAI). The publicly available dataset from Mississippi State University and Oak Ridge National Laboratory (MSU-ORNL) is used to conduct experiments. The experimental results show that the proposed method achieved a peak accuracy of 96.6% while scrutinizing the original MSU-ORNL data feature set and a maximum accuracy of 99% when analysing the selected feature set. Therefore, the proposed intrusion classification scheme may protect smart power grid systems against cyber security attacks.},
  archive      = {J_EXSY},
  author       = {Hamad Naeem and Farhan Ullah and Gautam Srivastava},
  doi          = {10.1111/exsy.13556},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13556},
  shortjournal = {Expert Syst.},
  title        = {Classification of intrusion cyber-attacks in smart power grids using deep ensemble learning with metaheuristic-based optimization},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-based deep learning architecture for DDoS cyber attack prediction. <em>EXSY</em>, <em>42</em>(1), e13552. (<a href='https://doi.org/10.1111/exsy.13552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional methodologies employed in detecting distributed denial-of-service attacks have frequently struggled to adapt to the dynamic and multi-faceted evolution of such threats. Furthermore, many of the contemporary detection and prevention solutions, while innovative, remain anchored to dedicated workstations, lacking the flexibility and scalability required in today's digital landscape. To bridge this technological chasm, this research introduces a state-of-the-art intrusion detection system firmly rooted in advanced Deep Learning techniques. By leveraging the expansive and adaptable nature of cloud-centric, service-oriented architectures, we not only bolster detection precision but also offer a solution designed for modern infrastructures. This system provides enterprises with a robust, easily deployable tool that is both versatile in its application and proactive in its defence approach, ensuring that networks remain resilient against the continuously evolving spectrum of cyber threats.},
  archive      = {J_EXSY},
  author       = {Jeferson Arango-López and Gustavo Isaza and Fabian Ramirez and Nestor Duque and Jose Montes},
  doi          = {10.1111/exsy.13552},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13552},
  shortjournal = {Expert Syst.},
  title        = {Cloud-based deep learning architecture for DDoS cyber attack prediction},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive solution to transfer learning of neural network controllers from earth to space environments. <em>EXSY</em>, <em>42</em>(1), e13549. (<a href='https://doi.org/10.1111/exsy.13549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compliant manipulation has long been a major constraint for grappling in robotic manipulators. To adopt robotic manipulators in space for the prospect of capturing space junk and transforming them into salvageable assets for re-use, robust adaptive manipulation would be key. We believe that a bio-inspired approach could provide human-like tactility required for robustness and adaptability in robotic manipulation. Given the similarity in form and dynamics between earth-based and space-based robotic manipulators, we first explored the transfer learning of neural network controllers as an avenue to address the challenges of limited computation resources onboard the spacecraft (space manipulator). We introduced a pre-trained and learned feedforward neural network for modelling the control error a priori. While the results were encouraging, there are major limitations of neural networks' capability to ensuring the transfer learning of similar earth-based dynamics to space-based dynamics, given that the parameters of contrast are fairly straightforward. We have demonstrated these limitations by presenting a novel approach that is inspired by human motor control. We explored the adaptability through a practical problem of transferring a neuro-controller from earth to space. With the results not as plausible as expected, an alternative adaptive controller has been learned to demonstrate a viable solution. The controller was trained entirely in simulation via rapid online adaptation of the robot's controller to the object's properties and environmental dynamics using only proprioception history. As a notable step, we have shown that appropriate models can be learned in this manner by training the control policy via reinforcement learning, which provides avenue for transferring the learned model from earth to space environments.},
  archive      = {J_EXSY},
  author       = {Collins Ogundipe and Alex Ellery},
  doi          = {10.1111/exsy.13549},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13549},
  shortjournal = {Expert Syst.},
  title        = {Adaptive solution to transfer learning of neural network controllers from earth to space environments},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial coordination and industrial pollution of urban agglomerations: Evidence from the yellow river basin in china. <em>EXSY</em>, <em>42</em>(1), e13548. (<a href='https://doi.org/10.1111/exsy.13548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population, economy, and other social factors of urban agglomerations in river basins bear the inescapable responsibility for water environmental pollution. This article utilizes data from 53 cities in three regional-level urban agglomerations in the Yellow River Basin (YRB) from 2006 to 2015 as research samples to analyse the impact of the coupling and coordination of physical space expansion (PSE) and social space structure (SSS) of urban agglomerations on industrial pollution. Multi-source remote sensing data is used to quantify the physical space expansion of urban agglomerations, while population and economic data are used to measure the social space structure. We analyse the coupling coordination degree and spatiotemporal evolution of the PSE and SSS of urban agglomerations by applying a coupling coordination degree model. Finally, the grey incidence model is used to rank the influencing factors of pollution. The research findings are summarized as follows: (1) Three regional-level urban agglomerations in the YRB show excessive expansion, with the Shandong Peninsula Urban Agglomeration (SPUA) being the most severe. (2) SPUA and Central Plains Urban Agglomeration (CPUA) exhibit the polycentric SSS, while the Guanzhong Plain Urban Agglomeration (GPUA) shows the monocentric SSS. (3) PSE and SSS of the three urban agglomerations are slightly uncoordinated, but show a trend of easing. The uncoordinated level between SPE and economic distribution in CPUA ranks the highest. (4) SSS exerts the greatest impact on pollution, while the pollution impact of PSE ranks lower. The pollution impact of the coupling coordination degree between SPE and SSS falls between the two. The research results provide a reference for sustainable urban development and planning in countries around the world.},
  archive      = {J_EXSY},
  author       = {Chao Hua and Zhenhua Zhang and Jianjun Miao and Jingwei Han and Zhiyuan Zhu},
  doi          = {10.1111/exsy.13548},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13548},
  shortjournal = {Expert Syst.},
  title        = {Spatial coordination and industrial pollution of urban agglomerations: Evidence from the yellow river basin in china},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stakeholder perception towards a machine-learning-based digital platform for detection and management of autism spectrum disorder. <em>EXSY</em>, <em>42</em>(1), e13545. (<a href='https://doi.org/10.1111/exsy.13545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (ASD) affects approximately 1% of the population, presenting a significant healthcare challenge due to limited resources, particularly a shortage of clinicians, which impedes timely ASD detection and management in children. This study investigates stakeholder viewpoints regarding the effectiveness of integrating machine learning (ML) into the information and communications technology (ICT) platform for ASD detection and intervention. Primary stakeholders, including parents and clinicians, provide first hand experiences with this technology during and after the COVID-19 pandemic. The research identifies critical technology adoption factors by synthesizing stakeholder input based on user experiences, technology design, technology utility, and its impact. Additionally, the study gathers insights from potential investors interested in assistive technologies. Stakeholders unanimously acknowledge the pivotal role of technology in enhancing current ASD detection and management. However, their attitudes toward technology adoption exhibit divergent trends during and after the COVID-19 pandemic. The study highlights a shift toward a technology-enabled, human-centred framework, which gained prominence post-pandemic. Various factors contributing to this shift in stakeholder perspective were identified, including caregiver stress, technostress, and pandemic-induced environmental factors affecting stakeholders’ stress levels and motivating them to shift towards a human-centric model. Stakeholders emphasize the paramount importance of human-centred approaches in ASD detection and intervention, with technology serving as an empowering tool. Stakeholders also highlight the imperative ethical and legal considerations to foster trust and enhance the adoption of ML-based technology. Consequently, future research should delve into stakeholder perspectives within the framework of fairness, accountability, transparency, and ethics (FATE) to ensure these technologies’ responsible development and implementation.},
  archive      = {J_EXSY},
  author       = {Manu Kohli and Arpan Kumar Kar and Shuchi Sinha and Swati Kohli},
  doi          = {10.1111/exsy.13545},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13545},
  shortjournal = {Expert Syst.},
  title        = {Stakeholder perception towards a machine-learning-based digital platform for detection and management of autism spectrum disorder},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A smart decentralized identifiable distributed ledger technology-based blockchain (DIDLT-BC) model for cloud-IoT security. <em>EXSY</em>, <em>42</em>(1), e13544. (<a href='https://doi.org/10.1111/exsy.13544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most important and difficult challenge the digital society has recently faced is ensuring data privacy and security in cloud-based Internet of Things (IoT) technologies. As a result, many researchers believe that the blockchain's Distributed Ledger Technology (DLT) is a good choice for various clever applications. Nevertheless, it encountered constraints and difficulties with elevated computing expenses, temporal demands, operational intricacy, and diminished security. Therefore, the proposed work aims to develop a Decentralized Identifiable Distributed Ledger Technology-Blockchain (DIDLT-BC) framework that is intelligent and effective, requiring the least amount of computing complexity to ensure cloud IoT system safety. In this case, the Rabin algorithm produces the digital signature needed to start the transaction. The public and private keys are then created to verify the transactions. The block is then built using the DIDLT model, which includes the block header information, hash code, timestamp, nonce message, and transaction list. The primary purpose of the Blockchain Consent Algorithm (BCA) is to find solutions for numerous unreliable nodes with varying hash values. The novel contribution of this work is to incorporate the operations of Rabin digital data signature generation, DIDLT-based blockchain construction, and BCA algorithms for ensuring overall data security in IoT networks. With proper digital signature generation, key generation, blockchain construction and validation operations, secured data storage and retrieval are enabled in the cloud-IoT systems. By using this integrated DIDLT-BCA model, the security performance of the proposed system is greatly improved with 98% security, less execution time of up to 150 ms, and reduced mining time of up to 0.98 s.},
  archive      = {J_EXSY},
  author       = {Shitharth Selvarajan and Achyut Shankar and Mueen Uddin and Abdullah Saleh Alqahtani and Taher Al-Shehari and Wattana Viriyasitavat},
  doi          = {10.1111/exsy.13544},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13544},
  shortjournal = {Expert Syst.},
  title        = {A smart decentralized identifiable distributed ledger technology-based blockchain (DIDLT-BC) model for cloud-IoT security},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IETAFusion: An illumination enhancement and target-aware infrared and visible image fusion network for security system of smart city. <em>EXSY</em>, <em>42</em>(1), e13538. (<a href='https://doi.org/10.1111/exsy.13538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the environmental security monitoring of smart cities, the infrared and visible image fusion method deployed on intelligent systems based on cloud and fog computing plays a vital role in providing enhanced images for target detection systems. However, the fusion quality can be significantly influenced by the illumination of the monitoring scenario in visible images. Therefore, conventional methods typically suffer a severe performance drop under the condition of insufficient illumination. To tackle this issue, we propose an illumination enhancement and target-aware fusion method-based on artificial intelligence, which breaks the boundaries between the task of illumination enhancement and image fusion and provide a fusion result with better visual perception in nighttime scene. Specifically, we use a light-weight contrast enhancement module restore the brightness of the visible image. Moreover, a Swin Transformer-based backbone network is utilized to facilitate information exchange between the source images and enhance the capabilities of target awareness. Finally, the fused images are reconstructed by the contrast-texture retention module and reconstructor. The extensive experiments indicate that the proposed approach achieves improved performance both in human perception and quantitative analysis compared with the state-of-the-art methods.},
  archive      = {J_EXSY},
  author       = {Shuang Guo and Kun Wu and Seunggil Jeon and Xiaomin Yang},
  doi          = {10.1111/exsy.13538},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13538},
  shortjournal = {Expert Syst.},
  title        = {IETAFusion: An illumination enhancement and target-aware infrared and visible image fusion network for security system of smart city},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep convolutional neural networks information fusion and improved whale optimization algorithm based smart oral squamous cell carcinoma classification framework using histopathological images. <em>EXSY</em>, <em>42</em>(1), e13536. (<a href='https://doi.org/10.1111/exsy.13536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most prevalent type of cancer worldwide is mouth cancer. Around 2.5% of deaths are reported annually due to oral cancer in 2023. Early diagnosis of oral squamous cell carcinoma (OSCC), a prevalent oral cavity cancer, is essential for treating and recovering patients. A few computerized techniques exist but are focused on traditional machine learning methods, such as handcrafted features. In this work, we proposed a fully automated architecture based on Self-Attention convolutional neural network and Residual Network information fusion and optimization. In the proposed framework, the augmentation process is performed on the training and testing samples, and then two developed deep models are trained. A self-attention MobileNet-V2 model is developed and trained using an augmented dataset. In parallel, a Self-Attention DarkNet-19 model is trained on the same dataset, whereas the hyperparameters have been initialized using the whale optimization algorithm (WOA). Features are extracted from the deeper layers of both models and fused using a canonical correlation analysis (CCA) approach. The CCA approach is further optimized using an improved WOA version named Quantum WOA that removes the irrelevant features and selects only important ones. The final selected features are classified using neural networks such as wide neural networks. The experimental process is performed on the augmented dataset that includes two sets: 100× and 400×. Using both sets, the proposed method obtained an accuracy of 98.7% and 96.3%. Comparison is conducted with a few state-of-the-art (SOTA) techniques and shows a significant improvement in accuracy and precision rate.},
  archive      = {J_EXSY},
  author       = {Momina Meer and Muhammad Attique Khan and Kiran Jabeen and Ahmed Ibrahim Alzahrani and Nasser Alalwan and Mohammad Shabaz and Faheem Khan},
  doi          = {10.1111/exsy.13536},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13536},
  shortjournal = {Expert Syst.},
  title        = {Deep convolutional neural networks information fusion and improved whale optimization algorithm based smart oral squamous cell carcinoma classification framework using histopathological images},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolution of confrontation and cooperation in simple organisms as a function of environmental resources and cost of a conflict. <em>EXSY</em>, <em>42</em>(1), e13527. (<a href='https://doi.org/10.1111/exsy.13527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The root cause of human conflict needs to be understood but it is currently unknown whether the decision to engage in conflict is an inherited or acquired trait. This article reports two experimental simulations which demonstrate that the level of confrontation in a population of simple organisms can be explained by the evolution of a simulated gene pool. Game theory and evolutionary algorithms were combined in a novel way to examine how six variables influenced the decision to confront in the competition for resources. The main variable was how the genetically determined rate of confrontation evolved as a function of environmental resources and cost of a conflict. The additional modulatory effects of four other variables were also considered in the first round of simulations. Two variables were responsive to the difference between resources and cost. Two other variables were responsive to the organism's health status. Taking a systematic approach, we examined how a population of 1000 organisms were evolving in environments with different levels of reward and punishment. During each cycle, each organism was paired with another organism and thus needed to decide whether to confront or cooperate. We used a genetic algorithm to simulate the evolution of the gene pool over 500 cycles. The first series of simulations demonstrated that the baseline rate of confrontation was very responsive to environmental conditions. Our results also indicate that the decision to confront or cooperate depended not only upon the immediate competitive conditions, in which the organisms evolved, but were also responsive to their own health status. The second series of simulations used zero-sum games to explore how risk levels varied as a function of the potential cost of engaging in a confrontation. In the second round of simulations, a simple form of memory was implemented. The results indicated that memory had a limited, but significant effect, while the cost of a conflict was highly predictive of the level of risk taken by the organisms. Our two series of simulations show that AI could contribute to answering psychological and societal questions. Our unique combination of techniques has brought to light several new insights into the mechanisms that drive the population towards cooperation and confrontation. The degree of generalizability of our results and future avenues for deepening our understanding of these evolutionary dynamics are discussed.},
  archive      = {J_EXSY},
  author       = {Philippe Chassy and Jon Cole and Chloe Brennan},
  doi          = {10.1111/exsy.13527},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13527},
  shortjournal = {Expert Syst.},
  title        = {Evolution of confrontation and cooperation in simple organisms as a function of environmental resources and cost of a conflict},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An AI knowledge-based system for police assistance in crime investigation. <em>EXSY</em>, <em>42</em>(1), e13524. (<a href='https://doi.org/10.1111/exsy.13524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fight against crime is often an arduous task overall when huge amounts of data have to be inspected, as is currently the case when it comes for example in the detection of criminal activity on the dark web. This work presents and describes an artificial intelligence (AI) based system that combines various tools to assist police or law enforcement agencies during their investigations, or at least mitigate the hard process of data collection, processing and analysis. The system is an early warning/early action system for crime investigation that supports law enforcement with different processes to collect and process data as well as having knowledge extraction tools. It helps to extract information during the investigation of a criminal case or even to detect possible criminal hotspots that may lead to further investigation or analysis of a criminal case Abu Al-Haija et al. (2022, Electronics , 11, 556). The functionality of the proposed system is illustrated through several examples using data collected from the dark web, which includes advertisements offering firearms-related products.},
  archive      = {J_EXSY},
  author       = {Carlos Fernandez-Basso and Karel Gutiérrez-Batista and Juan Gómez-Romero and M. Dolores Ruiz and Maria J. Martin-Bautista},
  doi          = {10.1111/exsy.13524},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13524},
  shortjournal = {Expert Syst.},
  title        = {An AI knowledge-based system for police assistance in crime investigation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved hybrid model for cardiovascular disease detection using machine learning in IoT. <em>EXSY</em>, <em>42</em>(1), e13520. (<a href='https://doi.org/10.1111/exsy.13520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease (CVD) believes to be a major cause of transience and indisposition worldwide. Early diagnosis and timely intervention are critical in preventing the progression of CVD and improving patient outcomes. Machine learning (ML) algorithms have emerged as powerful tools in CVD recognition, with the potential to assist physicians in making accurate and efficient diagnoses. This research paper explores the combination of multiple ML algorithms for CVD recognition, utilizing diverse datasets such as the Cleveland, Hungarian, Switzerland, statlog, and VA Long Beach datasets. Additionally, a CVD dataset comprising 12 attributes and 70,000 records is employed, demonstrating improved results through the proposed and trained model compared to previous prediction techniques for CVD. The performance of various ML techniques, including support vector machines (SVM), naive Bayes (NB), K-nearest neighbour (KNN), random forest (RF), and logistic regression (LR), is evaluated and compared. The impact of feature selection and feature scaling on the models' performance is also examined. An ensemble bagging technique is applied which is being embedded with other classifiers. LR classifier embedded with bagging techniques proved to be our proposed model. The findings reveal that the proposed Hybrid Linear Regression Bagging Model (HLRBM) outperforms other models. Furthermore, the study highlights the significance of data preprocessing techniques, such as data normalization and class balancing, which significantly enhance the performance of all models. To this end, standard scalar and synthetic minority over-sampling technique (SMOTE) are employed. The study emphasizes the importance of selecting an appropriate ensemble technique in conjunction with various ML algorithms and preprocessing methods for CVD prediction. Overall, the research provides valuable insights into the potential of ML in improving CVD risk assessment.},
  archive      = {J_EXSY},
  author       = {Arslan Naseer and Muhammad Muheet Khan and Fahim Arif and Waseem Iqbal and Awais Ahmad and Ijaz Ahmad},
  doi          = {10.1111/exsy.13520},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13520},
  shortjournal = {Expert Syst.},
  title        = {An improved hybrid model for cardiovascular disease detection using machine learning in IoT},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge maps for large-scale group decision making in social media content analysis. <em>EXSY</em>, <em>42</em>(1), e13509. (<a href='https://doi.org/10.1111/exsy.13509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale group decision making in social media content analysis, empowered by Artificial Intelligence tools, offers valuable insights in management research. The purpose of this paper is to identify and interpret the knowledge maps that emerge in social media research, based on deep learning algorithms and neural networks' impact in social content analytics. We conducted qualitative research, by analysing relevant articles focused on the role played by deep learning and neural networks in social media content analysis. The key themes retrieved from the selected articles have been included in visual representations, provided by Nvivo12 software, considering its text-mining capabilities. We are aware that large-scale group decision making (LSGDM) offers solutions for research issues on social media content analysis. This qualitative research highlights the value of LSGDM skills for researchers that developed an AI platform prototype for social media content analysis.},
  archive      = {J_EXSY},
  author       = {Alexandru Capatina and Adrian Micu and Angela-Eliza Micu and Samuel Ribeiro-Navarrete},
  doi          = {10.1111/exsy.13509},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13509},
  shortjournal = {Expert Syst.},
  title        = {Knowledge maps for large-scale group decision making in social media content analysis},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence web application firewall for advanced detection of web injection attacks. <em>EXSY</em>, <em>42</em>(1), e13505. (<a href='https://doi.org/10.1111/exsy.13505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, web services-based applications have an important presence in public and private organizations. The vulnerabilities that these types of applications may have pose an inherent potential risk to the business model of these organizations. These applications have the inherent risk of being used by organizations in such a way that their activity is affected and they become the main entry point for attackers who want to breach their security. The main barrier to this type of attack are web application firewalls (WAF), which are responsible for processing Hypertext Transfer Protocol requests between clients and web servers, classifying them and rejecting malicious requests. This type of (WAF) applications, for the most part, have regular expressions that correspond to general rules and allow detecting malicious requests that follow a pattern contained in them. However, due to the knowledge of these rules by attackers, it is easy to circumvent security and to impersonate a malicious request by an innocuous request. Therefore, in this article, we present a study of different models based on artificial intelligence techniques as Naïve Bayes, k-nearest neighbors, support vector machines, and linear regression to test their effectiveness in detecting malicious requests from a synthetic dataset containing more than 100,000 requests. The results obtained show that the implementation of these methods optimize the detection of malicious requests obtaining results between 92% and 99% of success in their classification.},
  archive      = {J_EXSY},
  author       = {Jesús-Ángel Román-Gallego and María-Luisa Pérez-Delgado and Marcos Luengo Viñuela and María-Concepción Vega-Hernández},
  doi          = {10.1111/exsy.13505},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13505},
  shortjournal = {Expert Syst.},
  title        = {Artificial intelligence web application firewall for advanced detection of web injection attacks},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Electronic music composition thinking using visual learning and visual sensing technology. <em>EXSY</em>, <em>42</em>(1), e13502. (<a href='https://doi.org/10.1111/exsy.13502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This exploration aims to provide a kind of interactive electronic music composition thinking based on visual learning and visual sensing technology to enhance the thinking process of electronic music composition. Initially, it summarizes the functionalities of the physics components in Unity and analyzes the principles of visual interaction implementation using Virtual Reality (VR) devices and Leap Motion in Unity. Subsequently, the composition of interactive electronic music works is designed and implemented based on ultrasonic sensor technology. Lastly, this exploration focuses on the complete visual design of the audio-visual integration of Musical Instrument Digital Interface (MIDI) music to represent Cymatics' images of the emotional content of the music. MIDI data can be assigned to provide various mapping possibilities between images and content in music visualization composition. This exploration also designs experiments based on sensory aftereffects and audio-visual synesthesia to further determine the counterpoint law between the shape of dynamic Cymatics' images and the timbre's audio-visual synesthetic results. The results indicate that Bass and Kick are distributed in the mid-low frequency and sub-low frequency range (20–160 Hz), Vocal and Lead are distributed in the mid-high frequency range (1280–2560 Hz), and Hihat is distributed in the high-frequency range (2560–5120 Hz). This exploration utilizes computer technology to create a music visualization method that conforms to the visual expression and aesthetic style of a multi-sensory experience.},
  archive      = {J_EXSY},
  author       = {Jian Pi},
  doi          = {10.1111/exsy.13502},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13502},
  shortjournal = {Expert Syst.},
  title        = {Electronic music composition thinking using visual learning and visual sensing technology},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fractal based automatic detection of complexity in COVID-19 X-ray images. <em>EXSY</em>, <em>42</em>(1), e13497. (<a href='https://doi.org/10.1111/exsy.13497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coronavirus was discovered in Wuhan, China, in December 2019. Scientists and medical practitioners have warned that this fatal virus could spread quickly from person to person in its early stages and its impact will be far more vigorous than the previously discovered viruses. The World Health Organization has given warning propaganda to all nations about this harmful virus. However, the diffusion speed of COVID-19 was so rapid that it spread to all countries much faster than the researchers predicted, causing the widespread human disaster. The genetic variations of COVID-19 have also astounded researchers today, as it modifies its mutation with very large genetic strains. The virus is regarded as a human version of the disease, resulting in the common cold, dry cough, and respiratory problems in severe cases. According to health organisations, the coronavirus directly affects the lungs, causing main problems such as difficulty breathing. It is also tough for physicians to diagnose the disease level properly by using the regular process. Even though the virus can be detected in regular testing methods, even computed tomography (CT) and X-ray images are widely used in the medical field to identify the respiratory problems caused by the COVID-19 virus. Like COVID-19, some other types of pneumonia respiratory diseases also affect the human lungs. The fractal dimension (FD) is an interesting non-linear measure to describe the complexity of visual images. In this context, the complexity of X-ray images is analysed by using the fractal dimension. The fractal dimension is an excellent non-linear measurement for describing the complexity of realistic images. The difference between the complexity of the X-ray images of COVID-19-infected patients and the X-ray images of other types of pneumonia respiratory diseases is well explained by using the fractal dimension. It is also concluded that the fractal dimension discriminates the lung diseases due to COVID-19 from various pneumonia respiratory diseases.},
  archive      = {J_EXSY},
  author       = {C. Thangaraj and D. Easwaramoorthy and G. Muhiuddin and Bilel Selmi and Vladimir Kulish},
  doi          = {10.1111/exsy.13497},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13497},
  shortjournal = {Expert Syst.},
  title        = {Fractal based automatic detection of complexity in COVID-19 X-ray images},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards explainable metaheuristics: Feature extraction from trajectory mining. <em>EXSY</em>, <em>42</em>(1), e13494. (<a href='https://doi.org/10.1111/exsy.13494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explaining the decisions made by population-based metaheuristics can often be considered difficult due to the stochastic nature of the mechanisms employed by these optimisation methods. As industries continue to adopt these methods in areas that increasingly require end-user input and confirmation, the need to explain the internal decisions being made has grown. In this article, we present our approach to the extraction of explanation supporting features using trajectory mining. This is achieved through the application of principal components analysis techniques to identify new methods of tracking population diversity changes post-runtime. The algorithm search trajectories were generated by solving a set of benchmark problems with a genetic algorithm and a univariate estimation of distribution algorithm and retaining all visited candidate solutions which were then projected to a lower dimensional sub-space. We also varied the selection pressure placed on high fitness solutions by altering the selection operators. Our results show that metrics derived from the projected sub-space algorithm search trajectories are capable of capturing key learning steps and how solution variable patterns that explain the fitness function may be captured in the principal component coefficients. A comparative study of variable importance rankings derived from a surrogate model built on the same dataset was also performed. The results show that both approaches are capable of identifying key features regarding variable interactions and their influence on fitness in a complimentary fashion.},
  archive      = {J_EXSY},
  author       = {Martin Fyvie and John A. W. McCall and Lee A. Christie and Alexander E. I. Brownlee and Manjinder Singh},
  doi          = {10.1111/exsy.13494},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13494},
  shortjournal = {Expert Syst.},
  title        = {Towards explainable metaheuristics: Feature extraction from trajectory mining},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploration and practice of data thinking in the application of E-commerce art design course under the background of big data. <em>EXSY</em>, <em>42</em>(1), e13493. (<a href='https://doi.org/10.1111/exsy.13493'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E-commerce art design major is a new major that has developed rapidly in recent years. In the era of big data, teaching methods, teaching methods and teaching contents have changed accordingly. At the same time, with the popularization and application of big data, the demand direction of e-commerce art design professional society for talents has also changed. Therewith, the model of teaching is also undergoing a transformation. The curriculum system of the e-commerce art design major needs to integrate the thinking in data in response to the social demand. Therefore, starting with the concept and connotation of thinking in data, status analysis of the art design major of application-oriented universities, and analysis of the application of the thinking in data in the education of the e-commerce art design major, this paper explores how to apply the thinking in data in the teaching of the e-commerce art design major of application-oriented universities and optimize the talent training in this major, so as to provide new ideas for the development in the e-commerce art design major.},
  archive      = {J_EXSY},
  author       = {Mengrui Zhu and Jiankun Wang},
  doi          = {10.1111/exsy.13493},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13493},
  shortjournal = {Expert Syst.},
  title        = {Exploration and practice of data thinking in the application of E-commerce art design course under the background of big data},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Android malware analysis and detection: A systematic review. <em>EXSY</em>, <em>42</em>(1), e13488. (<a href='https://doi.org/10.1111/exsy.13488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Android malware has been emerged as a significant threat, which includes exposure of confidential information, misrepresentation of facts and execution of applications without the knowledge of the users. Malware analysis plays an essential role in dealing with the unlawful behaviour of such malicious applications. Android malware analysis involves examining and understanding malware behaviour and its characteristics. It also includes potential adversarial impacts on Android devices. This paper presents a quick understanding and a holistic view of malware detection and analysis. The current investigation conducted a systematic literature review (SLR) to recognize the salient shifts in malware detection by examining a range of scholarly journals and conference papers. The SLR investigated 99 articles published between the years 2018 and 2023. The key observation of this SLR is that static analysis is the most implemented approach for detecting Android malware; Apktool and Androguard are the most frequently used tools. This study also conceded that deep learning and machine learning models have more potential to analyse the malicious behaviour of malware. Certain challenges are faced in Android malware analysis, that is, obfuscation techniques, dynamic code loading, and issues related to experimented datasets. Further, this study focuses on the following areas: the definition of the sample set, data optimisation and processing, feature extraction, machine learning application, and classifier validation. This investigation differs from previous analyses of Android malware detection by emphasizing additional methods based on machine learning.},
  archive      = {J_EXSY},
  author       = {Anuradha Dahiya and Sukhdip Singh and Gulshan Shrivastava},
  doi          = {10.1111/exsy.13488},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13488},
  shortjournal = {Expert Syst.},
  title        = {Android malware analysis and detection: A systematic review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shannon-cosine wavelet precision integration method for image enhancement based on fractional partial differential equations. <em>EXSY</em>, <em>42</em>(1), e13476. (<a href='https://doi.org/10.1111/exsy.13476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fractional order partial differential equations have a wide range of applications in image processing. The solution of partial differential equations is generally obtained using the finite difference method, which still requires improvement in terms of efficiency and effectiveness. In this work, a multi-scale interpolative wavelet operator is constructed by means of Shannon-Cosine wavelets with interpolation, smoothness, compact support and other excellent properties. We use the wavelet operator instead of the finite difference operator to solve fractional-order partial differential equations. The proposed method can limit the artefacts and remove the noises appeared in the processed images effectively. In addition, for the loss of texture in the denoised image, we enhance the image by employing fractional order differential equations to improve the quality of the image. Finally, the biological sections images are taken as the examples to illustrate the effectiveness of the proposed method.},
  archive      = {J_EXSY},
  author       = {Meng Liu and Zhao Min and Li Li and Piercarlo Cattani and Shuli Mei},
  doi          = {10.1111/exsy.13476},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13476},
  shortjournal = {Expert Syst.},
  title        = {Shannon-cosine wavelet precision integration method for image enhancement based on fractional partial differential equations},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mathematical analysis and dynamical transmission of monkeypox virus model with fractional operator. <em>EXSY</em>, <em>42</em>(1), e13475. (<a href='https://doi.org/10.1111/exsy.13475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monkeypox virus is one of the major causes of both smallpox and cowpox infection in our society. It is typically located next to tropical rain forests in remote villages in Central and West Africa. The disease is brought on by the monkeypox virus, a member of the Orthopoxvirus genus and the Poxviridae family. For analysis and the dynamical behaviour of the monkeypox virus infection, we developed a fractional order model with the Mittag-Leffler kernel. The uniqueness, positivity, and boundedness of the model are treated with fixed point theory results. A Lyapunov function is used to construct both local and global asymptotic stability of the system for both endemic and disease-free equilibrium points. Finally, numerical simulations are carried out using the effective numerical scheme with an extended Mittag-Leffler function to demonstrate the accuracy of the suggested approaches.},
  archive      = {J_EXSY},
  author       = {Muhammad Farman and Ali Akgül and Harish Garg and Dumitru Baleanu and Evren Hincal and Sundas Shahzeen},
  doi          = {10.1111/exsy.13475},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13475},
  shortjournal = {Expert Syst.},
  title        = {Mathematical analysis and dynamical transmission of monkeypox virus model with fractional operator},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based identification of aquatic and semi aquatic plants. <em>EXSY</em>, <em>42</em>(1), e13473. (<a href='https://doi.org/10.1111/exsy.13473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Botanists use morphological features of leaves for aquatic and semi-aquatic plants identification. In deep learning, the convolution process is efficacious to label images. Existing studies showed that, in deep learning-based leaf identification, direct image pixel values are used. Leaf images have similar sizes and almost equal pixel values. So, the pixel-based leaf naming procedure makes uncertainty in the feature results. The direct evaluation of image pixels for leaf image identification is not an effective method, because it is leading to a forgetting problem in continuous learning. Using the pre-processed databases, researchers are achieving more than 99% accurate results using deep learning. However, the same model fails to reproduce the same result in databases. In this study, morphological features of aquatic and semi-aquatic plant leaves are converted into digital descriptors to avoid uncertainty in the feature results. In this digital descriptor-based deep learning method, morphological features of aquatic and semi-aquatic plant leaves are used. This is equivalent to Botanists' morphological features-based leaf identification technology. Morphological feature extraction and digital descriptor generation helped this model to achieve 95% accuracy. In this model, leaf morphological features are used for training, so this help to understand the leaf properties of other leaf databases.},
  archive      = {J_EXSY},
  author       = {Ashwani Kumar Dubey and Jibi G. Thanikkal},
  doi          = {10.1111/exsy.13473},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13473},
  shortjournal = {Expert Syst.},
  title        = {Deep learning based identification of aquatic and semi aquatic plants},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure transmission of ocean images using deep learning-based data hiding. <em>EXSY</em>, <em>42</em>(1), e13469. (<a href='https://doi.org/10.1111/exsy.13469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data hiding has become a hot research topic in recent years due to increased attention placed on the copyright protection of ocean images and related digital records. Further, high image volumes put enormous pressure on transmission bandwidth and storage capabilities. This paper proposes an innovative deep learning-based data-hiding technique for ocean images. First, a down-sampling scheme is applied to compress the secret mark before embedding it in the host media. Then, a convolutional neural network is used to embed and recover compressed marks into or from the host ocean image. Finally, a generative adversarial network-based reconstruction network is used to reconstruct the high-quality mark image. Our experiments show that the proposed work not only maintains high imperceptibility and robustness against many attacks but also provides better data-hiding performance than related works.},
  archive      = {J_EXSY},
  author       = {Himanshu Kumar Singh and Kedar Nath Singh and Amit Kumar Singh},
  doi          = {10.1111/exsy.13469},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13469},
  shortjournal = {Expert Syst.},
  title        = {Secure transmission of ocean images using deep learning-based data hiding},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal pricing strategies and decision-making systems in e-commerce using integrated fuzzy multi-criteria method. <em>EXSY</em>, <em>42</em>(1), e13468. (<a href='https://doi.org/10.1111/exsy.13468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E-commerce online stores are now virtual platforms for connecting with millions of potential clients worldwide in the age of digitization. The marketing teams develop digital marketing tactics to attract traffic to their e-commerce sites and increase sales volume. With the vast amount of data provided by the cloud, decisions that were previously made with a significant level of intuition based on the knowledge and experience of decision-makers can now be backed using artificial intelligence algorithms. To identify the variables influencing pricing decisions for products launched on e-commerce shopping sites and to develop variable pricing techniques for each product found on an e-commerce site. This paper introduces a novel approach that applies Fuzzy association rule mining (FARM) and Fuzzy TOPSIS MCDM methodology. A B2B e-commerce marketing store based in Hong Kong has created and implemented Smart-Quo, a pricing decision support system for B2B e-commerce retail businesses. After a six-month trial period, there has been a substantial advance in the effectiveness and efficiency of choosing prices for each product. The case study illustrates the viability and potential advantages of implementing artificial intelligence tools in marketing management in the digital era.},
  archive      = {J_EXSY},
  author       = {Wenli Shan},
  doi          = {10.1111/exsy.13468},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13468},
  shortjournal = {Expert Syst.},
  title        = {Optimal pricing strategies and decision-making systems in e-commerce using integrated fuzzy multi-criteria method},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study of blockchain-based models of interest margin of secondary capital bond issuance in the internet of things environment. <em>EXSY</em>, <em>42</em>(1), e13465. (<a href='https://doi.org/10.1111/exsy.13465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of blockchain technology and the Internet of Things has improved the efficiency and quality of social production, thus accelerating the speed of economic development. The calculation of issue spreads for Tier 2 capital bonds, mainly using a centralized settlement system, ignores the transparency of the data and leads to a high degree of uncertainty in the results of issue spread analysis. The current status of secondary capital bond issuance is analysed and a conceptual definition of blockchain and secondary capital bond is completed as the theoretical support for the subsequent empirical study. Bond issuance data are collected within the wind database as empirical research data, and ordinary bonds are selected as the reference object to put forward three empirical research hypotheses. Utilizing the bond issuance spread research model centered on the blockchain model for empirical analysis, the research results show that the secondary capital bond issuance spread is significantly lower than ordinary bonds.},
  archive      = {J_EXSY},
  author       = {Zheng Xu and Yuzan Dai and Ji Liu},
  doi          = {10.1111/exsy.13465},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13465},
  shortjournal = {Expert Syst.},
  title        = {An empirical study of blockchain-based models of interest margin of secondary capital bond issuance in the internet of things environment},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning methods to detect alzheimer's disease from MRI: A systematic review. <em>EXSY</em>, <em>42</em>(1), e13463. (<a href='https://doi.org/10.1111/exsy.13463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer's disease (AD) is a progressive and irreversible neurodegenerative condition in the brain that affects memory, thinking, and behaviour. To overcome this problem, which according to the World Health Organization, is on the rise, creating strategies is essential to identify and predict the disease in its early stages before clinical manifestation. In addition to cognitive and mental tests, neuroimaging is promising in this field, especially in assessing brain matter loss. Therefore, computer-aided diagnosis systems have been imposed as fundamental tools to help imaging technicians as the diagnosis becomes less subjective and time-consuming. Thus, machine learning and deep learning (DL) techniques have come into play. In recent years, articles addressing the topic of Alzheimer's diagnosis through DL models are increasingly popular, with an exponential increase from year to year with increasingly higher accuracy values. However, the disease classification remains a challenging and progressing issue, not only in distinguishing between healthy controls and AD patients but mainly in differentiating intermediate stages such as mild cognitive impairment. Therefore, there is a need to develop more valuable and innovative techniques. This article presents an up-to-date systematic review of deep models to detect AD and its intermediate phase by evaluating magnetic resonance images. The DL models chosen by different authors are analysed, as well as their approaches regarding the used dataset and the data pre-processing and analysis techniques.},
  archive      = {J_EXSY},
  author       = {Mariana Coelho and Martin Cerny and João Manuel R. S. Tavares},
  doi          = {10.1111/exsy.13463},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13463},
  shortjournal = {Expert Syst.},
  title        = {Deep learning methods to detect alzheimer's disease from MRI: A systematic review},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customer preference analysis towards online shopping decisions based on optimized feature extraction. <em>EXSY</em>, <em>42</em>(1), e13460. (<a href='https://doi.org/10.1111/exsy.13460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online shopping has become an essential part of modern life, with food delivery being one of the most popular services. In this study, we investigate the factors influencing customers' preferences in online food ordering, such as easy payment, customization, and fast delivery. Analysing customer ratings and reviews is crucial in ensuring the quality of food services and promoting a healthy lifestyle. However, the trustworthiness of online reviews remains a challenge. Based on their rating, new customers can get help from online shoppers for their preferences or demands. Data are collected from customer reviews through online platforms to improve food service quality in online shopping. This work aims to predict the preferences of customers in online shopping based on optimized feature extraction using Principal Component Analysis with a Social Spider Optimization (PCA-SSO) algorithm. Despite existing algorithms for analysing customer preferences, most may have inaccurate predictions or take time to analyse data. To overcome these challenges, this model of PCA-SSO is proposed to predict customers' more accurate online shopping preferences. This model provides valuable implications for online shopping owners regarding customer satisfaction. Compared to the accuracy rates of KNN (78.22%), LDA (83.16%), and PCA (88.56%) algorithms, our proposed algorithm of PCA-SSO achieved a higher accuracy of 93.54%.},
  archive      = {J_EXSY},
  author       = {Weiguang Liu and Abdulmajeed Alqhatani and Fatima Asiri and Ely Salwana},
  doi          = {10.1111/exsy.13460},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13460},
  shortjournal = {Expert Syst.},
  title        = {Customer preference analysis towards online shopping decisions based on optimized feature extraction},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study on how social media influences on impulsive buying. <em>EXSY</em>, <em>42</em>(1), e13448. (<a href='https://doi.org/10.1111/exsy.13448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of the Internet has led to a shift in how people communicate, with social media becoming an essential vehicle for communication and information exchange. In this context, social media influencers have emerged. With expertise in a particular field and strong influence, they can change customers' attitudes, perceptions, and behaviors. Although social media influencers have been widely studied in the marketing field, most scholars at this stage mainly study the change in customers' attitudes and purchasing behaviors towards brands, without considering the contingency of consumers' participation in influencer marketing activities in the context of social media. Plus, researchers did not pay sufficient attention to customers' impulsive purchasing behaviors. Thus, this article investigated the factors influencing customers' impulsive buying behavior in social media influencer marketing situations. This study built an elaboration likelihood model (i.e., ELM) to explore the factors and mediating variables contributing to customers' impulsive buying behavior. Questionnaires were used to explore the factors influencing customers' impulsive buying behavior in a social media influencer marketing context. Respondents' attitudes on each dimension were measured using well-established scales from existing research, and the data were processed to draw conclusions. Findings showed that in the influencer marketing context, consumers' perceived social value and influencer expertise would not directly impact their impulsive buying behavior. Functional and emotional values could lead to impulse buying behavior, while social values would not lead to customer inspiration. Moreover, authenticity and influence both contributed to impulsive buying behaviors. The findings might have important implications for influencer marketing and further studies on impulsive buying. Companies should select authentic and influential influencers; what's more, the influencers should be sincere and enhance their influence to gain consumers' trust.},
  archive      = {J_EXSY},
  author       = {Huinan Liu and Mohd Feroz Shah De Costa Bin Mohd Faris De Costa and Megat Al-lmran Bin Yasin and Qijie Ruan},
  doi          = {10.1111/exsy.13448},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13448},
  shortjournal = {Expert Syst.},
  title        = {A study on how social media influences on impulsive buying},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large-scale decision-making model for the expediency of funding the development of tourism infrastructure in regions. <em>EXSY</em>, <em>42</em>(1), e13443. (<a href='https://doi.org/10.1111/exsy.13443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal of this study was to develop a hybrid decision-making support model regarding the feasibility of financing the development of tourism infrastructure of regions for V4 countries, based on the predicted assessment of the level of tourist movement in relation to the infrastructure and accessibility of the studied regions, expert opinions regarding the level of quality of tourist services and tourism development, as well as opinions of experts regarding the prospects of rapid growth of tourist movement in the region. For the first time, a hybrid fuzzy model for assessing the level of tourism quality in the region was developed, using the opinions of experts regarding the level of quality of tourist services and tourism development. For the first time, a five-layer neuro-fuzzy model was developed to derive a quantitative and linguistic assessment of the level of feasibility of financing the development of tourist infrastructure based on the experience, knowledge, and competences of experts regarding the prospects of rapid growth of tourist movement in the studied region. The research results were tested, and the developed model was verified on real data for 43 regions of the V4 countries.},
  archive      = {J_EXSY},
  author       = {Marinko Skare and Beata Gavurova and Volodymyr Polishchuk},
  doi          = {10.1111/exsy.13443},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13443},
  shortjournal = {Expert Syst.},
  title        = {A large-scale decision-making model for the expediency of funding the development of tourism infrastructure in regions},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective dehazing of night-time images using open dark channel prior and wavelet transform. <em>EXSY</em>, <em>42</em>(1), e13430. (<a href='https://doi.org/10.1111/exsy.13430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing night-time dehazing methods had been attempting to process light and non-light source regions based on dark channel prior (DCP). Since the bright and non-bright regions exhibit different features, the same daytime method cannot be applied to night images because light scatter from the multiple objects non-uniformly and DCP tends to over-estimate the depth of the scene making the image unrealistic. To overcome this limitation, wavelet decomposition was performed so that haze remains in the low occurrence region and noise in the high occurrence region and noise was removed by soft thresholding method. In the presented approach, the open DCP (ODCP) transmission map was computed for handling light source regions and estimated transmission was refined to enhance the texture in high-frequency part. Bilinear interpolation method of fast-guided filtering and recursive filter in the domain transform was used for edge preservation, enhancement of texture details and smoothness. The dehazed image was constructed by correlating the coefficients of low occurrence part recovered from haze and high occurrence component. The performance analysis was compared against state-of-the-art methods in terms of peak signal-to-noise ratio (PSNR) and Structural Similarity Index (SSIM).},
  archive      = {J_EXSY},
  author       = {Vivekanandan Dharmalingam and Lakshmi Harika Palivela and Pugazhendi Elangovan},
  doi          = {10.1111/exsy.13430},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13430},
  shortjournal = {Expert Syst.},
  title        = {Effective dehazing of night-time images using open dark channel prior and wavelet transform},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method of automatic field of view generation and path planning for automated x-ray inspection. <em>EXSY</em>, <em>42</em>(1), e13429. (<a href='https://doi.org/10.1111/exsy.13429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing accuracy and stability requirements of SMT equipment, automatic x-ray inspection (AXI) is used as a new popular type of testing technology. First, an imaging constraint model is proposed to ensure that the detected object is not truncated. Second, an improved iterative self-organizing clustering algorithm was proposed to realize an automatic optimal generation of the field of view and form a Hamiltonian path. Finally, the solution of the shortest Hamiltonian path problem is given by the proposed VTSP heuristic algorithm and VTSP + transformer deep learning algorithm. Experimental results show that, compared with the binary state compression DP method and the local shortest path planning method, the proposed method can support large-scale nodes, and the performance is improved by 16% on 75 nodes and 31% on 200 nodes. The proposed method achieves a balance between x-ray imaging efficiency and performance.},
  archive      = {J_EXSY},
  author       = {Guiling Song and Wei Xu},
  doi          = {10.1111/exsy.13429},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13429},
  shortjournal = {Expert Syst.},
  title        = {A method of automatic field of view generation and path planning for automated x-ray inspection},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating the effectiveness of twitter sentiment in cryptocurrency close price prediction by using deep learning. <em>EXSY</em>, <em>42</em>(1), e13428. (<a href='https://doi.org/10.1111/exsy.13428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cryptocurrencies' price prediction has attracted the interest of many people including investors, researchers and practitioners. In this study, we proposed a hybrid model for predicting the daily close price of cryptocurrencies based on different neural networks such as long short-term memory, convolutional neural network and attention mechanism. Using an ensemble of three pre-trained language models, we extracted sentiment of cryptocurrency-related tweets posted between 1 January 2021 and 31 December 2021. We constructed 20 different versions of our model and evaluated their performance on data of 27 most traded cryptocurrencies using a history of previous days' sentiment data along with close prices as input data. The flexible input layer of our model enables different ways of feeding data into the model to adjust it for different cryptocurrencies to obtain better predictions. Our analysis revealed several important findings. We showed that longer sequences of input data achieve most accurate predictions on average. More specifically, using a history of 14- and 21-days' data results in lowest RMSE values on average compared to using a history of 7 days. However, there is no significant difference between the results related to the input sequences with lengths of 14 and 21. In addition, our findings suggest that sentiment data can be useful in predicting prices for more than 70% of the studied cryptocurrencies. Thus, peoples' emotions, opinions, and sentiment that are expressed through their posts on Twitter platform play a significant role in prediction of cryptocurrencies' prices.},
  archive      = {J_EXSY},
  author       = {Bahareh Amirshahi and Salim Lahmiri},
  doi          = {10.1111/exsy.13428},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13428},
  shortjournal = {Expert Syst.},
  title        = {Investigating the effectiveness of twitter sentiment in cryptocurrency close price prediction by using deep learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep convolutional neural network model for medical data classification from computed tomography images. <em>EXSY</em>, <em>42</em>(1), e13427. (<a href='https://doi.org/10.1111/exsy.13427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning provides powerful techniques for several applications, including automated disease diagnosis through medical image classification. Recently, many studies reported that deep learning approaches have demonstrated significant performance and accuracy improvements over shallow learning techniques. The deep learning approaches have been used in many problems related to disease diagnoses, such as thyroid diagnosis, diabetic retinopathy detection, foetal localization, and breast cancer detection. Many deep learning methods have been reported in the recent past that uses medical images from various sources, such as healthcare providers and open data initiatives, and reported significant improvement in terms of precision, recall, and accuracy. This paper proposes a framework incorporating deep convolutional neural networks and an enhanced feature extraction technique for classifying medical data. To show the real-world usability of the proposed approach, it has been used for the classification of COVID-19 images from computed tomography scans. The experimental results show that the proposed approach outperformed some of the chosen baselines and obtained an accuracy of 98.91%, comparable with already reported accuracies.},
  archive      = {J_EXSY},
  author       = {S. Sreelakshmi and V. S. Anoop},
  doi          = {10.1111/exsy.13427},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13427},
  shortjournal = {Expert Syst.},
  title        = {A deep convolutional neural network model for medical data classification from computed tomography images},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk analysis of china's financial market collapse based on cloud computing and machine learning algorithms. <em>EXSY</em>, <em>42</em>(1), e13426. (<a href='https://doi.org/10.1111/exsy.13426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the rapid advancement of hardware and software algorithms, information technology has penetrated every aspect of our lives, creating waves of digital transformation in enterprises represented by four underlying technologies, namely: ‘artificial intelligence, big data, cloud computing, and blockchain’. As a result, digital technology has emerged as the primary force behind industrial transformation and upgrading, as well as high-quality development. Using the data of Shanghai and Shenzhen A-share listed companies from 2007 to 2020, this paper empirically examines the impact of the digital transformation of listed companies on their stock price crash risk and its mechanism using a two-way fixed effect model with time and individual effects. It can be seen from the research results that, overall, enterprise digital transformation can significantly reduce the risk of a stock price crash. Further analysis reveals that this risk-inhibiting effect is more apparent for private enterprises and small and medium-sized enterprises. In terms of impact mechanism, digital transformation can reduce the risk of a stock price crash by improving the quality of enterprise information internally, while the more analysts pay attention to an issue, the more obvious the role of digital transformation in reducing the risk of a stock price crash for enterprises externally. However, it is worth noting that if the stock price of an enterprise crashes, blindly implementing digital transformation may increase the crash risk. The paper illustrates the practical importance of digital technology from the perspective of enterprises in stabilizing the crash risk of stock prices and improving the performance of the capital market. Meanwhile, it contributes to theoretical research on the impact of digital transformation on microeconomic entities and provides practical knowledge and evidence for enterprises to accelerate digital transformation.},
  archive      = {J_EXSY},
  author       = {Fuxian Ning and Jiahui Jin and Lingling Zhang and Da Wang},
  doi          = {10.1111/exsy.13426},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13426},
  shortjournal = {Expert Syst.},
  title        = {Risk analysis of china's financial market collapse based on cloud computing and machine learning algorithms},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data and domain knowledge dual-driven artificial intelligence: Survey, applications, and challenges. <em>EXSY</em>, <em>42</em>(1), e13425. (<a href='https://doi.org/10.1111/exsy.13425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the mainstream mode of machine learning algorithms is the data-driven method, which mainly relies on the self-learning ability of deep neural networks and continuously evolving models in data-driven training. However, the pure data-driven method has some critical problems, such as high data collection cost, poor interpretability and easy to be be disturbed by noise. Although the knowledge-driven method has high stability, it lacks self-learning and evolution ability in the face of comprehensive and complex problems. In recent years, the convergence of data and domain knowledge has combined the advantages of both learning paradigms. One typical way is to embed domain knowledge into the data-driven model to improve the interpretability of the model, and then use the self-learning ability of the data-driven model to explore knowledge, and continuously iterate the domain knowledge to form a closed loop. The data-knowledge dual-driven methods have brought transformative innovations in machine learning. This review first introduced the advantages and necessity of the data-knowledge dual-driven model in the field of artificial intelligence. Then, the applications of the data-knowledge dual-driven model in the smart marine field were introduced. Finally, the challenges and trends of the data-knowledge dual-driven artificial intelligence are anticipated.},
  archive      = {J_EXSY},
  author       = {Jing Nie and Jiachen Jiang and Yang Li and Huting Wang and Sezai Ercisli and Linze Lv},
  doi          = {10.1111/exsy.13425},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13425},
  shortjournal = {Expert Syst.},
  title        = {Data and domain knowledge dual-driven artificial intelligence: Survey, applications, and challenges},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient adaptive ensembling for image classification. <em>EXSY</em>, <em>42</em>(1), e13424. (<a href='https://doi.org/10.1111/exsy.13424'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, with the exception of sporadic cases, the trend in computer vision is to achieve minor improvements compared to considerable increases in complexity. To reverse this trend, we propose a novel method to boost image classification performances without increasing complexity. To this end, we revisited ensembling, a powerful approach, often not used properly due to its more complex nature and the training time, so as to make it feasible through a specific design choice. First, we trained two EfficientNet-b0 end-to-end models (known to be the architecture with the best overall accuracy/complexity trade-off for image classification) on disjoint subsets of data (i.e., bagging). Then, we made an efficient adaptive ensemble by performing fine-tuning of a trainable combination layer. In this way, we were able to outperform the state-of-the-art by an average of 0.5% on the accuracy, with restrained complexity both in terms of the number of parameters (by 5–60 times), and the FLoating point Operations Per Second FLOPS by 10–100 times on several major benchmark datasets.},
  archive      = {J_EXSY},
  author       = {Bruno Antonio and Davide Moroni and Massimo Martinelli},
  doi          = {10.1111/exsy.13424},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13424},
  shortjournal = {Expert Syst.},
  title        = {Efficient adaptive ensembling for image classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CXRNet: CNN-attention based CXR image classifier. <em>EXSY</em>, <em>42</em>(1), e13423. (<a href='https://doi.org/10.1111/exsy.13423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest X-ray (CXR) images are widely accepted for the diagnosis of lung diseases. The X-ray machinery is widely available but the number of radiologists who interpret these images is very limited. Therefore, the development of an automated disease classification system is a need for the healthcare industry. The existing testing methods take hours to days to generate the testing result and have low detection accuracy and high false detection rate. Furthermore, the testing kits are costly, and availability is limited. Therefore, Convolutional Neural Network (CNN) based framework is proposed to address these limitations. Four pre-trained frameworks, ResNet50V2, InceptionV3, NASNetMobile, and Xception are used to generate highly dense features, refine the features using the attention module, and then fused these features to classify the diseases using CXR images. In addition, extensive experiments were carried out on the activation functions (Relu, Leaky Relu, and Tanh), which help to improve the results. Evaluation is done on the three enriched CXR datasets DS-1, DS-2, and DS-3 to examine the performance of the proposed framework in terms of binary classification and multi-class classification. The proposed model achieved a class-5 accuracy of 92.89% on dataset DS-3 and the class-3 classification accuracy of 95.22%, 92.25% on dataset DS-1 and DS-2, respectively. The class-2 classification accuracy on dataset DS-1 and DS-2 is 99.23% and 98.85%, respectively.},
  archive      = {J_EXSY},
  author       = {Saurabh Agarwal and K. V. Arya},
  doi          = {10.1111/exsy.13423},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13423},
  shortjournal = {Expert Syst.},
  title        = {CXRNet: CNN-attention based CXR image classifier},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pose recognition of dancing images using fuzzy deep learning technique in an IoT environment. <em>EXSY</em>, <em>42</em>(1), e13422. (<a href='https://doi.org/10.1111/exsy.13422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of smart technologies (e.g., Internet of Things (IoT), Artificial Intelligence (AI), and Big Data), people have start using them for various purposes. The real-time IoT devices generate an enormous amount of video and imaging data, leading to the concept of complex data structure. And people face major challenges in mining and extraction of useful features and information from such data. How to efficiently analyse and process video data to obtain valuable information has become a key research topic. The traditional manual annotation methods are unable to meet the current demand for the growing number of videos. Therefore, a more convenient method for processing video data needs to be developed. The research objective of this paper is dance videos, and the goal is to realize automatic recognition of dance movements. In this paper, a Dual Convolutional Neural Network Algorithm (DCNNA) is proposed for the automatic recognition of different dance movements in live and remote videos. DCNNA can extract video information more comprehensively and efficiently. It can simultaneously extract the light flow features corresponding to the action changes and the information contained in each frame of the video. Therefore, the dance movements can be more accurately identified. In the experiments, the performance of DCNNA is evaluated based on dance videos and compared with Inception V3 and 3D-CNN. All the experiments illustrate the superior performance of the proposed DCNN algorithm. From the experimental results, it is quite obvious that the F1 score of the proposed DCNNA is 11% and 6% higher than that of the Inception V3 and 3D-CNN, respectively.},
  archive      = {J_EXSY},
  author       = {Dongxia Zheng and Yi Yuan},
  doi          = {10.1111/exsy.13422},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13422},
  shortjournal = {Expert Syst.},
  title        = {Pose recognition of dancing images using fuzzy deep learning technique in an IoT environment},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improve the application of reinforcement learning and multi-modal information in music sentiment analysis. <em>EXSY</em>, <em>42</em>(1), e13416. (<a href='https://doi.org/10.1111/exsy.13416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the effect of music sentiment analysis, this paper proposes a music sentiment classification method based on lyrics and comments. This method combines lyrics and comment texts to mine richer sentiment information, and comprehensively considers the influence of the word frequency, sentiment strength and part of speech of sentiment words on sentiment classification when constructing sentiment vectors. Moreover, it matches the lyrics of the music and the substantive words in the comment with the emotional dictionary to obtain the emotional category and emotional weight of each substantive word, and calculates the statistical value of each emotional category. In addition, this paper combines reinforcement learning and multi-modal information technology to construct a music emotion research model.},
  archive      = {J_EXSY},
  author       = {Qi Yang and Songhu Liu and Tianzhuo Gong},
  doi          = {10.1111/exsy.13416},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13416},
  shortjournal = {Expert Syst.},
  title        = {Improve the application of reinforcement learning and multi-modal information in music sentiment analysis},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel class of adaptive observers for dynamic nonlinear uncertain systems. <em>EXSY</em>, <em>42</em>(1), e13412. (<a href='https://doi.org/10.1111/exsy.13412'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous techniques have been proposed in the literature to improve the performance of high-gain observers with noisy measurements. One such technique is the linear extended state observer, which is used to estimate the system's states and to account for the impact of internal uncertainties, undesirable nonlinearities, and external disturbances. This observer's primary purpose is to eliminate these disturbances from the input channel in real-time. This enables the observer to precisely track the system states while compensating for the various sources of uncertainty that can influence the system's behaviour. So, in this paper, a novel nonlinear higher-order extended state observer (NHOESO) is introduced to enhance the performance of high-gain observers under noisy measurement conditions. The NHOESO is designed to observe the system states and total disturbance while eliminating the latter in real time from the input channel. It is capable of handling disturbances of higher-order derivatives, including internal uncertainties, undesirable nonlinearities, and external disturbances. The paper also presents two innovative schemes for parametrizing the NHOESO parameters in the presence of measurement noise. These schemes are named time-varying bandwidth NHOESO (TVB-NHOESO) and online adaptive rule update NHOESO (OARU-NHOESO). Numerical simulations are conducted to validate the effectiveness of the proposed schemes, using a nonlinear uncertain system as a test case. The results demonstrate that the OARU technique outperforms the TVB technique in terms of its ability to sense the presence of noise components in the output and respond accordingly. However, it is noted that the OARU technique is slower than the TVB technique and requires more complex parameter tuning to adaptively account for the measurement noise.},
  archive      = {J_EXSY},
  author       = {Ahmed Alkhayyat and Ali Mahdi Zalzala and Asaad A. M. AL-Salih and Anwar Ja'afar Mohamad Jawad and Wameedh Riyadh Abdul-Adheem and Jamshed Iqbal and Ibraheem K. Ibraheem and Waleed K. Ibrahim and Mustafa Musa Jaber and Asaad Shakir Hameed},
  doi          = {10.1111/exsy.13412},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13412},
  shortjournal = {Expert Syst.},
  title        = {A novel class of adaptive observers for dynamic nonlinear uncertain systems},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating significant features in context-aware multimodal emotion recognition with XAI methods. <em>EXSY</em>, <em>42</em>(1), e13403. (<a href='https://doi.org/10.1111/exsy.13403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expert systems are being extensively used to make critical decisions involving emotional analysis in affective computing. The evolution of deep learning algorithms has improved the potential for extracting value from multimodal emotional data. However, these black-box algorithms do not often explain the heuristics behind processing the input features for achieving certain outputs. This study focuses on the risks of using black-box deep learning models for critical tasks, such as emotion recognition, and describes how human understandable interpretations of the workings of these models are extremely important. This study utilizes one of the largest multimodal datasets available–CMU-MOSEI. Many researchers have used the pre-extracted features provided by the CMU Multimodal SDK with black-box deep learning models making it difficult to interpret the contribution of its individual features. This study describes the implications of significant features from various modalities (audio, video, text) identified using XAI in Multimodal Emotion Recognition. It describes the process of curating reduced feature models by using the Gradient SHAP XAI method. These reduced models with highly contributing features achieve comparable and at times even better results compared to their corresponding all-feature models as well as the baseline model GraphMFN. This study reveals that carefully selecting significant features for a model can help filter out irrelevant features, and attenuate the noise or bias caused by them, leading to an improved performance efficiency of the expert systems by making them transparent, easily interpretable, and trustworthy.},
  archive      = {J_EXSY},
  author       = {Aaishwarya Khalane and Rikesh Makwana and Talal Shaikh and Abrar Ullah},
  doi          = {10.1111/exsy.13403},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13403},
  shortjournal = {Expert Syst.},
  title        = {Evaluating significant features in context-aware multimodal emotion recognition with XAI methods},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in business processes to improve business marketing processes through advanced signal processing. <em>EXSY</em>, <em>42</em>(1), e13401. (<a href='https://doi.org/10.1111/exsy.13401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current research shows tremendous improvements in the business marketing processes because some problems are long-term bond yields and macroeconomic data handling at a minimum level. Any industry can revolutionize business marketing processes if it takes a specific business marketing model to eliminate such deficiencies, examines the various elements that occur in them, and finds out how to fix them. As a result, some factors have to be controlled for in this research. Innovations and advancements in signal processing techniques on business processes occur spontaneously in business processes that are managed like this. In the below-mentioned business improvement success factors, score, learning best, future creation, people involvement, goals setting, plan and develop is the most essential factor. The elimination of these factors causes some disruption in business marketing processes. These gaps cause problems in the field of science and technology-based business marketing. The problem caused by these factors is the excess usage of two-wheeler, which generates more vehicular crises. The smart helmet business is the only solution to this automotive crisis. But because some of the above problems occur to this business process marketing. Considering that some sensors to make smart helmets will develop the business in the name of this imperative, considering the factors of production are land, labour, capital, and entrepreneurship as the centre, this research introduces signal processing technology to make a helmet. The signal processing-based helmet proposed here is constructive for business. This research suggests a smart helmet business with signal-processing technology. This study highlights how adopting the smart helmet product might improve corporate marketing procedures. According to the results of our research, when people use this smart helmet product, depending on how well they can use it, that specific business rises to the top and falls to the bottom. These business marketing methods operate at a greater level and are profitable whenever their capacity rises. The effectiveness of this smart helmet declines as accidents increase and corporate marketing procedures degrade. Since our production now depends on whether the intelligent helmets are beneficial or harmful and destructive. The analytical technique described below can be used to find them.},
  archive      = {J_EXSY},
  author       = {Azath Mubarakali and A. Shobanadevi and Hazhar Omer Mohammed and Rahul Pradhan and Vuyyuru Lakshma Reddy and B. Bizu and V. Praveena},
  doi          = {10.1111/exsy.13401},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13401},
  shortjournal = {Expert Syst.},
  title        = {Advancements in business processes to improve business marketing processes through advanced signal processing},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards adaptive and transparent tourism recommendations: A survey. <em>EXSY</em>, <em>42</em>(1), e13400. (<a href='https://doi.org/10.1111/exsy.13400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourced data streams are popular and extremely valuable in several domains, namely in tourism. Tourism crowdsourcing platforms rely on past tourist and business inputs to provide tailored recommendations to current users in real time. The continuous, open, dynamic and non-curated nature of the crowd-originated data demands specific stream mining techniques to support online profiling, recommendation, change detection and adaptation, explanation and evaluation. The sought techniques must, not only, continuously improve and adapt profiles and models; but must also be transparent, overcome biases, prioritize preferences, master huge data volumes and all in real time. This article surveys the state-of-art of adaptive and explainable stream recommendation, extends the taxonomy of explainable recommendations from the offline to the stream-based scenario, and identifies future research opportunities.},
  archive      = {J_EXSY},
  author       = {Fátima Leal and Bruno Veloso and Benedita Malheiro and Juan C. Burguillo},
  doi          = {10.1111/exsy.13400},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13400},
  shortjournal = {Expert Syst.},
  title        = {Towards adaptive and transparent tourism recommendations: A survey},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent path planning algorithm of autonomous underwater vehicle based on vision under ocean current. <em>EXSY</em>, <em>42</em>(1), e13399. (<a href='https://doi.org/10.1111/exsy.13399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Underwater Vehicle (AUV) is an important tool for intelligent ocean applications, which can be applied to detect underwater environment and search target. Path planning is the key technology to realize AUV intelligence, which has important research significance. Aiming at the dynamic path planning problem, the Regional Ocean Modeling System (ROMS) was first applied to the AUV three-dimensional (3D) dynamic path planning, and a Gate Recurrent Unit Proximal Policy Optimization with Local Vision (GPPO-LV) model based on local vision is proposed. The 3D ocean current environment is constructed based on the ROMS simulation data. The local vision matrix is constructed based on underwater images, and the features of local vision are extracted using convolutional neural network. Furthermore, Gate Recurrent Unit (GRU) network is used to mine the hidden information between observation states. Finally, the Actor network for strategy output and the Critical network for action value evaluation are constructed, and strategies are optimized in the process of interaction with the environment. The experiment shows that under various unknown environments, AUV can carry out real-time path planning under real ocean current data, and has good obstacle avoidance ability.},
  archive      = {J_EXSY},
  author       = {Yibo Wang and Meng Xi and Yubo Weng},
  doi          = {10.1111/exsy.13399},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13399},
  shortjournal = {Expert Syst.},
  title        = {Intelligent path planning algorithm of autonomous underwater vehicle based on vision under ocean current},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VLSI implementation of image compressor using probabilistic run length coding. <em>EXSY</em>, <em>42</em>(1), e13398. (<a href='https://doi.org/10.1111/exsy.13398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia applications, such as image processing including image and video transfer, heavily rely on reduction. The traditional device methods to picture reduction use more space, energy, and processing time. The majority of current efforts use the Golomb-Rice encoding, due to its larger memory requirement and higher computing difficulty. So, this research concentrated on hardware design-oriented probability run length (PRL) coding technique based on lossless colour image compression. The block truncation coding (BTC) features of the compression process are used by the suggested PRL method. The proposed image compression hardware consists of various modules such as a Parameter calculator, fuzzy table, bitmap generator, BTC parameters training, prediction, and error control, and PRL-based finite state machine (PRL-FSM). The proposed image compressor utilizes the parameter calculator block, which estimates the block type based on the image pixel intensities for each sub-block. Thus, each block of the image is compressed by using a new block type and generates a variable block size. The proposed method utilizes the PRL-BTC encoding method, which also calculates the probability of error between the compressed image to the test image. The process is iterated until the performance trade-off between hardware cost and compression ratio (CR) is achieved. Hence, both smooth regions and non-smooth regions of images are perfectly compressed by the probability-based block selection. The simulation results show that the proposed method resulted in a better area, power, delay metrics, peak signal-to-noise ratio (PSNR), and CR compared to the state of art approaches.},
  archive      = {J_EXSY},
  author       = {Mahesh Boddu and Soumitra Kumar Mandal},
  doi          = {10.1111/exsy.13398},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13398},
  shortjournal = {Expert Syst.},
  title        = {VLSI implementation of image compressor using probabilistic run length coding},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DHHoE: Deep hybrid homogenous ensemble for digital histological breast cancer classification. <em>EXSY</em>, <em>42</em>(1), e13397. (<a href='https://doi.org/10.1111/exsy.13397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progress of deep learning architectures, machine learning models and pathology slide digitization is an encouraging step toward meeting the growing demand for more precise classification and prediction diagnosis for the breast tumours. The BreakHis dataset with four magnification factors (40X, 100X, 200X and 400X), as well as seven deep learning architectures used for feature extraction (DenseNet 201, Inception ResNet V2, Inception V3, ResNet 50, MobileNet V2,VGG16 and VGG19), four machine learning models for classification (MLP, SVM, DT, and KNN), and two combination rules (hard and weighted voting) were investigated in this paper to design and evaluate a new proposed approach consisting of building deep hybrid homogenous ensemble. Additionally, the best proposed models were compared to deep stacked, deep bagging, deep boosting, and deep hybrid heterogenous ensemble to choose the best strategy in building deep ensemble learning techniques. The four performance measures accuracy, precision, recall, and F1-score were used in the empirical evaluations, as well as 5-fold cross-validation, the Scott Knott statistical test, and the Borda Count voting method. The results demonstrated the new approach's potential since it outscored both singles and other deep ensemble learning strategies, achieving accuracy values of 98.3% and 97.7% for the MFs 40X, 100X and 200X, 400X, respectively. The empirical results demonstrated that the proposed ensembles are impactful for histopathological breast cancer images classification, and they provided a promising tool to assist pathologists in the diagnosis of breast cancer.},
  archive      = {J_EXSY},
  author       = {Hasnae Zerouaoui and Ali Idri and Omar El Alaoui},
  doi          = {10.1111/exsy.13397},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13397},
  shortjournal = {Expert Syst.},
  title        = {DHHoE: Deep hybrid homogenous ensemble for digital histological breast cancer classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing a model semantic-based image retrieval by combining KD-tree structure with ontology. <em>EXSY</em>, <em>42</em>(1), e13396. (<a href='https://doi.org/10.1111/exsy.13396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes an alternative approach to improve the performance of image retrieval. In this work, a framework for image retrieval based on machine learning and semantic retrieval is proposed. In the preprocessing phase, the image is segmented objects by using Graph-cut, and the feature vectors of objects presented in the image and their visual relationships are extracted using R-CNN. The feature vectors, visual relationships, and their symbolic labels are stored in KD-Tree data structures which can be used to predict the label of objects and visual relationships later. To facilitate semantic query, the images use the RDF data model and create an ontology for the symbolic labels annotated. For each query image, after extracting their feature vectors, the KD-Tree is used to classify the objects and predict their relationship. After that, a SPARQL query is built to extract a set of similar images. The SPARQL query consists of triple statements describing the objects and their relationship which were previously predicted. The evaluation of the framework with the MS-COCO dataset and Flickr showed that the precision achieved scores of 0.9218 and 0.9370, respectively.},
  archive      = {J_EXSY},
  author       = {Thanh Manh Le and Nguyen Thi Dinh and Thanh The Van},
  doi          = {10.1111/exsy.13396},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13396},
  shortjournal = {Expert Syst.},
  title        = {Developing a model semantic-based image retrieval by combining KD-tree structure with ontology},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based one step and multi-steps ahead forecasting blood glucose level. <em>EXSY</em>, <em>42</em>(1), e13393. (<a href='https://doi.org/10.1111/exsy.13393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabling diabetic patients to predict their Blood Glucose Levels (BGL) is a crucial aspect of managing their metabolic condition, as it allows them to take appropriate measures to avoid hypo or hyperglycemia. Machine Learning (ML) and Deep Learning (DL) techniques have made this possible, and this paper evaluates and compares the performance of five distinct ML/DL models including: Convolutional Neural Network (CNN), Long Short Term Memory (LSTM), Support Vector Regression (SVR), Gated Reccurent Unit (GRU) and Deep Belief Network (DBN) for forecasting BGL, by applying two different forecasting methods, namely One Step Ahead (OSF) and Multi-Step Ahead (MSF) comprising five different variants. The performance is evaluated based on four metrics: Mean Absolute Error (MAE), Mean Magnitude Relative Error (MMRE), Root Mean Square Error (RMSE) and Predictive Level (PRED). Additionally, the statistical significance of the regressors was evaluated using the Scott-Knott (SK) test, while the Borda Count (BC) voting system was employed to rank them. The results indicate that the best performance was achieved with OSF using GRU. Furthermore, the effectiveness of an MSF strategy depends on the ML/DL technique used, and the best combinations were DBN with DirRec, DBN with Recursive, SVR with Recursive and SVR with DirRec. Additionally, DirRec was found to be the best strategy, as it consistently ranked first regardless of the ML/DL technique used.},
  archive      = {J_EXSY},
  author       = {Mamoune Benaida and Ibtissam Abnane and Ali Idri},
  doi          = {10.1111/exsy.13393},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13393},
  shortjournal = {Expert Syst.},
  title        = {Deep learning based one step and multi-steps ahead forecasting blood glucose level},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital transformation in healthcare using eagle perching optimizer with deep learning model. <em>EXSY</em>, <em>42</em>(1), e13390. (<a href='https://doi.org/10.1111/exsy.13390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 epidemic accelerated the digital change of several services, including healthcare, and increased access to telemedicine. As a result, an increasing number of web tools were introduced to meet patient needs. A safe database can be created in the healthcare industry as a result of digital transformation. This database can be used to protect, store, and share private patient data with healthcare workers, labs, and medical specialists. Designing efficient decision-making tools for COVID-19 diagnostics is now possible thanks to recent developments in information technology and deep learning (DL) models. In this paper, a novel method for diagnosing COVID-19 using deep learning-enhanced eagle perching optimizer (DTH-EPODL) model is presented. With the help of the IoT and the presented DTH-EPODL model, patient information can be gathered and analysed for illness detection. The DTH-EPODL model uses the Gaussian filtering (GF) method to remove noise in the initial step. Additionally, MixNet, a deep convolutional neural network-based method, is used for feature extraction. Using the deep autoencoder (DAE) algorithm, COVID-19 detection and categorization are accomplished. Finally, the DAE approach's associated hyperparameters can be best adjusted using the EPO method, which enhances categorization results. Benchmark chest x-ray datasets can be used to evaluate the experimental validity of the DTH-EPODL method. The experimental results showed that the DTH-EPODL technique outperformed more modern methods.},
  archive      = {J_EXSY},
  author       = {R. Thilagavathy and J. Jagadeesan and A. Parkavi and M. Radhika and S. Hemalatha and Mohammad Gouse Galety},
  doi          = {10.1111/exsy.13390},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13390},
  shortjournal = {Expert Syst.},
  title        = {Digital transformation in healthcare using eagle perching optimizer with deep learning model},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Food fraud detection using explainable artificial intelligence. <em>EXSY</em>, <em>42</em>(1), e13387. (<a href='https://doi.org/10.1111/exsy.13387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the global food supply chain has become increasingly complex, and its scalability has grown. From farm to fork, the performance of food-producing systems is influenced by significant changes in the environment, population and economy. These changes may cause an increase in food fraud and safety hazards and hence, harm human health. Adopting artificial intelligence (AI) technology in the food supply chain is one strategy to reduce these hazards. Although the use of AI has been rising in numerous industries, such as precision nutrition, self-driving cars, precision agriculture, precision medicine and food safety, much of what AI systems do is a black box due to its poor explainability. This study covers numerous use cases of food fraud risk prediction using explainable artificial intelligence (XAI) techniques, such as LIME, SHAP and WIT. We aimed to interpret the predictions of a machine learning model with the aid of these technologies. The case study was performed on a food fraud dataset using adulteration/fraud notifications retrieved from the Rapid Alert System for Food and Feed system and economically motivated adulteration database. A deep learning model was built based on this dataset and XAI tools have been investigated on the proposed deep learning model. Both features and shortcomings of the current XAI tools in the food fraud area have been presented.},
  archive      = {J_EXSY},
  author       = {Okan Buyuktepe and Cagatay Catal and Gorkem Kar and Yamine Bouzembrak and Hans Marvin and Anand Gavai},
  doi          = {10.1111/exsy.13387},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13387},
  shortjournal = {Expert Syst.},
  title        = {Food fraud detection using explainable artificial intelligence},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Marketing information system based on unsupervised visual data to manage transportation industry using signal processing. <em>EXSY</em>, <em>42</em>(1), e13384. (<a href='https://doi.org/10.1111/exsy.13384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advance signal processing techniques are becoming one of the essential requirements for secure and efficient communication in the future innovative world. These requirements are due to the tremendous advancement of various latest cutting-edge technologies that are very useful for the present generation. Marketing information system visualization is essential for the services of every advanced technology established in this way to reach people. Citing this, signal processing is a significant issue in most new technological innovations and challenges in various applications. A marketing information system, or MIS, based on these processes provides a valuable marketing method that helps marketers make better decisions. In this mode of operation, the existing methods are discarded, and the new methods, such as visual images, sound waves, and seismic waves, are implemented as digital signal processes, and their benefits are used as inputs for the processing of humankind. Based on this perspective, collaborative Management Information Systems (MIS) is proposed. This process-based bridge enables MIS professionals to benefit from their investments in personnel, equipment, and business processes. In particular, transportation challenges such as total order intensity ratio, rising fuel prices, unexpected delays in transportation, shortage of skilled workers, deplorable warehouse conditions, insufficient and over-stressed delivery staff, etc., and how road transport is giving its full attention to these fluctuations. Research paper explains very clearly and adequately explains the impact of such explained traffic problems, such as the volume of traffic and passengers, road network, lack of rehabilitation of half of the roads, and lack of national highways. Performance analysis for MIS measures with various factors is taken, and how the process varies in them and the answers to them are clarified through this research. This research citation shows how the marketing performance of an MIS measures the best practices in this research. Through this, online marketing performance based on MIS measures can be increased through MIS analysis reports for MIS measures. Finally, the Performance of the proposed MIS-Transport with the MIS-hospital Industry is taken as an example and explained.},
  archive      = {J_EXSY},
  author       = {Kapil Aggarwal and Bui Thanh Khoa and K. V. Daya Sagar and Ruchi Agrawal and Mallika Dhingra and Jagrit Dhingra and Lakshmana Kumar R},
  doi          = {10.1111/exsy.13384},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13384},
  shortjournal = {Expert Syst.},
  title        = {Marketing information system based on unsupervised visual data to manage transportation industry using signal processing},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classifying electroencephalogram signals using an innovative and effective machine learning method based on chaotic elephant herding optimum. <em>EXSY</em>, <em>42</em>(1), e13383. (<a href='https://doi.org/10.1111/exsy.13383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of electroencephalography (EEG) has made significant contributions to our understanding of the brain, our understanding of neurological diseases, and our ability to treat such diseases. Epileptic seizures, strokes, and even death can all be detected with the use of the electroencephalogram, a diagnostic technique used to record electrical activity in the brain. This research suggests using binary classification for automated epilepsy diagnosis. Patients' EEG signals are pre-processed after being recorded. On the basis of the results of the feature extraction technique, the best traits are picked for further examination by means of a structured genetic algorithm. The EEG data are analysed and categorized as either seizure-free or epileptic seizure-related based on the assumption of feature optimization utilizing the support vector classifier. As a result, categorizing EEG signals is an ideal application for the suggested technique. For this purpose of accelerating the implementation of distributed computing, a CEHOC (Chaotic Elephant Herding Optimization based Classification) is used to classify the vast scope of various datasets. The results show that the CEHOC algorithm is more effective than previous versions. Precision, recall, F score, sensitivity, specificity, and accuracy are some of the metrics used to assess the effectiveness of the work provided here. The suggested work has a 99.3019% accuracy rate, a 98.2018% sensitivity rate, and a 99.1125% specificity rate. There was an F score of 99.3204%, a precision of 99.1019%, and a recall of 98.3015%. These numbers indicate that the planned action was successful.},
  archive      = {J_EXSY},
  author       = {Ali Alqahtani and Nayef Alqahtani and Abdulaziz A. Alsulami and Stephen Ojo and Prashant Kumar Shukla and Shraddha V. Pandit and Piyush Kumar Pareek and Hany S. khalifa},
  doi          = {10.1111/exsy.13383},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13383},
  shortjournal = {Expert Syst.},
  title        = {Classifying electroencephalogram signals using an innovative and effective machine learning method based on chaotic elephant herding optimum},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object detection and classification from compressed video streams. <em>EXSY</em>, <em>42</em>(1), e13382. (<a href='https://doi.org/10.1111/exsy.13382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Analytics is widely used by the internet-based platforms to govern the mass consumption of videos. Traditionally, it is carried out from the decoded format of the videos. This requires the analytics server to perform both decoding and analytics computation. This process can be made fast and efficient if performed over the compressed format of the videos as it reduces the decoding stress over the analytics server. The field of video analytics from the binarized formats using modern deep learning techniques is still emerging and needs further exploration. This proposed work is based on the same notion. In this work, two analytics tasks that is, classification and object detection are carried out from the binarized videos. The binarized formats are produced by using an already-designed end-to-end video compression network. The experiments have been carried out over standard datasets. The proposed MobileNetv2-based classification network shows an accuracy of 66% over the YouTube UGC dataset and the YOLOX-S-based detection network shows mAP of 45% over IMAGENet datasets. The proposed work shows competitiveness and improvement in the detection outcomes on compressed data and also provides further motivation for the adoption of deep learning-based video compression in practical analytics domains.},
  archive      = {J_EXSY},
  author       = {Suvarna Joshi and Stephen Ojo and Sangeeta Yadav and Preeti Gulia and Nasib Singh Gill and Hassan Alsberi and Ali Rizwan and Mohamed M. Hassan},
  doi          = {10.1111/exsy.13382},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13382},
  shortjournal = {Expert Syst.},
  title        = {Object detection and classification from compressed video streams},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of digital influencers on product/service purchase decision making—An exploratory case study of portuguese people. <em>EXSY</em>, <em>42</em>(1), e13381. (<a href='https://doi.org/10.1111/exsy.13381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing use of technology and social media has resulted in the emergence of digital influencers, a new profession capable of changing the mentalities and behaviours of those who follow them. This study arises to better understand the potential impact digital influencers might have on the Portuguese population's purchase behaviour and patterns, and for this purpose, seven hypotheses were formulated. An online questionnaire was conducted to respond to these theoretical assumptions and collected data from 175 respondents. A total of 129 valid answers were considered. It was possible to conclude that purchase intention does not necessarily translate into a purchase action. It was also concluded that the relationship between social network use and the purchase of products/services recommended by influencers is only statistically significant for Instagram. Furthermore, the individuals' generation is not statistically significant / linked with purchasing a product/service recommended by influencers. Yet further, a small percentage of respondents have also identified themselves as impulsive shoppers and perceived Instagram as their favourite social network. With the results of this study, it is also possible to state that the influencer's opinion was classified as the last factor considered in the purchase decision process. Additionally, there is a weak negative association between purchasing a product/service recommended by influencers with sponsorship disclosure and remunerated partnership, which decreases credibility and discourages purchasing, in Portugal, a feminine culture which dislikes materialism.},
  archive      = {J_EXSY},
  author       = {Fábio Caiado and Joana Fonseca and Joana Silva and Soraia Neves and Ana Moreira and Ramiro Gonçalves and José Martins and Frederico Branco and Manuel Au-Yong-Oliveira},
  doi          = {10.1111/exsy.13381},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13381},
  shortjournal = {Expert Syst.},
  title        = {The impact of digital influencers on product/service purchase decision making—An exploratory case study of portuguese people},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning techniques on 3D-MRI lung images for detection and segmentation of COVID-19 virus. <em>EXSY</em>, <em>42</em>(1), e13378. (<a href='https://doi.org/10.1111/exsy.13378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Coronavirus infection 2019 (COVID-19) has influenced billions and has significantly affected the public medical care. Because of rising distrust toward the affectability of RT- PCR as screening technique, clinical imaging like registered tomography offers incredible potential as option. Notwithstanding, openly accessible COVID-19 imaging information is restricted which prompts over fitting of conventional methodologies. To address this issue, the incumbent article proposes the segmentation of Corona Virus with Edge Based Segmentation and Grey Level Co-occurrence Matrix-CNN model, a creative mechanized division pipeline for COVID-19 tainted districts in the lungs, which can deal with little datasets by use as variation information bases. For the screening of COVID-19, the converse record polymerase-based chain response (RT-PCR) has been viewed as best quality level. As a significant supplement for tests of RT-PCR, the strategies of radiological imaging, for instance, X-beams as also Magnetic Resonance Imaging (MRI), DICOM (Digital Imaging and Communications in Medicine). have additionally shown viability in both flow analysis, including subsequent appraisal and assessment of infection advancement. Our strategy centers on-the-fly age of novel and irregular picture patches for preparing by playing out a few preprocessing techniques and misusing broad information expansion. For additional decrease of the over fitting danger, we executed a standard 3D U-Net design rather than new or computational complex neural organization structures. Through a 5-crease cross-approval on 150 samples of the lung sweeps of COVID-19 patients, we had the option to build up a profoundly exact just as vigorous division model for lungs and COVID-19 tainted locales without over fitting on the restricted information. The article will strategize accomplished GCPSO with an accuracy of 98% for lungs and 0.761 for disease. It will show that the proposed technique beats related methodologies, propels the cutting edge for COVID-19 division and improves clinical picture examination with restricted information.},
  archive      = {J_EXSY},
  author       = {J. Syed Nizamudeen Ahmed and M. Mohamed Sathik and Krishnan Nallaperumal and Senthamarai Kannan Kaliaperumal and Parasuraman Kumar and Arumuga Maria Devi Thanu},
  doi          = {10.1111/exsy.13378},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13378},
  shortjournal = {Expert Syst.},
  title        = {Deep learning techniques on 3D-MRI lung images for detection and segmentation of COVID-19 virus},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial emotion recognition using convolutional neural network based krill head optimisation. <em>EXSY</em>, <em>42</em>(1), e13376. (<a href='https://doi.org/10.1111/exsy.13376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from facial image expression is always a problem for computer vision algorithms in classifying the human facial emotions. On other hand, over fitting is considered as a fundamental problem that prevents the deep learning model to fit the observed data on training data and unseen data on the test data. In this paper, we develop a convolutional neural network (CNN) based emotion recognition from facial images. Initially the study uses proper background removal tool to remove the image background and secondly, we use CNN to classify the facial features based mainly on the extraction of facial feature vector. The CNN is trained with huge number of facial features from the input images causes over fitting. This is mainly due to noise presence, limited training data size and classifier complexity and to avoid this, the study uses krill herd optimisation (KHO) that enables the fitting of the classifier. The simulation is conducted to test the efficacy of the model with expressional vector that recognizes the five types of facial features. The database consists of 20,000 images collected from 300 persons and the study obtains an accuracy of 98.6% during training and 98.45% during testing. The comparative analysis between various other deep learning models show that the proposed CNN-KHO has higher rate of accuracy than other methods.},
  archive      = {J_EXSY},
  author       = {Bhagyashri Devi and M. Mary Synthuja Jain Preetha},
  doi          = {10.1111/exsy.13376},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13376},
  shortjournal = {Expert Syst.},
  title        = {Facial emotion recognition using convolutional neural network based krill head optimisation},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Radial basis function networks with lightweight multiscale fusion strategy-based underwater image enhancement. <em>EXSY</em>, <em>42</em>(1), e13373. (<a href='https://doi.org/10.1111/exsy.13373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel underwater picture enhancement approach under non-uniform lighting is presented to solve the issues of underwater photographs with unevenness due to additional lighting in deep-sea and night-time environments. Water suspended particles can cause image noise, low contrast, and colour deviation. The heterogeneous feature fusion module aims to combine multiple levels and levels of features with improving the network's ability to perceive semantic and specific information. The capability of autonomous underwater and remotely driven cars to explore and comprehend their environments is contingent on improving underwater images, a crucial low-level computer vision challenge. Recent applications of deep learning models include enhancing aquatic image quality and resolving several computer vision problems. Although several deep learning-based techniques exist for enhancing underwater images, their implementation is challenging due to the high memory and model parameter requirements. We propose a solution based on radial basis function networks (RBFN) for lightweight multiscale data fusion (LMFS). The LMFS incorporates diverse branches with varying kernel sizes to generate multiscale feature maps. The proposed RBFN-LMFS The convolution layer with jump connection and the attention module produces the output from the feature extraction module, which aims to extract various features at the network's beginning. The outcomes of our experiments on diverse data sets demonstrate that our proposed RBFN-LMFS technique performs well in processing both synthetic and authentic underwater images and successfully recovers image colour and texture characteristics. The visual output is superior to existing underwater image enhancement algorithms and is consistent with aspects of human vision.},
  archive      = {J_EXSY},
  author       = {R. Mythili and B. Sathya bama and P. Santhosh Kumar and Sanchali Das and Ramya Thatikonda and Syed Inthiyaz},
  doi          = {10.1111/exsy.13373},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13373},
  shortjournal = {Expert Syst.},
  title        = {Radial basis function networks with lightweight multiscale fusion strategy-based underwater image enhancement},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning-based private medical knowledge graph for epidemic surveillance in internet of things. <em>EXSY</em>, <em>42</em>(1), e13372. (<a href='https://doi.org/10.1111/exsy.13372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive development of the Internet of Things (IoT), it is convenient and important to collect health data from medical sensors and smart devices and construct medical knowledge graph. The knowledge graph contributes to investigating the connection between patient and disease, especially for epidemic surveillance. However, it is possible to cause the leakage of sensitive health information due to the untrusted data collector or various malicious attackers. In this paper, we attempt to utilise federated learning to construct a special knowledge graph, that is, individual-symptom relationship diagram with local differential privacy (LDP-ISRD), for epidemic risk surveillance, which presents the underlying infectious relationship among individuals. At first, we propose a federated learning-based framework of LDP-ISRD by utilising individuals' smart devices in IoT. Then, we leverage locations to determine the connection among individuals in terms of physical contact. Next, we propose a randomised algorithm PrivISRD to implement federated learning-based LDP-ISRD, which consists of symptom perturbation and aggregation. Finally, extensive experiments evaluate the impact of various parameters and results demonstrate that LDP-ISRD has good performance.},
  archive      = {J_EXSY},
  author       = {Xiaotong Wu and Jiaquan Gao and Muhammad Bilal and Fei Dai and Xiaolong Xu and Lianyong Qi and Wanchun Dou},
  doi          = {10.1111/exsy.13372},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13372},
  shortjournal = {Expert Syst.},
  title        = {Federated learning-based private medical knowledge graph for epidemic surveillance in internet of things},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hermite-shannon-cosine spectral method for fractional partial differential equations. <em>EXSY</em>, <em>42</em>(1), e13370. (<a href='https://doi.org/10.1111/exsy.13370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shannon-Cosine wavelet function possesses almost all excellent characteristics such as interpolation, compact support, and smoothness. As an interpolation wavelet function, it could be applied in fractional partial differential equations effectively. However, when solving engineering problems in a finite interval, the treatment of the boundary is still not smooth enough. So, the Hermite-Shannon-Cosine interval wavelet is constructed using the Hermite interpolation function to achieve smoother transitions at the boundary of the interval, thereby reducing boundary effects. Based on this, a method for solving Fractional PDEs is proposed, the method's performance and its processing capability at the interval boundary are verified by taking the Fractional Fokker-Planck equation and the Time-Fractional Korteweg-de Vries equation as examples. Compared with the multi-scale Faber-Schauder wavelet collocation method, Point-Symmetric interval wavelet spectral method, Dynamic interval wavelet spectral method, and so forth, the experimental results show that the method performs better in terms of numerical accuracy and effectiveness.},
  archive      = {J_EXSY},
  author       = {Haitao Hu and Piercarlo Cattani and Shuli Mei},
  doi          = {10.1111/exsy.13370},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13370},
  shortjournal = {Expert Syst.},
  title        = {Hermite-shannon-cosine spectral method for fractional partial differential equations},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interferenceless coexistence of 6G networks and scientific instruments in the ka-band. <em>EXSY</em>, <em>42</em>(1), e13369. (<a href='https://doi.org/10.1111/exsy.13369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {6G networks are envisioned to provide an extremely high quality-of-service (QoS). Then, future 6G network must operate in the Ka-band, where more bandwidth and radio channels are available, and noise and interferences are lower. But even in this context, 6G base stations must adjust the transmission power to ensure the signal-to-noise ratio is good enough to enable the expected QoS. However, 6G networks are not the only infrastructure operating in that band. Actually, many scientific instruments are also working on those frequencies. Considering that 6G networks will be transmitting a relevant power level, they can interfere very easily with these scientific instruments. Therefore, in this paper we propose a new solution to enable the interferenceless coexistence between 6G networks and scientific instruments. This solution includes a three-dimensional model to analyse future positions of user devices. Using this information and an interference model, we design a decision model to adapt the transmitted power, so the QoS achieves the expected level. Besides, when the transmitted power is high enough to interfere with close scientific instruments, a scheduling algorithm based on swarm intelligence is triggered. This algorithm calculates the optimum distribution of time slots and radio channels, so the scientific instruments can operate, and the 6G networks can still provide the required QoS. An experimental validation is provided to analyse the performance of the proposed solution. Results show a complete coexistence may be achieved with an interference level of −26 dBm and a QoS above 95% of the expected level.},
  archive      = {J_EXSY},
  author       = {Borja Bordel and Ramón Alcarria and Tomás Robles},
  doi          = {10.1111/exsy.13369},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13369},
  shortjournal = {Expert Syst.},
  title        = {Interferenceless coexistence of 6G networks and scientific instruments in the ka-band},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The prediction of two-dimensional intelligent ocean temperature based on deep learning. <em>EXSY</em>, <em>42</em>(1), e13367. (<a href='https://doi.org/10.1111/exsy.13367'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important data in intelligent ocean is the sea surface temperature (SST). Most of the previous works on SST prediction deal with independent spatial point, ignoring the spatial correlation of two-dimensional intelligent ocean, which leads to instability of prediction accuracy. Therefore, in this paper, a deep learning model based on convolutional gated recurrent unit is proposed for the SST prediction of two-dimensional intelligent ocean. Input and output of the proposed model are both spatiotemporal SST data, which means the model directly process spatiotemporal data. This method adds the spatial information of the intelligent ocean temperature data, thereby improving the accuracy. From the experiments, it turns out that the proposed model shows superior performance compared to another three prevailing deep learning models.},
  archive      = {J_EXSY},
  author       = {Zichen Wu and Jingyi He and Siyuan Hu and Jiabao Wen},
  doi          = {10.1111/exsy.13367},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13367},
  shortjournal = {Expert Syst.},
  title        = {The prediction of two-dimensional intelligent ocean temperature based on deep learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the impact of online news sentiment and relevance on stock market risks: A signalling theory perspective. <em>EXSY</em>, <em>42</em>(1), e13364. (<a href='https://doi.org/10.1111/exsy.13364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the frame theory and signalling theory, this article uses financial technology (fintech) to analyse online news sentiment and proposes the relevant hypothesis of the influence of news relevance on stock market risk, and conducts an empirical study on the VAR model of the stock return rate by using the news relevance and news sentiment data of the Uqer database. The results show a two-way Granger causality between news sentiment and stock returns. News relevance is not Granger causality of yield but Granger causality of news sentiment. In addition, combined with the impulse response, variance decomposition, and relevance analysis, it is found that the news relevance degree positively affects news sentiment and then indirectly affects the stock return rate. Our findings indicate that news relevance is an essential variable in analysing online text sentiment and stock market volatility, and online news will increase the volatility of stock market risk. These results contribute to the financialization literature and guide fintech enterprises to balance market risk by combining signalling and frame theory to improve news relevance.},
  archive      = {J_EXSY},
  author       = {Huosong Xia and Yaqi Tian and Justin Zuopeng Zhang and Yulong Liu},
  doi          = {10.1111/exsy.13364},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13364},
  shortjournal = {Expert Syst.},
  title        = {Exploring the impact of online news sentiment and relevance on stock market risks: A signalling theory perspective},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi agent deep reinforcement learning for resource allocation in container-based clouds environments. <em>EXSY</em>, <em>42</em>(1), e13362. (<a href='https://doi.org/10.1111/exsy.13362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtualization enables the deployment of several virtual servers on the same physical layer, critical component of the cloud. As cloud services advance, more apps that use repositories are developed, which adds to the overburden. Containers have evolved into the most reliable and lightweight virtualization technology for cloud services thanks to their flexible sorting, mobility, and scalability. In container-based clouds, containers can potentially cut data centre energy usage more than virtual machines (VMs) do. Containers are less energy intensive than VMs. Resource allocation is the most prevalent method in cloud systems. However, resource allocation in container-based clouds (RAC) is innovative and complicated due to its two-level architecture. This includes the pairing of virtual machines and physical computers with containers. In cloud container services, planner components are essential. This lowers expenses while improving the performance and variety of workloads using cloud resources. The cloud infrastructure resource allocation framework is gaining popularity since it is energy-efficient and focuses on cloud data management to maximize income and minimize costs. In this paper, we proposed a deep learning-based architecture capable of achieving high data centre energy efficiency and preventing Service Level Agreement (SLA) violations from deploying green cloud resources. This research describes a hybrid optimum and multi-agent deep reinforcement learning (MADRL) technique for dynamic task scheduling (DTS) in a container cloud environment. The MADRL-DTS model for the RAC problem considers VM overheads, VM types, and an affinity restriction. Then, to address the RAC issue, we develop a DTS hyper-heuristic technique. MADRL-RAC may give allocation rules by recognizing workload trends and VM types from previous workload traces. Compared to modern procedures, the results demonstrate a significant reduction in energy consumption. The evaluation for energy-efficient resource allocation is tested in several virtualized environments to get a high power usage effectiveness and CPU usage.},
  archive      = {J_EXSY},
  author       = {S. Nagarajan and P. Shobha Rani and M. S. Vinmathi and V. Subba Reddy and Angel Latha Mary Saleth and D. Abdus Subhahan},
  doi          = {10.1111/exsy.13362},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13362},
  shortjournal = {Expert Syst.},
  title        = {Multi agent deep reinforcement learning for resource allocation in container-based clouds environments},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust sliding window adaptive filtering technique for phonocardiogram signal denoising. <em>EXSY</em>, <em>42</em>(1), e13361. (<a href='https://doi.org/10.1111/exsy.13361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Vishwanath Madhava Shervegar},
  doi          = {10.1111/exsy.13361},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13361},
  shortjournal = {Expert Syst.},
  title        = {A robust sliding window adaptive filtering technique for phonocardiogram signal denoising},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying multisensor in-car situations to detect violence. <em>EXSY</em>, <em>42</em>(1), e13356. (<a href='https://doi.org/10.1111/exsy.13356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Violence recognition is challenging because it can be presented in very different forms. For example, it can be present in an image by a person hitting another person or present in audio by a person being rude to another. Thus, audio and video are essential features to be analysed. In the audio approach, speech processing, music, and ambient sound are some of the main points of this problem since finding similarities and differences between these domains is necessary. Human activity can be classified into four different categories in the video approach, depending on the complexity and the number of body parts involved in the action. Examples of Human activity categories are considered: gestures, actions, interactions and activities. Recognizing human actions in the video becomes a challenge with this varied set of human activities. Furthermore, in the last years, the growth of deep learning techniques applied to this area has been enormous, and the reason is that their results surpass traditional signal processing on a large scale. This article is based on audio and video signals inside a vehicle to detect violence. Furthermore, the architecture used was ResNet model with Mel-spectrogram methodology for audio signals. The proposed method for video signal representation was RGB, which applied four different models: C2D, I3D, X3D, and Flow-Gated. Finally, multimodal fusion was applied at the end of the process.},
  archive      = {J_EXSY},
  author       = {Dalila Duraes and Flavio Santos and Francisco S. Marcondes and Niklas Hammerschmidt and Paulo Novais},
  doi          = {10.1111/exsy.13356},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13356},
  shortjournal = {Expert Syst.},
  title        = {Applying multisensor in-car situations to detect violence},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based image processing for financial audit risk quantification in healthcare. <em>EXSY</em>, <em>42</em>(1), e13355. (<a href='https://doi.org/10.1111/exsy.13355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning technology has gradually penetrated various fields. Today, the field of healthcare is also closely linked to deep learning technology. Image processing technology based on deep learning can accurately segment medical images, which is convenient for medical research and pathological analysis. Accurate distribution of images can effectively save medical resources. Therefore, image processing techniques can contribute to the quantification and assessment of economic audit risks. In recent years, medical image segmentation has achieved many research results. However, with the improvement of accuracy, the segmentation standards of medical images are also becoming more and more stringent. For medical images, they tend to have rough and fuzzy boundaries and noise disturbances of different shapes. The above problems pose challenges for accurate localization and segmentation of lesion regions. On the other hand, in the field of medical images, there are also problems such as unbalanced number of samples and scarcity of large medical image datasets. In response to these problems, this paper conducts research work and proposes an Attention Mechanism and Multi-Scale spatial Pooling-based conditional Adversarial Network (AM-MSP-cGAN) model to achieve automatic segmentation of medical images. AM-MSP-cGAN can learn more detailed features from fuzzy boundaries, and effectively solve the problem of data lack, thereby promoting economic audit risk quantification and assessment in the healthcare field.},
  archive      = {J_EXSY},
  author       = {Yanzhe Ma},
  doi          = {10.1111/exsy.13355},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13355},
  shortjournal = {Expert Syst.},
  title        = {Deep learning-based image processing for financial audit risk quantification in healthcare},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Categorical surrogation of agent-based models: A comparative study of machine learning classifiers. <em>EXSY</em>, <em>42</em>(1), e13342. (<a href='https://doi.org/10.1111/exsy.13342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent-based modelling has gained recognition in the last years because it provides a natural way to explore the behaviour of social systems. However, agent-based models usually have a considerable number of parameters that make it computationally prohibitive to explore the complete space of parameter combinations. A promising approach to overcome the computational constraints of agent-based models is the use of machine learning-based surrogates or metamodels, which can be used as efficient proxies of the original agent-based model. As the use of metamodels of agent-based simulations is still an incipient area of research, there are no guidelines on which algorithms are the most suitable candidates. In order to contribute to filling this gap, we conduct here a systematic comparative analysis to evaluate different machine learning-based approaches to agent-based model surrogation. A key innovation of our work is the focus on classification methods for categorical metamodeling, which is highly relevant because agent-based simulations are very often validated in a qualitative way. To analyse the performance of the classifiers we use three types of indicators—measures of correctness, efficiency, and robustness—and compare their results for different datasets and sample sizes using an agent-based artificial market as a case study.},
  archive      = {J_EXSY},
  author       = {Bàrbara Llacay and Gilbert Peffer},
  doi          = {10.1111/exsy.13342},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13342},
  shortjournal = {Expert Syst.},
  title        = {Categorical surrogation of agent-based models: A comparative study of machine learning classifiers},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech recognition and artificial intelligence based on the development of music system. <em>EXSY</em>, <em>42</em>(1), e13339. (<a href='https://doi.org/10.1111/exsy.13339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the face of this increasingly intelligent society and home life, it is necessary to research and design more intelligent music systems to meet people's needs. This paper uses music speech recognition algorithms when building a music system, and combines artificial intelligence technology to apply cloud-computing technology to the processing and transmission of music data. The system is formed by the fusion of various technologies. At the same time, using embedded technology as the core can handle multiple tasks at the same time, thereby effectively improving the efficiency of the system. In addition, based on speech recognition technology, this paper combines artificial intelligence to develop music systems. Through experimental analysis, it can be seen that the music system based on speech recognition and artificial intelligence proposed in this paper has good results.},
  archive      = {J_EXSY},
  author       = {Jinjing Chi},
  doi          = {10.1111/exsy.13339},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13339},
  shortjournal = {Expert Syst.},
  title        = {Speech recognition and artificial intelligence based on the development of music system},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Voice pathology identification system using a deep learning approach based on unique feature selection sets. <em>EXSY</em>, <em>42</em>(1), e13327. (<a href='https://doi.org/10.1111/exsy.13327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voice pathology diagnosis requires extracting significant features from voice signals, and classical machine learning models can overfit to the training data, which can cause difficult issues and pose challenges. The study aimed to develop a reliable and efficient system for identifying voice pathologies utilizing the long short-term memory (LSTM) method. The study combined unique feature sets such as the mel frequency cepstral coefficients (MFCCs), zero crossing rate (ZCR), and mel spectrograms, which have not been used together in previous works. Voice pathology identification improved the accuracy rate using the LSTM approach on the Saarbruecken voice database (SVD) samples. The best results achieved by the proposed system showed an accuracy rate of 99.3% for /u/ vowel samples in neutral pitch, 99.2% for /a/ vowel samples in high pitch, 99% for /i/ vowel samples in neutral pitch, and 99.2% for sentence samples. The experimental results were evaluated utilizing accuracy, precision, specificity, sensitivity, and F1 measures. Additionally, the study compared the performance of LSTM with that of artificial neural networks (ANNs) and found that LSTM achieved better outcomes.},
  archive      = {J_EXSY},
  author       = {Nuha Qais Abdulmajeed and Belal Al-Khateeb and Mazin Abed Mohammed},
  doi          = {10.1111/exsy.13327},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13327},
  shortjournal = {Expert Syst.},
  title        = {Voice pathology identification system using a deep learning approach based on unique feature selection sets},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on agent-based modelling assisted by machine learning. <em>EXSY</em>, <em>42</em>(1), e13325. (<a href='https://doi.org/10.1111/exsy.13325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent-based models have diversified their applications across various domains due to the ease with which different phenomena can be represented and simulated. These models incorporate heterogeneous, autonomous agents, local interactions, bounded rationality, and often feature explicit spatial representations. However, certain challenges have been identified in their application, including the complexity of design, difficulty in calibrating parameters, and interpreting and analysing results. Therefore, incorporating machine learning (ML) tools in the various stages of the agent-based modelling and simulation process presents a promising approach for current and future research. The main hypothesis of this study is that integrating ML techniques and tools into agent-based modelling can help address challenges encountered during different stages of implementation, ultimately leading to more accurate and effective simulations. The methodology employed in this study involves a comprehensive search and analysis of relevant literature on the topic. This survey reviews significant developments in the integration of ML into the agent-based modelling and simulation process in recent years. The results of this study summarize the fundamental concepts of ML and its applications in agent-based modelling, and provide insights into the prospects and challenges for ML-assisted agent-based modelling in the near future.},
  archive      = {J_EXSY},
  author       = {Alejandro Platas-López and Alejandro Guerra-Hernández and Marcela Quiroz-Castellanos and Nicandro Cruz-Ramirez},
  doi          = {10.1111/exsy.13325},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13325},
  shortjournal = {Expert Syst.},
  title        = {A survey on agent-based modelling assisted by machine learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-based offline reinforcement learning for sustainable fishery management. <em>EXSY</em>, <em>42</em>(1), e13324. (<a href='https://doi.org/10.1111/exsy.13324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisheries, as indispensable natural resources for human, need to be managed with both short-term economical benefits and long-term sustainability in consideration. This has remained a challenge, because the population and catch dynamics of the fisheries are complex and noisy, while the data available is often scarce and only provides partial information on the dynamics. To address these challenges, we formulate the population and catch dynamics as a Partially Observable Markov Decision Process (POMDP), and propose a model-based offline reinforcement learning approach to learn an optimal management policy. Our approach allows learning fishery management policies from possibly incomplete fishery data generated by a stochastic fishery system. This involves first learning a POMDP fishery model using a novel least squares approach, and then computing the optimal policy for the learned POMDP. The learned fishery dynamics model is useful for explaining the resulting policy's performance. We perform systematic and comprehensive simulation study to quantify the effects of stochasticity in fishery dynamics, proliferation rates, missing values in fishery data, dynamics model misspecification, and variability of effort (e.g., the number of boat days). When the effort is sufficiently variable and the noise is moderate, our method can produce a competitive policy that achieves 85% of the optimal value, even for the hardest case of noisy incomplete data and a misspecified model. Interestingly, the learned policies seem to be robust in the presence of model learning errors. However, non-identifiability kicks in if there is insufficient variability in the effort level and the fishery system is stochastic. This often results in poor policies, highlighting the need for sufficiently informative data. We also provide a theoretical analysis on model misspecification and discuss the tendency of a Schaefer model to overfit compared with a Beverton–Holt model.},
  archive      = {J_EXSY},
  author       = {Jun Ju and Hanna Kurniawati and Dirk Kroese and Nan Ye},
  doi          = {10.1111/exsy.13324},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13324},
  shortjournal = {Expert Syst.},
  title        = {Model-based offline reinforcement learning for sustainable fishery management},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic detection surface defects based on convolutional neural networks and deflectometry. <em>EXSY</em>, <em>42</em>(1), e13323. (<a href='https://doi.org/10.1111/exsy.13323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface defects in industrial refrigerator manufacturing processes can cause significant production losses and compromise product quality. This area is underexplored and currently, visual quality inspection is a subjective process that requires expert intervention, which limits process efficiency and can lead to errors in defect detection. This paper presents a novel approach for automatic surface defect detection using a combination of convolutional neural networks (CNN) and deflectometry. The proposed method takes advantage of the high accuracy and robustness of CNNs in image classification tasks and the sensitivity of deflectometry to detect subtle surface variations. First, a prototype was built to get the images from the refrigerator. Second, using video recordings, we captured surface topographic data using deflectometry, which we then use to generate surface images. Next, we train a CNN to classify the surface images as defective or normal. The proposed method offers a promising solution for automatic detection and quality control of surface defects in refrigerator manufacturing processes. However, this method could also improve the production of vehicles, household appliances in general, and any product that can suffer scratches and dents.},
  archive      = {J_EXSY},
  author       = {Felipe Buitrago and Luis Fernando Castillo Ossa and Jeferson Arango-López},
  doi          = {10.1111/exsy.13323},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13323},
  shortjournal = {Expert Syst.},
  title        = {Automatic detection surface defects based on convolutional neural networks and deflectometry},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive search personalization and privacy protection using federated learning. <em>EXSY</em>, <em>42</em>(1), e13318. (<a href='https://doi.org/10.1111/exsy.13318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized search comes under recommendation systems which provide more user-focused query results. Applications such as movie recommendations, document recommendations, and so forth are using machine learning algorithms to suggest user-tailored content. These algorithms train on historical user query data and predict documents based on user preferences. The existing methodologies on personalization have shown two main limitations: privacy concerns over the usage of personal data and a lack of efficient personalizing strategies for better accuracy and overall standardization. In this paper, we propose a novel federated learning (FL) algorithm for personalized search results using temporal characteristics of the user query data. Individual user query data is used for developing specific client models while the aggregate of such models is developed for general search suggestions. The introduction of time-series interpretation of queries provides for larger training data as well as a better understanding of a user's current needs and intent. The proposed algorithm is validated using the AOL4PS dataset and is evaluated on the efficiency of personalization, and the amount of data and time required to achieve the results. Its performance is compared with existing state-of-the-art personalization algorithms that utilize deep learning and FL. Mean reciprocal rank (MRR) is the primary metric for measuring the algorithm's performance. After training for 35 federated rounds, the server model yielded an MRR score of 0.8638 while the client models were able to yield an average MRR score of 0.9308.},
  archive      = {J_EXSY},
  author       = {Sagnik Sarkar and Shaashwat Agrawal and Aditi Chowdhuri and S. Ramani},
  doi          = {10.1111/exsy.13318},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13318},
  shortjournal = {Expert Syst.},
  title        = {Progressive search personalization and privacy protection using federated learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing deadlock in large-scale, complex rail networks via multi-agent deep reinforcement learning. <em>EXSY</em>, <em>42</em>(1), e13315. (<a href='https://doi.org/10.1111/exsy.13315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rail freight planning problems pose specific challenges that have attracted the attention of academics and industry professionals for many decades. They involve multiple types of assets (trains, stations, terminals, etc.) and are subjected to structural, operational and safety constraints. Even though various approaches have been proposed, few can address the complexity and size of real-world scenarios, and decentralized techniques, like multi-agent systems (MAS), have become more prevalent. The current state of the art in disciplines such as agent technology, reinforcement learning and discrete-event simulation allows the implementation of complex architectures, with multiple actors interacting and learning simultaneously. Therefore, this study takes advantage of these current advances and proposes an innovative approach to real-time traffic management problems in freight railway networks through multi-agent deep reinforcement learning (MADRL). This study was motivated by the decision-making scheduling problems arising in the Hunter Valley Coal Chain (HVCC), located in New South Wales, Australia. The MADRL algorithm uses as the training environment the simulation model currently utilized for capacity planning of the HVCC, allowing experiments with actual data. Thus, we enhanced the simulation model to accommodate a MAS with intelligent agents representing system elements, such as trains, dump stations, and load points. Furthermore, these agents act in a decentralized fashion based on local observations, constituting a partially-observed Markov decision process (dec-POMDP). Three variations of the MADRL approach are presented: a baseline model, an extended model, and one that directly addresses deadlocks. Finally, we present a transfer learning method that improves deadlock resolution and leverages performance. In the experiments, we explore specific, complex scenarios arising in the HVCC, where trains frequently face deadlock conditions. The baseline model outperforms a first-come-first-serve (FCFS) based heuristic used by HVCC's simulation model and a genetic algorithm in instances with up to 60 trains – but fails in more complex scenarios. On the other hand, the most advanced model, which addresses deadlocks via transfer learning, always finds feasible solutions and produces policies that outperform the FCFS-based heuristic in 94% of the instances.},
  archive      = {J_EXSY},
  author       = {A. M. C. Bretas and A. Mendes and S. Chalup and M. Jackson and R. Clement and C. Sanhueza},
  doi          = {10.1111/exsy.13315},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13315},
  shortjournal = {Expert Syst.},
  title        = {Addressing deadlock in large-scale, complex rail networks via multi-agent deep reinforcement learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep facial emotion recognition model using optimal feature extraction and dual-attention residual U-net classifier. <em>EXSY</em>, <em>42</em>(1), e13314. (<a href='https://doi.org/10.1111/exsy.13314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human facial emotion recognition (FER) system has become an active research area and it has attracted various research communities for its wide ranging and promising applications especially in the cybersecurity field. Recognizing various facial expressions corresponding to the emotional forms is considered as a significant task in the system of FER. Typically, the automated system of FER consists of two major and important steps like feature extraction, and facial emotion recognition. In this work, initially the input data is acquired and the features are extracted from the input facial image with the use of Fuzzy Eigen Weighted based feature extraction model (FEW-FE). Among the extracted features, an optimal and best features are selected by means of feature selection technique, which employs Chaotic Spider Monkey Optimization algorithm (CSMO) so as to find best fitness function solution. The use of this optimization technique for the feature subset selection aids the enhancement of classifier performance. Then, the recognition process is carried using Dual-attention residual U-Net classifier framework. The performance evaluation of this proposed model is carried over three input datasets considered such as CK+, FER2013, and JAFFE in terms of recognition accuracy, precision, recall, and F-measure. The comparison is made for the feature extraction and classifier model with that of various existing methodologies which shows the effectiveness of proposed system over other traditional schemes. This proposed design is applicable in the cybersecurity system to detect a person's emotional state from their expression.},
  archive      = {J_EXSY},
  author       = {Belhassen Akrout},
  doi          = {10.1111/exsy.13314},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13314},
  shortjournal = {Expert Syst.},
  title        = {Deep facial emotion recognition model using optimal feature extraction and dual-attention residual U-net classifier},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of transformational leadership in enhancing competitive advantage for E-business analysis in organisational practices. <em>EXSY</em>, <em>42</em>(1), e13307. (<a href='https://doi.org/10.1111/exsy.13307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research examines the connection between the adoption of e-commerce and leadership ideologies. The purpose of the study is to evaluate leaders' perspectives toward using e-commerce and to determine the sorts of management styles that are associated with the level of e-commerce adoption. Becoming a successful leader in the economic world of today is a difficult task. Businesses which have already adopted new business concepts nowadays often have a greater benefit and fulfil more clients than businesses that operate in the traditional way. The personality and leadership style of a manager directly affect the business ecology, according to experience. To improve the organisation's efficiency in practice, various methods are widely used to promote employees' personal development based on the “generating learning” and “organisational knowledge” systems. This study investigates the role of leadership in enhancing competitive advantage for E-business analysis in organisational practices that has been done at the Near East University in Cyprus. The study provided questionnaires among the university's teaching staff to know the role of leadership in the eyes of the academic staff. The results showed that transformational leadership influences the competitive advantage, proved by the data collected from the Near East University in Cyprus, which contributes to increasing the competitiveness of the university and its sustainable development.},
  archive      = {J_EXSY},
  author       = {Sarhang Salahadin Saleh and Khairi Ali Auso},
  doi          = {10.1111/exsy.13307},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13307},
  shortjournal = {Expert Syst.},
  title        = {The role of transformational leadership in enhancing competitive advantage for E-business analysis in organisational practices},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flock optimization induced deep learning for improved diabetes disease classification. <em>EXSY</em>, <em>42</em>(1), e13305. (<a href='https://doi.org/10.1111/exsy.13305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic disease classification requires a precise understanding of the clinical inputs and their intensity as observed through different stages. Automated and machine-centric classification requires validated data handling and non-converging inputs. For improving the classification precision impacted due by complex computations, this article introduces an assimilated method incorporating flock optimization and conventional deep learning. Deep learning trains the classification system through the best-fit solution generated by the flock optimization. The features from the input data are first identified for which an initial population is initiated. The identified features are classified based on their leap-up behaviour; this behaviour is induced if the data feature modifies the actual representation. If the data feature shows up over-fitting behaviour, then it is classified as abnormal and is discarded. Therefore the objective function is to identify the best-fitting data feature from the maximum flock members showing similar leap-up behaviour. This output is used for training the deep learning paradigm for classifying precision-less and high features. The precision is determined using existing classified data that matches better the flock output. If the classified data is under less precision, then the leap-up behaviours' objective is tuned to eliminate over-fitting inputs. Therefore, the variable features are thwarted for preventing precision degradation for varying diabetics' clinical observed data. The introduced system maximize the recognition accuracy by 8.47% and minimize the complexity by 7.65%.},
  archive      = {J_EXSY},
  author       = {Divager Balasubramaniyan and Nor Azura Husin and Norwati Mustapha and Nurfadhlina Mohd Sharef and Teh Noranis Mohd Aris},
  doi          = {10.1111/exsy.13305},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13305},
  shortjournal = {Expert Syst.},
  title        = {Flock optimization induced deep learning for improved diabetes disease classification},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drug–drug interaction extraction-based system: An natural language processing approach. <em>EXSY</em>, <em>42</em>(1), e13303. (<a href='https://doi.org/10.1111/exsy.13303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poly-medicated patients, especially those over 65, have increased. Multiple drug use and inappropriate prescribing increase drug–drug interactions, adverse drug reactions, morbidity, and mortality. This issue was addressed with recommendation systems. Health professionals have not followed these systems due to their poor alert quality and incomplete databases. Recent research shows a growing interest in using Text Mining via NLP to extract drug–drug interactions from unstructured data sources to support clinical prescribing decisions. NLP text mining and machine learning classifier training for drug relation extraction were used in this process. In this context, the proposed solution allows to develop an extraction system for drug–drug interactions from unstructured data sources. The system produces structured information, which can be inserted into a database that contains information acquired from three different data sources. The architecture outlined for the drug–drug interaction extraction system is capable of receiving unstructured text, identifying drug entities sentence by sentence, and determining whether or not there are interactions between them.},
  archive      = {J_EXSY},
  author       = {José Machado and Carla Rodrigues and Regina Sousa and Luis Mendes Gomes},
  doi          = {10.1111/exsy.13303},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13303},
  shortjournal = {Expert Syst.},
  title        = {Drug–drug interaction extraction-based system: An natural language processing approach},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MBRSDTC: Design of a multimodal bioinspired model to improve resource scheduling efficiency with differential task-level constraints. <em>EXSY</em>, <em>42</em>(1), e13302. (<a href='https://doi.org/10.1111/exsy.13302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling of resources in cloud environments requires design of multiple pattern analysis models that include but are not limited to, inter-task dependency pattern analysis, make-span pattern analysis, virtual machine (VM or resource) based capacity analysis, deadline analysis, and VM-to-task compatibility analysis. Existing scheduling models are either highly complex, or do not integrate comprehensive analysis modules for efficient scheduling of resources to tasks. Moreover, some of these models showcase limited scalability when applied to large-scale deployment scenarios. To overcome these issues, this text proposes design of a multimodal bioinspired model to improve resource-scheduling efficiency with differential task-level constraints. The proposed model initially collects multimodal information sets about tasks and underlying resources in order to augment analysis efficiency for different task types. These information sets are initially processed by a Grey Wolf Optimization (GWO)-based scheduling model that assists in grouping tasks based on their make-span, deadline and dependency levels. The grouped tasks are then scheduled via an incremental learning Elephant Herding Optimization (EHO) model that assists in assigning grouped tasks to capacity-tuned resources (or VMs). Due to integration of these optimization methods, the proposed model is capable of improving the efficiency of resource scheduling by 8.5%, while reducing computational complexity by 4.3%, while improving the deadline hit ratio by 5.9%, and lowering energy consumption by 1.5% when compared with standard machine learning based scheduling techniques. Due to which the proposed model is capable of deployment for a wide variety of real-time scheduling scenarios.},
  archive      = {J_EXSY},
  author       = {Madala Guru Brahmam and Vijay Anand R.},
  doi          = {10.1111/exsy.13302},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13302},
  shortjournal = {Expert Syst.},
  title        = {MBRSDTC: Design of a multimodal bioinspired model to improve resource scheduling efficiency with differential task-level constraints},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A citywide TD-learning based intelligent traffic signal control for autonomous vehicles: Performance evaluation using SUMO. <em>EXSY</em>, <em>42</em>(1), e13301. (<a href='https://doi.org/10.1111/exsy.13301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An autonomous vehicle can sense its environment and operate without human involvement. Its adequate management in an intelligent transportation system could significantly reduce traffic congestion and overall travel time in a network. Adaptive traffic signal controller (ATSC) based on multi-agent systems using state-action-reward-state-action (SARSA ( λ )) are well-known state-of-the-art models to manage autonomous vehicles within urban areas. However, this study found inefficient weights updating mechanisms of the conventional SARSA ( λ ) models. Therefore, it proposes a Gaussian function to regulate the eligibility trace vector's decay mechanism effectively. On the other hand, an efficient understanding of the state of the traffic environment is crucial for an agent to take optimal actions. The conventional models feed the state values to the agents through the MinMax normalization technique, which sometimes shows less efficiency and robustness. So, this study suggests the MaxAbs scaled state values instead of MinMax to address the problem. Furthermore, the combination of the A-star routing algorithm and proposed model demonstrated a good increase in performance relatively to the conventional SARSA ( )-based routing algorithms. The proposed model and the baselines were implemented in a microscopic traffic simulation environment using the SUMO package over a complex real-world-like -intersections network to evaluate their performance. The results showed a reduction of the vehicle's average total waiting time and total stops by a mean value of % and % compared to the considered baselines. Also, the A-star combined with the proposed controller outperformed the conventional approaches by increasing the vehicle's average trip speed by %.},
  archive      = {J_EXSY},
  author       = {Selim Reza and Marta Campos Ferreira and J. J. M. Machado and João Manuel R. S. Tavares},
  doi          = {10.1111/exsy.13301},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13301},
  shortjournal = {Expert Syst.},
  title        = {A citywide TD-learning based intelligent traffic signal control for autonomous vehicles: Performance evaluation using SUMO},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion estimation model for cognitive state analysis of learners in online education using deep learning. <em>EXSY</em>, <em>42</em>(1), e13289. (<a href='https://doi.org/10.1111/exsy.13289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Facial Expression Recognition, occlusion and position change that may drastically alter facial expressions are two important challenges (FER). Due to advances in automated FER over the last several decades, it has received less attention in the real world, where occlusion- and pose-invariant aspects of FER are critical. Online education learners' cognitive states may be assessed using this paper's focus on real-world stance and occlusion robust FER. The human visual system's attention mechanism inspired us to develop a new kind of spatial attention network (SAN-CNN). Saliency characteristics and spatial importance between adjacent pixels are emphasized in the SAN-CNN model. Preprocessing an input picture using a median contour filter is used here initially, followed by mask-based ROI for segmentation. Using CNNs for facial recognition, landmark localization, and head position estimation based on spatial attention networks, it is possible to do emotional categorization. Using the Kaggle public video-based facial expression datasets, we were able to demonstrate that our proposed approach is more accurate and faster than the usual techniques. In addition, we evaluated the suggested method's performance metrics with those of the already used approaches. FER with occlusion and variant posture performs better than standard approaches using our suggested method, as shown by the results of the experiments.},
  archive      = {J_EXSY},
  author       = {Maragoni Mahendar and Arun Malik and Isha Batra},
  doi          = {10.1111/exsy.13289},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13289},
  shortjournal = {Expert Syst.},
  title        = {Emotion estimation model for cognitive state analysis of learners in online education using deep learning},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A wavelet multifractal model for quality of life index measuring during pandemics and crises. <em>EXSY</em>, <em>42</em>(1), e13284. (<a href='https://doi.org/10.1111/exsy.13284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present article, we investigate the impact of the timescale factor on the quality of life index behaviour on specific time intervals characterized by the presence of socio-economic, political, and/or health severe movements such as pandemics and crises. We essentially aim to show that effectively the quality of life evaluation based on a single index as in the existing studies may be described more adequately by a variable index due to the social, political, economic, and also healthy environment. The variability discovered is expressed by the existence and the estimation of a multi-index instead of a single one with relatively too many factors. Our focus is mainly on the effect of the COVID-19 pandemic and crises or crashes on the quality of life. It turns out that the first essays of empirical treatments of such a series bring out a fractal/multifractal aspect. This motivates our main idea reposing on the fractal/multifractal structure of the data to construct a quantitative model based on wavelets combined with change-point analysis. Our model is applied empirically on a sample corresponding to Saudi Arabia as a case of study during the period from January 1990 to December 2021. The end of this period is strongly affected by the COVID-19 pandemic. The sample is based on social media conversations and texts discussing and describing the satisfaction with the quality of life. The study confirms effectively that the role of the timescale factor is more described when considering a multi-index rather than measurement on the whole time interval. Besides, this multi-index is clearly illustrated by means of the multifractal spectra of the data used.},
  archive      = {J_EXSY},
  author       = {Majed S. Balalaa and Anouar Ben Mabrouk},
  doi          = {10.1111/exsy.13284},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13284},
  shortjournal = {Expert Syst.},
  title        = {A wavelet multifractal model for quality of life index measuring during pandemics and crises},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PriMed: Private federated training and encrypted inference on medical images in healthcare. <em>EXSY</em>, <em>42</em>(1), e13283. (<a href='https://doi.org/10.1111/exsy.13283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In healthcare, patient information is a sparse critical asset considered as private data and is often protected by law. It is also the domain which is least explored in the field of Machine Learning. The main reason for this is to build efficient artificial intelligence (AI) based models for preliminary diagnosis of various diseases, it would require a large corpus of data which can be obtained by pooling in patient information from multiple sources. However, for these sources to agree to sharing their data across distributed systems for training algorithms and models, there has to be an assurance that there will be no disclosure of the personally identifiable information (PII) of the respective Data Owners. This paper proposes PriMed, an approach to build robust privacy preserving additions to convolutional neural networks (CNN) for training and performing inference on medical images without compromising privacy. Since privacy of the data is preserved, large amounts of data can be effectively accumulated to increase the accuracy and efficiency of AI models in the field of healthcare. This involves implementing a hybrid of privacy-enhancing techniques like Federated Learning, Differential Privacy, and Homomorphic Encryption to provide a private and secure environment for learning through data.},
  archive      = {J_EXSY},
  author       = {Aparna Gopalakrishnan and Narayan P. Kulkarni and Chethan B. Raghavendra and Raghavendra Manjappa and Prasad Honnavalli and Sivaraman Eswaran},
  doi          = {10.1111/exsy.13283},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13283},
  shortjournal = {Expert Syst.},
  title        = {PriMed: Private federated training and encrypted inference on medical images in healthcare},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency aware task scheduling using DVFS for energy efficiency in cloud data centre. <em>EXSY</em>, <em>42</em>(1), e13276. (<a href='https://doi.org/10.1111/exsy.13276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable processing capacity and flexible storage space make Cloud computing the most recent favourable technology. Many organizations have converted their conventional processing data centre to a Cloud data centre. Cloud computing provides promising execution and storage, which leads to massive growth in processing demand by Cloud users. It makes the Cloud data centre increase the number of virtual machines (VM) to execute the users tasks. Hence, it causes high frequency disbursed and has increased energy consumption. Many techniques were proposed, which focuses on Cloud energy saving. However, there is still a lack of trade-off between energy-efficient task allocation and frequency scaling for a given workload. In this work, we propose a task scheduling algorithm that aims to minimize energy consumption through the frequency scaling technique while improving task execution time. Specifically, our scheduler comprises two modules, which are the scaling frequency module and frequency-aware task scheduling module. In our first module, we utilize Dynamic Voltage and Frequency Scaling-Optimal Frequency (DVFS) to determine the optimal frequency and selecting the best server for the incoming tasks. The number of VM is created upon the best server. As for the second module, the VM processing capacity is scaled to the required frequency of the task. We identify it as a required processing capacity for executing the tasks. The experiment result shows that our algorithm has outperformed and efficiently minimized the energy consumption in the Cloud data centre as compared with existing energy-saving techniques. Meanwhile, the task allocation also has met the system's Quality of Service (QoS). Significantly, leveraging the resource processing frequency is able to gain better trade-off between performance and energy consumption in the Cloud data centre.},
  archive      = {J_EXSY},
  author       = {Joshua Samual and Masnida Hussin and Nor Asilah Wati Abdul Hamid and Azizol Abdullah},
  doi          = {10.1111/exsy.13276},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13276},
  shortjournal = {Expert Syst.},
  title        = {Frequency aware task scheduling using DVFS for energy efficiency in cloud data centre},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compressed lightweight deep learning models for resource-constrained internet of things devices in the healthcare sector. <em>EXSY</em>, <em>42</em>(1), e13269. (<a href='https://doi.org/10.1111/exsy.13269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of convolutional neural networks (CNNs) in image classification and object detection has been remarkable, even though they contain millions and billions of parameters. This over-parameterization of CNN makes them both memory-intensive and computationally complex and exhaustive. This greatly hinders the application of CNNs in resource-constrained environments such as Internet of things (IoT) and edge devices. This poses a critical challenge for CNNs in deploying these powerful computer vision tools to mobile devices, which needs immediate attention. In this study, we have proposed a novel technique based on non-convex optimization, max-norm regularization. The max-norm will structurally prune the number of parameters without compromising the model's performance. The proximal gradient descent algorithm is used for network optimization while using this non-convex regularizer. The max-norm is combined with the channel pruning to achieve more sparse CNN networks. Later, the pruned network can be easily deployed in the resource-constrained application environment. The proposed technique is tested on several benchmark datasets for validation. In addition, in this study, the sparsified CNNs are used for biomedical image analysis using the BRAIN MRI dataset. This sparsely trained CNN model can later serve as the best lightweight model applicable in the IoT healthcare sector for detecting and classifying three types of brain tumours, one of the most life-threatening diseases whose early detection can save the costly lives of human beings. This is the first paper to propose the novel max-norm regularizer to enforce sparse learning through CNNs. The paper provides a detailed analysis of convex and non-convex regularizers before presenting the proposed novel max-norm regularizer. Finally, the paper compares the proposed max-norm regularizer with existing regularization methods using state-of-the-art CNN models.},
  archive      = {J_EXSY},
  author       = {Gousia Habib and Shaima Qureshi},
  doi          = {10.1111/exsy.13269},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13269},
  shortjournal = {Expert Syst.},
  title        = {Compressed lightweight deep learning models for resource-constrained internet of things devices in the healthcare sector},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness and sensitivity of some wavelet multifractal models in fractal data modelling. <em>EXSY</em>, <em>42</em>(1), e13268. (<a href='https://doi.org/10.1111/exsy.13268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many multifractal models such as self-similar and scaling law types have been proved to be efficient modellers and estimators in many fields such as financial time series where the data hide fractal and multifractal structures, allowing its processing without sophisticated models to be difficult. However, in statistical analysis, a necessary part that should take place for any model and estimator consists in tests of performance such as confidence intervals and generally statistical tests to confirm the adequacy of the model. The present paper provides the consideration of multifractal models based on wavelets and self-similar type processes to study statistical tests. To test the efficiency, accuracy and robustness of the models, different inferential statistics are introduced, provided with some empirical examples due to the EURO/USD exchange rate time series with a sample covering the period 03/01/2000 to 30/08/2022. Contrarily to existing works, we showed in the present work that quasi-self-similar type models are better for many reasons. They indeed guarantee the well fitting of the data dynamics, the nonlinearity in both the model and the multifractal spectrum, the renormalization parameters which may differ from one scale to another and the preservation of the quasi multiplicative structure.},
  archive      = {J_EXSY},
  author       = {Sabrine Arfaoui and Nidhal Ben Abdallah},
  doi          = {10.1111/exsy.13268},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13268},
  shortjournal = {Expert Syst.},
  title        = {Robustness and sensitivity of some wavelet multifractal models in fractal data modelling},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph colouring using evolutionary computation: A case study of blind naked mole-rat algorithm. <em>EXSY</em>, <em>42</em>(1), e13262. (<a href='https://doi.org/10.1111/exsy.13262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph colouring problem (GCP) is an NP-complete optimization problem. It is famous for its applications in scheduling, register allocation, and map colouring. In recent years, biological inspired and especially Swarm intelligence (SI) techniques have gained popularity for solving complex optimization problems. In this article, we have proposed blind naked mole rat-based colouring (BNMR-Col) for graphs. BNMR-Col uses both exploitation and exploration to find the best solution in search space. Exploitation uses both local moves and global moves to find a better solution in the surroundings of an existing solution. On the other hand, exploration generates new solution by combining various solutions from the search space. BNMR-Col shows better convergence rate and approaches the lowest colour value in 83% of the cases when tested on standard benchmark graph instances.},
  archive      = {J_EXSY},
  author       = {Fahad Maqbool and Muhammad Fahad and Muhammad Ilyas and Hajira Jabeen},
  doi          = {10.1111/exsy.13262},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13262},
  shortjournal = {Expert Syst.},
  title        = {Graph colouring using evolutionary computation: A case study of blind naked mole-rat algorithm},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video-based real-time assessment and diagnosis of autism spectrum disorder using deep neural networks. <em>EXSY</em>, <em>42</em>(1), e13253. (<a href='https://doi.org/10.1111/exsy.13253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition (HAR) in untrimmed videos can make insightful predictions of human behaviour. Previous work on HAR-included models trained on spatial and temporal annotations and could classify limited actions from trimmed videos. These methods reported limitations such as (1) performance degradation due to the lack of precision temporal regions proposal and (2) poor adaptability of the models in the clinical domain because of unrelated actions of interest. We propose an innovative method that could analyse untrimmed behavioural videos to recommend actions of interest leading to diagnostic and functional assessments for children with Autism Spectrum Disorder (ASD). Our method entails end-to-end behaviour action recognition (BAR) pipeline, including child detection, temporal action localization, and actions of interest identification and classification. The model trained on the data of 400 ASD children and 125 with other developmental delays (ODD) accurately identified ASD, ODD, and Neurotypical children with 79.7%, 77.2%, and 80.8% accuracy, respectively. The model's performance on an independent benchmark Self-Stimulatory Behaviour Dataset (SSBD) reported top-1 accuracy of 78.57% for combined localization with action recognition, significantly higher than the earlier reported outcomes.},
  archive      = {J_EXSY},
  author       = {Varun Ganjigunte Prakash and Manu Kohli and Aragulla Prasad Prathosh and Monica Juneja and Manushree Gupta and Smitha Sairam and Sadasivan Sitaraman and Anjali Sanjeev Bangalore and John Vijay Sagar Kommu and Lokesh Saini and Prashant Ramesh Utage and Nishant Goyal},
  doi          = {10.1111/exsy.13253},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13253},
  shortjournal = {Expert Syst.},
  title        = {Video-based real-time assessment and diagnosis of autism spectrum disorder using deep neural networks},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fifth-generation fractal antenna design based on the koch snowflake geometry. a fractal theory application. <em>EXSY</em>, <em>42</em>(1), e13242. (<a href='https://doi.org/10.1111/exsy.13242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The projection of fifth-generation (5G) fractal antennas and their advantageous geometry are examined. The fact that fractal-shaped antennas based on Koch Snowflake geometry are suitable for higher frequencies was shown above all. By the instrumentality of this technology, which aims to serve a large number of subscribers by implementing revolutionary devices (e.g. Massive MIMO, beam-forming, high-performance processors, etc.), it is possible to obtain multiple facilities. Fractal geometrics forms have been made using fractalKoch function from MATLAB R2018b, having as initial characteristic the series of successful repetitions. 2D and 3D characteristics of directivity at the values of 455, 465, 755.5 and 789.5 MHz (designed antennas), for two, three respectively four iterations of Koch's Snowflake fractals have been highlighted. We mention that all the software and programs developed for the fractal antennas design obtained in this paper are new and genuine.},
  archive      = {J_EXSY},
  author       = {Maria-Alexandra Paun and Mihai-Virgil Nichita and Vladimir-Alexandru Paun and Viorel-Puiu Paun},
  doi          = {10.1111/exsy.13242},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13242},
  shortjournal = {Expert Syst.},
  title        = {Fifth-generation fractal antenna design based on the koch snowflake geometry. a fractal theory application},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structured knowledge creation for urdu language: A DBpedia approach. <em>EXSY</em>, <em>42</em>(1), e13223. (<a href='https://doi.org/10.1111/exsy.13223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wikipedia information is extracted by DBpedia and linked to other web resources as Linked Open Data, which is an important contribution to the field of semantics. As part of its internationalisation endeavour, DBpedia now has 20 language chapters that have been mapped to it; nonetheless, there have been very few attempts from Urdu. This article outlines the procedures and highlights the efforts put forward as the first contribution to the manual creation of Urdu mappings with DBpedia Ontology classes. Our approach led to an increase in the number of mapped infoboxes, thus enhancing the DBpedia. The mapping procedure is broken down into two parts. The infobox template is first mapped to the DBpedia ontology's relevant class, and then the attributes of the infobox are mapped to the properties of that class. In addition, alongside other mapped languages, Urdu labels are included to the description of Ontology classes. We have covered around a thousand properties and attributes of Urdu with English DBpedia Ontology on DBpedia mapping server.},
  archive      = {J_EXSY},
  author       = {Shanza Rasham and Habib Ullah Khan and Fahad Maqbool and Saad Razzaq and Sajid Anwar and Muhammad Ilyas},
  doi          = {10.1111/exsy.13223},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13223},
  shortjournal = {Expert Syst.},
  title        = {Structured knowledge creation for urdu language: A DBpedia approach},
  volume       = {42},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
