<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JAIR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jair">JAIR - 111</h2>
<ul>
<li><details>
<summary>
(2025). Banal deception and human-AI ecosystems: A study of people’s perceptions of LLM-generated deceptive behaviour. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.18724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) can provide users with false, inaccurate, or misleading information, and we consider the output of this type of information as what Natale calls ‘banal’ deceptive behaviour [53]. Here, we investigate peoples’ perceptions of ChatGPT-generated deceptive behaviour and how this affects people’s behaviour and trust. To do this, we use a mixed-methods approach comprising of (i) an online survey with 220 participants and (ii) semi-structured interviews with 12 participants. Our results show that (i) the most common types of deceptive information encountered were over-simplifications and outdated information; (ii) humans’ perceptions of trust and chat-worthiness of ChatGPT are impacted by ‘banal’ deceptive behaviour; (iii) the perceived responsibility for deception is influenced by education level and the perceived frequency of deceptive information; and (iv) users become more cautious after encountering deceptive information, but they come to trust the technology more when they identify advantages of using it. Our findings contribute to understanding human-AI interaction dynamics in the context of Deceptive AI Ecosystems and highlight the importance of user-centric approaches to mitigating the potential harms of deceptive AI technologies.},
  archive      = {J_JAIR},
  author       = {Xiao Zhan and Yifan Xu and Noura Abdi and Joe Collenette and Stefan Sarkadi},
  doi          = {10.1613/jair.1.18724},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {10},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Banal deception and human-AI ecosystems: A study of people’s perceptions of LLM-generated deceptive behaviour},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thousands of AI authors on the future of AI. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.19087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In October 2023, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace, nature and impacts of AI progress. Significant steps were taken to minimize and evaluate bias. In evaluations of participation bias, we found that most groups responded at similar rates. The participants estimated that several milestones had at least a 50% chance of being feasible for AI by 2028, including constructing a payment processing site and fine-tuning an LLM. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027 and 50% by 2047—13 years earlier than in our 2022 survey (N = 738). The chance of all occupations becoming fully automatable, however, was not expected to reach 10% until 2037, and 50% until 2116 (compared to 2164 in the 2022 survey. Most respondents expressed substantial uncertainty about long-term impacts: While 68% in 2023 thought good outcomes from high-level machine intelligence AI were more likely than bad ones, 48% of these net optimists gave at least a 5% chance of extremely bad outcomes. Conversely, 59% of net pessimists gave 5% or more to extremely good outcomes. Depending on how we asked, between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that “substantial” or “extreme” concern is warranted about AI increasing misinformation, boosting authoritarian control, worsening inequality, and other scenarios. There was broad agreement that research aimed at minimizing risks from AI systems ought to be more prioritized.},
  archive      = {J_JAIR},
  author       = {Katja Grace and Julia Fabienne Sandkühler and Harlan Stewart and Benjamin Weinstein-Raun and Stephen Thomas and Zach Stein-Perlman and John Salvatier and Jan Brauner and Richard C. Korzekwa},
  doi          = {10.1613/jair.1.19087},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {10},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Thousands of AI authors on the future of AI},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding minimal plan reductions using classical planning. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.19437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While classical planning research has made tremendous progress in the last decades, many complex tasks can still only be solved suboptimally. The satisficing plans found for these tasks often contain actions that can be removed while maintaining plan validity. Removing such redundant actions is desirable since it can decrease the plan cost and simplify the plan. Reducing a plan to a minimum-cost plan without redundant actions is NP-complete and previous work addressed this problem with a compilation to weighted MaxSAT. In this work, we propose several simple and natural formulations to encode this problem as a classical planning task, and prove that solving the resulting tasks optimally guarantees finding minimal plan reductions. We analyze the relation of the classical planning formulations to the MaxSAT compilation, and prove theoretical properties of the known concept of plan action landmarks. Finally, we evaluate the new approaches experimentally and show that they are competitive with the previous state of the art in minimal plan reduction.},
  archive      = {J_JAIR},
  author       = {Mauricio Salerno and Raquel Fuentetaja and Jendrik Seipp},
  doi          = {10.1613/jair.1.19437},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {10},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Finding minimal plan reductions using classical planning},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Viewpoint: The future of human-centric explainable artificial intelligence (XAI) is not post-hoc explanations. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.17970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable Artificial Intelligence (XAI) plays a crucial role in enabling human understanding and trust in deep learning systems. As models get larger, more ubiquitous, and pervasive in aspects of daily life, explainability is necessary to minimize adverse effects of model mistakes. Unfortunately, current approaches in human-centric XAI (e.g. predictive tasks in healthcare, education, or personalized ads) tend to rely on a single post-hoc explainer, whereas recent work has identified systematic disagreement between post-hoc explainers when applied to the same instances of underlying black-box models. In this viewpoint paper, we therefore present a call for action to address the limitations of current state-of-the-art explainers. We propose a shift from post-hoc explainability to designing interpretable neural network architectures. We identify five needs of human-centric XAI (real-time, accurate, actionable, human-interpretable, and consistent) and propose two possible routes forward for interpretable-by-design neural network workflows (adaptive routing and temporal diagnostics). We postulate that the future of human-centric XAI is neither in explaining black-boxes nor in reverting to traditional, interpretable models, but in neural networks that are intrinsically interpretable.},
  archive      = {J_JAIR},
  author       = {Vinitra Swamy and Jibril Frej and Tanja Käser},
  doi          = {10.1613/jair.1.17970},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Viewpoint: The future of human-centric explainable artificial intelligence (XAI) is not post-hoc explanations},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal decision trees for interpretable and constrained clustering. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.18144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained clustering is a semi-supervised approach to determining meaningful groupings of data that respect user-specified constraints. Such constraints are typically used to enforce desirable structural and domain-specific properties of the resulting clusters. Notably, such constraints can significantly improve the quality and accuracy of clustering. Data clustering solutions can take on many different forms. Decision trees are a particularly desirable solution form because of their inherent interpretability. Unfortunately, existing decision tree clustering approaches do not support clustering constraints and do not provide strong theoretical guarantees with respect to solution quality. To address the task of decision tree clustering with constraints, we present a novel SAT-based encoding that solves the problem to an approximated optimality in relation to a well-known bi-criteria objective. Our framework is the first exact approach for interpretable constrained clustering with decision trees. Experiments involving a range of real-world and synthetic datasets demonstrate that our approach can produce interpretable clustering solutions that are of superior quality compared to their non-interpretable counterparts, with or without the addition of constraints. We further provide new insights into the trade-off between interpretability and the satisfaction of user-specified constraints, presenting extensions to our clustering approach that treat the satisfaction of constraints as an additional optimization objective.},
  archive      = {J_JAIR},
  author       = {Pouya Shati and Yuliang Song and Eldan Cohen and Sheila McIlraith},
  doi          = {10.1613/jair.1.18144},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Optimal decision trees for interpretable and constrained clustering},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On generating monolithic and model reconciling explanations in probabilistic scenarios. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.18820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explanation generation frameworks aim to make AI systems’ decisions transparent and understandable to human users. However, generating explanations in uncertain environments characterized by incomplete information and probabilistic models remains a significant challenge. In this paper, we propose a novel framework for generating probabilistic monolithic explanations and model reconciling explanations. Monolithic explanations provide self-contained reasons for an explanandum without considering the agent receiving the explanation, while model reconciling explanations account for the knowledge of the agent receiving the explanation. For monolithic explanations, our approach integrates uncertainty by utilizing probabilistic logic to increase the probability of the explanandum. For model reconciling explanations, we propose a framework that extends the logic-based variant of the model reconciliation problem to account for probabilistic human models, where the goal is to find explanations that increase the probability of the explanandum while minimizing conflicts between the explanation and the probabilistic human model. We introduce explanatory gain and explanatory power as quantitative metrics to assess the quality of these explanations. Further, we present algorithms that exploit the duality between minimal correction sets and minimal unsatisfiable sets to efficiently compute both types of explanations in probabilistic contexts. Extensive experimental evaluations on various benchmarks demonstrate the effectiveness and scalability of our approach in generating explanations under uncertainty.},
  archive      = {J_JAIR},
  author       = {Stylianos Loukas Vasileiou and William Yeoh and Alessandro Previti and Tran Cao Son},
  doi          = {10.1613/jair.1.18820},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On generating monolithic and model reconciling explanations in probabilistic scenarios},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust reward design for markov decision processes. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.19154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of reward design examines the interaction between a leader and a follower, where the leader aims to shape the follower’s behavior to maximize the leader’s payoff by modifying the follower’s reward function. Current approaches to reward design rely on an accurate model of how the follower responds to reward modifications, which can be sensitive to modeling inaccuracies. To address this issue of sensitivity, we present a solution that offers robustness against uncertainties in modeling the follower, including 1) how the follower breaks ties in the presence of nonunique best responses, 2) inexact knowledge of how the follower perceives reward modifications, and 3) bounded rationality of the follower. Our robust solution is guaranteed to exist under mild conditions and can be obtained numerically by solving a mixed-integer linear program. Numerical experiments on multiple test cases demonstrate that our solution improves robustness compared to the standard approach without incurring significant additional computing costs.},
  archive      = {J_JAIR},
  author       = {Shuo Wu and Haoxiang Ma and Jie Fu and Shuo Han},
  doi          = {10.1613/jair.1.19154},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Robust reward design for markov decision processes},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a simple hedonic game with graph-restricted communication. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.14956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a hedonic game for which feasible coalitions are prescribed by a graph representing the agents’ social relations. A group of agents can form a feasible coalition if and only if their corresponding vertices can be spanned with a star. This requirement guarantees that agents are connected, close to each other, and one central agent can coordinate the actions of the group. In our game, everyone strives to join the largest feasible coalition. We study the existence and computational complexity of both Nash stable and core stable partitions. Then, we provide tight or asymptotically tight bounds on their efficiency, measured in terms of the price of anarchy and the price of stability, under two natural social functions, namely, the number of agents who are not in a singleton coalition, and the number of coalitions. We also derive refined bounds for games in which the social graph is claw-free. Finally, we investigate the complexity of computing socially optimal partitions, as well as extreme Nash stable ones.},
  archive      = {J_JAIR},
  author       = {Vittorio Bilò and Laurent Gourvès and Jérôme Monnot},
  doi          = {10.1613/jair.1.14956},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On a simple hedonic game with graph-restricted communication},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correct explanations and how to define them: Properties and metrics for measuring correctness of three forms of ML model Input/Output behaviour explanations. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.18691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In explainable AI, many explanation methods generate similar yet diverging explanations for machine learning (ML) models. How fair is it then to explain ML model behaviour by such explanations? Arguably, one needs to judge whether those explanations are good at explaining ML model input/output behaviour. We here attempt to formalise ways to judge goodness of such explanations in terms of their correctness. For assessing correctness, one needs to have desirable properties of explanation correctness in mind, as well as was to measure satisfaction of those properties. We submit two high-level properties of soundness and completeness for assessing explanation correctness: explaining is sound if the model behaves the way the explanations say; explaining is complete if explanations can be given for model’s outputs on any inputs. We formulate soundness and completeness properties for three forms of explanations: feature importance, counterfactuals and rules. We further formalise multiple general metrics, at least one for each property and form of explanation, for quantitatively measuring satisfaction of soundness and completeness. We argue that explanations are correct in as much as various aspects of the different forms of explanations are met as quantified by those metrics. We hope that being able to assess correctness of ML model input/output behaviour explanations against formal properties and metrics is a substantial step towards fairly explaining ML-based inference.},
  archive      = {J_JAIR},
  author       = {Vandita Singh and Kristijonas Cyras and Muhammad Zain Akram and Rafia Inam},
  doi          = {10.1613/jair.1.18691},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Correct explanations and how to define them: Properties and metrics for measuring correctness of three forms of ML model Input/Output behaviour explanations},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph collaborative filtering model combining time factor and attention mechanism. <em>JAIR</em>, <em>84</em>. (<a href='https://doi.org/10.1613/jair.1.18696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the triumph of deep learning, attention mechanism, and graph convolutional networks in their respective fields, using new representation learning techniques or introducing auxiliary information to improve the representation ability of embedding has become the core content of the recommendation algorithm research. Generally, most existing GNN-based recommendation methods recursively propagate embedding information on the graph structure and capture collaborative signals by exploring the high-level connectivity between users and items. Despite the great success, those methods do not consider the influence of temporal context on user preferences embedding information propagation, nor do they distinguish the contribution of different neighbor node information to the target node. In order to address the two problems, we propose a graph collaborative filtering model TAGCF combing time factors and attention based on the existing method. The model uses the time factor to integrate temporal information into the process of embedding information propagation and uses the attention mechanism to distinguish the influence of embedding information from different neighbors. The effectiveness of TAGCF, time information, and attention mechanism are verified through comparative experiments with multiple baseline methods on the two recommendation system datasets, MovieLens and Amazon-books.},
  archive      = {J_JAIR},
  author       = {Xianglin Zuo and Xin He and Tianhao Jia and Ying Wang},
  doi          = {10.1613/jair.1.18696},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {9},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Graph collaborative filtering model combining time factor and attention mechanism},
  volume       = {84},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unifying framework for causal modeling with infinitely many variables. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.15612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural-equations models (SEMs) are perhaps the most commonly used framework for modeling causality, but they do not capture all domains of interest. For example, dynamical systems that evolve in continuous time are an important class of domains that are not (naturally) captured by SEMs. A wide variety of approaches have been proposed to fill the gap, including dynamical structural causal models (Bongers, Blom and Mooij 2018), causal constraints models (Blom, Bongers and Mooij 2019), and counterfactual resimulation (Laurent, Yang, and Fontana 2018). These models complement common-sense causal interpretations of specific dynamical systems, such as systems of ODEs. All these approaches look quite different from each other and from SEMs. They are hard to compare, and concepts developed for one approach may not make sense for another. But they are capturing the same notion of causality as SEMs do, in the sense that interventions map to outcomes. We propose a class of models that are, in a certain natural sense, the most expressive generalization of SEMs. Our generalized SEMs (GSEMs) can be viewed as a unifying framework that recovers structural dynamical causal models, causal constraints models, counterfactual resimulation, and common-sense causal interpretations of systems of ODEs and hybrid automata (Alur et al. 1992) as special cases. The input-output behavior, or “interface”, of GSEMs is exactly that of SEMs, which means that definitions of concepts like actual cause, responsibility, blame, and explanation, can be immediately lifted from SEMs to GSEMs. The generality of GSEMs also makes them ideally suited to studying causality in the abstract; for example, they have been used to establish independence relationships among Halpern’s axioms for SEMs (Peters and Halpern 2022).},
  archive      = {J_JAIR},
  author       = {Spencer Peters and Joseph Halpern},
  doi          = {10.1613/jair.1.15612},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A unifying framework for causal modeling with infinitely many variables},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-leader congestion games with an adversary. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.15768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a multi-leader single-follower congestion game where multiple users choose subsets out of a given set of resources and an adversary attacks the resources with maximum loads, causing additional costs for the users. We first show that the resulting strategic game among the leaders admits a pure Nash equilibrium if the users’ strategy-spaces are given by matroids and the resource costs are linear and identical. However, equilibria may fail to exist already when strategy spaces are not matroids or the linear cost coefficients are not identical. We therefore consider approximate equilibria for the case that each user chooses one resource and the resource costs are linear but non-identical. Our main result establishes that K ≈ 1.1974 is the smallest possible factor such that the existence of a K-approximate equilibrium is guaranteed for all instances of the game. We also provide a polynomial time algorithm for computing an α-approximate equilibrium with the smallest possible factor α ≤ K for a given instance, in particular finding an exact equilibrium if one exists.},
  archive      = {J_JAIR},
  author       = {Tobias Harks and Mona Henle and Max Klimm and Jannik Matuschke and Anja Schedel},
  doi          = {10.1613/jair.1.15768},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Multi-leader congestion games with an adversary},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing relevance and diversity in online matching markets: A time-adaptive attenuation approach. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.16635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world online matching markets (OMMs) often involve multiple objectives, such as maximizing relevance and diversity in online recommendation and crowdsourcing systems. In this paper, we propose a generic bi-objective maximization model for OMMs with the following features: (1) there are two types of agents—offline and online—with online agents arriving dynamically and stochastically; (2) upon each online agent’s arrival, an immediate and irrevocable decision must be made regarding which subset of relevant offline agents to assign; and (3) each offline and online agent has a specific matching capacity, i.e., an upper bound on the number of allowable matchings. Our model supports two general linear objective functions defined over all possible assignments to online agents. We formulate a bi-objective linear program (LP) and design an LP-based parameterized algorithm. Departing from prevalent non-adaptive attenuation methods, we introduce a time-adaptive attenuation framework that achieves an almost tight competitive ratio for each objective. To complement our theoretical analysis, we implement the proposed algorithm and evaluate it against several heuristics using two real-world datasets. Extensive experimental results demonstrate the flexibility and effectiveness of our approach, validating our theoretical predictions.},
  archive      = {J_JAIR},
  author       = {Evan Yifan Xu and Pan Xu},
  doi          = {10.1613/jair.1.16635},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Optimizing relevance and diversity in online matching markets: A time-adaptive attenuation approach},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On computing probabilistic explanations for decision trees. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formal XAI (explainable AI) is a growing area that focuses on computing explanations with mathematical guarantees for the decisions made by ML models. Inside formal XAI, one of the most studied cases is that of explaining the choices taken by decision trees, as they are traditionally deemed as one of the most interpretable classes of models. Recent work has focused on studying the computation of sufficient reasons, a kind of explanation in which given a decision tree T and an instance x, one explains the decision T (x) by providing a subset y of the features of x such that for any other instance z compatible with y, it holds that T (z) = T (x), intuitively meaning that the features in y are already enough to fully justify the classification of x by T. It has been argued, however, that sufficient reasons constitute a restrictive notion of explanation. For such a reason, the community has started to study their probabilistic counterpart, in which one requires that the probability of T (z) = T (x) must be at least some value δ ∈ (0, 1], where z is a random instance that is compatible with y. Our paper settles the computational complexity of δ-sufficient-reasons over decision trees, showing that both (1) finding δ-sufficient-reasons that are minimal in size, and (2) finding δ-sufficient-reasons that are minimal inclusion-wise, are computationally intractable. By doing this, we answer two open problems originally raised by Izza et al. (2021), and extend the hardness of explanations for Boolean circuits presented by Wäldchen et al. (2021) to the more restricted case of decision trees. Furthermore, we present sharp non-approximability results under a widely believed complexity hypothesis. On the positive side, we identify structural restrictions of decision trees that make the problem tractable.},
  archive      = {J_JAIR},
  author       = {Marcelo Arenas and Pablo Barcelo and Alexander Kozachinskiy and Miguel Romero and Bernardo Subercaseaux},
  doi          = {10.1613/jair.1.17494},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On computing probabilistic explanations for decision trees},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on data selection for LLM instruction tuning. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instruction tuning is a vital step of training large language models (LLMs), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLMs. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances, and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.},
  archive      = {J_JAIR},
  author       = {Bolin Zhang and Jiahao Wang and Qianlong Du and Jiajun Zhang and Zhiying Tu and Dianhui Chu},
  doi          = {10.1613/jair.1.17625},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A survey on data selection for LLM instruction tuning},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A complexity-theoretic analysis of majority illusion in social networks. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Majority illusion occurs in a social network when the majority of the network vertices belong to a certain type but the majority of each vertex's neighbours belong to a different type, therefore creating the wrong perception, i.e., the illusion, that the majority type is different from the actual one. From a system engineering point of view, this motivates the search for algorithms to detect and, where possible, correct this often undesirable phenomenon. In this we provide a computational study of majority illusion in social networks, paying particular attention to the problem of its verification, i.e., whether majority illusion can occur on social networks, and elimination, i.e., how can we eliminate majority illusion by social network rewiring. While we show that the problems we consider are generally NP-complete, we also provide a parameterised complexity analysis, showing FPT-algorithms for the detection problem and W[1]-hardness for the elimination problem, using natural graph-theoretic parameters.},
  archive      = {J_JAIR},
  author       = {Umberto Grandi and Lawqueen Kanesh and Grzegorz Lisowski and M.S. Ramanujan and Paolo Turrini},
  doi          = {10.1613/jair.1.17741},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A complexity-theoretic analysis of majority illusion in social networks},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CluMo: Cluster-based modality fusion prompt for continual learning in visual question answering. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large vision-language models (VLMs) have shown significant performance boost in various application domains. However, adopting them to deal with several sequentially encountered tasks has been challenging because finetuning a VLM on a task normally leads to reducing its generalization power and the capacity of learning new tasks as well as causing catastrophic forgetting on previously learned tasks. Enabling using VLMs in multimodal continual learning (CL) settings can help to address such scenarios. To improve generalization capacity and prevent catastrophic forgetting, we propose a novel prompt-based CL method for VLMs, namely Cluster-based Modality Fusion Prompt (CluMo). We design a novel Key-Key-Prompt pair, where each prompt is associated with a visual prompt key and a textual prompt key. We adopt a two-stage training strategy. During the first stage, the single-modal keys are trained via K-means clustering algorithm to help select the best semantically matched prompt. During the second stage, the prompt keys are frozen, the selected prompt is attached to the input for training the VLM in the CL scenario. Experiments on two benchmarks demonstrate that our method achieves SOTA performance. The code is publicly available here.},
  archive      = {J_JAIR},
  author       = {Yuliang Cai and Mohammad Rostami},
  doi          = {10.1613/jair.1.17940},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {CluMo: Cluster-based modality fusion prompt for continual learning in visual question answering},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Policy-based primal-dual methods for concave CMDP with variance reduction. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study Concave Constrained Markov Decision Processes (Concave CMDPs) where both the objective and constraints are defined as concave functions of the state-action occupancy measure. We propose the Variance-Reduced Primal-Dual Policy Gradient Algorithm (VR-PDPG), which updates the primal variable via policy gradient ascent and the dual variable via projected sub-gradient descent. Despite the challenges posed by the loss of additivity structure and the nonconcave nature of the problem, we establish the global convergence of VR-PDPG by exploiting a form of hidden concavity. In the exact setting, we prove an O(T-1/3) convergence rate for both the average optimality gap and constraint violation, which further improves to O(T-1/2) under strong concavity of the objective in the occupancy measure. In the sample-based setting, we demonstrate that VR-PDPG achieves an O(ε-4) sample complexity for ε-global optimality. Moreover, by incorporating a diminishing pessimistic term into the constraint, we show that VR-PDPG can attain a zero constraint violation without compromising the convergence rate of the optimality gap. Finally, we validate our methods through numerical experiments.},
  archive      = {J_JAIR},
  author       = {Donghao Ying and Mengzi Amy Guo and Hyunin Lee and Yuhao Ding and Javad Lavaei and Zuo-Jun Max Shen},
  doi          = {10.1613/jair.1.18129},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Policy-based primal-dual methods for concave CMDP with variance reduction},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving and understanding the power of satisfaction-driven clause learning. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explain how to improve Satisfaction-Driven Clause Learning (SDCL) SAT solvers by using a MaxSAT-based technique that enables them to learn shorter, and hence better, redundant clauses. A thorough empirical evaluation of an implementation on the MapleSAT solver shows that the resulting system solves Mutilated Chess Board (MCB) problems significantly faster than CDCL solvers, without requiring any alteration to the branching heuristic used by the underlying CDCL SAT solver. Additionally we improve the understanding of the power of these solvers by proving that, given a refutation of a formula that consists of resolution and redundant-clause addition steps, an SDCL solver is able to produce a proof whose size is polynomial with respect to the size of the original refutation.},
  archive      = {J_JAIR},
  author       = {Albert Oliveras and Chunxiao Li and Darryl Wu and Jonathan Chung and Vijay Ganesh},
  doi          = {10.1613/jair.1.18286},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Improving and understanding the power of satisfaction-driven clause learning},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Principles for assumptions generation in enthymeme-based dialogue. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In enthymeme-based dialogues, involved participants create assumptions in order to decode arguments from the exchanged enthymemes. This work introduces the concept of assumptions operator, which formalizes the mechanism for generating these assumptions, and proposes a set of principles to guide the construction of these operators. Said principles are inspired by Grice’s Maxims of Conversation, as well as Govier’s ARG conditions for cogent arguments. Then, in order to analyze how the used operator influences the dialogue and how that dialogue differs from the one in which the original argument is sent, we propose a framework to compare both scenarios, the former being the enthymemic one and the latter the complete one. Finally, we formally show that if the used assumptions operator complies with a set of the aforementioned principles, then most arguments in the complete dialogue have their counterpart in the enthymemic one. Furthermore, we show that under certain conditions, the enthymemic dialogue preserves some semantic properties from the complete one, specifically: conflict-freeness, acceptability and admissibility.},
  archive      = {J_JAIR},
  author       = {Diego S. Orbe Leiva and Alejandro J. Garcia and Sebastian Gottifredi},
  doi          = {10.1613/jair.1.18395},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Principles for assumptions generation in enthymeme-based dialogue},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Argumentative reasoning in ASPIC+ under incomplete information. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning under incomplete information is an important research direction in the study of computational argumentation. Most advances in this direction so far have focused on abstract argumentation frameworks. In particular, development of computational approaches to reasoning under incomplete information in structured formalisms remains to a large extent a challenge. We address this challenge by studying the problems of determining stability and relevance—with the aim of analyzing aspects of resilience of acceptance statuses in light of new information—in the central structured formalism of ASPIC+.  The specific ASPIC+ instantiation and grounded argumentation semantics we focus on are motivated by current applications in criminal investigation at the Netherlands Police. Our contributions consist of a theoretical analysis of the complexity of deciding stability and relevance as well as first exact algorithms for reasoning about stability and relevance in incomplete ASPIC+ theories. In terms of complexity results, we show that deciding stability is coNP-complete for incomplete ASPIC+ when assuming a preference ordering on defeasible rules via the last-link ordering, while deciding relevance is significantly more complex, namely NP^NP-complete. Complementing the complexity results, we develop practical algorithms for deciding stability and relevance based on the declarative paradigm of answer set programming (ASP). Furthermore, we provide an open-source implementation of the algorithms, and show empirically that the implementation exhibits promising scalability on both real-world and synthetic data. Our exact approach to stability is competitive with a previously proposed inexact approach, and the run times of our algorithms for both stability and relevance are sufficiently low on real-world data to be used in online settings.},
  archive      = {J_JAIR},
  author       = {Daphne Odekerken and Tuomo Lehtonen and Johannes P. Wallner and Matti Järvisalo},
  doi          = {10.1613/jair.1.18404},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Argumentative reasoning in ASPIC+ under incomplete information},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental learning methodologies for addressing catastrophic forgetting: Analysis and experimental evaluation. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks have been reported to exhibit, and in some cases surpass, human level performance on individual rigid tasks. However, these networks remain static entities of knowledge for those specific tasks, which can lead to catastrophic forgetting ---i.e., forgetting old tasks--- when attempting to learn new tasks. The main objective of Incremental Learning (IL) is to address this issue. In order to tackle catastrophic forgetting, various approaches have been proposed so far. We survey those approaches and organize them in nine categories: regularization-based methods, exemplar replay-based methods, variational continual learning-based methods, parameter isolation-based methods, dynamic architectures-based methods, distillation-based methods, generative methods, data-free methods and unsupervised methods. Moreover, this review distinguishes between two scenarios, Task Incremental Learning (Task-IL) and Class Incremental Learning (Class-IL), and reports on the results obtained for a number of experiments that compare the performance achieved by a selection of diverse methods for both scenarios on the datasets most used by the related research community.},
  archive      = {J_JAIR},
  author       = {Miquel Serra-Perello and Alberto Ortiz},
  doi          = {10.1613/jair.1.18405},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Incremental learning methodologies for addressing catastrophic forgetting: Analysis and experimental evaluation},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recursive decomposition of logical thoughts: Framework for superior reasoning and knowledge propagation in large language models. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Large Language Models often struggle with multi-step reasoning due to cascading errors, rigid prompt structures, and underutilized intermediate reasoning steps. While prompting strategies such as Chain-of-Thought , CoT with Self-Consistency, and Least-to-Most offer partial improvements, they typically lack mechanisms for feedback-driven learning or structured reuse of prior thought sequences. Objectives: This work introduces Recursive Decomposition of Logical Thoughts (RDoLT), a cognitively inspired prompting framework that enhances LLM reasoning through hierarchical decomposition, multi-feature scoring, and knowledge propagation. The framework aims to overcome linear reasoning limitations by enabling structured, memory-aware exploration of thought spaces. Methods: RDoLT executes a three-stage iterative reasoning process across Easy, Intermediate, and Final tiers. At each level, multiple candidate thoughts are generated and scored on Logical Validity, Coherence, Simplicity, and Adaptiveness. The Knowledge Propagation Module (KPM) persistently tracks both selected and rejected thoughts, allowing future reasoning stages to reuse contextually relevant but previously discarded knowledge. The framework supports adaptive thresholding, controlled reasoning depth, and edge-case regeneration through structured feedback loops. Results: Extensive evaluation across five reasoning benchmarks demonstrates that RDoLT outperforms the most competitive prompting strategies in both accuracy and stability. On GSM8K, RDoLT achieves 90.98% accuracy with ChatGPT-4o, surpassing CoT-SC (89.4%) and ReAct (90.5%). It improves Gemma 2 (27B) performance on SVAMP from 69.86% (Vanilla) to 75.27%, and on MultiArith from 67.96% (Vanilla) to 72.49%. Across all benchmarks, RDoLT outperforms or matches the strongest baseline in over 60% of settings, highlighting its robustness across diverse reasoning tasks and model scales. Ablation studies reveal that generating three thoughts per stage yields the best trade-off between performance and efficiency, while the Knowledge Propagation Module (KPM) consistently reduces reasoning variance by leveraging both accepted and discarded thoughts across stages. Conclusions: RDoLT presents a scalable reasoning paradigm grounded in cognitive principles. Its integration of hierarchical decomposition, structured scoring, and selective memory propagation enables more reliable and adaptive reasoning in LLMs. These results establish RDoLT as a robust prompt engineering framework with broad applicability, and future work will focus on optimizing token efficiency and extending to domain-specific use cases.},
  archive      = {J_JAIR},
  author       = {Kaleem Ullah Qasim and Jiashu Zhang and Tariq Alsahfi and Ateeq Ur Rehman Butt},
  doi          = {10.1613/jair.1.18562},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Recursive decomposition of logical thoughts: Framework for superior reasoning and knowledge propagation in large language models},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning how to vote with principles: Axiomatic insights into the collective decisions of neural networks. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can neural networks be applied in voting theory, while satisfying the need for transparency in collective decisions? We propose axiomatic deep voting: a framework to build and evaluate neural networks that aggregate preferences, using the well-established axiomatic method of voting theory. Our findings are: (1) Neural networks, despite being highly accurate, often fail to align with the core axioms of voting rules, revealing a disconnect between mimicking outcomes and reasoning. (2) Training with axiom-specific data does not enhance alignment with those axioms. (3) By solely optimizing axiom satisfaction, neural networks can synthesize new voting rules that often surpass and substantially differ from existing ones. This offers insights for both fields: For AI, important concepts like bias and value-alignment are studied in a mathematically rigorous way; for voting theory, new areas of the space of voting rules are explored.},
  archive      = {J_JAIR},
  author       = {Levin Hornischer and Zoi Terzopoulou},
  doi          = {10.1613/jair.1.18890},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Learning how to vote with principles: Axiomatic insights into the collective decisions of neural networks},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MA-LAMA: Exploiting the multi-agent nature of temporal planning problems. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: In the field of Automated Planning, the existence of multiple agents in a certain temporal setting introduces the possibility of concurrency. This severely increases the complexity of planning in multi-agent temporal scenarios, as the possible states in the search space grow exponentially. Objectives: These types of domains are traditionally solved by making use of temporal reasoning techniques that do not directly address the “multi-agent nature” at their core. In contrast, we introduce MA-LAMA, a multi-agent temporal planner that makes use of multi-agent techniques to deal with the inherent complexity of multi-agent temporal scenarios. Methods: We propose a sequenced framework in which several multi-agent planning techniques are applied: automatic agent detection, task decomposition, cost-informed goal assignment, and agent interaction analysis; that are aimed to reduce the search complexity when solving temporal planning tasks. Results: Our results show that, in many cases, MA-LAMA outperforms other state-of-the-art temporal planners in plan cost optimization for well-known temporal domains. Conclusions: These results, along with the fact that MA-LAMA does not incorporate any temporal reasoning during search, suggest that several widely considered temporal domains are best suited to be solved with multi-agent planning techniques, rather than with temporal reasoning.},
  archive      = {J_JAIR},
  author       = {J. Caballero Testón and Maria D. R-Moreno},
  doi          = {10.1613/jair.1.18906},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {MA-LAMA: Exploiting the multi-agent nature of temporal planning problems},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An MRP formulation for supervised learning: Generalized temporal difference learning models. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.19171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Traditional supervised learning (SL) assumes data points are independently and identically distributed (i.i.d.), which overlooks dependencies in real-world data. Reinforcement learning (RL), in contrast, models dependencies through state transitions. Objectives: This study aims to bridge SL and RL by reformulating SL problems as RL tasks, enabling the application of RL techniques to a wider range of SL scenarios. We aim to model SL data as interconnected, and develop novel temporal difference (TD) algorithms that can accommodate diverse data types. Our objectives are to (1) establish conditions where TD outperforms ordinary least squares (OLS), (2) provide convergence guarantees for the generalized TD algorithm, and (3) validate the approach empirically using synthetic and real-world datasets. Methods: We reformulate traditional SL as a RL problem by modeling data points as a Markov Reward Process (MRP). We then introduce a concept analogous to the inverse link function in generalized linear models, allowing our TD algorithm to handle various data types. Our analysis, grounded in variance estimation, identifies conditions where TD outperforms OLS. We establish a convergence guarantee by conceptualizing the TD update rule as a generalized Bellman operator. Empirical validation begins with synthetic data progressively matching theoretical assumptions to verify our analysis, followed by evaluations on real-world datasets to demonstrate practical utility. Results: Our theoretical analysis shows that TD can outperform OLS in estimation accuracy when data noise is correlated. Our approach generalizes across various loss functions and SL datasets. We prove that the Bellman operator in our TD framework is a contraction, ensuring convergence for both expected and stochastic TD updates. Empirically, TD outperforms SL baselines when data aligns with its assumptions, remains competitive across diverse datasets, and is robust to hyperparameter choices. Conclusions: This study demonstrates that SL can be reformulated as a problem of interconnected data modeled by an MRP, effectively solved using TD learning. Our generalized TD is theoretically sound, with convergence guarantees, and practically effective. It generalizes OLS, offering superior performance on correlated data. This work enables RL techniques to benefit SL tasks, offering a pathway for future advancements.},
  archive      = {J_JAIR},
  author       = {Yangchen Pan and Junfeng Wen and Chenjun Xiao and Philip H. S. Torr},
  doi          = {10.1613/jair.1.19171},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {An MRP formulation for supervised learning: Generalized temporal difference learning models},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards an ontology-driven approach to document bias. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.19388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML)-powered systems are capable of reproducing and often amplifying undesired biases embedded in society, emphasizing the importance of operating under practices that enable the study and understanding of the intrinsic characteristics of ML pipelines. This supports the emergence of documentation frameworks with the idea that “any remedy for bias starts with awareness of its existence.” However, a resource that can formally describe ML pipelines in terms of detected biases is still missing. To address this gap, we present the Doc-BiasO ontology, a resource that sets out to create an integrated vocabulary of biases defined in the Trustworthy AI literature and their measures, as well as to incorporate relevant domain terminology and relationships between them. Overseeing ontology engineering best practices, we reuse existing vocabularies on machine learning and AI to foster knowledge sharing and interoperability between the actors concerned with its research, development, regulation, and others. In addition, we demonstrate the potential of Doc-BiasO with an experiment on an existing benchmark and as part of a neuro-symbolic system. Overall, our main objective is to contribute towards clarifying existing terminology on bias research as it rapidly expands to all areas of AI and to improve the interpretation of bias in data and downstream impact through its documentation.},
  archive      = {J_JAIR},
  author       = {Mayra Russo and Maria-Esther Vidal},
  doi          = {10.1613/jair.1.19388},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {8},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Towards an ontology-driven approach to document bias},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved regret bounds for bandits with expert advice. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.16738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research note, we revisit the bandits with expert advice problem. Under a restricted feedback model, we prove a lower bound of order [KT ln(N/K)]1/2 for the worst-case regret, where K is the number of actions, N &gt; K the number of experts, and T the time horizon. This matches a previously known upper bound of the same order and improves upon the best available lower bound of [KT ln(N)/ln(K)]1/2.  For the standard feedback model, we prove a new instance-based upper bound that depends on the agreement between the experts and provides a logarithmic improvement compared to prior results.},
  archive      = {J_JAIR},
  author       = {Nicolò Cesa-Bianchi and Khaled Eldowa and Emmanuel Esposito and Julia Olkhovskaya},
  doi          = {10.1613/jair.1.16738},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Improved regret bounds for bandits with expert advice},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameterized analysis of bribery in challenge the champ tournaments. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Challenge the champ tournaments are one of the simplest forms of competition, where a (initially selected) champ is repeatedly challenged by other players. If a player beats the champ, then that player is considered the new (current) champ. Each player in the competition challenges the current champ once in a fixed order. The champ of the last round is considered the winner of the tournament. We investigate a setting where players can be bribed to lower their winning probability against the initial champ. The goal is to maximize the probability of the initial champ winning the tournament by bribing the other players, while not exceeding a given budget for the bribes. Mattei et al. [Journal of Applied Logic, 2015] showed that the problem can be solved in pseudo-polynomial time, and that it is in XP when parameterized by the number of players. We show that the problem is weakly NP-hard and W[1]-hard when parameterized by the number of players. On the algorithmic side, we show that the problem is fixed-parameter tractable when parameterized either by the number of different bribe values or the number of different probability values. To this end, we establish several results that are of independent interest. In particular, we show that the product knapsack problem is W[1]-hard when parameterized by the number of items in the knapsack, and that constructive bribery for cup tournaments is W[1]-hard when parameterized by the number of players. Furthermore, we present a novel way of designing mixed integer linear programs, ensuring optimal solutions where all variables are integers.},
  archive      = {J_JAIR},
  author       = {Juhi Chaudhary and Hendrik Molter and Meirav Zehavi},
  doi          = {10.1613/jair.1.17239},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Parameterized analysis of bribery in challenge the champ tournaments},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainability via short formulas: The case of propositional logic with implementation. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conceptualize explainability in terms of logic and formula size, giving a number of related definitions of explainability in a very general setting. Our main interest is the so-called local explanation problem which aims to explain the truth value of an input formula in an input model. The explanation is a formula of minimal size that (1) obtains the same truth value as the input formula on the input model and (2) transmits that truth value to the input formula globally, i.e., on every model. As an important example case, we study propositional logic in this setting and show that the local explainability problem is complete for the second level of the polynomial hierarchy. The hardness result holds already for DNF-formulas. We also give parameterized versions of these problems leading to NP-completeness. The generality of our definitions allows us to lift complexity results also, e.g., to S5 modal logic and ensembles of decision trees. We also provide an implementation in answer set programming and investigate its capacity in relation to explaining answers to the n-queens and dominating set problems. Furthermore, we give an example of explaining the behavior of a black-box classifier.},
  archive      = {J_JAIR},
  author       = {Reijo Jaakkola and Tomi Janhunen and Antti Kuusisto and Masood Feyzbakhsh Rankooh and Miikka Vilander},
  doi          = {10.1613/jair.1.17422},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Explainability via short formulas: The case of propositional logic with implementation},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collective intelligence in decision-making with non-stationary experts. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.16228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When sufficient experience to make informed decisions is unavailable, expert advice can help us navigate uncertainty. As expertise evolves, driven by continuous learning in human experts or model updates in artificial experts, it is crucial to adopt adaptive approaches. Existing methods for exploiting non-stationary experts focus on competing with the single best expert. In contrast, this work harnesses the power of collective intelligence to facilitate better decision-making in the face of evolving expertise or dynamic environments. To achieve this, we propose the novel CORVAL approach which optimally combines the insights of multiple experts. By adapting to drifts in expertise, our novel approach can surpass the performance of the single best expert as well as previous approaches. Empirical evaluations on a diverse range of non-stationary problems, including active learning applications, showcase the improved performance of our approach in collective decision-making scenarios.},
  archive      = {J_JAIR},
  author       = {Axel Abels and Vito Trianni and Ann Nowé and Tom Lenaerts},
  doi          = {10.1613/jair.1.16228},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Collective intelligence in decision-making with non-stationary experts},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On CNF conversion for SAT and SMT enumeration. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.16870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern SAT and SMT solvers are designed to handle problems expressed in Conjunctive Normal Form (CNF) so that non-CNF problems must be CNF-ized upfront, typically by using variants of either Tseitin or Plaisted and Greenbaum transformations. When passing from plain solving to enumeration, however, the capability of producing partial satisfying assignments that are as small as possible becomes crucial, which raises the question of whether such CNF encodings are also effective for enumeration. In this paper, we investigate both theoretically and empirically the effectiveness of CNF conversions for SAT and SMT enumeration. On the negative side, we show that: (i) Tseitin transformation prevents the solver from producing short partial assignments, thus seriously affecting the effectiveness of enumeration; (ii) Plaisted and Greenbaum transformation overcomes this problem only in part. On the positive side, we prove theoretically and we show empirically that combining Plaisted and Greenbaum transformation with NNF preprocessing upfront —which is typically not used in solving— can fully overcome the problem and can drastically reduce both the number of partial assignments and the execution time.},
  archive      = {J_JAIR},
  author       = {Gabriele Masina and Giuseppe Spallitta and Roberto Sebastiani},
  doi          = {10.1613/jair.1.16870},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On CNF conversion for SAT and SMT enumeration},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invariant graph propagation in constraint-based local search. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In constraint-based local search, an assignment to the search variables is improved upon by an iterative procedure that replaces the current assignment with a similar assignment. The latter is selected by a heuristic that assesses the qualities of a subset of all similar assignments, where the quality of such assignments is determined via a process called invariant graph propagation. Since, typically, many similar assignments are considered in every iteration, invariant graph propagation must be as efficient as possible. Since invariant graph propagation is independent of the selection heuristic, any comparison between different invariant graph propagation styles under different selection heuristics can be misleading. In this paper, we describe and compare both theoretically and empirically the throughput of several invariant graph propagation styles, and give criteria when one style or another is to be used.},
  archive      = {J_JAIR},
  author       = {Frej Knutar Lewander and Pierre Flener and Justin Pearson},
  doi          = {10.1613/jair.1.17482},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Invariant graph propagation in constraint-based local search},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-causal patterns of sample growth. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.15675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different statistical samples (e.g., from different locations) offer populations and learning systems observations with distinct statistical properties. Samples under (1) ’Unconfounded’ growth preserve systems’ ability to determine their variables’ effects on outcomes-of-interest (and lead, therefore, to interpretable black-box predictions). Samples under (2) ’Externally-Valid’ growth preserve their ability to make predictions that generalize across out-of-sample variation. The first generates predictions that generalize over sample populations, the second over their common unobserved factors. We illustrate these theoretic patterns in the full American census from 1840 to 1940, and samples ranging from the street-level all the way to the national. This reveals new conditions for the generalizability of samples over space and time, and connections among the Shapley value, counterfactual statistics, and hyperbolic geometry.},
  archive      = {J_JAIR},
  author       = {Andre F. Ribeiro},
  doi          = {10.1613/jair.1.15675},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Spatio-causal patterns of sample growth},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving local search algorithm for pseudo boolean optimization. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.16626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo-Boolean optimization (PBO) is usually used to model combinatorial optimization problems, especially for some real-world applications. Despite its significant importance in both theory and applications, the performance of current PBO solvers is still limited. This paper develops a novel local search algorithm for PBO, which has four main ideas. First, we design a new primary scoring function and a two-level selection strategy to evaluate all candidate variables. Second, we introduce a new weighting scheme to accurately guide the search process toward more promising directions. Third, we propose a novel deep optimization strategy to disturb some search processes. Fourth, an efficient solution space exploration mechanism is applied to help the algorithm jump out of local optimum. We conduct experiments on a broad range of public benchmarks, including three large-scale practical application benchmarks, two benchmarks from PB competitions, an integer linear programming optimization benchmark, a crafted combinatorial benchmark, and a combinatorial optimization knapsack benchmark to compare our proposed algorithm against twelve state-of-the-art competitors, including seven recently-proposed pure stochastic local search PBO solvers, a non-traditional stochastic local search combined with complete oracle, two complete PB solvers, and two mixed integer programming (MIP) solvers. Our proposed algorithm has been shown to perform best on these three real-world benchmarks. On the other five benchmarks, our algorithm shows competitive performance compared to state-of-the-art competitors, and it significantly outperforms all other local search algorithms, indicating that our algorithm greatly advances the state of the art in local search for solving PBO.},
  archive      = {J_JAIR},
  author       = {Yujiao Zhao and Yiyuan Wang and Yi Chu and Wenbo Zhou and Shaowei Cai and Minghao Yin},
  doi          = {10.1613/jair.1.16626},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Improving local search algorithm for pseudo boolean optimization},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometrically inspired kernel machines for collaborative learning beyond gradient descent. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.16821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a novel mathematical framework for collaborative learning by means of geometrically inspired kernel machines which includes statements on the bounds of generalisation and approximation errors, and sample complexity. For classification problems, this approach allows us to learn bounded geometric structures around given data points and hence solve the global model learning problem in an efficient way by exploiting convexity properties of the related optimisation problem in a Reproducing Kernel Hilbert Space (RKHS). In this way, we can reduce classification problems to determining the closest bounded geometric structure from a given data point. Further advantages that come with our solution is that our approach does not require clients to perform multiple epochs of local optimisation using stochastic gradient descent, nor require rounds of communication between client/server for optimising the global model. We highlight that numerous experiments have shown that the proposed method is a competitive alternative to the state-of-the-art.},
  archive      = {J_JAIR},
  author       = {Mohit Kumar and Alexander Valentinitsch and Magdalena Fuchs and Mathias Brucker and Juliana Bowles and Adnan Husakovic and Ali Abbas and Bernhard A. Moser},
  doi          = {10.1613/jair.1.16821},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Geometrically inspired kernel machines for collaborative learning beyond gradient descent},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable synthesis of formally verified neural value function for hamilton-jacobi reachability analysis. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.16946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hamilton-Jacobi (HJ) reachability analysis provides a formal method for guaranteeing safety in constrained control problems. It synthesizes a value function to represent a long-term safe set called feasible region. Early synthesis methods based on state space discretization cannot scale to high-dimensional problems, while recent methods that use neural networks to approximate value functions result in unverifiable feasible regions. To achieve both scalability and verifiability, we propose a framework for synthesizing verified neural value functions for HJ reachability analysis. Our framework consists of three stages: pre-training, adversarial training, and verification-guided training. We design three techniques to address three challenges to improve scalability respectively: boundary-guided backtracking (BGB) to improve counterexample search efficiency, entering state regularization (ESR) to enlarge feasible region, and activation pattern alignment (APA) to accelerate neural network verification. We also provide a neural safety certificate synthesis and verification benchmark called Cersyve-9, which includes nine commonly used safe control tasks and supplements existing neural network verification benchmarks. Our framework successfully synthesizes verified neural value functions on all tasks, and our proposed three techniques exhibit superior scalability and efficiency compared with existing methods.},
  archive      = {J_JAIR},
  author       = {Yujie Yang and Hanjiang Hu and Tianhao Wei and Shengbo Eben Li and Changliu Liu},
  doi          = {10.1613/jair.1.16946},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Scalable synthesis of formally verified neural value function for hamilton-jacobi reachability analysis},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LabelCoRank: Revolutionizing long tail multi-label classification with co-occurrence reranking. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.16968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent advancements in semantic representation driven by pre-trained and large-scale language models, addressing long tail challenges in multi-label text classification remains a significant issue. Long tail challenges have persistently posed difficulties in accurately classifying less frequent labels. Current approaches often focus on improving text semantics while neglecting the crucial role of label relationships. This paper introduces LabelCoRank, a novel approach inspired by ranking principles. LabelCoRank leverages label co-occurrence relationships to refine initial label classifications through a dual-stage reranking process. The first stage uses initial classification results to form a preliminary ranking. In the second stage, a label co-occurrence matrix is utilized to rerank the preliminary results, enhancing the accuracy and relevance of the final classifications. By integrating the reranked label representations as additional text features, LabelCoRank effectively mitigates long tail issues in multi-label text classification. Experimental evaluations on popular datasets including MAG-CS, PubMed, and AAPD demonstrate the effectiveness and robustness of LabelCoRank. The implementation code is publicly available on https://github.com/821code/LabelCoRank.},
  archive      = {J_JAIR},
  author       = {Yan Yan and Junyuan Liu and Bo-Wen Zhang},
  doi          = {10.1613/jair.1.16968},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {LabelCoRank: Revolutionizing long tail multi-label classification with co-occurrence reranking},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Super level sets and exponential decay: A synergistic approach to stable neural network training. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a theoretically grounded optimization framework for neural network training that integrates an Exponentially Decaying Learning Rate with Lyapunov-based stability analysis. We develop a dynamic learning rate algorithm and prove that it induces connected and stable descent paths through the loss landscape by maintaining the connectivity of super-level sets Sλ = {θ ∈  ℝn : ℒ(θ) ≥ λ}.  Under the condition that the Lyapunov function V(θ) = ℒ(θ) satisfies  Δ V(θ) ⋅ Δ ℒ(θ) ≥ 0, we establish that these super-level sets are not only connected but also equiconnected across epochs, providing uniform topological stability.  We further derive convergence guarantees using a second-order Taylor expansion and demonstrate that our exponentially scheduled learning rate with gradient-based modulation leads to a monotonic decrease in loss. The proposed algorithm incorporates this schedule into a stability-aware update mechanism that adapts step sizes based on both curvature and energy-level geometry.  This work formalizes the role of topological structure in convergence dynamics and introduces a provably stable optimization algorithm for high-dimensional, non-convex neural networks.},
  archive      = {J_JAIR},
  author       = {Jatin Chaudary and Dipak Nidhi and Jukka Heikkonen and Harri Merisaari and Rajeev Kanth},
  doi          = {10.1613/jair.1.17272},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Super level sets and exponential decay: A synergistic approach to stable neural network training},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards generalist robot learning from internet video: A survey. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scaling deep learning to massive and diverse internet data has driven remarkable breakthroughs in domains such as video generation and natural language processing. Robot learning, however, has thus far failed to replicate this success and remains constrained by a scarcity of available data. Learning from Videos (LfV) methods aim to address this data bottleneck by augmenting traditional robot data with large-scale internet video. This video data provides foundational information regarding physical dynamics, behaviours, and tasks, and can be highly informative for general-purpose robots. This survey systematically examines the emerging field of LfV. We first outline essential concepts, including detailing fundamental LfV challenges such as distribution shift and missing action labels in video data. Next, we comprehensively review current methods for extracting knowledge from large-scale internet video, overcoming LfV challenges, and improving robot learning through video-informed training. The survey concludes with a critical discussion of future opportunities. Here, we emphasize the need for scalable foundation model approaches that can leverage the full range of available internet video and enhance the learning of robot policies and dynamics models. Overall, the survey aims to inform and catalyse future LfV research, driving progress towards general-purpose robots.},
  archive      = {J_JAIR},
  author       = {Robert McCarthy and Daniel C.H. Tan and Dominik Schmidt and Fernando Acero and Nathan Herr and Yilun Du and Thomas G. Thuruthel and Zhibin Li},
  doi          = {10.1613/jair.1.17400},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Towards generalist robot learning from internet video: A survey},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The search for stability: Learning dynamics of strategic publishers with initial documents. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a game-theoretic information retrieval model in which strategic publishers aim to maximize their chances of being ranked first by the search engine while maintaining the integrity of their original documents. We show that the commonly used Probability Ranking Principle (PRP) ranking scheme results in an unstable environment where games often fail to reach pure Nash equilibrium. We propose two families of ranking functions that do not adhere to the PRP. We provide both theoretical and empirical evidence that these methods lead to a stable search ecosystem, by providing positive results on the learning dynamics convergence. We also define the publishers’ and users’ welfare, demonstrate a possible publisher-user trade-off, and provide means for a search system designer to control it. Finally, we show how instability harms long-term users’ welfare.},
  archive      = {J_JAIR},
  author       = {Omer Madmon and Idan Pipano and Itamar Reinman and Moshe Tennenholtz},
  doi          = {10.1613/jair.1.17997},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The search for stability: Learning dynamics of strategic publishers with initial documents},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional relative frequency distributions with undefined observations and generalized fuzzy orthopartitions. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional relative frequency distributions are tools extensively employed in statistics and machine learning for analyzing connections of two or more categorical variables, examining patterns, and comparing data. As a first goal, we introduce the so-called conditional relative frequency distributions with undefined observations for representing frequencies characterized by uncertainty. After that, we show that conditional relative frequency distributions with undefined observations can be identified with particular generalized fuzzy orthopartitions, which are mathematical models describing vague partitions where the membership of elements to classes is only partially known.},
  archive      = {J_JAIR},
  author       = {Stefania Boffa and Davide Ciucci},
  doi          = {10.1613/jair.1.18020},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Conditional relative frequency distributions with undefined observations and generalized fuzzy orthopartitions},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal explanations for sequential decision making. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic sequential decision-making systems — such as Markov decision processes and their variants — are increasingly used in areas such as transportation, healthcare, and communication. However, the ability to explain these systems’ outputs to non-technical end users has not kept pace with their widespread adoption. This paper addresses that gap by extending prior work and presenting a unified framework for generating causal explanations of agent behavior in sequential decision-making settings, grounded in the structural causal model (SCM) paradigm. Our framework supports the generation of multiple, semantically distinct explanations for agent actions — capabilities that were previously unattainable. In addition to introducing a novel taxonomy of explanations for MDPs to guide empirical investigation, we develop both exact and approximate causal inference methods within the SCM framework. We analyze their applicability and derive run-time bounds for each. This leads to the proposed algorithm, MeanRESP, which operates flexibly across a spectrum of approximations tailored to external constraints. We further analyze the sample complexity and error rates of approximate MeanRESP, and provide a detailed comparison of its outputs—under varying definitions of responsibility—with popular Shapley-value-based methods. Empirically, we performed a series of experiments to evaluate the practicality and effectiveness of the proposed system, focusing on real-world computational demands and the validity and reliability of metrics for comparing approximate and exact causal methods. Finally, we present two user studies that reveal user preferences for certain types of explanations and demonstrate a strong preference for explanations generated by our framework compared to those from other state-of-the-art systems.},
  archive      = {J_JAIR},
  author       = {Samer B. Nashed and Saaduddin Mahmud and Claudia V. Goldman and Shlomo Zilberstein},
  doi          = {10.1613/jair.1.18126},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Causal explanations for sequential decision making},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness distributions in neural network verification. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are vulnerable to slight alterations to otherwise correctly classified inputs, leading to incorrect predictions. To rigorously assess the robustness of neural networks against such perturbations, verification techniques can be employed. Robustness is generally measured in terms of adversarial accuracy, based on an upper bound on the magnitude of perturbations commonly denoted by ε. For each input in a given set, a verifier determines whether a perturbation up to magnitude ε can deceive the network. In this work, we contribute novel analysis techniques for the verified robustness of neural networks for supervised classification problems and report on interesting findings we obtained using these techniques. We utilise the notion of robustness distributions, specifically those built using the concept of critical ε values. Critical ε values are defined as the maximum amount of perturbation for which a given input is provably correctly classified, such that any larger perturbations can cause misclassification. To effectively estimate the critical ε values for each input in a given set, we utilise a variant of the binary search algorithm. We then analyse the distributions of these critical ε values over a given set of inputs for 12 MNIST classifiers widely used in the literature on neural network verification. Using a Kolmogorov-Smirnov test, we obtain support for the hypothesis that the critical ε values of 11 of these networks follow a log-normal distribution. Furthermore, we found no statistically significant differences between the critical ε distributions for training and testing data for 12 feed-forward neural networks on the MNIST dataset. Generally, we find a strong positive correlation between the critical ε of an input image across various networks. However, in some cases, an input that is easily perturbed to deceive one network may require a considerably larger perturbation to deceive another. Furthermore, for a given input, the adversarial examples that we find differ across networks, with different predicted classes associated with them. We investigate the effect adversarial training can have on the critical ε distribution of various neural networks for MNIST, CIFAR and GTSRB datasets. We also find that complete verification is expensive for some of the CIFAR and GTSRB networks, which limits the precision of the robustness distributions we were able to obtain. Nonetheless, we observe that most of the critical ε distributions of the networks obtained through adversarial training do not follow a log-normal distribution. Furthermore, adversarial training significantly improves the critical ε distributions for testing as well as training data in most cases. Lastly, we provide a ready-to-use Python package available on GitHub that can be used for creating robustness distributions and enables others to build upon our work.},
  archive      = {J_JAIR},
  author       = {Annelot Bosman and Aaron Berger and Holger H. Hoos and Jan N. van Rijn},
  doi          = {10.1613/jair.1.18403},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {7},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Robustness distributions in neural network verification},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The fixed-point semantics of relational concept analysis. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Relational concept analysis (RCA) is an extension of formal concept analysis dealing with several related formal contexts simultaneously. It can learn description logic theories from data and has been used within various applications. However, RCA returns a single family of concept lattices, though, when the data feature circular dependencies, other solutions may be considered acceptable. The semantics of RCA, provided in an operational way, does not shed light on this issue. Objectives: This paper aims at defining precisely the semantics of RCA and identifying alternative solutions. Methods: We first characterise the acceptable solutions as those families of concept lattices which belong to the space determined by the initial contexts (well-formed), which cannot scale new attributes (saturated), and which refer only to concepts of the family (self-supported). We adopt a functional view on the RCA process by defining the space of well-formed solutions and two functions on that space: one expansive and the other contractive. In this context, the acceptable solutions are the common fixed points of both functions. Results: We show that RCA returns the least element of the set of acceptable solutions. In addition, it is possible to build dually an operation that generates its greatest element. The set of acceptable solutions is a complete sublattice of the interval between these two elements. Its structure, and how the defined functions traverse it, are studied in detail.},
  archive      = {J_JAIR},
  author       = {Jérôme Euzenat},
  doi          = {10.1613/jair.1.17882},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {6},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The fixed-point semantics of relational concept analysis},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient XAI: A low-cost data reduction approach to SHAP interpretability. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.18325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable Artificial Intelligence (XAI) has become a critical area of research, particularly in ensuring transparency and trustworthiness in machine learning (ML) models. In this context SHAP (SHapley Additive exPlanations) is widely recognized as a robust method for feature attribution, yet its computational cost poses significant challenges, especially for large datasets. This study explores a novel approach to optimizing SHAP computations by leveraging Slovin’s formula, a statistical sampling technique traditionally used in survey research. Unlike feature selection or dimensionality reduction methods, Slovin’s formula requires minimal prior knowledge of the dataset’s statistical properties while providing an efficient, heuristic-based alternative for data reduction. It offers a straightforward, low-cost sampling approach that can be applied without extensive preprocessing, making it accessible for computationally constrained environments. Through controlled experiments on synthetic datasets, we analyze the stability of SHAP values under Slovin-based subsampling across varying data characteristics, including feature and target types and distributions, and dataset sizes. Our findings reveal a U-shaped trade-off: SHAP values for midranked features remain stable, whereas extreme values exhibit higher fluctuations. Additionally, categorical and non-skewed distributed features maintain greater robustness, while highly skewed target distributions introduce variability. Importantly, the effectiveness of Slovin’s formula diminishes when the subsample-to-sample ratio falls below 5%. By integrating Slovin’s formula into SHAP workflows, we demonstrate a practical solution for balancing interpretability and computational efficiency in machine learning. This method reduces processing costs while retaining key feature attributions, making it particularly valuable for researchers and practitioners working with resource-constrained environments. Our study contributes to the broader discourse on sustainable AI, offering a scalable and interpretable framework for advancing explainability in modern machine learning systems.},
  archive      = {J_JAIR},
  author       = {Severin Bachmann},
  doi          = {10.1613/jair.1.18325},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {6},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Efficient XAI: A low-cost data reduction approach to SHAP interpretability},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data augmentation for time-series classification: An extensive empirical study and comprehensive survey. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Data Augmentation (DA) has become a critical approach in Time Series Classification (TSC), primarily for its capacity to expand training datasets, enhance model robustness, introduce diversity, and reduce overfitting. However, the current landscape of DA in TSC is plagued with fragmented literature reviews, nebulous methodological taxonomies, inadequate evaluative measures, and a dearth of accessible and user-oriented tools. Objectives: This study addresses these challenges through a comprehensive examination of DA methodologies within the TSC domain. Methods: Our research began with an extensive literature review spanning a decade, revealing significant gaps in existing surveys and necessitating a detailed analysis of over 100 scholarly articles to identify more than 60 distinct DA techniques. This rigorous review led to the development of a novel taxonomy tailored to the specific needs of DA in TSC, categorizing techniques into five primary categories to guide researchers in selecting appropriate methods with greater clarity. In response to the lack of comprehensive evaluations of foundational DA techniques, we conducted a thorough empirical study, testing nearly 20 DA strategies across 15 diverse datasets representing all types within the UCR time-series repository. To improve practical use, we have consolidated most of these methods into a unified Python Library, whose user-friendly interface facilitates experimenting with various augmentation techniques, offering practitioners and researchers a more convenient tool for innovation than currently available options. Results: Using ResNet and LSTM architectures, we employed a multifaceted evaluation approach, including metrics such as Accuracy, Method Ranking, and Residual Analysis, resulting in a benchmark accuracy of 84.98 ± 16.41% in ResNet and 82.41 ± 18.71% in LSTM. Our investigation underscored the inconsistent efficacies of DA techniques, for instance, methods like RGWs (with an average rank of 7.13 and average accuracy of 83.42 ± 17.53% in LSTM) and Random Permutation significantly improved model performance, whereas others, like EMD, were less effective. Furthermore, we found that the intrinsic characteristics of datasets significantly influence the success of DA methods, leading to targeted recommendations based on empirical evidence to help practitioners select the most suitable DA techniques for specific datasets. Conclusions: In essence, this research presents an integrative perspective on the contemporary landscape of data augmentation for time series classification, combining theoretical frameworks with empirical evidence. The revelations and resources introduced herein are positioned to catalyze continued progress in this domain, fortifying machine learning models against the challenges posed by data limitations, and enhancing their generalizability and robustness.},
  archive      = {J_JAIR},
  author       = {Zijun Gao and Haibao Liu and Lingbo Li},
  doi          = {10.1613/jair.1.17084},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {6},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Data augmentation for time-series classification: An extensive empirical study and comprehensive survey},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSAC: Distributional soft actor-critic for risk-sensitive reinforcement learning. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Distributional Soft Actor-Critic (DSAC), a distributional reinforcement learning (RL) algorithm that combines the strengths of distributional information of accumulated rewards and entropy-driven exploration from Soft Actor-Critic (SAC) algorithm. DSAC models the randomness in both action and rewards, surpassing baseline performances on various continuous control tasks. Unlike standard approaches that solely maximize expected rewards, we propose a unified framework for risk-sensitive learning, one that optimizes the risk-related objective while balancing entropy to encourage exploration. Extensive experiments demonstrate DSAC’s effectiveness in enhancing agent performances for both risk-neutral and risk-sensitive control tasks.},
  archive      = {J_JAIR},
  author       = {Xiaoteng Ma and Junyao Chen and Li Xia and Jun Yang and Qianchuan Zhao and Zhengyuan Zhou},
  doi          = {10.1613/jair.1.17526},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {6},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {DSAC: Distributional soft actor-critic for risk-sensitive reinforcement learning},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI for all: Identifying AI incidents related to diversity and inclusion. <em>JAIR</em>, <em>83</em>. (<a href='https://doi.org/10.1613/jair.1.17806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of Artificial Intelligence (AI) technologies has introduced both significant advancements and challenges, with diversity and inclusion (D&amp;I) emerging as a critical concern. Addressing D&amp;I in AI is essential to reduce biases and discrimination, enhance fairness, and prevent adverse societal impacts. Despite its importance, D&amp;I considerations are often overlooked, resulting in incidents marked by built-in biases and ethical dilemmas. Analyzing AI incidents through a D&amp;I lens is crucial for identifying causes of biases and developing strategies to mitigate them, ensuring fairer and more equitable AI technologies. However, systematic investigations of D&amp;I-related AI incidents are scarce. This study addresses these challenges by identifying and understanding D&amp;I issues within AI systems through a manual analysis of two AI incident databases, AI Incident Database (AIID) and AI, Algorithmic, and Automation Incidents and Controversies (AIAAIC). The research develops a decision tree to investigate D&amp;I issues tied to AI incidents and populate a public repository of D&amp;I-related AI incidents. The decision tree was validated through a card sorting exercise and focus group discussions. The research demonstrates that almost half of the analyzed AI incidents are related to D&amp;I, with a notable predominance of racial, gender, and age discrimination. The decision tree and resulting public repository aim to foster further research and responsible AI practices, promoting the development of inclusive and equitable AI systems.},
  archive      = {J_JAIR},
  author       = {Rifat Ara Shams and Didar Zowghi and Muneera Bano},
  doi          = {10.1613/jair.1.17806},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {6},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {AI for all: Identifying AI incidents related to diversity and inclusion},
  volume       = {83},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preference tuning with human feedback on language, speech, and vision tasks: A survey. <em>JAIR</em>, <em>82</em>, 2595-2661. (<a href='https://doi.org/10.1613/jair.1.17541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preference tuning is a crucial process for aligning deep generative models with human preferences. This survey offers a thorough overview of recent advancements in preference tuning and the integration of human feedback. The paper is organized into three main sections: 1) introduction and preliminaries: an introduction to reinforcement learning frameworks, preference tuning tasks, models, and datasets across various modalities: language, speech, and vision, as well as different policy approaches, 2) in-depth exploration of each preference tuning approach: a detailed analysis of the methods used in preference tuning, and 3) applications, discussion, and future directions: an exploration of the applications of preference tuning in downstream tasks, including evaluation methods for different modalities, and an outlook on future research directions. Our objective is to present the latest methodologies in preference tuning and model alignment, enhancing the understanding of this field for researchers and practitioners. We hope to encourage further engagement and innovation in this area. Additionally, we provide a GitHub link https://github.com/hanyang1999/Preference-Tuning-with-Human-Feedback.},
  archive      = {J_JAIR},
  author       = {Genta Indra Winata and Hanyang Zhao and Anirban Das and Wenpin Tang and David D. Yao and Shi-Xiong Zhang and Sambit Sahu},
  doi          = {10.1613/jair.1.17541},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2595-2661},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Preference tuning with human feedback on language, speech, and vision tasks: A survey},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards explainable goal recognition using weight of evidence (WoE): A human-centered approach. <em>JAIR</em>, <em>82</em>, 2535-2594. (<a href='https://doi.org/10.1613/jair.1.17173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goal recognition (GR) involves inferring an agent's unobserved goal from a sequence of observations. This is a critical problem in AI with diverse applications. Traditionally, GR has been addressed using 'inference to the best explanation' or abduction, where hypotheses about the agent's goals are generated as the most plausible explanations for observed behavior. Alternatively, some approaches enhance interpretability by ensuring that an agent's behavior aligns with an observer's expectations or by making the reasoning behind decisions more transparent. In this work, we tackle a different challenge: explaining the GR process in a way that is comprehensible to humans. We introduce and evaluate an explainable model for goal recognition (GR) agents, grounded in the theoretical framework and cognitive processes underlying human behavior explanation. Drawing on insights from two human-agent studies, we propose a conceptual framework for human-centered explanations of GR. Using this framework, we develop the eXplainable Goal Recognition (XGR) model, which generates explanations for both why and why not questions. We evaluate the model computationally across eight GR benchmarks and through three user studies. The first study assesses the efficiency of generating human-like explanations within the Sokoban game domain, the second examines perceived explainability in the same domain, and the third evaluates the model's effectiveness in aiding decision-making in illegal fishing detection. Results demonstrate that the XGR model significantly enhances user understanding, trust, and decision-making compared to baseline models, underscoring its potential to improve human-agent collaboration.},
  archive      = {J_JAIR},
  author       = {Abeer Alshehri and Amal Abdulrahman and Hajar Alamri and Tim Miller and Mor Vered},
  doi          = {10.1613/jair.1.17173},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2535-2594},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Towards explainable goal recognition using weight of evidence (WoE): A human-centered approach},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative curricula for multi-agent path finding via unsupervised and reinforcement learning. <em>JAIR</em>, <em>82</em>, 2471-2534. (<a href='https://doi.org/10.1613/jair.1.17403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Path Finding (MAPF) is the challenging problem of finding collision-free paths for multiple agents, which has a wide range of applications, such as automated warehouses, smart manufacturing, and traffic management. Recently, machine learning-based approaches have become popular in addressing MAPF problems in a decentralized and potentially generalizing way. Most learning-based MAPF approaches use reinforcement and imitation learning to train agent policies for decentralized execution under partial observability. However, current state-of-the-art approaches suffer from a prevalent bias to micro-aspects of particular MAPF problems, such as congestions in corridors and potential delays caused by single agents, leading to tight specializations through extensive engineering via oversized models, reward shaping, path finding algorithms, and communication. These specializations are generally detrimental to the sample efficiency, i.e., the learning progress given a certain amount of experience, and generalization to previously unseen scenarios. In contrast, curriculum learning offers an elegant and much simpler way of training agent policies in a step-by-step manner to master all aspects implicitly without extensive engineering. In this paper, we propose a generative curriculum approach to learning-based MAPF using Variational Autoencoder Utilized Learning of Terrains (VAULT). We introduce a two-stage framework to (I) train the VAULT via unsupervised learning to obtain a latent space representation of maps and (II) use the VAULT to generate curricula in order to improve sample efficiency and generalization of learning-based MAPF methods. For the second stage, we propose a bi-level curriculum scheme by combining our VAULT curriculum with a low-level curriculum method to improve sample efficiency further. Our framework is designed in a modular and general way, where each proposed component serves its purpose in a black-box manner without considering specific micro-aspects of the underlying problem. We empirically evaluate our approach in maps of the public MAPF benchmark set as well as novel artificial maps generated with the VAULT. Our results demonstrate the effectiveness of the VAULT as a map generator and our VAULT curriculum in improving sample efficiency and generalization of learning-based MAPF methods compared to alternative approaches. We also demonstrate how data pruning can further reduce the dependence on available maps without affecting the generalization potential of our approach.},
  archive      = {J_JAIR},
  author       = {Thomy Phan and Timy Phan and Sven Koenig},
  doi          = {10.1613/jair.1.17403},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2471-2534},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Generative curricula for multi-agent path finding via unsupervised and reinforcement learning},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAT2I: Enhancing perceptual authenticity in text-to-image synthesis using multi-attribute generative adversarial networks. <em>JAIR</em>, <em>82</em>, 2453-2469. (<a href='https://doi.org/10.1613/jair.1.18237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating visuals from text involves deriving visual representations from textual descriptions and transforming them into corresponding visuals. This technique finds vast application in various fields, such as graphic design and image editing. Generative adversarial networks (GANs) are the widely used and better performers for this task. A primary hurdle in this process is producing perceptually authentic visuals. This study introduces a MultiAttribute Text to Image Synthesis Generative Adversarial Network (MAT2I) to address these challenges. The enhancements encompass attribute-control-net, feature alignment, and perceptual loss. The attribute-control-net is used for the fast and attribute-specific generation to maintain authenticity in perceptuality with adaptability. Feature alignment and perceptual loss motivate the generator to create visuals that closely resemble real visuals based on the accompanying text and to reduce randomness. The effectiveness of the proposed model is gauged on the CUB and COCO datasets. Empirical findings illustrate that this approach generates visuals with greater content diversity, enhanced realism, and improved semantic alignment with provided text descriptions. Furthermore, the proposed method surpasses comparative techniques in terms of inception score, further establishing its competitive performance.},
  archive      = {J_JAIR},
  author       = {Varsha Singh and Vijai Singh and Uma Shanker Tiwary},
  doi          = {10.1613/jair.1.18237},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2453-2469},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {MAT2I: Enhancing perceptual authenticity in text-to-image synthesis using multi-attribute generative adversarial networks},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability in online coalition formation. <em>JAIR</em>, <em>82</em>, 2423-2452. (<a href='https://doi.org/10.1613/jair.1.16420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coalition formation is concerned with the question of how to partition a set of agents into disjoint coalitions according to their preferences. Deviating from most of the previous work, we consider an online variant of the problem, where agents arrive in sequence. Whenever an agent arrives, they must be assigned to a coalition immediately and irrevocably. The scarce existing literature on online coalition formation has focused on maximizing social welfare, a demanding requirement, even in the offline setting. Instead, we seek to achieve stable coalition structures online and treat the most common stability concepts based on deviations by single agents and groups of agents. We present a comprehensive picture in additively separable hedonic games, leading to dichotomies, where positive results are obtained by deterministic algorithms and negative results even hold for randomized algorithms.},
  archive      = {J_JAIR},
  author       = {Martin Bullinger and René Romen},
  doi          = {10.1613/jair.1.16420},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2423-2452},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Stability in online coalition formation},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI governance in the GCC states: A comparative analysis of national AI strategies. <em>JAIR</em>, <em>82</em>, 2389-2422. (<a href='https://doi.org/10.1613/jair.1.17619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gulf Cooperation Council (GCC) states increasingly adopt Artificial Intelligence (AI) to drive economic diversification and enhance services. This paper investigates the evolving AI governance landscape across the six GCC nations, the United Arab Emirates, Saudi Arabia, Qatar, Oman, Bahrain, and Kuwait, through an in-depth document analysis of six National AI Strategies (NASs) and related policies published between 2018 and 2024. Drawing on the Multiple Streams Framework (MSF) and Multi-stakeholder Governance theory, the findings highlight a “soft regulation” approach that emphasizes national strategies and ethical principles rather than binding regulations. While this approach fosters rapid innovation, it also raises concerns regarding the enforceability of ethical standards, potential ethicswashing, and alignment with global frameworks, particularly the EU AI Act. Common challenges include data limitations, talent shortages, and reconciling AI applications with cultural values. Despite these hurdles, GCC governments aspire to leverage AI for robust economic growth, better public services, and regional leadership in responsible AI. The analysis suggests that strengthening legal mechanisms, enhancing stakeholder engagement, and aligning policies with local contexts and international norms will be essential for harnessing AI’s transformative potential in the GCC. This article appears in the AI &amp; Society Track},
  archive      = {J_JAIR},
  author       = {Mohammad Rashed Albous and Odeh Rashed Al-Jayyousi and Melodena Stephens},
  doi          = {10.1613/jair.1.17619},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2389-2422},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {AI governance in the GCC states: A comparative analysis of national AI strategies},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composition and zero-shot transfer with lattice structures in reinforcement learning. <em>JAIR</em>, <em>82</em>, 2325-2388. (<a href='https://doi.org/10.1613/jair.1.16817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important property of long-lived agents is the ability to reuse existing knowledge to solve new tasks. An appealing approach towards obtaining such agents is by leveraging logical composition over tasks, where new tasks are defined by applying logic operators to previously-solved ones. This composition is particularly powerful since it provides a human-understandable mechanism for task specification. However, no unifying formalism for applying logic operators to tasks and generalising combinatorially over them has yet been developed. We address the problem by formally defining logical composition as operators acting on a set of tasks in a lattice structure—the algebraic structure that generalises the study of Boolean logic. This provides a theoretically rigorous method for composing tasks, allowing us to formulate new tasks in terms of the negation, disjunction, and conjunction of a set of base tasks. We prove that by learning a new type of goal-oriented value function model free, called the world value function, an agent can solve composite tasks involving arbitrary logical operators with no further learning. We verify our approach in high-dimensional domains—including a video game environment and continuous-control task—where an agent first learns to solve a set of base tasks, and then composes these solutions to solve a super-exponential number of new tasks.},
  archive      = {J_JAIR},
  author       = {Geraud Nangue Tasse and Steven James and Benjamin Rosman},
  doi          = {10.1613/jair.1.16817},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2325-2388},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Composition and zero-shot transfer with lattice structures in reinforcement learning},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counterfactual situation testing: From single to multidimensional discrimination. <em>JAIR</em>, <em>82</em>, 2279-2323. (<a href='https://doi.org/10.1613/jair.1.17935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As machine learning models enable decisions once performed only by humans, it is central to develop tools that assess the fairness of such models. Notably, within high-stake settings like hiring and lending, these tools must be able to detect potentially discriminatory models. We present counterfactual situation testing (CST), a causal data mining framework for detecting individual discrimination in a dataset of classifier decisions. CST answers the question “what would have been the model outcome had the individual, or complainant, been of a different protected status?” It extends the legally-grounded situation testing (ST) of Thanh et al. (2011) by operationalizing the notion of fairness given the difference via counterfactual reasoning. ST finds for each complainant similar protected and non-protected instances in the dataset; constructs, respectively, a control and test group; and compares the groups such that a difference in model outcomes implies a potential case of individual discrimination. CST, instead, avoids this idealized comparison by establishing the test group on the complainant’s generated counterfactual, which reflects how the protected attribute when changed influences other seemingly neutral attributes of the complainant. Under CST we test for discrimination for each complainant by comparing similar individuals within the control and test group but dissimilar individuals across these groups. We consider single (e.g., gender) and multidimensional (e.g., gender and race) discrimination testing. For multidimensional discrimination we study multiple and intersectional discrimination and, as feared by legal scholars, find evidence that the former fails to account for the latter kind. Using a k-nearest neighbor implementation, we showcase CST on synthetic and real data. Experimental results show that CST uncovers a higher number of cases than ST, even when the model is counterfactually fair. CST, in fact, extends counterfactual fairness (CF) of Kusner et al. (2017) by equipping CF with confidence intervals, which we report for all experiments.},
  archive      = {J_JAIR},
  author       = {Jose M. Alvarez and Salvatore Ruggieri},
  doi          = {10.1613/jair.1.17935},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2279-2323},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Counterfactual situation testing: From single to multidimensional discrimination},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting AI-generated text: Factors influencing detectability with current methods. <em>JAIR</em>, <em>82</em>, 2233-2278. (<a href='https://doi.org/10.1613/jair.1.16665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize stateof-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how “detectable” AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge.},
  archive      = {J_JAIR},
  author       = {Kathleen C. Fraser and Hillary Dawkins and Svetlana Kiritchenko},
  doi          = {10.1613/jair.1.16665},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2233-2278},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Detecting AI-generated text: Factors influencing detectability with current methods},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defensive alliances in signed networks. <em>JAIR</em>, <em>82</em>, 2189-2232. (<a href='https://doi.org/10.1613/jair.1.17165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of social networks and community detection is a central theme in Artificial Intelligence. One line of research deals with finding groups of agents that could work together to achieve a certain goal. To this end, different notions of so-called clusters or communities have been introduced in the literature of graphs and networks. Among these, a defensive alliance is a kind of quantitative group structure. However, all studies on alliances so far have ignored one aspect that is central to the formation of alliances on a very intuitive level, assuming that the agents are preconditioned concerning their attitude towards other agents: they prefer to be in some group (or in an alliance) together with the agents they like, so that they are happy to help each other towards their common aim, possibly then working against the agents outside of their group that they dislike. Signed networks were introduced in the psychology literature to model liking and disliking between agents, generalizing graphs in a natural way. Hence, we propose the novel notion of a defensive alliance in the context of signed networks. We then investigate several natural algorithmic questions related to this notion. These, and also combinatorial findings, connect our notion to that of correlation clustering, which is a well-established idea of finding groups of agents within a signed network. Also, we introduce a new structural parameter for signed graphs, the signed neighborhood diversity snd, and exhibit a snd-parameterized algorithm that finds one of the smallest defensive alliances in a signed graph.},
  archive      = {J_JAIR},
  author       = {Emmanuel Arrighi and Zhidan Feng and Henning Fernau and Kevin Mann and Xingqin Qi and Petra Wolf},
  doi          = {10.1613/jair.1.17165},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2189-2232},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Defensive alliances in signed networks},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the phase transition of the euclidean travelling salesman problem with time windows. <em>JAIR</em>, <em>82</em>, 2167-2188. (<a href='https://doi.org/10.1613/jair.1.18334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithms are often evaluated on randomly generated instances to study scale-up properties with respect to features such as the size, for example. Also, machine learning based approaches often train models on randomly generated instances as they need large sets of training instances. In this paper, we consider the Euclidean Travelling Salesman Problem with Time Windows (TSPTW), and we study the impact of parameters used to randomly generate TSPTW instances on hardness and feasibility. We first consider the decision version of the problem, where feasibility depends on start and end times of time windows. We introduce two parameters, α and β, for controlling the tightness of the time horizon and the time windows. We show that instance hardness is related to a phase transition phenomenon: as we increase α and β, we pass from an unfeasible region (where almost all generated instances have no solution) to a feasible region (where almost all generated instances have solutions), and the hardest instances are located within the transition zone. We formally relate this transition zone with respect to α and β, thus allowing us to control hardness and feasibility when randomly generating instances. Then, we study the optimization problem, the goal of which is to find the smallest tour that satisfies all time windows. We show that the empirical hardness is still related to the phase transition: hardness increases when moving from the infeasible region to the transition zone, as in the decision problem. However, unlike the decision problem, some hard instances are also located in the feasible region where instances are very loosely constrained.},
  archive      = {J_JAIR},
  author       = {Omar Rifki and Christine Solnon},
  doi          = {10.1613/jair.1.18334},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2167-2188},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the phase transition of the euclidean travelling salesman problem with time windows},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving overlapping coalition structure generation in task-based settings. <em>JAIR</em>, <em>82</em>, 2125-2166. (<a href='https://doi.org/10.1613/jair.1.17138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The overlapping coalition structure generation problem (OCSGP) is a challenging computational problem in multi-agent systems. It focuses on selecting possibly overlapping coalitions from a set of agents to maximize the social welfare of all coalitions while containing all agents. However, in practical applications, coalitions may be formed to selectively respond to tasks from a pool of potential tasks assigned to agents. Consequently, this study considers OCSGP in a task-based setting, where each agent has finite resources and can only respond to tasks of interest, and each coalition can only take on mutually disjoint subsets of tasks. Specifically, we first present a model of the task-based OCSGP and investigate its computational complexity. Our theoretical results demonstrate that this specific OCSGP remains intractable even under restrictive assumptions. Subsequently, we develop a generic evolutionary algorithm framework (EAF) to find an approximately optimal overlapping coalition structure (OCS) in time quartic polynomial in the size of the instance. Particularly, we devise a specific solution-repair based heuristic of cubic time complexity to generate a feasible OCS. Finally, we compare the proposed EAF with a task-oriented heuristic and a hybrid algorithm for OCSGP, and examine its applicability in the pursuit-evasion problem. The experimental results reveal that the proposed EAF exhibits superior performance in finding feasible OCSs and demonstrates flexible adaptability to problem size and resource status.},
  archive      = {J_JAIR},
  author       = {Guofu Zhang and Zhaopin Su and Xiaoxiao Song and Zixuan Gao and Miqing Li and Xin Yao},
  doi          = {10.1613/jair.1.17138},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2125-2166},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Solving overlapping coalition structure generation in task-based settings},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From expert systems to generative artificial experts: A new concept for human-AI collaboration in knowledge work. <em>JAIR</em>, <em>82</em>, 2101-2124. (<a href='https://doi.org/10.1613/jair.1.17175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Generative Artificial Experts (GAEs) - a concept of a new type of generative AI agents designed for human-AI collaboration in knowledge work. GAEs have specialized domain expertise, perform tasks within bounded autonomy, include a synthetic persona and possess multimodal generative AI capabilities, among other features. We provide a definition of GAEs which includes seven defining traits, offering a taxonomy which sets them apart from other generative AI systems. We use literature-review based conceptual analysis with abductive reasoning to propose the new concept that addresses identified limitations in existing systems. The paper explores the emergence of GAEs as a leap from expert systems. We name two enablers for GAEs - ongoing development of a research field of human-AI collaboration and growing capabilities of generative artificial intelligence systems. We discuss existing generative AI agents, noting that GAEs as such do not exist yet, but are starting to emerge. Due conceptual nature of this paper we do not explore the technical aspects of GAEs development. Instead, we use illustrative examples to present possible applications of GAEs and their potential role in the future of knowledge work. This article appears in the AI &amp; Society track.},
  archive      = {J_JAIR},
  author       = {Konrad Sowa and Aleksandra Przegalinska},
  doi          = {10.1613/jair.1.17175},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {4},
  pages        = {2101-2124},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {From expert systems to generative artificial experts: A new concept for human-AI collaboration in knowledge work},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Certified knowledge compilation with application to formally verified model counting. <em>JAIR</em>, <em>82</em>, 2057–2099. (<a href='https://doi.org/10.1613/jair.1.15958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing many useful properties of Boolean formulas, such as their weighted or unweighted model count, is intractable on general representations. It can become tractable when formulas are expressed in a special form, such as the decision decomposable negation normal form (decision-DNNF). Knowledge compilation is the process of converting a formula into such a form. Unfortunately existing knowledge compilers provide no guarantee that their output correctly represents the original formula, and therefore they cannot validate a model count, or any other computed value. We present Partitioned-Operation Graphs (POGs), a form that can encode all of the representations used by existing knowledge compilers. We have designed CPOG, a framework that can express proofs of equivalence between a POG and a Boolean formula in conjunctive normal form (CNF). We have developed a program that generates POG representations from decision-DNNF graphs produced by the state-of-the-art knowledge compiler D4, as well as checkable CPOG proofs certifying that the output POGs are equivalent to the input CNF formulas. Our toolchain for generating and verifying POGs scales to all but the largest graphs produced by D4 for formulas from a recent model counting competition. Additionally, we have developed a formally verified CPOG checker and model counter for POGs in the Lean 4 proof assistant. In doing so, we proved the soundness of our proof framework. These programs comprise the first formally verified toolchain for weighted and unweighted model counting.},
  archive      = {J_JAIR},
  author       = {Randal E. Bryant and Wojciech Nawrocki and Jeremy Avigad and Marijn J. H. Heule},
  doi          = {10.1613/jair.1.15958},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {2057–2099},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Certified knowledge compilation with application to formally verified model counting},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TabID: Automatic identification and tabulation of subproblems in constraint models. <em>JAIR</em>, <em>82</em>, 1999-2056. (<a href='https://doi.org/10.1613/jair.1.17032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of a constraint model can often be improved by converting a subproblem into a single table constraint (referred to as tabulation). Finding subproblems to tabulate is traditionally a manual and time-intensive process, even for expert modellers. This paper presents TabID, an entirely automated method to identify promising subproblems for tabulation in constraint programming. We introduce a diverse set of heuristics designed to identify promising candidates for tabulation, aiming to improve solver performance. These heuristics are intended to encapsulate various factors that contribute to useful tabulation. We also present additional checks to limit the potential drawbacks of suboptimal tabulation. We comprehensively evaluate our approach using benchmark problems from existing literature that previously relied on manual identification by constraint programming experts of constraints to tabulate. We demonstrate that our automated identification and tabulation process achieves comparable, and in some cases improved results. We empirically evaluate the efficacy of our approach on a variety of solvers, including standard CP (Minion and Gecode), clause-learning CP (Chuffed and OR-Tools) and SAT solvers (Kissat). Our findings highlight the substantial potential of fully automated tabulation, suggesting its integration into automated model reformulation tools.},
  archive      = {J_JAIR},
  author       = {Özgür Akgün and Ian Gent and Christopher Jefferson and Zeynep Kiziltan and Ian Miguel and Peter Nightingale and András Salamon and Felix Ulrich-Oltean},
  doi          = {10.1613/jair.1.17032},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1999-2056},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {TabID: Automatic identification and tabulation of subproblems in constraint models},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A divide, align and conquer strategy for program synthesis. <em>JAIR</em>, <em>82</em>, 1961-1997. (<a href='https://doi.org/10.1613/jair.1.16847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major bottleneck in search-based program synthesis, which learns programs from input/output examples, is the synthesis of large programs. As the size of the target program increases, so does the search depth, which leads to an exponentially growing number of candidate programs. Humans mitigate the combinatorial explosion that arises from deep program search: they build complex programs from smaller parts. We introduce a new strategy for program synthesis called Divide, Align &amp; Conquer (DA&amp;C) that exploits the compositionality of real-world domains to guide the synthesis towards useful subprograms. Divide decomposes each example using a segmentation procedure that is synthesized as part of the learning problem. Align matches the components in the decomposed input/output examples to steer the search toward combinations that lead to the synthesis of useful subprograms, and Conquer then solves a standalone synthesis problem on each pair of aligned input/output components. We show how replacing a deep program search with a linear number of much smaller synthesis tasks leads us to efficiently discover useful subprograms that are then combined into a solution program. Our agent outperforms current Inductive Logic Programming (ILP) methods on string transformation tasks even with minimal knowledge priors. Unlike existing methods, the predictive accuracy of our agent monotonically increases for additional examples. It approximates an average time complexity of O(m) in the size m of subprograms for highly structured and, hence, decomposable domains such as strings. Finally, we demonstrate the scalability of our technique on highdimensional abstract visual reasoning tasks from the Abstract Reasoning Corpus (ARC) for which ILP methods were previously infeasible. We are competitive with state-of-the-art agents outside of ILP, despite generating only 0.2% as many candidate programs from a knowledge prior of only 11 generic geometric primitives.},
  archive      = {J_JAIR},
  author       = {Jonas Witt and Sebastijan Dumančić and Tias Guns and Claus-Christian Carbon},
  doi          = {10.1613/jair.1.16847},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1961-1997},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A divide, align and conquer strategy for program synthesis},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new regret-analysis framework for budgeted multi-armed bandits. <em>JAIR</em>, <em>82</em>, 1943-1959. (<a href='https://doi.org/10.1613/jair.1.16261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider two versions of the (stochastic) budgeted Multi-Armed Bandit problem. The first one was introduced by Tran-Thanh et al. (AAAI, 2012): Pulling each arm incurs a fixed deterministic cost and yields a random reward i.i.d. sampled from an unknown distribution (prior free). We have a global budget B and aim to devise a strategy to maximize the expected total reward. The second one was introduced by Ding et al. (AAAI, 2013): It has the same setting as before except costs of each arm are i.i.d. samples from an unknown distribution (and independent from its rewards). We propose a new budget-based regret-analysis framework and design two simple algorithms to illustrate the power of our framework. Our regret bounds for both problems not only match the optimal bound of O(ln B) but also significantly reduce the dependence on other input parameters (assumed constants), compared with the two studies of Tran-Thanh et al. (AAAI, 2012) and Ding et al. (AAAI, 2013) where both utilized a time-based framework. Extensive experimental results show the effectiveness and computation efficiency of our proposed algorithms and confirm our theoretical predictions.},
  archive      = {J_JAIR},
  author       = {Evan Yifan Xu and Pan Xu},
  doi          = {10.1613/jair.1.16261},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1943-1959},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A new regret-analysis framework for budgeted multi-armed bandits},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combinatorial multi-armed bandits with fairness constraints: An online convex optimization perspective. <em>JAIR</em>, <em>82</em>, 1909-1942. (<a href='https://doi.org/10.1613/jair.1.16580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of multi-armed bandit (MAB) with fairness constraints has emerged as an important research topic recently. For such problems, one common objective is to maximize the total rewards within a fixed number of pull rounds, while satisfying the fairness requirement of a minimum selection fraction for each individual arm in the long run. Previous works have made substantial advancements in designing various online selection solutions for MAB, however, when incorporating such fairness constraints, they fail to achieve a sublinear regret bound. In this paper, we study a combinatorial MAB problem with concave objective and fairness constraints. In particular, we design a new selection algorithm that solves MAB problems from an online convex optimization perspective. Our algorithm is computationally efficient, and more importantly, manages to achieve a sublinear regret bound of O( √ T ln T) with high probability guarantees in T selection rounds. We also extend our framework to include more general knapsack constraints. Finally, we assess the performance of our algorithm through extensive simulations and real dataset applications, demonstrating its significant advantages over baseline schemes.},
  archive      = {J_JAIR},
  author       = {Xiaosong Chen and Hanqin Zhuang and Yang Liu and Huanle Xu and Wing Cheong Lau},
  doi          = {10.1613/jair.1.16580},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1909-1942},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Combinatorial multi-armed bandits with fairness constraints: An online convex optimization perspective},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path-planning on a spherical surface with disturbances and exclusion zones. <em>JAIR</em>, <em>82</em>, 1845-1907. (<a href='https://doi.org/10.1613/jair.1.16746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An algorithm is presented for path-planning in a non-uniform spheroid mesh containing exclusion zones, vector and scalar fields. The mesh models physical environments such as ocean regions, together with a variety of environmental phenomena such as wind, current and ice conditions which impact on routing decisions. The path-planning method can be used to optimise the travel time of journeys between points in the mesh. We provide the algorithmic details and the mathematical foundations of the algorithms. To demonstrate that the method has basic desirable properties, we show that long paths in unconstrained regions of the mesh closely approximate great circle arcs. We go on to show that the method path-plans efficiently in environments with complex interacting conditions.},
  archive      = {J_JAIR},
  author       = {Jonathan Smith and Samuel Hall and George Coombs and Harrison Abbot and Ayat Fekry and Michael Thorne and Derek Long and Maria Fox},
  doi          = {10.1613/jair.1.16746},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1845-1907},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Path-planning on a spherical surface with disturbances and exclusion zones},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive few-shot class-incremental learning via latent variable models. <em>JAIR</em>, <em>82</em>, 1807-1843. (<a href='https://doi.org/10.1613/jair.1.17006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approaches to class-incremental learning aim to successfully learn from continuously arriving classes. One added level of difficulty usually arises when the training data belonging to each class is scarce, which is the case in several open-world machine learning applications. In this paradigm, which is referred to as few-shot class-incremental learning, a typical learner needs to both be able to learn incrementally from the sequentially arriving classes, and preserve the knowledge which already exists about the old (i.e. already existing) classes. We propose a few-shot class-incremental learner which adapts the representations of the new few-shot classes as well as relevant previous knowledge based on a latent variable model. The proposed latent variable model is a form of a variational autoencoder that is designed to address the main challenges of the few-shot class-incremental learning paradigm, namely catastrophic forgetting and potential bias. During the few-shot learning of new classes, the amortization and high fidelity characteristics of the proposed model are leveraged to adapt not only the current class, but also the relevant previously encountered classes, in order to consistently mitigate the impact of catastrophic forgetting, bias and overfitting. We also derive a generalization upper bound on the error of an upcoming class. Experiments on several widely used few-shot class-incremental learning benchmarks, as well as a medical benchmark consisting of real-world medical images, demonstrate that the proposed model leads to improved performance, as measured by average overall and final classification accuracy, and in terms of alleviating catastrophic forgetting.},
  archive      = {J_JAIR},
  author       = {Tameem Adel},
  doi          = {10.1613/jair.1.17006},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1807-1843},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Adaptive few-shot class-incremental learning via latent variable models},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalizing the regret: An analysis of lower and upper bounds. <em>JAIR</em>, <em>82</em>, 1773-1806. (<a href='https://doi.org/10.1613/jair.1.17614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The (expected cumulative) regret is the customary index to judge the performance of online sequential decision-making algorithms. In the traditional form, it is defined as the expected sum over a learning horizon T of the sub-optimality gaps ∆It (i.e., expected instantaneous regret) the agent suffers when playing arm It at round t. In this paper, we propose and investigate a generalization of this notion, named g-(expected cumulative) regret, obtained by applying a transformation function g to the sub-optimality gaps, making the agent suffer g(∆It) instead of just ∆It . Intuitively, function g embeds the “perception” that the agent manifests when performing a sub-optimal decision. We first show that sublinear g-regret is not achievable for a generic transformation function g. Then, we introduce a mild condition on g and provide instance-dependent and worst-case (i.e., minimax) lower bounds for the g-regret. Finally, we show that state-of-the-art stochastic bandit algorithms with no modification surprisingly display optimal performances for the g-regret. Specifically, we prove that UCB1 matches (up to constant factors) the instance-dependent lower bound regardless of function g and that MOSS matches (up to constant factors) the minimax lower bound at least for a wide class of transformation functions.},
  archive      = {J_JAIR},
  author       = {Marco Mussi and Alberto Maria Metelli},
  doi          = {10.1613/jair.1.17614},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1773-1806},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Generalizing the regret: An analysis of lower and upper bounds},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the consistency between belief revision and belief update. <em>JAIR</em>, <em>82</em>, 1743-1771. (<a href='https://doi.org/10.1613/jair.1.17917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Belief revision and belief update are two fundamental and well-studied processes of belief change. In the present article, we introduce a consistency principle which dictates that the revision and update policies employed by a rational agent are not independent, but ought to be related in a certain coherent way. We formalize our consistency principle both axiomatically and semantically, and we establish a representation result explicitly connecting the two formalizations. Furthermore, we show that two important concrete types of belief change, namely uniform belief change and parametrized-difference belief change, serve as proof-of-concept examples for the introduced consistency principle, as they fully comply with it. Additionally, we identify an intriguing property of uniform belief change in which revision and update become indistinguishable when an epistemic input contradicts the initial state of belief, as both processes produce identical outcomes. Lastly, it is shown that, unlike parametrized-difference belief change, uniform belief change is incompatible with Parikh’s notion of relevance. Consequently, building on previous results, it is demonstrated that parametrized-difference belief change is relevance-sensitive — indicating that the proposed principle of consistency is compatible with relevance — while uniform belief change is not.},
  archive      = {J_JAIR},
  author       = {Theofanis I. Aravanis},
  doi          = {10.1613/jair.1.17917},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1743-1771},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the consistency between belief revision and belief update},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Existence, computation and efficiency of nash stable outcomes in hedonic skill games. <em>JAIR</em>, <em>82</em>, 1711-1742. (<a href='https://doi.org/10.1613/jair.1.17157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with hedonic skill games, a non-transferable utility counterpart of coalitional skill games which model collaboration among entities through the abstract notions of tasks and the skills required to complete them. In the weighted tasks setting, we show that deciding whether an instance of the game admits a Nash stable outcome is NP-complete. We then characterize the instances admitting a Nash stable outcome. This characterization relies on the fact that every agent holds (resp., every task requires) either a single skill or more than one skill. For these instances, the complexity of computing a Nash stable outcome is determined, together with the possibility that natural dynamics converge to a Nash stable outcome from any initial configuration. Our study is completed with a thorough analysis of the price of anarchy of instances always admitting a Nash stable outcome.},
  archive      = {J_JAIR},
  author       = {Laurent Gourvès and Gianpiero Monaco},
  doi          = {10.1613/jair.1.17157},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1711-1742},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Existence, computation and efficiency of nash stable outcomes in hedonic skill games},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MRC and transfer learning framework for document-level event factuality identification with heterogeneous spectral attention networks. <em>JAIR</em>, <em>82</em>, 1691-1710. (<a href='https://doi.org/10.1613/jair.1.17292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concentrates on Document-level Event Factuality Identification (DEFI) that predicts event factuality values from the viewpoint of the document. At present, the shortcomings of previous studies are multi-fold, including data limitation and scarcity, coarsegrained interpretability without span-level factuality clues, no unified model for different datasets. This paper is devoted to address the above problems by building unified Machine Reading Comprehension (MRC) frameworks comprised of both span-extraction and multiple-choice styles, which exploit Heterogeneous Spectral Attention Networks (HSAN) with spectral networks and hypergraph attention networks as the fine-grained encoders, especially for span-level encoding. Moreover, we integrate Transfer Learning (TL) as cross-domain data augmentation to learn more span-level information from classical MRC datasets by source and target adapters. Experimental performance on ExDLEF corpus, which contains both English and Chinese documents, shows that our span-extraction MRC model is superior to several state-of-the-art baselines, and proves the effectiveness of transfer learning under MRC paradigms.},
  archive      = {J_JAIR},
  author       = {Zhong Qian and Peifeng Li and Qiaoming Zhu and Guodong Zhou},
  doi          = {10.1613/jair.1.17292},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1691-1710},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {MRC and transfer learning framework for document-level event factuality identification with heterogeneous spectral attention networks},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Principles for responsible AI consciousness research. <em>JAIR</em>, <em>82</em>, 1673-1690. (<a href='https://doi.org/10.1613/jair.1.17310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research suggests that it may be possible to build conscious AI systems now or in the near future. Conscious AI systems would arguably deserve moral consideration, and it may be the case that large numbers of conscious systems could be created and caused to suffer. Furthermore, AI systems or AI-generated characters may increasingly give the impression of being conscious, leading to debate about their moral status. Organisations involved in AI research must establish principles and policies to guide research and deployment choices and public communication concerning consciousness. Even if an organisation chooses not to study AI consciousness as such, it will still need policies in place, as those developing advanced AI systems risk inadvertently creating conscious entities. Responsible research and deployment practices are essential to address this possibility. We propose five principles for responsible research and argue that research organisations should make voluntary, public commitments to principles on these lines. Our principles concern research objectives and procedures, knowledge sharing and public communications. This article appears in the AI &amp; Society track.},
  archive      = {J_JAIR},
  author       = {Patrick Butlin and Theodoros Lappas},
  doi          = {10.1613/jair.1.17310},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1673-1690},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Principles for responsible AI consciousness research},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From knowledge to action: Logics of permitted and obligatory announcements. <em>JAIR</em>, <em>82</em>, 1629-1672. (<a href='https://doi.org/10.1613/jair.1.17180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formalize the notions of “permitted and obligatory announcements” in the context of information security, such as privacy policy compliance. In a sender-receiver setting, we define the sender’s permitted and obligatory announcements in terms of the receiver’s ideal epistemic states (i.e., the epistemic states that comply with the given security policies). We propose two logics, LPOA and DLPOA, to reason about permitted and obligatory announcements in static and dynamic contexts, respectively. These two logics are completely axiomatized, and we also study generalizations in which the receiver’s knowledge is characterized by non-S5 logics. Our paper makes two main contributions to the formalization of permitted and obligatory announcements: First, we clarify the interplay between the sender’s permitted and obligatory announcements and the receiver’s knowledge. Second, we distinguish between weakly and strongly permitted announcements.},
  archive      = {J_JAIR},
  author       = {Xu Li and Guillaume Aucher and Dov Gabbay and Réka Markovich},
  doi          = {10.1613/jair.1.17180},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1629-1672},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {From knowledge to action: Logics of permitted and obligatory announcements},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational machine ethics: A survey. <em>JAIR</em>, <em>82</em>, 1581-1628. (<a href='https://doi.org/10.1613/jair.1.16836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational Machine Ethics (CME) is an interdisciplinary field that integrates moral philosophy into an agent’s decision-making process, contributing to the broader domain of Artificial Intelligence Ethics. Technological advancements have transformed the world, where technology has become an integral part of society, progressively given more autonomy in making judgments within various domains in our lives. Inevitably, issues of ethics come into play in these judgments, making ethical decision-making in machines an increasingly critical problem to solve. This survey provides an overview of CME, highlighting the breadth of directions and the use of techniques within the field. We also provide some background on the ethical dimension before introducing our taxonomy used to categorise and detail the variety of existing approaches from a more technical perspective. Finally, we identify limitations in the research and suggest potential open challenges for future work.},
  archive      = {J_JAIR},
  author       = {Tammy Zhong and Yang Song and Raynaldio Limarga and Maurice Pagnucco},
  doi          = {10.1613/jair.1.16836},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1581-1628},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Computational machine ethics: A survey},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximal combinations of fairness definitions. <em>JAIR</em>, <em>82</em>, 1495-1579. (<a href='https://doi.org/10.1613/jair.1.16776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The so-called ‘Impossibility Theorem’ for fairness definitions is one of the more striking research results with both theoretical and practical consequences, as it states that satisfying certain combinations of fairness definitions is impossible. To date, this negative result has not yet been complemented with a positive one: a characterization of which combinations of fairness notions are possible. This work aims to fill this gap by identifying maximal sets of commonly used fairness definitions for binary classification that can be simultaneously satisfied. The fairness definitions used are demographic parity, equal opportunity, predictive equality, predictive parity, false omission rate parity, overall accuracy equality and treatment equality. We conclude that in total 12 maximal sets of these fairness definitions are possible, among which are seven combinations of two definitions, and five combinations of three definitions. Our findings also shed light on the practical relevance and utility of each of these 12 maximal fairness definitions in various scenarios, regarding the accuracy of the classifier and ratios of false positives and false negatives, considering the base rates.},
  archive      = {J_JAIR},
  author       = {MaryBeth Defrance and Tijl De Bie},
  doi          = {10.1613/jair.1.16776},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1495-1579},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Maximal combinations of fairness definitions},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An extensive empirical evaluation of inferring preconditions and effects of compound tasks in ground HTN planning problems. <em>JAIR</em>, <em>82</em>, 1407-1444. (<a href='https://doi.org/10.1613/jair.1.17279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HTN planning requires the decomposition of compound tasks into primitive and executable actions. In the currently most frequently used formalism, compound tasks lack explicit preconditions and effects. Those are, however, useful, e.g., for pruning techniques, heuristics, or the comprehension of domains. Previously, we introduced and formalized different kinds of inferred preconditions and effects of compound tasks based on their decomposition methods together with a complexity analysis. In this paper, we present an empirical evaluation of computing these inferred preconditions and effects using the IPC benchmark sets. Specifically, we analyze their frequency of occurrence and compare the performance of an approximation to the exact preconditions and effects. Our goal is to provide a comprehensive overview of the proposed techniques, enabling researchers to determine the extent to which they can be utilized in their given application.},
  archive      = {J_JAIR},
  author       = {Conny Olz and Alexander Lodemann and Benedikt Jutz and Mario Schmautz and Maximilian Borowiec and Susanne Biundo and Pascal Bercher},
  doi          = {10.1613/jair.1.17279},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1407-1444},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {An extensive empirical evaluation of inferring preconditions and effects of compound tasks in ground HTN planning problems},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbolic search for cost-optimal planning with expressive model extensions. <em>JAIR</em>, <em>82</em>, 1349–1405. (<a href='https://doi.org/10.1613/jair.1.16869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classical planning, the task is to derive a sequence of deterministic actions that changes the current fully-observable world state into one that satisfies a set of goal criteria. Algorithms for classical planning are domain-independent, i.e., they are not limited to a particular application and instead can be used to solve different types of reasoning problems. The main language for modeling such problems is the Planning Domain Definition Language (PDDL). Even though it provides many language features for expressing a wide range of planning tasks, most of today’s classical planners, especially optimal ones, support only a small subset of its features. The most widely supported fragment is lifted STRIPS plus types and action costs. While this fragment suffices to model some interesting planning tasks, using it to model more realistic problems often incurs a much higher modeling effort. Even if modeling is possible at all, solving the resulting tasks is often infeasible in practice, as the required encoding size increases exponentially. To address these issues, we show how to support more expressive modeling languages natively in optimal classical planning algorithms. Specifically, we focus on symbolic search, a state-of-the-art search algorithm that operates on sets of world states. We show how to extend symbolic search to support classical planning with conditional effects, axioms, and state-dependent action costs. All of these modeling features are expressive in the sense that compiling them away incurs a significant blow-up, so is it often necessary to support them natively. Except for blind (non-symbolic) search, our new symbolic search is the first optimal classical planning algorithm that supports these three modeling extensions in combination, and it even compares favorably to other state-of-the-art approaches that only support a subset of the extensions.},
  archive      = {J_JAIR},
  author       = {David Speck and Jendrik Seipp and Alvaro Torralba},
  doi          = {10.1613/jair.1.16869},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1349–1405},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Symbolic search for cost-optimal planning with expressive model extensions},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ConSCompF: Consistency-focused similarity comparison framework for generative large language models. <em>JAIR</em>, <em>82</em>, 1325-1347. (<a href='https://doi.org/10.1613/jair.1.17028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLM) are one of the most important discoveries in machine learning in recent years. LLM-based artificial intelligence (AI) assistants, such as ChatGPT, have consistently attracted attention from researchers, investors, and the general public, driving the rapid growth of this industry. With dozens of new LLMs released every month, it becomes quite challenging to differentiate between them, thereby creating a demand for new LLM comparison methods. In this research, the Consistency-focused Similarity Comparison Framework (ConSCompF) for generative large language models is proposed. It compares texts generated by two LLMs and produces a similarity score, indicating the overall degree of similarity between their responses. The main advantage of this framework is that it can operate on a small number of unlabeled data, such as chatbot instruction prompts, and does not require LLM developers to disclose any information about their product. To evaluate the efficacy of ConSCompF, two experiments aimed at identifying similarities between multiple LLMs are conducted. Additionally, these experiments examine the correlation between the similarity scores generated by ConSCompF and the differences in outputs produced by other benchmarking techniques, such as ROUGE-L. Finally, a series of few-shot LLM comparison experiments is conducted to evaluate the performance of ConSCompF in a few-shot LLM comparison scenario. The proposed framework can be used for calculating similarity matrices of multiple LLMs, which can be effectively visualized using principal component analysis (PCA). The outputs of ConSCompF may provide useful insights into data that might have been used during LLM training and help detect potential investment fraud attempts.},
  archive      = {J_JAIR},
  author       = {Alexey Karev and Dong Xu},
  doi          = {10.1613/jair.1.17028},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1325-1347},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {ConSCompF: Consistency-focused similarity comparison framework for generative large language models},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Laplace-HDC: Understanding the geometry of binary hyperdimensional computing. <em>JAIR</em>, <em>82</em>, 1293-1323. (<a href='https://doi.org/10.1613/jair.1.17688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the geometry of binary hyperdimensional computing (HDC), a computational scheme in which data are encoded using high-dimensional binary vectors. We establish a result about the similarity structure induced by the HDC binding operator and show that the Laplace kernel naturally arises in this setting, motivating our new encoding method Laplace-HDC, which improves upon previous methods. We describe how our results indicate limitations of binary HDC in encoding spatial information from images and discuss potential solutions, including using Haar convolutional features and the definition of a translation-equivariant HDC encoding. Several numerical experiments highlighting the improved accuracy of Laplace-HDC in contrast to alternative methods are presented. We also numerically study other aspects of the proposed framework, such as robustness and the underlying translation-equivariant encoding.},
  archive      = {J_JAIR},
  author       = {Saeid Pourmand and Wyatt D. Whiting and Alireza Aghasi and Nicholas F. Marshall},
  doi          = {10.1613/jair.1.17688},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1293-1323},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Laplace-HDC: Understanding the geometry of binary hyperdimensional computing},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving mutual information based feature selection by boosting unique relevance. <em>JAIR</em>, <em>82</em>, 1267-1292. (<a href='https://doi.org/10.1613/jair.1.17219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutual Information (MI) based feature selection makes use of MI to evaluate each feature and eventually shortlists a relevant feature subset, in order to address issues associated with high-dimensional datasets. Despite the effectiveness of MI in feature selection, we notice that many state-of-the-art algorithms disregard the so-called unique relevance (UR) of features, which is a necessary condition for the optimal feature subset. In our study of five representative MI based feature selection (MIBFS) algorithms, we find that all of them underperform as they ignore the UR of features and arrive at a suboptimal selected feature subset. We point out that the heart of the problem is that all these MIBFS algorithms follow the criterion of Maximize Relevance with Minimum Redundancy (MRwMR), which does not explicitly target UR. This motivates us to augment the existing criterion with the objective of boosting unique relevance (BUR), leading to a new criterion called MRwMR-BUR. Depending on the task being addressed, MRwMR-BUR has two variants, termed MRwMR-BUR-KSG and MRwMR-BUR-CLF, which estimate UR differently. MRwMR-BUR-KSG estimates UR via a nearest-neighbor based approach called the KSG estimator and is designed for three major tasks: (i) Classification Performance (i.e., higher classification accuracy). (ii) Feature Interpretability (i.e., a more precise selected feature subset for practitioners to explore the hidden relationship between features and labels). (iii) Classifier Generalization (i.e., the selected feature subset generalizes well to various classifiers). MRwMR-BUR-CLF estimates UR via a classifier based approach. It adapts UR to different classifiers, further improving the competitiveness of MRwMR-BUR for classification performance oriented tasks. The performance of MRwMR-BUR-KSG and MRwMR-BUR-CLF is validated via experiments using six public datasets and four popular classifiers. Specifically, as compared to MRwMR, the proposed MRwMR-BUR-KSG improves the test accuracy by 2% – 3% with 25% – 30% fewer features being selected, without increasing the algorithm complexity. MRwMR-BUR-CLF further improves the classification performance by 3.8% – 5.5% (relative to MRwMR), and it also outperforms three popular classifier dependent feature selection methods.},
  archive      = {J_JAIR},
  author       = {Shiyu Liu and Mehul Motani},
  doi          = {10.1613/jair.1.17219},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1267-1292},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Improving mutual information based feature selection by boosting unique relevance},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic alignment of malicious question based on contrastive semantic networks and data augmentation. <em>JAIR</em>, <em>82</em>, 1243-1266. (<a href='https://doi.org/10.1613/jair.1.16369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {&nbsp;The identification and filtration of malicious texts in social media environments represent a significant technical challenge aimed at protecting users from online violence and disinformation. This complexity stems from the diversity and innovativeness of social media texts, which include unique expressions and special sentence structures. Particularly, malicious texts in interrogative forms pose alignment challenges with traditional corpora due to existing methods' failure to exploit the text's deep global semantic representations. This issue is compounded by the scant research on Chinese texts, leading to inefficiencies in recognition accuracy. To mitigate these challenges, we introduce an innovative framework based on a Global Contrastive Semantic Network (GCSN), designed to enhance malicious text recognition efficiency and accuracy by deeply learning global semantic knowledge. It comprises an encoder for global semantic information modelling and a graph-matching network for semantic similarity evaluation between question pairs, enabling the accurate identification and filtering of malicious texts with complex structures. Furthermore, we introduce a semantic consistency-based data augmentation method (COMBINE), using real-world data to generate balanced positive and negative samples, enriching the dataset and enhancing the model's ability to distinguish semantic consistency through contrastive learning. Experimental validation on two Chinese datasets demonstrates our model's exceptional performance, affirming its applicationa value in social media malicious text recognition. Our code is available at https://github.com/Wxy13131313131/GCSN-COMBINE},
  archive      = {J_JAIR},
  author       = {Xinyan Wang and Jinshuo Liu and Juan Deng and Meng Wang and Qian Deng and Youcheng Yan and Lina Wang and Yunsong Ma and Jeff Z. Pan},
  doi          = {10.1613/jair.1.16369},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1243-1266},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Semantic alignment of malicious question based on contrastive semantic networks and data augmentation},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for belief-based programs and their verification. <em>JAIR</em>, <em>82</em>, 1205-1242. (<a href='https://doi.org/10.1613/jair.1.15796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Belief-based programming is a probabilistic extension of the GOLOG program family where every action and sensing result can be noisy and every test condition refers to the agent’s subjective beliefs. Inherited from GOLOG programs, the action-centered feature makes belief programs fairly suitable for high-level robot control under uncertainty. An important step before deploying such a program is to verify whether it satisfies certain properties. At least two problems exist in verifying such programs: how to formally specify program properties and what is the complexity of the verification problem. In this paper, we propose a formalism for belief programs based on a modal logic of actions and beliefs which allows us to conveniently express PCTL-like temporal properties. We also investigate the decidability and undecidability of the verification problem.},
  archive      = {J_JAIR},
  author       = {Daxin Liu and Gerhard Lakemeyer},
  doi          = {10.1613/jair.1.15796},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1205-1242},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A framework for belief-based programs and their verification},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rumor detection with adaptive data augmentation and adversarial training. <em>JAIR</em>, <em>82</em>, 1175-1204. (<a href='https://doi.org/10.1613/jair.1.16963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rumors are widely spread on social media, which has a negative impact on social stability. To address this problem, many rumor detection methods have been proposed. However, most existing methods overlook the potential impact of noise and adversarial attacks on their detection performance, which could compromise their effectiveness when applied in an unknown environment. To overcome these challenges and improve the framework robustness to noise and adversarial attacks, we propose a novel rumor detection framework with Adaptive Data Augmentation and Adversarial Training, named ADAAT. Our framework utilizes the adaptive data augmentation module to calculate the importance of edges and features and adaptively modify the less important among them with a greater probability. In addition, it contains a hard sample generation module which generates adversarial representations through adversarial training. These adversarial representations are treated as hard samples, which are utilized in contrastive learning to learn essential features, thereby improving the robustness of the framework. Our framework proves superiority in rumor detection tasks, increasing the accuracy by an average of 3.6%, 4.5% and 2.5% over the state-of-the-art methods on Twitter15, Twitter16 and PHEME, respectively. When the ADAAT framework is applied to attacked test data, the detection accuracy decreases by only 1.3%, 1.4%, and 1.2%. This paper appears in the AI &amp; Society Track.},
  archive      = {J_JAIR},
  author       = {Ying Wang and Fuyuan Ma and Zhaoqi Yang and Yaodi Zhu and Bo Yang and Pengfei Shen and Lei Yun},
  doi          = {10.1613/jair.1.16963},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {3},
  pages        = {1175-1204},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Rumor detection with adaptive data augmentation and adversarial training},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced recommendation systems with retrieval-augmented large language model. <em>JAIR</em>, <em>82</em>, 1147-1173. (<a href='https://doi.org/10.1613/jair.1.17809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have long struggled with challenges such as cold start and data sparsity, which can lead to poor recommendation performance. While previous approaches have attempted to address these issues by incorporating side information, they often introduce noise, lack flexibility for data expansion, and suffer from inconsistent data quality—factors that hinder accurate user preference inference and reduce recommendation performance. With the vast knowledge bases and advanced reasoning capabilities of large language models (LLMs), these models are particularly well-suited to supplement auxiliary information and capture implicit user intent. To address these challenges, we propose a novel framework, ER2ALM, which leverages the capabilities of LLMs enhanced by Retrieval-Augmented Generation (RAG) to improve recommendation outcomes. Our framework specifically addresses the challenges by flexibly and accurately augmenting auxiliary information and capturing users’ implicit preferences and interests. Additionally, to mitigate the risk of introducing noise, we incorporate a noise reduction strategy to ensure the reliability of the augmented information. Experimental validation on two real-world datasets demonstrates the efficacy of our approach, significantly enhancing both the accuracy and robustness of recommendations compared to state-of-the-art methods. This demonstrates the potential of our framework as a new paradigm for preference mining in recommendation systems.},
  archive      = {J_JAIR},
  author       = {Chuyuan Wei and Ke Duan and Shengda Zhuo and Hongchun Wang and Shuqiang Huang and Jie Liu},
  doi          = {10.1613/jair.1.17809},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {1147-1173},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Enhanced recommendation systems with retrieval-augmented large language model},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control by adding or deleting edges in graph-restricted weighted voting games. <em>JAIR</em>, <em>82</em>, 1077–1145. (<a href='https://doi.org/10.1613/jair.1.16940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-restricted weighted voting games generalize weighted voting games, a well-studied class of succinct simple games, by embedding them into a communication structure: a graph whose vertices are the players some of which are connected by edges. In such games, only sufficiently connected coalitions are taken into consideration for calculating the players' power indices. Focusing on the probabilistic Penrose-Banzhaf index (which Dubey and Shapley proposed in 1979 as an alternative to the normalized Penrose-Banzhaf index) and the Shapley-Shubik index, we study control of these games by an agent who can add edges to or delete edges from the given graph. We determine upper and lower bounds on how much such control actions can change a distinguished player's power and we study the computational complexity of the related problems.},
  archive      = {J_JAIR},
  author       = {Joanna Kaczmarek and Jörg Rothe and Nimrod Talmon},
  doi          = {10.1613/jair.1.16940},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {1077–1145},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Control by adding or deleting edges in graph-restricted weighted voting games},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical game theoretic analysis: A survey. <em>JAIR</em>, <em>82</em>, 1017-1076. (<a href='https://doi.org/10.1613/jair.1.16146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the empirical approach to game-theoretic analysis (EGTA), the model of the game comes not from declarative representation, but is derived by interrogation of a procedural description of the game environment. The motivation for developing this approach was to enable game-theoretic reasoning about strategic situations too complex for analytic specification and solution. Since its introduction over twenty years ago, EGTA has been applied to a wide range of multiagent domains, from auctions and markets to recreational games to cyber-security. We survey the extensive methodology developed for EGTA over the years, organized by the elemental subproblems comprising the EGTA process. We describe key EGTA concepts and techniques, and the questions at the frontier of EGTA research. Recent advances in machine learning are accelerating progress in EGTA, and promise to significantly expand our capacities for reasoning about complex game situations.},
  archive      = {J_JAIR},
  author       = {Michael P. Wellman and Karl Tuyls and Amy Greenwald},
  doi          = {10.1613/jair.1.16146},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {1017-1076},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Empirical game theoretic analysis: A survey},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new literature review of 3D object detection on autonomous driving. <em>JAIR</em>, <em>82</em>, 973-1015. (<a href='https://doi.org/10.1613/jair.1.15961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the realm of computer vision has experienced a significant surge in the importance of 3D object detection, especially in the context of autonomous driving. The capability to precisely identify the locations, dimensions, and types of key 3D objects surrounding an autonomous vehicle is crucial, rendering 3D object detection a vital component of any advanced perception system. This review delivers an extensive overview of the emerging technologies in 3D object detection tailored for autonomous vehicles. It encompasses a thorough examination, evaluation, and integration of the current research landscape in this domain, staying up-to-date with the latest advancements in 3D object detection and suggesting prospective avenues for future research. Our survey begins by clarifying the principles of 3D object detection and addressing its present challenges in the 3D domain. We then introduce three distinct taxonomies: camera-based, point cloudbased, and multi-modality-based approaches, providing a comprehensive classification of contemporary 3D object detection methodologies from various angles. Diverging from previous reviews, this paper also highlights and scrutinizes common issues and solutions for specific scenarios (such as pedestrian detection, lane lines, roadside cameras, and weather conditions) in object detection. Furthermore, we conduct an in-depth analysis and comparison of different classifications and methods, utilizing various datasets and experimental outcomes. Conclusively, we suggest several potential research directions, offering valuable insights for the ongoing evolution of 3D object detection technology. This review aims to serve as a comprehensive resource for researchers and practitioners in the field, guiding future innovations in 3D object detection for autonomous driving.},
  archive      = {J_JAIR},
  author       = {Peng Zhang and Xin Li and Xin Lin and Liang He},
  doi          = {10.1613/jair.1.15961},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {973-1015},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A new literature review of 3D object detection on autonomous driving},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Promoting the responsible development of speech datasets for mental health and neurological disorders research. <em>JAIR</em>, <em>82</em>, 937-972. (<a href='https://doi.org/10.1613/jair.1.16406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current research in machine learning and artificial intelligence is largely centered on modeling and performance evaluation, less so on data collection. However, recent research demonstrated that limitations and biases in data may negatively impact trustworthiness and reliability. These aspects are particularly impactful on sensitive domains such as mental health and neurological disorders, where speech data are used to develop AI applications for patients and healthcare providers. In this paper, we chart the landscape of available speech datasets for this domain, to highlight possible pitfalls and opportunities for improvement and promote fairness and diversity. We present a comprehensive list of desiderata for building speech datasets for mental health and neurological disorders and distill it into an actionable checklist focused on ethical concerns to foster more responsible research.},
  archive      = {J_JAIR},
  author       = {Eleonora Mancini and Ana Tanevska and Andrea Galassi and Alessio Galatolo and Federico Ruggeri and Paolo Torroni},
  doi          = {10.1613/jair.1.16406},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {937-972},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Promoting the responsible development of speech datasets for mental health and neurological disorders research},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A query-based constraint acquisition approach for enhanced precision in program precondition inference. <em>JAIR</em>, <em>82</em>, 901-936. (<a href='https://doi.org/10.1613/jair.1.16206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Program annotations in the form of function pre/postconditions play a crucial role in various software engineering and program verification tasks. However, the frequent unavailability of these annotations necessitates manual retrofitting. This paper shows how constraint acquisition, a learning framework derived from constraint programming and version space learning, can be extended for automatically inferring program preconditions. Our approach performs this inference in a black-box manner through automatic query generation and input-output observations of program executions. We introduce PreCA, the first-ever precondition inference framework leveraging query-based constraint acquisition. Notably, we specialize PreCA to handle memory-related preconditions on binary code, which pose significant challenges in data and information management systems. In contrast to prior black-box techniques, PreCA provides well-defined guarantees. Specifically, it employs a sound and complete method to generate preconditions consistent with all the observed input-output relationships of the program. Furthermore, empirical evaluations on our benchmark demonstrate that PreCA outperforms the results of state-of-the-art approaches, delivering comparable or superior results in 5s, as opposed to the 1-hour runtime of existing approaches on identical machines. We also present two successful use cases from the standard libc and the mbedtls cryptographic library. PreCA notably infers for the former one a more precise precondition than specified in the documentation.},
  archive      = {J_JAIR},
  author       = {Grégoire Menguy and Sébastien Bardin and Arnaud Gotlieb and Nadjib Lazaar},
  doi          = {10.1613/jair.1.16206},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {901-936},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A query-based constraint acquisition approach for enhanced precision in program precondition inference},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient ontology-mediated query answering: Extending DL-liteR and linear ELH. <em>JAIR</em>, <em>82</em>, 851-899. (<a href='https://doi.org/10.1613/jair.1.16401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The OWL 2 QL profile of the OWL 2 Web Ontology Language, based on the family of description logics called DL-Lite, is designed so that data stored in a standard relational database system (RDBMS) can be queried through an ontology via a rewriting mechanism, i.e., by rewriting the query into an SQL query that is then answered by the RDBMS system, without any changes to the data. In this paper we propose a language whose expressive power goes beyond that of DL-Lite while still allowing query answering via rewriting of queries into unions of conjunctive two-way regular path queries (UC2RPQs) instead of SQL queries. Our language is an extension of both OWL 2 QL and linear ELH: OWL 2 QL is extended by allowing qualified existential quantification on the left-hand side of concept inclusion axioms, and linear ELH by allowing inverses in role inclusion axioms. We identify a syntactic property of the extended language that guarantees UC2RPQ-rewritability. We propose a novel rewriting technique for conjunctive queries (CQs) under our ontology language that makes use of nondeterministic finite state automata. We show that CQ answering in our setting is NLOGSPACE-complete with respect to data complexity and NP-complete for combined complexity; we also show that answering instance queries is NLOGSPACE-complete for data complexity and in PTIME for combined complexity.},
  archive      = {J_JAIR},
  author       = {Mirko M. Dimartino and Peter T. Wood and Andrea Cali and Alexandra Poulovassilis},
  doi          = {10.1613/jair.1.16401},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {851-899},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Efficient ontology-mediated query answering: Extending DL-liteR and linear ELH},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Value preferences estimation and disambiguation in hybrid participatory systems. <em>JAIR</em>, <em>82</em>, 819-850. (<a href='https://doi.org/10.1613/jair.1.14958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding citizens’ values in participatory systems is crucial for citizen-centric policy-making. We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them. We focus on situations where a conflict is detected between participants’ choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants. We operationalize the philosophical stance that “valuing is deliberatively consequential.” That is, if a participant’s choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice. Thus, we propose and compare value preferences estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone. Then, we introduce a disambiguation strategy that combines Natural Language Processing and Active Learning to address the detected inconsistencies between choices and motivations. We evaluate the proposed methods on a dataset of a large-scale survey on energy transition. The results show that explicitly addressing inconsistencies between choices and motivations improves the estimation of an individual’s value preferences. The disambiguation strategy does not show substantial improvements when compared to similar baselines—however, we discuss how the novelty of the approach can open new research avenues and propose improvements to address the current limitations.},
  archive      = {J_JAIR},
  author       = {Enrico Liscio and Luciano C. Siebert and Catholijn M. Jonker and Pradeep K. Murukannaiah},
  doi          = {10.1613/jair.1.14958},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {819-850},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Value preferences estimation and disambiguation in hybrid participatory systems},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reasoning about decidability of strategic logics with imperfect information and perfect recall strategies. <em>JAIR</em>, <em>82</em>, 777-817. (<a href='https://doi.org/10.1613/jair.1.17237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In logics for strategic reasoning the main challenge is represented by their verification in contexts of imperfect information and perfect recall strategies. In this work, we show the combination of two techniques to approximate the verification of Alternating-time Temporal Logic (ATL∗ ) under imperfect information and perfect recall, which is known to be undecidable. Given a model M and a formula φ, we propose a verification procedure that generates sub-models of M in which each sub-model M′ satisfies a sub-formula φ′ of φ and the verification of φ′ in M′ is decidable. Then, we use CTL∗ model checking to provide a verification result of φ on M. In case the previous step does not give a final result, we exploit a runtime verification mechanism to provide some intermediate result. We prove that our procedure is sound and in the same complexity class of ATL∗ model checking under perfect information and perfect recall. Moreover, we present a tool that uses our procedure and provide experimental results.},
  archive      = {J_JAIR},
  author       = {Davide Catta and Angelo Ferrando and Vadim Malvone},
  doi          = {10.1613/jair.1.17237},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {777-817},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Reasoning about decidability of strategic logics with imperfect information and perfect recall strategies},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Against the achilles' heel: A survey on red teaming for generative models. <em>JAIR</em>, <em>82</em>, 687-775. (<a href='https://doi.org/10.1613/jair.1.17654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safe use as various vulnerabilities are exposed. In light of this, the field of red teaming is undergoing fast-paced growth, highlighting the need for a comprehensive survey covering the entire pipeline and addressing emerging topics. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the “searcher” framework to unify various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around LLM-based agents, overkill of harmless queries, and the balance between harmlessness and helpfulness. Warning: This paper contains examples that may be offensive, harmful, or biased.},
  archive      = {J_JAIR},
  author       = {Lizhi Lin and Honglin Mu and Zenan Zhai and Minghan Wang and Yuxia Wang and Renxi Wang and Junjie Gao and Yixuan Zhang and Wanxiang Che and Timothy Baldwin and Xudong Han and Haonan Li},
  doi          = {10.1613/jair.1.17654},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {687-775},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Against the achilles' heel: A survey on red teaming for generative models},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of SAT-based and ASP-based algorithms for inconsistency measurement. <em>JAIR</em>, <em>82</em>, 563-685. (<a href='https://doi.org/10.1613/jair.1.16888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present algorithms based on satisfiability problem (SAT) solving, as well as answer set programming (ASP), for solving the problem of determining inconsistency degrees in propositional knowledge bases. We consider six different inconsistency measures whose respective decision problems lie on the first level of the polynomial hierarchy. Namely, these are the contension, forgetting-based, hitting set, max-distance, sum-distance, and hit-distance inconsistency measures. In an extensive experimental analysis, we compare the SAT-based and ASP-based approaches with each other, as well as with a set of naive baseline algorithms. Our results demonstrate that, overall, both the SAT-based and the ASP-based approaches clearly outperform the naive baseline methods in terms of runtime. The results further show that the proposed ASP-based approaches perform superior to the SAT-based ones with regard to all six inconsistency measures considered in this work. Moreover, we conduct additional experiments to explain the aforementioned results in greater detail.},
  archive      = {J_JAIR},
  author       = {Isabelle Kuhlmann and Anna Gessler and Vivien Laszlo and Matthias Thimm},
  doi          = {10.1613/jair.1.16888},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {563-685},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Comparison of SAT-based and ASP-based algorithms for inconsistency measurement},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAC-chernoff bounds: Understanding generalization in the interpolation regime. <em>JAIR</em>, <em>82</em>, 503-562. (<a href='https://doi.org/10.1613/jair.1.17036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a distribution-dependent PAC-Chernoff bound that exhibits perfect tightness for interpolators, even within over-parameterized model classes. This bound, which relies on basic principles of Large Deviation Theory, defines a natural measure of the smoothness of a model, characterized by simple real-valued functions. Building upon this bound and the new concept of smoothness, we present an unified theoretical framework revealing why certain interpolators show an exceptional generalization, while others falter. We theoretically show how a wide spectrum of modern learning methodologies, encompassing techniques such as ℓ2-norm, distance-from-initialization and input-gradient regularization, in combination with data augmentation, invariant architectures, and over-parameterization, collectively guide the optimizer toward smoother interpolators, which, according to our theoretical framework, are the ones exhibiting superior generalization performance. This study shows that distribution-dependent bounds serve as a powerful tool to understand the complex dynamics behind the generalization capabilities of over-parameterized interpolators.},
  archive      = {J_JAIR},
  author       = {Andres R. Masegosa and Luis A. Ortega},
  doi          = {10.1613/jair.1.17036},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {503-562},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {PAC-chernoff bounds: Understanding generalization in the interpolation regime},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI reliance and decision quality: Fundamentals, interdependence, and the effects of interventions. <em>JAIR</em>, <em>82</em>, 471–501. (<a href='https://doi.org/10.1613/jair.1.15873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In AI-assisted decision-making, a central promise of having a human-in-the-loop is that they should be able to complement the AI system by overriding its wrong recommendations. In practice, however, we often see that humans cannot assess the correctness of AI recommendations and, as a result, adhere to wrong or override correct advice. Different ways of relying on AI recommendations have immediate, yet distinct, implications for decision quality. Unfortunately, reliance and decision quality are often inappropriately conflated in the current literature on AI-assisted decision-making. In this work, we disentangle and formalize the relationship between reliance and decision quality, and we characterize the conditions under which human-AI complementarity is achievable. To illustrate how reliance and decision quality relate to one another, we propose a visual framework and demonstrate its usefulness for interpreting empirical findings, including the effects of interventions like explanations. Overall, our research highlights the importance of distinguishing between reliance behavior and decision quality in AI-assisted decision-making.},
  archive      = {J_JAIR},
  author       = {Jakob Schoeffer and Johannes Jakubik and Michael Vössing and Niklas Kühl and Gerhard Satzger},
  doi          = {10.1613/jair.1.15873},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {471–501},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {AI reliance and decision quality: Fundamentals, interdependence, and the effects of interventions},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oracle-guided approach to constrained policy synthesis under uncertainty. <em>JAIR</em>, <em>82</em>, 433-469. (<a href='https://doi.org/10.1613/jair.1.16593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dealing with aleatoric uncertainty is key in many domains involving sequential decision making, e.g., planning in AI, network protocols, and symbolic program synthesis. This paper presents a general-purpose model-based framework to obtain policies operating in uncertain environments in a fully automated manner. The new concept of coloured Markov Decision Processes (MDPs) enables a succinct representation of a wide range of synthesis problems. A coloured MDP describes a collection of possible policy configurations with their structural dependencies. The framework covers the synthesis of (a) programmatic policies from probabilistic program sketches and (b) finite-state controllers representing policies for partially observable MDPs (POMDPs), including decentralised POMDPs as well as constrained POMDPs. We show that all these synthesis problems can be cast as exploring memoryless policies in the corresponding coloured MDP. This exploration uses a symbiosis of two orthogonal techniques: abstraction refinement—using a novel refinement method—and counter-example generalisation. Our approach outperforms dedicated synthesis techniques on some problems and significantly improves an earlier version of this framework.},
  archive      = {J_JAIR},
  author       = {Roman Andriushchenko and Milan Češka and Filip Macák and Sebastian Junges and Joost-Pieter Katoen},
  doi          = {10.1613/jair.1.16593},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {2},
  pages        = {433-469},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {An oracle-guided approach to constrained policy synthesis under uncertainty},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forgetting in abstract argumentation: Limits and possibilities. <em>JAIR</em>, <em>82</em>, 389–431. (<a href='https://doi.org/10.1613/jair.1.17149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of forgetting, which loosely speaking means losing, removing, or even hiding some variables, propositions, or formulas, has been extensively studied in the field of knowledge representation and reasoning for many major formalisms. In this article, we convey this topic to the highly active field of abstract argumentation. We provide an in-depth analysis of desirable syntactical and/or semantical properties of possible forgetting operators. In doing so, we included well-known logic programming conditions, such as strong persistence or strong invariance. Further, we argue that although abstract argumentation and logic programming are closely related, it is not possible to reduce forgetting in abstract argumentation to forgetting in logic programming in a straightforward manner. The analysis of desiderata, adapted to the specifics of abstract argumentation, includes implications among them, individual and collective satisfiability, and identifying inherent limits for a set of prominent semantics. Finally, we conduct a case study on stable semantics incorporating concrete forgetting operators.},
  archive      = {J_JAIR},
  author       = {Ringo Baumann and Matti Berthold and Dov Gabbay and Odinaldo Rodrigues},
  doi          = {10.1613/jair.1.17149},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {389–431},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Forgetting in abstract argumentation: Limits and possibilities},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CPM-based hierarchical text classification. <em>JAIR</em>, <em>82</em>, 367-388. (<a href='https://doi.org/10.1613/jair.1.16943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of natural language processing, hierarchical text classification (HTC) has emerged as a critical task for organizing and analyzing large volumes of text data. The previous work of HTC often falls short in fully leveraging the hierarchical structure of labels, resulting in suboptimal performance. In addition, it is difficult to capture nuanced relationships between parent and child classes, leading to inaccurate predictions and insufficient differentiation between sibling classes under the same parent category. This gap underscores the need for approaches that can more effectively integrate and utilize both hierarchical and corpus-specific information to improve HTC performance. To address these issues, Concept-aware Prompt Mechanism (CPM) is proposed for HTC, which leverages concept information embedded within hierarchical labels to enhance the representation of these labels and improve classification accuracy. Specifically, we introduce a concept initialization module that extracts concept features from hierarchical labels and a novel concept prompt template to integrate these features into the classification process. Our experimental results demonstrate that the proposed CPM achieves state-of-the-art performance on two benchmark datasets, improving Micro-F1 and Macro-F1 scores to varying degrees, particularly in datasets with complex label hierarchies.},
  archive      = {J_JAIR},
  author       = {Biqing Zeng and Yihao Peng and Jichen Yang and Peilin Hong and Junjie Liang},
  doi          = {10.1613/jair.1.16943},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {367-388},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {CPM-based hierarchical text classification},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CADS: A systematic literature review on the challenges of abstractive dialogue summarization. <em>JAIR</em>, <em>82</em>, 313-365. (<a href='https://doi.org/10.1613/jair.1.16674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although focused reviews have been conducted on this topic, there is a lack of comprehensive work that details the core challenges of dialogue summarization, unifies the differing understanding of the task, and aligns proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. Recent advances in training methods have led to substantial improvements in language-related challenges. However, challenges such as comprehension, factuality, and salience remain difficult and present significant research opportunities. We further investigate how these approaches are typically analyzed, covering the datasets for the subdomains of dialogue (e.g., meeting, customer service, and medical), the established automatic metrics (e.g., ROUGE), and common human evaluation approaches for assigning scores and evaluating annotator agreement. We observe that only a few datasets (i.e., SAMSum, AMI, DialogSum) are widely used. Despite its limitations, the ROUGE metric is the most commonly used, while human evaluation, considered the gold standard, is frequently reported without sufficient detail on the inter-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that our described challenge taxonomy remains relevant despite a potential shift in relevance and difficulty.},
  archive      = {J_JAIR},
  author       = {Frederic Kirstein and Jan Philip Wahle and Bela Gipp and Terry Ruas},
  doi          = {10.1613/jair.1.16674},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {313-365},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {CADS: A systematic literature review on the challenges of abstractive dialogue summarization},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TOMA: Computational theory of mind with abstractions for hybrid intelligence. <em>JAIR</em>, <em>82</em>, 285-311. (<a href='https://doi.org/10.1613/jair.1.16402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theory of mind refers to the human ability to reason about the mental content of other people, such as their beliefs, desires, and goals. People use their theory of mind to understand, reason about, and explain the behaviour of others. Having a theory of mind is especially useful when people collaborate, since individuals can then reason on what the other individual knows as well as what reasoning they might do. Similarly, hybrid intelligence systems, where AI agents collaborate with humans, necessitate that the agents reason about the humans using computational theory of mind. However, to try to keep track of all individual mental attitudes of all other individuals becomes (computationally) very difficult. Accordingly, this paper provides a mechanism for computational theory of mind based on abstractions of single beliefs into higher-level concepts. These abstractions can be triggered by social norms and roles. Their use in decision making serves as a heuristic to choose among interactions, thus facilitating collaboration. We provide a formalization based on epistemic logic to explain how various inferences enable such a computational theory of mind. Using examples from the medical domain, we demonstrate how having such a theory of mind enables an agent to interact with humans effectively and can increase the quality of the decisions humans make.},
  archive      = {J_JAIR},
  author       = {Emre Erdogan and Frank Dignum and Rineke Verbrugge and Pinar Yolum},
  doi          = {10.1613/jair.1.16402},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {285-311},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {TOMA: Computational theory of mind with abstractions for hybrid intelligence},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The complexity of pure maxmin strategies in two-player extensive-form games. <em>JAIR</em>, <em>82</em>, 241-284. (<a href='https://doi.org/10.1613/jair.1.16872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive-form games model strategic interaction between players, with an emphasis on the sequential aspect of decision-making: players take turns to move until an ending is reached, and receive a reward according to which ending is reached. We study the complexity of computing the pure maxmin value for such games, i.e. the maximum reward that a player can guarantee by playing a pure strategy, whatever their opponents play. We focus on two-player and two-team games and perform a systematic study depending on the degree of imperfect information of each player or team: perfect information, perfect recall, or perfect recall for each agent in a team (which we call multi-agent perfect recall). For each combination, we settle the complexity of deciding whether the maxmin value is at least as high as a given threshold. We give a complete complexity picture for three orthogonal settings: games represented explicitly by their game tree; games represented compactly by game rules, for which we propose two new formalisms; games in which the set of strategies of the opponents is restricted to a known set of opponent models.},
  archive      = {J_JAIR},
  author       = {Junkang Li and Bruno Zanuttini and Véronique Ventos},
  doi          = {10.1613/jair.1.16872},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {241-284},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The complexity of pure maxmin strategies in two-player extensive-form games},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence-based estimators for predictive performance in model monitoring. <em>JAIR</em>, <em>82</em>, 209-240. (<a href='https://doi.org/10.1613/jair.1.16709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After a machine learning model has been deployed into production, its predictive performance needs to be monitored. Ideally, such monitoring can be carried out by comparing the model’s predictions against ground truth labels. For this to be possible, the ground truth labels must be available relatively soon after inference. However, there are many use cases where ground truth labels are available only after a significant delay, or in the worst case, not at all. In such cases, directly monitoring the model’s predictive performance is impossible. Recently, novel methods for estimating the predictive performance of a model when ground truth is unavailable have been developed. Many of these methods leverage model confidence or other uncertainty estimates and are experimentally compared against a naive baseline method, namely Average Confidence (AC), which estimates model accuracy as the average of confidence scores for a given set of predictions. However, until now the theoretical properties of the AC method have not been properly explored. In this paper, we bridge this gap by reviewing the AC method and show that under certain general assumptions, it is an unbiased and consistent estimator of model accuracy. We also augment the AC method by deriving valid confidence intervals for the estimates it produces. These contributions elevate AC from an ad-hoc estimator to a principled one, encouraging its use in practice. We complement our theoretical results with empirical experiments, comparing AC against more complex estimators in a monitoring setting under covariate shift. We conduct our experiments using synthetic datasets, which allow for full control over the nature of the shift. Our experiments with binary classifiers show that the AC method is able to beat other estimators in many cases. However, the comparative quality of the different estimators is found to be heavily case-dependent.},
  archive      = {J_JAIR},
  author       = {Juhani Kivimäki and Jukka K. Nurminen and Jakub Białek and Wojtek Kuberski},
  doi          = {10.1613/jair.1.16709},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {209-240},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Confidence-based estimators for predictive performance in model monitoring},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized, decomposition-based observation scheduling for a large-scale satellite constellation. <em>JAIR</em>, <em>82</em>, 169-208. (<a href='https://doi.org/10.1613/jair.1.16997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying multi-satellite constellations for Earth observation requires coordinating potentially hundreds of spacecraft. With increasing onboard capability for autonomy, we can view the constellation as a multi-agent system (MAS) and employ decentralized scheduling solutions. We analyze the multi-satellite constellation observation scheduling problem (COSP) and formulate it as a distributed constraint optimization problem (DCOP). COSP requires scalable inter-agent communication and computation and consists of millions of variables which, coupled with the assumptions and structure, make existing DCOP algorithms inadequate for this application. We develop a scheduling approach that employs a carefully constructed heuristic, referred to as the Geometric Neighborhood Decomposition (GND) heuristic, to decompose the global DCOP into sub-problems to enable the application of DCOP techniques. We present the Neighborhood Stochastic Search (NSS) algorithm, a decentralized algorithm to effectively solve COSP and other large-scale distributed problems, using decomposition. The experiments confirm the efficacy of the approach against baseline algorithms, and we discuss the generality of NSS, GND, and properties of COSP to other domains.},
  archive      = {J_JAIR},
  author       = {Itai Zilberstein and Ananya Rao and Matthew Salis and Steve Chien},
  doi          = {10.1613/jair.1.16997},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {169-208},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Decentralized, decomposition-based observation scheduling for a large-scale satellite constellation},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A portfolio approach to massively parallel bayesian optimization. <em>JAIR</em>, <em>82</em>, 137-167. (<a href='https://doi.org/10.1613/jair.1.16868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One way to reduce the time of conducting optimization studies is to evaluate designs in parallel rather than just one-at-a-time. For expensive-to-evaluate black-boxes, batch versions of Bayesian optimization have been proposed. They work by building a surrogate model of the black-box to simultaneously select multiple designs via an infill criterion. Still, despite the increased availability of computing resources that enable large-scale parallelism, the strategies that work for selecting a few tens of parallel designs for evaluations become limiting due to the complexity of selecting more designs. It is even more crucial when the black-box is noisy, necessitating more evaluations as well as repeating experiments. Here we propose a scalable strategy that can keep up with massive batching natively, focused on the exploration/exploitation trade-off and a portfolio allocation. We compare the approach with related methods on noisy functions, for mono and multi-objective optimization tasks. These experiments show orders of magnitude speed improvements over existing methods with similar or better performance.},
  archive      = {J_JAIR},
  author       = {Mickael Binois and Nicholson Collier and Jonathan Ozik},
  doi          = {10.1613/jair.1.16868},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {137-167},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A portfolio approach to massively parallel bayesian optimization},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A selective under-sampling (SUS) method for imbalanced regression. <em>JAIR</em>, <em>82</em>, 111-136. (<a href='https://doi.org/10.1613/jair.1.16062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many mainstream machine learning approaches, such as neural networks, are not well suited to work with imbalanced data. Yet, this problem is frequently present in many real-world data sets. Collection methods are imperfect, and often not able to capture enough data in a specific range of the target variable. Furthermore, in certain tasks data is inherently imbalanced with many more normal events than edge cases. This problem is well studied within the classification context. However, only several methods have been proposed to deal with regression tasks. In addition, the proposed methods often do not yield good performance with high-dimensional data, while imbalanced high-dimensional regression has scarcely been explored. In this paper we present a selective under-sampling (SUS) algorithm for dealing with imbalanced regression and its iterative version SUSiter. We assessed this method on 15 regression data sets from different imbalanced domains, 5 synthetic high-dimensional imbalanced data sets and 2 more complex imbalanced age estimation image data sets. Our results suggest that SUS and SUSiter typically outperform other state-of-the-art techniques like SMOGN, or random under-sampling, when used with neural networks as learners.},
  archive      = {J_JAIR},
  author       = {Jovana Aleksic and Miguel García-Remesal},
  doi          = {10.1613/jair.1.16062},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {111-136},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A selective under-sampling (SUS) method for imbalanced regression},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prosociality in microtransit. <em>JAIR</em>, <em>82</em>, 77-110. (<a href='https://doi.org/10.1613/jair.1.16777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study (public) microtransit, a type of transportation service wherein a municipality offers point-to-point rides to residents, for a fixed, nominal fare. Microtransit exemplifies practical resource allocation problems that are often over-constrained in that not all ride requests (pickup and dropoff locations at specified times) can be satisfied or satisfied only by violating soft goals such as sustainability, and where economic signals (e.g., surge pricing) are not applicable—they would lead to unethical outcomes by effectively coercing poor people. We posit that instead of taking rider preferences as fixed, shaping them prosocially will lead to improved societal outcomes. Prosociality refers to an attitude or behavior that is intended to benefit others. This paper demonstrates a computational approach to prosociality in the context of a (public) microtransit service for disadvantaged riders. Prosociality appears as a willingness to adjust one’s pickup and dropoff times and locations to accommodate the schedules of others and to enable sharing rides (which increases the number of riders served with the same resources). This paper describes an interdisciplinary study of prosociality in microtransit between a transportation researcher, psychologists, a social scientist, and AI researchers. Our contributions are these: (1) empirical support for the viability of prosociality in microtransit (and constraints on it) through interviews with drivers and focus groups of riders; (2) a prototype mobile app demonstrating how our prosocial intervention can be combined with the transportation backend; (3) a reinforcement learning approach to model a rider and determine the best interventions to persuade that rider toward prosociality; and (4) a cognitive model of rider personas to enable evaluation of alternative interventions.},
  archive      = {J_JAIR},
  author       = {Divya Sundaresan and Akhira Watson and Eleni Bardaka and Crystal Chen Lee and Christopher B. Mayhorn and Munindar P. Singh},
  doi          = {10.1613/jair.1.16777},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {77-110},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Prosociality in microtransit},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical parallel algorithms for non-monotone submodular maximization. <em>JAIR</em>, <em>82</em>, 39-75. (<a href='https://doi.org/10.1613/jair.1.16801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Submodular maximization has found extensive applications in various domains within the field of artificial intelligence, including but not limited to machine learning, computer vision, and natural language processing. With the increasing size of datasets in these domains, there is a pressing need to develop efficient and parallelizable algorithms for submodular maximization. One measure of the parallelizability of a submodular maximization algorithm is its adaptive complexity, which indicates the number of sequential rounds where a polynomial number of queries to the objective function can be executed in parallel. In this paper, we study the problem of non-monotone submodular maximization subject to a knapsack constraint, and propose a low-adaptivity algorithm achieving an (1/8 − ϵ)- approximation with practical Õ(n) query complexity. Moreover, we also propose the first algorithm with both provable approximation ratio and sublinear adaptive complexity for the problem of non-monotone submodular maximization subject to a k-system constraint. As a by-product, we show that our two algorithms can also be applied to the special case of submodular maximization subject to a cardinality constraint, and achieve performance bounds comparable with those of state-of-the-art algorithms. Finally, the effectiveness of our algorithms is demonstrated by extensive experiments on real-world applications.},
  archive      = {J_JAIR},
  author       = {Shuang Cui and Kai Han and Jing Tang and Xueying Li and Aakas Zhiyuli and Hanxiao Li},
  doi          = {10.1613/jair.1.16801},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {39-75},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Practical parallel algorithms for non-monotone submodular maximization},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate implication for probabilistic graphical models. <em>JAIR</em>, <em>82</em>, 1-37. (<a href='https://doi.org/10.1613/jair.1.16467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graphical structure of Probabilistic Graphical Models (PGMs) represents the conditional independence (CI) relations that hold in the modeled distribution. Every separator in the graph represents a conditional independence relation in the distribution, making them the vehicle through which new conditional independence relations are inferred and verified. The notion of separation in graphs depends on whether the graph is directed (i.e., a Bayesian Network), or undirected (i.e., a Markov Network). The premise of all current systems-of-inference for deriving CIs in PGMs, is that the set of CIs used for the construction of the PGM hold exactly. In practice, algorithms for extracting the structure of PGMs from data discover approximate CIs that do not hold exactly in the distribution. In this paper, we ask how the error in this set propagates to the inferred CIs read off the graphical structure. More precisely, what guarantee can we provide on the inferred CI when the set of CIs that entailed it hold only approximately? It has recently been shown that in the general case, no such guarantee can be provided. In this work, we prove new negative and positive results concerning this problem. We prove that separators in undirected PGMs do not necessarily represent approximate CIs. In other words, no guarantee can be provided for CIs inferred from the structure of undirected graphs. We prove that such a guarantee exists for the set of CIs inferred in directed graphical models, making the d-separation algorithm a sound and complete system for inferring approximate CIs. We also establish improved approximation guarantees for independence relations derived from marginal and saturated CIs.},
  archive      = {J_JAIR},
  author       = {Batya Kenig},
  doi          = {10.1613/jair.1.16467},
  journal      = {Journal of Artificial Intelligence Research},
  month        = {1},
  pages        = {1-37},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Approximate implication for probabilistic graphical models},
  volume       = {82},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
