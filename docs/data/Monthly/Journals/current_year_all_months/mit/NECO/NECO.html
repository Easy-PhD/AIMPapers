<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco">NECO - 61</h2>
<ul>
<li><details>
<summary>
(2025). Feature normalization prevents collapse of noncontrastive learning dynamics. <em>NECO</em>, <em>37</em>(11), 2079--2124. (<a href='https://doi.org/10.1162/neco.a.27'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning is a self-supervised representation learning framework where two positive views generated through data augmentation are made similar by an attraction force in a data representation space, while a repulsive force makes them far from negative examples. Noncontrastive learning, represented by BYOL and SimSiam, gets rid of negative examples and improves computational efficiency. While learned representations may collapse into a single point due to the lack of the repulsive force at first sight, Tian et al. ( 2021 ) revealed through learning dynamics analysis that the representations can avoid collapse if data augmentation is sufficiently stronger than regularization. However, their analysis does not take into account commonly used feature normalization, a normalizer before measuring the similarity of representations, and hence excessively strong regularization may still collapse the dynamics, an unnatural behavior under the presence of feature normalization. Therefore, we extend the previous theory based on the L2 loss by considering the cosine loss instead, which involves feature normalization. We show that the cosine loss induces sixth-order dynamics (while the L2 loss induces a third-order one), in which a stable equilibrium dynamically emerges even if there are only collapsed solutions with given initial parameters. Thus, we offer a new understanding that feature normalization plays an important role in robustly preventing the dynamics collapse.},
  archive      = {J_NECO},
  author       = {Bao, Han},
  doi          = {10.1162/neco.a.27},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2079--2124},
  shortjournal = {Neural Comput.},
  title        = {Feature normalization prevents collapse of noncontrastive learning dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling higher-order interactions in sparse and heavy-tailed neural population activity. <em>NECO</em>, <em>37</em>(11), 2011--2078. (<a href='https://doi.org/10.1162/neco.a.35'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons process sensory stimuli efficiently, showing sparse yet highly variable ensemble spiking activity involving structured higher-order interactions. Notably, while neural populations are mostly silent, they occasionally exhibit highly synchronous activity, resulting in sparse and heavy-tailed spike-count distributions. However, its mechanistic origin—specifically, what types of nonlinear properties in individual neurons induce such population-level patterns—remains unclear. In this study, we derive sufficient conditions under which the joint activity of homogeneous binary neurons generates sparse and widespread population firing rate distributions in infinitely large networks. We then propose a subclass of exponential family distributions that satisfy this condition. This class incorporates structured higher-order interactions with alternating signs and shrinking magnitudes, along with a base-measure function that offsets distributional concentration, giving rise to parameter-dependent sparsity and heavy-tailed population firing rate distributions. Analysis of recurrent neural networks that recapitulate these distributions reveals that individual neurons possess threshold-like nonlinearity, followed by supralinear activation that jointly facilitates sparse and synchronous population activity. These nonlinear features resemble those in modern Hopfield networks, suggesting a connection between widespread population activity and the network’s memory capacity. The theory establishes sparse and heavy-tailed distributions for binary patterns, forming a foundation for developing energy-efficient spike-based learning machines.},
  archive      = {J_NECO},
  author       = {Rodríguez-Domínguez, Ulises and Shimazaki, Hideaki},
  doi          = {10.1162/neco.a.35},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {2011--2078},
  shortjournal = {Neural Comput.},
  title        = {Modeling higher-order interactions in sparse and heavy-tailed neural population activity},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Encoding of numerosity with robustness to object and scene identity in biologically inspired object recognition networks. <em>NECO</em>, <em>37</em>(11), 1975--2010. (<a href='https://doi.org/10.1162/neco.a.30'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Number sense, the ability to rapidly estimate object quantities in a visual scene without precise counting, is a crucial cognitive capacity found in humans and many other animals. Recent studies have identified artificial neurons tuned to numbers of items in biologically inspired vision models, even before training, and proposed these artificial neural networks as candidate models for the emergence of number sense in the brain. But real-world numerosity perception requires abstraction from the properties of individual objects and their contexts, unlike the simplified dot patterns used in previous studies. Using novel synthetically generated photorealistic stimuli, we show that deep convolutional neural networks optimized for object recognition encode information on approximate numerosity across diverse objects and scene types, which could be linearly read out from distributed activity patterns of later convolutional layers of different network architectures tested. In contrast, untrained networks with random weights failed to represent numerosity with abstractness to other visual properties and instead captured mainly low-level visual features. Our findings emphasize the importance of using complex, naturalistic stimuli to investigate mechanisms of number sense in both biological and artificial systems, and they suggest that the capacity of untrained networks to account for early-life numerical abilities should be reassessed. They further point to a possible, so far underappreciated, contribution of the brain's ventral visual pathway to representing numerosity with abstractness to other high-level visual properties.},
  archive      = {J_NECO},
  author       = {Chapalain, Thomas and Thirion, Bertrand and Eger, Evelyn},
  doi          = {10.1162/neco.a.30},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {1975--2010},
  shortjournal = {Neural Comput.},
  title        = {Encoding of numerosity with robustness to object and scene identity in biologically inspired object recognition networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A chimera model for motion anticipation in the retina and the primary visual cortex. <em>NECO</em>, <em>37</em>(11), 1925--1974. (<a href='https://doi.org/10.1162/neco.a.34'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a mean field model of the primary visual cortex (V1), connected to a realistic retina model, to study the impact of the retina on motion anticipation. We first consider the case where the retina does not itself provide anticipation—which is then only triggered by a cortical mechanism, the “anticipation by latency”—and unravel the effects of the retinal input amplitude, of stimulus features such as speed and contrast and of the size of cortical extensions and fiber conduction speed. Then we explore the changes in the cortical wave of anticipation when V1 is triggered by retina-driven anticipatory mechanisms: gain control and lateral inhibition by amacrine cells. Here, we show how retinal and cortical anticipation combine to provide an efficient processing where the simulated cortical response is in advance over the moving object that triggers this response, compensating the delays in visual processing.},
  archive      = {J_NECO},
  author       = {Emonet, Jérôme and Souihel, Selma and Chavane, Frédéric and Destexhe, Alain and Volo, Matteo di and Cessac, Bruno},
  doi          = {10.1162/neco.a.34},
  journal      = {Neural Computation},
  month        = {10},
  number       = {11},
  pages        = {1925--1974},
  shortjournal = {Neural Comput.},
  title        = {A chimera model for motion anticipation in the retina and the primary visual cortex},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential learning in the dense associative memory. <em>NECO</em>, <em>37</em>(10), 1877--1924. (<a href='https://doi.org/10.1162/neco.a.20'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential learning involves learning tasks in a sequence and proves challenging for most neural networks. Biological neural networks regularly succeed at the sequential learning challenge and are even capable of transferring knowledge both forward and backward between tasks. Artificial neural networks often totally fail to transfer performance between tasks and regularly suffer from degraded performance or catastrophic forgetting on previous tasks. Models of associative memory have been used to investigate the discrepancy between biological and artificial neural networks due to their biological ties and inspirations, of which the Hopfield network is the most studied model. The dense associative memory (DAM), or modern Hopfield network, generalizes the Hopfield network, allowing for greater capacities and prototype learning behaviors while still retaining the associative memory structure. We give a substantial review of the sequential learning space with particular respect to the Hopfield network and associative memories. We present the first published benchmarks of sequential learning in the DAM using various sequential learning techniques and analyze the results of the sequential learning to demonstrate previously unseen transitions in the behavior of the DAM. This letter also discusses the departure from biological plausibility that may affect the utility of the DAM as a tool for studying biological neural networks. We present our findings, including the effectiveness of a range of state-of-the-art sequential learning methods when applied to the DAM, and use these methods to further the understanding of DAM properties and behaviors.},
  archive      = {J_NECO},
  author       = {McAlister, Hayden and Robins, Anthony and Szymanski, Lech},
  doi          = {10.1162/neco.a.20},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1877--1924},
  shortjournal = {Neural Comput.},
  title        = {Sequential learning in the dense associative memory},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distance-based logistic matrix factorization. <em>NECO</em>, <em>37</em>(10), 1863--1876. (<a href='https://doi.org/10.1162/neco.a.25'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix factorization is a central paradigm in matrix completion and collaborative filtering. Low-rank factorizations have been extremely successful in reconstructing and generalizing high-dimensional data in a wide variety of machine learning problems from drug-target discovery to music recommendations. Virtually all proposed matrix factorization techniques use the dot product between latent factor vectors to reconstruct the original matrix. We propose a reformulation of the widely used logistic matrix factorization in which we use the distance, rather than the dot product, to measure similarity between latent factors. We show that this measure of similarity, which can draw nonlinear decision boundaries and respect triangle inequalities between points, has more expressive power and modeling capacity. The distance-based model implemented in Euclidean and hyperbolic space outperforms previous formulations of logistic matrix factorization on three different biological test problems with disparate structure and statistics. In particular, we show that a distance-based factorization (1) generalizes better to test data, (2) achieves optimal performance at lower factor space dimension, and (3) clusters data better in the latent factor space.},
  archive      = {J_NECO},
  author       = {Praturu, Anoop and Sharpee, Tatyana O.},
  doi          = {10.1162/neco.a.25},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1863--1876},
  shortjournal = {Neural Comput.},
  title        = {Distance-based logistic matrix factorization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid reweighting of sensory inputs and predictions in visual perception. <em>NECO</em>, <em>37</em>(10), 1853--1862. (<a href='https://doi.org/10.1162/neco.a.26'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A striking perceptual phenomenon has recently been described wherein people report seeing abrupt jumps in the location of a smoothly moving object (“position resets”). Here, we show that this phenomenon can be understood within the framework of recursive Bayesian estimation as arising from transient gain changes, temporarily prioritizing sensory input over predictive beliefs. From this perspective, position resets reveal a capacity for rapid adaptive precision weighting in human visual perception and offer a possible test bed within which to study the timing and flexibility of sensory gain control.},
  archive      = {J_NECO},
  author       = {Turner, William and Kwon, Oh-Sang and Kim, Minwoo J.B. and Hogendoorn, Hinze},
  doi          = {10.1162/neco.a.26},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1853--1862},
  shortjournal = {Neural Comput.},
  title        = {Rapid reweighting of sensory inputs and predictions in visual perception},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer models for signal processing: Scaled dot-product attention implements constrained filtering. <em>NECO</em>, <em>37</em>(10), 1839--1852. (<a href='https://doi.org/10.1162/neco.a.29'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable success of the transformer machine learning architecture for processing language sequences far exceeds the performance of classical signal processing methods. A unique component of transformer models is the scaled dot-product attention (SDPA) layer, which does not appear to have an analog in prior signal processing algorithms. Here, we show that SDPA operates using a novel principle that projects the current state estimate onto the space spanned by prior estimates. We show that SDPA, when used for causal recursive state estimation, implements constrained state estimation in circumstances where the constraint is unknown and may be time varying. Since constraints in high-dimensional space may represent the complex relationships that define nonlinear signals and models, this suggests that the SDPA layer and transformer models leverage constrained estimation to achieve their success. This also suggests that transformers and the SPDA layer could be a computational model for previously unexplained capabilities of human behavior.},
  archive      = {J_NECO},
  author       = {Sanger, Terence D.},
  doi          = {10.1162/neco.a.29},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1839--1852},
  shortjournal = {Neural Comput.},
  title        = {Transformer models for signal processing: Scaled dot-product attention implements constrained filtering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Firing rate models as associative memory: Synaptic design for robust retrieval. <em>NECO</em>, <em>37</em>(10), 1807--1838. (<a href='https://doi.org/10.1162/neco.a.28'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firing rate models are dynamical systems widely used in applied and theoretical neuroscience to describe local cortical dynamics in neuronal populations. By providing a macroscopic perspective of neuronal activity, these models are essential for investigating oscillatory phenomena, chaotic behavior, and associative memory processes. Despite their widespread use, the application of firing rate models to associative memory networks has received limited mathematical exploration, and most existing studies are focused on specific models. Conversely, well-established associative memory designs, such as Hopfield networks, lack key biologically relevant features intrinsic to firing rate models, including positivity and interpretable synaptic matrices reflecting the action of long-term potentiation and long-term depression. To address this gap, we propose a general framework that ensures the emergence of rescaled memory patterns as stable equilibria in the firing rate dynamics. Furthermore, we analyze the conditions under which the memories are locally and globally asymptotically stable, providing insights into constructing biologically plausible and robust systems for associative memory retrieval.},
  archive      = {J_NECO},
  author       = {Betteti, Simone and Baggio, Giacomo and Bullo, Francesco and Zampieri, Sandro},
  doi          = {10.1162/neco.a.28},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1807--1838},
  shortjournal = {Neural Comput.},
  title        = {Firing rate models as associative memory: Synaptic design for robust retrieval},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity deconstrains component limitations in sensorimotor control. <em>NECO</em>, <em>37</em>(10), 1783--1806. (<a href='https://doi.org/10.1162/neco.a.24'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human sensorimotor control is remarkably fast and accurate at the system level despite severe speed-accuracy trade-offs at the component level. The discrepancy between the contrasting speed-accuracy trade-offs at these two levels is a paradox. Meanwhile, speed accuracy trade-offs, heterogeneity, and layered architectures are ubiquitous in nerves, skeletons, and muscles, but they have only been studied in isolation using domain-specific models. In this article, we develop a mechanistic model for how component speed-accuracy trade-offs constrain sensorimotor control that is consistent with Fitts’ law for reaching. The model suggests that diversity among components deconstrains the limitations of individual components in sensorimotor control. Such diversity-enabled sweet spots (DESSs) are ubiquitous in nature, explaining why large heterogeneities exist in the components of biological systems and how natural selection routinely evolves systems with fast and accurate responses using imperfect components.},
  archive      = {J_NECO},
  author       = {Nakahira, Yorie and Liu, Quanying and Deng, Xiyu and Sejnowski, Terrence J. and Doyle, John C.},
  doi          = {10.1162/neco.a.24},
  journal      = {Neural Computation},
  month        = {9},
  number       = {10},
  pages        = {1783--1806},
  shortjournal = {Neural Comput.},
  title        = {Diversity deconstrains component limitations in sensorimotor control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast multigroup gaussian process factor models. <em>NECO</em>, <em>37</em>(9), 1709--1782. (<a href='https://doi.org/10.1162/neco.a.22'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes are now commonly used in dimensionality reduction approaches tailored to neuroscience, especially to describe changes in high-dimensional neural activity over time. As recording capabilities expand to include neuronal populations across multiple brain areas, cortical layers, and cell types, interest in extending gaussian process factor models to characterize multipopulation interactions has grown. However, the cubic runtime scaling of current methods with the length of experimental trials and the number of recorded populations (groups) precludes their application to large-scale multipopulation recordings. Here, we improve this scaling from cubic to linear in both trial length and group number. We present two approximate approaches to fitting multigroup gaussian process factor models based on inducing variables and the frequency domain. Empirically, both methods achieved orders of magnitude speed-up with minimal impact on statistical performance, in simulation and on neural recordings of hundreds of neurons across three brain areas. The frequency domain approach, in particular, consistently provided the greatest runtime benefits with the fewest trade-offs in statistical performance. We further characterize the estimation biases introduced by the frequency domain approach and demonstrate effective strategies to mitigate them. This work enables a powerful class of analysis techniques to keep pace with the growing scale of multipopulation recordings, opening new avenues for exploring brain function.},
  archive      = {J_NECO},
  author       = {Gokcen, Evren and Jasper, Anna I. and Kohn, Adam and Machens, Christian K. and Yu, Byron M.},
  doi          = {10.1162/neco.a.22},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1709--1782},
  shortjournal = {Neural Comput.},
  title        = {Fast multigroup gaussian process factor models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From function to implementation: Exploring degeneracy in evolved artificial agents. <em>NECO</em>, <em>37</em>(9), 1677--1708. (<a href='https://doi.org/10.1162/neco.a.19'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Degeneracy—the ability of different structures to perform the same function—is a fundamental feature of biological systems, contributing to their robustness and evolvability. However, the ubiquity of degeneracy in systems generated through adaptive processes complicates our understanding of the behavioral and computational strategies they employ. In this study, we investigated degeneracy in simple computational agents, known as Markov brains, trained using an artificial evolution algorithm to solve a spatial navigation task with or without associative memory. We analyzed degeneracy at three levels: behavioral, structural, and computational, with a focus on the last. Using information-theoretical concepts, Tononi et al. (1999) proposed a functional measure of degeneracy within biological networks. Here, we extended this approach to compare degeneracy across multiple networks. Using information-theoretical tools and causal analysis, we explored the computational strategies of the evolved agents and quantified their computational degeneracy. Our findings reveal a hierarchy of degenerate solutions, from varied behaviors to diverse structures and computations. Even agents with identical evolved behaviors demonstrated different underlying structures and computations. These results underscore the pervasive nature of degeneracy in neural networks, blurring the lines between the algorithmic and implementation levels in adaptive systems, and highlight the importance of advanced analytical tools to understand their complex behaviors.},
  archive      = {J_NECO},
  author       = {Hu, Zhimin and Cingiler, Oğulcan and Bohm, Clifford and Albantakis, Larissa},
  doi          = {10.1162/neco.a.19},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1677--1708},
  shortjournal = {Neural Comput.},
  title        = {From function to implementation: Exploring degeneracy in evolved artificial agents},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward generalized entropic sparsification for convolutional neural networks. <em>NECO</em>, <em>37</em>(9), 1648--1676. (<a href='https://doi.org/10.1162/neco.a.21'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are reported to be overparametrized. The search for optimal (minimal) and sufficient architecture is an NP-hard problem: if the network has N neurons, then there are 2 N possibilities to connect them—and therefore 2 N possible architectures and 2 N Boolean hyperparameters to encode them. Selecting the best possible hyperparameter out of them becomes an N p -hard problem since 2 N grows in N faster then any polynomial N p ⁠ . Here, we introduce a layer-by-layer data-driven pruning method based on the mathematical idea aiming at a computationally scalable entropic relaxation of the pruning problem. The sparse subnetwork is found from the pretrained (full) CNN using the network entropy minimization as a sparsity constraint. This allows deploying a numerically scalable algorithm with a sublinear scaling cost. The method is validated on several benchmarks (architectures): on MNIST (LeNet), resulting in sparsity of 55% to 84% and loss in accuracy of just 0.1% to 0.5%, and on CIFAR-10 (VGG-16, ResNet18), resulting in sparsity of 73% to 89% and loss in accuracy of 0.1% to 0.5%.},
  archive      = {J_NECO},
  author       = {Barisin, Tin and Horenko, Illia},
  doi          = {10.1162/neco.a.21},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1648--1676},
  shortjournal = {Neural Comput.},
  title        = {Toward generalized entropic sparsification for convolutional neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring stimulus information transfer between neural populations through the communication subspace. <em>NECO</em>, <em>37</em>(9), 1600--1647. (<a href='https://doi.org/10.1162/neco.a.17'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensory processing arises from the communication between neural populations across multiple brain areas. While the widespread presence of neural response variability shared throughout a neural population limits the amount of stimulus-related information those populations can accurately represent, how this variability affects the interareal communication of sensory information is unknown. We propose a mathematical framework to understand the impact of neural population response variability on sensory information transmission. We combine linear Fisher information, a metric connecting stimulus representation and variability, with the framework of communication subspaces, which suggests that functional mappings between cortical populations are low-dimensional relative to the space of population activity patterns. From this, we partition Fisher information depending on the alignment between the population covariance and the mean tuning direction projected onto the communication subspace or its orthogonal complement. We provide mathematical and numerical analyses of our proposed decomposition of Fisher information and examine theoretical scenarios that demonstrate how to leverage communication subspaces for flexible routing and gating of stimulus information. This work will provide researchers investigating interareal communication with a theoretical lens through which to understand sensory information transmission and guide experimental design.},
  archive      = {J_NECO},
  author       = {Weiss, Oren and Coen-Cagli, Ruben},
  doi          = {10.1162/neco.a.17},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1600--1647},
  shortjournal = {Neural Comput.},
  title        = {Measuring stimulus information transfer between neural populations through the communication subspace},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the architectural biases of the cortical microcircuit. <em>NECO</em>, <em>37</em>(9), 1551--1599. (<a href='https://doi.org/10.1162/neco.a.23'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cortex plays a crucial role in various perceptual and cognitive functions, driven by its basic unit, the canonical cortical microcircuit. Yet, we remain short of a framework that definitively explains the structure-function relationships of this fundamental neuroanatomical motif. To better understand how physical substrates of cortical circuitry facilitate their neuronal dynamics, we employ a computational approach using recurrent neural networks and representational analyses. We examine the differences manifested by the inclusion and exclusion of biologically motivated interareal laminar connections on the computational roles of different neuronal populations in the microcircuit of hierarchically related areas throughout learning. Our findings show that the presence of feedback connections correlates with the functional modularization of cortical populations in different layers and provides the microcircuit with a natural inductive bias to differentiate expected and unexpected inputs at initialization, which we justify mathematically. Furthermore, when testing the effects of training the microcircuit and its variants with a predictive-coding-inspired strategy, we find that doing so helps better encode noisy stimuli in areas of the cortex that receive feedback, all of which combine to suggest evidence for a predictive-coding mechanism serving as an intrinsic operative logic in the cortex.},
  archive      = {J_NECO},
  author       = {Balwani, Aishwarya and Cho, Suhee and Choi, Hannah},
  doi          = {10.1162/neco.a.23},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1551--1599},
  shortjournal = {Neural Comput.},
  title        = {Exploring the architectural biases of the cortical microcircuit},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic pathways of modulation enable robust task packing within neural dynamics. <em>NECO</em>, <em>37</em>(9), 1529--1550. (<a href='https://doi.org/10.1162/neco.a.18'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how brain networks learn and manage multiple tasks simultaneously is of interest in both neuroscience and artificial intelligence. In this regard, a recent research thread in theoretical neuroscience has focused on how recurrent neural network models and their internal dynamics enact multitask learning. To manage different tasks requires a mechanism to convey information about task identity or context into the model, which from a biological perspective may involve mechanisms of neuromodulation. In this study, we use recurrent network models to probe the distinctions between two forms of contextual modulation of neural dynamics, at the level of neuronal excitability and at the level of synaptic strength. We characterize these mechanisms in terms of their functional outcomes, focusing on their robustness to context ambiguity and, relatedly, their efficiency with respect to packing multiple tasks into finite-size networks. We also demonstrate the distinction between these mechanisms at the level of the neuronal dynamics they induce. Together, these characterizations indicate complementarity and synergy in how these mechanisms act, potentially over many timescales, toward enhancing the robustness of multitask learning.},
  archive      = {J_NECO},
  author       = {Vedovati, Giacomo and Ching, ShiNung},
  doi          = {10.1162/neco.a.18},
  journal      = {Neural Computation},
  month        = {8},
  number       = {9},
  pages        = {1529--1550},
  shortjournal = {Neural Comput.},
  title        = {Synergistic pathways of modulation enable robust task packing within neural dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crystal-LSBO: Automated design of de novo crystals with latent space bayesian optimization. <em>NECO</em>, <em>37</em>(8), 1505--1527. (<a href='https://doi.org/10.1162/neco_a_01767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative modeling of crystal structures is significantly challenged by the complexity of input data, which constrains the ability of these models to explore and discover novel crystals. This complexity often confines de novo design methodologies to merely small perturbations of known crystals and hampers the effective application of advanced optimization techniques. One such optimization technique, latent space Bayesian optimization (LSBO), has demonstrated promising results in uncovering novel objects across various domains, especially when combined with variational autoencoders (VAEs). Recognizing LSBO’s potential and the critical need for innovative crystal discovery, we introduce Crystal-LSBO, a de novo design framework for crystals specifically tailored to enhance explorability within LSBO frameworks. Crystal-LSBO employs multiple VAEs, each dedicated to a distinct aspect of crystal structure—lattice, coordinates, and chemical elements—orchestrated by an integrative model that synthesizes these components into a cohesive output. This setup not only streamlines the learning process but also produces explorable latent spaces thanks to the decreased complexity of the learning task for each model, enabling LSBO approaches to operate. Our study pioneers the use of LSBO for de novo crystal design, demonstrating its efficacy through optimization tasks focused mainly on formation energy values. Our results highlight the effectiveness of our methodology, offering a new perspective for de novo crystal discovery.},
  archive      = {J_NECO},
  author       = {Boyar, Onur and Gu, Yanheng and Tanaka, Yuji and Tonogai, Shunsuke and Itakura, Tomoya and Takeuchi, Ichiro},
  doi          = {10.1162/neco_a_01767},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1505--1527},
  shortjournal = {Neural Comput.},
  title        = {Crystal-LSBO: Automated design of de novo crystals with latent space bayesian optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear neural dynamics and classification accuracy in reservoir computing. <em>NECO</em>, <em>37</em>(8), 1469--1504. (<a href='https://doi.org/10.1162/neco_a_01770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir computing information processing based on untrained recurrent neural networks with random connections is expected to depend on the nonlinear properties of the neurons and the resulting oscillatory, chaotic, or fixed-point dynamics of the network. However, the degree of nonlinearity required and the range of suitable dynamical regimes for a given task remain poorly understood. To clarify these issues, we study the classification accuracy of a reservoir computer in artificial tasks of varying complexity while tuning both the neuron’s degree of nonlinearity and the reservoir’s dynamical regime. We find that even with activation functions of extremely reduced nonlinearity, weak recurrent interactions, and small input signals, the reservoir can compute useful representations. These representations, detectable only in higher-order principal components, make complex classification tasks linearly separable for the readout layer. Increasing the recurrent coupling leads to spontaneous dynamical behavior. Nevertheless, some input-related computations can “ride on top” of oscillatory or fixed-point attractors with little loss of accuracy, whereas chaotic dynamics often reduces task performance. By tuning the system through the full range of dynamical phases, we observe in several classification tasks that accuracy peaks at both the oscillatory/chaotic and chaotic/fixed-point phase boundaries, supporting the edge of chaos hypothesis. We also present a regression task with the opposite behavior. Our findings, particularly the robust weakly nonlinear operating regime, may offer new perspectives for both technical and biological neural networks with random connectivity.},
  archive      = {J_NECO},
  author       = {Metzner, Claus and Schilling, Achim and Maier, Andreas and Krauss, Patrick},
  doi          = {10.1162/neco_a_01770},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1469--1504},
  shortjournal = {Neural Comput.},
  title        = {Nonlinear neural dynamics and classification accuracy in reservoir computing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous-time neural networks can stably memorize random spike trains. <em>NECO</em>, <em>37</em>(8), 1439--1468. (<a href='https://doi.org/10.1162/neco_a_01768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter explores the capability of continuous-time recurrent neural networks to store and recall precisely timed scores of spike trains. We show (by numerical experiments) that this is indeed possible: within some range of parameters, any random score of spike trains (for all neurons in the network) can be robustly memorized and autonomously reproduced with stable accurate relative timing of all spikes, with probability close to one. We also demonstrate associative recall under noisy conditions. In these experiments, the required synaptic weights are computed offline to satisfy a template that encourages temporal stability.},
  archive      = {J_NECO},
  author       = {Aguettaz, Hugo and Loeliger, Hans-Andrea},
  doi          = {10.1162/neco_a_01768},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1439--1468},
  shortjournal = {Neural Comput.},
  title        = {Continuous-time neural networks can stably memorize random spike trains},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A categorical framework for quantifying emergent effects in network topology. <em>NECO</em>, <em>37</em>(8), 1409--1438. (<a href='https://doi.org/10.1162/neco_a_01766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergent effect is crucial to understanding the properties of complex systems that do not appear in their basic units, but there has been a lack of theories to measure and understand its mechanisms. In this letter, we consider emergence as a kind of structural nonlinearity, discuss a framework based on homological algebra that encodes emergence as the mathematical structure of cohomologies, and then apply it to network models to develop a computational measure of emergence. This framework ties the potential for emergent effects of a system to its network topology and local structures, paving the way to predict and understand the cause of emergent effects. We show in our numerical experiment that our measure of emergence correlates with the existing information-theoretic measure of emergence.},
  archive      = {J_NECO},
  author       = {Li, Johnny Jingze and Pardo-Guerra, Sebastian and Basu, Kalyan and Silva, Gabriel A.},
  doi          = {10.1162/neco_a_01766},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1409--1438},
  shortjournal = {Neural Comput.},
  title        = {A categorical framework for quantifying emergent effects in network topology},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive coding model detects novelty on different levels of representation hierarchy. <em>NECO</em>, <em>37</em>(8), 1373--1408. (<a href='https://doi.org/10.1162/neco_a_01769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novelty detection, also known as familiarity discrimination or recognition memory, refers to the ability to distinguish whether a stimulus has been seen before. It has been hypothesized that novelty detection can naturally arise within networks that store memory or learn efficient neural representation because these networks already store information on familiar stimuli. However, existing computational models supporting this idea have yet to reproduce the high capacity of human recognition memory, leaving the hypothesis in question. This article demonstrates that predictive coding, an established model previously shown to effectively support representation learning and memory, can also naturally discriminate novelty with high capacity. The predictive coding model includes neurons encoding prediction errors, and we show that these neurons produce higher activity for novel stimuli, so that the novelty can be decoded from their activity. Additionally, hierarchical predictive coding networks detect novelty at different levels of abstraction within the hierarchy, from low-level sensory features like arrangements of pixels to high-level semantic features like object identities. Overall, based on predictive coding, this article establishes a unified framework that brings together novelty detection, associative memory, and representation learning, demonstrating that a single model can capture these various cognitive functions.},
  archive      = {J_NECO},
  author       = {Li, T. Ed and Tang, Mufeng and Bogacz, Rafal},
  doi          = {10.1162/neco_a_01769},
  journal      = {Neural Computation},
  month        = {7},
  number       = {8},
  pages        = {1373--1408},
  shortjournal = {Neural Comput.},
  title        = {Predictive coding model detects novelty on different levels of representation hierarchy},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Excitation–Inhibition balance controls synchronization in a simple model of coupled phase oscillators. <em>NECO</em>, <em>37</em>(7), 1353--1372. (<a href='https://doi.org/10.1162/neco_a_01763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collective neuronal activity in the brain synchronizes during rest and desynchronizes during active behaviors, influencing cognitive processes such as memory consolidation, knowledge abstraction, and creative thinking. These states involve significant modulation of inhibition, which alters the excitation–inhibition (EI) balance of synaptic inputs. However, the influence of the EI balance on collective neuronal oscillation remains only partially understood. In this study, we introduce the EI-Kuramoto model, a modified version of the Kuramoto model, in which oscillators are categorized into excitatory and inhibitory groups with four distinct interaction types: excitatory–excitatory, excitatory–inhibitory, inhibitory–excitatory, and inhibitory–inhibitory. Numerical simulations identify three dynamic states—synchronized, bistable, and desynchronized—that can be controlled by adjusting the strength of the four interaction types. Theoretical analysis further demonstrates that the balance among these interactions plays a critical role in determining the dynamic states. This study provides valuable insights into the role of EI balance in synchronizing coupled oscillators and neurons.},
  archive      = {J_NECO},
  author       = {Kuroki, Satoshi and Mizuseki, Kenji},
  doi          = {10.1162/neco_a_01763},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1353--1372},
  shortjournal = {Neural Comput.},
  title        = {Excitation–Inhibition balance controls synchronization in a simple model of coupled phase oscillators},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rapid memory encoding in a spiking hippocampus circuit model. <em>NECO</em>, <em>37</em>(7), 1320--1352. (<a href='https://doi.org/10.1162/neco_a_01762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory is a complex process in the brain that involves the encoding, consolidation, and retrieval of previously experienced stimuli. The brain is capable of rapidly forming memories of sensory input. However, applying the memory system to real-world data poses challenges in practical implementation. This article demonstrates that through the integration of sparse spike pattern encoding scheme population tempotron, and various spike-timing-dependent plasticity (STDP) learning rules, supported by bounded weights and biological mechanisms, it is possible to rapidly form stable neural assemblies of external sensory inputs in a spiking neural circuit model inspired by the hippocampal structure. The model employs neural ensemble module and competitive learning strategies that mimic the pattern separation mechanism of the hippocampal dentate gyrus (DG) area to achieve nonoverlapping sparse coding. It also uses population tempotron and NMDA-(N-methyl-D-aspartate)mediated STDP to construct associative and episodic memories, analogous to the CA3 and CA1 regions. These memories are represented by strongly connected neural assemblies formed within just a few trials. Overall, this model offers a robust computational framework to accommodate rapid memory throughout the brain-wide memory process.},
  archive      = {J_NECO},
  author       = {Wang, Jiashuo and Yuan, Mengwen and Shen, Jiangrong and Chai, Qingao and Tang, Huajin},
  doi          = {10.1162/neco_a_01762},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1320--1352},
  shortjournal = {Neural Comput.},
  title        = {Rapid memory encoding in a spiking hippocampus circuit model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Closed-loop multistep planning. <em>NECO</em>, <em>37</em>(7), 1288--1319. (<a href='https://doi.org/10.1162/neco_a_01761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Living organisms interact with their surroundings in a closed-loop fashion, where sensory inputs dictate the initiation and termination of behaviors. Even simple animals are able to develop and execute complex plans, which has not yet been replicated in robotics using pure closed-loop input control. We propose a solution to this problem by defining a set of discrete and temporary closed-loop controllers, called “Tasks,” each representing a closed-loop behavior. We further introduce a supervisory module that has an innate understanding of physics and causality, through which it can simulate the execution of Task sequences over time and store the results in a model of the environment. On the basis of this model, plans can be made by chaining temporary closed-loop controllers. Our proposed framework was implemented for a robot and tested in two scenarios as proof of concept.},
  archive      = {J_NECO},
  author       = {Lafratta, Giulia and Porr, Bernd and Chandler, Christopher and Miller, Alice},
  doi          = {10.1162/neco_a_01761},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1288--1319},
  shortjournal = {Neural Comput.},
  title        = {Closed-loop multistep planning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision threshold learning in the basal ganglia for multiple alternatives. <em>NECO</em>, <em>37</em>(7), 1256--1287. (<a href='https://doi.org/10.1162/neco_a_01760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, researchers have integrated the historically separate, reinforcement learning (RL), and evidence-accumulation-to-bound approaches to decision modeling. A particular outcome of these efforts has been the RL-DDM, a model that combines value learning through reinforcement with a diffusion decision model (DDM). While the RL-DDM is a conceptually elegant extension of the original DDM, it faces a similar problem to the DDM in that it does not scale well to decisions with more than two options. Furthermore, in its current form, the RL-DDM lacks flexibility when it comes to adapting to rapid, context-cued changes in the reward environment. The question of how to best extend combined RL and DDM models so they can handle multiple choices remains open. Moreover, it is currently unclear how these algorithmic solutions should map to neurophysical processes in the brain, particularly in relation to so-called go/no-go-type models of decision making in the basal ganglia. Here, we propose a solution that addresses these issues by combining a previously proposed decision model based on the multichoice sequential probability ratio test (MSPRT), with a dual-pathway model of decision threshold learning in the basal ganglia region of the brain. Our model learns decision thresholds to optimize the trade-off between time cost and the cost of errors and so efficiently allocates the amount of time for decision deliberation. In addition, the model is context dependent and hence flexible to changes to the speed-accuracy trade-off (SAT) in the environment. Furthermore, the model reproduces the magnitude effect, a phenomenon seen experimentally in value-based decisions and is agnostic to the types of evidence and so can be used on perceptual decisions, value-based decisions, and other types of modeled evidence. The broader significance of the model is that it contributes to the active research area of how learning systems interact by linking the previously separate models of RL-DDM to dopaminergic models of motivation and risk taking in the basal ganglia, as well as scaling to multiple alternatives.},
  archive      = {J_NECO},
  author       = {Griffith, Thom and Baker, Sophie-Anne and Lepora, Nathan F.},
  doi          = {10.1162/neco_a_01760},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1256--1287},
  shortjournal = {Neural Comput.},
  title        = {Decision threshold learning in the basal ganglia for multiple alternatives},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on artificial neural networks in Human—Robot interaction. <em>NECO</em>, <em>37</em>(7), 1193--1255. (<a href='https://doi.org/10.1162/neco_a_01764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs) have shown great potential in enhancing human-robot interaction (HRI). ANNs are computational models inspired by the structure and function of biological neural networks in the brain, which can learn from examples and generalize to new situations. ANNs can be used to enable robots to interact with humans in a more natural and intuitive way by allowing them to recognize human gestures and expressions, understand natural language, and adapt to the environment. ANNs can also be used to improve robot autonomy, allowing robots to learn from their interactions with humans and to make more informed decisions. However, there are also challenges to using ANNs in HRI, including the need for large amounts of training data, issues with explainability, and the potential for bias. This review explores the current state of research on ANNs in HRI, highlighting both the opportunities and challenges of this approach and discussing potential directions for future research. The AI contribution involves applying ANNs to various aspects of HRI, while the application in engineering involves using ANNs to develop more interactive and intuitive robotic systems.},
  archive      = {J_NECO},
  author       = {Świetlicka, Aleksandra},
  doi          = {10.1162/neco_a_01764},
  journal      = {Neural Computation},
  month        = {6},
  number       = {7},
  pages        = {1193--1255},
  shortjournal = {Neural Comput.},
  title        = {A survey on artificial neural networks in Human—Robot interaction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank, high-order tensor completion via t- product-induced tucker (tTucker) decomposition. <em>NECO</em>, <em>37</em>(6), 1171--1192. (<a href='https://doi.org/10.1162/neco_a_01756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NECO},
  author       = {Li, Yaodong and Tan, Jun and Yang, Peilin and Zhou, Guoxu and Zhao, Qibin},
  doi          = {10.1162/neco_a_01756},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1171--1192},
  shortjournal = {Neural Comput.},
  title        = {Low-rank, high-order tensor completion via t- product-induced tucker (tTucker) decomposition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory states from almost nothing: Representing and computing in a nonassociative algebra. <em>NECO</em>, <em>37</em>(6), 1154--1170. (<a href='https://doi.org/10.1162/neco_a_01755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NECO},
  author       = {Reimann, Stefan},
  doi          = {10.1162/neco_a_01755},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1154--1170},
  shortjournal = {Neural Comput.},
  title        = {Memory states from almost nothing: Representing and computing in a nonassociative algebra},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural code translation with LIF neuron microcircuits. <em>NECO</em>, <em>37</em>(6), 1124--1153. (<a href='https://doi.org/10.1162/neco_a_01754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NECO},
  author       = {Karlsson, Ville and Kämäräinen, Joni},
  doi          = {10.1162/neco_a_01754},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1124--1153},
  shortjournal = {Neural Comput.},
  title        = {Neural code translation with LIF neuron microcircuits},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics and bifurcation structure of a mean-field model of adaptive exponential integrate-and-fire networks. <em>NECO</em>, <em>37</em>(6), 1102--1123. (<a href='https://doi.org/10.1162/neco_a_01758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NECO},
  author       = {Kusch, Lionel and Depannemaecker, Damien and Destexhe, Alain and Jirsa, Viktor},
  doi          = {10.1162/neco_a_01758},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1102--1123},
  shortjournal = {Neural Comput.},
  title        = {Dynamics and bifurcation structure of a mean-field model of adaptive exponential integrate-and-fire networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamics of continuous attractor neural networks with spike frequency adaptation. <em>NECO</em>, <em>37</em>(6), 1057--1101. (<a href='https://doi.org/10.1162/neco_a_01757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NECO},
  author       = {Li, Yujun and Chu, Tianhao and Wu, Si},
  doi          = {10.1162/neco_a_01757},
  journal      = {Neural Computation},
  month        = {5},
  number       = {6},
  pages        = {1057--1101},
  shortjournal = {Neural Comput.},
  title        = {Dynamics of continuous attractor neural networks with spike frequency adaptation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reformulation of RBM to unify linear and nonlinear dimensionality reduction. <em>NECO</em>, <em>37</em>(5), 1034--1055. (<a href='https://doi.org/10.1162/neco_a_01751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A restricted Boltzmann machine (RBM) is a two-layer neural network with shared weights and has been extensively studied for dimensionality reduction, data representation, and recommendation systems in the literature. The traditional RBM requires a probabilistic interpretation of the values on both layers and a Markov chain Monte Carlo (MCMC) procedure to generate samples during the training. The contrastive divergence (CD) is efficient to train the RBM, but its convergence has not been proved mathematically. In this letter, we investigate the RBM by using a maximum a posteriori (MAP) estimate and the expectation–maximization (EM) algorithm. We show that the CD algorithm without MCMC is convergent for the conditional likelihood object function. Another key contribution in this letter is the reformulation of the RBM into a deterministic model. Within the reformulated RBM, the CD algorithm without MCMC approximates the gradient descent (GD) method. This reformulated RBM can take the continuous scalar and vector variables on the nodes with flexibility in choosing the activation functions. Numerical experiments show its capability in both linear and nonlinear dimensionality reduction, and for the nonlinear dimensionality reduction, the reformulated RBM can outperform principal component analysis (PCA) by choosing the proper activation functions. Finally, we demonstrate its application to vector-valued nodes for the CIFAR-10 data set (color images) and the multivariate sequence data, which cannot be configured naturally with the traditional RBM. This work not only provides theoretical insights regarding the traditional RBM but also unifies the linear and nonlinear dimensionality reduction for scalar and vector variables.},
  archive      = {J_NECO},
  author       = {You, Jiangsheng and Liu, Chun-Yen},
  doi          = {10.1162/neco_a_01751},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {1034--1055},
  shortjournal = {Neural Comput.},
  title        = {Reformulation of RBM to unify linear and nonlinear dimensionality reduction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel data representation for training deep helmholtz machines. <em>NECO</em>, <em>37</em>(5), 1010--1033. (<a href='https://doi.org/10.1162/neco_a_01748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vast majority of the current research in the field of machine learning is done using algorithms with strong arguments pointing to their biological implausibility such as backpropagation, deviating the field’s focus from understanding its original organic inspiration to a compulsive search for optimal performance. Yet there have been a few proposed models that respect most of the biological constraints present in the human brain and are valid candidates for mimicking some of its properties and mechanisms. In this letter, we focus on guiding the learning of a biologically plausible generative model called the Helmholtz machine in complex search spaces using a heuristic based on the human image perception mechanism. We hypothesize that this model’s learning algorithm is not fit for deep networks due to its Hebbian-like local update rule, rendering it incapable of taking full advantage of the compositional properties that multilayer networks provide. We propose to overcome this problem by providing the network’s hidden layers with visual queues at different resolutions using multilevel data representation. The results on several image data sets showed that the model was able to not only obtain better overall quality but also a wider diversity in the generated images, corroborating our intuition that using our proposed heuristic allows the model to take more advantage of the network’s depth growth. More important, they show the unexplored possibilities underlying brain-inspired models and techniques.},
  archive      = {J_NECO},
  author       = {Ramos, Jose Miguel and Sa-Couto, Luis and Wichert, Andreas},
  doi          = {10.1162/neco_a_01748},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {1010--1033},
  shortjournal = {Neural Comput.},
  title        = {Multilevel data representation for training deep helmholtz machines},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed synaptic connection strength changes dynamics in a population firing rate model in response to continuous external stimuli. <em>NECO</em>, <em>37</em>(5), 987--1009. (<a href='https://doi.org/10.1162/neco_a_01749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network complexity allows for diverse neuronal population dynamics and realizes higherorder brain functions such as cognition and memory. Complexity is enhanced through chemical synapses with exponentially decaying conductance and greater variation in the neuronal connection strength due to synaptic plasticity. However, in the macroscopic neuronal population model, synaptic connections are often described by spike connections, and connection strengths within the population are assumed to be uniform. Thus, the effects of synaptic connections variation on network synchronization remain unclear. Based on recent advances in mean field theory for the quadratic integrate-and-fire neuronal network model, we introduce synaptic conductance and variation of connection strength into the excitatory and inhibitory neuronal population model and derive the macroscopic firing rate equations for faithful modeling. We then introduce a heuristic switching rule of the dynamic system with respect to the mean membrane potentials to avoid divergences in the computation caused by variations in the neuronal connection strength. We show that the switching rule agrees with the numerical computation of the microscopic level model. In the derived model, variations in synaptic conductance and connection strength strongly alter the stability of the solutions to the equations, which is related to the mechanism of synchronous firing. When we apply physiologically plausible values from layer 4 of the mammalian primary visual cortex to the derived model, we observe event-related desynchronization at the alpha and beta frequencies and event-related synchronization at the gamma frequency over a wide range of balanced external currents. Our results show that the introduction of complex synaptic connections and physiologically valid numerical values into the low-dimensional mean field equations reproduces dynamic changes such as eventrelated (de)synchronization, and provides a unique mathematical insight into the relationship between synaptic strength variation and oscillatory mechanism.},
  archive      = {J_NECO},
  author       = {Sugino, Masato and Tanaka, Mai and Shimba, Kenta and Kotani, Kiyoshi and Jimbo, Yasuhiko},
  doi          = {10.1162/neco_a_01749},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {987--1009},
  shortjournal = {Neural Comput.},
  title        = {Distributed synaptic connection strength changes dynamics in a population firing rate model in response to continuous external stimuli},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adding space to random networks of spiking neurons: A method based on scaling the network size. <em>NECO</em>, <em>37</em>(5), 957--986. (<a href='https://doi.org/10.1162/neco_a_01747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many spiking neural network models are based on random graphs that do not include topological and structural properties featured in real brain networks. To turn these models into spatial networks that describe the topographic arrangement of connections is a challenging task because one has to deal with neurons at the spatial network boundary. Addition of space may generate spurious network behavior like oscillations introduced by periodic boundary conditions or unbalanced neuronal spiking due to lack or excess of connections. Here, we introduce a boundary solution method for networks with added spatial extension that prevents the occurrence of spurious spiking behavior. The method is based on a recently proposed technique for scaling the network size that preserves first- and second-order statistics.},
  archive      = {J_NECO},
  author       = {Romaro, Cecilia and Piqueira, Jose Roberto Castilho and Roque, A. C.},
  doi          = {10.1162/neco_a_01747},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {957--986},
  shortjournal = {Neural Comput.},
  title        = {Adding space to random networks of spiking neurons: A method based on scaling the network size},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The leaky integrate-and-fire neuron is a change-point detector for compound poisson processes. <em>NECO</em>, <em>37</em>(5), 926--956. (<a href='https://doi.org/10.1162/neco_a_01750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animal nervous systems can detect changes in their environments within hundredths of a second. They do so by discerning abrupt shifts in sensory neural activity. Many neuroscience studies have employed change-point detection (CPD) algorithms to estimate such abrupt shifts in neural activity. But very few studies have suggested that spiking neurons themselves are online change-point detectors. We show that a leaky integrate-and-fire (LIF) neuron implements an online CPD algorithm for a compound Poisson process. We quantify the CPD performance of an LIF neuron under various regions of its parameter space. We show that CPD can be a recursive algorithm where the output of one algorithm can be input to another. Then we show that a simple feedforward network of LIF neurons can quickly and reliably detect very small changes in input spiking rates. For example, our network detects a 5% change in input rates within 20 ms on average, and false-positive detections are extremely rare. In a rigorous statistical context, we interpret the salient features of the LIF neuron: its membrane potential, synaptic weight, time constant, resting potential, action potentials, and threshold. Our results potentially generalize beyond the LIF neuron model and its associated CPD problem. If spiking neurons perform change-point detection on their inputs, then the electrophysiological properties of their membranes must be related to the spiking statistics of their inputs. We demonstrate one example of this relationship for the LIF neuron and compound Poisson processes and suggest how to test this hypothesis more broadly. Maybe neurons are not noisy devices whose action potentials must be averaged over time or populations. Instead, neurons might implement sophisticated, optimal, and online statistical algorithms on their inputs.},
  archive      = {J_NECO},
  author       = {Mani, Shivaram and Hurley, Paul and van Schaik, André and Monk, Travis},
  doi          = {10.1162/neco_a_01750},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {926--956},
  shortjournal = {Neural Comput.},
  title        = {The leaky integrate-and-fire neuron is a change-point detector for compound poisson processes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks. <em>NECO</em>, <em>37</em>(5), 886--925. (<a href='https://doi.org/10.1162/neco_a_01752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training spiking neural networks to approximate universal functions is essential for studying information processing in the brain and for neuromorphic computing. Yet the binary nature of spikes poses a challenge for direct gradient-based training. Surrogate gradients have been empirically successful in circumventing this problem, but their theoretical foundation remains elusive. Here, we investigate the relation of surrogate gradients to two theoretically well-founded approaches. On the one hand, we consider smoothed probabilistic models, which, due to the lack of support for automatic differentiation, are impractical for training multilayer spiking neural networks but provide derivatives equivalent to surrogate gradients for single neurons. On the other hand, we investigate stochastic automatic differentiation, which is compatible with discrete randomness but has not yet been used to train spiking neural networks. We find that the latter gives surrogate gradients a theoretical basis in stochastic spiking neural networks, where the surrogate derivative matches the derivative of the neuronal escape noise function. This finding supports the effectiveness of surrogate gradients in practice and suggests their suitability for stochastic spiking neural networks. However, surrogate gradients are generally not gradients of a surrogate loss despite their relation to stochastic automatic differentiation. Nevertheless, we empirically confirm the effectiveness of surrogate gradients in stochastic multilayer spiking neural networks and discuss their relation to deterministic networks as a special case. Our work gives theoretical support to surrogate gradients and the choice of a suitable surrogate derivative in stochastic spiking neural networks.},
  archive      = {J_NECO},
  author       = {Gygax, Julia and Zenke, Friedemann},
  doi          = {10.1162/neco_a_01752},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {886--925},
  shortjournal = {Neural Comput.},
  title        = {Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized time rescaling theorem for temporal point processes. <em>NECO</em>, <em>37</em>(5), 871--885. (<a href='https://doi.org/10.1162/neco_a_01745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal point processes are essential for modeling event dynamics in fields such as neuroscience and social media. The time rescaling theorem is commonly used to assess model fit by transforming a point process into a homogeneous Poisson process. However, this approach requires that the process be nonterminating and that complete (hence, unbounded) realizations are observed—conditions that are often unmet in practice. This article introduces a generalized time-rescaling theorem to address these limitations and, as such, facilitates a more widely applicable evaluation framework for point process models in diverse real-world scenarios.},
  archive      = {J_NECO},
  author       = {Zhang, Xi and Aravamudan, Akshay and Anagnostopoulos, Georgios C.},
  doi          = {10.1162/neco_a_01745},
  journal      = {Neural Computation},
  month        = {4},
  number       = {5},
  pages        = {871--885},
  shortjournal = {Neural Comput.},
  title        = {A generalized time rescaling theorem for temporal point processes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nearly optimal learning using sparse deep ReLU networks in regularized empirical risk minimization with lipschitz loss. <em>NECO</em>, <em>37</em>(4), 815--870. (<a href='https://doi.org/10.1162/neco_a_01742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a sparse deep ReLU network (SDRN) estimator of the regression function obtained from regularized empirical risk minimization with a Lipschitz loss function. Our framework can be applied to a variety of regression and classification problems. We establish novel nonasymptotic excess risk bounds for our SDRN estimator when the regression function belongs to a Sobolev space with mixed derivatives. We obtain a new, nearly optimal, risk rate in the sense that the SDRN estimator can achieve nearly the same optimal minimax convergence rate as one-dimensional nonparametric regression with the dimension involved in a logarithm term only when the feature dimension is fixed. The estimator has a slightly slower rate when the dimension grows with the sample size. We show that the depth of the SDRN estimator grows with the sample size in logarithmic order, and the total number of nodes and weights grows in polynomial order of the sample size to have the nearly optimal risk rate. The proposed SDRN can go deeper with fewer parameters to well estimate the regression and overcome the overfitting problem encountered by conventional feedforward neural networks.},
  archive      = {J_NECO},
  author       = {Huang, Ke and Liu, Mingming and Ma, Shujie},
  doi          = {10.1162/neco_a_01742},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {815--870},
  shortjournal = {Neural Comput.},
  title        = {Nearly optimal learning using sparse deep ReLU networks in regularized empirical risk minimization with lipschitz loss},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced EEG forecasting: A probabilistic deep learning approach. <em>NECO</em>, <em>37</em>(4), 793--814. (<a href='https://doi.org/10.1162/neco_a_01743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting electroencephalography (EEG) signals, that is, estimating future values of the time series based on the past ones, is essential in many real-time EEG-based applications, such as brain–computer interfaces and closed-loop brain stimulation. As these applications are becoming more and more common, the importance of a good prediction model has increased. Previously, the autoregressive model (AR) has been employed for this task; however, its prediction accuracy tends to fade quickly as multiple steps are predicted. We aim to improve on this by applying probabilistic deep learning to make robust longer-range forecasts. For this, we applied the probabilistic deep neural network model WaveNet to forecast resting-state EEG in theta- (4–7.5 Hz) and alpha-frequency (8–13 Hz) bands and compared it to the AR model. WaveNet reliably predicted EEG signals in both theta and alpha frequencies 150 ms ahead, with mean absolute errors of 1.0 ± 1.1 µV (theta) and 0.9 ± 1.1 µV (alpha), and outperformed the AR model in estimating the signal amplitude and phase. Furthermore, we found that the probabilistic approach offers a way of forecasting even more accurately while effectively discarding uncertain predictions. We demonstrate for the first time that probabilistic deep learning can be used to forecast resting-state EEG time series. In the future, the developed model can enhance the real-time estimation of brain states in brain–computer interfaces and brain stimulation protocols. It may also be useful for answering neuroscientific questions and for diagnostic purposes.},
  archive      = {J_NECO},
  author       = {Pankka, Hanna and Lehtinen, Jaakko and Ilmoniemi, Risto J. and Roine, Timo},
  doi          = {10.1162/neco_a_01743},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {793--814},
  shortjournal = {Neural Comput.},
  title        = {Enhanced EEG forecasting: A probabilistic deep learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge as a breaking of ergodicity. <em>NECO</em>, <em>37</em>(4), 742--792. (<a href='https://doi.org/10.1162/neco_a_01741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct a thermodynamic potential that can guide training of a generative model defined on a set of binary degrees of freedom. We argue that upon reduction in description, so as to make the generative model computationally manageable, the potential develops multiple minima. This is mirrored by the emergence of multiple minima in the free energy proper of the generative model itself. The variety of training samples that employ N binary degrees of freedom is ordinarily much lower than the size 2 N of the full phase space. The nonrepresented configurations, we argue, should be thought of as comprising a high-temperature phase separated by an extensive energy gap from the configurations composing the training set. Thus, training amounts to sampling a free energy surface in the form of a library of distinct bound states, each of which breaks ergodicity. The ergodicity breaking prevents escape into the near continuum of states comprising the high-temperature phase; thus, it is necessary for proper functionality. It may, however, have the side effect of limiting access to patterns that were underrepresented in the training set. At the same time, the ergodicity breaking within the library complicates both learning and retrieval. As a remedy, one may concurrently employ multiple generative models—up to one model per free energy minimum.},
  archive      = {J_NECO},
  author       = {He, Yang and Lubchenko, Vassiliy},
  doi          = {10.1162/neco_a_01741},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {742--792},
  shortjournal = {Neural Comput.},
  title        = {Knowledge as a breaking of ergodicity},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in wilson-cowan model for metapopulation. <em>NECO</em>, <em>37</em>(4), 701--741. (<a href='https://doi.org/10.1162/neco_a_01744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Wilson-Cowan model for metapopulation, a neural mass network model, treats different subcortical regions of the brain as connected nodes, with connections representing various types of structural, functional, or effective neuronal connectivity between these regions. Each region comprises interacting populations of excitatory and inhibitory cells, consistent with the standard Wilson-Cowan model. In this article, we show how to incorporate stable attractors into such a metapopulation model’s dynamics. By doing so, we transform the neural mass network model into a biologically inspired learning algorithm capable of solving different classification tasks. We test it on MNIST and Fashion MNIST in combination with convolutional neural networks, as well as on CIFAR-10 and TF-FLOWERS, and in combination with a transformer architecture (BERT) on IMDB, consistently achieving high classification accuracy.},
  archive      = {J_NECO},
  author       = {Marino, Raffaele and Buffoni, Lorenzo and Chicchi, Lorenzo and Patti, Francesca Di and Febbe, Diego and Giambagli, Lorenzo and Fanelli, Duccio},
  doi          = {10.1162/neco_a_01744},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {701--741},
  shortjournal = {Neural Comput.},
  title        = {Learning in wilson-cowan model for metapopulation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active inference and intentional behavior. <em>NECO</em>, <em>37</em>(4), 666--700. (<a href='https://doi.org/10.1162/neco_a_01738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in theoretical biology suggest that key definitions of basal cognition and sentient behavior may arise as emergent properties of in vitro cell cultures and neuronal networks. Such neuronal networks reorganize activity to demonstrate structured behaviors when embodied in structured information landscapes. In this article, we characterize this kind of self-organization through the lens of the free energy principle, that is, as self-evidencing. We do this by first discussing the definitions of reactive and sentient behavior in the setting of active inference, which describes the behavior of agents that model the consequences of their actions. We then introduce a formal account of intentional behavior that describes agents as driven by a preferred end point or goal in latent state-spaces. We then investigate these forms of (reactive, sentient, and intentional) behavior using simulations. First, we simulate the in vitro experiments, in which neuronal cultures modulated activity to improve gameplay in a simplified version of Pong by implementing nested, free energy minimizing processes. The simulations are then used to deconstruct the ensuing predictive behavior, leading to the distinction between merely reactive, sentient, and intentional behavior with the latter formalized in terms of inductive inference. This distinction is further studied using simple machine learning benchmarks (navigation in a grid world and the Tower of Hanoi problem) that show how quickly and efficiently adaptive behavior emerges under an inductive form of active inference.},
  archive      = {J_NECO},
  author       = {Friston, Karl J. and Salvatori, Tommaso and Isomura, Takuya and Tschantz, Alexander and Kiefer, Alex and Verbelen, Tim and Koudahl, Magnus and Paul, Aswin and Parr, Thomas and Razi, Adeel and Kagan, Brett J. and Buckley, Christopher L. and Ramstead, Maxwell J. D.},
  doi          = {10.1162/neco_a_01738},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {666--700},
  shortjournal = {Neural Comput.},
  title        = {Active inference and intentional behavior},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiking neuron-astrocyte networks for image recognition. <em>NECO</em>, <em>37</em>(4), 635--665. (<a href='https://doi.org/10.1162/neco_a_01740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From biological and artificial network perspectives, researchers have started acknowledging astrocytes as computational units mediating neural processes. Here, we propose a novel biologically inspired neuron-astrocyte network model for image recognition, one of the first attempts at implementing astrocytes in spiking neuron networks (SNNs) using a standard data set. The architecture for image recognition has three primary units: the preprocessing unit for converting the image pixels into spiking patterns, the neuron-astrocyte network forming bipartite (neural connections) and tripartite synapses (neural and astrocytic connections), and the classifier unit. In the astrocyte-mediated SNNs, an astrocyte integrates neural signals following the simplified Postnov model. It then modulates the integrate-and-fire (IF) neurons via gliotransmission, thereby strengthening the synaptic connections of the neurons within the astrocytic territory. We develop an architecture derived from a baseline SNN model for unsupervised digit classification. The spiking neuron-astrocyte networks (SNANs) display better network performance with an optimal variance-bias trade-off than SNN alone. We demonstrate that astrocytes promote faster learning, support memory formation and recognition, and provide a simplified network architecture. Our proposed SNAN can serve as a benchmark for future researchers on astrocyte implementation in artificial networks, particularly in neuromorphic systems, for its simplified design.},
  archive      = {J_NECO},
  author       = {Lorenzo, Jhunlyn and Rico-Gallego, Juan-Antonio and Binczak, Stéphane and Jacquir, Sabir},
  doi          = {10.1162/neco_a_01740},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {635--665},
  shortjournal = {Neural Comput.},
  title        = {Spiking neuron-astrocyte networks for image recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-sensitive processing in a model neocortical pyramidal cell with two sites of input integration. <em>NECO</em>, <em>37</em>(4), 588--634. (<a href='https://doi.org/10.1162/neco_a_01739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neocortical layer 5 thick-tufted pyramidal cells are prone to exhibiting burst firing on receipt of coincident basal and apical dendritic inputs. These inputs carry different information, with basal inputs coming from feedforward sensory pathways and apical inputs coming from diverse sources that provide context in the cortical hierarchy. We explore the information processing possibilities of this burst firing using computer simulations of a noisy compartmental cell model. Simulated data on stochastic burst firing due to brief, simultaneously injected basal and apical currents allow estimation of burst firing probability for different stimulus current amplitudes. Information-theory-based partial information decomposition (PID) is used to quantify the contributions of the apical and basal input streams to the information in the cell output bursting probability. Four different operating regimes are apparent, depending on the relative strengths of the input streams, with output burst probability carrying more or less information that is uniquely contributed by either the basal or apical input, or shared and synergistic information due to the combined streams. We derive and fit transfer functions for these different regimes that describe burst probability over the different ranges of basal and apical input amplitudes. The operating regimes can be classified into distinct modes of information processing, depending on the contribution of apical input to output bursting: apical cooperation, in which both basal and apical inputs are required to generate a burst; apical amplification, in which basal input alone can generate a burst but the burst probability is modulated by apical input; apical drive, in which apical input alone can produce a burst; and apical integration, in which strong apical or basal inputs alone, as well as their combination, can generate bursting. In particular, PID and the transfer function clarify that the apical amplification mode has the features required for contextually modulated information processing.},
  archive      = {J_NECO},
  author       = {Graham, Bruce P. and Kay, Jim W. and Phillips, William A.},
  doi          = {10.1162/neco_a_01739},
  journal      = {Neural Computation},
  month        = {3},
  number       = {4},
  pages        = {588--634},
  shortjournal = {Neural Comput.},
  title        = {Context-sensitive processing in a model neocortical pyramidal cell with two sites of input integration},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering dynamical equations of stochastic decision models using data-driven SINDy algorithm. <em>NECO</em>, <em>37</em>(3), 569--587. (<a href='https://doi.org/10.1162/neco_a_01736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision formation in perceptual decision making involves sensory evidence accumulation instantiated by the temporal integration of an internal decision variable toward some decision criterion or threshold, as described by sequential sampling theoretical models. The decision variable can be represented in the form of experimentally observable neural activities. Hence, elucidating the appropriate theoretical model becomes crucial to understanding the mechanisms underlying perceptual decision formation. Existing computational methods are limited to either fitting of choice behavioral data or linear model estimation from neural activity data. In this work, we made use of sparse identification of nonlinear dynamics (SINDy), a data-driven approach, to elucidate the deterministic linear and nonlinear components of often-used stochastic decision models within reaction time task paradigms. Based on the simulated decision variable activities of the models and assuming the noise coefficient term is known beforehand, SINDy, enhanced with approaches using multiple trials, could readily estimate the deterministic terms in the dynamical equations, choice accuracy, and decision time of the models across a range of signal-to-noise ratio values. In particular, SINDy performed the best using the more memory-intensive multi-trial approach while trial-averaging of parameters performed more moderately. The single-trial approach, although expectedly not performing as well, may be useful for real-time modeling. Taken together, our work offers alternative approaches for SINDy to uncover the dynamics in perceptual decision making and, more generally, for first-passage time problems.},
  archive      = {J_NECO},
  author       = {Lenfesty, Brendan and Bhattacharyya, Saugat and Wong-Lin, KongFatt},
  doi          = {10.1162/neco_a_01736},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {569--587},
  shortjournal = {Neural Comput.},
  title        = {Uncovering dynamical equations of stochastic decision models using data-driven SINDy algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradual domain adaptation via normalizing flows. <em>NECO</em>, <em>37</em>(3), 522--568. (<a href='https://doi.org/10.1162/neco_a_01734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard domain adaptation methods do not work well when a large gap exists between the source and target domains. Gradual domain adaptation is one of the approaches used to address the problem. It involves leveraging the intermediate domain, which gradually shifts from the source domain to the target domain. In previous work, it is assumed that the number of intermediate domains is large and the distance between adjacent domains is small; hence, the gradual domain adaptation algorithm, involving self-training with unlabeled data sets, is applicable. In practice, however, gradual self-training will fail because the number of intermediate domains is limited and the distance between adjacent domains is large. We propose the use of normalizing flows to deal with this problem while maintaining the framework of unsupervised domain adaptation. The proposed method learns a transformation from the distribution of the target domains to the gaussian mixture distribution via the source domain. We evaluate our proposed method by experiments using real-world data sets and confirm that it mitigates the problem we have explained and improves the classification performance.},
  archive      = {J_NECO},
  author       = {Sagawa, Shogo and Hino, Hideitsu},
  doi          = {10.1162/neco_a_01734},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {522--568},
  shortjournal = {Neural Comput.},
  title        = {Gradual domain adaptation via normalizing flows},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward a free-response paradigm of decision making in spiking neural networks. <em>NECO</em>, <em>37</em>(3), 481--521. (<a href='https://doi.org/10.1162/neco_a_01733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have attracted significant interest in the development of brain-inspired computing systems due to their energy efficiency and similarities to biological information processing. In contrast to continuous-valued artificial neural networks, which produce results in a single step, SNNs require multiple steps during inference to achieve a desired accuracy level, resulting in a burden in real-time response and energy efficiency. Inspired by the tradeoff between speed and accuracy in human and animal decision-making processes, which exhibit correlations among reaction times, task complexity, and decision confidence, an inquiry emerges regarding how an SNN model can benefit by implementing these attributes. Here, we introduce a theory of decision making in SNNs by untangling the interplay between signal and noise. Under this theory, we introduce a new learning objective that trains an SNN not only to make the correct decisions but also to shape its confidence. Numerical experiments demonstrate that SNNs trained in this way exhibit improved confidence expression, reduced trial-to-trial variability, and shorter latency to reach the desired accuracy. We then introduce a stopping policy that can stop inference in a way that further enhances the time efficiency of SNNs. The stopping time can serve as an indicator to whether a decision is correct, akin to the reaction time in animal behavior experiments. By integrating stochasticity into decision making, this study opens up new possibilities to explore the capabilities of SNNs and advance SNNs and their applications in complex decision-making scenarios where model performance is limited.},
  archive      = {J_NECO},
  author       = {Zhu, Zhichao and Qi, Yang and Lu, Wenlian and Wang, Zhigang and Cao, Lu and Feng, Jianfeng},
  doi          = {10.1162/neco_a_01733},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {481--521},
  shortjournal = {Neural Comput.},
  title        = {Toward a free-response paradigm of decision making in spiking neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving recall in sparse associative memories that use neurogenesis. <em>NECO</em>, <em>37</em>(3), 437--480. (<a href='https://doi.org/10.1162/neco_a_01732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation of future low-power neuromorphic solutions requires specialist spiking neural network (SNN) algorithms that are optimized for neuromorphic settings. One such algorithmic challenge is the ability to recall learned patterns from their noisy variants. Solutions to this problem may be required to memorize vast numbers of patterns based on limited training data and subsequently recall the patterns in the presence of noise. To solve this problem, previous work has explored sparse associative memory (SAM)—associative memory neural models that exploit the principle of sparse neural coding observed in the brain. Research into a subcategory of SAM has been inspired by the biological process of adult neurogenesis, whereby new neurons are generated to facilitate adaptive and effective lifelong learning. Although these neurogenesis models have been demonstrated in previous research, they have limitations in terms of recall memory capacity and robustness to noise. In this article, we provide a unifying framework for characterizing a type of SAM network that has been pretrained using a learning strategy that incorporated a simple neurogenesis model. Using this characterization, we formally define network topology and threshold optimization methods to empirically demonstrate greater than 10 4 times improvement in memory capacity compared to previous work. We show that these optimizations can facilitate the development of networks that have reduced interneuron connectivity while maintaining high recall efficacy. This paves the way for ongoing research into fast, effective, low-power realizations of associative memory on neuromorphic platforms.},
  archive      = {J_NECO},
  author       = {Warr, Katy and Hare, Jonathon and Thomas, David},
  doi          = {10.1162/neco_a_01732},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {437--480},
  shortjournal = {Neural Comput.},
  title        = {Improving recall in sparse associative memories that use neurogenesis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Replay as a basis for backpropagation through time in the brain. <em>NECO</em>, <em>37</em>(3), 403--436. (<a href='https://doi.org/10.1162/neco_a_01735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How episodic memories are formed in the brain is a continuing puzzle for the neuroscience community. The brain areas that are critical for episodic learning (e.g., the hippocampus) are characterized by recurrent connectivity and generate frequent offline replay events. The function of the replay events is a subject of active debate. Recurrent connectivity, computational simulations show, enables sequence learning when combined with a suitable learning algorithm such as backpropagation through time (BPTT). BPTT, however, is not biologically plausible. We describe here, for the first time, a biologically plausible variant of BPTT in a reversible recurrent neural network, R2N2, that critically leverages offline replay to support episodic learning. The model uses forward and backward offline replay to transfer information between two recurrent neural networks, a cache and a consolidator, that perform rapid one-shot learning and statistical learning, respectively. Unlike replay in standard BPTT, this architecture requires no artificial external memory store. This approach outperforms existing solutions like random feedback local online learning and reservoir network. It also accounts for the functional significance of hippocampal replay events. We demonstrate the R2N2 network properties using benchmark tests from computer science and simulate the rodent delayed alternation T-maze task.},
  archive      = {J_NECO},
  author       = {Cheng, Huzi and Brown, Joshua W.},
  doi          = {10.1162/neco_a_01735},
  journal      = {Neural Computation},
  month        = {2},
  number       = {3},
  pages        = {403--436},
  shortjournal = {Neural Comput.},
  title        = {Replay as a basis for backpropagation through time in the brain},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalization guarantees of gradient descent for shallow neural networks. <em>NECO</em>, <em>37</em>(2), 344--402. (<a href='https://doi.org/10.1162/neco_a_01725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made recently in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling. Here, network scaling corresponds to the normalization of the layers. In this article, we greatly extend the previous work (Lei et al., 2022 ; Richards & Kuzborskij, 2021 ) by conducting a comprehensive stability and generalization analysis of GD for two-layer and three-layer NNs. For two-layer NNs, our results are established under general network scaling, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of overparameterization. As a direct application of our general findings, we derive the excess risk rate of O ( 1 / n ) for GD in both two-layer and three-layer NNs. This sheds light on sufficient or necessary conditions for underparameterized and overparameterized NNs trained by GD to attain the desired risk rate of O ( 1 / n ) ⁠ . Moreover, we demonstrate that as the scaling factor increases or the network complexity decreases, less overparameterization is required for GD to achieve the desired error rates. Additionally, under a low-noise condition, we obtain a fast risk rate of O ( 1 / n ) for GD in both two-layer and three-layer NNs.},
  archive      = {J_NECO},
  author       = {Wang, Puyu and Lei, Yunwen and Wang, Di and Ying, Yiming and Zhou, Ding-Xuan},
  doi          = {10.1162/neco_a_01725},
  journal      = {Neural Computation},
  month        = {1},
  number       = {2},
  pages        = {344--402},
  shortjournal = {Neural Comput.},
  title        = {Generalization guarantees of gradient descent for shallow neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in associative networks through pavlovian dynamics. <em>NECO</em>, <em>37</em>(2), 311--343. (<a href='https://doi.org/10.1162/neco_a_01730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hebbian learning theory is rooted in Pavlov’s classical conditioning While mathematical models of the former have been proposed and studied in the past decades, especially in spin glass theory, only recently has it been numerically shown that it is possible to write neural and synaptic dynamics that mirror Pavlov conditioning mechanisms and also give rise to synaptic weights that correspond to the Hebbian learning rule. In this article we show that the same dynamics can be derived with equilibrium statistical mechanics tools and basic and motivated modeling assumptions. Then we show how to study the resulting system of coupled stochastic differential equations assuming the reasonable separation of neural and synaptic timescale. In particular, we analytically demonstrate that this synaptic evolution converges to the Hebbian learning rule in various settings and compute the variance of the stochastic process. Finally, drawing from evidence on pure memory reinforcement during sleep stages, we show how the proposed model can simulate neural networks that undergo sleep-associated memory consolidation processes, thereby proving the compatibility of Pavlovian learning with dreaming mechanisms.},
  archive      = {J_NECO},
  author       = {Lotito, Daniele and Aquaro, Miriam and Marullo, Chiara},
  doi          = {10.1162/neco_a_01730},
  journal      = {Neural Computation},
  month        = {1},
  number       = {2},
  pages        = {311--343},
  shortjournal = {Neural Comput.},
  title        = {Learning in associative networks through pavlovian dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast algorithm for the real-valued combinatorial pure exploration of the multi-armed bandit. <em>NECO</em>, <em>37</em>(2), 294--310. (<a href='https://doi.org/10.1162/neco_a_01728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the real-valued combinatorial pure exploration problem in the stochastic multi-armed bandit (R-CPE-MAB). We study the case where the size of the action set is polynomial with respect to the number of arms. In such a case, the R-CPE-MAB can be seen as a special case of the so-called transductive linear bandits. We introduce the combinatorial gap-based exploration (CombGapE) algorithm, whose sample complexity upper-bound-matches the lower bound up to a problem-dependent constant factor. We numerically show that the CombGapE algorithm outperforms existing methods significantly in both synthetic and real-world data sets.},
  archive      = {J_NECO},
  author       = {Nakamura, Shintaro and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01728},
  journal      = {Neural Computation},
  month        = {1},
  number       = {2},
  pages        = {294--310},
  shortjournal = {Neural Comput.},
  title        = {A fast algorithm for the real-valued combinatorial pure exploration of the multi-armed bandit},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalization analysis of transformers in distribution regression. <em>NECO</em>, <em>37</em>(2), 260--293. (<a href='https://doi.org/10.1162/neco_a_01726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, models based on the transformer architecture have seen widespread applications and have become one of the core tools in the field of deep learning. Numerous successful and efficient techniques, such as parameter-efficient fine-tuning and efficient scaling, have been proposed surrounding their applications to further enhance performance. However, the success of these strategies has always lacked the support of rigorous mathematical theory. To study the underlying mechanisms behind transformers and related techniques, we first propose a transformer learning framework motivated by distribution regression, with distributions being inputs, connect a two-stage sampling process with natural language processing, and present a mathematical formulation of the attention mechanism called attention operator. We demonstrate that by the attention operator, transformers can compress distributions into function representations without loss of information. Moreover, with the advantages of our novel attention operator, transformers exhibit a stronger capability to learn functionals with more complex structures than convolutional neural networks and fully connected networks. Finally, we obtain a generalization bound within the distribution regression framework. Throughout theoretical results, we further discuss some successful techniques emerging with large language models (LLMs), such as prompt tuning, parameter-efficient fine-tuning, and efficient scaling. We also provide theoretical insights behind these techniques within our novel analysis framework.},
  archive      = {J_NECO},
  author       = {Liu, Peilin and Zhou, Ding-Xuan},
  doi          = {10.1162/neco_a_01726},
  journal      = {Neural Computation},
  month        = {1},
  number       = {2},
  pages        = {260--293},
  shortjournal = {Neural Comput.},
  title        = {Generalization analysis of transformers in distribution regression},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the compressive power of autoencoders with linear and ReLU activation functions. <em>NECO</em>, <em>37</em>(2), 235--259. (<a href='https://doi.org/10.1162/neco_a_01729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we mainly study the depth and width of autoencoders consisting of rectified linear unit (ReLU) activation functions. An autoencoder is a layered neural network consisting of an encoder, which compresses an input vector to a lower-dimensional vector, and a decoder, which transforms the low-dimensional vector back to the original input vector exactly (or approximately). In a previous study, Melkman et al. ( 2023 ) studied the depth and width of autoencoders using linear threshold activation functions with binary input and output vectors. We show that similar theoretical results hold if autoencoders using ReLU activation functions with real input and output vectors are used. Furthermore, we show that it is possible to compress input vectors to one-dimensional vectors using ReLU activation functions, although the size of compressed vectors is trivially Ω(log n ) for autoencoders with linear threshold activation functions, where n is the number of input vectors. We also study the cases of linear activation functions. The results suggest that the compressive power of autoencoders using linear activation functions is considerably limited compared with those using ReLU activation functions.},
  archive      = {J_NECO},
  author       = {Sun, Liangjie and Wu, Chenyao and Ching, Wai-Ki and Akutsu, Tatsuya},
  doi          = {10.1162/neco_a_01729},
  journal      = {Neural Computation},
  month        = {1},
  number       = {2},
  pages        = {235--259},
  shortjournal = {Neural Comput.},
  title        = {On the compressive power of autoencoders with linear and ReLU activation functions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computation with sequences of assemblies in a model of the brain. <em>NECO</em>, <em>37</em>(1), 193--233. (<a href='https://doi.org/10.1162/neco_a_01720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even as machine learning exceeds human-level performance on many applications, the generality, robustness, and rapidity of the brain’s learning capabilities remain unmatched. How cognition arises from neural activity is the central open question in neuroscience, inextricable from the study of intelligence itself. A simple formal model of neural activity was proposed in Papadimitriou et al. ( 2020 ) and has been subsequently shown, through both mathematical proofs and simulations, to be capable of implementing certain simple cognitive operations via the creation and manipulation of assemblies of neurons. However, many intelligent behaviors rely on the ability to recognize, store, and manipulate temporal sequences of stimuli (planning, language, navigation, to list a few). Here we show that in the same model, sequential precedence can be captured naturally through synaptic weights and plasticity, and, as a result, a range of computations on sequences of assemblies can be carried out. In particular, repeated presentation of a sequence of stimuli leads to the memorization of the sequence through corresponding neural assemblies: upon future presentation of any stimulus in the sequence, the corresponding assembly and its subsequent ones will be activated, one after the other, until the end of the sequence. If the stimulus sequence is presented to two brain areas simultaneously, a scaffolded representation is created, resulting in more efficient memorization and recall, in agreement with cognitive experiments. Finally, we show that any finite state machine can be learned in a similar way, through the presentation of appropriate patterns of sequences. Through an extension of this mechanism, the model can be shown to be capable of universal computation. Taken together, these results provide a concrete hypothesis for the basis of the brain’s remarkable abilities to compute and learn, with sequences playing a vital role.},
  archive      = {J_NECO},
  author       = {Dabagia, Max and Papadimitriou, Christos H. and Vempala, Santosh S.},
  doi          = {10.1162/neco_a_01720},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {193--233},
  shortjournal = {Neural Comput.},
  title        = {Computation with sequences of assemblies in a model of the brain},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selective inference for change point detection by recurrent neural network. <em>NECO</em>, <em>37</em>(1), 160--192. (<a href='https://doi.org/10.1162/neco_a_01724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we investigate the quantification of the statistical reliability of detected change points (CPs) in time series using a recurrent neural network (RNN). Thanks to its flexibility, RNN holds the potential to effectively identify CPs in time series characterized by complex dynamics. However, there is an increased risk of erroneously detecting random noise fluctuations as CPs. The primary goal of this study is to rigorously control the risk of false detections by providing theoretically valid p -values to the CPs detected by RNN. To achieve this, we introduce a novel method based on the framework of selective inference (SI). SI enables valid inferences by conditioning on the event of hypothesis selection, thus mitigating bias from generating and testing hypotheses on the same data. In this study, we apply an SI framework to RNN-based CP detection, where characterizing the complex process of RNN selecting CPs is our main technical challenge. We demonstrate the validity and effectiveness of the proposed method through artificial and real data experiments.},
  archive      = {J_NECO},
  author       = {Shiraishi, Tomohiro and Miwa, Daiki and Le Duy, Vo Nguyen and Takeuchi, Ichiro},
  doi          = {10.1162/neco_a_01724},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {160--192},
  shortjournal = {Neural Comput.},
  title        = {Selective inference for change point detection by recurrent neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relating human Error–Based learning to modern deep RL algorithms. <em>NECO</em>, <em>37</em>(1), 128--159. (<a href='https://doi.org/10.1162/neco_a_01721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human error–based learning, the size and direction of a scalar error (i.e., the “directed error”) are used to update future actions. Modern deep reinforcement learning (RL) methods perform a similar operation but in terms of scalar rewards. Despite this similarity, the relationship between action updates of deep RL and human error–based learning has yet to be investigated. Here, we systematically compare the three major families of deep RL algorithms to human error–based learning. We show that all three deep RL approaches are qualitatively different from human error–based learning, as assessed by a mirror-reversal perturbation experiment. To bridge this gap, we developed an alternative deep RL algorithm inspired by human error–based learning, model-based deterministic policy gradients (MB-DPG). We showed that MB-DPG captures human error–based learning under mirror-reversal and rotational perturbations and that MB-DPG learns faster than canonical model-free algorithms on complex arm-based reaching tasks, while being more robust to (forward) model misspecification than model-based RL.},
  archive      = {J_NECO},
  author       = {Garibbo, Michele and Ludwig, Casimir J. H. and Lepora, Nathan F. and Aitchison, Laurence},
  doi          = {10.1162/neco_a_01721},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {128--159},
  shortjournal = {Neural Comput.},
  title        = {Relating human Error–Based learning to modern deep RL algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bounded rational decision networks with belief propagation. <em>NECO</em>, <em>37</em>(1), 76--127. (<a href='https://doi.org/10.1162/neco_a_01719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex information processing systems that are capable of a wide variety of tasks, such as the human brain, are composed of specialized units that collaborate and communicate with each other. An important property of such information processing networks is locality: there is no single global unit controlling the modules, but information is exchanged locally. Here, we consider a decision-theoretic approach to study networks of bounded rational decision makers that are allowed to specialize and communicate with each other. In contrast to previous work that has focused on feedforward communication between decision-making agents, we consider cyclical information processing paths allowing for back-and-forth communication. We adapt message-passing algorithms to suit this purpose, essentially allowing for local information flow between units and thus enabling circular dependency structures. We provide examples that show how repeated communication can increase performance given that each unit’s information processing capability is limited and that decision-making systems with too few or too many connections and feedback loops achieve suboptimal utility.},
  archive      = {J_NECO},
  author       = {Schmid, Gerrit and Gottwald, Sebastian and Braun, Daniel A.},
  doi          = {10.1162/neco_a_01719},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {76--127},
  shortjournal = {Neural Comput.},
  title        = {Bounded rational decision networks with belief propagation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Realizing synthetic active inference agents, part II: Variational message updates. <em>NECO</em>, <em>37</em>(1), 38--75. (<a href='https://doi.org/10.1162/neco_a_01713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The free energy principle (FEP) describes (biological) agents as minimizing a variational free energy (FE) with respect to a generative model of their environment. Active inference (AIF) is a corollary of the FEP that describes how agents explore and exploit their environment by minimizing an expected FE objective. In two related papers, we describe a scalable, epistemic approach to synthetic AIF by message passing on free-form Forney-style factor graphs (FFGs). A companion paper (part I of this article; Koudahl et al., 2023 ) introduces a constrained FFG (CFFG) notation that visually represents (generalized) FE objectives for AIF. This article (part II) derives message-passing algorithms that minimize (generalized) FE objectives on a CFFG by variational calculus. A comparison between simulated Bethe and generalized FE agents illustrates how the message-passing approach to synthetic AIF induces epistemic behavior on a T-maze navigation task. Extension of the T-maze simulation to learning goal statistics and a multiagent bargaining setting illustrate how this approach encourages reuse of nodes and updates in alternative settings. With a full message-passing account of synthetic AIF agents, it becomes possible to derive and reuse message updates across models and move closer to industrial applications of synthetic AIF.},
  archive      = {J_NECO},
  author       = {van de Laar, Thijs and Koudahl, Magnus and de Vries, Bert},
  doi          = {10.1162/neco_a_01713},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {38--75},
  shortjournal = {Neural Comput.},
  title        = {Realizing synthetic active inference agents, part II: Variational message updates},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing with residue numbers in high-dimensional representation. <em>NECO</em>, <em>37</em>(1), 1--37. (<a href='https://doi.org/10.1162/neco_a_01723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce residue hyperdimensional computing, a computing framework that unifies residue number systems with an algebra defined over random, high-dimensional vectors. We show how residue numbers can be represented as high-dimensional vectors in a manner that allows algebraic operations to be performed with component-wise, parallelizable operations on the vector elements. The resulting framework, when combined with an efficient method for factorizing high-dimensional vectors, can represent and operate on numerical values over a large dynamic range using resources that scale only logarithmically with the range, a vast improvement over previous methods. It also exhibits impressive robustness to noise. We demonstrate the potential for this framework to solve computationally difficult problems in visual perception and combinatorial optimization, showing improvement over baseline methods. More broadly, the framework provides a possible account for the computational operations of grid cells in the brain, and it suggests new machine learning architectures for representing and manipulating numerical data.},
  archive      = {J_NECO},
  author       = {Kymn, Christopher J. and Kleyko, Denis and Frady, E. Paxon and Bybee, Connor and Kanerva, Pentti and Sommer, Friedrich T. and Olshausen, Bruno A.},
  doi          = {10.1162/neco_a_01723},
  journal      = {Neural Computation},
  month        = {1},
  number       = {1},
  pages        = {1--37},
  shortjournal = {Neural Comput.},
  title        = {Computing with residue numbers in high-dimensional representation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
