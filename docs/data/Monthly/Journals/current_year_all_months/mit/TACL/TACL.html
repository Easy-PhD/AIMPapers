<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TACL</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tacl">TACL - 54</h2>
<ul>
<li><details>
<summary>
(2025). Active knowledge structuring for large language models in materials science text mining. <em>TACL</em>, <em>13</em>, 1186-1203. (<a href='https://doi.org/10.1162/TACL.a.36'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) offer a promising alternative to traditional Materials Science Text Mining (MSTM) by reducing the need for extensive data labeling and fine-tuning. However, existing zero-/few-shot methods still face limitations in aligning with personalized needs in scientific discovery. To address this, we propose ClassMATe, an active knowledge structuring approach for MSTM. Specifically, we first propose a class definition stylization method to structure knowledge, enabling explicit clustering of latent material knowledge in LLMs for enhanced inference. To align with the scientists’ needs, we propose an active needs refining strategy that iteratively clarifies needs by learning from uncertainty-aware hard samples of LLMs, further refining the knowledge structuring. Extensive experiments on seven tasks and eight datasets show that ClassMATe, as a plug-and-play method, achieves performance comparable to supervised learning without requiring fine-tuning or extra knowledge base, highlighting the potential to bridge the gap between LLMs’ latent knowledge and real-world scientific applications. 1},
  archive      = {J_TACL},
  author       = {Zhang, Xin and Yuan, Jingling and Zhang, Peiliang and Liu, Jia and Li, Lin},
  doi          = {10.1162/TACL.a.36},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1186-1203},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Active knowledge structuring for large language models in materials science text mining},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explanatory summarization with discourse-driven planning. <em>TACL</em>, <em>13</em>, 1146-1170. (<a href='https://doi.org/10.1162/TACL.a.30'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination. The project information is available at https://dongqi.me/projects/ExpSum .},
  archive      = {J_TACL},
  author       = {Liu, Dongqi and Yu, Xi and Demberg, Vera and Lapata, Mirella},
  doi          = {10.1162/TACL.a.30},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1146-1170},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Explanatory summarization with discourse-driven planning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DARE: Diverse visual question answering with robustness evaluation. <em>TACL</em>, <em>13</em>, 1121-1145. (<a href='https://doi.org/10.1162/TACL.a.29'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Language Models (VLMs) extend remarkable capabilities of text-only large language models and vision-only models, being able to learn from and process multi-modal vision-text input. While modern VLMs perform well on a number of standard image classification and image-text matching tasks, they still struggle with a number of crucial vision-language (VL) reasoning abilities such as counting and spatial reasoning. Moreover, while they might be very brittle to small variations in instructions and/or evaluation protocols, existing benchmarks fail to evaluate their robustness (or rather the lack of it). In order to couple challenging VL scenarios with comprehensive robustness evaluation, we introduce DARE , D iverse Visual Question A nswering with R obustness E valuation, a carefully created and curated multiple-choice VQA benchmark. DARE evaluates VLM performance on five diverse categories and includes four robustness-oriented evaluations based on the variations of prompts, the subsets of answer options, the output format, and the number of correct answers. Among a spectrum of other findings, we report that state-of-the-art VLMs still struggle with questions in most categories and are unable to consistently deliver their peak performance across the tested robustness evaluations. Consequently, our work calls for the systematic addition of robustness evaluations in future VLM research.},
  archive      = {J_TACL},
  author       = {Sterz, Hannah and Pfeiffer, Jonas and Vulić, Ivan},
  doi          = {10.1162/TACL.a.29},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1121-1145},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {DARE: Diverse visual question answering with robustness evaluation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing and adapting large language models for few-shot multilingual NLU: Are we there yet?. <em>TACL</em>, <em>13</em>, 1096-1120. (<a href='https://doi.org/10.1162/TACL.a.33'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised fine-tuning (SFT), supervised instruction tuning (SIT), and in-context learning (ICL) are three alternative, de facto standard approaches to few-shot learning. ICL has gained popularity recently with the advent of LLMs due to its versatile simplicity and sample efficiency. Prior research has conducted only limited investigation into how these approaches work for multilingual few-shot learning, and the focus so far has been mostly on their performance. In this work, we present an extensive and systematic comparison of the three approaches, testing them on a variety of high- and low-resource languages over five different NLU tasks, and a myriad of language and domain setups. Importantly, performance is only one aspect of the comparison, where we also analyze and discuss the approaches through the optics of their computational, inference and financial costs. Some of the highlighted findings concern an excellent trade-off between performance and resource requirements/cost for SIT. We further analyze the impact of target language adaptation of pretrained LLMs and find that the standard adaptation approaches can (superficially) improve target language generation capabilities, but language understanding elicited through ICL does not improve accordingly and remains limited, especially for low-resource languages.},
  archive      = {J_TACL},
  author       = {Razumovskaia, Evgeniia and Vulić, Ivan and Korhonen, Anna},
  doi          = {10.1162/TACL.a.33},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1096-1120},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Analyzing and adapting large language models for few-shot multilingual NLU: Are we there yet?},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BenCzechMark: A czech-centric multitask and multimetric benchmark for large language models with duel scoring mechanism. <em>TACL</em>, <em>13</em>, 1068-1095. (<a href='https://doi.org/10.1162/TACL.a.32'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present B en C zech M ark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its duel scoring system is grounded in statistical significance theory and uses aggregation across tasks inspired by social preference theory. Our benchmark encompasses 50 challenging tasks, with corresponding test datasets, primarily in native Czech, with 14 newly collected ones. These tasks span 8 categories and cover diverse domains, including historical Czech news, essays from pupils or language learners, and spoken word. Furthermore, we collect and clean BUT-Large Czech Collection, the largest publicly available clean Czech language corpus, and use it for (i) contamination analysis and (ii) continuous pretraining of the first Czech-centric 7B language model with Czech-specific tokenization. We use our model as a baseline for comparison with publicly available multilingual models. Lastly, we release and maintain a leaderboard with existing 50 model submissions, where new model submissions can be made at https://huggingface.co/spaces/CZLC/BenCzechMark .},
  archive      = {J_TACL},
  author       = {Fajcik, Martin and Docekal, Martin and Dolezal, Jan and Ondrej, Karel and Beneš, Karel and Kapsa, Jan and Smrz, Pavel and Polok, Alexander and Hradis, Michal and Neverilova, Zuzana and Horak, Ales and Sabol, Radoslav and Stefanik, Michal and Jirkovsky, Adam and Adamczyk, David and Hyner, Petr and Hula, Jan and Kydlicek, Hynek},
  doi          = {10.1162/TACL.a.32},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1068-1095},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {BenCzechMark: A czech-centric multitask and multimetric benchmark for large language models with duel scoring mechanism},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KEFT: Knowledge-enhanced fine-tuning for large language models in domain-specific question answering. <em>TACL</em>, <em>13</em>, 1056-1067. (<a href='https://doi.org/10.1162/TACL.a.31'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of large language models (LLMs) has opened up promising opportunities for their downstream applications in question-answering (QA), such as ChatGPT, ChatGLM, etc. However, such LLMs do not perform very well in domain-specific QA tasks without fine-tuning. But directly fine-tuning LLMs on domain-specific corpus data may lead to catastrophic forgetting, causing the LLMs to lose their general language capability. To address this problem, we propose the Knowledge-Enhanced Fine-Tuning (KEFT) method, an unsupervised fine-tuning approach to enhance the knowledge capability of LLMs in domain-specific QA tasks while preserving their general language capability. KEFT leverages the inherent language comprehension of pre-trained LLMs to generate synthetic-QA datasets from domain-specific corpus data autonomously for fine-tuning, and adopts a Low-Rank Adaptation (LoRA) method to further alleviate over-fitting. Furthermore, to enhance the representation of domain-specific knowledge, we introduce a knowledge-enhanced fine-tuning loss function, which encourages the model to learn the knowledge-question connection, thereby generating natural and knowledgeable answers. Our evaluations across multiple domain-specific datasets demonstrate that KEFT surpasses state-of-the-art fine-tuning approaches, enhancing the performance of various LLMs in QA tasks in both English and Chinese languages.},
  archive      = {J_TACL},
  author       = {Li, Haiyun and Zhang, Jixin and Shen, Hua and Cheng, Ke and Huang, Xiaofeng},
  doi          = {10.1162/TACL.a.31},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {9},
  pages        = {1056-1067},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {KEFT: Knowledge-enhanced fine-tuning for large language models in domain-specific question answering},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MURI: High-quality instruction tuning datasets for low-resource languages via reverse instructions. <em>TACL</em>, <em>13</em>, 1032-1055. (<a href='https://doi.org/10.1162/TACL.a.18'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instruction tuning enhances large language models (LLMs) by aligning them with human preferences across diverse tasks. Traditional approaches to create instruction tuning datasets face serious challenges for low-resource languages due to their dependence on data annotation. This work introduces a novel method, Multilingual Reverse Instructions (MURI), which generates high-quality instruction tuning datasets for low-resource languages without requiring human annotators or pre-existing multilingual models. Utilizing reverse instructions and a translation pipeline, MURI produces instruction-output pairs from existing human-written texts in low-resource languages. This method ensures cultural relevance and diversity by sourcing texts from different native domains and applying filters to eliminate inappropriate content. Our dataset, MURI-IT , includes more than 2 million instruction-output pairs across 200 languages. Evaluation by native speakers and fine-tuning experiments with mT5 models demonstrate the approach’s effectiveness for both NLU and open-ended generation. We publicly release datasets and models at https://github.com/akoksal/muri .},
  archive      = {J_TACL},
  author       = {Köksal, Abdullatif and Thaler, Marion and Imani, Ayyoob and Üstün, Ahmet and Korhonen, Anna and Schütze, Hinrich},
  doi          = {10.1162/TACL.a.18},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {1032-1055},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {MURI: High-quality instruction tuning datasets for low-resource languages via reverse instructions},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient tuning of large language models for knowledge-grounded dialogue generation. <em>TACL</em>, <em>13</em>, 1007-1031. (<a href='https://doi.org/10.1162/TACL.a.17'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain-specific knowledge not included in their training data. To address this gap, we introduce KEDiT, an efficient method for fine-tuning LLMs for knowledge-grounded dialogue generation. KEDiT operates in two main phases. First, it employs an information bottleneck to compress retrieved knowledge into learnable parameters, retaining essential information while minimizing computational overhead. Second, a lightweight knowledge-aware adapter integrates these compressed knowledge vectors into the LLM during fine-tuning, updating less than 2% of the model parameters. The experimental results on the Wizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate that KEDiT excels in generating contextually relevant and informative responses, outperforming competitive baselines in automatic, LLM-based, and human evaluations. This approach effectively combines the strengths of pretrained LLMs with the adaptability needed for incorporating dynamic knowledge, presenting a scalable solution for fields such as medicine. 1},
  archive      = {J_TACL},
  author       = {Zhang, Bo and Ma, Hui and Li, Dailin and Ding, Jian and Wang, Jian and Xu, Bo and Lin, HongFei},
  doi          = {10.1162/TACL.a.17},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {1007-1031},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Efficient tuning of large language models for knowledge-grounded dialogue generation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human choice prediction in language-based persuasion games: Simulation-based off-policy evaluation. <em>TACL</em>, <em>13</em>, 980-1006. (<a href='https://doi.org/10.1162/TACL.a.16'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in Large Language Models (LLMs) have spurred interest in designing LLM-based agents for tasks that involve interaction with human and artificial agents. This paper addresses a key aspect in the design of such agents: predicting human decisions in off-policy evaluation (OPE). We focus on language-based persuasion games, where an expert aims to influence the decision-maker through verbal messages. In our OPE framework, the prediction model is trained on human interaction data collected from encounters with one set of expert agents, and its performance is evaluated on interactions with a different set of experts. Using a dedicated application, we collected a dataset of 87K decisions from humans playing a repeated decision-making game with artificial agents. To enhance off-policy performance, we propose a simulation technique involving interactions across the entire agent space and simulated decision-makers. Our learning strategy yields significant OPE gains, e.g., improving prediction accuracy in the top 15% challenging cases by 7.1%. 1},
  archive      = {J_TACL},
  author       = {Shapira, Eilam and Madmon, Omer and Apel, Reut and Tennenholtz, Moshe and Reichart, Roi},
  doi          = {10.1162/TACL.a.16},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {980-1006},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Human choice prediction in language-based persuasion games: Simulation-based off-policy evaluation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating adversarial trigger transfer in large language models. <em>TACL</em>, <em>13</em>, 953-979. (<a href='https://doi.org/10.1162/TACL.a.27'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has developed optimization procedures to find token sequences, called adversarial triggers , which can elicit unsafe responses from aligned language models. These triggers are believed to be highly transferable, i.e., a trigger optimized on one model can jailbreak other models. In this paper, we concretely show that such adversarial triggers are not consistently transferable. We extensively investigate trigger transfer among 13 open models and observe poor and inconsistent transfer. Our experiments further reveal a significant difference in robustness to adversarial triggers between models A ligned by P reference O ptimization (APO) and models A ligned by F ine- T uning (AFT). We find that APO models are extremely hard to jailbreak even when the trigger is optimized directly on the model. On the other hand, while AFT models may appear safe on the surface, exhibiting refusals to a range of unsafe instructions, we show that they are highly susceptible to adversarial triggers. Lastly, we observe that most triggers optimized on AFT models also generalize to new unsafe instructions from five diverse domains, further emphasizing their vulnerability. Overall, our work highlights the need for more comprehensive safety evaluations for aligned language models. 1 Warning: This paper contains examples that may be offensive or upsetting.},
  archive      = {J_TACL},
  author       = {Meade, Nicholas and Patel, Arkil and Reddy, Siva},
  doi          = {10.1162/TACL.a.27},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {953-979},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Investigating adversarial trigger transfer in large language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAKE: Memory-associated knowledge editing. <em>TACL</em>, <em>13</em>, 938-952. (<a href='https://doi.org/10.1162/TACL.a.26'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since their emergence, large language models (LLMs) have rapidly advanced, exerting substantial influence across various domains. Consequently, the importance of model editing techniques, aimed at locally correcting outdated or incorrect knowledge within language models, has grown significantly. However, traditional model editing methods face limitations: They cannot guarantee that highly related knowledge will transfer to the post-edited model, and they often rely on external knowledge bases to address this issue. In this paper, we propose a novel approach that leverages the internal knowledge of the language model to overcome the shortcomings of existing methods. First, we explore how to recall indirect associated knowledge from the model itself, which can be utilized in the editing process. Building on this, we propose MAKE (Memory-Associated Knowledge Editing) , an editing method that takes into account the transfer of associated knowledge. As a result, MAKE successfully updates associated knowledge and achieves state-of-the-art performance in experiments conducted on the zsRE+, CounterFact + and MQuAKE datasets.},
  archive      = {J_TACL},
  author       = {Park, Seongsik and Park, Sangmin and Kim, Jaieun and Kim, Harksoo},
  doi          = {10.1162/TACL.a.26},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {938-952},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {MAKE: Memory-associated knowledge editing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STPar: A structure-aware triaffine parser for screenplay character coreference resolution. <em>TACL</em>, <em>13</em>, 923-937. (<a href='https://doi.org/10.1162/TACL.a.28'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Character Coreference Resolution in Movie Screenplays (MovieCoref) is a newly emerging task for understanding complex movie plots and character relationships. This task poses greater challenges than traditional coreference resolution, due to the intricate narrative structures and character interactions unique to screenplays. In light of these challenges, we introduce a novel approach: a Structure-aware Triaffine Parser (STPar) for the MovieCoref task. STPar combines discourse and syntactic structures in the feature encoding process, enabling comprehensive analysis of ternary relationships and complex interactions. During the pairing process, STPar utilizes a triaffine scorer to consider high-order relations between candidate mention pairs, thus enhancing its ability to capture detailed narrative structures. In addition, STPar incorporates multi-task learning, encompassing singleton and span detection tasks, to further improve coreference resolution performance. Our evaluations on the MovieCoref dataset demonstrate that STPar significantly outperforms the best baseline by 7.4%, 21.5%, 7.1%, and 10.2% in F1 scores of B 3 , CEAF e , LEA, and CoNLL. Further analysis highlights the benefits of integrating structural discourse and syntactic information as well as the combined approaches of triaffine and multi-task learning. 1},
  archive      = {J_TACL},
  author       = {Zheng, Li and Fei, Hao and Chen, Lei and Li, Bobo and Li, Fei and Teng, Chong and Zhao, Liang and Ji, Donghong},
  doi          = {10.1162/TACL.a.28},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {923-937},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {STPar: A structure-aware triaffine parser for screenplay character coreference resolution},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). (Perhaps) beyond human translation: Harnessing multi-agent collaboration for translating ultra-long literary texts. <em>TACL</em>, <em>13</em>, 901-922. (<a href='https://doi.org/10.1162/TACL.a.25'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Literary translations remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language, cultural nuances, and unique stylistic elements. In this work, we introduce TransAgents , a novel multi-agent framework that simulates the roles and collaborative practices of a human translation company, including a CEO, Senior Editor, Junior Editor, Translator, Localization Specialist, and Proofreader. The translation process is divided into two stages: a preparation stage where the team is assembled and comprehensive translation guidelines are drafted, and an execution stage that involves sequential translation, localization, proofreading, and a final quality check. Furthermore, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP), which evaluates translations based solely on target language quality and cultural appropriateness, and BLP, which leverages large language models like gpt-4 for direct text comparison. Although TransAgents achieves lower d - BLEU scores, due to the limited diversity of references, its translations are significantly better than those of other baselines and are preferred by both human evaluators and LLMs over traditional human references and gpt-4 translations. Our findings highlight the potential of multi-agent collaboration in enhancing translation quality, particularly for longer texts. 1},
  archive      = {J_TACL},
  author       = {Wu, Minghao and Xu, Jiahao and Yuan, Yulin and Haffari, Gholamreza and Wan, Longyue and Luo, Weihua and Zhang, Kaifu},
  doi          = {10.1162/TACL.a.25},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {901-922},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {(Perhaps) beyond human translation: Harnessing multi-agent collaboration for translating ultra-long literary texts},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextualized evaluations: Judging language model responses to underspecified queries. <em>TACL</em>, <em>13</em>, 878-900. (<a href='https://doi.org/10.1162/TACL.a.24'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language model users often issue queries that lack specification, where the context under which a query was issued—such as the user’s identity, the query’s intent, and the criteria for a response to be useful—is not explicit. For instance, a good response to a subjective query like “ What book should I read next? ” would depend on the user’s preferences, and a good response to an open-ended query like “ How do antibiotics work against bacteria? ” would depend on the user’s expertise. This makes evaluation of responses to such queries an ill-posed task, as evaluators may make arbitrary judgments about the response quality. To remedy this, we present contextualized evaluations , a protocol that synthetically constructs context surrounding an underspecified query and provides it during evaluation. We find that the presence of context can 1) alter conclusions drawn from evaluation, even flipping benchmark rankings between model pairs, 2) nudge evaluators to make fewer judgments based on surface-level criteria, like style, and 3) provide new insights about model behavior across diverse contexts. Specifically, our procedure suggests a potential bias towards WEIRD (Western, Educated, Industrialized, Rich and Democratic) contexts in models’ “default” responses and we find that models are not equally sensitive to following different contexts, even when they are provided in prompts. 1},
  archive      = {J_TACL},
  author       = {Malaviya, Chaitanya and Chang, Joseph Chee and Roth, Dan and Iyyer, Mohit and Yatskar, Mark and Lo, Kyle},
  doi          = {10.1162/TACL.a.24},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {878-900},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Contextualized evaluations: Judging language model responses to underspecified queries},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate and efficient fine-tuning of quantized large language models through optimal balance in adaptation. <em>TACL</em>, <em>13</em>, 861-877. (<a href='https://doi.org/10.1162/TACL.a.23'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have demonstrated impressive performance across various domains. However, the enormous number of model parameters makes fine-tuning challenging, significantly limiting their application and deployment. Existing solutions combine parameter quantization with Low-Rank Adaptation (LoRA), reducing memory usage but causing performance degradation. Additionally, converting fine-tuned models to low-precision representations further degrades performance. In this paper, we identify an imbalance in fine-tuning quantized LLMs with LoRA: overly complex adapter inputs and outputs versus low effective trainability of the adapter, leading to underfitting during fine-tuning. Thus, we propose Quantized LLMs fine-tuning with Balanced Low-Rank Adaptation (Q-BLoRA), which simplifies the adapter’s inputs and outputs while increasing the adapter’s rank to alleviate underfitting during fine-tuning. For low-precision deployment, we propose Quantization-Aware fine-tuning with Balanced Low-Rank Adaptation (QA-BLoRA), which aligns with the block-wise quantization and facilitates quantization-aware fine-tuning of low-rank adaptation based on the parameter merging of Q-BLoRA. Both Q-BLoRA and QA-BLoRA are easily implemented and offer the following optimizations: (i) Q-BLoRA consistently achieves state-of-the-art accuracy compared to baselines and other variants; (ii) QA-BLoRA enables the direct generation of low-precision inference models, which exhibit significant performance improvements over other low-precision models. We validate the effectiveness of Q-BLoRA and QA-BLoRA across various models and scenarios. Code has been made available at https://github.com/xiaocaigou/qbaraqahira .},
  archive      = {J_TACL},
  author       = {Shen, Ao and Lai, Zhiquan and Wang, Qiang and Li, Xionglve and Zhang, Lizhi and Li, Dongsheng and Li, Jiaxin},
  doi          = {10.1162/TACL.a.23},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {861-877},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Accurate and efficient fine-tuning of quantized large language models through optimal balance in adaptation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt contrastive transformation: An enhanced strategy for efficient prompt transfer in natural language processing. <em>TACL</em>, <em>13</em>, 848-860. (<a href='https://doi.org/10.1162/TACL.a.22'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt transfer is a transfer learning method based on prompt tuning, which enhances the parameter performance of prompts in target tasks by transferring source prompt embeddings. Among existing methods, weighted aggregation is effective and possesses the advantages of being lightweight and modular. However, these methods may transfer redundant or irrelevant information from the source prompts to the target prompt, leading to negative impacts. To alleviate this problem, we propose P rompt C ontrastive T ransformation ( PCT ), which achieves efficient prompt transfer through prompt contrastive transformation and attentional fusion. PCT transforms the source prompt into task-agnostic embedding and task-specific embeddings through singular value decomposition and contrastive learning, reducing information redundancy among source prompts. The attention module in PCT selects more effective task-specific embeddings and fuses them with task-agnostic embedding into the target prompt. Experimental results show that, despite tuning only 0.035% of task-specific parameters, PCT achieves improvements in prompt transfer for single target task adaptation across various NLP tasks.},
  archive      = {J_TACL},
  author       = {Zhao, Shu and Yang, Shiji and Tan, Shicheng and Yang, Zhen and Mei, Congyao and Duan, Zhen and Zhang, Yanping and Chen, Jie},
  doi          = {10.1162/TACL.a.22},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {848-860},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Prompt contrastive transformation: An enhanced strategy for efficient prompt transfer in natural language processing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual pre-training on character-level noisy texts makes decoder-based language models robust few-shot learners. <em>TACL</em>, <em>13</em>, 831-847. (<a href='https://doi.org/10.1162/TACL.a.21'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent decoder-based pre-trained language models (PLMs) generally use subword tokenizers. However, adding character-level perturbations drastically changes the delimitation of texts by the tokenizers, leading to the vulnerability of PLMs. This study proposes a method of continual pre-training to convert decoder-based PLMs with subword tokenizers into perturbation-robust few-shot in-context learners. Our method continually trains decoder-based PLMs to predict the next tokens conditioning on artificially created character-level noisy texts. Since decoder-based language models are auto-regressive, we skip noised words from the target optimization. In addition, to maintain the same word prediction performance under noisy text as clean text, our method employs word distribution matching between the original PLMs and training models. We conducted experiments on various subword-based PLMs, including GPT2, Pythia, Mistral, Gemma2, and Llama3, ranging from 1B to 8B parameters. The results demonstrate that our method consistently improves the performance of few-shot in-context learning on downstream tasks which contain actual typos or misspellings as well as artificial noise. 1},
  archive      = {J_TACL},
  author       = {Kojima, Takeshi and Matsuo, Yutaka and Iwasawa, Yusuke},
  doi          = {10.1162/TACL.a.21},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {831-847},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Continual pre-training on character-level noisy texts makes decoder-based language models robust few-shot learners},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data contamination quiz: A tool to detect and estimate contamination in large language models. <em>TACL</em>, <em>13</em>, 809-830. (<a href='https://doi.org/10.1162/TACL.a.20'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it. While accounting for positional biases in LLMs, the quiz performance reveals the contamination level for the tested model with the dataset partition to which the quiz pertains. Applied to various datasets and LLMs, under controlled and uncontrolled contamination, our findings—while fully lacking access to training data and model parameters—suggest that DCQ achieves state-of-the-art results and uncovers greater contamination levels through memorization compared to existing methods. Also, it proficiently bypasses more safety filters, especially those set to avoid generating copyrighted content. 1},
  archive      = {J_TACL},
  author       = {Golchin, Shahriar and Surdeanu, Mihai},
  doi          = {10.1162/TACL.a.20},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {809-830},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Data contamination quiz: A tool to detect and estimate contamination in large language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MtRAG: A multi-turn conversational benchmark for evaluating retrieval-augmented generation systems. <em>TACL</em>, <em>13</em>, 784-808. (<a href='https://doi.org/10.1162/TACL.a.19'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieval-augmented generation (RAG) has recently become a very popular task for Large Language Models (LLMs). Evaluating them on multi-turn RAG conversations, where the system is asked to generate a response to a question in the context of a preceding conversation, is an important and often overlooked task with several additional challenges. We present mt RAG, an end-to-end human-generated multi-turn RAG benchmark that reflects several real-world properties across diverse dimensions for evaluating the full RAG pipeline. mt RAG contains 110 conversations averaging 7.7 turns each across four domains for a total of 842 tasks. We also explore automation paths via synthetic data and LLM-as-a-Judge evaluation. Our human and automatic evaluations show that even state-of-the-art LLM RAG systems struggle on mt RAG. We demonstrate the need for strong retrieval and generation systems that can handle later turns, unanswerable questions, non-standalone questions, and multiple domains. mt RAG is available at https://github.com/ibm/mt-rag-benchmark .},
  archive      = {J_TACL},
  author       = {Katsis, Yannis and Rosenthal, Sara and Fadnis, Kshitij and Gunasekara, Chulaka and Lee, Young-Suk and Popa, Lucian and Shah, Vraj and Zhu, Huaiyu and Contractor, Danish and Danilevsky, Marina},
  doi          = {10.1162/TACL.a.19},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {8},
  pages        = {784-808},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {MtRAG: A multi-turn conversational benchmark for evaluating retrieval-augmented generation systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REAL sampling: Boosting factuality and diversity of open-ended generation by extrapolating the entropy of an infinitely large LM. <em>TACL</em>, <em>13</em>, 760-783. (<a href='https://doi.org/10.1162/tacl_a_00757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decoding methods for large language models (LLMs) usually struggle with the tradeoff between ensuring factuality and maintaining diversity. In this paper, we propose REAL ( R esidual E ntropy from A symptotic L ine) sampling, 1 which predicts the step-wise hallucination likelihood of an LLM. When an LLM is likely to hallucinate, REAL lowers the p threshold in nucleus sampling. Otherwise, REAL sampling increases the p threshold to boost the diversity. To predict the step-wise hallucination likelihood without supervision, we construct a THF ( T oken-level H allucination F orecasting) model, which predicts the asymptotic entropy (i.e., inherent uncertainty) of the next token by extrapolating the next-token entropies of an infinitely large language model from a series of LLMs with different sizes. If an LLM’s entropy is higher than the asymptotic entropy (i.e., the LLM is more uncertain than it should be), the THF model predicts a high hallucination hazard, which leads to a lower p threshold in REAL sampling. In the FactualityPrompts benchmark (Lee et al., 2022 ), we demonstrate that REAL sampling based on a 70M THF model can substantially improve the factuality and diversity of 7B LLMs simultaneously. After combined with contrastive decoding, REAL sampling outperforms 13 sampling methods, and generates texts that are more factual than the greedy sampling and more diverse than the nucleus sampling with p = 0.5.},
  archive      = {J_TACL},
  author       = {Chang, Haw-Shiuan and Peng, Nanyun and Bansal, Mohit and Ramakrishna, Anil and Chung, Tagyoung},
  doi          = {10.1162/tacl_a_00757},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {760-783},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {REAL sampling: Boosting factuality and diversity of open-ended generation by extrapolating the entropy of an infinitely large LM},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Patchwise cooperative game-based interpretability method for large vision-language models. <em>TACL</em>, <em>13</em>, 744-759. (<a href='https://doi.org/10.1162/tacl_a_00756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amidst the rapid advancement of artificial intelligence, research on large vision-language models (LVLMs) has emerged as a pivotal area. However, understanding their internal mechanisms remains challenging due to the limitations of existing interpretability methods, especially regarding faithfulness and plausibility. To address this, we first construct a human response interpretability dataset that evaluates the plausibility of model explanations by comparing the attention regions between the model and humans when answering the same questions. We then propose a patchwise cooperative game-based interpretability method for LVLMs, which employs Shapley values to quantify the impact of individual image patches on generation likelihood and enhances computational efficiency through a single input approximation approach. Experimental results demonstrate our method’s faithfulness, plausibility, and robustness. Our method provides researchers with deeper insights into model behavior, allowing for an examination of the specific image regions each layer relies on during response generation, ultimately enhancing model reliability. Our code is available at https://github.com/ZY123-GOOD/Patchwise_Cooperative .},
  archive      = {J_TACL},
  author       = {Zhu, Yao and Zhang, Yunjian and Wang, Zizhe and Yan, Xiu and Sun, Peng and Ji, Xiangyang},
  doi          = {10.1162/tacl_a_00756},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {744-759},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Patchwise cooperative game-based interpretability method for large vision-language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NLP security and ethics, in the wild. <em>TACL</em>, <em>13</em>, 709-743. (<a href='https://doi.org/10.1162/tacl_a_00762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As NLP models are used by a growing number of end-users, an area of increasing importance is NLP Security (NLPSec): assessing the vulnerability of models to malicious attacks and developing comprehensive countermeasures against them. While work at the intersection of NLP and cybersecurity has the potential to create safer NLP for all, accidental oversights can result in tangible harm (e.g., breaches of privacy or proliferation of malicious models). In this emerging field, however, the research ethics of NLP have not yet faced many of the long-standing conundrums pertinent to cybersecurity, until now. We thus examine contemporary works across NLPSec, and explore their engagement with cybersecurity’s ethical norms. We identify trends across the literature, ultimately finding alarming gaps on topics like harm minimization and responsible disclosure. To alleviate these concerns, we provide concrete recommendations to help NLP researchers navigate this space more ethically, bridging the gap between traditional cybersecurity and NLP ethics, which we frame as “white hat NLP”. The goal of this work is to help cultivate an intentional culture of ethical research for those working in NLP Security.},
  archive      = {J_TACL},
  author       = {Lent, Heather and Galinkin, Erick and Chen, Yiyi and Pedersen, Jens Myrup and Derczynski, Leon and Bjerva, Johannes},
  doi          = {10.1162/tacl_a_00762},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {709-743},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {NLP security and ethics, in the wild},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sense-specific historical word usage generation. <em>TACL</em>, <em>13</em>, 690-708. (<a href='https://doi.org/10.1162/tacl_a_00761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale sense-annotated corpora are important for a range of tasks but are hard to come by. Dictionaries that record and describe the vocabulary of a language often offer a small set of real-world example sentences for each sense of a word. However, on their own, these sentences are too few to be used as diachronic sense-annotated corpora. We propose a targeted strategy for training and evaluating generative models producing historically and semantically accurate word usages given any word, sense definition, and year triple. Our results demonstrate that fine-tuned models can generate usages with the same properties as real-world example sentences from a reference dictionary. Thus the generated usages will be suitable for training and testing computational models where large-scale sense-annotated corpora are needed but currently unavailable.},
  archive      = {J_TACL},
  author       = {Cassotti, Pierluigi and Tahmasebi, Nina},
  doi          = {10.1162/tacl_a_00761},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {690-708},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Sense-specific historical word usage generation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Culturally aware and adapted NLP: A taxonomy and a survey of the state of the art. <em>TACL</em>, <em>13</em>, 652-689. (<a href='https://doi.org/10.1162/tacl_a_00760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge of interest in culture in NLP has inspired much recent research, but a shared understanding of “culture” remains unclear, making it difficult to evaluate progress in this emerging area. Drawing on prior research in NLP and related fields, we propose a fine-grained taxonomy of elements in culture that can provide a systematic framework for analyzing and understanding research progress. Using the taxonomy, we survey existing resources and methods for culturally aware and adapted NLP, providing an overview of the state of the art and the research gaps that still need to be filled.},
  archive      = {J_TACL},
  author       = {Liu, Chen Cecilia and Gurevych, Iryna and Korhonen, Anna},
  doi          = {10.1162/tacl_a_00760},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {652-689},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Culturally aware and adapted NLP: A taxonomy and a survey of the state of the art},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding epistemic language with a language-augmented bayesian theory of mind. <em>TACL</em>, <em>13</em>, 613-637. (<a href='https://doi.org/10.1162/tacl_a_00752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do people understand and evaluate claims about others’ beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents’ goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic “language-of-thought” with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent’s beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.},
  archive      = {J_TACL},
  author       = {Ying, Lance and Zhi-Xuan, Tan and Wong, Lionel and Mansinghka, Vikash and Tenenbaum, Joshua B.},
  doi          = {10.1162/tacl_a_00752},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {613-637},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Understanding epistemic language with a language-augmented bayesian theory of mind},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative approach for auditing multilingual phonetic transcript archives. <em>TACL</em>, <em>13</em>, 595-612. (<a href='https://doi.org/10.1162/tacl_a_00759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Curating datasets that span multiple languages is challenging. To make the collection more scalable, researchers often incorporate one or more imperfect classifiers in the process, like language identification models. These models, however, are prone to failure, resulting in some language partitions being unreliable for downstream tasks. We introduce a statistical test, the Preference Proportion Test, for identifying such unreliable partitions. By annotating only 20 samples for a language partition, we are able to identify systematic transcription errors for 10 language partitions in a recent large multilingual transcribed audio archive, X-IPAPack (Zhu et al., 2024 ). We find that filtering these low-quality partitions out when training models for the downstream task of phonetic transcription brings substantial benefits, most notably a 25.7% relative improvement on transcribing recordings in out-of-distribution languages. Our work contributes an effective method for auditing multilingual audio archives. 1},
  archive      = {J_TACL},
  author       = {Samir, Farhan and Ahn, Emily P. and Prakash, Shreya and Soskuthy, Márton and Shwartz, Vered and Zhu, Jian},
  doi          = {10.1162/tacl_a_00759},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {595-612},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A comparative approach for auditing multilingual phonetic transcript archives},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring practical gaps in using cross entropy to implement maximum mutual information criterion for rationalization. <em>TACL</em>, <em>13</em>, 577-594. (<a href='https://doi.org/10.1162/tacl_a_00758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rationalization is a framework that aims to build self-explanatory NLP models by extracting a subset of human-intelligible pieces of their inputting texts. It involves a cooperative game where a selector selects the most human-intelligible parts of the input as the rationale, followed by a predictor that makes predictions based on these selected rationales. Existing literature uses the cross-entropy between the model’s predictions and the ground-truth labels to measure the informativeness of the selected rationales, guiding the selector to choose better ones. In this study, we first theoretically analyze the objective of rationalization by decomposing it into two parts: the model-agnostic informativeness of the rationale candidates and the predictor’s degree of fit. We then provide various empirical evidence to support that, under this framework, the selector tends to sample from a limited small region, causing the predictor to overfit these localized areas. This results in a significant mismatch between the cross-entropy objective and the informativeness of the rationale candidates, leading to suboptimal solutions. To address this issue, we propose a simple yet effective method that introduces random vicinal 1 perturbations to the selected rationale candidates. This approach broadens the predictor’s assessment to a vicinity around the selected rationale candidate. Compared to recent competitive methods, our method significantly improves rationale quality (by up to 6.6%) across six widely used classification datasets. The term “vicinal” is borrowed from vicinal risk minimization (Chapelle et al., 2000 ); “vicinal” means neighboring or adjacent .},
  archive      = {J_TACL},
  author       = {Liu, Wei and Deng, Zhiying and Niu, Zhongyu and Wang, Jun and Wang, Haozhao and Li, Ruixuan},
  doi          = {10.1162/tacl_a_00758},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {577-594},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Exploring practical gaps in using cross entropy to implement maximum mutual information criterion for rationalization},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TaxoPro: A plug-in LoRA-based cross-domain method for low-resource taxonomy completion. <em>TACL</em>, <em>13</em>, 557-576. (<a href='https://doi.org/10.1162/tacl_a_00755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-resource taxonomy completion aims to automatically insert new concepts into the existing taxonomy, in which only a few in-domain training samples are available. Recent studies have achieved considerable progress by incorporating prior knowledge from pre-trained language models (PLMs). However, these studies tend to overly rely on such knowledge and neglect the shareable knowledge across different taxonomies. In this paper, we propose TaxoPro, a p lug-in Lo R A-based cr o ss-domain method, that captures shareable knowledge from the high- resource taxonomy to improve PLM-based low-resource taxonomy completion techniques. To prevent negative interference between domain-specific and domain-shared knowledge, TaxoPro decomposes cross- domain knowledge into domain-shared and domain-specific components, storing them using low-rank matrices (LoRA). Additionally, TaxoPro employs two auxiliary losses to regulate the flow of shareable knowledge. Experimental results demonstrate that TaxoPro improves PLM-based techniques, achieving state-of-the-art performance in completing low-resource taxonomies. Code is available at https://github.com/cyclexu/TaxoPro .},
  archive      = {J_TACL},
  author       = {Xu, Hongyuan and Niu, Yuhang and Liu, Ciyi and Wen, Yanlong and Yuan, Xiaojie},
  doi          = {10.1162/tacl_a_00755},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {557-576},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {TaxoPro: A plug-in LoRA-based cross-domain method for low-resource taxonomy completion},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Know your limits: A survey of abstention in large language models. <em>TACL</em>, <em>13</em>, 529-556. (<a href='https://doi.org/10.1162/tacl_a_00754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstention, the refusal of large language models (LLMs) to provide an answer, is increasingly recognized for its potential to mitigate hallucinations and enhance safety in LLM systems. In this survey, we introduce a framework to examine abstention from three perspectives: the query, the model, and human values. We organize the literature on abstention methods, benchmarks, and evaluation metrics using this framework, and discuss merits and limitations of prior work. We further identify and motivate areas for future research, such as whether abstention can be achieved as a meta-capability that transcends specific tasks or domains, and opportunities to optimize abstention abilities in specific contexts. In doing so, we aim to broaden the scope and impact of abstention methodologies in AI systems. 1},
  archive      = {J_TACL},
  author       = {Wen, Bingbing and Yao, Jihan and Feng, Shangbin and Xu, Chenjun and Tsvetkov, Yulia and Howe, Bill and Wang, Lucy Lu},
  doi          = {10.1162/tacl_a_00754},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {7},
  pages        = {529-556},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Know your limits: A survey of abstention in large language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating the landscape of hint generation research: From the past to the future. <em>TACL</em>, <em>13</em>, 505-528. (<a href='https://doi.org/10.1162/tacl_a_00751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital education has gained popularity in the last decade, especially after the COVID-19 pandemic. With the improving capabilities of large language models to reason and communicate with users, envisioning intelligent tutoring systems that can facilitate self-learning is not very far-fetched. One integral component to fulfill this vision is the ability to give accurate and effective feedback via hints to scaffold the learning process. In this survey article, we present a comprehensive review of prior research on hint generation, aiming to bridge the gap between research in education and cognitive science, and research in AI and Natural Language Processing. Informed by our findings, we propose a formal definition of the hint generation task, and discuss the roadmap of building an effective hint generation system aligned with the formal definition, including open challenges, future directions and ethical considerations.},
  archive      = {J_TACL},
  author       = {Jangra, Anubhav and Mozafari, Jamshid and Jatowt, Adam and Muresan, Smaranda},
  doi          = {10.1162/tacl_a_00751},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {505-528},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Navigating the landscape of hint generation research: From the past to the future},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot multilingual open-domain QA from five examples. <em>TACL</em>, <em>13</em>, 481-504. (<a href='https://doi.org/10.1162/tacl_a_00750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent approaches to multilingual open- domain question answering (MLODQA) have achieved promising results given abundant language-specific training data. However, the considerable annotation cost limits the application of these methods for underrepresented languages. We introduce a few-shot learning approach to synthesize large-scale multilingual data from large language models (LLMs). Our method begins with large-scale self-supervised pre-training using WikiData, followed by training on high-quality synthetic multilingual data generated by prompting LLMs with few-shot supervision. The final model, FsModQA , significantly outperforms existing few-shot and supervised baselines in MLODQA and cross-lingual and monolingual retrieval. We further show our method can be extended for effective zero-shot adaptation to new languages through a cross-lingual prompting strategy with only English-supervised data, making it a general and applicable solution for MLODQA tasks without costly large-scale annotation.},
  archive      = {J_TACL},
  author       = {Jiang, Fan and Drummond, Tom and Cohn, Trevor},
  doi          = {10.1162/tacl_a_00750},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {481-504},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Few-shot multilingual open-domain QA from five examples},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TANQ: An open domain dataset of table answered questions. <em>TACL</em>, <em>13</em>, 461-480. (<a href='https://doi.org/10.1162/tacl_a_00749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language models, potentially augmented with tool usage such as retrieval, are becoming the go-to means of answering questions. Understanding and answering questions in real-world settings often requires retrieving information from different sources, processing and aggregating data to extract insights, and presenting complex findings in form of structured artifacts such as novel tables, charts, or infographics. In this paper, we introduce TANQ, 1 the first open-domain question answering dataset where the answers require building tables from information across multiple sources. We release the full source attribution for every cell in the resulting table and benchmark state-of-the-art language models in open, oracle, and closed book setups. Our best-performing baseline, Gemini Flash, reaches an overall F1 score of 60.7, lagging behind human performance by 12.3 points. We analyze baselines’ performance across different dataset attributes such as different skills required for this task, including multi-hop reasoning, math operations, and unit conversions. We further discuss common failures in model-generated answers, suggesting that TANQ is a complex task with many challenges ahead.},
  archive      = {J_TACL},
  author       = {Akhtar, Mubashara and Pang, Chenxi and Marzoca, Andreea and Altun, Yasemin and Eisenschlos, Julian Martin},
  doi          = {10.1162/tacl_a_00749},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {6},
  pages        = {461-480},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {TANQ: An open domain dataset of table answered questions},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anchored preference optimization and contrastive revisions: Addressing underspecification in alignment. <em>TACL</em>, <em>13</em>, 442-460. (<a href='https://doi.org/10.1162/tacl_a_00748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are often aligned using contrastive alignment objectives and preference pair datasets. The interaction between model, paired data, and objective makes alignment a complicated procedure, sometimes producing subpar results. We study this and find that (i) preference data gives a better learning signal when the underlying responses are contrastive, and (ii) alignment objectives lead to better performance when they specify more control over the model during training. Based on these insights, we introduce Contrastive Learning from AI Revisions (CLAIR), a data-creation method which leads to more contrastive preference pairs, and Anchored Preference Optimization (APO), a controllable and more stable alignment objective. We align Llama-3-8B-Instruct using various comparable datasets and alignment objectives and measure MixEval-Hard scores, which correlate highly with human judgments. The CLAIR preferences lead to the strongest performance out of all datasets, and APO consistently outperforms less controllable objectives. Our best model, trained on 32K CLAIR preferences with APO, improves Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code and datasets are available.},
  archive      = {J_TACL},
  author       = {D'Oosterlinck, Karel and Xu, Winnie and Develder, Chris and Demeester, Thomas and Singh, Amanpreet and Potts, Christopher and Kiela, Douwe and Mehri, Shikib},
  doi          = {10.1162/tacl_a_00748},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {442-460},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Anchored preference optimization and contrastive revisions: Addressing underspecification in alignment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phonetic reconstruction of the consonant system of middle chinese via mixed integer optimization. <em>TACL</em>, <em>13</em>, 424-441. (<a href='https://doi.org/10.1162/tacl_a_00742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with phonetic reconstruction of the consonant system of Middle Chinese. We propose to cast the problem as a Mixed Integer Programming problem, which is able to automatically explore homophonic information from ancient rhyme dictionaries and phonetic information from modern Chinese dialects, the descendants of Middle Chinese. Numerical evaluation on a wide range of synthetic and real data demonstrates the effectiveness and robustness of the new method. We apply the method to information from Guǎngyùn and 20 modern Chinese dialects to obtain a new phonetic reconstruction result. A linguistically motivated discussion of this result is also provided. 1},
  archive      = {J_TACL},
  author       = {Luo, Xiaoxi and Sun, Weiwei},
  doi          = {10.1162/tacl_a_00742},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {424-441},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Phonetic reconstruction of the consonant system of middle chinese via mixed integer optimization},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How much semantic information is available in large language model tokens?. <em>TACL</em>, <em>13</em>, 408-423. (<a href='https://doi.org/10.1162/tacl_a_00747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models segment many words into multiple tokens, and companies that make those models claim that meaningful subword tokens are essential. To investigate whether subword tokens bear meaning, we segmented tens of thousands of words from each of 41 languages according to three generations of GPT tokenizers. We found that words sharing tokens are more semantically similar than expected by chance or expected from length alone, that tokens capture morphological information even when they don’t look like morphemes, and that tokens capture more information than is explained by morphology. In languages that use a script other than the Latin alphabet, GPT-4 tokens are uninformative, but GPT-4o has improved this situation. These results suggest that comparing tokens to morphemes overlooks the wider variety of semantic information available in word form and that standard tokenization methods successfully capture much of that information.},
  archive      = {J_TACL},
  author       = {Haslett, David A. and Cai, Zhenguang G.},
  doi          = {10.1162/tacl_a_00747},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {408-423},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {How much semantic information is available in large language model tokens?},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse AI feedback for large language model alignment. <em>TACL</em>, <em>13</em>, 392-407. (<a href='https://doi.org/10.1162/tacl_a_00746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in large language models (LLMs) focus on aligning models with human values to minimize harmful content. However, existing methods often rely on a single type of feedback, such as preferences, annotated labels, or critiques, which can lead to overfitting and suboptimal performance. In this paper, we propose D iverse AI F eedback (DAIF), a novel approach that integrates three types of feedback—critique, refinement, and preference—tailored to tasks of varying uncertainty levels. Through an analysis of information gain, we show that critique feedback is most effective for low-uncertainty tasks, refinement feedback for medium-uncertainty tasks, and preference feedback for high-uncertainty tasks. Training with this diversified feedback reduces overfitting and improves alignment. Experimental results across three tasks—question answering, dialog generation, and text summarization–demonstrate that DAIF outperforms traditional methods relying on a single feedback type. 1},
  archive      = {J_TACL},
  author       = {Yu, Tianshu and Lin, Ting-En and Wu, Yuchuan and Yang, Min and Huang, Fei and Li, Yongbin},
  doi          = {10.1162/tacl_a_00746},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {392-407},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Diverse AI feedback for large language model alignment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The thai universal dependency treebank. <em>TACL</em>, <em>13</em>, 376-391. (<a href='https://doi.org/10.1162/tacl_a_00745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic dependency parsing of Thai sentences has been underexplored, as evidenced by the lack of large Thai dependency treebanks with complete dependency structures and the lack of a published evaluation of state-of-the-art models, especially transformer-based parsers. In this work, we addressed these gaps by introducing the Thai Universal Dependency Treebank (TUD), a new Thai treebank consisting of 3,627 trees annotated according to the Universal Dependencies (UD) framework. We then benchmarked 92 dependency parsing models that incorporate pretrained transformers on Thai-PUD and our TUD, achieving state-of-the-art results and shedding light on the optimal model components for Thai dependency parsing. Our error analysis of the models also reveals that polyfunctional words, serial verb construction, and lack of rich morphosyntactic features present main challenges for Thai dependency parsing.},
  archive      = {J_TACL},
  author       = {Sriwirote, Panyut and Leong, Wei Qi and Polpanumas, Charin and Thanyawong, Santhawat and Tjhi, William Chandra and Aroonmanakun, Wirote and Rutherford, Attapol T.},
  doi          = {10.1162/tacl_a_00745},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {376-391},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {The thai universal dependency treebank},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM reading tea leaves: Automatically evaluating topic models with large language models. <em>TACL</em>, <em>13</em>, 357-375. (<a href='https://doi.org/10.1162/tacl_a_00744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic modeling has been a widely used tool for unsupervised text analysis. However, comprehensive evaluations of a topic model remain challenging. Existing evaluation methods are either less comparable across different models (e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic quality or document representation quality) at a time, which is insufficient to reflect the overall model performance. In this paper, we propose WALM ( W ord A greement with L anguage M odel), a new evaluation method for topic modeling that considers the semantic quality of document representations and topics in a joint manner, leveraging the power of Large Language Models (LLMs). With extensive experiments involving different types of topic models, WALM is shown to align with human judgment and can serve as a complementary evaluation method to the existing ones, bringing a new perspective to topic modeling. Our software package is available at https://github.com/Xiaohao-Yang/Topic_Model_Evaluation .},
  archive      = {J_TACL},
  author       = {Yang, Xiaohao and Zhao, He and Phung, Dinh and Buntine, Wray and Du, Lan},
  doi          = {10.1162/tacl_a_00744},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {357-375},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {LLM reading tea leaves: Automatically evaluating topic models with large language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEAR: Disentangled event-agnostic representation learning for early fake news detection. <em>TACL</em>, <em>13</em>, 343-356. (<a href='https://doi.org/10.1162/tacl_a_00743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting fake news early is challenging due to the absence of labeled articles for emerging events in training data. To address this, we propose a D isentangled E vent- A gnostic R epresentation ( DEAR ) learning approach. Our method begins with a BERT-based adaptive multi-grained semantic encoder that captures hierarchical and comprehensive textual representations of the input news content. To effectively separate latent authenticity-related and event-specific knowledge within the news content, we employ a disentanglement architecture. To further enhance the decoupling effect, we introduce a cross-perturbation mechanism that perturbs authenticity-related representation with the event-specific one, and vice versa, deriving a robust and discerning authenticity-related signal. Additionally, we implement a refinement learning scheme to minimize potential interactions between two decoupled representations, ensuring that the authenticity signal remains strong and unaffected by event-specific details. Experimental results demonstrate that our approach effectively mitigates the impact of event-specific influence, outperforming state-of-the-art methods. In particular, it achieves a 6.0% improvement in accuracy on the PHEME dataset over MDDA, a similar approach that decouples latent content and style knowledge, in scenarios involving articles from unseen events different from the topics of the training set.},
  archive      = {J_TACL},
  author       = {Pu, Xiao and Wu, Hao and Bi, Xiuli and Wu, Yu and Gao, Xinbo},
  doi          = {10.1162/tacl_a_00743},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {343-356},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {DEAR: Disentangled event-agnostic representation learning for early fake news detection},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-rationalization in the wild: A large-scale out-of-distribution evaluation on NLI-related tasks. <em>TACL</em>, <em>13</em>, 314-342. (<a href='https://doi.org/10.1162/tacl_a_00741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free-text explanations are expressive and easy to understand, but many datasets lack annotated explanation data, making it challenging to train models for explainable predictions. To address this, we investigate how to use existing explanation datasets for self-rationalization and evaluate models’ out-of-distribution (OOD) performance. We fine-tune T5-Large and OLMo-7B models and assess the impact of fine-tuning data quality, the number of fine-tuning samples, and few-shot selection methods. The models are evaluated on 19 diverse OOD datasets across three tasks: natural language inference (NLI), fact-checking, and hallucination detection in abstractive summarization. For the generated explanation evaluation, we conduct a human study on 13 selected models and study its correlation with the Acceptability score (T5-11B) and three other LLM-based reference-free metrics. Human evaluation shows that the Acceptability score correlates most strongly with human judgments, demonstrating its effectiveness in evaluating free-text explanations. Our findings reveal: 1) few annotated examples effectively adapt models for OOD explanation generation; 2) compared to sample selection strategies, fine-tuning data source has a larger impact on OOD performance; and 3) models with higher label prediction accuracy tend to produce better explanations, as reflected by higher Acceptability scores. 1},
  archive      = {J_TACL},
  author       = {Yang, Jing and Glockner, Max and Rocha, Anderson and Gurevych, Iryna},
  doi          = {10.1162/tacl_a_00741},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {314-342},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Self-rationalization in the wild: A large-scale out-of-distribution evaluation on NLI-related tasks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How “Real” is your real-time simultaneous speech-to-text translation system?. <em>TACL</em>, <em>13</em>, 281-313. (<a href='https://doi.org/10.1162/tacl_a_00740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous speech-to-text translation (SimulST) translates source-language speech into target-language text concurrently with the speaker’s speech, ensuring low latency for better user comprehension. Despite its intended application to unbounded speech, most research has focused on human pre-segmented speech, simplifying the task and overlooking significant challenges. This narrow focus, coupled with widespread terminological inconsistencies, is limiting the applicability of research outcomes to real-world applications, ultimately hindering progress in the field. Our extensive literature review of 110 papers not only reveals these critical issues in current research but also serves as the foundation for our key contributions. We: 1) define the steps and core components of a SimulST system, proposing a standardized terminology and taxonomy; 2) conduct a thorough analysis of community trends; and 3) offer concrete recommendations and future directions to bridge the gaps in existing literature, from evaluation frameworks to system architectures, for advancing the field towards more realistic and effective SimulST solutions.},
  archive      = {J_TACL},
  author       = {Papi, Sara and Polák, Peter and Macháček, Dominik and Bojar, Ondřej},
  doi          = {10.1162/tacl_a_00740},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {281-313},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {How “Real” is your real-time simultaneous speech-to-text translation system?},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating cultural chasms: Exploring and unlocking the cultural POV of text-to-image models. <em>TACL</em>, <em>13</em>, 142-166. (<a href='https://doi.org/10.1162/tacl_a_00732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TACL},
  author       = {Ventura, Mor and Ben-David, Eyal and Korhonen, Anna and Reichart, Roi},
  doi          = {10.1162/tacl_a_00732},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {4},
  pages        = {142-166},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Navigating cultural chasms: Exploring and unlocking the cultural POV of text-to-image models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From robustness to improved generalization and calibration in pre-trained language models. <em>TACL</em>, <em>13</em>, 264-280. (<a href='https://doi.org/10.1162/tacl_a_00739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enforcing representation smoothness in pre-trained language models (PLMs) through Jacobian and Hessian regularization provides an effective approach for enhancing both robustness and generalization. Although such regularization methods have proven effective in computer vision, their application in natural language processing, where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce JacHess , a regularization approach for PLMs that minimizes the norms of the Jacobian and Hessian matrices in intermediate representations, using embeddings as substitutes for discrete token inputs. JacHess supports dual-mode regularization, alternating between fine-tuning with labeled data and regularization with unlabeled data. We evaluate JacHess on the GLUE benchmark and demonstrate that it consistently and significantly improves in-distribution generalization and enhances performance under domain shift. Across diverse PLMs, JacHess outperforms comparable representation-based regularization methods and unregularized fine-tuning, while also improving model calibration. Our findings, coupled with a computationally efficient estimator for the Jacobian and Hessian norms, position JacHess as a robust and widely applicable solution for enhancing PLM performance.},
  archive      = {J_TACL},
  author       = {Jukić, Josip and Šnajder, Jan},
  doi          = {10.1162/tacl_a_00739},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {3},
  pages        = {264-280},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {From robustness to improved generalization and calibration in pre-trained language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised neural topic modeling with label alignment. <em>TACL</em>, <em>13</em>, 249-263. (<a href='https://doi.org/10.1162/tacl_a_00738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural topic modeling is a scalable automated technique for text data mining. In various downstream tasks of topic modeling, it is preferred that the discovered topics well align with labels. However, due to the lack of guidance from labels, unsupervised neural topic models are less powerful in this situation. Existing supervised neural topic models often adopt a label-free prior to generate the latent document-topic distributions and use them to predict the labels and thus achieve label-topic alignment indirectly. Such a mechanism faces the following issues: 1) The label-free prior leads to topics blending the latent patterns of multiple labels; and 2) One is unable to intuitively identify the explicit relationships between labels and the discovered topics. To tackle these problems, we develop a novel supervised neural topic model which utilizes a chain-structured graphical model with a label-conditioned prior. Soft indicators are introduced to explicitly construct the label-topic relationships. To obtain well-organized label-topic relationships, we formalize an entropy-regularized optimal transport problem on the embedding space and model them as the transport plan. Moreover, our proposed method can be flexibly integrated with most existing unsupervised neural topic models. Experimental results on multiple datasets demonstrate that our model can greatly enhance the alignment between labels and topics while maintaining good topic quality.},
  archive      = {J_TACL},
  author       = {Chen, Ruihao and Chen, Hegang and Lu, Yuyin and Rao, Yanghui and Zhu, Chunjiang},
  doi          = {10.1162/tacl_a_00738},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {3},
  pages        = {249-263},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Supervised neural topic modeling with label alignment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benchmarking uncertainty quantification methods for large language models with LM-polygraph. <em>TACL</em>, <em>13</em>, 220-248. (<a href='https://doi.org/10.1162/tacl_a_00737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of large language models (LLMs) has stimulated researchers to seek effective and efficient approaches to deal with LLM hallucinations and low-quality outputs. Uncertainty quantification (UQ) is a key element of machine learning applications in dealing with such challenges. However, research to date on UQ for LLMs has been fragmented in terms of techniques and evaluation methodologies. In this work, we address this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines and offers an environment for controllable and consistent evaluation of novel UQ techniques over various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across eleven tasks, identifying the most effective approaches.},
  archive      = {J_TACL},
  author       = {Vashurin, Roman and Fadeeva, Ekaterina and Vazhentsev, Artem and Rvanova, Lyudmila and Vasilev, Daniil and Tsvigun, Akim and Petrakov, Sergey and Xing, Rui and Sadallah, Abdelrahman and Grishchenkov, Kirill and Panchenko, Alexander and Baldwin, Timothy and Nakov, Preslav and Panov, Maxim and Shelmanov, Artem},
  doi          = {10.1162/tacl_a_00737},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {3},
  pages        = {220-248},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Benchmarking uncertainty quantification methods for large language models with LM-polygraph},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformers as transducers. <em>TACL</em>, <em>13</em>, 200-219. (<a href='https://doi.org/10.1162/tacl_a_00736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of (total functional) transductions. We do so using variants of RASP, a programming language designed to help people “think like transformers,” as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence transductions and show that it computes exactly the first-order rational transductions (such as string rotation). Then, we introduce two new extensions. B-RASP[ pos ] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular transductions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular transductions. Finally, we show that masked average-hard attention transformers can simulate S-RASP.},
  archive      = {J_TACL},
  author       = {Strobl, Lena and Angluin, Dana and Chiang, David and Rawski, Jonathan and Sabharwal, Ashish},
  doi          = {10.1162/tacl_a_00736},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {200-219},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Transformers as transducers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OPT-tree: Speculative decoding with adaptive draft tree structure. <em>TACL</em>, <em>13</em>, 188-199. (<a href='https://doi.org/10.1162/tacl_a_00735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a “draft and then verify” mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which do not adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we propose OPT-Tree, an algorithm to construct adaptive and scalable draft trees, which can be applied to any autoregressive draft model. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree .},
  archive      = {J_TACL},
  author       = {Wang, Jikai and Su, Yi and Li, Juntao and Xia, Qingrong and Ye, Zi and Duan, Xinyu and Wang, Zhefeng and Zhang, Min},
  doi          = {10.1162/tacl_a_00735},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {188-199},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {OPT-tree: Speculative decoding with adaptive draft tree structure},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A confidence-based acquisition model for self-supervised active learning and label correction. <em>TACL</em>, <em>13</em>, 167-187. (<a href='https://doi.org/10.1162/tacl_a_00734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labeling. To address these challenges, we present CAMEL ( C onfidence-based A cquisition M odel for E fficient self-supervised active L earning), a pool-based active learning framework tailored to sequential multi-output problems. CAMEL possesses two core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, and (2) it facilitates self-supervision for the remainder of the sequence. By deploying a label correction mechanism, CAMEL can also be utilized for data cleaning. We evaluate CAMEL on two sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited and noisy datasets. Our experiments demonstrate that CAMEL significantly outperforms the baselines in terms of efficiency. Furthermore, the data corrections suggested by our method contribute to an overall improvement in the quality of the resulting datasets. 1},
  archive      = {J_TACL},
  author       = {Niekerk, Carel van and Geishauser, Christian and Heck, Michael and Feng, Shutong and Lin, Hsien-chin and Lubis, Nurul and Ruppik, Benjamin and Vukovic, Renato and Gašić, Milica},
  doi          = {10.1162/tacl_a_00734},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {167-187},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {A confidence-based acquisition model for self-supervised active learning and label correction},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning syntax without planting trees: Understanding hierarchical generalization in transformers. <em>TACL</em>, <em>13</em>, 121-141. (<a href='https://doi.org/10.1162/tacl_a_00733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers trained on natural language data have been shown to exhibit hierarchical generalization without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such preference for hierarchical generalization. We extensively experiment with transformers trained on five synthetic, controlled datasets using several training objectives and show that, while objectives such as sequence-to-sequence modeling, classification, etc., often fail to lead to hierarchical generalization, the language modeling objective consistently leads to transformers generalizing hierarchically. We then study how different generalization behaviors emerge during the training by conducting pruning experiments that reveal the joint existence of subnetworks within the model implementing different generalizations. Finally, we take a Bayesian perspective to understand transformers’ preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and if the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization. Overall, our work presents new insights on the origins of hierarchical generalization in transformers and provides a theoretical framework for studying generalization in language models.},
  archive      = {J_TACL},
  author       = {Ahuja, Kabir and Balachandran, Vidhisha and Panwar, Madhur and He, Tianxing and Smith, Noah A. and Goyal, Navin and Tsvetkov, Yulia},
  doi          = {10.1162/tacl_a_00733},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {121-141},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Learning syntax without planting trees: Understanding hierarchical generalization in transformers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating critical period effects in language acquisition through neural language models. <em>TACL</em>, <em>13</em>, 96-120. (<a href='https://doi.org/10.1162/tacl_a_00725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans appear to have a critical period (CP) for language acquisition: Second language (L 2 ) acquisition becomes harder after early childhood, and ceasing exposure to a first language (L 1 ) after this period (but not before) typically does not lead to substantial loss of L 1 proficiency. It is unknown whether these CP effects result from innately determined brain maturation or as a stabilization of neural connections naturally induced by experience. In this study, we use language models (LMs) to test the extent to which these phenomena are peculiar to humans, or shared by a broader class of language learners. We vary the age of exposure by training LMs on language pairs in various experimental conditions, and find that LMs, which lack any direct analog to innate maturational stages, do not show CP effects when the age of exposure of L 2 is delayed. Our results contradict the claim that CP effects are an inevitable result of statistical learning, and they are consistent with an innate mechanism for CP effects. We show that we can reverse-engineer the CP by introducing a regularizer partway through training to simulate a maturational decrease in plasticity. All in all, our results suggest that L 1 learning on its own may not be enough to induce a CP, and additional engineering is necessary to make language models more cognitively plausible.},
  archive      = {J_TACL},
  author       = {Constantinescu, Ionut and Pimentel, Tiago and Cotterell, Ryan and Warstadt, Alex},
  doi          = {10.1162/tacl_a_00725},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {96-120},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Investigating critical period effects in language acquisition through neural language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Salute the classic: Revisiting challenges of machine translation in the age of large language models. <em>TACL</em>, <em>13</em>, 73-95. (<a href='https://doi.org/10.1162/tacl_a_00730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017 ) that have acted as benchmarks for progress in this field. This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch , amount of parallel data , rare word prediction , translation of long sentences , attention model as word alignment , and sub-optimal beam search . Our empirical findings show that LLMs effectively reduce reliance on parallel data for major languages during pretraining and significantly improve translation of long sentences containing approximately 80 words, even translating documents up to 512 words. Despite these improvements, challenges in domain mismatch and rare word prediction persist. While NMT-specific challenges like word alignment and beam search may not apply to LLMs, we identify three new challenges in LLM-based translation: inference efficiency, translation of low-resource languages during pretraining, and human-aligned evaluation.},
  archive      = {J_TACL},
  author       = {Pang, Jianhui and Ye, Fanghua and Wong, Derek Fai and Yu, Dian and Shi, Shuming and Tu, Zhaopeng and Wang, Longyue},
  doi          = {10.1162/tacl_a_00730},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {73-95},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Salute the classic: Revisiting challenges of machine translation in the age of large language models},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLAPnq: Cohesive long-form answers from passages in natural questions for RAG systems. <em>TACL</em>, <em>13</em>, 53-72. (<a href='https://doi.org/10.1162/tacl_a_00729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present CLAPnq , a benchmark Long-form Question Answering dataset for the full RAG pipeline. CLAPnq includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The CLAPnq answers are concise , 3x smaller than the full passage, and cohesive , meaning that the answer is composed fluently, often by integrating multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at CLAPnq . We present baseline experiments and analysis for CLAPnq that highlight areas where there is still significant room for improvement in grounded RAG. CLAPnq is publicly available at https://github.com/primeqa/clapnq .},
  archive      = {J_TACL},
  author       = {Rosenthal, Sara and Sil, Avirup and Florian, Radu and Roukos, Salim},
  doi          = {10.1162/tacl_a_00729},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {53-72},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {CLAPnq: Cohesive long-form answers from passages in natural questions for RAG systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpiRit-LM: Interleaved spoken and written language model. <em>TACL</em>, <em>13</em>, 30-52. (<a href='https://doi.org/10.1162/tacl_a_00728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce SpiRit-LM , a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code. 1 , 2},
  archive      = {J_TACL},
  author       = {Nguyen, Tu Anh and Muller, Benjamin and Yu, Bokai and Costa-jussa, Marta R. and Elbayad, Maha and Popuri, Sravya and Ropers, Christophe and Duquenne, Paul-Ambroise and Algayres, Robin and Mavlyutov, Ruslan and Gat, Itai and Williamson, Mary and Synnaeve, Gabriel and Pino, Juan and Sagot, Benoît and Dupoux, Emmanuel},
  doi          = {10.1162/tacl_a_00728},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {30-52},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {SpiRit-LM: Interleaved spoken and written language model},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dolomites: Domain-specific long-form methodical tasks. <em>TACL</em>, <em>13</em>, 1-29. (<a href='https://doi.org/10.1162/tacl_a_00727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experts in various fields routinely perform methodical writing tasks to plan, organize, and report their work. From a clinician writing a differential diagnosis for a patient, to a teacher writing a lesson plan for students, these tasks are pervasive, requiring to methodically generate structured long-form output for a given input. We develop a typology of methodical tasks structured in the form of a task objective, procedure, input, and output, and introduce DoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited from hundreds of experts from across 25 fields. Our benchmark further contains specific instantiations of methodical tasks with concrete input and output examples (1,857 in total) which we obtain by collecting expert revisions of up to 10 model-generated examples of each task. We use these examples to evaluate contemporary language models, highlighting that automating methodical tasks is a challenging long-form generation problem, as it requires performing complex inferences, while drawing upon the given context as well as domain knowledge. Our dataset is available at https://dolomites-benchmark.github.io/ .},
  archive      = {J_TACL},
  author       = {Malaviya, Chaitanya and Agrawal, Priyanka and Ganchev, Kuzman and Srinivasan, Pranesh and Huot, Fantine and Berant, Jonathan and Yatskar, Mark and Das, Dipanjan and Lapata, Mirella and Alberti, Chris},
  doi          = {10.1162/tacl_a_00727},
  journal      = {Transactions of the Association for Computational Linguistics},
  month        = {2},
  pages        = {1-29},
  shortjournal = {Trans. Assoc. Comput. Lingu.},
  title        = {Dolomites: Domain-specific long-form methodical tasks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
