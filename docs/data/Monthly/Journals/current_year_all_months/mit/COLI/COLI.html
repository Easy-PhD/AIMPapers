<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COLI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coli">COLI - 28</h2>
<ul>
<li><details>
<summary>
(2025). Natural language processing RELIES on linguistics. <em>COLI</em>, <em>51</em>(3), 1009--1032. (<a href='https://doi.org/10.1162/coli_a_00560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES , which encapsulates six major facets where linguistics contributes to NLP: R esources, E valuation, L ow-resource settings, I nterpretability, E xplanation, and the S tudy of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-à-vis systems of human language.},
  archive      = {J_COLI},
  author       = {Opitz, Juri and Wein, Shira and Schneider, Nathan},
  doi          = {10.1162/coli_a_00560},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1009--1032},
  shortjournal = {Comput. Lingu.},
  title        = {Natural language processing RELIES on linguistics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated essay scoring. <em>COLI</em>, <em>51</em>(3), 1005--1008. (<a href='https://doi.org/10.1162/coli_r_00513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated essay scoring is concerned with the development of language technologies that make it possible for an essay—or any piece of writing for that matter—to be evaluated or scored by a computer. These technologies find their utility primarily in the context of educational measurement, where they serve a dual purpose. On the one hand, they provide crucial support to educators and institutions, facilitating the assessment of students’ writing skills and content knowledge. A good example is the TOEFL iBT®, the Internet-based Test of English as a Foreign Language, administered by the Educational Testing Service and widely adopted by institutions worldwide. On the other hand, these technologies benefit writers themselves, including students, by offering a platform to assess and enhance their writing skills. One illustrative tool for this purpose is the Write &amp; Improve software developed at the University of Cambridge.The field of automated essay scoring emerged in the pioneering era of artificial intelligence and computational linguistics, with the inception of Ellis Page’s Project Essay Grade system at the University of Connecticut in 1964, and the subsequent publication of his seminal article “The Imminence of…Grading Essays by Computer” in 1966. Page’s automated scoring system is seen by the authors of this book as one of the first concrete applications of natural language processing, after the Audrey system for speech recognition and the Georgetown–IBM demonstration of machine translation. But just like speech recognition and machine translation, the industrialization of automated essay scoring mostly gained momentum in the 1990s. This decade saw an increase in richly annotated language data and corpora, which enabled the use of statistical and supervised machine learning in developing essay-scoring systems. Pearson’s Intelligent Essay Assessor™, released in 1998, is a prominent example of this evolution. A year later, in 1999, the e-rater® scoring engine was launched by the Educational Testing Service, commonly abbreviated as ETS, a private organization overseeing several standardized tests and high-stakes examinations such as TOEFL® and GRE®.This book is written by Beata Beigman Klebanov and Nitin Madnani, two researchers at ETS with many years of research experience and numerous peer-reviewed publications in the field of automated essay scoring. Their book provides a concise yet indispensable introduction to the field. After this short introduction, the eager reader is invited to take a deep dive into the scientific literature, encompassing slightly over half of the book’s content, approximately 115 pages. The literature review primarily caters to computational linguists and NLP practitioners, as it delves comprehensively into diverse machine learning models—ranging from linear regression to artificial neural networks—and intricate linguistic features. These features include general indicators of writing quality, such as text organization, coherence, and grammaticality, as well as genre-specific features, such as argument structure in argumentative essays. Furthermore, the reader not only gains insight into the extensive research carried out at ETS throughout the years but also delves into their technical expertise through a series of guided experiments with RSMTool—an open-source tool developed by the second author and colleagues. In addition, the authors also provide insight into a scalable and production-ready computer architecture used to build ETS products such as c-rater™, e-rater®, Language Muse®, and Writing Mentor™.The book is divided into five parts counting 13 chapters in total. The first part contains an introductory chapter in which the authors introduce the reader to Page’s aforementioned seminal 1966 article. Chapter 1 enumerates several arguments made by Page in favor of automated essay scoring, including its educational need, computational feasibility, high quality, and low cost. The chapter then sets forth four challenges associated with automated essay scoring. Two among them are the evaluation of original and source-based writing. Original writing, which reflects an author’s unique voice and thus stands out from existing and conventional works, poses a challenge because its originality could be overlooked or even under-evaluated by a computer. Source-based writing, a form of writing that reviews main points from external sources, presents a different problem because the assessment focuses on the correctness of content rather than measuring writing skills and essay quality. Another challenge is avoiding potential gaming strategies that test takers may employ to inflate their scores. A final challenge is automated feedback, as the effectiveness of providing linguistic and stylistic commentary on written work is not always proven.The second part contains two chapters covering a series of guided experiments and a set of best practices when building an automated essay scoring system. Chapter 2 provides a step-by-step guide for building an automated scoring system with supervised machine learning. First, the reader learns the usual engineering setup, including the use of a standard dataset (viz., the Automated Student Assessment Prize competition), an interpretable machine learning model (viz., linear regression), and a set of basic features derived from a scoring rubric. The authors also introduce the reader to their RSMTool. Then, the reader is guided through a series of ten experiments illustrating the incremental development (experiments 1–5) and evaluation (experiments 6–10) of the machine learning model. In each experiment, the reader is taught an important lesson such as feature fairness. For each experiment, the authors refer the reader to a report (i.e., correction key) available online.Chapter 3 provides some best practices for building an automated essay scoring system. First, the authors identify potentially conflicting perspectives between NLP developers and other stakeholders, including end users, subject matter experts, and business units. The authors then describe three natural use cases for automatic essay scoring, where it is added to a pre-existing assessment, developed concurrently with a new assessment, or implemented in a classroom. The authors provide some practical considerations and concrete actions that are well thought out and will appeal to many different readers.The third part contains five chapters that provide an up-to-date overview of the scientific literature and a more detailed account of the concepts introduced in the previous chapters. Chapter 4 describes various statistical models, including linear regression, latent semantic analysis, support vector machines, random forests, ensemble methods, and neural networks. For each of these models, the authors describe their mathematical underpinnings and explain how they can be used to score an essay. The chapter pays special attention to recent deep-learning architectures for automated essay scoring.Chapters 5 and 6 describe computational features that capture various aspects of the writing construct and the scoring rubric. Chapter 5 deals with general features. The features are organized into three classes: discourse features aiming to capture essay organization, development, and coherence; content features related to vocabulary use and topicality; and conventional features based on grammatical error detection. Chapter 6 then gives an in-depth overview of computational features that pertain to specific writing genres. The chapter is focused on four genres: argumentative writing, narrative writing, source-based writing, and reflective writing. Argumentative writing involves defending a particular position on a given topic (e.g., why technology should be integrated into education), presenting several claims as to why this position is valid, and supporting these claims with premises and evidence. Narrative writing involves telling a story that describes, for example, the historical evolution of technology in education. Source-based writing involves summarizing and comparing key points from external sources to compose an informed essay, which, for example, reviews the effectiveness of integrating technology into education based on scholarly sources. Finally, reflective writing involves examining personal experiences, such as teachers describing their experiences integrating technology into the classroom and reflecting on important lessons learned from this experience. This detailed overview of different writing genres also interestingly introduces the reader to research from related fields, such as computational argumentation and text summarization.Chapters 7 and 8 address two concerns in setting up real-world applications of automated essay scoring. Chapter 7 deals with the issues of reliability, scalability, and flexibility when deploying a scoring system at a large scale. The chapter describes and illustrates an Apache Storm architecture implemented in several ETS systems. Chapter 8 is about evaluating construct validity and fairness. When deploying an essay scoring system for high-stakes testing, fundamental issues arise when the system fails to measure the construct, assigns scores influenced by factors irrelevant to the measured construct, or is biased towards specific personal characteristics in the population of test takers. If an essay scoring system overlooks important features or favors particular writing styles or cultural representations, it undermines the validity and fairness of high-stakes assessments.The fourth part contains four chapters examining some broader challenges introduced in the first chapter and which remain to be solved for automated scoring. Chapter 9 deals with the automated generation of useful feedback on writing. The authors review several existing feedback systems and discuss how to define and evaluate the usefulness of feedback. Chapter 10 focuses on evaluating essay content, which is separate from assessing essay quality. Content scoring emphasizes measuring test-takers’ content knowledge, prioritizing these elements over writing skills. This evaluation can adopt either a reference-based or a response-based approach. Reference-based scoring involves comparing responses to a set of predefined reference answers, while response-based scoring independently assesses the response content. The chapter primarily explores the latter, investigating computational features and models tailored for this approach. Chapter 11 deals with another task related to but different from essay scoring, namely the automated scoring of spontaneous speech. After a brief account of the challenges with automated speech recognition, the authors review three sets of features for speech scoring: the delivery and fluency of spontaneous speech, vocabulary and grammar use, and topic development. The authors also contrast features relevant for scoring speech with those relevant for scoring writing. Lastly, chapter 12 examines several gaming strategies test-takers could use to fool the automated scoring system into giving a higher score. The authors review four types of strategies: the unnecessary use of shell language, the artificial generation of essays, the submission of off-topic responses, and the use of canned responses or plagiarized essays.The fifth and final part of the book contains a concluding chapter. The authors revisit the desiderata put forth by Ellis Page in his 1966 publication and summarize the overall achievements and remaining challenges in this respect. In addition, the authors discuss other challenging aspects that Page did not envision, such as the present-day ubiquity of technology, dealing with multiple languages, and setting up high-stakes tests that are valid, defensible, and fair.In sum, the book offers an excellent introduction to and deepening of the field of automated essay scoring. The book is well-structured and easy to read. Throughout the book, the authors provide thoughtful insights and practical advice based on their many years of experience at ETS. Compared to other books on the subject, the book offers a valuable combination of practical lessons and scientific deepening. By the end of the book, the reader has acquired a broad knowledge of the possibilities, challenges, and practical concerns involved with the automated scoring of student writing.},
  archive      = {J_COLI},
  author       = {Tack, Anaïs},
  doi          = {10.1162/coli_r_00513},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {1005--1008},
  shortjournal = {Comput. Lingu.},
  title        = {Automated essay scoring},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey of cultural awareness in language models: Text and beyond. <em>COLI</em>, <em>51</em>(3), 907--1004. (<a href='https://doi.org/10.1162/COLI.a.14'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive, going beyond multilinguality and building on findings from psychology and anthropology. In this article, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking definitions of culture from the anthropology and psychology literature as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of human–computer interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature. 1},
  archive      = {J_COLI},
  author       = {Pawar, Siddhesh and Park, Junyeong and Jin, Jiho and Arora, Arnav and Myung, Junho and Yadav, Srishti and Haznitrama, Faiz Ghifari and Song, Inhwa and Oh, Alice and Augenstein, Isabelle},
  doi          = {10.1162/COLI.a.14},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {907--1004},
  shortjournal = {Comput. Lingu.},
  title        = {Survey of cultural awareness in language models: Text and beyond},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models are biased because they are large language models. <em>COLI</em>, <em>51</em>(3), 885--906. (<a href='https://doi.org/10.1162/coli_a_00558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This position paper’s primary goal is to provoke thoughtful discussion about the relationship between bias and fundamental properties of large language models (LLMs). I do this by seeking to convince the reader that harmful biases are an inevitable consequence arising from the design of any large language model as LLMs are currently formulated. To the extent that this is true, it suggests that the problem of harmful bias cannot be properly addressed without a serious reconsideration of AI driven by LLMs, going back to the foundational assumptions underlying their design.},
  archive      = {J_COLI},
  author       = {Resnik, Philip},
  doi          = {10.1162/coli_a_00558},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {885--906},
  shortjournal = {Comput. Lingu.},
  title        = {Large language models are biased because they are large language models},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics. <em>COLI</em>, <em>51</em>(3), 843--883. (<a href='https://doi.org/10.1162/coli_a_00543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate two essential challenges in the context of hierarchical topic modeling (HTM)—(i) the impact of data representation and (ii) topic evaluation. The data representation directly influences the performance of the topic generation, and the impact of new representations such as contextual embeddings in this task has been under-investigated. Topic evaluation , responsible for driving the advances in the field, assesses the overall quality of the topic generation process. HTM studies exploit the exact topic modeling (TM) evaluation metrics as traditional TM to measure the quality of topics. One significant result of our work is demonstrating that the HTM’s hierarchical nature demands novel ways of evaluating the quality of topics. As our main contribution, we propose two new topic quality metrics to assess the topical quality of the hierarchical structures. Uniqueness considers topic topological consistency, while the Semantic Hierarchical Structure (SHS) captures the semantic relatedness of the hierarchies. We also present an additional advance to the state-of-the-art by proposing the c-CluHTM. To the best of our knowledge, c-CluHTM is the first method that exploits contextual embeddings into NMF in HTM tasks. c-CluHTM enhances the topics’ semantics while preserving the hierarchical structure. We perform an experimental evaluation, and our results demonstrate the superiority of our proposal with gains between 12% and 21%, regarding NPMI and Coherence over the best baselines. Regarding the newly proposed metrics, our results reveal that Uniqueness and SHS can capture relevant information about the structure of the hierarchical topics that traditional metrics cannot.},
  archive      = {J_COLI},
  author       = {Viegas, Felipe and Pereira, Antonio and Cunha, Washington and França, Celso and Andrade, Claudio and Tuler, Elisa and Rocha, Leonardo and Gonçalves, Marcos André},
  doi          = {10.1162/coli_a_00543},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {843--883},
  shortjournal = {Comput. Lingu.},
  title        = {Exploiting contextual embeddings in hierarchical topic modeling and investigating the limits of the current evaluation metrics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The emergence of chunking structures with hierarchical RNN. <em>COLI</em>, <em>51</em>(3), 815--841. (<a href='https://doi.org/10.1162/coli_a_00545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Natural Language Processing (NLP), predicting linguistic structures, such as parsing and chunking, has mostly relied on manual annotations of syntactic structures. This article introduces an unsupervised approach to chunking, a syntactic task that involves grouping words in a non-hierarchical manner. We present a Hierarchical Recurrent Neural Network (HRNN) designed to model word-to-chunk and chunk-to-sentence compositions. Our approach involves a two-stage training process: pretraining with an unsupervised parser and finetuning on downstream NLP tasks. Experiments on multiple datasets reveal a notable improvement of unsupervised chunking performance in both pretraining and finetuning stages. Interestingly, we observe that the emergence of the chunking structure is transient during the neural model’s downstream-task training. This study contributes to the advancement of unsupervised syntactic structure discovery and opens avenues for further research in linguistic theory. 1},
  archive      = {J_COLI},
  author       = {Wu, Zijun and Deshmukh, Anup Anand and Wu, Yongkang and Lin, Jimmy and Mou, Lili},
  doi          = {10.1162/coli_a_00545},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {815--841},
  shortjournal = {Comput. Lingu.},
  title        = {The emergence of chunking structures with hierarchical RNN},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tokenization changes meaning in large language models: Evidence from chinese. <em>COLI</em>, <em>51</em>(3), 785--814. (<a href='https://doi.org/10.1162/coli_a_00557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models segment many words into multiple tokens, and there is mixed evidence as to whether tokenization affects how state-of-the-art models represent meanings. Chinese characters present an opportunity to investigate this issue: They contain semantic radicals, which often convey useful information; characters with the same semantic radical tend to begin with the same one or two bytes (when using UTF-8 encodings); and tokens are common strings of bytes, so characters with the same radical often begin with the same token. This study asked GPT-4, GPT-4o, and Llama 3 whether characters contain the same semantic radical, elicited semantic similarity ratings, and conducted odd-one-out tasks (i.e., which character is not like the others). In all cases, misalignment between tokens and radicals systematically corrupted representations of Chinese characters. In experiments comparing characters represented by single tokens to multi-token characters, the models were less accurate for single-token characters, which suggests that segmenting words into fewer, longer tokens obscures valuable information in word form and will not resolve the problems introduced by tokenization. In experiments with 12 European languages, misalignment between tokens and suffixes systematically corrupted categorization of words by all three models, which suggests that the tendency to treat malformed tokens like linguistic units is pervasive.},
  archive      = {J_COLI},
  author       = {Haslett, David A.},
  doi          = {10.1162/coli_a_00557},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {785--814},
  shortjournal = {Comput. Lingu.},
  title        = {Tokenization changes meaning in large language models: Evidence from chinese},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniASA: A unified generative framework for argument structure analysis. <em>COLI</em>, <em>51</em>(3), 739--784. (<a href='https://doi.org/10.1162/coli_a_00553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation is a fundamental human activity that involves reasoning and persuasion, which also serves as the basis for the development of AI systems capable of complex reasoning. In NLP, to better understand human argumentation, argument structure analysis aims to identify argument components, such as claims and premises, and their relations from free text. It encompasses a variety of divergent tasks, such as end-to-end argument mining, argument pair extraction, and argument quadruplet extraction. Existing methods are usually tailored to only one specific argument structure analysis task, overlooking the inherent connections among different tasks. We observe that the fundamental goal of these tasks is similar: identifying argument components and their interrelations. Motivated by this, we present a unified generative framework for argument structure analysis (UniASA). It can uniformly address multiple argument structure analysis tasks in a sequence-to-sequence manner. Further, we enhance UniASA with a multi-view learning strategy based on subtask decomposition. We conduct experiments on seven datasets across three tasks. The results indicate that UniASA can address these tasks uniformly and achieve performance that is either superior to or comparable with the previous state-of-the-art methods. Also, we show that UniASA can be effectively integrated with large language models, such as Llama, through fine-tuning or in-context learning.},
  archive      = {J_COLI},
  author       = {Bao, Jianzhu and Jing, Mohan and Dong, Kuicai and Sun, Aixin and Sun, Yang and Xu, Ruifeng},
  doi          = {10.1162/coli_a_00553},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {739--784},
  shortjournal = {Comput. Lingu.},
  title        = {UniASA: A unified generative framework for argument structure analysis},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graded suspiciousness of adversarial texts to humans. <em>COLI</em>, <em>51</em>(3), 705--738. (<a href='https://doi.org/10.1162/coli_a_00555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples pose a significant challenge to deep neural networks across both image and text domains, with the intent to degrade model performance through carefully altered inputs. Adversarial texts, however, are distinct from adversarial images due to their requirement for semantic similarity and the discrete nature of the textual contents. This study delves into the concept of human suspiciousness, a quality distinct from the traditional focus on imperceptibility found in image-based adversarial examples, where adversarial changes are often desired to be indistinguishable to the human eye even when placed side by side with originals. Although this is generally not possible with text, textual adversarial content must still often remain undetected or non-suspicious to human readers. Even when the text’s purpose is to deceive NLP systems or bypass filters, the text is often expected to be natural to read. In this research, we expand the study of human suspiciousness by analyzing how individuals perceive adversarial texts. We gather and publish a novel dataset of Likert-scale human evaluations on the suspiciousness of adversarial sentences, crafted by four widely used adversarial attack methods and assess their correlation with the human ability to detect machine-generated alterations. Additionally, we develop a regression-based model to predict levels of suspiciousness and establish a baseline for future research in reducing the suspiciousness in adversarial text generation. We also demonstrate how the regressor-generated suspicious scores can be incorporated into adversarial generation methods to produce texts that are less likely to be perceived as computer-generated.},
  archive      = {J_COLI},
  author       = {Tonni, Shakila Mahjabin and Faustini, Pedro and Dras, Mark},
  doi          = {10.1162/coli_a_00555},
  journal      = {Computational Linguistics},
  month        = {9},
  number       = {3},
  pages        = {705--738},
  shortjournal = {Comput. Lingu.},
  title        = {Graded suspiciousness of adversarial texts to humans},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Socially aware language technologies: Perspectives and practices. <em>COLI</em>, <em>51</em>(2), 689--703. (<a href='https://doi.org/10.1162/coli_a_00556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language technologies have advanced substantially, particularly with the introduction of large language models. However, these advancements can exacerbate several issues that models have traditionally faced, including bias, evaluation, and risk. In this perspective piece, we argue that many of these issues share a common core: a lack of awareness of the social factors, interactions, and implications of the social environment in which NLP operates. We call this social awareness . While NLP is improving at addressing linguistic issues, there has been relatively limited progress in incorporating social awareness into models to work in all situations for all users. Integrating social awareness into NLP will improve the naturalness, usefulness, and safety of applications while also opening up new applications. Today, we are only at the start of a new, important era in the field.},
  archive      = {J_COLI},
  author       = {Yang, Diyi and Hovy, Dirk and Jurgens, David and Plank, Barbara},
  doi          = {10.1162/coli_a_00556},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {689--703},
  shortjournal = {Comput. Lingu.},
  title        = {Socially aware language technologies: Perspectives and practices},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-based NLG evaluation: Current status and challenges. <em>COLI</em>, <em>51</em>(2), 661--687. (<a href='https://doi.org/10.1162/coli_a_00561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating natural language generation (NLG) is a vital but challenging problem in natural language processing. Traditional evaluation metrics mainly capturing content (e.g., n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human–LLM collaborative evaluation. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. Lastly, we discuss several open problems in this area and point out future research directions.},
  archive      = {J_COLI},
  author       = {Gao, Mingqi and Hu, Xinyu and Yin, Xunjian and Ruan, Jie and Pu, Xiao and Wan, Xiaojun},
  doi          = {10.1162/coli_a_00561},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {661--687},
  shortjournal = {Comput. Lingu.},
  title        = {LLM-based NLG evaluation: Current status and challenges},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Language models and externalism: A reply to mandelkern and linzen. <em>COLI</em>, <em>51</em>(2), 651--659. (<a href='https://doi.org/10.1162/coli_a_00551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Do texts generated by language models (LMs) refer? Mandelkern and Linzen ( 2024 ) argue that externalist principles point to an affirmative conclusion. What grounds reference, according to their externalism, is a term’s “natural history”. For example, ‘water’ refers to H 2 O among English speakers, and not to the phenomenally indistinguishable chemical XYZ, because H 2 O, and not XYZ, is implicated in the natural history of ‘water’. Appealing to the literature on contrastive explanation, I show that a term’s natural history does not generally ground its referential properties. Thus, Mandelkern and Linzen’s quick route to the referentiality of LM-generated texts fails.},
  archive      = {J_COLI},
  author       = {Ostertag, Gary},
  doi          = {10.1162/coli_a_00551},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {651--659},
  shortjournal = {Comput. Lingu.},
  title        = {Language models and externalism: A reply to mandelkern and linzen},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kallini et al. (2024) do not compare impossible languages with constituency-based ones. <em>COLI</em>, <em>51</em>(2), 641--650. (<a href='https://doi.org/10.1162/coli_a_00554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central goal of linguistic theory is to find a precise characterization of the notion “possible human language”, in the form of a computational device that is capable of describing all and only the languages that can be acquired by a typically developing human child. The success of recent large language models (LLMs) in NLP applications arguably raises the possibility that LLMs might be computational devices that meet this goal. This would only be the case if, in addition to succeeding in learning human languages, LLMs struggle to learn “impossible” human languages. Kallini et al. ( 2024 ) conducted experiments aiming to test this by training GPT-2 on a variety of synthetic languages, and found that it learns some more successfully than others. They present these asymmetries as support for the idea that LLMs’ inductive biases align with what is regarded as “possible” for human languages, but the most significant comparison has a confound that makes this conclusion unwarranted.},
  archive      = {J_COLI},
  author       = {Hunter, Tim},
  doi          = {10.1162/coli_a_00554},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {641--650},
  shortjournal = {Comput. Lingu.},
  title        = {Kallini et al. (2024) do not compare impossible languages with constituency-based ones},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LMLPA: Language model linguistic personality assessment. <em>COLI</em>, <em>51</em>(2), 599--640. (<a href='https://doi.org/10.1162/coli_a_00550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) are increasingly used in everyday life and research. One of the most common use cases is conversational interactions, enabled by the language generation capabilities of LLMs. Just as between two humans, a conversation between an LLM-powered entity and a human depends on the personality of the conversants. However, measuring the personality of a given LLM is currently a challenge. This article introduces the Language Model Linguistic Personality Assessment (LMLPA), a system designed to evaluate the linguistic personalities of LLMs. Our system helps to understand LLMs’ language generation capabilities by quantitatively assessing the distinct personality traits reflected in their linguistic outputs. Unlike traditional human-centric psychometrics, the LMLPA adapts a personality assessment questionnaire, specifically the Big Five Inventory, to align with the operational capabilities of LLMs, and also incorporates the findings from previous language-based personality measurement literature. To mitigate sensitivity to the order of options, our questionnaire is designed to be open-ended, resulting in textual answers. Thus, the Artificial Intelligence (AI) rater is needed to transform ambiguous personality information from text responses into clear numerical indicators of personality traits. Utilizing Principal Component Analysis and reliability validation methods, our findings demonstrate that LLMs possess distinct personality traits that can be effectively quantified by the LMLPA. This research contributes to Human-Centered AI and Computational Linguistics, providing a robust framework for future studies to refine AI personality assessments and expand their applications in multiple areas, including education and manufacturing.},
  archive      = {J_COLI},
  author       = {Zheng, Jingyao and Wang, Xian and Hosio, Simo and Xu, Xiaoxian and Lee, Lik-Hang},
  doi          = {10.1162/coli_a_00550},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {599--640},
  shortjournal = {Comput. Lingu.},
  title        = {LMLPA: Language model linguistic personality assessment},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dotless arabic text for natural language processing. <em>COLI</em>, <em>51</em>(2), 557--598. (<a href='https://doi.org/10.1162/coli_a_00535'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel representation of Arabic text as an alternative approach for Arabic NLP, inspired by the dotless script of ancient Arabic. We explored this representation through extensive analysis on various text corpora, differing in size and domain, and tokenized using multiple tokenization techniques. Furthermore, we examined the information density of this representation and compared it with the standard dotted Arabic text using text entropy analysis. Utilizing parallel corpora, we also drew comparisons between Arabic and English text analysis to gain additional insights. Our investigation extended to various upstream and downstream NLP tasks, including language modeling, text classification, sequence labeling, and machine translation, examining the implications of both the representations. Specifically, we performed seven different downstream tasks using various tokenization schemes comparing the standard dotted text with dotless Arabic text representations. Performance using both the representations was comparable across different tokenizations. However, dotless representation achieves these results with significant reduction in vocabulary sizes, and in some scenarios showing reduction of up to 50%. Additionally, we present a system that restores dots to the dotless Arabic text. This system is useful for tasks that require Arabic texts as output.},
  archive      = {J_COLI},
  author       = {Al-Shaibani, Maged S. and Ahmad, Irfan},
  doi          = {10.1162/coli_a_00535},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {557--598},
  shortjournal = {Comput. Lingu.},
  title        = {Dotless arabic text for natural language processing},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating idiomaticity in word representations. <em>COLI</em>, <em>51</em>(2), 505--555. (<a href='https://doi.org/10.1162/coli_a_00546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Idiomatic expressions are an integral part of human languages, often used to express complex ideas in compressed or conventional ways (e.g., eager beaver as a keen and enthusiastic person). However, their interpretations may not be straightforwardly linked to the meanings of their individual components in isolation and this may have an impact for compositional approaches. In this article, we investigate to what extent word representation models are able to go beyond compositional word combinations and capture multiword expression idiomaticity and some of the expected properties related to idiomatic meanings. We focus on noun compounds of varying levels of idiomaticity in two languages (English and Portuguese), presenting a dataset of minimal pairs containing human idiomaticity judgments for each noun compound at both type and token levels, their paraphrases and their occurrences in naturalistic and sense-neutral contexts, totalling 32,200 sentences. We propose this set of minimal pairs for evaluating how well a model captures idiomatic meanings, and define a set of fine-grained metrics of Affinity and Scaled Similarity, to determine how sensitive the models are to perturbations that may lead to changes in idiomaticity. Affinity is a comparative measure of the similarity between an experimental item, a target and a potential distractor, and Scaled Similarity incorporates a rescaling factor to magnify the meaningful similarities within the spaces defined by each specific model. The results obtained with a variety of representative and widely used models indicate that, despite superficial indications to the contrary in the form of high similarities, idiomaticity is not yet accurately represented in current models. Moreover, the performance of models with different levels of contextualization suggests that their ability to capture context is not yet able to go beyond more superficial lexical clues provided by the words and to actually incorporate the relevant semantic clues needed for idiomaticity. By proposing model-agnostic measures for assessing the ability of models to capture idiomaticity, this article contributes to determining limitations in the handling of non-compositional structures, which is one of the directions that needs to be considered for more natural, accurate, and robust language understanding. The source code and additional materials related to this paper are available at our GitHub repository. 1},
  archive      = {J_COLI},
  author       = {He, Wei and Vieira, Tiago Kramer and Garcia, Marcos and Scarton, Carolina and Idiart, Marco and Villavicencio, Aline},
  doi          = {10.1162/coli_a_00546},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {505--555},
  shortjournal = {Comput. Lingu.},
  title        = {Investigating idiomaticity in word representations},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eliciting and improving the causal reasoning abilities of large language models with conditional statements. <em>COLI</em>, <em>51</em>(2), 467--504. (<a href='https://doi.org/10.1162/coli_a_00548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal reasoning, the ability to identify cause-and-effect relationships, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Complex causal structures are rarely expressed explicitly in the text, which could make learning them challenging for LLMs. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like if , we want to explore whether large language models of code (Code-LLMs) acquire better causal reasoning abilities, and whether code prompts better describe the causal structure than text prompts. Our experiments show that compared with general-purpose LLMs like Llama -2 and GPT-3 , Code-LLMs like CodeLlama and Codex are significantly better in causal reasoning. Code prompts not only work well for Code-LLMs, but also help improve the performance of most general-purpose LLMs. To understand why code prompts are effective, we intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while models are more robust towards format perturbations. We further explore whether exposing models with more code with conditional statements aids in enhancing causal reasoning abilities. We finetune LLMs on such code corpus, and find their performance improves when prompted with either code prompts or text prompts. 1},
  archive      = {J_COLI},
  author       = {Liu, Xiao and Yin, Da and Zhang, Chen and Zhao, Dongyan and Feng, Yansong},
  doi          = {10.1162/coli_a_00548},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {467--504},
  shortjournal = {Comput. Lingu.},
  title        = {Eliciting and improving the causal reasoning abilities of large language models with conditional statements},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Train and constrain: Phonologically informed tongue twister generation from topics and paraphrases. <em>COLI</em>, <em>51</em>(2), 415--466. (<a href='https://doi.org/10.1162/coli_a_00544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous work in phonologically and phonetically grounded language generation has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of English tongue twisters—a form of language that is required to be conditioned on a phoneme level to maximize sound overlap, while maintaining semantic consistency with an input topic or phrase and still being grammatically correct. We present TwisterLister , a pipeline for generating phonologically informed tongue twisters from large language models (LLMs) that we use to generate TwistList 2.0 , the largest annotated dataset of tongue twisters to date, consisting of 17 k + examples from a combination of human and LLM authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside LLM prompting to generate novel, non-derivative tongue twister examples. We additionally present the results of automatic and human evaluation of smaller models trained on our generated dataset to demonstrate the extent to which phonologically motivated language types can be generated without explicit injection of phonological knowledge. Additionally, we introduce a phoneme-aware constrained decoding module ( PACD ) that can be integrated into an autoregressive language model and demonstrate that this method generates good quality tongue twisters both with and without fine-tuning the underlying language model. We also design and implement a range of automatic metrics for the task of tongue twister generation that is phonologically motivated and captures the unique essence of tongue twisters, primarily based on phonemic edit distance ( PED ). 1},
  archive      = {J_COLI},
  author       = {Loakman, Tyler and Tang, Chen and Lin, Chenghua},
  doi          = {10.1162/coli_a_00544},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {415--466},
  shortjournal = {Comput. Lingu.},
  title        = {Train and constrain: Phonologically informed tongue twister generation from topics and paraphrases},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiBiMT: A gold evaluation benchmark for studying lexical ambiguity in machine translation. <em>COLI</em>, <em>51</em>(2), 343--413. (<a href='https://doi.org/10.1162/coli_a_00541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable progress made in the field of Machine Translation (MT), current systems still struggle when translating ambiguous words, especially when these express infrequent meanings. In order to investigate and analyze the impact of lexical ambiguity on automatic translations, several tasks and evaluation benchmarks have been proposed over the course of the last few years. However, work in this research direction suffers from critical shortcomings. Indeed, existing evaluation datasets are not entirely manually curated, which significantly compromises their reliability. Furthermore, current literature fails to provide detailed insights into the nature of the errors produced by models translating ambiguous words, lacking a thorough manual analysis across languages. With a view to overcoming these limitations, we propose Disambiguation Biases in MT ( DiBiMT ), an entirely manually curated evaluation benchmark for investigating disambiguation biases in eight language combinations and assessing the ability of both commercial and non-commercial systems to handle ambiguous words. We also examine and detail the errors produced by models in this scenario by carrying out a manual error analysis in all language pairs. Additionally, we perform an extensive array of experiments aimed at studying the behavior of models when dealing with ambiguous words. Finally, we show the ineffectiveness of standard MT evaluation settings for assessing the disambiguation capabilities of systems and highlight the need for additional efforts in this research direction and ad-hoc testbeds such as DiBiMT . Our benchmark is available at: https://nlp.uniroma1.it/dibimt/ .},
  archive      = {J_COLI},
  author       = {Martelli, Federico and Perrella, Stefano and Campolungo, Niccolò and Munda, Tina and Koeva, Svetla and Tiberius, Carole and Navigli, Roberto},
  doi          = {10.1162/coli_a_00541},
  journal      = {Computational Linguistics},
  month        = {6},
  number       = {2},
  pages        = {343--413},
  shortjournal = {Comput. Lingu.},
  title        = {DiBiMT: A gold evaluation benchmark for studying lexical ambiguity in machine translation},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic language identification in texts. <em>COLI</em>, <em>51</em>(1), 339--341. (<a href='https://doi.org/10.1162/coli_r_00521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language identification (LI) for text data, in the ideal scenario, determines the human languages used at every location in a corpus. In practice this often means choosing the likeliest language at the document level: This is already quite useful, for example, when presenting a webpage to the user and deciding (a) whether to translate it and (b) which model to use for that purpose. However, nuances like code-switching (language alternation), dialect variation, and ambiguously short content are increasingly common with the ubiquity of digital communication like text messaging and micro-blogs. Geographic areas like Africa and the Indian subcontinent bring enormous linguistic diversity and flexibility that break the document-level LI paradigm. While standard references (Jurafsky and Martin 2023) introduce LI, touch on these subtleties, and often present related methods and models in other contexts, Automatic Language Identification in Texts is specifically dedicated to LI in its full practical variety.In the course of producing a broad and thorough survey, perhaps the most striking takeaway from Jauhiainen et al. is the chaotic state of research on this critical task. This might be due to the view that, for digitally well-attested languages occurring in domains with monolingual documents of at least modest length, LI is solved: These circumstances are common, and the emphasis on massive data sets can make the rarer cases seem less important. When challenges arise in specific, applied downstream research, they are often addressed in an ad hoc fashion, such as through active learning techniques for gathering human annotations or linear programming to incorporate prior knowledge (Lippincott and Van Durme 2016), without consolidation into broader outcomes for the research community. Throughout Automatic Language Identification in Texts, the authors have the consistent goal of improving this situation. The book is structured into six chapters:Chapter 1 introduces the history of LI, stretching from early feature-engineering approaches to still-standard models based on character n-grams closely related to fundamental models of communication (Shannon 1948), and the burgeoning collection of shared tasks aimed at specific domains, such as ancient scripts or regional dialects. Unlike much of machine learning for natural language processing tasks, traditional models have remained highly competitive for LI compared with deep neural networks: perhaps data sparsity prevents effective training, or traditional features are already well-suited for LI. Downstream use-cases and challenges are summarized, with copious citations to prior and ongoing work.Chapter 2 begins with the authors’ efforts to standardize the discourse around LI by specifying a common notation that subsumes the variety utilized in the literature. While the notation is a modest shift from those that treat data as a sequence of fully distinct documents, treating documents as boundaries within a single large sequence of characters consolidates the spectrum of methods that will be covered. In terms of linguistic features, the focus is on character n-grams, and the authors address several standard concerns: weighting, smoothing, and incorporating linguistic knowledge. The latter is particularly interesting and perhaps under-explored, since there is often less practical motivation to move beyond the immediate use-case and consider, for example, the phylogenetic structure of world languages. The bulk of the chapter is devoted to describing a wide range of classification methods that use these features, some standard (e.g., logistic regression, naive Bayes), others the specific ensembles or statistical tests adopted by existing research.Chapter 3 addresses evaluation, the other end of the experimental pipeline that requires standardization. While a handful of metrics have been used historically, most research has converged on macro balanced F-score, which equally weights precision and recall as well as performance on each language. In the absence of a clearly articulated reason to do otherwise, this is the most even-handed approach. The bulk of the chapter is devoted to a survey of standard data sets and shared tasks, both historical and ongoing. This is a useful reference for researchers in search of venues aimed at their specific goals, or looking for broader patterns in outcomes.Chapter 4 considers the primary axes that may elevate LI from “solved” to “challenging”: language similarity, low-resource languages, orthographic systems and variation, short text, and code-switching. Some of these involve questions of representation: What do we treat as a “language”? What is the “correct” label of a short text that’s valid in multiple languages, such as “quando?”, which is a common question in Portuguese and Italian? How should one label a text containing multiple languages, such as “I’ll ask mi hombre next time I see him”? Chapter 5 then considers the pursuit of a maximally general model capable of characterizing massive collections of heterogeneous content, unknown languages, and domain shift.Chapter 6 discusses several prominent or otherwise compelling uses of LI, from the pragmatic needs of machine translation to subtle tasks like determining the native language based on characteristic patterns in a second language. For instance, corpora of writing from known L2 speakers of English are widespread due to the popularity of English as a second language throughout education, allowing the study of orthographic mistakes grounded in phonetic properties of a native language. Stylistics and authorship attribution share useful features with LI, as they strive to avoid learning topical properties that are often correlated with language.The authors conclude by reiterating the diversity of phenomena that existing LI techniques rarely treat as first-order challenges (until they become immediately relevant), and the difficulty of drawing broader conclusions from the current literature. The book effectively catalogues these challenges and heterogeneity while also providing a stable reference for the community working to organize and extend research in this area. This is useful for several audiences and purposes: Students seeking to understand the history and landscape of LIResearchers hoping to unify or extend existing methodsPractitioners or stakeholders who need to select and justify an approach to a specific taskThe only notable “limitation” of the book is in fact endemic to the topic: The poorly mapped variety of LI research is naturally going to show through any thorough survey. The authors are up-front about this state of affairs and succeed at improving on it.},
  archive      = {J_COLI},
  author       = {Lippincott, Tom},
  doi          = {10.1162/coli_r_00521},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {339--341},
  shortjournal = {Comput. Lingu.},
  title        = {Automatic language identification in texts},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on LLM-generated text detection: Necessity, methods, and future directions. <em>COLI</em>, <em>51</em>(1), 275--338. (<a href='https://doi.org/10.1162/coli_a_00549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable ability of large language models (LLMs) to comprehend, interpret, and generate complex language has rapidly integrated LLM-generated text into various aspects of daily life, where users increasingly accept it. However, the growing reliance on LLMs underscores the urgent need for effective detection mechanisms to identify LLM-generated text. Such mechanisms are critical to mitigating misuse and safeguarding domains like artistic expression and social networks from potential negative consequences. LLM-generated text detection, conceptualized as a binary classification task, seeks to determine whether an LLM produced a given text. Recent advances in this field stem from innovations in watermarking techniques, statistics-based detectors, and neural-based detectors. Human-assisted methods also play a crucial role. In this survey, we consolidate recent research breakthroughs in this field, emphasizing the urgent need to strengthen detector research. Additionally, we review existing datasets, highlighting their limitations and developmental requirements. Furthermore, we examine various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues, and ineffective evaluation frameworks. Finally, we outline intriguing directions for future research in LLM-generated text detection to advance responsible artificial intelligence. This survey aims to provide a clear and comprehensive introduction for newcomers while offering seasoned researchers valuable updates in the field. 1},
  archive      = {J_COLI},
  author       = {Wu, Junchao and Yang, Shu and Zhan, Runzhe and Yuan, Yulin and Chao, Lidia Sam and Wong, Derek Fai},
  doi          = {10.1162/coli_a_00549},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {275--338},
  shortjournal = {Comput. Lingu.},
  title        = {A survey on LLM-generated text detection: Necessity, methods, and future directions},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural semantic parsing with extremely rich symbolic meaning representations. <em>COLI</em>, <em>51</em>(1), 235--274. (<a href='https://doi.org/10.1162/coli_a_00542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current open-domain neural semantics parsers show impressive performance. However, closer inspection of the symbolic meaning representations they produce reveals significant weaknesses: Sometimes they tend to merely copy character sequences from the source text to form symbolic concepts, defaulting to the most frequent word sense based in the training distribution. By leveraging the hierarchical structure of a lexical ontology, we introduce a novel compositional symbolic representation for concepts based on their position in the taxonomical hierarchy. This representation provides richer semantic information and enhances interpretability. We introduce a neural “taxonomical” semantic parser to utilize this new representation system of predicates, and compare it with a standard neural semantic parser trained on the traditional meaning representation format, employing a novel challenge set and evaluation metric for evaluation. Our experimental findings demonstrate that the taxonomical model, trained on much richer and complex meaning representations, is slightly subordinate in performance to the traditional model using the standard metrics for evaluation, but outperforms it when dealing with out-of-vocabulary concepts. We further show through neural model probing that training on a taxonomic representation enhances the model’s ability to learn the taxonomical hierarchy. This finding is encouraging for research in computational semantics that aims to combine data-driven distributional meanings with knowledge-based symbolic representations.},
  archive      = {J_COLI},
  author       = {Zhang, Xiao and Bouma, Gosse and Bos, Johan},
  doi          = {10.1162/coli_a_00542},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {235--274},
  shortjournal = {Comput. Lingu.},
  title        = {Neural semantic parsing with extremely rich symbolic meaning representations},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating synthetic data generation from user generated text. <em>COLI</em>, <em>51</em>(1), 191--233. (<a href='https://doi.org/10.1162/coli_a_00540'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-generated content provides a rich resource to study social and behavioral phenomena. Although its application potential is currently limited by the paucity of expert labels and the privacy risks inherent in personal data, synthetic data can help mitigate this bottleneck. In this work, we introduce an evaluation framework to facilitate research on synthetic language data generation for user-generated text. We define a set of aspects for assessing data quality, namely, style preservation, meaning preservation, and divergence, as a proxy for privacy. We introduce metrics corresponding to each aspect. Moreover, through a set of generation strategies and representative tasks and baselines across domains, we demonstrate the relation between the quality aspects of synthetic user generated content, generation strategies, metrics, and downstream performance. To our knowledge, our work is the first unified evaluation framework for user-generated text in relation to the specified aspects, offering both intrinsic and extrinsic evaluation. We envisage it will facilitate developments towards shareable, high-quality synthetic language data.},
  archive      = {J_COLI},
  author       = {Chim, Jenny and Ive, Julia and Liakata, Maria},
  doi          = {10.1162/coli_a_00540},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {191--233},
  shortjournal = {Comput. Lingu.},
  title        = {Evaluating synthetic data generation from user generated text},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compositionality and sentence meaning: Comparing semantic parsing and transformers on a challenging sentence similarity dataset. <em>COLI</em>, <em>51</em>(1), 139--190. (<a href='https://doi.org/10.1162/coli_a_00536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major outstanding questions in computational semantics is how humans integrate the meaning of individual words into a sentence in a way that enables understanding of complex and novel combinations of words, a phenomenon known as compositionality. Many approaches to modeling the process of compositionality can be classified as either “vector-based” models, in which the meaning of a sentence is represented as a vector of numbers, or “syntax-based” models, in which the meaning of a sentence is represented as a structured tree of labeled components. A major barrier in assessing and comparing these contrasting approaches is the lack of large, relevant datasets for model comparison. This article aims to address this gap by introducing a new dataset, STS3k, which consists of 2,800 pairs of sentences rated for semantic similarity by human participants. The sentence pairs have been selected to systematically vary different combinations of words, providing a rigorous test and enabling a clearer picture of the comparative strengths and weaknesses of vector-based and syntax-based methods. Our results show that when tested on the new STS3k dataset, state-of-the-art transformers poorly capture the pattern of human semantic similarity judgments, while even simple methods for combining syntax- and vector-based components into a novel hybrid model yield substantial improvements. We further show that this improvement is due to the ability of the hybrid model to replicate human sensitivity to specific changes in sentence structure. Our findings provide evidence for the value of integrating multiple methods to better reflect the way in which humans mentally represent compositional meaning.},
  archive      = {J_COLI},
  author       = {Fodor, James and Deyne, Simon De and Suzuki, Shinsuke},
  doi          = {10.1162/coli_a_00536},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {139--190},
  shortjournal = {Comput. Lingu.},
  title        = {Compositionality and sentence meaning: Comparing semantic parsing and transformers on a challenging sentence similarity dataset},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine translation meta evaluation through translation accuracy challenge sets. <em>COLI</em>, <em>51</em>(1), 73--137. (<a href='https://doi.org/10.1162/coli_a_00537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent machine translation (MT) metrics calibrate their effectiveness by correlating with human judgment. However, these results are often obtained by averaging predictions across large test sets without any insights into the strengths and weaknesses of these metrics across different error types. Challenge sets are used to probe specific dimensions of metric behavior but there are very few such datasets and they either focus on a limited number of phenomena or a limited number of language pairs. We introduce ACES , a contrastive challenge set spanning 146 language pairs, aimed at discovering whether metrics can identify 68 translation accuracy errors. These phenomena range from basic alterations at the word/character level to more intricate errors based on discourse and real-world knowledge. We conducted a large-scale study by benchmarking ACES on 47 metrics submitted to the WMT 2022 and WMT 2023 metrics shared tasks. We also measure their sensitivity to a range of linguistic phenomena. We further investigate claims that large language models (LLMs) are effective as MT evaluators, addressing the limitations of previous studies by using a dataset that covers a range of linguistic phenomena and language pairs and includes both low- and medium-resource languages. Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods are unreliable. We expose a number of major flaws with existing methods: Most metrics ignore the source sentence; metrics tend to prefer surface level overlap; and over-reliance on language-agnostic representations leads to confusion when the target language is similar to the source language. To further encourage detailed evaluation beyond singular scores, we expand ACES to include error span annotations, denoted as SPAN-ACES, and we use this dataset to evaluate span-based error metrics, showing that these metrics also need considerable improvement. Based on our observations, we provide a set of recommendations for building better MT metrics, including focusing on error labels instead of scores, ensembling, designing metrics to explicitly focus on the source sentence, focusing on semantic content rather than relying on the lexical overlap, and choosing the right pre-trained model for obtaining representations.},
  archive      = {J_COLI},
  author       = {Moghe, Nikita and Fazla, Arnisa and Amrhein, Chantal and Kocmi, Tom and Steedman, Mark and Birch, Alexandra and Sennrich, Rico and Guillou, Liane},
  doi          = {10.1162/coli_a_00537},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {73--137},
  shortjournal = {Comput. Lingu.},
  title        = {Machine translation meta evaluation through translation accuracy challenge sets},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ERST: A signaled graph theory of discourse relations and organization. <em>COLI</em>, <em>51</em>(1), 23--72. (<a href='https://doi.org/10.1162/coli_a_00538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we present Enhanced Rhetorical Structure Theory (eRST), a new theoretical framework for computational discourse analysis, based on an expansion of Rhetorical Structure Theory (RST). The framework encompasses discourse relation graphs with tree-breaking, non-projective and concurrent relations, as well as implicit and explicit signals which give explainable rationales to our analyses. We survey shortcomings of RST and other existing frameworks, such as Segmented Discourse Representation Theory, the Penn Discourse Treebank, and Discourse Dependencies, and address these using constructs in the proposed theory. We provide annotation, search, and visualization tools for data, and present and evaluate a freely available corpus of English annotated according to our framework, encompassing 12 spoken and written genres with over 200K tokens. Finally, we discuss automatic parsing, evaluation metrics, and applications for data in our framework.},
  archive      = {J_COLI},
  author       = {Zeldes, Amir and Aoyama, Tatsuya and Liu, Yang Janet and Peng, Siyao and Das, Debopam and Gessler, Luke},
  doi          = {10.1162/coli_a_00538},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {23--72},
  shortjournal = {Comput. Lingu.},
  title        = {ERST: A signaled graph theory of discourse relations and organization},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MUCking in, or fifty years in information extraction. <em>COLI</em>, <em>51</em>(1), 7--22. (<a href='https://doi.org/10.1162/coli_a_00547'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I want to thank the ACL for this Lifetime Achievement Award. I am deeply honored to be receiving it. I would also like to thank the students, faculty, and researchers who were members of the Proteus Project during most of my professional lifetime. It was an honor to serve that group.},
  archive      = {J_COLI},
  author       = {Grishman, Ralph},
  doi          = {10.1162/coli_a_00547},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {7--22},
  shortjournal = {Comput. Lingu.},
  title        = {MUCking in, or fifty years in information extraction},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opening a new chapter for computational linguistics. <em>COLI</em>, <em>51</em>(1), 1--5. (<a href='https://doi.org/10.1162/coli_e_00552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By the end of 2024, the journal Computational Linguistics has reached a significant milestone: It has published exactly 50 volumes over the past half-century. As we launch the first issue of Volume 51, this is an opportune moment to reflect on the journal’s legacy, ongoing evolution, and the exciting changes that lie ahead. Together, we embark on a journey to open a new chapter for this storied publication.},
  archive      = {J_COLI},
  author       = {Lu, Wei},
  doi          = {10.1162/coli_e_00552},
  journal      = {Computational Linguistics},
  month        = {3},
  number       = {1},
  pages        = {1--5},
  shortjournal = {Comput. Lingu.},
  title        = {Opening a new chapter for computational linguistics},
  volume       = {51},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
