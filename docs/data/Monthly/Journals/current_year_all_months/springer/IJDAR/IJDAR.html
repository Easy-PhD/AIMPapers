<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDAR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijdar">IJDAR - 31</h2>
<ul>
<li><details>
<summary>
(2025). Neurosymbolic information extraction from transactional documents. <em>IJDAR</em>, <em>28</em>(3), 475-485. (<a href='https://doi.org/10.1007/s10032-025-00530-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $$F_1$$ -scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.},
  archive      = {J_IJDAR},
  author       = {Hemmer, Arthur and Coustaty, Mickaël and Bartolo, Nicola and Ogier, Jean-Marc},
  doi          = {10.1007/s10032-025-00530-0},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {475-485},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Neurosymbolic information extraction from transactional documents},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SlimDoc: Lightweight distillation of document transformer models. <em>IJDAR</em>, <em>28</em>(3), 457-473. (<a href='https://doi.org/10.1007/s10032-025-00542-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying state-of-the-art document understanding models remains resource-intensive and impractical in many real-world scenarios, particularly where labeled data is scarce and computational budgets are constrained. To address these challenges, this work proposes a novel approach towards parameter-efficient document understanding models capable of adapting to specific tasks and document types without the need for labeled data. Specifically, we propose an approach coined SlimDoc to distill multimodal document transformer encoder models into smaller student models, using internal signals at different training stages, followed by external signals. Our approach is inspired by TinyBERT and adapted to the domain of document understanding transformers. We demonstrate SlimDoc to outperform both a single-stage distillation and a direct fine-tuning of the student. Experimental results across six document understanding datasets demonstrate our approach’s effectiveness: Our distilled student models achieve on average $$93.0\%$$ of the teacher’s performance, while the fine-tuned students achieve $$87.0\%$$ of the teacher’s performance. Without requiring any labeled data, we create a compact student which achieves $$96.0\%$$ of the performance of its supervised-distilled counterpart and $$86.2\%$$ of the performance of a supervised-fine-tuned teacher model. We demonstrate our distillation approach to pick up on document geometry and to be effective on the two popular document understanding models LiLT and LayoutLMv3. Our implementation and training data is available at https://github.com/marcel-lamott/SlimDoc .},
  archive      = {J_IJDAR},
  author       = {Lamott, Marcel and Shakir, Muhammad Armaghan and Ulges, Adrian and Weweler, Yves-Noel and Shafait, Faisal},
  doi          = {10.1007/s10032-025-00542-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {457-473},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {SlimDoc: Lightweight distillation of document transformer models},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing music score analysis with monte carlo dropout: A probabilistic approach to staff-region detection. <em>IJDAR</em>, <em>28</em>(3), 441-456. (<a href='https://doi.org/10.1007/s10032-025-00541-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layout Analysis (LA) is a critical process for detecting and isolating different components within a scanned document, allowing for more straightforward and precise processing of each part independently. In Optical Music Recognition (OMR), LA is essential for identifying and extracting music staves, which enables effective music notation recognition and processing. While the literature includes several studies exploring methods for staff retrieval, there remains room for improvement in terms of robustness and accuracy. In this work, we introduce a methodology that integrates Monte Carlo Dropout (MCD) into a neural network model in order to improve reliability in staff retrieval from scanned sheet music. Our approach leverages multiple non-deterministic predictions using standard dropout layers during inference and aggregates them through pixel-level combination policies. We extend the MCD technique, originally designed for classification and regression tasks using averaged predictions, to the LA task and introduce new combination strategies: maximum and voting criteria. Experiments on three diverse music score corpora, including printed and handwritten documents, demonstrated the effectiveness of our approach. The averaging and voting (with 25% and 50% of votes) criteria reduced the relative error by 63.6% compared to the baseline and achieved a 32.1% improvement over state-of-the-art methods. Our methodology notably enhanced detection accuracy without requiring modifications to the neural architecture, especially at the edges of staves, where conventional models tend to show higher error rates.},
  archive      = {J_IJDAR},
  author       = {Oliva-Bulpitt, Samuel B. and Martinez-Esteso, Juan P. and Galan-Cuenca, Alejandro and Castellanos, Francisco J. and Gallego, Antonio Javier},
  doi          = {10.1007/s10032-025-00541-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {441-456},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Enhancing music score analysis with monte carlo dropout: A probabilistic approach to staff-region detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level style control for chinese handwriting generation. <em>IJDAR</em>, <em>28</em>(3), 429-440. (<a href='https://doi.org/10.1007/s10032-025-00533-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of artificial intelligence-generated content (AIGC) has made significant progress in handwriting generation, including complex scripts such as Chinese. Previous offline Chinese handwriting generation methods focus on isolated character image generation, without addressing the task of offline Chinese handwritten text line generation. This paper proposed a novel multi-level style control method for generating Chinese handwritten text lines, which consists of two key steps: style-controlled character image generation and text-line layout generation. For style-controlled character image generation, we implement a pre-training method utilizing multi-modal radical-level contrastive loss to align image features with radical embeddings in encoders. Additionally, a multi-level style representation control is achieved through multi-modal feature aggregation. We also propose a style-consistent text-line layout generation scheme by using prompt engineering with a large language model. Experimental results demonstrate that our method achieves comparable or even better performance in character image generation compared to diffusion model-based methods, while also delivering faster generation speeds. By incorporating text-line layout generation, the generated text-line samples can be effectively used for training handwriting recognition models.},
  archive      = {J_IJDAR},
  author       = {Yao, Gang and Peng, Liangrui and Li, Zhiyu and zhao, Tianqi and Zhao, Kemeng and Ding, Ning and Tao, Yao},
  doi          = {10.1007/s10032-025-00533-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {429-440},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Multi-level style control for chinese handwriting generation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight cross-attention-based HookNet for historical handwritten document layout analysis. <em>IJDAR</em>, <em>28</em>(3), 409-427. (<a href='https://doi.org/10.1007/s10032-025-00519-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten document layout analysis is a fundamental step in digitizing scanned ancient documents for further processing (e.g., optical character recognition). So far, single branch-based fully convolutional networks (FCN) dominate this field. However, we contend that this task faces significant challenges, particularly in layouts with only semantic differences rather than differences in character appearance. For example, in the U-DIADS-Bib dataset, distinguishing between the main text and chapter headings can confuse existing FCNs due to the presence of similar distractors. It is, thus, critical to integrate layout structural information into the network learning processes. Moreover, the single branch-based networks have an upper limit of constructing document contextual relationships. Therefore, we propose a novel two-branch framework, called lightweight cross-attention-based HookNet (Light-HookNet), for handwritten document layout segmentation. The layout contextual information is connected and interacted with the cross-attention mechanism between a global context branch and a local target branch. This allows to achieve information enhancement inside the target branch and information exchange across both branches. Additionally, the reduced network parameters and computational costs make the proposed method both lightweight and efficient. Extensive experimental results and performance comparisons with state-of-the-art approaches on the newly proposed U-DIADS-Bib dataset and the popular DIVA-HisDB dataset demonstrate the superiority and effectiveness of the proposed method.},
  archive      = {J_IJDAR},
  author       = {Wu, Fei and Seuret, Mathias and Mayr, Martin and Kordon, Florian and Zöllner, Jochen and Wind, Sebastian and Maier, Andreas and Christlein, Vincent},
  doi          = {10.1007/s10032-025-00519-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {409-427},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Lightweight cross-attention-based HookNet for historical handwritten document layout analysis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A low-intervention dual-loop iterative process for efficient dataset expansion and classification in palm leaf manuscript analysis. <em>IJDAR</em>, <em>28</em>(3), 391-408. (<a href='https://doi.org/10.1007/s10032-025-00532-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palm leaf manuscripts, celebrated for their historical and cultural significance, pose considerable challenges for digital analysis due to their intricate script structures and age-related degradation. Existing studies frequently highlight the difficulties in assembling realistic datasets and generating accurate ground truth labels, as interpreting ancient scripts requires extensive human expertise and substantial computational resources. To address these challenges, this paper proposes a low-intervention, dual-loop iterative framework designed to efficiently expand datasets and enhance glyph classification in palm leaf manuscript analysis. The framework comprises two primary stages: preprocessing and classification. In the preprocessing stage, state-of-the-art methods are employed for text-line detection, glyph extraction, and synthetic data generation, significantly reducing reliance on manual annotation. The classification stage introduces tailored enhancements to vision transformers (ViTs), incorporating CNN-based feature extraction, Dynamic Stride Shift Patch Tokenization (DS-SPT), and Multi-Scale Locality Self-Attention (MS-LSA). These methods enhance the model’s flexibility and adaptability to the unique characteristics of palm leaf datasets. Moreover, the classification phase facilitates the generation of labels for newly extracted and generated datasets, employing an iterative process to progressively refine model performance. In our experiments, we evaluate the framework using both the ICFHR 2018 palm leaf collection and newly extracted datasets. The experimental results demonstrate improvements in classifying complex glyphs, providing a scalable and efficient solution for low-resource historical document analysis. This framework establishes a foundation for advanced research in the preservation and study of ancient scripts, enabling long-term accessibility and conservation of these cultural heritage documents with minimal human intervention.},
  archive      = {J_IJDAR},
  author       = {Thuon, Nimol and Du, Jun and Theang, Panhapin and Thuon, Ratana},
  doi          = {10.1007/s10032-025-00532-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {391-408},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A low-intervention dual-loop iterative process for efficient dataset expansion and classification in palm leaf manuscript analysis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The PARES database: Information extraction over historical parish records. <em>IJDAR</em>, <em>28</em>(3), 377-389. (<a href='https://doi.org/10.1007/s10032-025-00531-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historical census records convey information that is key to perform genealogical research and demographic studies. Given the large number of documents of this type that exist, it is crucial to research methods that allow the automatic extraction of information from this type of document. In this work, we present a new corpus of this kind, comprising 535 historical census tables from French archives. Alongside this dataset, we have assessed three different baseline methods for information extraction. The first two methods employ a traditional sequential approach, where table rows are detected before extracting information. The third baseline uses an end-to-end model that directly extracts information from the table images without prior row detection. Our results demonstrate the effectiveness of all three baselines in tackling the information extraction task.},
  archive      = {J_IJDAR},
  author       = {Andrés, José and Wall, Casey and Tarride, Solène and Coustaty, Mickaël and Toselli, Alejandro H. and Vidal, Enrique},
  doi          = {10.1007/s10032-025-00531-z},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {377-389},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {The PARES database: Information extraction over historical parish records},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tabular context-aware optical character recognition and tabular data reconstruction for historical records. <em>IJDAR</em>, <em>28</em>(3), 357-376. (<a href='https://doi.org/10.1007/s10032-025-00543-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digitizing historical tabular records is essential for preserving and analyzing valuable data across various fields, but it presents challenges due to complex layouts, mixed text types, and degraded document quality. This paper introduces a comprehensive framework to address these issues through three key contributions. First, it presents UoS_Data_Rescue, a novel dataset of 1,113 historical logbooks with over 594,000 annotated text cells, designed to handle the complexities of handwritten entries, aging artifacts, and intricate layouts. Second, it proposes a novel context-aware text extraction approach (TrOCR-ctx) to reduce cascading errors during table digitization. Third, it proposes an enhanced end-to-end OCR pipeline that integrates TrOCR-ctx with ByT5, combining OCR and post-OCR correction in a unified training framework. This framework enables the system to produce both the raw OCR output and a corrected version in a single pass, improving recognition accuracy, particularly for multilingual and degraded text, within complex table digitization tasks. The model achieves superior performance with a 0.049 word error rate and a 0.035 character error rate, outperforming existing methods by up to 41% in OCR tasks and 10.74% in table reconstruction tasks. This framework offers a robust solution for large-scale digitization of tabular documents, extending its applications beyond climate records to other domains requiring structured document preservation. The dataset and implementation are available as open-source resources.},
  archive      = {J_IJDAR},
  author       = {Singh, Loitongbam Gyanendro and Middleton, Stuart E.},
  doi          = {10.1007/s10032-025-00543-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {357-376},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Tabular context-aware optical character recognition and tabular data reconstruction for historical records},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Character recognition for greek squeezes. <em>IJDAR</em>, <em>28</em>(3), 345-356. (<a href='https://doi.org/10.1007/s10032-025-00540-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Squeezes are three-dimensional paper impressions made of stone inscriptions, a form of historical document where digital processing methods have received relatively little study to date. This paper reports on experiments in text recognition, working with a collection of approximately 30,000 squeezes originally collected from museums and archaeological sites in classical Greece. It explores a number of complementary strategies for character recognition in this medium, with the aim of establishing a strong benchmark in performance. Based on these experiments, we identify a recognition pipeline based on scene text recognition algorithms combined with line-based recognition that achieves a character error rate around 13%.},
  archive      = {J_IJDAR},
  author       = {Howe, Nicholas R. and Chang, Feiran and Falbo, Isabella and Brown, Tajhini and Hershkowitz, Aaron},
  doi          = {10.1007/s10032-025-00540-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {345-356},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Character recognition for greek squeezes},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On self-supervision in historical handwritten document segmentation. <em>IJDAR</em>, <em>28</em>(3), 329-344. (<a href='https://doi.org/10.1007/s10032-025-00538-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historical document analysis plays a crucial role in understanding and preserving our past. However, this task is often hindered by challenges such as limited annotated training data and the diverse nature of historical handwritten documents. In this paper, we explore the potential of self-supervised learning (SSL) in historical document analysis, with a particular focus on historical handwritten document segmentation, to overcome the need for extensive annotated data while enhancing efficiency and robustness. We present an overview of SSL methods suitable for historical document analysis and discuss their potential applications and benefits. Furthermore, we present an approach for SSL in the document domain, considering various setups, augmentations, and resolutions. We also provide experimental results that demonstrate its feasibility and effectiveness. Our findings indicate that most document segmentation tasks can be effectively addressed using SSL features, highlighting the potential of SSL to advance historical document analysis and pave the way for more efficient and robust document processing workflows.},
  archive      = {J_IJDAR},
  author       = {Baloun, Josef and Prantl, Martin and Lenc, Ladislav and Martínek, Jiří and Král, Pavel},
  doi          = {10.1007/s10032-025-00538-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {329-344},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {On self-supervision in historical handwritten document segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Special issue on advanced topics in document analysis (2025 ICDAR-IJDAR journal track). <em>IJDAR</em>, <em>28</em>(3), 327-328. (<a href='https://doi.org/10.1007/s10032-025-00551-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDAR},
  author       = {Lopresti, Daniel and Karatzas, Dimosthenis and Yin, Xu-Cheng},
  doi          = {10.1007/s10032-025-00551-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {9},
  number       = {3},
  pages        = {327-328},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Special issue on advanced topics in document analysis (2025 ICDAR-IJDAR journal track)},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on artificial intelligence-based approaches for personality analysis from handwritten documents. <em>IJDAR</em>, <em>28</em>(2), 287-325. (<a href='https://doi.org/10.1007/s10032-024-00496-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human personality is a blend of different traits and virtues. It’s modeling is challenging due to its inherent complexity. There are multitudinous cues to predict personality and handwriting is one of them. This is because it is distinctive to a large extent and varies at the individual level. The allied field of science which deals with the analysis of handwriting for understanding personality is known as Graphology. Researchers have discovered disparate features of handwriting that can reveal the personality traits of an individual. Several attempts have been made to model personality from handwriting in different languages but significant advancement is required for commercialization. In this paper, we present the reported aspects of handwriting, techniques for processing handwritten documents and evaluation measures for personality identification to draw a horizon and aid in further advancement of research in this field.},
  archive      = {J_IJDAR},
  author       = {Saha Biswas, Suparna and Mukherjee, Himadri and Dhar, Ankita and Md, Obaidullah Sk and Roy, Kaushik},
  doi          = {10.1007/s10032-024-00496-5},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {287-325},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A survey on artificial intelligence-based approaches for personality analysis from handwritten documents},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting and recognizing characters in greek papyri with YOLOv8, DeiT and SimCLR. <em>IJDAR</em>, <em>28</em>(2), 277-285. (<a href='https://doi.org/10.1007/s10032-024-00504-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capacity to isolate and recognize individual characters from facsimile images of papyrus manuscripts yields rich opportunities for digital analysis. For this reason the ‘ICDAR 2023 Competition on Detection and Recognition of Greek Letters on Papyri’ was held as part of the 17th International Conference on Document Analysis and Recognition. This paper discusses our submission to the competition. We used an ensemble of YOLOv8 models to detect and classify individual characters and employed two different approaches for refining the character predictions, including a transformer based DeiT approach and a ResNet-50 model trained on a large corpus of unlabeled data using SimCLR, a self-supervised learning method. Our submission won the recognition challenge with a mAP of 42.2%, and was runner-up in the detection challenge with a mean average precision of 51.4%. At a more relaxed intersection over union threshold of 0.5, we achieved the highest precision and recall for both detection and classification. The results demonstrate the potential of these techniques for automated character recognition on historical manuscripts. We ran the prediction pipeline on more than 4500 images from the Oxyrhynchus Papyri to illustrate the utility of our approach and release the results publicly in multiple formats.},
  archive      = {J_IJDAR},
  author       = {Turnbull, Robert and Mannix, Evelyn},
  doi          = {10.1007/s10032-024-00504-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {277-285},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Detecting and recognizing characters in greek papyri with YOLOv8, DeiT and SimCLR},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised approach to text line extraction in belfort civil registers of births. <em>IJDAR</em>, <em>28</em>(2), 255-276. (<a href='https://doi.org/10.1007/s10032-024-00507-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Historical documents are invaluable resources for understanding the development of civilizations and cultures. However, the transcription process of these documents comprises many challenges such as complex layouts, degradation, various handwritten styles, and skewed text. This paper presents an unsupervised approach for text line extraction in the Belfort Civil Registers of Births, a historical dataset containing a mix of printed and handwritten text with marginal annotations. The proposed method employs a series of image processing techniques to identify text line cores. The method also utilizes a dynamic gap identification and segment point localization strategy based on text density and histogram analysis to effectively identify the borders of the text lines in polygon shape. An XML file generation tool is then utilized to structure the resulting components and link them with their corresponding text. The method exhibits competitive accuracy in segmenting text lines on both the Belfort dataset and standard benchmarks such as the Saint Gall and READ Bozen datasets. This work contributes to the preservation and accessibility of historical documents by facilitating accurate transcription and structured data representation.},
  archive      = {J_IJDAR},
  author       = {AlKendi, Wissam and Gechter, Franck and Heyberger, Laurent and Guyeux, Christophe},
  doi          = {10.1007/s10032-024-00507-5},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {255-276},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Unsupervised approach to text line extraction in belfort civil registers of births},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PrecisionGAN: Enhanced image-to-image translation for preserving structural integrity in skeletonized images. <em>IJDAR</em>, <em>28</em>(2), 239-253. (<a href='https://doi.org/10.1007/s10032-024-00505-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Character skeletonization is a critical aspect of the accurate digital analysis of ancient manuscripts. Our study introduces PrecisionGAN, a generative adversarial network (GAN) specifically tailored for the skeletonization of ancient manuscript characters, marking an advancement in cultural heritage analysis. PrecisionGAN utilizes a U-Net-based generator with innovative multi-head attention and residual connections, combined with a PatchGAN-based discriminator. It is fine-tuned with a hybrid loss function optimized to enhance accuracy and reduce artifacts, even when the training data is imperfect. Our evaluations show that this GAN not only excels in manuscript character skeletonization, achieving superior accuracy and image quality over existing methods but also demonstrates the potential for a range of image processing applications, such as underwater image restoration. Evaluated using the Gaussian similarity measure, our method outperformed the conventional techniques up to 81% of the cases. Further assessments with the structural similarity index measure and a customized metric focused on endpoints and junctions yielded comparable or superior results. This research highlights the versatility of our approach to preserving and interpreting cultural heritage through cutting-edge digital technologies.},
  archive      = {J_IJDAR},
  author       = {Ahmed, Maaz and Kim, Min-Beom and Choi, Kang-Sun},
  doi          = {10.1007/s10032-024-00505-7},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {239-253},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {PrecisionGAN: Enhanced image-to-image translation for preserving structural integrity in skeletonized images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Newspaper elements detection and newspaper pages categorization using CNNs and transformers. <em>IJDAR</em>, <em>28</em>(2), 225-237. (<a href='https://doi.org/10.1007/s10032-024-00503-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Newspaper digitization has gained wide interest around the world. Archives of digitized newspapers and magazines contain a wealth of information that spans decades. To extract this abundance of information, optical character recognition (OCR) techniques with extensive manual page annotation have been employed. The OCR techniques extract the text from the raw image of the page, while page annotation adds meta-data about the content of the page such as the category of the page and the location of articles and other elements. To automate this process, I propose a framework for detecting newspaper pages elements and categorizing newspaper pages. The framework will use visual features of the digitized newspaper to classify the printed media type and decompose the newspaper page into its main elements (news articles, advertisements, and page headers) using object detection. Then, it will use both visual and textual features to categorize the newspaper page into its main sections (first page, politics, economy, sports, and advertisement). The element detection stage is leveraged by using only news articles for categorizing the pages, since other elements, such as advertisements, may contain visual and textual features that are not related to the page section. This framework will prepare the newspaper page for the OCR methods to extract meaningful information. The page elements detection phase of the framework is language-agnostic, which allows it to extract the articles from newspapers in different languages (e.g., Arabic, English, French, German, etc). The framework will use two deep neural networks architectures, Faster R-CNN which is based on the convolutional neural network (CNN) architecture and Transformers to classify and detect elements in the printed media.},
  archive      = {J_IJDAR},
  author       = {Almutairi, Abdullah},
  doi          = {10.1007/s10032-024-00503-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {225-237},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Newspaper elements detection and newspaper pages categorization using CNNs and transformers},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unpaired document image denoising for OCR using BiLSTM enhanced CycleGAN. <em>IJDAR</em>, <em>28</em>(2), 207-224. (<a href='https://doi.org/10.1007/s10032-024-00499-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition performance of optical character recognition (OCR) models can be sub-optimal when document images suffer from various degradations. Supervised deep learning methods for image enhancement can generate high-quality enhanced images. However, these methods demand the availability of corresponding clean images or ground truth text. Sometimes this requirement is difficult to fulfill for real-world noisy documents. For instance, it can be challenging to create paired noisy/clean training datasets or obtain ground truth text for noisy point-of-sale receipts and invoices. Unsupervised methods have been explored in recent years to enhance images in the absence of ground truth images or text. However, these methods focus on enhancing natural scene images. In the case of document images, preserving the readability of text in the enhanced images is of utmost importance for improved OCR performance. In this work, we propose a modified architecture to the CycleGAN model to improve its performance in enhancing document images with better text preservation. Inspired by the success of CNN-BiLSTM combination networks in text recognition models, we propose modifying the discriminator network in the CycleGAN model to a combined CNN-BiLSTM network for better feature extraction from document images during classification by the discriminator network. The results demonstrate that the proposed model significantly enhances text preservation and OCR performance compared to the standard CycleGAN discriminator network. Specifically, when assessing the Tesseract engine’s word accuracy on real-world noisy receipt images from the POS dataset, the proposed model achieved an improvement of up to 61.66% over the original CycleGAN model and 23.32% over the original noisy receipt images. Additionally, the proposed model consistently outperformed other unsupervised classical techniques across all OCR engines evaluated.},
  archive      = {J_IJDAR},
  author       = {Singh, Katyani and Tata, Ganesh and Oeveren, Eric Van and Ray, Nilanjan},
  doi          = {10.1007/s10032-024-00499-2},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {207-224},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Unpaired document image denoising for OCR using BiLSTM enhanced CycleGAN},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient title text detection using multi-loss. <em>IJDAR</em>, <em>28</em>(2), 195-205. (<a href='https://doi.org/10.1007/s10032-024-00500-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {YouTube’s “Video Chapter” feature segments videos into different sections, marked by timestamps on the slider, enhancing user navigation. Given the vast volume of video data, processing these efficiently demands substantial time and computational resources. This paper addresses two key objectives: reducing the computational cost of deep model training for text detection and enhancing overall performance with minimal effort. We introduce a classroom-based multi-loss learning approach for text detection, extending its application to title detection without requiring annotations. In deep learning, loss functions play a crucial role in updating model weights. Our proposed multi-loss functions facilitate faster convergence compared to baseline methods. Additionally, we present a novel technique to handle annotation-less data by employing a text grouping method to differentiate between regular text and title text. Experimental results on the COCO-Text and Slidin’ Videos AI-5G Challenge datasets demonstrate the efficacy and practicality of our approach.},
  archive      = {J_IJDAR},
  author       = {Prasad, Shitala and Abraham, Anuj},
  doi          = {10.1007/s10032-024-00500-y},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {195-205},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Efficient title text detection using multi-loss},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subword recognition in historical arabic manuscripts using handcrafted features and deep learning approaches. <em>IJDAR</em>, <em>28</em>(2), 177-193. (<a href='https://doi.org/10.1007/s10032-024-00501-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen significant endeavors to improve handwriting recognition systems and digitize historical manuscripts. Nevertheless, recognizing historical Arabic manuscripts remains a considerable challenge. The purpose of this study is to investigate subword recognition in historical Arabic manuscripts. Two systems are established. The first system involves using a variety of handcrafted feature methods with diverse machine learning algorithms. The second system uses a deep learning architecture that integrates convolutional neural network and bidirectional long short-term memory based on a character model approach with connectionist temporal classification as a decoder. By utilizing the IBN SINA dataset, the histogram of oriented gradients descriptor demonstrated superior performance in the first system, while the second system achieved notable results. The findings of this study provide a framework for the development of historical manuscript recognition systems.},
  archive      = {J_IJDAR},
  author       = {Dahbali, Mohamed and Aboutabit, Noureddine and Lamghari, Nidal},
  doi          = {10.1007/s10032-024-00501-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {177-193},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Subword recognition in historical arabic manuscripts using handcrafted features and deep learning approaches},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In-domain versus out-of-domain transfer learning for document layout analysis. <em>IJDAR</em>, <em>28</em>(2), 161-175. (<a href='https://doi.org/10.1007/s10032-024-00497-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data availability is a big concern in the field of document analysis, especially when working on tasks that require a high degree of precision when it comes to the definition of the ground truths on which to train deep learning models. A notable example is represented by the task of document layout analysis in handwritten documents, which requires pixel-precise segmentation maps to highlight the different layout components of each document page. These segmentation maps are typically very time-consuming and require a high degree of domain knowledge to be defined, as they are intrinsically characterized by the content of the text. For this reason in the present work, we explore the effects of different initialization strategies for deep learning models employed for this type of task by relying on both in-domain and cross-domain datasets for their pre-training. To test the employed models we use two publicly available datasets with heterogeneous characteristics both regarding their structure as well as the languages of the contained documents. We show how a combination of cross-domain and in-domain transfer learning approaches leads to the best overall performance of the models, as well as speeding up their convergence process.},
  archive      = {J_IJDAR},
  author       = {De Nardin, Axel and Zottin, Silvia and Piciarelli, Claudio and Foresti, Gian Luca and Colombi, Emanuela},
  doi          = {10.1007/s10032-024-00497-4},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {161-175},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {In-domain versus out-of-domain transfer learning for document layout analysis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust page object detection network for heterogeneous document images. <em>IJDAR</em>, <em>28</em>(2), 143-159. (<a href='https://doi.org/10.1007/s10032-024-00498-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document Layout Analysis (DLA) has emerged as a challenging problem in the field of computer vision. The primary goal of DLA involves the identification of page objects including tables, figures, images, and equations from document images. In this paper, we propose a Lightweight and Robust Page Object Detection Network (LR-PODNet) for page object detection (POD) from heterogeneous document images. The proposed network improves the object detection capabilities of the YOLOv5 model by integrating the two components: Convolutional Global Attention Block (C3-AB) and Hybrid Dilated Atrous spatial pyramid pooling Block (HDAB) for POD. The C3-AB is an enhanced version of the C3 module of YOLOv5 which incorporates a global attention block instead of bottleneck-CSP block. It enhances the capability of the model to capture global dimensional features and suppresses the redundant content. The output from C3-AB is passed to the HDAB for extraction of both local and contextual features. The HDAB is strategically incorporated instead of SPPF within the YOLOv5 architecture to enhance multiple feature extraction capabilities. The experimental results show that the proposed LR-PODNet outperforms the existing methods by achieving the mAP@0.5:0.95 of 77.5% and 76.2% on the IIIT-AR-13K and NCERT5K-IITRPR datasets, respectively. Additionally, we have also evaluated the robustness of the proposed model on these two datasets by varying the IoU threshold.},
  archive      = {J_IJDAR},
  author       = {Kawoosa, Hadia Showkat and Kanroo, Muhammad Suhaib and Rana, Kapil and Goyal, Puneet},
  doi          = {10.1007/s10032-024-00498-3},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {6},
  number       = {2},
  pages        = {143-159},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Robust page object detection network for heterogeneous document images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning approaches for information extraction from visually rich documents: Datasets, challenges and methods. <em>IJDAR</em>, <em>28</em>(1), 121-142. (<a href='https://doi.org/10.1007/s10032-024-00493-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on Information Extraction from Visually Rich Documents, exploring how deep learning methods are applied in this field. For the purpose of comparing the performance of available resources, including datasets and methods, we first investigate an overview of the existing datasets. Then, we categorize and review published methods, highlighting their strengths and weaknesses in addressing key challenges like text recognition, layout analysis, and information fusion. This survey serves as a valuable resource for researchers and practitioners seeking to advance the field of information extraction (IE) from visually rich documents (VRD) and contribute to its real-world applications.},
  archive      = {J_IJDAR},
  author       = {Gbada, Hamza and Kalti, Karim and Mahjoub, Mohamed Ali},
  doi          = {10.1007/s10032-024-00493-8},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {121-142},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Deep learning approaches for information extraction from visually rich documents: Datasets, challenges and methods},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based modified-EAST scene text detector: Insights from a novel multiscript dataset. <em>IJDAR</em>, <em>28</em>(1), 97-119. (<a href='https://doi.org/10.1007/s10032-024-00491-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of computer vision has seen significant transformation with the emergence and advancement of deep learning models. Deep learning waves have a significant impact on scene text detection, a vital and active area in computer vision. Numerous scientific, industrial, and academic procedures make use of text analysis. Natural scene text detection is more difficult than document image text detection owing to variations in font, size, style, brightness, etc. The National Institute of Technology Jalandhar-Text Detection dataset (NITJ-TD) is a new dataset that we have put forward in this study for various text analysis tasks including text detection, text segmentation, script identification, text recognition, etc. a deep learning model that seeks to identify the text’s location within the image,which are gathered in an unrestricted setting. The system consists of an NMS to choose the best match and prevent repeated predictions, and a modified EAST to pinpoint the exact ROI in the image. To improve the model’s performance, an enhancement module is added to the fundamental Efficient and Accurate Scene Text detector (EAST). The suggested approach is contrasted in terms of text word detection in the image. Several pre-trained models are used to assign the text word to various intersections over Union (IoU) values. We made use of our NITJ-TD dataset, which is made up of 1500 photos that were gathered from various North Indian sites. Punjabi, English, and Hindi scripts can be seen on the images. We also examined the outcomes of the ICDAR-2013 benchmark dataset. On both the suggested dataset and the benchmarked dataset, our approach performed better.},
  archive      = {J_IJDAR},
  author       = {Mahajan, Shilpa and Rani, Rajneesh and Kamboj, Aman},
  doi          = {10.1007/s10032-024-00491-w},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {97-119},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Deep learning-based modified-EAST scene text detector: Insights from a novel multiscript dataset},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A 3D scanning based image processing technique for measuring the sequence of intersecting lines. <em>IJDAR</em>, <em>28</em>(1), 85-96. (<a href='https://doi.org/10.1007/s10032-024-00495-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of the sequence of two intersecting lines is still an ongoing and important issue in questioned document examination. In literature, scanning electron microscopy, optical profilometry, laser profilometry, and video reflectance spectroscopy were used for analyzing the intersecting lines. In this study, a cheap and easy-to-operate 3D scanner was used to study this problem. Intersections of homogenous and heterogeneous strokes were drawn using different brands of 1.0 mm blue, black, and red ballpoint pens on conventional printing paper, i.e., 80 g/m2, and uncoated A4 white paper by two different handwriting examiners. The analysis of the 3D surface of the crossing lines shows that the bottom of the profile of the first line has a considerable trough, while the second line has comparably smaller fluctuations. The 3D scan analysis is not affected by the brands and colors of the pen and is independent of the substrate and the number of sheets lying underneath the paper. The effect of the pen pressure shows that if the pressure of the second line is less than the first one, the sequence determination becomes harder and sometimes impossible. As these cases can be eliminated before the 3D scan, the developed method is sensitive, cheap, and easy to operate.},
  archive      = {J_IJDAR},
  author       = {Asicioglu, Faruk and Gelir, Ali and Yilmaz, Aysegul Sen and De Kinder, Jan and Kadi, Omer F. and Ozdemir, Onur B. and Pekacar, Ilgim and Sasun, Ugur and Ciftci, Saltuk B. and Dayioglu, Nurten},
  doi          = {10.1007/s10032-024-00495-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {85-96},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {A 3D scanning based image processing technique for measuring the sequence of intersecting lines},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards fully automated processing and analysis of construction diagrams: AI-powered symbol detection. <em>IJDAR</em>, <em>28</em>(1), 71-84. (<a href='https://doi.org/10.1007/s10032-024-00492-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction drawings are frequently stored in undigitised formats and consequently, their analysis requires substantial manual effort. This is true for many crucial tasks, including material takeoff where the purpose is to obtain a list of the equipment and respective amounts required for a project. Engineering drawing digitisation has recently attracted increased attention, however construction drawings have received considerably less interest compared to other types. To address these issues, this paper presents a novel framework for the automatic processing of construction drawings. Extensive experiments were performed using two state-of-the-art deep learning models for object detection in challenging high-resolution drawings sourced from industry. The results show a significant reduction in the time required for drawing analysis. Promising performance was achieved for symbol detection across various classes, with a mean average precision of 79% for the YOLO-based method and 83% for the Faster R-CNN-based method. This framework enables the digital transformation of construction drawings, improving tasks such as material takeoff and many others.},
  archive      = {J_IJDAR},
  author       = {Jamieson, Laura and Moreno-Garcia, Carlos Francisco and Elyan, Eyad},
  doi          = {10.1007/s10032-024-00492-9},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {71-84},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Towards fully automated processing and analysis of construction diagrams: AI-powered symbol detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN-based text line segmentation method for challenging handwritten documents. <em>IJDAR</em>, <em>28</em>(1), 59-69. (<a href='https://doi.org/10.1007/s10032-024-00488-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text line segmentation (TLS) is an essential step of the end-to-end document analysis systems. The main purpose of this step is to extract the individual text lines of any handwritten documents with high accuracy. Handwritten and historical documents mostly contain touching and overlapping characters, heavy diacritics, footnotes and side notes added over the years. In this work, we present a new TLS method based on generative adversarial networks (GAN). TLS problem is tackled as an image-to-image translation problem and the GAN model was trained to learn the spatial information between the individual text lines and their corresponding masks including the text lines. To evaluate the segmentation performance of the proposed GAN model, two challenging datasets, VML-AHTE and VML-MOC, were used. According to the qualitative and quantitative results, the proposed GAN model achieved the best segmentation accuracy on the VML-MOC dataset and showed competitive performance on the VML-AHTE dataset.},
  archive      = {J_IJDAR},
  author       = {Özşeker, İbrahim and Demir, Ali Alper and Özkaya, Ufuk},
  doi          = {10.1007/s10032-024-00488-5},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {59-69},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {GAN-based text line segmentation method for challenging handwritten documents},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image quality determination of palm leaf heritage documents using integrated discrete cosine transform features with vision transformer. <em>IJDAR</em>, <em>28</em>(1), 41-57. (<a href='https://doi.org/10.1007/s10032-024-00490-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of Palm leaf images into various quality categories is an important step towards the digitization of these heritage documents. Manual inspection and categorization is not only laborious, time-consuming and costly but also subject to inspector’s biases and errors. This study aims to automate the classification of palm leaf document images into three different visual quality categories. A comparative analysis between various structural and statistical features and classifiers against deep neural networks is performed. VGG16, VGG19 and ResNet152v2 architectures along with a custom CNN model are used, while Discrete Cosine Transform (DCT), Grey Level Co-occurrence Matrix (GLCM), Tamura, and Histogram of Gradient (HOG) are chosen from the traditional methods. Based on these extracted features, various classifiers, namely, k-Nearest Neighbors (k-NN), multi-layer perceptron (MLP), Support Vector Machines (SVM), Decision Tree (DT) and Logistic Regression (LR) are trained and evaluated. Accuracy, precision, recall, and F1 scores are used as performance metrics for the evaluation of various algorithms. Results demonstrate that CNN embeddings and DCT features have emerged as superior features. Based on these findings, we integrated DCT with a Vision Transformer (ViT) for the document classification task. The result illustrates that this incorporation of DCT with ViT outperforms all other methods with 96% train F1 score and a test F1 score of 90%.},
  archive      = {J_IJDAR},
  author       = {Sivan, Remya and Pati, Peeta Basa and Kesiman, Made Windu Antara},
  doi          = {10.1007/s10032-024-00490-x},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {41-57},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Image quality determination of palm leaf heritage documents using integrated discrete cosine transform features with vision transformer},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scene text recognition: An indic perspective. <em>IJDAR</em>, <em>28</em>(1), 31-40. (<a href='https://doi.org/10.1007/s10032-024-00489-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring Scene Text Recognition (STR) in Indian languages is an important research domain due to its wide applications. This paper proposes a spatial attention-based model (LaSA-Net) that combines visual features and language knowledge for word recognition from scene image word segments. We augment the classical cross-entropy loss with a novel language-attunement loss that enables the model to learn valid and prevalent character sequences in the word. This enhances the model’s ability to perform zero-shot word recognition. Further, to compensate for the lack of rotational invariance in CNN based feature extraction backbone, we propose a training data augmentation strategy involving the creation of glyphs: images of individual characters of different orientations. This improves LaSA-Net’s ability to recognize words in images with curved/vertically aligned text, alleviating the need for computationally expensive preprocessing modules. Our experiments with Tamil, Malayalam, and Telugu scripts on the IIIT-ILST datasets have achieved new benchmark results and outperformed other state-of-the-art STR models.},
  archive      = {J_IJDAR},
  author       = {Vijayan, Vasanthan P. and Chanda, Sukalpa and Doermann, David and Krishnan, Narayanan C.},
  doi          = {10.1007/s10032-024-00489-4},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {31-40},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Scene text recognition: An indic perspective},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic floor plan analysis using a boundary attention-based deep network. <em>IJDAR</em>, <em>28</em>(1), 19-30. (<a href='https://doi.org/10.1007/s10032-024-00487-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Floor plan is an important communication tool between architects, construction engineers, and clients for a building project. Estimation of building features from a floor plan image is often a time-consuming task. Automatic analysis of floor plan images can significantly improve work efficiency and accuracy. A few research works have been reported in the literature on automated floor image analysis. However, the scope and performance of the existing techniques are limited. In this paper, a CNN-based technique, referred to as FloorNet, is proposed for the multiclass semantic segmentation of a floor plan. The proposed FloorNet has five modules: Encoder, Room type decoder, Room boundary decoder, Multiscale room boundary attention model and Floor classification. The proposed technique is evaluated using simple brochure type and complex architectural type floor plan images. Experimental results show that the proposed technique provides an improvement of 5–11% mIoU for semantic segmentation (for 9–11 classes) compared to the state-of-the-art techniques.},
  archive      = {J_IJDAR},
  author       = {Xu, Zhongguo and Yang, Cheng and Alheejawi, Salah and Jha, Naresh and Mehadi, Syed and Mandal, Mrinal},
  doi          = {10.1007/s10032-024-00487-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {19-30},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Automatic floor plan analysis using a boundary attention-based deep network},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handwritten stenography recognition and the LION dataset. <em>IJDAR</em>, <em>28</em>(1), 3-18. (<a href='https://doi.org/10.1007/s10032-024-00479-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we establish the first baseline for handwritten stenography recognition, using the novel LION dataset, and investigate the impact of including selected aspects of stenographic theory into the recognition process. We make the LION dataset publicly available with the aim of encouraging future research in handwritten stenography recognition. A state-of-the-art text recognition model is trained to establish a baseline. Stenographic domain knowledge is integrated by transforming the target sequences into representations which approximate diplomatic transcriptions, wherein each symbol in the script is represented by its own character in the transliteration, as opposed to corresponding combinations of characters from the Swedish alphabet. Four such encoding schemes are evaluated and results are further improved by integrating a pre-training scheme, based on synthetic data. The baseline model achieves an average test character error rate (CER) of 29.81% and a word error rate (WER) of 55.14%. Test error rates are reduced significantly (p< 0.01) by combining stenography-specific target sequence encodings with pre-training and fine-tuning, yielding CERs in the range of 24.5–26% and WERs of 44.8–48.2%. An analysis of selected recognition errors illustrates the challenges that the stenographic writing system poses to text recognition. This work establishes the first baseline for handwritten stenography recognition. Our proposed combination of integrating stenography-specific knowledge, in conjunction with pre-training and fine-tuning on synthetic data, yields considerable improvements. Together with our precursor study on the subject, this is the first work to apply modern handwritten text recognition to stenography. The dataset and our code are publicly available via Zenodo.},
  archive      = {J_IJDAR},
  author       = {Heil, Raphaela and Nauwerck, Malin},
  doi          = {10.1007/s10032-024-00479-6},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {3-18},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {Handwritten stenography recognition and the LION dataset},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). International journal on document analysis and recognition editorial leadership change. <em>IJDAR</em>, <em>28</em>(1), 1-2. (<a href='https://doi.org/10.1007/s10032-025-00520-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDAR},
  author       = {Lopresti, Daniel and Kise, Koichi and Marinai, Simone},
  doi          = {10.1007/s10032-025-00520-2},
  journal      = {International Journal on Document Analysis and Recognition},
  month        = {3},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Doc. Anal. Recognit.},
  title        = {International journal on document analysis and recognition editorial leadership change},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
