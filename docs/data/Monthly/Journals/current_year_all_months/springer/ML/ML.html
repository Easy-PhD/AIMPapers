<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ML</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ml">ML - 262</h2>
<ul>
<li><details>
<summary>
(2025). Learning de-biased environment models for delivery incentive policy optimization on food delivery platforms. <em>ML</em>, <em>114</em>(12), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06846-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate environmental modeling is essential for model-based offline policy optimization. Traditional empirical risk minimization can yield biased models if data collection is subject to selection bias, potentially misguiding policy optimization. This issue is especially pertinent in real-world decision-making scenarios where data collection often depends on optimized, non-random behavior policies. This paper addresses such a practical challenge of offline policy optimization under selection bias, with a focus on delivery incentive policies for food delivery platforms. We propose a novel framework for offline optimization of these policies, based on a de-biased environmental model. Initially, the framework learns a de-biased order acceptance rate and delivery time prediction model from historical data through adversarial weighted empirical risk minimization, constituting the environment model. Subsequently, it employs operation research solvers to derive historic best actions based on the learned de-biased environment model, determining the optimal bonus amount and reasonable incentive time limit for each order under budget constraints. Finally, a policy neural network is trained to map environmental states to these optimized actions, enabling efficient and executable policies for real-time decision-making. To verify the effectiveness and efficiency of our framework, both offline experiments on a real-world dataset and online A/B tests on the Meituan food delivery platform are conducted. Results demonstrate that our framework outperforms baseline methods in both model accuracy and policy optimization performance in offline experiments and realizes a 9% reduction in the customer complaint rate in reality.},
  archive      = {J_ML},
  author       = {Liu, Yu-Ren and Chen, Xiong-Hui and Xiao, Siyuan and Yang, Xinyu and Qi, Xintong and Zhou, Linjun and Yu, Yang and Huang, Fangsheng},
  doi          = {10.1007/s10994-025-06846-6},
  journal      = {Machine Learning},
  month        = {12},
  number       = {12},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Learning de-biased environment models for delivery incentive policy optimization on food delivery platforms},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causality from bottom to top: A survey. <em>ML</em>, <em>114</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10994-025-06855-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality has become a fundamental approach for explaining the relationships between events, phenomena, and outcomes in various fields of study. It has invaded various fields and applications, such as medicine, healthcare, economics, finance, fraud detection, cybersecurity, education, public policy, recommender systems, anomaly detection, robotics, control, sociology, marketing, and advertising. In this paper, we survey its development over the past five decades, shedding light on the differences between causality and other approaches, as well as the preconditions for using it. Furthermore, the paper illustrates how causality interacts with new approaches such as Artificial Intelligence (AI), Generative AI (GAI), Machine and Deep Learning, Reinforcement Learning (RL), and Fuzzy Logic. We study the impact of causality on various fields, its contribution, and its interaction with state-of-the-art approaches. Additionally, the paper exemplifies the trustworthiness and explainability of causality models. We offer several ways to evaluate causality models and discuss future directions.},
  archive      = {J_ML},
  author       = {Weinberg, Abraham Itzhak and Premebida, Cristiano and Faria, Diego Resende},
  doi          = {10.1007/s10994-025-06855-5},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Causality from bottom to top: A survey},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Special issue on machine learning in soccer. <em>ML</em>, <em>114</em>(11), 1-8. (<a href='https://doi.org/10.1007/s10994-025-06783-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Berrar, Daniel and Davis, Jesse and Lopes, Philippe and Dubitzky, Werner},
  doi          = {10.1007/s10994-025-06783-4},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-8},
  shortjournal = {Mach. Learn.},
  title        = {Guest editorial: Special issue on machine learning in soccer},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with confidence: Training better classifiers from soft labels. <em>ML</em>, <em>114</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10994-025-06860-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised machine learning, models are typically trained using data with hard labels, i.e., definite assignments of class membership. This traditional approach, however, does not take the inherent uncertainty in these labels into account. We investigate whether incorporating label uncertainty, represented for each instance as a discrete probability distribution over the class labels, known as a soft label, improves the predictive performance of classification models, focusing on tabular data. We first demonstrate the potential value of soft label learning (SLL) for estimating model parameters in a simulation experiment, particularly for limited sample sizes and imbalanced data. Subsequently, we compare the performance of various wrapper methods for learning from both hard and soft labels using identical base classifiers. On real-world-inspired synthetic data with clean labels, the SLL methods consistently outperform the hard label methods. Since real-world data is often noisy and precise soft labels are challenging to obtain, we study the effect that noisy probability estimates have on model performance. Alongside conventional noise models, our study examines four types of miscalibration that are known to affect human annotators. The results show that SLL methods outperform the hard label methods in the majority of settings. Finally, we evaluate the methods on a real-world dataset with confidence scores, where the SLL methods are shown to match the traditional methods for predicting the (noisy) hard labels while providing more accurate confidence estimates.},
  archive      = {J_ML},
  author       = {de Vries, Sjoerd and Thierens, Dirk},
  doi          = {10.1007/s10994-025-06860-8},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Learning with confidence: Training better classifiers from soft labels},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-based feature generation from text for interpretable machine learning. <em>ML</em>, <em>114</em>(11), 1-30. (<a href='https://doi.org/10.1007/s10994-025-06867-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional text representations like embeddings and bag-of-words hinder rule learning and other interpretable machine learning methods due to high dimensionality and poor comprehensibility. This article investigates using Large Language Models (LLMs) to extract a small number of interpretable text features. We propose two workflows: one fully automated by the LLM (feature proposal and value calculation), and another where users define features and the LLM calculates values. This LLM-based feature extraction enables interpretable rule learning, overcoming issues like spurious interpretability seen with bag-of-words. We evaluated the proposed methods on five diverse datasets (including scientometrics, banking, hate speech, and food hazard). LLM-generated features yielded predictive performance similar to the SciBERT embedding model but used far fewer, interpretable features. Most generated features were considered relevant for the corresponding prediction tasks by human users. We illustrate practical utility on a case study focused on mining recommendation action rules for the improvement of research article quality and citation impact.},
  archive      = {J_ML},
  author       = {Balek, Vojtěch and Sýkora, Lukáš and Sklenák, Vilém and Kliegr, Tomáš},
  doi          = {10.1007/s10994-025-06867-1},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {LLM-based feature generation from text for interpretable machine learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISA3: A 3-dimensional expansion of instance space analysis. <em>ML</em>, <em>114</em>(11), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06871-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The experimental validation of algorithms depends strongly on the characteristics of the test set used. Ideally, such a set should exhibit diverse characteristics that challenge an algorithm and are present in real-world problems. An approach to examine the diversity and representativeness of a test set is Instance Space Analysis, which uses a 2D projection to visualise the test set while identifying the strengths and weaknesses of competing algorithms. However, this has the limitation of discarding potentially useful information while crowding out the space as more features and algorithms are considered. This paper describes an extension of Instance Space Analysis into 3D, which retains a higher degree of information while maintaining the explainability of the visualisation by finding the rotations that maximise the linear trends. In addition to the expansion to 3D, a new algorithm for identifying portfolio footprints is introduced, offering a more robust and reliable method for identifying footprints in both 2D and 3D instance spaces. As a case study, we present a performance analysis of unsupervised anomaly detection methods, subject to changes in the normalisation technique. The results demonstrate the advantages by identifying regions of strength of algorithms previously thought to be underpowered.},
  archive      = {J_ML},
  author       = {Simpson, Connor and Muñoz, Mario Andrés and Kandanaarachchi, Sevvandi and Campello, Ricardo J. G. B.},
  doi          = {10.1007/s10994-025-06871-5},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {ISA3: A 3-dimensional expansion of instance space analysis},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASS-CSP: Mining with answer set solving for contrast sequential pattern mining. <em>ML</em>, <em>114</em>(11), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06876-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present MASS-CSP (Mining with Answer Set Solving - Contrast Sequential Patterns), a declarative approach to the Contrast Sequential Pattern Mining (CSPM) task, which is based on the logic-based framework of Answer Set Programming (ASP). The CSPM task focuses on identifying significant differences in frequent sequences relative to specific classes, leading to the concept of a contrast sequential pattern. The article describes how MASS-CSP addresses the CSPM task and related extensions-mining closed, maximal and constrained patterns. Evaluation aims at comparing the basic version of MASS-CSP against the extended versions as regards the size of output and time-memory requirements.},
  archive      = {J_ML},
  author       = {Sterlicchio, Gioacchino and Lisi, Francesca Alessandra},
  doi          = {10.1007/s10994-025-06876-0},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {MASS-CSP: Mining with answer set solving for contrast sequential pattern mining},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evo-path: A two-stage temporal knowledge graph reasoning model and its application in human behavior prediction. <em>ML</em>, <em>114</em>(11), 1-28. (<a href='https://doi.org/10.1007/s10994-025-06886-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Knowledge Graphs (TKGs) contain a vast number of facts with timestamps in the real world and have more abundant semantic information compared with Knowledge Graphs (KGs). However, TKGs are usually incomplete. Reasoning potential facts in the future is a challenging task and has emerged as a hotspot in the research of TKGs. One key of this task is to dive deep into the evolutional patterns contained in historical facts, which are helpful to predict future facts. However, most of the existing models focus on modeling evolutional patterns based on the entire graph, which ignores the important role of the query-related paths. Furthermore, the existing graph neural networks (GNNs) cannot efficiently extract the graph structural information that is vital to model the evolutional pattern. In order to address these two problems, we propose a two-stage reasoning model Evo-Path in this paper. At the temporal path searching stage, Evo-Path learns a temporal-semantic policy network and employs beam search policy to obtain the clue paths and candidate answers based on Deep Reinforcement Learning (DRL). At the evolutional reasoning stage, we propose a Multi-Relational Graph Attention Network (MRGAT) to encode the structural information of clue subgraphs so as to model the evolutional patterns over clue paths and deduce final answers. We evaluate our model on four public TKG datasets: ICEWS14, ICEWS18-7000, ICEWS05-15-7000, and GDELT. Extensive experimental results show that Evo-Path not only outperforms other state-of-the-art baselines in MRR and Hits@1 but also has interpretability for reasoning results. Three application case studies highlight the significant value of our model for predicting human behavior.},
  archive      = {J_ML},
  author       = {He, Mingsheng and Zhu, Lin and Bai, Luyi},
  doi          = {10.1007/s10994-025-06886-y},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Evo-path: A two-stage temporal knowledge graph reasoning model and its application in human behavior prediction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DP-FedSecure: A secure and efficient federated learning scheme based on adaptive differential privacy. <em>ML</em>, <em>114</em>(11), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06888-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning, as a paradigm of distributed machine learning, allows participants to collaboratively model without sharing data, effectively addressing the data island problem. However, relying solely on model transmission still poses privacy leakage risks. Hao et al. proposed an efficient and lightweight federated learning scheme, HLX+19, based on differential privacy, which demonstrates significant advantages in privacy protection and performance. However, we found that HLX+19 has inherent security vulnerabilities and limited scalability. Therefore, this paper first designs a reconstruction attack experiment aimed at approximating the recovery of original data from the noise-added data of HLX+19 in a collusion scenario. Subsequently, we propose DP-FedSecure, a federated learning privacy protection scheme based on adaptive noise. By adopting adaptive noise, we enhance the randomness of the noise distribution, thereby improving the security of the scheme. We conducted a security analysis of DP-FedSecure and experimentally validated its effectiveness in resisting reconstruction attacks. Finally, we performed comparative experiments on encryption efficiency and accuracy. The results indicate that, DP-FedSecure achieves approximately 97.43% improvement in encryption efficiency compared to HLX+19, and the impact of security parameters on encryption efficiency has been reduced from exponential to linear. Therefore, DP-FedSecure demonstrates high efficiency and good scalability in terms of encryption. Experiments on real-world datasets further validate the high accuracy of DP-FedSecure, while we balanced the relationship between security and accuracy through adaptive noise.},
  archive      = {J_ML},
  author       = {Chen, Shuo and Zhou, Tanping and Xie, Huiyu and Du, Weidong and Yang, Xiaoyuan},
  doi          = {10.1007/s10994-025-06888-w},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {DP-FedSecure: A secure and efficient federated learning scheme based on adaptive differential privacy},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing RL components for wagner’s framework via brouwer’s conjecture. <em>ML</em>, <em>114</em>(11), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06890-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we continue the study and systematization started in our previous work (Angileri et al. in: Lecture notes in computer science 15243 LNAI, 2025, pp 325–338. https://doi.org/10.1007/978-3-031-78977-9_21 ) of Wagner’s Reinforcement Learning framework to investigate graph conjectures. After identifying three main directions that impact the framework’s performance (the environment dynamics, the RL algorithm and the neural network used as a function approximator), we conduct an ablation study to evaluate the effectiveness of each component, analyzing several variations of them. The experiments compare three environment dynamics, implemented as Gym spaces (Linear, Local and Global), two algorithms [PPO and the Cross-Entropy method (Wagner in Constructions in combinatorics via neural networks, 2021, https://arxiv.org/abs/2104.14516 )], different neural network structures (Multi-Layer Perceptron and Graph Neural Networks) and reward systems. This study was intended not only to test the framework’s capabilities, but also to identify a configuration of environment, algorithm, and neural network that can be effective when exploring graph spaces, even with a complex target. For this reason, all the experiments were executed on Brouwer’s Conjecture. We also present the data collected with the various trained models, as these interesting configurations can be used in the inference process on the problem. Our analysis shows that a proper calibration of the individual components of the framework can significantly improve its performance, suggesting effective settings for addressing complex problems and contributing to the study of Brouwer’s Conjecture. All the codes and data are open source and available at https://github.com/CuriosAI/graph_conjectures .},
  archive      = {J_ML},
  author       = {Angileri, Flora and Lombardi, Giulia and Fois, Andrea and Faraone, Renato and Metta, Carlo and Salvi, Michele and Bianchi, Luigi Amedeo and Fantozzi, Marco and Galfrè, Silvia Giulia and Pavesi, Daniele and Parton, Maurizio and Morandin, Francesco},
  doi          = {10.1007/s10994-025-06890-2},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Analyzing RL components for wagner’s framework via brouwer’s conjecture},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining exceptional social behavior on attributed interaction networks. <em>ML</em>, <em>114</em>(11), 1-34. (<a href='https://doi.org/10.1007/s10994-025-06831-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social interactions are prevalent in our lives. These can be observed, e. g., online using social media, however, also offline specifically using sensors. In such contexts, typically time-stamped interactions are recorded, which can also be inferred from real-time location of humans. Such interaction data can then be modeled as so-called social interaction networks. For their analysis, a variety of different approaches can be applied. A prominent research direction is then the detection of patterns describing specific subgroups with exceptional behavioral characteristics, given some measure of interest. In the standard case of plain graphs modeling the interaction networks, methods for identifying such subgroups mainly focus on structural characteristics of the network and/or the induced subgraph. For attributed social networks, then additional attributive information can be exploited. This paper proposes to focus on the dyadic structure of the attributed social interaction networks, thus enabling a compositional perspective for identifying interesting subgroup patterns. Specifically, we can then analyze spatio-temporal data modeled as attributed social interaction networks for identifying exceptional social behavior. The presented approach adapts local pattern mining using subgroup discovery to the dyadic setting, exploiting attribute information of the spatio-temporal attributed interaction networks. With this, specific characteristics of social interactions are considered, i. e., duration and frequency, for identifying subgroups capturing social behavior that deviates from the norm. For subgroup discovery, we propose according interestingness measures in the form of seven novel quality functions and discuss their properties. In our experimentation, we perform an evaluation demonstrating the efficacy of the presented approach using four real-world datasets on face-to-face interactions in academic conferencing as well as school playground contexts. Our results indicate that the proposed method returns interesting, meaningful, and valid findings and results.},
  archive      = {J_ML},
  author       = {Atzmueller, Martin and Centeio Jorge, Carolina and Rebelo de Sá, Cláudio and Heravi, Behzad M. and Gibson, Jenny L. and Rossetti, Rosaldo J. F.},
  doi          = {10.1007/s10994-025-06831-z},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-34},
  shortjournal = {Mach. Learn.},
  title        = {Mining exceptional social behavior on attributed interaction networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive collaborative minority oversampling for multi-class imbalanced classification. <em>ML</em>, <em>114</em>(11), 1-27. (<a href='https://doi.org/10.1007/s10994-025-06899-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-class classification tasks often encounter imbalanced data in real-world applications, misclassifying minority classes leads to severe losses. Although linear interpolation is an effective oversampling method for addressing class imbalance problem, it tends to generate noisy and overlapping examples. To tackle these challenges, we propose an adaptive collaborative minority oversampling (ACo-MO) method specifically designed for multi-class imbalanced classification. Different from existing oversampling methods that rely on k-nearest neighbors to select and generate examples, our method first leverages boosting to identify difficult-to-classify examples (e.g., those near decision boundaries and in small disjuncts). This strategy ensures that the minority class boundary is effectively expanded. Subsequently, we introduce the global distribution of the minority class and collaborate with the boosting iteration to synthesize examples, which adaptively adjusts the interpolation range to minimize noise generation. Furthermore, synthetic examples enable interpolation in the vicinity of the selected example, not just along a linear path, thereby reducing the probability of overlap among the synthesized examples. We validated the effectiveness of ACo-MO through extensive experiments on 23 datasets. The results demonstrate its superiority over ten state-of-the-art multi-class imbalanced classification methods in three performance measures.},
  archive      = {J_ML},
  author       = {Zheng, Su-Yang and Chen, Chou-Yong and Zhao, Xiao-Xi and Zhang, Zhong-Liang},
  doi          = {10.1007/s10994-025-06899-7},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Adaptive collaborative minority oversampling for multi-class imbalanced classification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benchmarking adversarial robustness to bias elicitation in large language models: Scalable automated assessment with LLM-as-a-judge. <em>ML</em>, <em>114</em>(11), 1-41. (<a href='https://doi.org/10.1007/s10994-025-06862-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Such biases may stem from historical inequalities in training data, linguistic imbalances, or adversarial manipulation. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. This work proposes a scalable benchmarking framework to assess LLM robustness to adversarial bias elicitation. Our methodology involves: (i) systematically probing models across multiple tasks targeting diverse sociocultural biases, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach, and (iii) employing jailbreak techniques to reveal safety vulnerabilities. To facilitate systematic benchmarking, we release a curated dataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying DeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is uneven, with age, disability, and intersectional biases among the most prominent. Some small models outperform larger ones in safety, suggesting that training and architecture may matter more than scale. However, no model is fully robust to adversarial elicitation, with jailbreak attacks using low-resource languages or refusal suppression proving effective across model families. We also find that successive LLM generations exhibit slight safety gains, while models fine-tuned for the medical domain tend to be less safe than their general-purpose counterparts.},
  archive      = {J_ML},
  author       = {Cantini, Riccardo and Orsino, Alessio and Ruggiero, Massimo and Talia, Domenico},
  doi          = {10.1007/s10994-025-06862-6},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-41},
  shortjournal = {Mach. Learn.},
  title        = {Benchmarking adversarial robustness to bias elicitation in large language models: Scalable automated assessment with LLM-as-a-judge},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating human knowledge for explainable AI. <em>ML</em>, <em>114</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10994-025-06879-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a methodology for integrating human expert knowledge into machine learning (ML) workflows to improve both model interpretability and the quality of explanations produced by explainable AI (XAI) techniques. We strive to enhance standard ML and XAI pipelines without modifying underlying algorithms, focusing instead on embedding domain knowledge at two stages: (1) during model development through expert-guided data structuring and feature engineering, and (2) during explanation generation via domain-aware synthetic neighbourhoods. Visual analytics is used to support experts in transforming raw data into semantically richer representations. We validate the methodology in two case studies: predicting COVID-19 incidence and classifying vessel movement patterns. The studies demonstrated improved alignment of models with expert reasoning and better quality of synthetic neighbourhoods. We also explore using large language models (LLMs) to assist experts in developing domain-compliant data generators. Our findings highlight both the benefits and limitations of existing XAI methods and point to a research direction for addressing these gaps.},
  archive      = {J_ML},
  author       = {Cappuccio, Eleonora and Kathirgamanathan, Bahavathy and Rinzivillo, Salvatore and Andrienko, Gennady and Andrienko, Natalia},
  doi          = {10.1007/s10994-025-06879-x},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Mach. Learn.},
  title        = {Integrating human knowledge for explainable AI},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mast: Interpretable stress testing via meta-learning for forecasting model robustness evaluation. <em>ML</em>, <em>114</em>(11), 1-28. (<a href='https://doi.org/10.1007/s10994-025-06881-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating and documenting the robustness of forecasting models to different input conditions is important for their responsible deployment in real-world applications. Time series forecasting models often exhibit degraded performance in the form of unusually large errors, high uncertainty, or hubris (high errors coupled with low uncertainty). Traditional stress testing approaches rely on manually designed adverse scenarios that fail to systematically identify unknown stress factors, in which data characteristics indicate potential issues. To overcome this limitation, this paper introduces MAST (Meta-learning and data Augmentation for Stress Testing), a novel method for stress testing forecasting models. MAST leverages model outputs (error scores and prediction intervals) to automatically identify and characterize input conditions that induce stress. Specifically, MAST is a binary probabilistic classifier that predicts the likelihood of forecasting model stress based on time series features. An additional contribution is a novel time series data augmentation approach based on oversampling or synthetic time series generation, that improves the information about stress factors in the input space, resulting in increased stress classification performance. Experiments were conducted using 6 benchmark datasets containing a total of 97.829 time series. We demonstrate how MAST is able to identify and explain input conditions that lead to manifestations of stress, namely large errors, high uncertainty, or hubris.},
  archive      = {J_ML},
  author       = {Inácio, Ricardo and Cerqueira, Vitor and Barandas, Marília and Soares, Carlos},
  doi          = {10.1007/s10994-025-06881-3},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Mast: Interpretable stress testing via meta-learning for forecasting model robustness evaluation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ST-PPO: A spatio-temporal attention enhanced proximal policy optimization algorithm for autonomous driving in complex traffic scenarios. <em>ML</em>, <em>114</em>(11), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06887-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving in complex traffic environments poses significant challenges due to the dynamic nature of multi-agent interactions and varying road conditions. This paper proposes ST-PPO, a novel reinforcement learning framework that integrates spatio-temporal attention mechanisms with Proximal Policy Optimization for autonomous vehicle control. The framework addresses three critical challenges: spatial feature extraction from complex traffic scenes, temporal dependency modeling of vehicle behaviors, and adaptive policy learning in dynamic environments. The spatial attention module captures crucial spatial relationships between traffic participants, while the temporal attention module models the sequential dependencies of driving behaviors. We evaluate our approach on challenging scenarios including merging areas, continuous curves, intersections, and adverse weather conditions. Comprehensive experiments demonstrate that ST-PPO significantly outperforms baseline methods, achieving 25.2% improvement in training efficiency and 7.6% increase in overall performance compared to vanilla PPO. The method demonstrates remarkable stability with 18.7% lower KL divergence and higher value estimation accuracy (0.8 explained variance). Ablation studies further validate the effectiveness of both spatial and temporal attention components. Our method shows particular strength in handling complex scenarios such as dense traffic and adverse weather conditions, where it maintains stable performance while baseline methods deteriorate significantly. The proposed approach represents a significant step toward robust autonomous driving systems capable of handling real-world traffic complexity.},
  archive      = {J_ML},
  author       = {Da, Cheng and Qian, Yongsheng and Zeng, Junwei and Wei, Xunting and Zhang, Futao},
  doi          = {10.1007/s10994-025-06887-x},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {ST-PPO: A spatio-temporal attention enhanced proximal policy optimization algorithm for autonomous driving in complex traffic scenarios},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal distillation: Compressing a policy in space and time. <em>ML</em>, <em>114</em>(11), 1-27. (<a href='https://doi.org/10.1007/s10994-025-06889-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying deep reinforcement learning on resource-constrained devices remains a significant challenge due to the energy-intensive nature of the sequential decision-making process. Model compression can reduce the spatial (e.g. storage, memory) requirements of a policy network, but this does not always translate to a proportional increase in inference speed and computational efficiency. We introduce a novel temporal compression paradigm that improves the efficiency more directly, by reducing the number of predictions needed to complete a task. This method, based on policy distillation, allows a student model to learn when a change of action will be required by observing sequences of identical actions in the trajectories of an existing teacher model. At each decision, the student can then predict both an action and how many times to perform this action consecutively. This approach allows any existing policy for discrete action spaces to be optimized for energy efficiency through both spatial and temporal compression simultaneously. Experiments on devices ranging from a microcontroller and smartphone processor to a data centre GPU show how this method can decrease the average time it takes to predict an action by up to 13.5 times, compared to 4 times through spatial compression alone, while maintaining a similar average return as the original teacher. In practice, this allows complex models to be deployed on ultra-low-power devices, enabling them to conserve energy by remaining in sleep mode for longer periods, and still achieve high runtime and task performance.},
  archive      = {J_ML},
  author       = {Avé, Thomas and Hutsebaut-Buysse, Matthias and Mets, Kevin},
  doi          = {10.1007/s10994-025-06889-9},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Temporal distillation: Compressing a policy in space and time},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The forget-set identification problem. <em>ML</em>, <em>114</em>(11), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06897-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Unlearning (MU) is the problem of removing the influence of user’s unwanted evidence from a trained machine-learning model. MU is typically formulated so that the input unwanted evidence corresponds to a subset of the training set utilized to train the model upstream, which is commonly referred to as the “forget set”. However, this requirement is often difficult to satisfy in real-world scenarios, as users may be unaware of the peculiarities of the training set or simply they do not have access to it. In a more realistic setting, users provide their unwanted evidence in a form that is more abstract than or anyway different from a precise subset of training data. In such cases, executing MU methods requires an essential and challenging preliminary step, which, to the best of our knowledge, has never been addressed so far: identifying the forget set based on user’s unwanted evidence. In this paper, we fill this important gap in the MU literature and introduce the Forget-Set Identification (ForSId) problem: given a trained machine-learning model, an “unwanted set” of samples (evidence to unlearn), and a “wanted set” of samples (evidence to retain), identify the forget set as a subset of the training set, such that the similarity in the predictions of the original model and the model retrained on the training data remaining after the removal of the forget set is: (i) low on the unwanted set, indicating that the unwanted samples have been effectively unlearned by the model, and (ii) high on the wanted set, to ensure that the model keeps its original performance on the data to be retained. We define ForSId as an optimization problem, prove its NP-hardness, and devise an algorithm based on a theoretical connection to Red-Blue Set Cover. Our ForSId is a novel complementary problem to MU. It serves as a foundational step to be performed before executing MU methods, allowing for extending the range of applicability of MU to all those settings where user’s unlearning evidence does not correspond to (or is too hard to be directly expressed in terms of) a forget set. We conduct extensive experiments based on the exact unlearning task (which is the most reliable one) on several real-world datasets and settings, involving nontrivial baselines. Results demonstrate high performance of our proposed algorithm and clear superiority over the baselines.},
  archive      = {J_ML},
  author       = {D’Angelo, Andrea and Gullo, Francesco and Stilo, Giovanni},
  doi          = {10.1007/s10994-025-06897-9},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {The forget-set identification problem},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised learning from tabular data with autoencoders: When does it work?. <em>ML</em>, <em>114</em>(11), 1-46. (<a href='https://doi.org/10.1007/s10994-025-06898-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labeled data scarcity remains a significant challenge in machine learning. Semi-supervised learning (SSL) offers a promising solution to this problem by simultaneously leveraging both labeled and unlabeled examples during training. While SSL with neural networks has been successful on image classification tasks, its application to tabular data remains limited. In this work, we propose SSLAE, a lightweight yet effective autoencoder-based SSL architecture that integrates reconstruction and classification losses into a single composite objective. We conduct an extensive evaluation of the proposed approach across 90 tabular benchmark datasets, comparing SSLAE’s performance to its supervised baseline and several other neural approaches for both supervised and semi-supervised learning, on varying amounts of labeled data. Our results show that SSLAE consistently outperforms its competitors, particularly in low-label regimes. To better understand when unlabeled data can improve performance, we perform a meta-analysis linking dataset characteristics to SSLAE’s relative gains over its supervised baseline. This analysis reveals key properties—such as class imbalance, feature variability, and alignment between features and labels—that influence the success of SSL, contributing to a deeper understanding of when the inclusion of unlabeled data is beneficial in neural tabular learning.},
  archive      = {J_ML},
  author       = {Stevanoska, Sintija and Levatić, Jurica and Džeroski, Sašo},
  doi          = {10.1007/s10994-025-06898-8},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-46},
  shortjournal = {Mach. Learn.},
  title        = {Semi-supervised learning from tabular data with autoencoders: When does it work?},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance and interaction assessment of neural network architectures and bivariate smart predict-then-optimize. <em>ML</em>, <em>114</em>(11), 1-20. (<a href='https://doi.org/10.1007/s10994-025-06910-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart “predict, then optimize” (SPO) (Elmachtoub in Manag Sci 68(1): 9–26, 2022) is an end-to-end learning strategy for models that predict parameters in optimization problems. Unlike minimizing mean squared error (MSE) which cares about prediction accuracies, SPO aims to ensure that predictions lead to the best possible decisions. The associated loss function, termed SPO loss, measures the decision’s regret from optimal outcomes with parameter realizations. Existing literature has demonstrated the viability of SPO, however, these studies often focus on classical optimization problems and employ a limited set of models for benchmarking. In this study, we tackled a decision-making task inspired by real-world challenges across a wide range of neural network models. Unlike classical problems, our task requires a unique approach: collaboratively training two models to predict different variables. On top of that, one of the decision variables also affects the feasibility of the decisions, further increasing the complexity. While our implementation validates the benefits of SPO, we were surprised to find that models trained exclusively on SPO loss do not consistently attain the minimum regret. Our further investigation into hyperparameters illustrates that the well-tuned models learned very similar patterns from the feature set, irrespective of whether MSE or SPO loss was used. In other words, the change from MSE to SPO loss in training primarily affected the layer biases. Therefore, to improve the learning efficacy with SPO loss, we propose prioritizing learning feature patterns as the fundamental step. Possible strategies include using specialized neural network layers to capture deeper patterns more effectively or simply warming up by training with MSE. Specifically, a warming-up process is particularly advantageous for model(s) where the outputs are closely tied to constraints, as their prediction accuracy significantly impacts the decision feasibility. The insights are investigated empirically through two real-world trading scenarios. By leveraging datasets with diverse properties, we demonstrate the novelty and generalizability of our investigation.},
  archive      = {J_ML},
  author       = {Wen, Junhan and Abeel, Thomas and de Weerdt, Mathijs},
  doi          = {10.1007/s10994-025-06910-1},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Performance and interaction assessment of neural network architectures and bivariate smart predict-then-optimize},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A utility-driven approach to instance-based transfer learning for relational domains. <em>ML</em>, <em>114</em>(11), 1-38. (<a href='https://doi.org/10.1007/s10994-025-06864-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical relational learning involves exploring a complex search space of objects, their relationships, and probability parameters to find an optimal model. To reduce search complexity, previous work has explored taking advantage of a learned model in a source domain and transfer it to a target domain. However, these models are not always available and imperfect learning in the source domain can hinder the performance in the target domain. This paper proposes to leverage the instances of a source domain instead of its learned model. A simple solution, such as concatenating instances from both domains, is likely ineffective due to the potential negative impact of irrelevant or poor-quality instances. We address this by framing instance selection as a task of fair resource allocation, where utilities are parameterized to capture the relevance of each instance. We introduce a method called UTIL-BRDN, which applies this utility-driven approach to Boosted Relational Dependency Networks (RDN-Boost). Our experimental results show that UTIL-BRDN effectively transfers knowledge by reusing instances from other domains and is robust against negative transfer. Our contributions include introducing instance-based transfer learning to statistical relational learning, developing a utility-driven approach to instance selection, extending RDN-Boost to handle multiple domains and utilities, and conducting an extensive empirical evaluation of the proposed method.},
  archive      = {J_ML},
  author       = {Pereira, Cainã F. and Menasché, Daniel S. and Zaverucha, Gerson and Paes, Aline and Barbosa, Valmir C.},
  doi          = {10.1007/s10994-025-06864-4},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {A utility-driven approach to instance-based transfer learning for relational domains},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boolean matrix logic programming for active learning of gene functions in genome-scale metabolic network models. <em>ML</em>, <em>114</em>(11), 1-35. (<a href='https://doi.org/10.1007/s10994-025-06868-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning about hypotheses and updating knowledge through empirical observations are central to scientific discovery. In this work, we applied logic-based machine learning methods to drive biological discovery by guiding experimentation. Genome-scale metabolic network models (GEMs) - comprehensive representations of metabolic genes and reactions - are widely used to evaluate genetic engineering of biological systems. However, GEMs often fail to accurately predict the behaviour of genetically engineered cells, primarily due to incomplete annotations of gene interactions. The task of learning the intricate genetic interactions within GEMs presents computational and empirical challenges. To efficiently predict using GEM, we describe a novel approach called Boolean Matrix Logic Programming (BMLP) by leveraging Boolean matrices to evaluate large logic programs. We developed a new system, $$BMLP_{active}$$ , which guides cost-effective experimentation and uses interpretable logic programs to encode a state-of-the-art GEM of a model bacterial organism. Notably, $$BMLP_{active}$$ successfully learned the interaction between a gene pair with fewer training examples than random experimentation, overcoming the increase in experimental design space. $$BMLP_{active}$$ enables rapid optimisation of metabolic models to reliably engineer biological systems for producing useful compounds. It offers a realistic approach to creating a self-driving lab for biological discovery, which would then facilitate microbial engineering for practical applications.},
  archive      = {J_ML},
  author       = {Ai, Lun and Muggleton, Stephen H. and Liang, Shi-Shun and Baldwin, Geoff S.},
  doi          = {10.1007/s10994-025-06868-0},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-35},
  shortjournal = {Mach. Learn.},
  title        = {Boolean matrix logic programming for active learning of gene functions in genome-scale metabolic network models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing workforce attendance evaluation in vocational schools: A decision support framework powered by explainable machine learning. <em>ML</em>, <em>114</em>(11), 1-20. (<a href='https://doi.org/10.1007/s10994-025-06882-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the underexplored potential of integrating explainable machine learning techniques into Decision Support Systems (DSS) for workforce attendance evaluation, specifically in vocational high schools. While prior research often emphasizes predictive modeling, this paper proposes a non-predictive, descriptive framework that enhances interpretability and actionability. Utilizing a dataset of 52,000 biometric attendance records collected from five vocational schools over six months, the study applies K-Means clustering, anomaly detection (Isolation Forest, LOF), and SHAP-based feature importance to uncover patterns, irregularities, and key drivers of attendance behavior. The findings reveal three distinct behavioral clusters, significant anomaly segments, and key influencing variables, such as workload, weekday cycles, and seasonal peaks. These insights are operationalized into an interactive DSS framework that supports real-time segmentation, alerts, and decision-making for school administrators. Unlike conventional DSS approaches, the proposed system is equipped with explainable outputs and visual dashboards, increasing transparency and decision relevance. The study’s contributions include: (1) an explainable ML-enhanced DSS architecture tailored for workforce evaluation, (2) a shift from predictive modeling to descriptive, interpretable analytics, and (3) empirical evidence linking attendance trends to workforce performance metrics. This framework offers scalable applicability for educational institutions aiming to adopt data-driven, interpretable decision support tools in human resource management.},
  archive      = {J_ML},
  author       = {Santoso, Joseph Teguh and Hendry, Hendry and Manongga, Daniel},
  doi          = {10.1007/s10994-025-06882-2},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Enhancing workforce attendance evaluation in vocational schools: A decision support framework powered by explainable machine learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pdarts: Projected differentiable architecture search for seismic inversion. <em>ML</em>, <em>114</em>(11), 1-31. (<a href='https://doi.org/10.1007/s10994-025-06883-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seismic inversion is an inverse problem that minimizes both amplitude and phase differences between simulated signals and real observed signals through an optimization problem. Due to its high computational cost and the industry demand for higher resolution, researchers often explore ways to accelerate the process and improve its accuracy. In the last decade, deep learning emerged as a promising alternative for seismic inversion; however, it still requires laborious trial and error processes, integration of domain knowledge, and hyperparameter tuning. To improve the model architectures for this task, this article introduces PDARTS (Projected Differentiable Architecture Search), a method inspired by DARTS and originally designed for seismic inversion. PDARTS enables the use of Fourier and U-Fourier neural blocks, which are expected to generalize well in this context, as they naturally encode long-range spatial correlations and operate in the frequency domain, better capturing the physics properties of seismic data. Since the 2D Fast Fourier Transforms used in this article require square inputs for efficient computation, the asymmetric shape of the original seismic data requires the enforcement of a square input format to ensure proper operation. To achieve this, PDARTS employs fixed encoder layers connected to a projection layer to reshape spatial dimensions and a convolutional layer to mitigate potential artifacts introduced by this reshaping. The decoder, along with the final layers of the encoder, is implemented as a DARTS supernetwork, where neural architecture search (NAS) is conducted to explore the potential of Fourier and U Fourier neural blocks, with the aim of discovering new neural networks with better performance. Experiments demonstrated that the best architecture discovered by our method, PDARTSNet, outperforms current state-of-the-art neural networks for seismic inversion. Furthermore, it demonstrates that despite the increased number of network parameters and consequent higher computational costs, PDARTS has proven capable of discovering neural networks with superior performance for seismic inversion.},
  archive      = {J_ML},
  author       = {Souza, Lucas C. and Junior, Carlos G. C. and Cerri, Ricardo and Gomi, Edson S. and Carmo, Bruno S. and Senger, Hermes and Naldi, Murilo},
  doi          = {10.1007/s10994-025-06883-1},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-31},
  shortjournal = {Mach. Learn.},
  title        = {Pdarts: Projected differentiable architecture search for seismic inversion},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging XAI and spectral analysis to investigate the inductive biases of deep graph networks. <em>ML</em>, <em>114</em>(11), 1-21. (<a href='https://doi.org/10.1007/s10994-025-06884-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the inductive bias of Deep Graph Networks (DGNs) is crucial because it reveals how these models generalize from training data to unseen data. Discovering these learning assumptions, and their alignment to the task’s characteristics, allows informed architectural design choices and facilitates interpretation. With this goal, we analyze the different inductive biases of DGNs by relating the node-level explanations produced by explainable AI (XAI) methods to known network science measures of lower-order (local) and higher-order (increasingly global) connectivity. We then apply graph signal processing to refine this analysis at the granularity of the graph frequency spectrum spanned by the explanation signals. Our main finding is that different DGNs focus on different regions of the graph frequency spectrum, and in particular, high-frequency DGNs generalize by focusing on lower-order graph connectivity, while low-frequency DGNs generalize by recognizing higher-order graph structures. This characterization is first derived on synthetic benchmarks by showing that explanations align with network science measures sitting at the two extremes of the spectrum (Katz centrality in the high frequencies, and Fiedler eigenvector scores in the low frequencies). Moving to real-world chemical benchmarks, this result is generalized by showing that inductive biases do indeed lie on a continuum that corresponds to sub-regions of the frequency spectrum.},
  archive      = {J_ML},
  author       = {Fontanesi, Michele and Micheli, Alessio and Podda, Marco and Tortorella, Domenico},
  doi          = {10.1007/s10994-025-06884-0},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {Bridging XAI and spectral analysis to investigate the inductive biases of deep graph networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing task conflicts in LLMs multi-task fine-tuning with task-specific subnetwork refinement. <em>ML</em>, <em>114</em>(11), 1-30. (<a href='https://doi.org/10.1007/s10994-025-06885-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instruction tuning is a widely-adopted technique for adapting Large Language Models (LLMs) to specific domains such as coding and mathematics. However, when instruction tuning is performed on multiple tasks simultaneously, LLMs often experience performance degradation due to conflicts between different tasks. In this work, we explore how task conflicts arise by analyzing the subnetworks within the model for accomplishing different tasks. Our analysis reveals that these subnetworks operate largely independently with minimal overlap. The findings indicate that task conflicts occur because traditional training methods update all model parameters simultaneously for all tasks, causing gradients from one task to disrupt the specialized subnetworks of others. To address this issue, we propose Task-Specific Subnetwork Refinement (TSSR), a novel training framework that mitigates task conflicts by selectively refining the subnetworks associated with each task. Our approach first locates these task-specific subnetworks and then applies gradients exclusively to the relevant subnetwork during training, thereby reducing interference between tasks. Experimental results show that TSSR significantly improves model performance across multiple tasks, including coding, translation, and reasoning tasks. Furthermore, training efficiency is enhanced by reducing the computational burden of gradient updates.},
  archive      = {J_ML},
  author       = {Wang, Yiqun and Wan, Chaoqun and Tian, Xiang and Liu, Xuesong and Chen, Yaowu},
  doi          = {10.1007/s10994-025-06885-z},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {Addressing task conflicts in LLMs multi-task fine-tuning with task-specific subnetwork refinement},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive differentiable trees for transparent learning on data streams. <em>ML</em>, <em>114</em>(11), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06906-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining learning models in dynamic environments requires transparency for trust and compliance, particularly under regulatory frameworks like the Artificial Intelligence (AI) Act by the European Union. Data stream models must balance adaptability with interpretability, and to keep AI models effective in evolving contexts, maintaining transparency is essential. To address this, we introduce Soft Hoeffding Trees (SoHoT) as transparent, differentiable decision trees for data streams. SoHoTs use a novel routing function, leveraging the Hoeffding inequality for tree expansion, while gradient descent updates tree weights to adapt to drifting data distributions. Transparency is further enhanced with decision-rule-based feature importance and a sparse activation function, enabling selective subtree consideration for final predictions. We also provide a visualization of the model’s decision-making process for user interpretability. Evaluated on 20 data streams, SoHoT outperforms Hoeffding trees and competes with Hoeffding adaptive trees and soft trees under AUROC. We also demonstrate how to balance transparency and performance, by looking at the trade-off and measuring prediction performance per complexity, which showcases SoHoT’s benefits compared to existing data stream algorithms.},
  archive      = {J_ML},
  author       = {Köbschall, Kirsten and Hartung, Lisa and Kramer, Stefan},
  doi          = {10.1007/s10994-025-06906-x},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Adaptive differentiable trees for transparent learning on data streams},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Squared wasserstein-2 loss functions for efficient learning of stochastic differential equations. <em>ML</em>, <em>114</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10994-025-06908-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide an analysis of the squared Wasserstein-2 ( $$W_2$$ ) distance between two probability distributions associated with two stochastic differential equations (SDEs). Based on this analysis, we propose using squared $$W_2$$ distance-based loss functions to train parametrized neural networks in order to reconstruct SDEs from noisy data. Specifically, we propose minimizing a time-decoupled squared $$W_2$$ distance loss function. To demonstrate the practicality of our Wasserstein distance-based loss functions, we performed numerical experiments that demonstrate the efficiency of our method in learning SDEs that arise across a number of applications.},
  archive      = {J_ML},
  author       = {Xia, Mingtao and Li, Xiangting and Shen, Qijing and Chou, Tom},
  doi          = {10.1007/s10994-025-06908-9},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Squared wasserstein-2 loss functions for efficient learning of stochastic differential equations},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLAIRE: Clustering evaluation based on item response theory and model agreement. <em>ML</em>, <em>114</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10994-025-06911-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering evaluation is a complex task. External measures, such as the Rand index, are often used for benchmarking, but they are not applicable in real, unsupervised scenarios due to the lack of ground truth. Thus, we often turn to internal measures for model evaluation, e.g. silhouette, Dunn, and Davies-Bouldin. These indexes have the advantage of evaluating models based on the clustered data points themselves, however, they rely on a chosen distance, so they are only meaningful for models that use the same distance. Additionally, they fail if all instances are assigned to a single cluster and they aim to evaluate separation and cohesion instead of quantifying a model’s ability to recover any underlying classes. Thus, internal measures are not suited to compare models that are estimated differently. In this paper, we propose CLAIRE (CLuster Agreement-based Item REsponses), a method for global evaluation of clustering models, by assuming that good models agree on whether pairs of instances should be clustered together or not. We leverage Item Response Theory to estimate model ability and instance difficulty, using response matrices obtained by measuring the agreement between models. Experiments were carried out using diverse sets of clustering methods and datasets with different numbers of clusters and varying shapes and levels of overlapping and noise. Results show that CLAIRE is robust to the presence of random partitions in the pool of models and correctly ranks models across the many tested scenarios with a surprisingly high correlation with external measures of clustering quality, meaning it also indirectly evaluates the recovery of underlying classes.},
  archive      = {J_ML},
  author       = {Ferreira-Junior, Manuel and Lima Neto, Eufrasio A. and Ferreira, Marcelo R. P. and Silva Filho, Telmo M. and Prudêncio, Ricardo B. C.},
  doi          = {10.1007/s10994-025-06911-0},
  journal      = {Machine Learning},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Mach. Learn.},
  title        = {CLAIRE: Clustering evaluation based on item response theory and model agreement},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TFAS: Zero-shot NAS for general time-series analysis with time-frequency aware scoring. <em>ML</em>, <em>114</em>(10), 1-33. (<a href='https://doi.org/10.1007/s10994-025-06832-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing effective neural networks from scratch for various time-series analysis tasks, such as activity recognition, fault detection, and traffic forecasting, is time-consuming and heavily relies on human labor. To reduce the reliance on human labor, recent studies adopt neural architecture search (NAS) to design neural networks for time series automatically. Still, existing NAS frameworks for time series only focus on one specific analysis, such as forecasting and classification, with expensive search methods. This paper therefore aims to build a unified zero-shot NAS framework that effectively searches neural architectures for a variety of tasks and time-series data. However, to build a general framework for different tasks, we need a zero-shot proxy that consistently correlates with the downstream performance across different characteristics of time-series datasets. To address these challenges, we propose a zero-shot NAS framework with novel Time-Frequency Aware Scoring for general time-series analysis, named TFAS. To incorporate TFAS into existing foundation time-series models, we adopt time-frequency decomposition methods and introduce the concept of augmented architecture to the foundation models. This augmented architecture enables the zero-shot proxy to be aware of the decomposed time and frequency information, resulting a more accurate estimation of downstream performance for particular datasets. Empirically, we show that the architectures found by TFAS gain improvement of up to 23.6% over state-of-the-art hand-crafted baselines in five mainstream time-series data mining tasks, including short- and long-term forecasting, classification, anomaly detection, and imputation.},
  archive      = {J_ML},
  author       = {Trirat, Patara and Lee, Jae-Gil},
  doi          = {10.1007/s10994-025-06832-y},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {TFAS: Zero-shot NAS for general time-series analysis with time-frequency aware scoring},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pasco (PArallel structured COarsening): An overlay to speed up graph clustering algorithms. <em>ML</em>, <em>114</em>(10), 1-36. (<a href='https://doi.org/10.1007/s10994-025-06837-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering the nodes of a graph is a cornerstone of graph analysis and has been extensively studied. However, some popular methods are not suitable for very large graphs: e.g., spectral clustering requires the computation of the spectral decomposition of the Laplacian matrix, which is not applicable for large graphs with a large number of communities. This work introduces PASCO, an overlay that accelerates clustering algorithms. Our method consists of three steps: (1) We compute several independent small graphs representing the input graph by applying an efficient and structure-preserving coarsening algorithm. (2) A clustering algorithm is run in parallel onto each small graph and provides several partitions of the initial graph. (3) These partitions are aligned and combined with an optimal transport method to output the final partition. The PASCO framework is based on two key contributions: a novel global algorithm structure designed to enable parallelization and a fast, empirically validated graph coarsening algorithm that preserves structural properties. We demonstrate the strong performance of PASCO in terms of computational efficiency, structural preservation, and output partition quality, evaluated on both synthetic and real-world graph datasets.},
  archive      = {J_ML},
  author       = {Etienne, Lasalle and Rémi, Vaudaine and Titouan, Vayer and Pierre, Borgnat and Paulo, Gonçalves and Rémi, Gribonval and Márton, Karsai},
  doi          = {10.1007/s10994-025-06837-7},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-36},
  shortjournal = {Mach. Learn.},
  title        = {Pasco (PArallel structured COarsening): An overlay to speed up graph clustering algorithms},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mind the gap: From plausible to valid self-explanations in large language models. <em>ML</em>, <em>114</em>(10), 1-38. (<a href='https://doi.org/10.1007/s10994-025-06838-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the reliability of explanations generated by large language models (LLMs) when prompted to explain their previous output. We evaluate two kinds of such self-explanations (SE)—extractive and counterfactual—using state-of-the-art LLMs (1B to 70B parameters) on three different classification tasks (both objective and subjective). In line with Agarwal et al. (Faithfulness versus plausibility: On the (Un)reliability of explanations from large language models. 2024. https://doi.org/10.48550/arXiv.2402.04614 ), our findings indicate a gap between perceived and actual model reasoning: while SE largely correlate with human judgment (i.e. are plausible), they do not fully and accurately follow the model’s decision process (i.e. are not faithful). Additionally, we show that counterfactual SE are not even necessarily valid in the sense of actually changing the LLM’s prediction. Our results suggest that extractive SE provide the LLM’s “guess” at an explanation based on training data. Conversely, counterfactual SE can help understand the LLM’s reasoning: We show that the issue of validity can be resolved by sampling counterfactual candidates at high temperature—followed by a validity check—and introducing a formula to estimate the number of tries needed to generate valid explanations. This simple method produces plausible and valid explanations that offer a 16 times faster alternative to SHAP on average in our experiments.},
  archive      = {J_ML},
  author       = {Randl, Korbinian and Pavlopoulos, John and Henriksson, Aron and Lindgren, Tony},
  doi          = {10.1007/s10994-025-06838-6},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Mind the gap: From plausible to valid self-explanations in large language models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning green’s function efficiently using low-rank approximations. <em>ML</em>, <em>114</em>(10), 1-17. (<a href='https://doi.org/10.1007/s10994-025-06845-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the Green’s function using deep learning models enables efficient parametrization of partial differential equations. However, a practical limitation of this approach is the repeated computation of Monte Carlo integral approximations, which makes the learning process computationally expensive. To address this, we propose DecGreenNet, a novel algorithm that uses low-rank decomposition to learn the Green’s function efficiently. This novel architecture predicts the solution of PDE at a grid element using the product of two networks; one taking each grid element as input and the other taking the Monte Carlo samples as input. Experimental results show that the proposed method achieves faster training times compared to MOD-Net while maintaining comparable or lower prediction error relative to both PINNs and MOD-Net. We also provide a theoretical analysis for Green’s function based PINNs, including both DecGreenNet and MOD-Net, using a clipped Green’s function. Our analysis shows that both MOD-Net and DecGreenNet obtains similar convergence rates.},
  archive      = {J_ML},
  author       = {Wimalawarne, Kishan and Suzuki, Taiji and Langer, Sophie},
  doi          = {10.1007/s10994-025-06845-7},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-17},
  shortjournal = {Mach. Learn.},
  title        = {Learning green’s function efficiently using low-rank approximations},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Epidemic-guided deep learning for spatiotemporal forecasting of tuberculosis outbreak. <em>ML</em>, <em>114</em>(10), 1-59. (<a href='https://doi.org/10.1007/s10994-025-06848-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuberculosis (TB) remains a formidable global health challenge, driven by complex spatiotemporal transmission dynamics and influenced by factors such as population mobility and behavioral changes. We propose an Epidemic-Guided Deep Learning (EGDL) approach that fuses mechanistic epidemiological principles with advanced deep learning techniques to enhance early warning systems and intervention strategies for TB outbreaks. Our framework is built upon a modified networked Susceptible-Infectious-Recovered (MN-SIR) model augmented with a saturated incidence rate and graph Laplacian diffusion, capturing both long-term transmission dynamics and region-specific population mobility patterns. Compartmental model parameters are rigorously estimated using Bayesian inference via the Markov Chain Monte Carlo approach. Theoretical analysis leveraging the comparison principle and Green’s formula establishes global stability properties of the disease-free and endemic equilibria. Building on these epidemiological insights, we design two forecasting architectures, EGDL-Parallel and EGDL-Series, that integrate the mechanistic outputs of the MN-SIR model within deep neural networks. This integration mitigates the overfitting risks commonly encountered in data-driven methods and filters out noise inherent in surveillance data, resulting in reliable forecasts of real-world epidemic trends. Experiments conducted on TB incidence data from 47 prefectures in Japan and 31 provinces in mainland China demonstrate that our approach delivers robust and accurate predictions across multiple time horizons (short to medium-term forecasts), supporting its generalizability across regions with different population dynamics. Additionally, incorporating uncertainty quantification through conformal prediction and explainability via temporal gradient-based class activation maps enhances the model’s practical utility for guiding targeted public health interventions.},
  archive      = {J_ML},
  author       = {Barman, Madhab and Panja, Madhurima and Mishra, Nachiketa and Chakraborty, Tanujit},
  doi          = {10.1007/s10994-025-06848-4},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-59},
  shortjournal = {Mach. Learn.},
  title        = {Epidemic-guided deep learning for spatiotemporal forecasting of tuberculosis outbreak},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RIADNet: Single image deraining network for raindrops and rain streaks removal. <em>ML</em>, <em>114</em>(10), 1-21. (<a href='https://doi.org/10.1007/s10994-025-06854-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both raindrops and rain streaks are common degradations in images captured on rainy days, which not only reduce the quality and visibility of images but also significantly affect downstream tasks such as object detection. However, most existing derain algorithms focus on one specific degradation, failing to provide a comprehensive analysis across other scenarios. In this paper, we propose RIADNet, a Rain Information Attention Deraining Network, which jointly removes raindrops and rain streaks while preserving critical image details. Initially, we devise a simple and efficient rain information attention module(RIAM) to extract raindrop and rain streak information from images accurately, guiding the network to focus on rainy regions and enhancing deraining performance. Furthermore, a multi-scale dilated convolution feature fusion module(MDFFM) integrates encoder features from multiple receptive fields through parallel dilated convolutions with varying dilation rates, which significantly improves multi-scale feature representation. Moreover, a deformable wavelet sampling module(DWSM) replaces traditional sampling with deformable wavelet-based kernels, adaptively preserving high-frequency details during feature extraction and minimizing information loss. Qualitative and quantitative experimental results on three public datasets demonstrate the superior performance of RIADNet in addressing diverse rain degradations. Notably, on the RDS dataset (mixed raindrops and rain streaks), RIADNet achieves a PSNR of 29.78 dB and SSIM of 0.921, outperforming all compared state-of-the-art deraining models while reducing parameters by 54.52% versus the second-best method.},
  archive      = {J_ML},
  author       = {Yu, Changle and Fan, Ping and Zhang, Yi and Yang, Jiyu},
  doi          = {10.1007/s10994-025-06854-6},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {RIADNet: Single image deraining network for raindrops and rain streaks removal},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal CSSE: Integrating counterfactuals and causality in the explanation of machine learning models. <em>ML</em>, <em>114</em>(10), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06856-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning methods are widely used in various industries and research sectors, including health, agriculture, and law, to support decision-making. However, many of these models are inherently uninterpretable, necessitating the development of explainability methods. Among those methods, counterfactual methods aim to explain the decisions of black-box models by making minimal modifications to an instance’s attributes, thereby identifying the necessary changes to alter its classification. This article examines the relationship between counterfactuals and causality, emphasizing the significance of understanding causality for achieving more consistent explainability. We present the Causal CSSE method, which incorporates aspects of causality into the counterfactual CSSE (Social Explanations for Classification Models) method, providing more realistic and applicable explanations. Causal CSSE uses an ad-hoc genetic model to generate counterfactual examples, considering the causality between attributes to improve the interpretability and applicability of the generated explanations. This enhancement aims to provide more realistic and valuable counterfactual answers, thereby contributing to more effective solutions for real-world problems. This project is available at https://github.com/virion1996/causal_csse .},
  archive      = {J_ML},
  author       = {Krauss, Omar F. P. and Balbino, Marcelo S. and Nobre, Cristiane N.},
  doi          = {10.1007/s10994-025-06856-4},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Causal CSSE: Integrating counterfactuals and causality in the explanation of machine learning models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cluster weighted models for functional data. <em>ML</em>, <em>114</em>(10), 1-42. (<a href='https://doi.org/10.1007/s10994-025-06858-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method, funWeightClust, based on a family of parsimonious models for clustering heterogeneous functional linear regression data. These models extend cluster weighted models to functional data, and they allow for multivariate functional responses and predictors. The proposed methodology follows the approach used by the the functional high dimensional data clustering (funHDDC) method. We construct an expectation maximization (EM) algorithm for parameter estimation. Using simulated and benchmark data we show that funWeightClust outperforms funHDDC and several two-steps clustering methods. We also use funWeightClust to analyze traffic patterns in Edmonton, Canada.},
  archive      = {J_ML},
  author       = {Anton, Cristina and Smith, Iain},
  doi          = {10.1007/s10994-025-06858-2},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-42},
  shortjournal = {Mach. Learn.},
  title        = {Cluster weighted models for functional data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EchoRAG: A framework for enhancing language models with graph-RAG and in-context learning. <em>ML</em>, <em>114</em>(10), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06859-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs), and more recently Large Reasoning Models (LRMs), have demonstrated remarkable performance in various Natural Language Processing (NLP) tasks, such as classification and information extraction. However, their high computational demands, need for expensive infrastructure, and security issues—especially regarding API usage—limit their practical adoption in many scenarios. Compact and open-source models, like DeepSeek-R1 8B and Llama3 8B, have emerged as more cost-effective, self-hosted alternatives, enabling local execution and reducing dependence on external services. Nevertheless, their smaller number of parameters frequently provides limited performance in tasks requiring complex reasoning or specialized knowledge. In this work, we investigate how compact LLMs and LRMs can achieve competitive results without fine-tuning, by leveraging graph-based retrieval-augmented generation (Graph-RAG) and in-context learning (ICL). We conduct a comparative evaluation of both compact and large-scale LLMs and LRMs across four NLP tasks – classification, information extraction, sentiment analysis, and fake news detection—using eight datasets, and analyzing accuracy, latency, and cost. Our experiments explore: (i) the effectiveness of Graph-RAG for factual enrichment; (ii) the impact of ICL with examples the model struggles to answer; (iii) the combination of both techniques, culminating in the new framework EchoRAG; and (iv) the trade-offs between LLMs and LRMs. The results indicate that compact models, when enhanced with Graph-RAG and ICL, can achieve performance close to or even surpassing that of LLMs, while LRMs yielded underwhelming results and exhibited up to 97% higher latency compared to compact models.},
  archive      = {J_ML},
  author       = {Beckhauser, William Jones and Fileto, Renato},
  doi          = {10.1007/s10994-025-06859-1},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {EchoRAG: A framework for enhancing language models with graph-RAG and in-context learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free domain adaptation requires penalized diversity. <em>ML</em>, <em>114</em>(10), 1-30. (<a href='https://doi.org/10.1007/s10994-025-06863-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While neural networks are capable of achieving human-like performance in many tasks such as image classification, the impressive performance of each model is limited to its own dataset. Source-free domain adaptation (SFDA) was introduced to address knowledge transfer between different domains in the absence of source data, thus, increasing data privacy. Diversity in representation space can be vital to a model’s adaptability in varied and difficult domains. In unsupervised SFDA, the diversity is limited to learning a single hypothesis on the source or learning multiple hypotheses with a shared feature extractor. Motivated by the improved predictive performance of ensembles, we propose a novel unsupervised SFDA algorithm that promotes representational diversity through the use of separate feature extractors with Distinct Backbone Architectures (DBA). Although diversity in feature space is increased, the unconstrained mutual information (MI) maximization may potentially introduce amplification of weak hypotheses. Thus we introduce the Weak Hypothesis Penalization (WHP) regularizer as a mitigation strategy. Our work proposes Penalized Diversity (PD) where the synergy of DBA and WHP is applied to unsupervised source-free domain adaptation for covariate shift. In addition, PD is augmented with a weighted MI maximization objective for label distribution shift. Empirical results on natural, synthetic, and medical domains demonstrate the effectiveness of PD under different distributional shifts.},
  archive      = {J_ML},
  author       = {Rafiee Sevyeri, Laya and Sheth, Ivaxi and Farahnak, Farhood and See, Alexandre and Kahou, Samira Ebrahimi and Fevens, Thomas and Havaei, Mohammad},
  doi          = {10.1007/s10994-025-06863-5},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {Source-free domain adaptation requires penalized diversity},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially trained graph convolutional networks resist oversmoothing. <em>ML</em>, <em>114</em>(10), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06865-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we investigate an observation made by Kipf and Welling (5th International Conference on Learning Representations, 2017), who suggested that untrained Graph Convolutional Networks (GCNs) can generate meaningful node embeddings. In particular, we investigate the effect of training only a single layer of a GCN or a GAT (Graph Attention Network), while keeping the rest of the layers frozen. We propose a basis on which the effect of the untrained layers and their contribution to the generation of embeddings can be predicted. Moreover, we show that network width influences the dissimilarity of node embeddings produced after the initial node features pass through the untrained part of the model. Additionally, we establish a connection between partially trained GCNs and oversmoothing, showing that they are capable of reducing it. We verify our theoretical results experimentally and show the benefits of using deep networks that resist oversmoothing, in a “cold start” scenario, where there is a lack of feature information for unlabeled nodes.},
  archive      = {J_ML},
  author       = {Kelesis, Dimitrios and Fotakis, Dimitris and Paliouras, Georgios},
  doi          = {10.1007/s10994-025-06865-3},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Partially trained graph convolutional networks resist oversmoothing},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Splitting stump forests: Tree ensemble compression for edge devices (extended version). <em>ML</em>, <em>114</em>(10), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06866-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Splitting Stump Forests—small ensembles of weak learners extracted from a trained random forest. The high memory consumption of random forests renders them unfit for resource-constrained devices. We show empirically that we can significantly reduce the model size and inference time by selecting nodes that evenly split the arriving training data and applying a linear model on the resulting representation. Our extensive empirical evaluation indicates that Splitting Stump Forests outperform random forests and state-of-the-art compression methods on memory-limited embedded devices.},
  archive      = {J_ML},
  author       = {Alkhoury, Fouad and Buschjäger, Sebastian and Welke, Pascal},
  doi          = {10.1007/s10994-025-06866-2},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Splitting stump forests: Tree ensemble compression for edge devices (extended version)},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel applications of item response theory for analysing data set complexity and benchmark selection. <em>ML</em>, <em>114</em>(10), 1-30. (<a href='https://doi.org/10.1007/s10994-025-06873-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Item response theory (IRT) was developed in psychometrics to measure the latent skills of human respondents based on their observed responses to items with different difficulty levels. Human ability is high in IRT when one correctly responds to difficult items despite random mistakes in easy items. IRT has been recently framed as a powerful tool to characterise instance hardness in classification problems by measuring difficulty and discrimination levels of instances in a data set based on the correctness of a set of classifiers. Here, we generalise such a concept to the data set level by taking a pool of 509 classification data sets and assessing their difficulties and discriminations based on the performance achieved by 95 classifiers when solving these problems. The ability is estimated such that high abilities are assigned to classifiers with better behaviour in hard data sets. We further evaluated IRT in two distinct applications. First, we build a regression meta-model where complexity measures are used to predict the IRT parameters of new data sets without the need to retrain the IRT model. Second, we propose two IRT-based benchmarks with 30 data sets each to test classifiers, one selected for diversity and another selected for greater difficulty. Both benchmarks may be used to evaluate new methods more broadly, instead of the common practice of gathering random data sets from public repositories.},
  archive      = {J_ML},
  author       = {Pereira, João Luiz Junho and Exposito de Queiroz, Alfredo Antonio Alencar and Silva Filho, Telmo de Menezes e and Lorena, Ana Carolina and Mantovani, Rafael Gomes and Pappa, Gisele Lobo and Prudêncio, Ricardo Bastos Cavalcante},
  doi          = {10.1007/s10994-025-06873-3},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {Novel applications of item response theory for analysing data set complexity and benchmark selection},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linearly-interpretable concept embedding models for text analysis. <em>ML</em>, <em>114</em>(10), 1-38. (<a href='https://doi.org/10.1007/s10994-025-06839-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their success, Large-Language Models (LLMs) still face criticism due to their lack of interpretability. Traditional post-hoc interpretation methods, based on attention and gradient-based analysis, offer limited insights as they only approximate the model’s decision-making processes and have been proved to be unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been lately proposed in the textual field to provide interpretable predictions based on human-understandable concepts. However, CBMs still exhibit several limitations due to their architectural constraints limiting their expressivity, to the absence of task-interpretability when employing non-linear task predictors and for requiring extensive annotations that are impractical for real-world text data. In this paper, we address these challenges by proposing a novel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the current accuracy-interpretability trade-off. LICEMs classification accuracy is better than existing interpretable models and matches black-box ones. We show that the explanations provided by our models are more intervenable and causally consistent with respect to existing solutions. Finally, we show that LICEMs can be trained without requiring any concept supervision, as concepts can be automatically predicted when using an LLM backbone.},
  archive      = {J_ML},
  author       = {De Santis, Francesco and Bich, Philippe and Ciravegna, Gabriele and Barbiero, Pietro and Giordano, Danilo and Cerquitelli, Tania},
  doi          = {10.1007/s10994-025-06839-5},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Linearly-interpretable concept embedding models for text analysis},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Region-aware minimal counterfactual rules for model-agnostic explainable classification. <em>ML</em>, <em>114</em>(10), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06847-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for transparency in machine learning has spurred the development of techniques that provide faithful explanations for complex black-box models. In this work, we introduce RaMiCo (Region Aware Minimal Counterfactual Rules), a model-agnostic method that extracts global counterfactual rules by mining instances from diverse regions of the input space. RaMiCo focuses on single-feature substitutions to generate minimal and region-aware rules that encapsulate the overall decision-making process of the target model. These global rules can be further localised to specific input instances, enabling users to obtain tailored explanations for individual predictions. Comprehensive experiments on multiple benchmark datasets demonstrate that RaMiCo achieves competitive fidelity in replicating black-box behaviour and exhibits high coverage in capturing the intrinsic structure of white-box classifiers. RaMiCo supports the development of trustworthy and secure machine learning systems by providing transparent, human-understandable explanations in the form of concise global rules. This design enables users to verify and inspect the model’s decision logic, reducing the risk of hidden biases, unintended behaviours, or adversarial exploitation. These features make RaMiCo particularly suitable for applications where the reliability, safety, and verifiability of automated decisions are essential.},
  archive      = {J_ML},
  author       = {Gagliardi, Guido and Alfeo, Antonio Luca and Guidotti, Riccardo and Cimino, Mario G. C. A.},
  doi          = {10.1007/s10994-025-06847-5},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Region-aware minimal counterfactual rules for model-agnostic explainable classification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized YOLOv8 for lightweight and high-precision metal surface defect detection in industrial applications. <em>ML</em>, <em>114</em>(10), 1-35. (<a href='https://doi.org/10.1007/s10994-025-06857-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the metal industrial manufacturing process, surface defect detection is critical because these defects can seriously affect material quality and production efficiency. In recent years, although deep learning technology has made significant advances in the field of surface defect detection for images, it still faces many challenges in metal surface defect detection, such as high variability and sample imbalance. To address these challenges, this paper proposes an improved algorithm based on YOLOv8. First, this paper designs the C2f_GhostDynamic module, which reduces model size and computational overhead, lowering hardware requirements and energy consumption, making the model more suitable for deployment on embedded devices. Secondly, the CARAFE algorithm replaces the traditional upsampling method, expanding the model’s receptive field and enhancing feature extraction and fusion capabilities. Additionally, the RFAHead module was designed as the detection head, with RFAConv improving the model’s detection accuracy. At the same time, the SPPELAN pyramid pooling structure was introduced, combining multi-scale pooling and local attention mechanisms to generate more expressive feature maps. Finally, AKConv replaces traditional convolution operations, enabling adaptive adjustment of kernel size based on defect features and shapes, further reducing model size and improving performance. Experiments on the public NEU-DET dataset show that the improved model, compared to the baseline, reduces parameters by 9.2% , FLOPs by 31.7%, and increases mAP50 by 1.7 to 79.3%. Additionally, the model’s detection speed reaches 38 frames per second, meeting the requirements for real-time detection. Code is available at https://github.com/IamSunday/YOLOv8-Steel-Detection .},
  archive      = {J_ML},
  author       = {Li, Ruiping and Zhao, Linchang and Wei, Hao and OuYang, Bocheng and Zhang, Mu and Fang, Bing and Hu, Guoqing and Tan, Jin},
  doi          = {10.1007/s10994-025-06857-3},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-35},
  shortjournal = {Mach. Learn.},
  title        = {Optimized YOLOv8 for lightweight and high-precision metal surface defect detection in industrial applications},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SafeGen: Safeguarding privacy and fairness through a genetic method. <em>ML</em>, <em>114</em>(10), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06835-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure that Machine Learning systems produce unharmful outcomes, pursuing a joint optimization of performance and ethical profiles such as privacy and fairness is crucial. However, jointly optimizing these two ethical dimensions while maintaining predictive accuracy remains a fundamental challenge. Indeed, privacy-preserving techniques may worsen fairness and restrain the model’s ability to learn accurate statistical patterns, while data mitigation techniques may inadvertently compromise privacy. Aiming to bridge this gap, we propose safeGen, a preprocessing fairness enhancing and privacy-preserving method for tabular data. SafeGen employs synthetic data generation through a genetic algorithm to ensure that sensitive attributes are protected while maintaining the necessary statistical properties. We assess our method across multiple datasets, comparing it against state-of-the-art privacy-preserving and fairness approaches through a threefold evaluation: privacy preservation, fairness enhancement, and generated data plausibility. Through extensive experiments, we demonstrate that SafeGen consistently achieves strong anonymization while preserving or improving dataset fairness across several benchmarks. Additionally, through hybrid privacy-fairness constraints and the use of a genetic synthesizer, SafeGen ensures the plausibility of synthetic records while minimizing discrimination. Our findings demonstrate that modeling fairness and privacy within a unified generative method yields significantly better outcomes than addressing these constraints separately, reinforcing the importance of integrated approaches when multiple ethical objectives must be simultaneously satisfied.},
  archive      = {J_ML},
  author       = {Cinquini, Martina and Marchiori Manerba, Marta and Mazzoni, Federico and Pratesi, Francesca and Guidotti, Riccardo},
  doi          = {10.1007/s10994-025-06835-9},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {SafeGen: Safeguarding privacy and fairness through a genetic method},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FediOS: Decoupling orthogonal subspaces for personalization in feature-skew federated learning. <em>ML</em>, <em>114</em>(10), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06861-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (pFL) enables collaborative training among multiple clients to enhance the capability of customized local models. In pFL, clients may have heterogeneous (also known as non-IID) data, which poses a key challenge in how to decouple the data knowledge into generic knowledge for global sharing and personalized knowledge for preserving local personalization. A typical way of pFL focuses on label distribution skew, and they adopt a decoupling scheme where the model is split into a common feature extractor and two prediction heads (generic and personalized). However, such a decoupling scheme cannot solve the essential problem of feature-skew heterogeneity, because one feature extractor only outputs one feature map that may not realize generic and personalized features at the same time. Therefore, in this paper, we rethink the decoupling in feature-skew pFL and subsequently propose FediOS inspired by orthogonal techniques in continual learning. In FediOS, we decouple the features by implementing two feature extractors (generic and personalized) and realize implicit feature decoupling by set-and-fixed orthogonal projections. For two feature extractors, orthogonal projections are used to map the generic features into one common subspace and scatter the personalized features into different subspaces. The proposed shared prediction head can adapt to the sample-wise importance of generic and personalized features for prediction, given that samples would have distinct proportions of generic and personalized features. Extensive experiments on four vision datasets demonstrate our method reaches state-of-the-art pFL performances under feature skew, even label skew, and mix-heterogeneity of both label and feature skew.},
  archive      = {J_ML},
  author       = {Gao, Lingzhi and Li, Zexi and Shang, Xinyi and Lu, Yang and Wu, Chao},
  doi          = {10.1007/s10994-025-06861-7},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {FediOS: Decoupling orthogonal subspaces for personalization in feature-skew federated learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelradar: Aspect-based forecast evaluation. <em>ML</em>, <em>114</em>(10), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06877-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate evaluation of forecasting models is essential for ensuring reliable predictions. Current practices for evaluating and comparing forecasting models focus on summarising performance into a single score, using metrics such as SMAPE. While convenient, averaging performance over all samples dilutes relevant information about model behaviour under varying conditions. This limitation is especially problematic for time series forecasting, where multiple layers of averaging–across time steps, horizons, and multiple time series in a dataset–can mask relevant performance variations. We address this limitation by proposing ModelRadar, a framework for evaluating univariate time series forecasting models across multiple aspects, such as stationarity, presence of anomalies, or forecasting horizons. We demonstrate the advantages of this framework by comparing 24 forecasting methods, including classical approaches and different machine learning algorithms. PatchTST, a state-of-the-art transformer-based neural network architecture, performs best overall but its superiority varies with forecasting conditions. For instance, concerning the forecasting horizon, we found that PatchTST (and also other neural networks) only outperforms classical approaches for multi-step ahead forecasting. Another relevant insight is that classical approaches such as ETS or Theta are notably more robust in the presence of anomalies. These and other findings highlight the importance of aspect-based model evaluation for both practitioners and researchers. ModelRadar is available as a Python package.},
  archive      = {J_ML},
  author       = {Cerqueira, Vitor and Roque, Luis and Soares, Carlos},
  doi          = {10.1007/s10994-025-06877-z},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Modelradar: Aspect-based forecast evaluation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rényi divergence in hidden markov models. <em>ML</em>, <em>114</em>(10), 1-44. (<a href='https://doi.org/10.1007/s10994-025-06872-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we examine the existence of the Rényi divergence between two time invariant hidden Markov models with arbitrary positive initial distributions. By making use of a Markov chain representation of the probability distribution for the hidden Markov model and eigenvalue for the associated Markovian operator, we obtain, under some regularity conditions, convergence of the Rényi divergence. By using this device, we also characterize the Rényi divergence and obtain the Kullback–Leibler divergence as $$\alpha \rightarrow 1$$ of the Rényi divergence. Several examples, including classical finite state hidden Markov models, Markov switching models, and recurrent neural networks, are given for illustration. Moreover, we develop a non-Monte Carlo method that computes the Rényi divergence of two-state Markov switching models via the underlying invariant probability measure, which is characterized by the Fredholm integral equation.},
  archive      = {J_ML},
  author       = {Fuh, Cheng-Der and Fuh, Su-Chi and Liu, Yuan-Chen and Wang, Chuan-Ju},
  doi          = {10.1007/s10994-025-06872-4},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-44},
  shortjournal = {Mach. Learn.},
  title        = {Rényi divergence in hidden markov models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new filter for deformation-invariant persistence diagram. <em>ML</em>, <em>114</em>(10), 1-38. (<a href='https://doi.org/10.1007/s10994-025-06874-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies of Topological Data Analysis have focused on producing isotropic stretching-invariant Persistence Diagram (PD). We show that current methods to generate PDs are sensitive to a general form of deformation of a point cloud, where all the deformed versions share the same topology. We analyze the effect of this deformation on the generated PD, and propose a new filter to produce a deformation-invariant PD. In addition, we provide a theoretical result on our proposed filter’s robustness against outliers in the point cloud. Our empirical evaluation shows that, in the presence of the deformation in a point cloud, our proposed filter outperforms existing filters in clustering tasks at point cloud level. As for point level clustering, our proposed filter produces better outcome when clustering points according to what topological features they contribute to.},
  archive      = {J_ML},
  author       = {Zhang, Kaifeng and Zhang, Hang and Ting, Kai Ming and Liang, Tianrun},
  doi          = {10.1007/s10994-025-06874-2},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {A new filter for deformation-invariant persistence diagram},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling spatio-temporal locality in multi-step forecasting of geo-referenced time series. <em>ML</em>, <em>114</em>(10), 1-32. (<a href='https://doi.org/10.1007/s10994-025-06875-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting future measurements from geographically distributed sensors is essential across many application domains. However, the spatial distribution of these sensors raises multiple challenges, primarily due to spatial autocorrelation phenomena, that introduce inter-dependencies among nearby locations, that cannot therefore be treated independently by learning algorithms. While some existing approaches can capture such phenomena, they generally model the spatial dimension globally across all locations. On the other hand, the method we propose in this paper, called SPALT, focuses on capturing spatial relationships specifically among time series with similar trends, even if these trends occur at different times, thus modeling the spatio-temporal locality. SPALT leverages linear model trees, which allow us to naturally consider the spatial autocorrelation in a local manner: during the tree-building process, the adopted heuristics aim to group time series exhibiting similar trends into the same node, on which additional features considering the spatial dimension are selectively injected. Additionally, we propose a new pruning strategy, based on Reduced Error Pruning (REP), that also considers the spatio-temporal locality during the tree simplification. Designed for a multi-step setting, SPALT provides forecasts for multiple future time steps across multiple sensors simultaneously. The characteristics exhibited by SPALT can provide significant benefits in different domains, where measurements come from geographically distributed sensors. In this paper, we focus on data produced by sensors located in multiple renewable power plants measuring their energy production at regular, short intervals. Experiments on three real-world datasets demonstrate the effectiveness of SPALT in forecasting the production of energy at different time horizons, and its superior performance in comparison with tree-based models and state-of-the-art neural networks that incorporate both temporal and spatial dimensions.},
  archive      = {J_ML},
  author       = {D’Aversa, Annunziata and Pio, Gianvito and Ceci, Michelangelo},
  doi          = {10.1007/s10994-025-06875-1},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-32},
  shortjournal = {Mach. Learn.},
  title        = {Modeling spatio-temporal locality in multi-step forecasting of geo-referenced time series},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counterfactual ensembles for interpretable churn prediction: From real-world to privacy-preserving synthetic data. <em>ML</em>, <em>114</em>(10), 1-27. (<a href='https://doi.org/10.1007/s10994-025-06880-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual explanations identify minimal input changes needed to alter a machine learning model’s prediction, offering actionable insights in tasks like churn analysis. However, existing methods often produce counterfactuals that vary in quality, coherence, and plausibility, limiting their practical value. We propose an ensemble evaluation framework that integrates multiple generation techniques and ranks their outputs using a tunable scoring function balancing multiple relevant metrics. Our approach addresses two key deployment scenarios: (i) in-house churn analysis, where decision-makers can interactively adjust scoring weights for tailored, user-driven explanations; and (ii) outsourced churn prediction, where counterfactuals must be generated on synthetic data to preserve privacy while remaining representative of real cases. Experiments on benchmark churn datasets demonstrate that our ensemble approach improves the consistency, interpretability, and utility of counterfactuals across both real and synthetic settings, supporting more reliable and privacy-aware decision-making.},
  archive      = {J_ML},
  author       = {Tonati, Samuele and Vece, Marzio Di and Giannotti, Fosca and Pellungrini, Roberto},
  doi          = {10.1007/s10994-025-06880-4},
  journal      = {Machine Learning},
  month        = {10},
  number       = {10},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Counterfactual ensembles for interpretable churn prediction: From real-world to privacy-preserving synthetic data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving text processing via adversarial low-rank adaptation. <em>ML</em>, <em>114</em>(9), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06817-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter-efficient fine-tuning (PEFT) has emerged as a predominant approach for adapting large language models(LLMs) to downstream natural language processing tasks. This method achieves significant savings in time and computational resources by training a small set of adapters parameter while preserving the main structure of the LLMs. Among various PEFT methods, low-rank adaptation (LoRA) is one of the widely used methods for fine-tuning LLMs. However, the parameter scale of the adapters is significantly smaller than that of the LLMs. This discrepancy may introduce notable instability when the model adapts to various downstream tasks, thereby potentially limiting the performance of LoRA. To address this issue, we propose an adversarial training-based low-rank adaptation method, termed ADV-LoRA. The core idea of this method is to increase the model’s instability by introducing adversarial perturbations and then reduce this instability through an adversarial training mechanism. Specifically, ADV-LoRA applies adversarial perturbations at the adapter parameter layer, causing a temporary decline in the performance of the LLMs on specific tasks. Subsequently, the adversarial training process gradually mitigates performance fluctuations, encouraging the LLMs to exhibit more consistent and stable performance on specific task data, thereby enhancing the robustness of model fine-tuning. Extensive experiments have demonstrated the effectiveness of ADV-LoRA, particularly in text processing tasks such as text classification and text generation.},
  archive      = {J_ML},
  author       = {Wu, Hao and Luo, Xiangfeng and Gao, Jianqi and Huang, Dian},
  doi          = {10.1007/s10994-025-06817-x},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Improving text processing via adversarial low-rank adaptation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing interpretability in generative modeling: Statistically disentangled latent spaces guided by generative factors in scientific datasets. <em>ML</em>, <em>114</em>(9), 1-17. (<a href='https://doi.org/10.1007/s10994-025-06816-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the challenge of statistically extracting generative factors from complex, high-dimensional datasets in unsupervised or semi-supervised settings. We investigate encoder–decoder-based generative models for nonlinear dimensionality reduction, focusing on disentangling low-dimensional latent variables corresponding to independent physical factors. Introducing Aux-VAE, a novel architecture within the classical Variational Autoencoder framework, we achieve disentanglement with minimal modifications to the standard VAE loss function by leveraging prior statistical knowledge through auxiliary variables. These variables guide the shaping of the latent space by aligning latent factors with learned auxiliary variables. We validate the efficacy of Aux-VAE through comparative assessments on multiple datasets, including astronomical simulations.},
  archive      = {J_ML},
  author       = {Ganguli, Arkaprabha and Ramachandra, Nesar and Bessac, Julie and Constantinescu, Emil},
  doi          = {10.1007/s10994-025-06816-y},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-17},
  shortjournal = {Mach. Learn.},
  title        = {Enhancing interpretability in generative modeling: Statistically disentangled latent spaces guided by generative factors in scientific datasets},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-informed variational autoencoders for improved robustness to environmental factors of variation. <em>ML</em>, <em>114</em>(9), 1-40. (<a href='https://doi.org/10.1007/s10994-025-06829-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of machine learning models with physical models is a recent research path to learn robust data representations. In this paper, we introduce $$\hbox {p}^3$$ VAE, a variational autoencoder that integrates prior physical knowledge modeling the generative latent factors of variation that are related to the data acquisition conditions. $$\hbox {p}^3$$ VAE combines standard neural network layers with non-trainable physics layers in order to partially ground the latent space to physical variables. In order to fully leverage our physics-informed machine learning model, we introduce a semi-supervised learning algorithm that strikes a balance between the machine learning part and the physics part. Experiments on simulated and real data sets demonstrate the benefits of our framework against competing physics-informed and conventional machine learning models, in terms of extrapolation capabilities and interpretability. In particular, we show that $$\hbox {p}^3$$ VAE naturally has interesting disentanglement capabilities. Our code and data have been made publicly available at https://github.com/Romain3Ch216/p3VAE .},
  archive      = {J_ML},
  author       = {Thoreau, Romain and Risser, Laurent and Achard, Véronique and Berthelot, Béatrice and Briottet, Xavier},
  doi          = {10.1007/s10994-025-06829-7},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {Physics-informed variational autoencoders for improved robustness to environmental factors of variation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MbExplainer: Multilevel bandit-based explanations for downstream models with augmented graph embeddings. <em>ML</em>, <em>114</em>(9), 1-42. (<a href='https://doi.org/10.1007/s10994-025-06830-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are a highly useful tool for performing various machine learning prediction tasks on graph-structured data. In many industrial applications, it is common that the graph embeddings generated from training GNNs are used in an ensemble model where the embeddings are combined with other tabular features, e.g., original node or edge features, in a downstream machine learning task. The tabular features may even arise naturally if, e.g., one tries to build a graph such that some of the node or edge features are stored in a tabular format. In this paper we address the problem of explaining the output of such ensemble models for which the input features consist of learned neural graph embeddings combined with additional tabular features. Therefore, we propose MbExplainer, a model-agnostic explanation approach for downstream models with augmented graph embeddings. MbExplainer returns a human-comprehensible triple as an explanation for an instance prediction of the whole pipeline consisting of three components: a subgraph with the highest importance, the topmost important node features, and the topmost important augmented downstream features. A game-theoretic formulation is used to take the contributions of each component and their interactions into account by assigning three Shapley values corresponding to their own specific games. Finding the explanation requires an efficient search through the local search spaces corresponding to each component. MbExplainer applies a novel multilevel search algorithm that enables simultaneous pruning of local search spaces in a computationally tractable way. In particular, three interweaved Monte Carlo Tree Searches are utilized to iteratively prune the local search spaces. MbExplainer also includes a global search algorithm that uses contextual bandits to efficiently allocate pruning budget among the local search spaces. We demonstrate the effectiveness of MbExplainer by presenting a set of comprehensive numerical examples on multiple public graph datasets for node classification, graph classification, and graph regression tasks.},
  archive      = {J_ML},
  author       = {Golgoon, Ashkan and Franks, Ryan and Filom, Khashayar and Ravi Kannan, Arjun},
  doi          = {10.1007/s10994-025-06830-0},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-42},
  shortjournal = {Mach. Learn.},
  title        = {MbExplainer: Multilevel bandit-based explanations for downstream models with augmented graph embeddings},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auction-based incentive mechanism with personalized privacy protection in federated learning. <em>ML</em>, <em>114</em>(9), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06836-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy in federated learning (DP-FL) aims to prevent the leakage of sensitive information through client-uploaded model parameters. However, existing DP-FL frameworks typically enforce uniform privacy protection across all clients, neglecting individual privacy preferences and ultimately degrading the performance of the global model. To address this limitation, we propose an auction-based incentive mechanism for personalized privacy protection in federated learning (IMPP-FL). By integrating clients’ reported private information with data quality assessments, our mechanism encourages clients to truthfully disclose their privacy preferences and maintain high model quality. This, in turn, enhances the overall performance of federated learning. Theoretically, we prove that our mechanism satisfies dominant strategy incentive compatibility, budget constraints, and individual rationality, enabling clients to reveal their minimum acceptable privacy budgets, data volumes, and training costs. Extensive experiments on MNIST, Fashion-MNIST, and CIFAR-10 (IID & Non-IID) show that, under a total privacy budget of 10,000, IMPP-FL achieves up to 14% higher test accuracy than UPSM-FL, delivers up to 4% improvement over KGM-FL, and remains within 1% of the noise-free CDSM-FL upper bound on MNIST-IID.},
  archive      = {J_ML},
  author       = {Siqin, Zeng and Xiaohong, Wu and Yonggen, Gu and Jie, Tao and BenFeng, Chen and GuoQiang, Li},
  doi          = {10.1007/s10994-025-06836-8},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Auction-based incentive mechanism with personalized privacy protection in federated learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploration and generalization in deep learning with SwitchPath activations. <em>ML</em>, <em>114</em>(9), 1-34. (<a href='https://doi.org/10.1007/s10994-025-06840-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work provides a comprehensive theoretical and empirical analysis of SwitchPath, a stochastic activation function that improves learning dynamics by probabilistically toggling between a neuron standard activation and its negation. We develop theoretical foundations and demonstrate its impact in multiple scenarios. By maintaining gradient flow and injecting controlled stochasticity, the method improves generalization, uncertainty estimation, and training efficiency. Experiments in classification show consistent gains over ReLU and Leaky ReLU across CNNs and Vision Transformers, with reduced overfitting and better test accuracy. In generative modeling, a novel two-phase training scheme significantly mitigates mode collapse and accelerates convergence. Our theoretical analysis reveals that SwitchPath introduces a form of multiplicative noise that acts as a structural regularizer. Additional empirical investigations show improved information propagation and reduced model complexity. These results establish this activation mechanism as a simple yet effective way to enhance exploration, regularization, and reliability in modern neural networks.},
  archive      = {J_ML},
  author       = {Di Cecco, Antonio and Papini, Andrea and Metta, Carlo and Fantozzi, Marco and Galfrè, Silvia Giulia and Morandin, Francesco and Parton, Maurizio},
  doi          = {10.1007/s10994-025-06840-y},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-34},
  shortjournal = {Mach. Learn.},
  title        = {Exploration and generalization in deep learning with SwitchPath activations},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A calibration test for evaluating set-based epistemic uncertainty representations. <em>ML</em>, <em>114</em>(9), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06844-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate representation of epistemic uncertainty is a challenging yet essential task in machine learning. A widely used representation corresponds to convex sets of probabilistic predictors, also known as credal sets. One popular way of constructing these credal sets is via ensembling or specialized supervised learning methods, where the epistemic uncertainty can be quantified through measures such as the set size or the disagreement among members. In principle, these sets should contain the true data-generating distribution. As a necessary condition for this validity, we adopt the strongest notion of calibration as a proxy. Concretely, we propose a novel statistical test to determine whether there is a convex combination of the set’s predictions that is calibrated in distribution. In contrast to previous methods, our framework allows the convex combination to be instance-dependent, recognizing that different ensemble members may be better calibrated in different regions of the input space. Moreover, we learn this combination via proper scoring rules, which inherently optimize for calibration. Building on differentiable, kernel-based estimators of calibration errors, we introduce a nonparametric testing procedure and demonstrate the benefits of capturing instance-level variability on synthetic and real-world experiments.},
  archive      = {J_ML},
  author       = {Jürgens, Mira and Mortier, Thomas and Hüllermeier, Eyke and Bengs, Viktor and Waegeman, Willem},
  doi          = {10.1007/s10994-025-06844-8},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {A calibration test for evaluating set-based epistemic uncertainty representations},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncover and unlearn nuisances: Agnostic fully test-time adaptation. <em>ML</em>, <em>114</em>(9), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06842-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.},
  archive      = {J_ML},
  author       = {Srey, Ponhvoan and Shi, Yaxin and Qian, Hangwei and Li, Jing and Tsang, Ivor W.},
  doi          = {10.1007/s10994-025-06842-w},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Uncover and unlearn nuisances: Agnostic fully test-time adaptation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the risk of discriminatory bias in classification datasets. <em>ML</em>, <em>114</em>(9), 1-19. (<a href='https://doi.org/10.1007/s10994-025-06843-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bias in machine learning models remains a critical challenge, particularly in datasets with numeric features where discrimination may be subtle and hard to detect. Existing fairness frameworks rely on expert knowledge of marginalized groups, such as specific racial groups, and categorical features defining them. Furthermore, most frameworks evaluate bias in models rather than datasets, despite the fact that model bias can often be traced back to dataset shortcomings. Our research aims to remedy this gap by capturing dataset flaws in a set of meta-features at the dataset level, and to warn practitioners of bias risk when using such datasets for model training. We neither restrict the feature type nor expect domain knowledge. To this end, we develop methods to synthesize biased datasets and extend current fairness metrics to continuous features in order to quantify dataset-level discrimination risks. Our approach constructs a meta-database of diverse datasets, from which we derive transferable meta-features that capture dataset properties indicative of bias risk. Our findings demonstrate that dataset-level characteristics can serve as cost-effective indicators of bias risk, providing a novel method for data auditing that does not rely on expert knowledge. This work lays the foundation for early-warning systems, moving beyond model-focused assessments toward a data-centric approach.},
  archive      = {J_ML},
  author       = {Dai, Kejun and Kim, Jonathan and Džeroski, Sašo and Wicker, Jörg and Dobbie, Gillian and Dost, Katharina},
  doi          = {10.1007/s10994-025-06843-9},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Assessing the risk of discriminatory bias in classification datasets},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Raising the numbers: Multi-generation adversarial attack and frequency-based defense for heightened NLP security. <em>ML</em>, <em>114</em>(9), 1-32. (<a href='https://doi.org/10.1007/s10994-025-06833-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Artificial Intelligence (AI) into applications of Natural Language Processing (NLP), such as spam detection and sentiment analysis, necessitates robust and explainable defenses against adversarial attacks—subtle input perturbations that can compromise model integrity. In this paper, we propose two novel methods, driven by Explainable AI (XAI), for enhancing the robustness of deep learning models in NLP. Traditional methods for generating adversarial examples are often resource-intensive relative to the number of samples produced, limiting their effectiveness in large-scale adversarial training. To address this problem, we propose, as our first contribution, a Multi-Generation Attack Strategy (MGAS) that leverages explainability techniques to generate a diverse set of adversarial examples for adversarial training. After a baseline adversarial text is crafted, we carefully perform three actions: swapping perturbations with alternatives, rolling back low-contributing words, and exchanging perturbed indices, thereby creating a diverse set of adversarial samples. Our second contribution introduces an additive correction defense mechanism that actively revises input texts at inference time. Using XAI to identify the most critical words in the input text, we substitute them with their most frequent suitable synonyms, thereby reducing the adversarial impact while preserving the model’s performance on clean data. Comprehensive evaluations demonstrate that both approaches, individually or combined, significantly enhance the robustness and transparency of AI models, offering a promising pathway for improving the security and reliability of NLP systems through XAI-driven techniques.},
  archive      = {J_ML},
  author       = {Khemis, Salim and Amara, Yacine and Benatia, Mohamed Akrem and Messalti, Ishak and Khanous, Mohammed Elamin Ilyas and De Baets, Bernard},
  doi          = {10.1007/s10994-025-06833-x},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-32},
  shortjournal = {Mach. Learn.},
  title        = {Raising the numbers: Multi-generation adversarial attack and frequency-based defense for heightened NLP security},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensuring medical AI safety: Interpretability-driven detection and mitigation of spurious model behavior and associated data. <em>ML</em>, <em>114</em>(9), 1-52. (<a href='https://doi.org/10.1007/s10994-025-06834-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are increasingly employed in high-stakes medical applications, despite their tendency for shortcut learning in the presence of spurious correlations, which can have potentially fatal consequences in practice. Whereas a multitude of works address either the detection or mitigation of such shortcut behavior in isolation, the Reveal2Revise approach provides a comprehensive bias mitigation framework combining these steps. However, effectively addressing these biases often requires substantial labeling efforts from domain experts. In this work, we review the steps of the Reveal2Revise framework and enhance it with semi-automated interpretability-based bias annotation capabilities. This includes methods for the sample- and feature-level bias annotation, providing valuable information for bias mitigation methods to unlearn the undesired shortcut behavior. We show the applicability of the framework using four medical datasets across two modalities, featuring controlled and real-world spurious correlations caused by data artifacts. We successfully identify and mitigate these biases in VGG16, ResNet50, and contemporary Vision Transformer models, ultimately increasing their robustness and applicability for real-world medical tasks. Our code is available at https://github.com/frederikpahde/medical-ai-safety .},
  archive      = {J_ML},
  author       = {Pahde, Frederik and Wiegand, Thomas and Lapuschkin, Sebastian and Samek, Wojciech},
  doi          = {10.1007/s10994-025-06834-w},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-52},
  shortjournal = {Mach. Learn.},
  title        = {Ensuring medical AI safety: Interpretability-driven detection and mitigation of spurious model behavior and associated data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified view of forward and backward losses for learning from weak labels. <em>ML</em>, <em>114</em>(9), 1-34. (<a href='https://doi.org/10.1007/s10994-025-06841-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training multiclass classifiers on weakly labeled datasets, where labels provide only partial or noisy information about the true class, poses a significant challenge in machine learning. To address various forms of label corruption, including noisy, complementary, supplementary, or partial labels, as well as positive-unlabeled data, forward and backward correction losses have been widely employed. Adopting a general formulation that encompasses all these types of label corruption, we introduce a new family of loss functions, termed forward-backward losses, which generalizes both forward and backward correction. We analyze the theoretical properties of this family, providing sufficient conditions under which these losses are proper, ranking-calibrated, classification-calibrated, convex, or lower-bounded. This unified view will be useful to show, through theoretical analysis and experiments, that proper forward losses consistently outperform other forward-backward losses in terms of robustness and accuracy. However, the optimal choice of loss for ranking- and classification-calibrated settings remains an open question. Our work provides a comprehensive framework for weak label learning, offering new directions to develop more robust and effective algorithms.},
  archive      = {J_ML},
  author       = {Bacaicoa-Barber, Daniel and Cid-Sueiro, Jesús},
  doi          = {10.1007/s10994-025-06841-x},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-34},
  shortjournal = {Mach. Learn.},
  title        = {A unified view of forward and backward losses for learning from weak labels},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALM-PU: Positive and unlabeled learning with constrained optimization. <em>ML</em>, <em>114</em>(9), 1-22. (<a href='https://doi.org/10.1007/s10994-025-06849-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive and Unlabeled Learning (PUL) is a special semi-supervised learning paradigm trained on datasets comprising positive and unlabeled samples, and it is widely applied in many real-world applications. Due to the lack of reliable labeled negative samples, PUL is much more challenging than traditional semi-supervised learning. Previous works primarily focus on two fronts: applying varying loss weights for different samples or leveraging algorithms to construct auxiliary datasets with reliable labels. Despite the impressive performance the previous works have already achieved, they are still faced with the challenge of negative classification bias, while struggling to get rid of the need for class prior knowledge. In this paper, we have transformed the PUL task into a constrained optimization problem, and propose a new PUL framework to solve it, namely ALM-PU. More detailed, our ALM-PU aims to minimize the classifier’s classification loss on reliable labeled set as the ultimate optimization goal, while subject to the constraint of reducing the cost on the unlabeled set to a certain extent. Subsequently, ALM-PU integrates the primary objective with the constraints to construct the PUL-constrained optimization framework, and implement it in the neural network structure. During the training process, our approach corrects the model’s negative classification bias, achieving superior classification performance compared to previous methods. Additionally, a prediction sequence-based algorithm is utilized to aid the classifier in better distinguishing positive from negative samples with training results. We conducted extensive experiments on multiple PUL benchmarks. ALM-PU achieves an average improvement of 2% in key metrics, attaining state-of-the-art performance. These findings validate the effectiveness of our ALM-PU approach. Complete code and more experimental details can be found at ALM-PU.},
  archive      = {J_ML},
  author       = {Wei, Jiazhe and Wu, Yuefei and Shi, Bin and Li, Ken and Dong, Bo},
  doi          = {10.1007/s10994-025-06849-3},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-22},
  shortjournal = {Mach. Learn.},
  title        = {ALM-PU: Positive and unlabeled learning with constrained optimization},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution estimation for time series via DNN-based GANs with an application to change-point estimation. <em>ML</em>, <em>114</em>(9), 1-37. (<a href='https://doi.org/10.1007/s10994-025-06851-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative adversarial networks (GANs) have recently been applied to estimating the distribution of independent and identically distributed data, and have attracted a lot of research attention. In this paper, we demonstrate the effectiveness of GANs in estimating the joint distribution of stationary time series. Theoretically, we derive a non-asymptotic error bound for the Deep Neural Network (DNN)-based GANs estimator for the stationary distribution of the time series. Our approach is based on the blocking technique and the M-dependence approximation technique that divides the time series into interlacing blocks of equal size and then constructs independent blocks. Based on the theoretical analysis, we propose an algorithm for estimating the position of the change-point in a time series. Numerical results of Monte Carlo experiments and a real data application are given to validate our theory and algorithm.},
  archive      = {J_ML},
  author       = {Lu, Jianya and Mo, Yingjun and Xiao, Zhijie and Xu, Lihu and Yao, Qiuran},
  doi          = {10.1007/s10994-025-06851-9},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Distribution estimation for time series via DNN-based GANs with an application to change-point estimation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward practical human-interpretable explanations. <em>ML</em>, <em>114</em>(9), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06852-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-agnostic feature attribution techniques are used to explain the decisions of complex machine learning (ML) models including ensemble models, and deep neural networks (DNNs). However, since complex ML models perform best when trained on low-level features, the explanations generated by these algorithms are often not interpretable or usable by humans. Recently proposed model-agnostic methods that support the generation of human-interpretable explanations are impractical because they require a fully invertible transformation function that maps the model’s input features to human-interpretable features. While some practical human-interpretable explainability methods exist (e.g., concept-based methods), they typically require direct access to the model and are not fully model-agnostic. In this paper, we introduce Latent SHAP, a model-agnostic black-box feature attribution framework that provides human-interpretable explanations without necessitating a fully invertible transformation function. We validate the fidelity of Latent SHAP ’s explanations through quantitative faithfulness assessments on two controlled datasets—a self-generated artificial dataset and the dSprites dataset. Furthermore, we showcase the practical utility of Latent SHAP in various real-world scenarios across domains such as computer vision, natural language processing, and cybersecurity. Each domain involves complex models (ensembles, DNNs, and LLMs), where invertible transformation functions are not available.},
  archive      = {J_ML},
  author       = {Malach, Alon and Meiseles, Amiel and Bitton, Ron and Momiyama, Satoru and Araki, Toshinori and Furukawa, Jun and Elovici, Yuval and Shabtai, Asaf},
  doi          = {10.1007/s10994-025-06852-8},
  journal      = {Machine Learning},
  month        = {9},
  number       = {9},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Toward practical human-interpretable explanations},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RCC-MAS: A new algorithm for computing all rough-set-constructs. <em>ML</em>, <em>114</em>(8), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06786-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In rough set theory, a construct is defined as a subset of attributes possessing the same capacity as the complete set of attributes to discern objects from different classes, while preserving similarity between objects from the same class. In the literature, it has been shown that algorithms designed for computing reducts or typical testors can be modified to calculate constructs. However, in practice, there are scenarios where even the fastest algorithms in the current state-of-the-art struggle to compute all constructs within a reasonable time-frame. This paper presents a novel algorithm to compute all constructs within a decision table to reduce this gap. Our proposed algorithm, RCC-MAS, works on the binary discernibility-similarity matrix and employs a recursive approach to reduce the search space systematically by analyzing minimum attribute subsets whose attributes, when excluded, lead to rows with zeros in those attributes in the matrix, violating the construct definition. This strategy reduces the number of subsets generated, focusing on attributes essential for constructs; additionally, we demonstrate theoretically that all constructs are computed. Experimental evaluations spanning several synthetic and real-world decision tables reveal that RCC-MAS is the best option to compute constructs regardless of the density of the SBDSM.},
  archive      = {J_ML},
  author       = {González Díaz, Yanir and Lazo Cortés, Manuel S. and Martínez Trinidad, José Fco. and Carrasco Ochoa, Jesús A.},
  doi          = {10.1007/s10994-025-06786-1},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {RCC-MAS: A new algorithm for computing all rough-set-constructs},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segmentation and feature extraction-based classification of pavement damages using hybrid computer vision and machine learning approaches. <em>ML</em>, <em>114</em>(8), 1-33. (<a href='https://doi.org/10.1007/s10994-025-06803-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the safety, durability, and cost-effectiveness of road infrastructure maintenance requires accurate and efficient damage assessment. However, the heterogeneous nature of pavement deterioration, ranging from cracks and potholes to exposed aggregate areas, poses significant challenges for its analysis and classification. This study introduces a novel hybrid methodology that integrates advanced segmentation, feature extraction, and classification techniques to enhance the accuracy and robustness of road-damage detection. Unlike conventional approaches, the proposed method leverages Enhanced Fuzzy C-Means and vessel segmentation to achieve precise damage detection, thereby overcoming the limitations of traditional threshold-based techniques. For feature extraction, the method combines Mel-frequency cepstral coefficients and Local Binary Patterns to capture both frequency and textural characteristics and improve feature discriminability. Furthermore, Linear Discriminant Analysis optimizes dimensionality reduction, ensuring a compact yet highly informative representation of pavement conditions. The classification stage evaluates multiple approaches, including a Support Vector Machine (SVM) and Artificial Neural Network (ANN) for supervised learning, K-Means for unsupervised learning, and a Convolutional Neural Network (CNN) based on EfficientNetB7. 6464 road images were processed using the proposed methodology. The results show that SVM and ANN achieved F1-Scores above 90% owing to the quality of the extracted features. Additionally, K-Means obtained an F1-Score of 88%, outperforming EfficientNetB7, which achieved 87%, demonstrating the effectiveness of the proposed approach for segmentation and feature extraction. These findings highlight the advantages of integrating frequency-based and texture-based descriptors with advanced segmentation and classification strategies, ultimately contributing to more reliable and scalable pavement damage assessment systems.},
  archive      = {J_ML},
  author       = {Tello-Cifuentes, Lizette and Marulanda, Johannio and Thomson, Peter},
  doi          = {10.1007/s10994-025-06803-3},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Segmentation and feature extraction-based classification of pavement damages using hybrid computer vision and machine learning approaches},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HFIA: A parasitic feature inference attack and gradient-based defense strategy in SplitNN-based vertical federated learning. <em>ML</em>, <em>114</em>(8), 1-35. (<a href='https://doi.org/10.1007/s10994-025-06804-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) is widely adopted in industries like healthcare, enabling collaborators to enhance model performance using disparate data sources. Split Neural Networks (SplitNN) are central to two-party VFL setups, providing enhanced data privacy during collaboration. However, an untrustworthy server owner, referred to as the host, may exploit its position to infer sensitive client-side features during training. Our research introduces Hitchhike Feature Inference Attack (HFIA), where the host leverages a minimal auxiliary dataset (less than 1% of total data) to infer sensitive features with high accuracy (up to 99%) before VFL training is completed. To mitigate this privacy risk, we propose a client-side defense strategy. Clients construct shadow models to simulate the attacker’s approach and introduce gradient-based adversarial noise to local embeddings, significantly reducing feature leakage. Experiments demonstrate that HFIA achieves high attack success rates, while defense method can reduce attack macro_auc to approximately 60%, with minimal impact ( $$<5\%$$ decrease) on the normal VFL task. The defense can reduce attack macro_auc by over 20% and does not impose restrictions on VFL model construction. In practical applications, participants can adopt this approach to effectively mitigate training-time privacy leakage and protect sensitive client-side data from malicious inference.},
  archive      = {J_ML},
  author       = {Dong, Qixuan and Zhou, Boyang and Ru, ZhiQiang and He, Ying and Hua, Jingyu and Zhong, Sheng},
  doi          = {10.1007/s10994-025-06804-2},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-35},
  shortjournal = {Mach. Learn.},
  title        = {HFIA: A parasitic feature inference attack and gradient-based defense strategy in SplitNN-based vertical federated learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MUSO: Achieving exact machine unlearning in over-parameterized regimes. <em>ML</em>, <em>114</em>(8), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06806-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine unlearning (MU) is to make a well-trained model behave as if it had never been trained on specific data. In today’s over-parameterized models, dominated by neural networks, a common approach is to manually relabel data and fine-tune the well-trained model. It can approximate the MU model in the output space, but the question remains whether it can achieve exact MU, i.e., in the parameter space. We answer this question by employing random feature techniques to construct an analytical framework. Under the premise of model optimization via stochastic gradient descent, we theoretically demonstrated that over-parameterized linear models can achieve exact MU through relabeling specific data. We also extend this work to real-world nonlinear networks and propose an alternating optimization algorithm that unifies the tasks of unlearning and relabeling. The algorithm’s effectiveness, confirmed through numerical experiments, highlights its superior performance in unlearning across various scenarios compared to current state-of-the-art methods, particularly excelling over similar relabeling-based MU approaches.},
  archive      = {J_ML},
  author       = {Yang, Ruikai and He, Mingzhen and He, Zhenghao and Qiu, Youmei and Huang, Xiaolin},
  doi          = {10.1007/s10994-025-06806-0},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {MUSO: Achieving exact machine unlearning in over-parameterized regimes},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and interpretable raw audio classification with diagonal state space models. <em>ML</em>, <em>114</em>(8), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06807-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State Space Models have achieved good performance on long sequence modeling tasks such as raw audio classification. Their definition in continuous time allows for discretization and operation of the network at different sampling rates. However, this property has not yet been utilized to decrease the computational demand on a per-layer basis. We propose a family of hardware-friendly S-Edge models with a layer-wise downsampling approach to adjust the temporal resolution between individual layers. Applying existing methods from linear control theory allows us to analyze state/memory dynamics and provides an understanding of how and where to downsample. Evaluated on the Google Speech Command dataset, our autoregressive/causal S-Edge models range from 8–141k parameters at 90–95% test accuracy in comparison to a causal S5 model with 208k parameters at 95.8% test accuracy. Using our C++17 header-only implementation on an ARM Cortex-M4F the largest model requires 103 sec. inference time with 95.19% test accuracy, and the smallest model with 88.01% test accuracy, requires 0.29 sec. Our solutions cover a design space that spans 17x in model size, 358x in inference latency, and 7.18 percentage points in accuracy.},
  archive      = {J_ML},
  author       = {Bittner, Matthias and Schnöll, Daniel and Wess, Matthias and Jantsch, Axel},
  doi          = {10.1007/s10994-025-06807-z},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Efficient and interpretable raw audio classification with diagonal state space models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HE-GAD: A behavior-enhanced contrastive learning framework for graph anomaly detection. <em>ML</em>, <em>114</em>(8), 1-18. (<a href='https://doi.org/10.1007/s10994-025-06809-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective detection of anomalies in graph data is crucial for various applications. Although existing contrastive learning methods have made some progress, they still struggle to handle diverse anomaly types concealed in the complex graph structures and the noises introduced by graph augmentations adopted. In this work, we propose a beHavior-Enhanced contrastive learning framework for Graph Anomaly Detection(HE-GAD) to address these specific challenges. We propose a novel node embedding method that guides the aggregation of node behavioral features using attribute features, maximizing the exploration of information within the graph structure. Furthermore, a new and effective contrastive learning framework is introduced to spare additional effort for devising graph augmentation methods to reduce the possible noises. Extensive evaluations with three real-world datasets demonstrate the effectiveness of our method.},
  archive      = {J_ML},
  author       = {Zheng, Ling and Song, Qi and Wang, Yihan and Wang, Zhitao and Li, Xiangyang},
  doi          = {10.1007/s10994-025-06809-x},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-18},
  shortjournal = {Mach. Learn.},
  title        = {HE-GAD: A behavior-enhanced contrastive learning framework for graph anomaly detection},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All-time safety and sample-efficient meta update for online safe meta reinforcement learning under markov task transition. <em>ML</em>, <em>114</em>(8), 1-28. (<a href='https://doi.org/10.1007/s10994-025-06810-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the issues of ensuring all-time safety and sample-efficient meta update in online safe meta reinforcement learning (MRL) on physical agents (e.g., mobile robots). We propose a novel masked Follow-the-Last-Parameter-Policy (FTLPP) framework, which is composed of a policy masking framework and a sample-efficient online meta update method. The policy masking framework applies a masking function over the learned control policy and ensures all-time safety by suppressing the probability of executing unsafe actions to a sufficiently small value. To enhance sample efficiency, the problem of online update of the meta parameter is transformed into a policy optimization problem, where the tasks are the states and the meta parameters for the next task are the actions, and then is solved using an off-policy reinforcement learning algorithm. We evaluate our method on Frozen Lake, Acrobot, Half Cheetah and Hopper from OpenAI gym and compare it with baseline methods Meta SRL and the variants of FTML and SAILR.},
  archive      = {J_ML},
  author       = {Yuan, Zhenyuan and Xu, Siyuan and Zhu, Minghui},
  doi          = {10.1007/s10994-025-06810-4},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {All-time safety and sample-efficient meta update for online safe meta reinforcement learning under markov task transition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entangle-then-disentangle: A novel approach for enhancing large vision-language model. <em>ML</em>, <em>114</em>(8), 1-28. (<a href='https://doi.org/10.1007/s10994-025-06811-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale foundation models, such as the contrastive language-image pre-training model and the align language model, have shown promising performance on downstream tasks. However, despite their accomplishments, these large-scale foundation models still exhibit limitations in handling certain out-of-distribution downstream tasks, especially in the field of few-shot domain adaptation (FSDA). Advanced works propose prompt learning to overcome the distribution shift. However, the existing methods mainly concentrate on learning universal prompts applicable across available domains, neglecting to learn domain-specific prompts for the target domain already known in FSDA tasks. To fill this gap, we propose a novel learning approach, termed entangle-then-disentangle (EntDi), where each domain is assigned a distinct prompt to model the domain knowledge. The insight is that visual features from two domains, once entangled into a single representation, could be disentangled by leveraging domain-specific knowledge. Specifically, EntDi first entangles visual features from two images of disparate labels and domains. Subsequently, EntDi learns domain-specific prompts by predicting labels of these entangled features, where the labels are contingent on the domain-specific prompt used for prediction. Comprehensive experiments verify the efficacy of the proposed prompt learning approach.},
  archive      = {J_ML},
  author       = {Yuan, Jiajun and Zheng, Haiting and Yu, Hang and Luo, Xiangfeng},
  doi          = {10.1007/s10994-025-06811-3},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Entangle-then-disentangle: A novel approach for enhancing large vision-language model},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Panda: Partially approximate newton methods for distributed minimax optimization with unbalanced dimensions. <em>ML</em>, <em>114</em>(8), 1-40. (<a href='https://doi.org/10.1007/s10994-025-06813-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unbalanced dimensions are crucial characteristics in various minimax optimization problems, such as few-shot learning (Cortes and Mohri in Adv Neural Inf Process Syst 16, 2003; Ying et al. in Adv Neural Inf Process Syst 29, 2016) and fairness-aware machine learning (Lowd and Meek, in: Proceedings of the eleventh ACM SIGKDD international conference on knowledge discovery in data mining, 2005; Zhang et al., in: Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society, 2018). In this paper, we propose a communication-efficient second-order method named PANDA (Partially Approximate Newton methods for Distributed minimAx) to solve problems with unbalanced dimensions. PANDA requires almost the same per-iteration communication cost as the first-order methods by utilizing the special problem structure in its design for data exchange between the client and server. More importantly, it exhibits a superior linear-quadratic convergence rate and significantly reduces the total number of communication rounds through the efficient use of second-order information. We also develop GIANT-PANDA based on the framework of PANDA, which further reduces the computation cost of the latter one by performing sketching operations on each client. Through comprehensive theoretical analysis and empirical evaluations, we demonstrate the superior performance of the proposed methods compared to existing state-of-the-art methods.},
  archive      = {J_ML},
  author       = {Xiao, Minheng and Liu, Chengchang and Chen, Cheng and Lui, John C. S. and Na, Sen},
  doi          = {10.1007/s10994-025-06813-1},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {Panda: Partially approximate newton methods for distributed minimax optimization with unbalanced dimensions},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JANET: Joint adaptive predictioN-region estimation for time-series. <em>ML</em>, <em>114</em>(8), 1-33. (<a href='https://doi.org/10.1007/s10994-025-06812-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction provides machine learning models with prediction sets that offer theoretical guarantees, but the underlying assumption of exchangeability limits its applicability to time series data. Furthermore, existing approaches struggle to handle multi-step ahead prediction tasks, where uncertainty estimates across multiple future time points are crucial. We propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a novel framework for constructing conformal prediction regions that are valid for both univariate and multivariate time series. JANET generalises the inductive conformal framework and efficiently produces joint prediction regions with controlled K-familywise error rates, enabling flexible adaptation to specific application needs. Our empirical evaluation demonstrates JANET’s superior performance in multi-step prediction tasks across diverse time series datasets, highlighting its potential for reliable and interpretable uncertainty quantification in sequential data.},
  archive      = {J_ML},
  author       = {English, Eshant and Wong-Toi, Eliot and Fontana, Matteo and Mandt, Stephan and Smyth, Padhraic and Lippert, Christoph},
  doi          = {10.1007/s10994-025-06812-2},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {JANET: Joint adaptive predictioN-region estimation for time-series},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving graph neural networks through feature importance learning. <em>ML</em>, <em>114</em>(8), 1-23. (<a href='https://doi.org/10.1007/s10994-025-06815-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are among the most widely used methods for node classification in graphs. A common strategy to improve their predictive performance is to enrich nodes with additional features. A weakness of this method is that the set of appropriate features can vary from graph to graph. We address this shortcoming by proposing a novel method. In a preprocessing step, a first GNN is trained on a set of graphs with varying structural properties, using a candidate set of node features fixed in advance. The resulting GNN model is then used to predict the most relevant features from the candidate set for unseen target graphs, which are later processed for node classification. For each target graph, a second GNN is trained on the graph, which is enriched with the node feature vectors calculated for the features selected by the first GNN. A key advantage of the proposed method is that the features are selected without computing the candidate features for the target graph. Our experimental results on synthetic and real-world graphs show that even a few features selected in this way is sufficient to significantly improve the predictive performance of GNNs that use either none or all of the candidate features. Moreover, the time needed to learn the second GNN for the target graph can be reduced by up to two orders of magnitude.},
  archive      = {J_ML},
  author       = {Alkhoury, Fouad and Horváth, Tamás and Bauckhage, Christian and Wrobel, Stefan},
  doi          = {10.1007/s10994-025-06815-z},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Improving graph neural networks through feature importance learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Construction of the kolmogorov-arnold networks using the newton-kaczmarz method. <em>ML</em>, <em>114</em>(8), 1-36. (<a href='https://doi.org/10.1007/s10994-025-06800-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that any continuous multivariate function can be represented exactly by a composition functions of a single variable—the so-called Kolmogorov-Arnold representation. It can be a convenient tool for tasks where it is required to obtain a predictive model that maps some vector input of a black box system into a scalar output. In this case, the representation may not be exact, and it is more correct to refer to such structure as the Kolmogorov-Arnold model (or, as more recently popularised, ‘network’). Construction of such model based on the recorded input–output data is a challenging task. In the present paper, it is suggested to decompose the underlying functions of the representation into continuous basis functions and parameters. It is then proposed to find the parameters using the Newton-Kaczmarz method for solving systems of non-linear equations. The algorithm is then modified to support parallelisation. The paper demonstrates that such approach is also an excellent tool for data-driven solution of partial differential equations. Numerical examples show that for the considered model, the Newton-Kaczmarz method for parameter estimation is efficient and more robust with respect to the section of the initial guess than the straightforward application of the Gauss-Newton method. Finally, the Kolmogorov-Arnold model is compared to the MATLAB’s built-in neural networks on a relatively large-scale problem (25 inputs, datasets of 10 million records), significantly outperforming the multilayer perceptrons in this particular problem (4–10 min vs. 4–8 h of training time, as well as higher accuracy, lower CPU usage, and smaller memory footprint).},
  archive      = {J_ML},
  author       = {Poluektov, Michael and Polar, Andrew},
  doi          = {10.1007/s10994-025-06800-6},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-36},
  shortjournal = {Mach. Learn.},
  title        = {Construction of the kolmogorov-arnold networks using the newton-kaczmarz method},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCE-GNN: A node feature-enhanced graph neural network with pre-clustering strategy. <em>ML</em>, <em>114</em>(8), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06802-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) exhibit excellent performance in extracting node features from graph-structured data. To enhance the representation of central node features and mitigate the over-smoothing issue, several models have refined their methods for acquiring information from distant neighbor nodes. However, most of these methods overlook the impact of distant same-type nodes on the central node and are unable to adequately mine the information contained in these distant neighbor nodes, which limits their performance. To address this, we propose a GNN model with a pre-clustering strategy, called PCE-GNN. Specifically, PCE-GNN enhances node representations through two collaborative modules: the local aggregation module effectively aggregates 1-hop neighbor information via a multi-head graph attention mechanism, while the long-distance similar neighbor aggregation module combines a pre-clustering strategy with GNN layers to utilize reconstructed star-shaped subgraphs for capturing information of distant neighbor nodes with similar features. Subsequently, these two parts of information are integrated via a max-pooling layer to form the final representation of the central node. Experimental results show that the dual-module collaborative approach of PCE-GNN possesses strong feature enhancement capabilities, outperforming baselines in node classification tasks on both public datasets and equipment maintenance datasets. The source code is available at http://github.cn/SanJinCabbage/PCE-GNN.},
  archive      = {J_ML},
  author       = {Li, Yongbo and Xie, Fangfang and Li, Xi and Chen, Kaiyan and Yao, Jiangyi and Li, Xiongwei},
  doi          = {10.1007/s10994-025-06802-4},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {PCE-GNN: A node feature-enhanced graph neural network with pre-clustering strategy},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransFed: Cross-domain feature alignment for semi-supervised federated transfer learning. <em>ML</em>, <em>114</em>(8), 1-30. (<a href='https://doi.org/10.1007/s10994-025-06805-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare institutions often need to collaborate on developing predictive models while adhering to privacy regulations and handling heterogeneous data collection practices. Traditional federated learning approaches assume shared feature spaces or patient populations across institutions, limiting their applicability in real-world healthcare settings where different institutions collect distinct sets of patient data. We propose TransFed, a novel semi-supervised federated transfer learning framework that enables effective collaboration across healthcare institutions with heterogeneous feature spaces. Our framework combines cross-domain feature alignment with semi-supervised learning to leverage both labeled and unlabeled data, while maintaining privacy through federated learning principles. Using two large real-world clinical datasets, we demonstrate that TransFed effectively enables knowledge transfer without requiring direct data sharing or common feature spaces to improve prediction performance across domains and generalizes well to unseen healthcare systems.},
  archive      = {J_ML},
  author       = {Zeng, Linghui and Liu, Ruixuan and Xiong, Li and Ho, Joyce C.},
  doi          = {10.1007/s10994-025-06805-1},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {TransFed: Cross-domain feature alignment for semi-supervised federated transfer learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics encoded blocks in residual neural network architectures for digital twin models. <em>ML</em>, <em>114</em>(8), 1-40. (<a href='https://doi.org/10.1007/s10994-025-06808-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics Informed Machine Learning has emerged as a popular approach for modeling and simulation in digital twins, enabling the generation of accurate models of processes and behaviors in real-world systems. However, existing methods either rely on simple loss regularizations that offer limited physics integration or employ highly specialized architectures that are difficult to generalize across diverse physical systems. This paper presents a generic approach based on a novel physics-encoded residual neural network (PERNN) architecture that seamlessly combines data-driven and physics-based analytical models to overcome these limitations. Our method integrates differentiable physics blocks–implementing mathematical operators from physics-based models–with feed-forward learning blocks, while intermediate residual blocks ensure stable gradient flow during training. Consequently, the model naturally adheres to the underlying physical principles even when prior physics knowledge is incomplete, thereby improving generalizability with low data requirements and reduced model complexity. We investigate our approach in two application domains. The first is a steering model for autonomous vehicles in a simulation environment, and the second is a digital twin for climate modeling using an ordinary differential equation (ODE)-based model of Net Ecosystem Exchange (NEE) to enable gap-filling in flux tower data. In both cases, our method outperforms conventional neural network approaches as well as state-of-the-art Physics Informed Machine Learning methods.},
  archive      = {J_ML},
  author       = {Zia, Muhammad Saad and Houpert, Corentin and Anjum, Ashiq and Liu, Lu and Conway, Anthony and Peña-Rios, Anasol},
  doi          = {10.1007/s10994-025-06808-y},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {Physics encoded blocks in residual neural network architectures for digital twin models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single image inpainting and super-resolution with simultaneous uncertainty guarantees by universal reproducing kernels. <em>ML</em>, <em>114</em>(8), 1-21. (<a href='https://doi.org/10.1007/s10994-025-06814-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a statistical learning approach to the problem of estimating missing pixels of images, crucial for image inpainting and super-resolution problems. One of the main novelties of the method is that it also provides uncertainty quantifications together with the estimated values. Our core assumption is that the underlying data-generating function comes from a reproducing kernel Hilbert space (RKHS). A special emphasis is put on band-limited functions, central to signal processing, which form Paley–Wiener type RKHSs. The proposed method, which we call simultaneously guaranteed kernel interpolation (SGKI), is an extension and refinement of a recently developed kernel method. An advantage of SGKI is that it not only estimates the missing pixels, but also builds non-asymptotic confidence bands for the unobserved values, which are simultaneously guaranteed for all missing pixels. We also show how to compute these bands efficiently using Schur complements, we discuss a generalization to vector-valued functions, and we present a series of numerical experiments on various datasets containing synthetically generated and benchmark images, as well.},
  archive      = {J_ML},
  author       = {Horváth, Bálint and Csáji, Balázs Csanád},
  doi          = {10.1007/s10994-025-06814-0},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {Single image inpainting and super-resolution with simultaneous uncertainty guarantees by universal reproducing kernels},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Community detection via structured adaptive block-diagonal learning with topology-subspace fusion. <em>ML</em>, <em>114</em>(8), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06818-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering can capture the structural features of complex networks. However, existing subspace-based methods encounter challenges related to dependence on priori knowledge, difficulties in exploiting the connection between two stages and addressing networks with fuzzy boundaries. In response to these challenges, a novel subspace clustering-based community detection algorithm, Structured Adaptive Block Diagonal Subspace Learning with Fusion (SABDSLF) is proposed. First, adaptive block diagonal subspace learning strategy is designed. This approach establishes a convex objective function without the need for prior knowledge. Second, structured subspace learning strategy uses a structure matrix to capture the connection between the two stages of the subspace algorithm. Finally, an information fusion strategy is designed to combine topological information and subspace information, enabling the handling of complex networks with fuzzy boundaries. Experiments were conducted on real-world and synthetic networks, demonstrating that SABDSLF outperforms several state-of-the-art community detection methods in terms of precision and robustness.},
  archive      = {J_ML},
  author       = {Wu, Ling and Cai, Ziqi and Yang, Yingjie and Guo, Kun},
  doi          = {10.1007/s10994-025-06818-w},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Community detection via structured adaptive block-diagonal learning with topology-subspace fusion},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Search or split: Policy gradient with adaptive policy space. <em>ML</em>, <em>114</em>(8), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06820-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy search is one of the most effective reinforcement learning classes of methods for solving continuous control tasks. These methodologies attempt to find a good policy for an agent by fixing a family of parametric policies and then searching directly for the parameters that optimize the long-term reward. However, this parametric policy space represents just a subset of all possible Markovian policies, and finding a good parametrization for a given task is a challenging problem in its own right, typically left to human expertise. In this paper, we propose a novel, model-free, adaptive-space policy search algorithm, GAPS (Gradient-based Adaptive Policy Search). We start from a simple policy space; once we have found a good policy within this policy space, based on the observations we receive from the unknown environment, we evaluate the possibility of expanding the policy space. Iterating this process, we obtain a parametric policy whose structure (including the number of parameters) is fitted to the problem at hand without any prior knowledge of the task. Finally, our algorithm is tested on a selection of continuous control tasks, evaluating the learning process with adaptive policy spaces and comparing the results with traditional policy optimization methods that employ a fixed policy space.},
  archive      = {J_ML},
  author       = {Tedeschi, Gianmarco and Papini, Matteo and Metelli, Alberto Maria and Restelli, Marcello},
  doi          = {10.1007/s10994-025-06820-2},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Search or split: Policy gradient with adaptive policy space},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing the effect of residual connections to oversmoothing in graph neural networks. <em>ML</em>, <em>114</em>(8), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06822-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of Graph Neural Networks (GNNs) diminishes as their depth increases. That is mainly attributed to oversmoothing, which leads to similar node representations through repeated graph convolutions. To enable deep GNNs, several approaches have been proposed, among which the use of residual connections. Residual connections have proven effective in benchmark datasets, but the way in which they improve the performance of deep GNNs has not been fully studied. We show that residual connections force the model to focus on the local neighborhood of graph nodes, making the GNN equivalent to the sum of shallow GCNs. We explain theoretically why this is the case and verify the theoretical results experimentally. However, our findings raise the question of whether residual connections are helpful in cases where deep networks are necessary. We assess this experimentally, in two situations: (a) in the presence of the “cold start" problem, i.e. when there is no feature information about unlabeled nodes; and (b) in a new synthetic dataset of controllable long-interactions. These experiments highlight the drawbacks of GNNs using residual connections, while showing that simpler methods can be more effective.},
  archive      = {J_ML},
  author       = {Kelesis, Dimitrios and Fotakis, Dimitris and Paliouras, Georgios},
  doi          = {10.1007/s10994-025-06822-0},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Analyzing the effect of residual connections to oversmoothing in graph neural networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust partial-label learning by leveraging class activation values. <em>ML</em>, <em>114</em>(8), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06796-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world training data is often noisy; for example, human annotators assign conflicting class labels to the same instances. Partial-label learning (PLL) is a weakly supervised learning paradigm that allows training classifiers in this context without manual data cleaning. While state-of-the-art methods have good predictive performance, their predictions are sensitive to high noise levels, out-of-distribution data, and adversarial perturbations. We propose a novel PLL method based on subjective logic, which explicitly represents uncertainty by leveraging the magnitudes of the underlying neural network’s class activation values. Thereby, we effectively incorporate prior knowledge about the class labels by using a novel label weight re-distribution strategy that we prove to be optimal. We empirically show that our method yields more robust predictions in terms of predictive performance under high PLL noise levels, handling out-of-distribution examples, and handling adversarial perturbations on the test instances.},
  archive      = {J_ML},
  author       = {Fuchs, Tobias and Kalinke, Florian},
  doi          = {10.1007/s10994-025-06796-z},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Robust partial-label learning by leveraging class activation values},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perfect counterfactuals in imperfect worlds: Modelling noisy implementation of actions in sequential algorithmic recourse. <em>ML</em>, <em>114</em>(8), 1-35. (<a href='https://doi.org/10.1007/s10994-025-06821-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic recourse suggests actions to individuals who have been adversely affected by automated decision-making, helping them to achieve the desired outcome. Knowing the recourse, however, does not guarantee that users can implement it perfectly, either due to environmental variability or personal choices. Recourse generation should thus anticipate its sub-optimal or noisy implementation. While several approaches construct recourse that is robust to small perturbations – e.g., arising due to its noisy implementation – they assume that the entire recourse is implemented in a single step, thus model the noise as one-off and uniform. But these assumptions are unrealistic since recourse often entails multiple sequential steps, which makes it harder to implement and subject to increasing noise. In this work, we consider recourse under plausible noise that adheres to the local data geometry and accumulates at every step of the way. We frame this problem as a Markov Decision Process and demonstrate that such a distribution of plausible noise satisfies the Markov property. We then propose the RObust SEquential (ROSE) recourse generator for tabular data; our method produces a series of steps leading to the desired outcome even when they are implemented imperfectly. Given plausible modelling of sub-optimal human actions and greater recourse robustness to accumulated uncertainty, ROSE provides users with a high chance of success while maintaining low recourse cost. Empirical evaluation shows that our algorithm effectively navigates the inherent trade-off between recourse robustness and cost while ensuring its sparsity and computational efficiency.},
  archive      = {J_ML},
  author       = {Xuan, Yueqing and Sokol, Kacper and Sanderson, Mark and Chan, Jeffrey},
  doi          = {10.1007/s10994-025-06821-1},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-35},
  shortjournal = {Mach. Learn.},
  title        = {Perfect counterfactuals in imperfect worlds: Modelling noisy implementation of actions in sequential algorithmic recourse},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving collective welfare in multi-agent reinforcement learning via suggestion sharing. <em>ML</em>, <em>114</em>(8), 1-27. (<a href='https://doi.org/10.1007/s10994-025-06823-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human society, the conflict between self-interest and collective well-being often obstructs efforts to achieve shared welfare. Related concepts like the Tragedy of the Commons and Social Dilemmas frequently manifest in our daily lives. As artificial agents increasingly serve as autonomous proxies for humans, we propose a novel multi-agent reinforcement learning (MARL) method to address this issue - learning policies to maximise collective returns even when individual agents’ interests conflict with the collective one. Unlike traditional cooperative MARL solutions that involve sharing rewards, values, and policies or designing intrinsic rewards to encourage agents to learn collectively optimal policies, we propose a novel MARL approach where agents exchange action suggestions. Our method reveals less private information compared to sharing rewards, values, or policies, while enabling effective cooperation without the need to design intrinsic rewards. Our algorithm is supported by our theoretical analysis that establishes a bound on the discrepancy between collective and individual objectives, demonstrating how sharing suggestions can align agents’ behaviours with the collective objective. Experimental results demonstrate that our algorithm performs competitively with baselines that rely on value or policy sharing or intrinsic rewards.},
  archive      = {J_ML},
  author       = {Jin, Yue and Wei, Shuangqing and Montana, Giovanni},
  doi          = {10.1007/s10994-025-06823-z},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Achieving collective welfare in multi-agent reinforcement learning via suggestion sharing},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep latent force models: ODE-based process convolutions for bayesian deep learning. <em>ML</em>, <em>114</em>(8), 1-35. (<a href='https://doi.org/10.1007/s10994-025-06824-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling the behaviour of highly nonlinear dynamical systems with robust uncertainty quantification is a challenging task which typically requires approaches specifically designed to address the problem at hand. We introduce a domain-agnostic model to address this issue termed the deep latent force model (DLFM), a deep Gaussian process with physics-informed kernels at each layer, derived from ordinary differential equations using the framework of process convolutions. Two distinct formulations of the DLFM are presented which utilise weight-space and variational inducing points-based Gaussian process approximations, both of which are amenable to doubly stochastic variational inference. We present empirical evidence of the capability of the DLFM to capture the dynamics present in highly nonlinear real-world multi-output time series data. Additionally, we find that the DLFM is capable of achieving comparable performance to a range of non-physics-informed probabilistic models on benchmark univariate regression tasks. We also empirically assess the negative impact of the inducing points framework on the extrapolation capabilities of LFM-based models.},
  archive      = {J_ML},
  author       = {Baldwin-McDonald, Thomas and Shi, Xinxing and Shen, Mingxin and Álvarez, Mauricio A.},
  doi          = {10.1007/s10994-025-06824-y},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-35},
  shortjournal = {Mach. Learn.},
  title        = {Deep latent force models: ODE-based process convolutions for bayesian deep learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReDMan: Reliable dexterous manipulation with safe reinforcement learning. <em>ML</em>, <em>114</em>(8), 1-41. (<a href='https://doi.org/10.1007/s10994-025-06825-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dexterous hand manipulation is a crucial ability for robots in various applications. However, ensuring safety and reliability during manipulation poses significant challenges. Safe Reinforcement Learning (Safe RL) algorithms are important to ensure robust performance and prevent damage to the robotic hand, manipulated object, or environment. Realistic and complex simulation platforms are needed to develop and evaluate such algorithms. Unfortunately, existing platforms have limitations in terms of realism, complexity, and customizability. To address these issues, we introduce ReDMan, an open-source simulation platform that provides a standardized implementation of safe RL algorithms for Reliable Dexterous Manipulation. ReDMan features challenging tasks based on real-world scenarios that require safety awareness, such as Jenga, as well as multi-modal observations and customizable robotic hardware. This platform facilitates the replication and comparison of experimental results and demonstrates the effectiveness of safe RL methods compared to classical RL algorithms. ReDMan is the first benchmark for safe dexterous manipulation and aims to bridge the gap between safe RL and dexterous manipulation research. The code and demonstration can be found at https://github.com/OmniSafeAI/ReDMan .},
  archive      = {J_ML},
  author       = {Geng, Yiran and Ji, Jiaming and Chen, Yuanpei and Geng, Haoran and Zhong, Fangwei and Yang, Yaodong},
  doi          = {10.1007/s10994-025-06825-x},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-41},
  shortjournal = {Mach. Learn.},
  title        = {ReDMan: Reliable dexterous manipulation with safe reinforcement learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Offline reinforcement learning for learning to dispatch for job shop scheduling. <em>ML</em>, <em>114</em>(8), 1-43. (<a href='https://doi.org/10.1007/s10994-025-06826-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Job Shop Scheduling Problem (JSSP) is a complex combinatorial optimization problem. While online Reinforcement Learning (RL) has shown promise by quickly finding acceptable solutions for JSSP, it faces key limitations: it requires extensive training interactions from scratch leading to sample inefficiency, cannot leverage existing high-quality solutions from traditional methods like Constraint Programming (CP), and require simulated environments to train in, which are impracticable to build for complex scheduling environments. We introduce Offline Learned Dispatching (Offline-LD), an offline reinforcement learning approach for JSSP, which addresses these limitations by learning from historical scheduling data. Our approach is motivated by scenarios where historical scheduling data and expert solutions are available or scenarios where online training of RL approaches with simulated environments is impracticable. Offline-LD introduces maskable variants of two Q-learning methods, namely, Maskable Quantile Regression DQN (mQRDQN) and discrete maskable Soft Actor-Critic (d-mSAC), that are able to learn from historical data, through Conservative Q-Learning (CQL), whereby we present a novel entropy bonus modification for d-mSAC, for maskable action spaces. Moreover, we introduce a novel reward normalization method for JSSP in an offline RL setting. Our experiments demonstrate that Offline-LD outperforms online RL on both generated and benchmark instances when trained on only 100 solutions generated by CP. Notably, introducing noise to the expert dataset yields comparable or superior results to using the expert dataset, with the same amount of instances, a promising finding for real-world applications, where data is inherently noisy and imperfect.},
  archive      = {J_ML},
  author       = {Remmerden, Jesse van and Bukhsh, Zaharah and Zhang, Yingqian},
  doi          = {10.1007/s10994-025-06826-w},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-43},
  shortjournal = {Mach. Learn.},
  title        = {Offline reinforcement learning for learning to dispatch for job shop scheduling},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breaking domain barriers: Mixture of experts for cross-domain fake news detection. <em>ML</em>, <em>114</em>(8), 1-22. (<a href='https://doi.org/10.1007/s10994-025-06827-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media have become a key tool for rapidly spreading information worldwide, amplifying the risks of misinformation and fake news. This is also intensified by the fact that fake news covers a wide range of topics across multiple domains. Machine learning, particularly language models, offers a promising solution for detecting fake news. However, a major limitation of existing methods is their inability to classify instances from new or unseen domains. To tackle this issue, we introduce MERMAID, a mixture of experts approach that leverages the knowledge from different specialized models to classify examples from unknown domains. Each expert is initially trained on a specific known domain and then fine-tuned using data from other known domains. A model merging procedure is then applied to combine related experts, reducing the number of models required for predicting instances from unknown domains. In addition, our approach can effectively be used in few-shot learning scenarios, where a small amount of data from the target/unknown domain is available during training. Experiments on five benchmark datasets demonstrate the effectiveness of our method in both zero-shot and few-shot learning settings.},
  archive      = {J_ML},
  author       = {Liguori, Angelica and Pisani, Francesco Sergio and Comito, Carmela and Guarascio, Massimo and Manco, Giuseppe},
  doi          = {10.1007/s10994-025-06827-9},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-22},
  shortjournal = {Mach. Learn.},
  title        = {Breaking domain barriers: Mixture of experts for cross-domain fake news detection},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated causal inference from observational data. <em>ML</em>, <em>114</em>(8), 1-79. (<a href='https://doi.org/10.1007/s10994-025-06819-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated due to privacy constraints, and differences in data distributions and missing values across them can introduce bias into causal estimands. In this article, we propose a unified framework for estimating causal effects from decentralized observational data without sharing raw data. This contributes to privacy-preserving causal learning across heterogeneous and incomplete environments. The framework includes three novel instances, each tailored to address a distinct challenge in federated causal inference. First, we introduce a Bayesian framework based on Gaussian processes that estimates posterior distributions of causal effects and computes higher-order statistics to quantify uncertainty. Second, we develop an adaptive transfer algorithm that uses Random Fourier Features to learn similarities among data sources and disentangle the loss function into source-specific components–without requiring prior knowledge of similarity metrics. Third, we present a method for federated causal inference from incomplete data, enabling the estimation of causal effects under the missing-at-random assumption while also capturing higher-order uncertainty in the estimands. Together, these components address core limitations of existing approaches, which often handle privacy, heterogeneity, and missingness in isolation. Our framework offers a principled and scalable solution for robust, privacy-aware causal inference across decentralized data landscapes.},
  archive      = {J_ML},
  author       = {Vo, Thanh Vinh and Lee, Young and Leong, Tze-Yun},
  doi          = {10.1007/s10994-025-06819-9},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-79},
  shortjournal = {Mach. Learn.},
  title        = {Federated causal inference from observational data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing the distance between unbalanced distributions: The flat metric. <em>ML</em>, <em>114</em>(8), 1-34. (<a href='https://doi.org/10.1007/s10994-025-06828-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide an implementation to compute the flat metric in any dimension. The flat metric, also called dual bounded Lipschitz distance, generalizes the well-known Wasserstein distance $$W_1$$ to the case that the distributions are of unequal total mass. Thus, our implementation adapts very well to mass differences and uses them to distinguish between different distributions. This is of particular interest for unbalanced optimal transport tasks and for the analysis of data distributions where the sample size is important or normalization is not possible. The core of the method is based on a neural network to determine an optimal test function realizing the distance between two given measures. Special focus was put on achieving comparability of pairwise computed distances from independently trained networks. We tested the quality of the output in several experiments where ground truth was available as well as with simulated data.},
  archive      = {J_ML},
  author       = {Schmidt, Henri and Düll, Christian},
  doi          = {10.1007/s10994-025-06828-8},
  journal      = {Machine Learning},
  month        = {8},
  number       = {8},
  pages        = {1-34},
  shortjournal = {Mach. Learn.},
  title        = {Computing the distance between unbalanced distributions: The flat metric},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning answer set programs with aggregates via sampling and genetic programming. <em>ML</em>, <em>114</em>(7), 1-23. (<a href='https://doi.org/10.1007/s10994-025-06780-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of inductive logic programming is to learn a logic program that models the examples provided as input. The search space of the possible programs is constrained by a language bias, which defines the atoms and literals allowed in rules. Answer set programming is a powerful formalism to represent complex combinatorial domains, also thanks to syntactic constructs such as aggregates. However, learning answer set programs from data is challenging, and often existing tools do not support the specification of aggregates in the language bias. In this paper, we introduce GENTIANS, a tool based on a genetic algorithm to learn answer set programs possibly with aggregates, arithmetic, and comparison operators, from examples. Empirical results, also against an existing solver, show that GENTIANS is able to provide accurate solutions even when the search space contains millions of clauses. Additionally, experiments on noisy datasets show the effectiveness of our approach.},
  archive      = {J_ML},
  author       = {Azzolini, Damiano},
  doi          = {10.1007/s10994-025-06780-7},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Learning answer set programs with aggregates via sampling and genetic programming},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLight: Optimization algorithm for traffic lights based on short-term traffic state forecast. <em>ML</em>, <em>114</em>(7), 1-23. (<a href='https://doi.org/10.1007/s10994-025-06785-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent transportation systems play a crucial role in building smart cities. However, existing traffic signal control systems heavily rely on knowledge of experts and lack real-time adaptability to dynamic traffic states. Most reinforcement learning-based approaches define traffic states using features such as current signal phases and queue lengths in each lane. Unfortunately, these methods overlook the spatiotemporal characteristics of real-world traffic flow and also fail to consider the moving vehicles when designing signal strategies. In this paper, we propose a Short-Term Traffic Flow Forecast Model that captures both temporal and spatial features of traffic flow. Our approach combines Temporal Convolutional Networks for extracting time series features from historical traffic flow, and Long Short-Term Memory networks to model complex temporal dependencies, and obtains more accurate time dependency relationships by assigning weights through self-attention layers. We also use Graph Attention Networks to learn spatial features between intersections and neighboring junctions. In addition, we have proposed an Enhanced Traffic State representation that captures both waiting and moving vehicles. We divide the lane into multiple segments based on the distance to the intersection and count the number of vehicles driving on different segments. In comparison to conventional traffic signal control methods and several reinforcement learning-based algorithms, our proposed algorithm has achieved superior results, especially, the performance can be increased by up to 4.57% on the real-world datasets.},
  archive      = {J_ML},
  author       = {Liu, Daimin and Huang, Jian},
  doi          = {10.1007/s10994-025-06785-2},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {FLight: Optimization algorithm for traffic lights based on short-term traffic state forecast},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rope-net: Deep convolutional neural network via robust principal component analysis. <em>ML</em>, <em>114</em>(7), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06782-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank decomposition methods can compress parameters and accelerate training of deep convolutional neural networks (DCNNs) by mining the low-rankness in weights. However, for an image classification task, the test accuracy of DCNNs compressed by low-rank decomposition usually decreases. Our experiments show that the error term of low-rank decomposition is not sparse and not suitable for reutilizing to maintain accuracy. To overcome this problem, in this paper, we propose an effective compression approach of DCNNs by robust principal component analysis (RPCA) on the weights of convolutional and fully connected layers. Low-rank term of RPCA keeps advantages of low-rank decomposition. Sparse term of RPCA can be retrained via sparse learning to improve test accuracy because of its sparsity. We name this kind of DCNN as Rope-Net (DCNN via Robust Principal Component Analysis). Our Rope-ResNet110 (ResNet110 via RPCA) experiment on CIFAR10 demonstrates that Rope-ResNet110 can achieve: a) 3.3 $$\times$$ parameter compression ratio and 0.33% accuracy improvement, b) 5 $$\times$$ parameter compression ratio without accuracy drop. More remarkably, Rope-VGG16 achieves 8.5 $$\times$$ parameter compression ratio and 5.48% accuracy improvement on randomly cutout CIFAR10 test set compared with VGG16, which validates strong robustness of Rope-Net.},
  archive      = {J_ML},
  author       = {Liu, Baichen and Han, Zhi and Chen, Xi’ai and Wang, Yanmei and Tang, Yandong},
  doi          = {10.1007/s10994-025-06782-5},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Rope-net: Deep convolutional neural network via robust principal component analysis},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison of latent space modeling techniques in a plain-vanilla autoencoder setting. <em>ML</em>, <em>114</em>(7), 1-35. (<a href='https://doi.org/10.1007/s10994-025-06784-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By sampling from the latent space of an autoencoder and decoding the latent space samples to the original data space, any autoencoder can be turned into a generative model. For this to work, it is necessary to model the latent space with a distribution from which samples can be obtained. Several simple possibilities such as kernel density estimates or a Gaussian distribution and more sophisticated ones such as Gaussian mixture models, copula models, and normalization flows can be thought of and have been tried recently. In a plain-vanilla autoencoder setting, this study aims to discuss, assess, and compare various techniques that can be used to capture the latent space so that an autoencoder can become a generative model. Furthermore, we provide insights into further aspects of these methods, such as targeted sampling or synthesizing new data with specific features.},
  archive      = {J_ML},
  author       = {Kächele, Fabian and Coblenz, Maximilian and Grothe, Oliver},
  doi          = {10.1007/s10994-025-06784-3},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-35},
  shortjournal = {Mach. Learn.},
  title        = {A comparison of latent space modeling techniques in a plain-vanilla autoencoder setting},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tight mixed-integer optimization formulations for prescriptive trees. <em>ML</em>, <em>114</em>(7), 1-47. (<a href='https://doi.org/10.1007/s10994-025-06771-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on modeling the relationship between an input feature vector and the predicted outcome of a trained decision tree using mixed-integer optimization. This can be used in many practical applications where a decision tree or a tree ensemble is incorporated into an optimization problem to model the predicted outcomes of a decision. We propose novel tight mixed-integer optimization formulations for this problem. Existing formulations can be shown to have linear relaxations that have fractional extreme points, even for the simple case of modeling a single decision tree or a very large number of constraints, which leads to slow solve times in practice. A formulation we propose, based on a projected union of polyhedra approach, is ideal (i.e., the extreme points of the linear relaxation are integer when required) for a single decision tree. Although the formulation is generally not ideal for tree ensembles, it generally has fewer extreme points, leading to a faster time to solve. We also study formulations with a binary representation of the feature vector and present multiple approaches to tighten existing formulations. We show that fractional extreme points are removed when multiple splits are on the same feature. At an extreme, we prove that this results in an ideal formulation for a tree ensemble modeling a one-dimensional feature vector. Building on this result, we also show that these additional constraints result in significantly tighter linear relaxations when the feature vector is low dimensional.},
  archive      = {J_ML},
  author       = {Biggs, Max and Perakis, Georgia},
  doi          = {10.1007/s10994-025-06771-8},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-47},
  shortjournal = {Mach. Learn.},
  title        = {Tight mixed-integer optimization formulations for prescriptive trees},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constraint guided autoencoders for joint optimization of condition indicator estimation and anomaly detection in machine condition monitoring. <em>ML</em>, <em>114</em>(7), 1-38. (<a href='https://doi.org/10.1007/s10994-025-06779-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal of machine condition monitoring is, as the name implies, to monitor the condition of industrial applications. Effective asset monitoring relies on an accurate assessment of its condition, often represented by a Condition Indicator (CI). The CI should exhibit specific behaviors to ensure its reliability: (i) it should maintain a consistent range, enabling clear differentiation between normal and anomalous data, and (ii) it should demonstrate monotonic behavior over time, reflecting the expected gradual degradation of the asset’s condition. This work proposes an extension to Constraint Guided AutoEncoders (CGAE), which is a robust AD method, that enables building a single model that can estimate a CI that shows the aforementioned behaviors. To improve the monotonic behavior of the CI, the proposed extension incorporates a constraint that enforces this behavior during the training of the model. Experimental results, on two datasets containing run-to-failure data from bearings, indicate that the proposed extension retains the performance of CGAE with regards to CGAE, while also improving the monotonic behavior of the CI. Beyond the improved CI, an additional advantage of the proposed extension is its ability to leverage unlabeled data without requiring assumptions about the data. Furthermore, an ablation study revealed that reconstructing unlabeled data also contributed to enhancing the CI.},
  archive      = {J_ML},
  author       = {Meire, Maarten and Van Baelen, Quinten and Ooijevaar, Ted and Karsmakers, Peter},
  doi          = {10.1007/s10994-025-06779-0},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Constraint guided autoencoders for joint optimization of condition indicator estimation and anomaly detection in machine condition monitoring},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning using statistical invariants: When they work and when they don’t. <em>ML</em>, <em>114</em>(7), 1-19. (<a href='https://doi.org/10.1007/s10994-025-06789-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Learning Using Statistical Invariants (LUSI) method is designed to integrate domain knowledge into machine learning models using structured elements referred to as “predicates.”This approach aims to improve model performance and reduce the amount of data required for training by leveraging the inherent properties of the domain. However, through a thorough theoretical analysis and numerical experiments, we demonstrate that LUSI often falls short of its intended goal. Our findings reveal significant flaws in the current formulation of LUSI; instead of effectively narrowing the set of admissible functions, the predicates act primarily as constraints on the output of the model, which can often lead to underwhelming performance and limited generalization. While LUSI may produce marginal improvements under specific conditions, its overall limitations and inability to consistently deliver intelligence-driven enhancements suggest a need for further refinement. Despite the identified limitations, the introduction of LUSI represents a timely and important step towards integrating domain-specific knowledge into machine learning, highlighting a promising direction for future research and development.},
  archive      = {J_ML},
  author       = {Liu, Yang and Fokoue, Ernest and Krutz, Daniel E.},
  doi          = {10.1007/s10994-025-06789-y},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Learning using statistical invariants: When they work and when they don’t},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph spring neural ODEs for link sign prediction. <em>ML</em>, <em>114</em>(7), 1-19. (<a href='https://doi.org/10.1007/s10994-025-06794-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signed graphs allow for encoding positive and negative relations between nodes and are used to model various online activities. Node representation learning for signed graphs is a well-studied task with important applications such as sign prediction. While the size of datasets is ever-increasing, recent methods often sacrifice scalability for accuracy. We propose a novel message-passing layer architecture called graph spring network (GSN) modeled after spring forces. We combine it with a graph neural ordinary differential equations (ODEs) formalism to optimize the system dynamics in embedding space to solve a downstream prediction task. Once the dynamics is learned, embedding generation for novel datasets is done by solving the ODEs in time using a numerical integration scheme. Our GSN layer leverages the fast-to-compute edge vector directions and learnable scalar functions that only depend on nodes’ distances in latent space to compute the nodes’ positions. Conversely, graph convolution and graph attention network layers rely on learnable vector functions that require the full positions of input nodes in latent space. We propose a specific implementation called spring-neural-network using a set of small neural networks mimicking attracting and repulsing spring forces that we train for link sign prediction. Experiments show that our method achieves accuracy close to the state-of-the-art methods with node generation time speedups factor of up to 28,000 on large graphs.},
  archive      = {J_ML},
  author       = {Rehmann, Andrin and Bovet, Alexandre},
  doi          = {10.1007/s10994-025-06794-1},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Graph spring neural ODEs for link sign prediction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new adaptive gradient method with gradient decomposition. <em>ML</em>, <em>114</em>(7), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06797-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive gradient methods, especially Adam-type methods (such as Adam, AMSGrad, and AdaBound), have been proposed to speed up the training process with an element-wise scaling term on learning rates. However, they often generalize poorly compared with stochastic gradient descent (SGD) and its accelerated schemes such as SGD with momentum (SGDM). In this paper, we propose a new adaptive method called DecGD, which aims at achieving both good generalization like SGDM and rapid convergence like Adam-type methods. In particular, DecGD decomposes the current gradient into the product of two terms including a surrogate gradient and a loss vector. Our method adjusts the learning rates adaptively according to the current loss vector instead of the squared gradients used in Adam-type methods. The intuition for adaptive learning rates of DecGD is that a good optimizer needs to decrease the learning rates as the loss decreases, which is similar to the learning rates decay scheduling technique. Therefore, DecGD gets a rapid convergence by enabling learning rate decay in accordance with the loss vector. Convergence analysis is discussed in both convex and non-convex situations. Finally, empirical results on widely-used tasks demonstrate that DecGD shows better generalization performance than SGDM and offers the same rapid convergence as Adam-type methods.},
  archive      = {J_ML},
  author       = {Shao, Zhou and Zhou, Hang and Lin, Tong},
  doi          = {10.1007/s10994-025-06797-y},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {A new adaptive gradient method with gradient decomposition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards interpretable and robust UAV-based foundation model for endangered species monitoring in complex ecosystems. <em>ML</em>, <em>114</em>(7), 1-31. (<a href='https://doi.org/10.1007/s10994-025-06787-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in foundation models have significantly enhanced the robustness and scalability of traditional methods in a variety of domains. However, their application to specialized ecological environments, where challenges such as data scarcity, camouflage, and environmental noise persist, remains an area requiring further exploration. This study investigates the application of foundation models in species monitoring within complex ecological systems, with a focus on juvenile Tri-spine horseshoe crabs (Tachypleus tridentatus) in Hong Kong’s intertidal zones. Traditional methods for monitoring these endangered species are labor-intensive, imprecise, and disruptive to fragile ecosystems, particularly in environments where juveniles exhibit excellent camouflage and small-scale behavioral markers. Unmanned aerial vehicles (UAVs) offer a promising solution, yet their use in these settings is hampered by tidal movements, water turbidity, and complex backgrounds. To address these challenges, we apply a foundation model, Segment Anything Model 2 (SAM2), to UAV-based high-resolution imagery. By leveraging expert knowledge to design and extract domain-specific features, we fine-tune SAM2 using a few-shot learning strategy, enhancing its ability to accurately segment foraging trails with limited data. The fine-tuned model incorporates interpretable morphological features, such as trail length, width, and continuity, to distinguish biological trails from environmental noise, thereby improving both model robustness and interpretability. This approach demonstrates the efficacy of adapting foundation models for domain-specific challenges, advancing both the interpretability and reliability of ecological monitoring systems. The resulting species distribution maps provide valuable insights into population patterns, offering a scalable and transferable solution for monitoring endangered species in dynamic, data-scarce environments. This research highlights the potential of foundation models to revolutionize ecological monitoring by improving model trustworthiness and extending their application to complex, real-world problems.},
  archive      = {J_ML},
  author       = {Zhang, Jihan and Han, Mingqiao and Laurie, K. H. and Zhao, Benyun and Lei, Lei and Chen, Xi and Wan, Hon Chi Judy and Cheung, Siu Gin and Hong, Wenxing and Chen, Ben M.},
  doi          = {10.1007/s10994-025-06787-0},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-31},
  shortjournal = {Mach. Learn.},
  title        = {Towards interpretable and robust UAV-based foundation model for endangered species monitoring in complex ecosystems},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revealing vulnerable regions through diverse adversarial examples. <em>ML</em>, <em>114</em>(7), 1-31. (<a href='https://doi.org/10.1007/s10994-025-06788-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current explainable AI approaches to Deep Neural Networks (DNNs) primarily aim to understand network behavior by identifying key input features that influence predictions. However, these methods often fail to identify vulnerable regions in the input that are sensitive to minor perturbations and pose significant security risks. The vulnerability of DNNs is typically studied through adversarial examples, but traditional norm-based algorithms, lacking spatial constraints, distribute perturbations across the entire image, obscuring these critical areas. To overcome this limitation, we propose the Vulnerable Region Discovery Attack (VrdAttack), an efficient method that leverages Differential Evolution to generate diverse one-pixel perturbations, enabling the discovery of vulnerable regions and uncovering pixel-level vulnerabilities in Deep Neural Networks (DNNs). Our extensive experiments on CIFAR-10 and ImageNet demonstrate that our proposed VrdAttack outperforms existing methods in identifying diverse critical weak points in an input, highlighting model-specific vulnerabilities, and revealing the impact of adversarial training on these vulnerable regions.},
  archive      = {J_ML},
  author       = {Zhao, Yunce and Huang, Wei and Liu, Wei and Yao, Xin},
  doi          = {10.1007/s10994-025-06788-z},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-31},
  shortjournal = {Mach. Learn.},
  title        = {Revealing vulnerable regions through diverse adversarial examples},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Modeling PU learning using probabilistic logic programming. <em>ML</em>, <em>114</em>(7), 1. (<a href='https://doi.org/10.1007/s10994-025-06790-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Verreet, Victor and Raedt, Luc De and Bekker, Jessa},
  doi          = {10.1007/s10994-025-06790-5},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Modeling PU learning using probabilistic logic programming},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jensen–Tsallis divergence for supervised classification under data imbalance. <em>ML</em>, <em>114</em>(7), 1-16. (<a href='https://doi.org/10.1007/s10994-025-06791-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised classification problems using Deep Neural Networks, the loss function is typically based on the Kullback–Leibler divergence. However, alternative entropic divergence formulations, such as the Jensen–Shannon Divergence (JSD), have recently garnered attention for their unique properties. In this study, we delve deeper into the interpretation of the JSD and its generalized form, the Jensen–Tsallis Divergence (JTD), as alternative loss functions for supervised classification. When provided with one-hot encoded distributions for the true label probabilities, we demonstrate that these novel divergences impose an intrinsic output confidence regularization that prevents overfitting. Additionally, the q non-extensive parameter of the JTD directly influences the structure of the regularizer, offering increased flexibility in the formulation of the loss function. Through experiments conducted on artificially imbalanced versions of MNIST, Fashion-MNIST, SVHN and CIFAR-10 we showcase how JTD outperforms JSD and other traditional loss functions in terms of generalization performance, especially for highly imbalanced datasets.},
  archive      = {J_ML},
  author       = {Squicciarini, Antonio and Trigano, Tom and Luengo, David},
  doi          = {10.1007/s10994-025-06791-4},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-16},
  shortjournal = {Mach. Learn.},
  title        = {Jensen–Tsallis divergence for supervised classification under data imbalance},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cherry on the cake: Fairness is NOT an optimization problem. <em>ML</em>, <em>114</em>(7), 1-43. (<a href='https://doi.org/10.1007/s10994-025-06792-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Fair AI literature, the practice of maliciously creating unfair models that nevertheless satisfy fairness constraints is known as cherry-picking. A cherry-picking model is a model that makes mistakes on purpose, selecting bad individuals from a minority class instead of better candidates from the same minority. The model literally cherry-picks whom to select to superficially meet the fairness constraints while making minimal changes to the unfair model. This practice has been described as “blatantly unfair” and has a negative impact on already marginalized communities, undermining the intended purpose of fairness measures specifically designed to protect these communities. A common assumption is that cherry-picking arises solely from malicious intent and that models designed only to optimize fairness metrics would avoid this behavior. We show that this is not the case: models optimized to minimize fairness metrics while maximizing performance are often forced to cherry-pick to some degree. In other words, cherry-picking might be an inevitable outcome of the optimization process itself. To demonstrate this, we use tools from fair cake-cutting, a mathematical subfield that studies the problem of fairly dividing a resource, referred to as the “cake,” among a number of participants. This concept is connected to supervised multi-label classification: any dataset can be thought of as a cake that needs to be distributed among different labels, and the model is the function that divides the cake. We adapt these classical results for machine learning and demonstrate how this connection can be prolifically used for fairness and classification in general.},
  archive      = {J_ML},
  author       = {Favier, Marco and Calders, Toon},
  doi          = {10.1007/s10994-025-06792-3},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-43},
  shortjournal = {Mach. Learn.},
  title        = {Cherry on the cake: Fairness is NOT an optimization problem},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous outlier detection and elimination in hyperspectral unmixing via weighted non-negative matrix tri-factorization. <em>ML</em>, <em>114</em>(7), 1-35. (<a href='https://doi.org/10.1007/s10994-025-06793-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral unmixing (HU) involves separating mixed pixel spectra into pure endmember spectra and their corresponding abundance fractions. However, it faces significant challenges due to outliers in the hyperspectral data, which often appear as pixel and band anomalies. Outliers in pixels could result in incorrect classification and inaccurate quantification of materials, while outliers in bands could alter spectral characteristics, leading to misidentifying endmembers and incorrect estimates of abundance. To tackle these issues, this paper introduces a new approach, named simultaneous outlier detection and elimination via weighted non-negative matrix tri-factorization (SODE-WNMTF), which offers an efficient means of addressing the impact of outliers in the unmixing process. Leveraging the co-clustering property of NMTF, SODE-WNMTF introduces a novel weighting matrix, which involves simultaneous clustering of both pixels and spectral bands to effectively detect and mitigate the negative impact of both pixel and band outliers during the unmixing process. At the same time, the inherent structure of the hyperspectral image (HSI) is utilized through the examination of local and global connections among pixels and spectral bands, consequently improving the co-clustering procedure. In addition, SODE-WNMTF proposes a spatial weighting factor, which utilizes the similarity of adjacent pixels, to promote piecewise smoothness in abundance maps while mitigating the impact of outliers. Moreover, since pixels in regions dominated by a single endmember exhibit spectra closely resembling that endmember, SODE-WNMTF incorporates a sparse estimation technique for endmember signatures. Finally, to verify the performance of SODE-WNMTF, a series of experiments is conducted on both synthetic and real HSIs, with outcomes proving its superiority against other cutting-edge approaches. The source code is also available at https://github.com/yasinhashemi/SODE-WNMTF .},
  archive      = {J_ML},
  author       = {Hashemi-Nazari, Yasin and Saberi-Movahed, Farid and Tajaddini, Azita and Moreira, Catarina and Ning, Xin and Tiwari, Prayag},
  doi          = {10.1007/s10994-025-06793-2},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-35},
  shortjournal = {Mach. Learn.},
  title        = {Simultaneous outlier detection and elimination in hyperspectral unmixing via weighted non-negative matrix tri-factorization},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The KANDY benchmark: Incremental neuro-symbolic learning and reasoning with kandinsky patterns. <em>ML</em>, <em>114</em>(7), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06798-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence is continuously seeking novel challenges and benchmarks to effectively measure performance and to advance the state-of-the-art. In this paper we introduce KANDY, a benchmarking framework that can be used to generate a variety of learning and reasoning tasks inspired by Kandinsky patterns. By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions, KANDY can be used to implement benchmarks for continual and semi-supervised learning, with a specific focus on symbol compositionality. The ground truth is also augmented with classification rules to enable analysis of interpretable solutions. Together with the benchmark generation pipeline, we release two curricula, an easier and a harder one, that we propose as new challenges for the research community. With a thorough experimental evaluation, we show how state-of-the-art neural models, purely symbolic approaches, and vision language models struggle with solving most of the tasks, thus calling for the application of advanced neuro-symbolic methods trained over time.},
  archive      = {J_ML},
  author       = {Lorello, Luca Salvatore and Lippi, Marco and Melacci, Stefano},
  doi          = {10.1007/s10994-025-06798-x},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {The KANDY benchmark: Incremental neuro-symbolic learning and reasoning with kandinsky patterns},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially-private data synthetisation for efficient re-identification risk control. <em>ML</em>, <em>114</em>(7), 1-20. (<a href='https://doi.org/10.1007/s10994-025-06799-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protecting user data privacy can be achieved via many methods, from statistical transformations to generative models. However, they all have critical drawbacks. For example, creating a transformed data set using traditional techniques is highly time-consuming. Also, recent deep learning-based solutions require significant computational resources in addition to long training phases, and differentially private-based solutions may undermine data utility. In this paper, we propose $$\epsilon$$ -PrivateSMOTE, a technique designed to protect against re-identification and linkage attacks, particularly addressing cases with a high re-identification risk. Our proposal combines synthetic data generation via noise-induced interpolation with differential privacy principles to obfuscate high-risk cases. We demonstrate how $$\epsilon$$ -PrivateSMOTE is capable of achieving competitive results in privacy risk and better predictive performance when compared to multiple traditional and state-of-the-art privacy-preservation methods, including generative adversarial networks, variational autoencoders, and differential privacy baselines. We also show how our method improves time requirements by at least a factor of 9 and is a resource-efficient solution that ensures high performance without specialised hardware.},
  archive      = {J_ML},
  author       = {Carvalho, Tânia and Moniz, Nuno and Antunes, Luís and Chawla, Nitesh},
  doi          = {10.1007/s10994-025-06799-w},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Differentially-private data synthetisation for efficient re-identification risk control},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal multistep-ahead multivariate time-series forecasting. <em>ML</em>, <em>114</em>(7), 1-51. (<a href='https://doi.org/10.1007/s10994-024-06722-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series forecasts underpin decision-making processes in a wide range of application domains. Recently it has been shown that these processes can be strengthened by conformal prediction, a framework that allows adding prediction intervals to point forecasts. The prediction intervals quantify the uncertainty of a predictive model with mathematical coverage guarantees, giving the user a range of scenarios to consider. However, applying conformal prediction to time-series tasks is not trivial. This is either because the exchangeability condition the framework places on the data is violated, or because the framework only allows for one-step-ahead univariate forecasts. In this article we combine two existing methods derived from conformal prediction, one built for multi-target regression and one designed to handle non-exchangeable data. The resulting method, called non-exchangeable multi-target conformal prediction (nmtCP) produces provably robust prediction regions for multi-step ahead multidimensional time-series forecasts, meaning that the miscoverage rate is bound. Additionally, nmtCP is computationally efficient and easy to implement. Due to its model-agnostic nature, nmtCP can be used on top of any time-series model that produces point forecasts. A theoretical analysis proves the method’s robustness while experiments on real-world data sets give insights into its practical behavior and performance.},
  archive      = {J_ML},
  author       = {Schlembach, Filip and Smirnov, Evgueni and Koprinska, Irena and Winands, Mark H. M.},
  doi          = {10.1007/s10994-024-06722-9},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-51},
  shortjournal = {Mach. Learn.},
  title        = {Conformal multistep-ahead multivariate time-series forecasting},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fedflow: A personalized federated learning framework for passenger flow prediction. <em>ML</em>, <em>114</em>(7), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06795-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Intelligent Public Transportation Systems (IPTS) domain, predicting the number of commuters on-board, entering or leaving a metro train or a bus, i.e. the Passenger Flow (PF), is crucial for optimizing resource allocation and enhancing commuter satisfaction. In urban scenarios, the public transport system is often managed by distinct competing mobility providers. Traditional centralized machine learning models for PF prediction usually require data sharing among such competitors, leading to privacy and economic concerns. To overcome these issues, we propose exploiting Federated Learning (FL) in the PF predictions problem, as only model parameters must be shared among entities. Still, a straightforward application of FL can have some pitfalls. On one hand, it is widely recognized that FL can struggle with data heterogeneity, which is likely in the case of data acquired by distinct companies managing different public mobility services. Moreover, spatio-temporal features are not explicitly handled by classical FL. In this paper, we propose FedFlow: a personalized federated learning framework tailored for PF prediction. The proposed framework encompasses a personalized mechanism meant to refine local models based on client similarities, calculated by only leveraging publicly available domain-dependent information. The proposed framework has been experimentally validated on mobility data collected in a major Italian city, comparing FL predictions obtained by FedFlow against those obtained by LSTM models trained on local data, centralized data, FedAvg, and PerFedAvg. Results show that FedFlow outperforms all the considered adversary techniques. This work demonstrates that our proposal of personalized FL is effective in predicting PF while ensuring data privacy.},
  archive      = {J_ML},
  author       = {Rocco di Torrepadula, Franca and Fisichella, Marco and Di Martino, Sergio and Mazzocca, Nicola},
  doi          = {10.1007/s10994-025-06795-0},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Fedflow: A personalized federated learning framework for passenger flow prediction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging differentiable NAS and abstract genetic algorithms for optimizing on-mobile VSR performance. <em>ML</em>, <em>114</em>(7), 1-23. (<a href='https://doi.org/10.1007/s10994-025-06801-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of mobile video applications has for real-time Video Super-Resolution (VSR) technologies, necessitating architectures that simultaneously achieve superior fidelity and ultra-low latency. Existing Neural Architecture Search (NAS) methods achieve efficient performance within fixed topologies and feature dimensionalities, but inherently struggle to meet the flexible design demands of lightweight on-mobile architectures. When applied to on-mobile VSR, their insufficient exploration under real-time constraints results in suboptimal performance, exposing a critical gap in current approaches. To address these challenges, we propose a novel bilevel NAS framework designed to systematically explore diverse topological configurations and feature dimensionalities within an expanded search space. Our framework incorporates a collection of independent differentiable search spaces. At the upper level, we employ an Abstract Genetic Algorithm (AGA) to explore a broad spectrum of topological variations and feature dimensionality configurations. At the lower level, we introduce a latency-aware differentiable NAS method, which facilitates efficient fidelity-latency optimization within each differentiable search space. Additionally, we provide theoretical convergence guarantees for our bilevel NAS framework, ensuring the discovery of optimal architectures. Experimental validation demonstrates that our method achieves state-of-the-art performance, delivering a 5.78ms inference latency while significantly improving fidelity metrics across standard benchmarks. The proposed framework not only breaks the fidelity-latency trade-off but also demonstrates remarkable efficiency in early AGA generations, reducing architecture search iterations from 100 to several generations while maintaining superior results. Our implementation is available at https://github.com/liuxunchenglxc/Reducio .},
  archive      = {J_ML},
  author       = {Liu, Xuncheng and Zhang, Weizhan and Gong, Tieliang and Yan, Caixia and Li, Rui},
  doi          = {10.1007/s10994-025-06801-5},
  journal      = {Machine Learning},
  month        = {7},
  number       = {7},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Leveraging differentiable NAS and abstract genetic algorithms for optimizing on-mobile VSR performance},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-sensitive classification with cost uncertainty: Do we need surrogate losses?. <em>ML</em>, <em>114</em>(6), 1-36. (<a href='https://doi.org/10.1007/s10994-024-06634-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many binary classification applications, the costs of false positives and negatives are imbalanced. Furthermore, there is often uncertainty about the exact costs of these errors. A natural measure-of-interest to be minimised in such scenarios is the expected misclassification cost. We identify many situations where this measure has analytic gradients, and thus it can be used as a training loss and optimised directly using empirical risk minimisation. In particular, we derive such losses from the Beta, Gamma and Gaussian distributions to model different kinds of cost uncertainty. The Beta family includes commonly used losses such as cross-entropy, squared error and 0–1 loss as special cases. The question then arises as to when it is appropriate to directly optimize the measure-of-interest, versus using a standard surrogate like cross-entropy or focal loss during training. After revisiting the theory of surrogate losses, proper losses and cost-sensitive learning to obtain good candidate surrogates out of derived families, we conduct an empirical comparison of derived training losses that, to our knowledge, were never tried on deep neural networks before, with the aim to minimise cost-sensitive measures-of-interest. The findings show that using Beta losses in training leads to improved performance compared to traditional training objectives like cross-entropy, label smoothing, and focal loss. This improvement is seen not only in terms of misclassification cost metrics, but (perhaps surprisingly) also in conventional metrics such as accuracy, mean squared error, and the area under the ROC curve.},
  archive      = {J_ML},
  author       = {Komisarenko, Viacheslav and Kull, Meelis},
  doi          = {10.1007/s10994-024-06634-8},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-36},
  shortjournal = {Mach. Learn.},
  title        = {Cost-sensitive classification with cost uncertainty: Do we need surrogate losses?},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient bayes error rate estimation method. <em>ML</em>, <em>114</em>(6), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06761-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bayes error rate is the lowest error rate that any classifier can achieve, and estimating the Bayes error rate is a generally acknowledged difficult problem in machine learning. Recently, a consistent estimator of the Bayes error rate has been proposed, and the method to calculate this estimator is called BN-BER. Although the consistency of the estimator has been proven, whether the estimator is unbiased remains to be analyzed, because consistency and unbiasedness are two important properties to measure the effectiveness of an estimator. Besides, the time and space complexity of BN-BER are high, resulting in high runnning costs for large-scale data. To address the above issues, we first prove the unbiasedness of the estimator. Subsequently, addressing the high time and space complexity of BN-BER, we propose an approach for estimating Bayes error rates based on hierarchical k-means clustering and approximate k-nearest neighbor algorithm. Through analysis, we found that the time and space complexity of the proposed method is $$O(n(\log _{2}n)^2)$$ , whereas the time and space complexity of the BN-BER method is $$O(n^2)$$ . The effectiveness and efficiency of the proposed method are verified on a large number of synthetic datasets and benchmark datasets.},
  archive      = {J_ML},
  author       = {Chen, Qingqiang and Cao, Fuyuan and Xing, Ying and Liang, Jiye},
  doi          = {10.1007/s10994-025-06761-w},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {An efficient bayes error rate estimation method},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-efficient reinforcement learning by generalized value estimation. <em>ML</em>, <em>114</em>(6), 1-23. (<a href='https://doi.org/10.1007/s10994-025-06763-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A promising way to address sequential decision-making is through reinforcement learning methods. One significant factor hindering the application of reinforcement learning in the real world is its low sample efficiency, primarily due to the large number of trial-and-error samples required for value estimation. Reducing the number of environmental samples necessary to fit the value function has become essential to improve sample efficiency. However, with limited data, overly frequent policy iterations can cause existing value estimation methods to fall into the trap of overestimation. This issue arises from the accumulation of value bias in a single class of bootstrapping paths. To address this, we design a novel state-action value estimation method called generalized Q estimation (QGene) in this paper. QGene is based on a novel generalized Q function, which evaluates the expected cumulative rewards that can be generated after any length of state-action sequences. This newly defined generalized Q function inherently possesses multiple Bellman equations, allowing it to be fitted with diverse targets and generate diversified bootstrapping paths to mitigate the accumulation of value bias. Furthermore, we incorporate a conservative estimation technique to effectively avoid overestimation. Experiments show that QGene can more accurately evaluate the policy in the online setting and significantly improve sample efficiency.},
  archive      = {J_ML},
  author       = {Zhou, Junjie and Tian, Ying and Ren, Minglun},
  doi          = {10.1007/s10994-025-06763-8},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Data-efficient reinforcement learning by generalized value estimation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-resolution human pose estimation and action recognition via pose-driven super-resolution reconstruction. <em>ML</em>, <em>114</em>(6), 1-17. (<a href='https://doi.org/10.1007/s10994-025-06759-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, human pose estimation quality has been greatly improved by deep learning. However, for a tiny human image, the limited information carried by the low-resolution image brings robustness issues to human pose estimation. To increase the amount of information, we introduce super-resolution (SR) reconstruction into human pose estimation. We propose a novel low-resolution human pose estimation method, which effectively combines SR reconstruction and human pose estimation. Different from other SR reconstruction algorithms, our pose-driven SR reconstruction is guided to generate intermediate results conducive to human pose estimation. Moreover, considering that good pose estimation results are crucial to pose-related action recognition, we present a low-resolution human action recognition solution that applies our pose estimation method to pose-related action recognition. Experimental results show that our method can significantly improve the performance of the existing pose estimation and action recognition networks when processing low-resolution images.},
  archive      = {J_ML},
  author       = {Zhang, Zhizhuo and Wan, Lili and Xu, Wanru and Wang, Shenghui},
  doi          = {10.1007/s10994-025-06759-4},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Mach. Learn.},
  title        = {Low-resolution human pose estimation and action recognition via pose-driven super-resolution reconstruction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inductive biases for zero-shot systematic generalization in language-informed reinforcement learning. <em>ML</em>, <em>114</em>(6), 1-32. (<a href='https://doi.org/10.1007/s10994-025-06764-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample efficiency and systematic generalization remain persistent challenges in reinforcement learning. While previous studies demonstrate that incorporating natural language with other observation modalities enhances generalization and sample efficiency due to its compositional and open-ended nature, effectively leveraging these properties requires robust language grounding mechanisms. To address this, we propose Instruction Conditioned MOdular network (ICMO) by introducing language entrance and memory feedback techniques on top of an existing modular and sparse architecture, NPS. The memory feedback mechanism aggregates high-level information, guides selective attention in NPS via attentional feedback, and strengthens the decision-making process in the presence of language guidance. ICMO achieves superior performance compared to previous methods during our rigorous experiments, demonstrating near-zero generalization gap that highlights its robustness. Additionally, an extensive ablation study confirms the contributions of these techniques to improving generalization, sample efficiency, and training stability.},
  archive      = {J_ML},
  author       = {Dijujin, Negin Hashemi and Rohani, Seyed Roozbeh Razavi and Samiei, Mohammad Mahdi and Baghshah, Mahdieh Soleymani},
  doi          = {10.1007/s10994-025-06764-7},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-32},
  shortjournal = {Mach. Learn.},
  title        = {Inductive biases for zero-shot systematic generalization in language-informed reinforcement learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing safe and responsible large language model: Can we balance bias reduction and language understanding?. <em>ML</em>, <em>114</em>(6), 1-41. (<a href='https://doi.org/10.1007/s10994-025-06767-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have advanced various Natural Language Processing (NLP) tasks, such as text generation and translation, among others. However, these models often generate texts that can perpetuate biases. Existing approaches to mitigate these biases usually compromise knowledge retention. This study explores whether LLMs can produce safe, unbiased outputs without sacrificing knowledge or comprehension. We introduce the Safe and Responsible Large Language Model (SR $$_{\text {LLM}}$$ ), which has been instruction fine-tuned atop of a safe fine-tuned auto-regressive decoder-only LLM to reduce biases in generated texts. We developed a specialized dataset with examples of unsafe and corresponding safe variations to train SR $$_{\text {LLM}}$$ to identify and correct biased text. Experiments on our specialized dataset and out-of-distribution test sets reveal that SR $$_{\text {LLM}}$$ effectively reduces biases while preserving knowledge integrity. This performance surpasses that of traditional fine-tuning of smaller language models and base LLMs that merely reply on prompting techniques. Our findings demonstrate that instruction fine-tuning on custom datasets tailored for tasks such as debiasing is a highly effective strategy for minimizing bias in LLM while preserving their inherent knowledge and capabilities. The code and dataset are accessible at https://github.com/shainarazavi/Safe-Responsible-LLM .},
  archive      = {J_ML},
  author       = {Raza, Shaina and Bamgbose, Oluwanifemi and Ghuge, Shardul and Tavakoli, Fatemeh and Reji, Deepak John and Bashir, Syed Raza},
  doi          = {10.1007/s10994-025-06767-4},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-41},
  shortjournal = {Mach. Learn.},
  title        = {Developing safe and responsible large language model: Can we balance bias reduction and language understanding?},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drop-in efficient self-attention approximation method. <em>ML</em>, <em>114</em>(6), 1-20. (<a href='https://doi.org/10.1007/s10994-025-06768-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have achieved state-of-the-art performance in most common tasks to which they have been applied. Those achievements are attributed to the Self-Attention mechanism at their core. Self-Attention is understood to map the relationship between tokens of any given sequence. This exhaustive mapping incurs massive costs in memory and inference time, as Self-Attention scales quadratically with regard to sequence length. Standard Self-Attention has required increasingly large compute and memory usage when applied to long input sequences because of this memory and time bottleneck. Efficient Transformers emerged as performant alternatives demonstrating good scalability and occasionally better tracking of long-range dependencies. Their efficiency gains are obtained through different methods, usually focusing on the linear scaling of the attention matrix through sparsification, approximation, or other methods. Among existing approaches, those using low-rank approximation present particular advantages because of their compatibility with standard Self-Attention-based models, allowing for weight transfers and other time-saving schemes. More recently, hardware-aware versions of Self-Attention (e.g., FlashAttention) have mitigated all memory bottlenecks and have alleviated its compute burden. Unfortunately, hardware-aware Self-Attentions have stricter hardware compatibility requirements making Efficient Transformers still relevant for use on older or less powerful hardware. Furthermore, some Efficient Transformers can even be applied in an hardware-aware manner to further improve training and inference speed. In this paper, we propose a novel linear approximation method for Self-Attention inspired by the CUR approximation method. This method, proposed in two versions (one leveraging FlashAttention), is conceived as a drop-in replacement for standard Self-Attention with weights compatibility. Our method compares favorably to standard Transformers’ and Efficient Transformers’ performances on varied tasks and demonstrates a significant decrease in memory footprint as well as competitive performance in training speed, even compared to similar methods.},
  archive      = {J_ML},
  author       = {François, Damien and Saillot, Mathis and Klein, Jacques and Bissyandé, Tegawendé F. and Skupin, Alexander},
  doi          = {10.1007/s10994-025-06768-3},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Drop-in efficient self-attention approximation method},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improve global generalization for personalized federated learning within a stackelberg game. <em>ML</em>, <em>114</em>(6), 1-35. (<a href='https://doi.org/10.1007/s10994-025-06770-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining the personalization of local models under unique local data has driven the development of various personalized federated learning approaches. However, these methods often neglect the inference and generalization capabilities of the aggregated global model. Ensuring optimal local personalization with enhanced global generalization remains a significant challenge. In this work, we introduce personalized federated learning within a Stackelberg strategy, a novel personalized federated learning framework aimed at improving global generalization in personalized federated learning. We formulate federated learning as a Stackelberg game and leverage the soft actor-critic reinforcement learning method to explore the optimal global model, considering the dynamic changes in model parameters and feature outputs.},
  archive      = {J_ML},
  author       = {Xie, Wei and Xiong, Runqun and Luo, Junzhou},
  doi          = {10.1007/s10994-025-06770-9},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Mach. Learn.},
  title        = {Improve global generalization for personalized federated learning within a stackelberg game},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An in-depth review and analysis of mode collapse in generative adversarial networks. <em>ML</em>, <em>114</em>(6), 1-45. (<a href='https://doi.org/10.1007/s10994-025-06772-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) are a type of generative modeling that utilizes deep learning techniques to generate highly realistic synthetic data. Since their introduction in 2014, GANs have gained significant attention across various fields for their ability to generate synthetic data, which can be utilized for training machine learning models. Although GANs have demonstrated extensive efficacy, they still face several constraints, one of which is the issue of mode collapse. Mode collapse occurs in GANs when the generator model fails to produce a diverse range of outputs that accurately capture every aspect of diversity present in the real data distribution, which reduces the efficacy of the synthetic data. This research aims to provide an overview of the mode collapse issue in GANs. First, we investigate the causes of mode collapse. Second, we provide an overview of the progression and status of the mode collapse issue across various GAN variants over the years. Third, we identify the gaps and shortcomings in existing mode collapse mitigation approaches. Finally, we present some potential research directions to effectively handle the issue.},
  archive      = {J_ML},
  author       = {Barsha, Farhat Lamia and Eberle, William},
  doi          = {10.1007/s10994-025-06772-7},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-45},
  shortjournal = {Mach. Learn.},
  title        = {An in-depth review and analysis of mode collapse in generative adversarial networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group feature selection using non-class data. <em>ML</em>, <em>114</em>(6), 1-23. (<a href='https://doi.org/10.1007/s10994-025-06773-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing embedded feature selection methods barely let non-class data contribute to feature selection. However, in some learning tasks, when non-class data have contribution to classification, they should also have an influence to the selection of useful features. For instance, $$F_\infty$$ -norm support vector machine is an effective embedded group feature selection method that performs classification simultaneously. In this paper, we find out that it implicitly uses a kind of non-class data formulated as coordinate Universum when implementing group feature selection, and the information contained in this non-class data could be a meaningful group-wise $$F_{\infty }$$ -norm penalization. As far as we know, this is the first time that $$F_{\infty }$$ -norm penalization is understood from this angle. We prove that useful features can be identified through this non-class data that contribute to classifier construction. In addition, to fully explore the classification information provided by this non-class data, we improve $$F_\infty$$ -norm support vector machine by deeming the non-class data as a middle class to better classify positive and negative classes. Experiments show that the non-class data in the proposed method help reduce the labelled data in some sense. Furthermore, it improves $$F_\infty$$ -norm support vector machine in terms of both classification and group feature selection.},
  archive      = {J_ML},
  author       = {Li, Chunna and Pan, Yuangang and Chen, Weijie and Tsang, Ivor W. and Shao, Yuanhai},
  doi          = {10.1007/s10994-025-06773-6},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Group feature selection using non-class data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gentle local robustness implies generalization. <em>ML</em>, <em>114</em>(6), 1-32. (<a href='https://doi.org/10.1007/s10994-025-06775-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robustness and generalization ability of machine learning models are of utmost importance in various application domains. There is a wide interest in efficient ways to analyze those properties. One important direction is to analyze connection between those two properties. Prior theories suggest that a robust learning algorithm can produce trained models with a high generalization ability. However, we show in this work that the existing error bounds are vacuous for the Bayes optimal classifier which is the best among all measurable classifiers for a classification problem with overlapping classes. Those bounds cannot converge to the true error of this ideal classifier. This is undesirable, surprizing, and never known before. We then present a class of novel bounds, which are model-dependent and provably tighter than the existing robustness-based ones. Unlike prior ones, our bounds are guaranteed to converge to the true error of the best classifier, as the number of samples increases. We further provide an extensive experiment and find that two of our bounds are often non-vacuous for a large class of deep neural networks, pretrained from ImageNet.},
  archive      = {J_ML},
  author       = {Than, Khoat and Phan, Dat and Vu, Giang},
  doi          = {10.1007/s10994-025-06775-4},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-32},
  shortjournal = {Mach. Learn.},
  title        = {Gentle local robustness implies generalization},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imbalanced regression pipeline recommendation. <em>ML</em>, <em>114</em>(6), 1-48. (<a href='https://doi.org/10.1007/s10994-025-06766-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced problems are prevalent in various real-world scenarios and are extensively explored in classification tasks. However, they also present challenges for regression tasks due to the rarity of certain target values. A common alternative is to employ balancing algorithms in preprocessing to address dataset imbalance. However, due to the variety of resampling methods and learning models, determining the optimal solution requires testing many combinations. Furthermore, the learning model, dataset, and evaluation metric affect the best strategies. This work proposes the Meta-learning for Imbalanced Regression (Meta-IR) framework, which diverges from existing literature by training meta-classifiers to recommend the best pipeline composed of the resampling strategy and learning model per task in a zero-shot fashion. The meta-classifiers are trained using a set of meta-features to learn how to map the meta-features to the classes indicating the best pipeline. We propose two formulations: Independent and Chained. Independent trains the meta-classifiers to separately indicate the best learning algorithm and resampling strategy. Chained involves a sequential procedure where the output of one meta-classifier is used as input for another to model intrinsic relationship factors. The Chained scenario showed superior performance, suggesting a relationship between the learning algorithm and the resampling strategy per task. Compared with AutoML frameworks, Meta-IR obtained better results. Moreover, compared with baselines of six learning algorithms and six resampling algorithms plus no resampling, totaling 42 (6 $$\times$$ 7) configurations, Meta-IR outperformed all of them. The code, data, and further information of the experiments can be found on GitHub: https://github.com/JusciAvelino/Meta-IR .},
  archive      = {J_ML},
  author       = {Avelino, Juscimara G. and Cavalcanti, George D. C. and Cruz, Rafael M. O.},
  doi          = {10.1007/s10994-025-06766-5},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-48},
  shortjournal = {Mach. Learn.},
  title        = {Imbalanced regression pipeline recommendation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating explanatory multiverse through counterfactual path geometry. <em>ML</em>, <em>114</em>(6), 1-33. (<a href='https://doi.org/10.1007/s10994-025-06769-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to technical and domain-specific constraints that aim to maximise their real-life utility. In addition to considering desiderata pertaining to the counterfactual instance itself, guaranteeing existence of a viable path connecting it with the factual data point has recently gained relevance. While current explainability approaches ensure that the steps of such a journey as well as its destination adhere to selected constraints, they neglect the multiplicity of these counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys. We define it using vector spaces, showing how to navigate, reason about and compare the geometry of counterfactual trajectories found within it. To this end, we overview their spatial properties–such as affinity, branching, divergence and possible future convergence–and propose an all-in-one metric, called opportunity potential, to quantify them. Notably, the explanatory process offered by our method grants explainees more agency by allowing them to select counterfactuals not only based on their absolute differences but also according to the properties of their connecting paths. To demonstrate real-life flexibility, benefit and efficacy of explanatory multiverse we propose its graph-based implementation, which we use for qualitative and quantitative evaluation on six tabular and image data sets.},
  archive      = {J_ML},
  author       = {Sokol, Kacper and Small, Edward and Xuan, Yueqing},
  doi          = {10.1007/s10994-025-06769-2},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Navigating explanatory multiverse through counterfactual path geometry},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heat transfer flow of non-newtonian eyring-powell fluid with mixed convection heterogeneous and homogeneous reactions using linear regression based machine learning approach. <em>ML</em>, <em>114</em>(6), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06774-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research examines the heat transfer and flow behavior of non-Newtonian Eyring-Powell fluids under mixed convection and reactive conditions employing a supervised machine learning-based linear regression methodology. The most appropriate transformations are utilized to convert non-linear coupled partial differential equations into an ordinary differential equations. The model predicts thermal behavior on velocity, energy, and concentration boundary layers for four scenarios. To validate the reliability and accuracy of the supervised machine learning model, a comparison and performance evaluation are conducted by using Mean Squared Error (MSE), error histograms, and fitness curves. The average squared difference (MSE) between the observed and predicted values is calculated for the training, validation, and test sets. An error histogram displays the distribution of prediction errors across different sets to aid in identifying biases or inconsistent model performance. It is seen that the higher model performance appears by lower MSE values. It is also observed that the differences in predictions between machine learning and MATLAB methods indicate that machine learning (ML) can better handle a large variety of complex data. It is also concluded that the machine learning method surpass traditional computational techniques, especially in accuracy and efficiency.},
  archive      = {J_ML},
  author       = {Ellahi, R. and Khalid, N. and Zeeshan, A. and Sait, Sadiq M. and Khan, M. I.},
  doi          = {10.1007/s10994-025-06774-5},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Heat transfer flow of non-newtonian eyring-powell fluid with mixed convection heterogeneous and homogeneous reactions using linear regression based machine learning approach},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prioritizing the essential: A robust evaluation framework for novelty detection. <em>ML</em>, <em>114</em>(6), 1-29. (<a href='https://doi.org/10.1007/s10994-025-06777-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to traditional machine learning methods that presume static data distributions, novelty detection tackles the challenge of identifying novel classes within continuous data streams as they evolve over time. The inherent implementation challenges of novelty detection are compounded by the difficulty of correctly assessing its performance, as it can be highly sensitive to data characteristics and necessitate specialized metrics to take the temporal dimension into account. Consequently, the absence of a consensus on how to properly evaluate these approaches persists as a significant challenge. In this study, we propose and formalize a comprehensive evaluation framework that aims to provide a fair assessment of novelty detection algorithms. Specifically, we propose a list of existing and novel metrics to accurately evaluate all aspects of novelty detection on data streams, including their temporal aspect. We provide a list of data characteristics that impact the performance of these algorithms and show how to report them. We empirically demonstrate the effect of these characteristics on various datasets and compare our proposed metrics to other previously used ones.},
  archive      = {J_ML},
  author       = {Gaudreault, Jean-Gabriel and Branco, Paula},
  doi          = {10.1007/s10994-025-06777-2},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Prioritizing the essential: A robust evaluation framework for novelty detection},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One transformer for all time series: Representing and training with time-dependent heterogeneous tabular data. <em>ML</em>, <em>114</em>(6), 1-27. (<a href='https://doi.org/10.1007/s10994-025-06778-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a recent growing interest in applying Deep Learning techniques to tabular data in order to replicate the success of other Artificial Intelligence areas in this structured domain. Particularly interesting is the case in which tabular data have a time dependence, such as, for instance, financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical features, makes this adaptation difficult. In this paper we propose UniTTab, a Transformer based architecture whose goal is to uniformly represent heterogeneous time-dependent tabular data, in which both numerical and categorical features are described using continuous embedding vectors. Moreover, differently from common approaches, which use a combination of different loss functions for training with both numerical and categorical targets, UniTTab is uniformly trained with a unique Masked Token pretext task. Finally, UniTTab can also represent time series in which the individual row components have a variable internal structure with a variable number of fields, which is a common situation in many application domains, such as in real world transactional data. Using extensive experiments with five datasets of variable size and complexity, we empirically show that UniTTab consistently and significantly improves the prediction accuracy over several downstream tasks and with respect to both Deep Learning and more standard Machine Learning approaches. Our code and our models are available at: https://github.com/fabriziogaruti/UniTTab .},
  archive      = {J_ML},
  author       = {Luetto, Simone and Garuti, Fabrizio and Sangineto, Enver and Forni, Lorenzo and Cucchiara, Rita},
  doi          = {10.1007/s10994-025-06778-1},
  journal      = {Machine Learning},
  month        = {6},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {One transformer for all time series: Representing and training with time-dependent heterogeneous tabular data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimum discrepancy principle strategy for choosing k in k-NN regression. <em>ML</em>, <em>114</em>(5), 1-33. (<a href='https://doi.org/10.1007/s10994-024-06645-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel data-driven strategy to choose the hyperparameter k in the k-NN regression estimator without using any hold-out data. We treat the problem of choosing the hyperparameter as an iterative procedure (over k) and propose using an easily implemented in practice strategy based on the idea of early stopping and the minimum discrepancy principle. This model selection strategy is proven to be minimax-optimal, under the fixed-design assumption on covariates, over some smoothness function classes, for instance, the Lipschitz functions class on a bounded domain. The novel method often improves statistical performance on artificial and real-world data sets in comparison to other model selection strategies, such as the Hold-out method, 5–fold cross-validation, and AIC criterion. The novelty of the strategy comes from reducing the computational time of the model selection procedure while preserving the statistical (minimax) optimality of the resulting estimator. More precisely, given a sample of size n, if one should choose k among $$\left\{ 1, \ldots , n \right\}$$ , and $$\left\{ f^1, \ldots , f^n \right\}$$ are the estimators of the regression function, the minimum discrepancy principle requires calculation of a fraction of the estimators, while this is not the case for the generalized cross-validation, Akaike’s AIC criteria or Lepskii principle.},
  archive      = {J_ML},
  author       = {Averyanov, Yaroslav and Celisse, Alain},
  doi          = {10.1007/s10994-024-06645-5},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Minimum discrepancy principle strategy for choosing k in k-NN regression},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Theoretical guarantees for domain adaptation with hierarchical optimal transport. <em>ML</em>, <em>114</em>(5), 1-27. (<a href='https://doi.org/10.1007/s10994-025-06749-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation arises as an important problem in statistical learning theory, arising when the data-generating processes differ between the training and test samples, respectively called source and target domains. Recent theoretical advances have demonstrated that the success of domain adaptation algorithms heavily relies on their ability to minimize the divergence between the probability distributions of the source and target domains. However, minimizing this divergence cannot be achieved independently of other key ingredients, such as the source risk or the combined error of the ideal joint hypothesis. The trade-off between these terms is often ensured through algorithmic solutions that remain implicit and are not directly reflected by the theoretical guarantees. To get to the bottom of this issue, we propose in this paper a new theoretical framework for domain adaptation through hierarchical optimal transport. This framework provides more explicit generalization bounds and enables us to consider the natural hierarchical organization of samples in both domains into structures, i.e. classes or clusters. Additionally, we provide a new divergence measure between the source and target domains called Hierarchical Wasserstein distance that indicates under mild assumptions, which structures need to be aligned to achieve successful adaptation.},
  archive      = {J_ML},
  author       = {El Hamri, Mourad and Bennani, Younès and Falih, Issam},
  doi          = {10.1007/s10994-025-06749-6},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Theoretical guarantees for domain adaptation with hierarchical optimal transport},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic instance dependent label refinement for noisy label learning. <em>ML</em>, <em>114</em>(5), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06668-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label refinement methods are designed to improve the quality of training labels by incorporating model predictions into the original training labels. By adjusting the combination coefficient of the noisy label, the impact of noise is reduced, which in turn makes the training process more robust. However, previous label refinement methods are unable to model instance-dependent noise, which is the most realistic type of noise. To address this limitation, we propose a simple approach, probabilistic instance-dependent label refinement (referred to as $$\pi$$ -LR). Inspired by the fact that humans are more likely to make mistakes when annotating confusing instances, we propose to estimate the probability of whether a sample is confusing, which can be useful for modeling noise generation. Our approach exploits this concept by assigning a confusing probability $$\eta _i$$ to each instance $$\varvec{x}_i$$ from a probabilistic perspective. This provides a clear understanding of how instance-dependent noise affects true labels. Empirical evaluations show that $$\pi$$ -LR improves the robustness of the model in the presence of label noise and outperforms all compared methods on both realistic and synthetic label noise, while maintaining high efficiency in time and space.},
  archive      = {J_ML},
  author       = {He, Hao-Yuan and Liu, Yu and Liu, Ren-Biao and Xie, Zheng and Li, Ming},
  doi          = {10.1007/s10994-024-06668-y},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Probabilistic instance dependent label refinement for noisy label learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gender disentangled representation learning in neural rankers. <em>ML</em>, <em>114</em>(5), 1-33. (<a href='https://doi.org/10.1007/s10994-024-06664-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated that while neural ranking methods excel in retrieval effectiveness, they also tend to amplify stereotypical biases, especially those related to gender. Current mitigation strategies often focus on adjusting training methods, like adversarial techniques or data balancing, but typically overlook explicit consideration of gender as an attribute. In this paper, we introduce a systematic approach that treats gender as a distinct component within neural ranker representations. Our neural disentanglement method separates content semantics from gender information, enabling the neural ranker to evaluate document relevance based on content alone, without the interference of gender-related information during retrieval. Our extensive experiments demonstrate that: (1) our disentanglement approach matches the effectiveness of baseline models and offers more consistent performance across queries of different gender affiliations; (2) isolating gender within the representations allows the neural ranker to produce an unbiased list of documents, not favoring any specific gender; and (3) the disentangled gender component effectively and concisely captures gender information independently from the semantic content.},
  archive      = {J_ML},
  author       = {Seyedsalehi, Shirin and Salamat, Sara and Arabzadeh, Negar and Ebrahimi, Sajad and Zihayat, Morteza and Bagheri, Ebrahim},
  doi          = {10.1007/s10994-024-06664-2},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Gender disentangled representation learning in neural rankers},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Conformal load prediction with transductive graph autoencoders. <em>ML</em>, <em>114</em>(5), 1. (<a href='https://doi.org/10.1007/s10994-025-06762-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Luo, Rui and Colombo, Nicolo},
  doi          = {10.1007/s10994-025-06762-9},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Conformal load prediction with transductive graph autoencoders},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced route planning with calibrated uncertainty set. <em>ML</em>, <em>114</em>(5), 1-16. (<a href='https://doi.org/10.1007/s10994-024-06697-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the application of probabilistic prediction methodologies in route planning within a road network context. Specifically, we introduce the Conformalized Quantile Regression for Graph Autoencoders (CQR-GAE), which leverages the conformal prediction technique to offer a coverage guarantee, thus improving the reliability and robustness of our predictions. By incorporating uncertainty sets derived from CQR-GAE, we substantially improve the decision-making process in route planning under a robust optimization framework. We demonstrate the effectiveness of our approach by applying the CQR-GAE model to a real-world traffic scenario. The results indicate that our model significantly outperforms baseline methods, offering a promising avenue for advancing intelligent transportation systems.},
  archive      = {J_ML},
  author       = {Tang, Lingxuan and Luo, Rui and Zhou, Zhixin and Colombo, Nicolo},
  doi          = {10.1007/s10994-024-06697-7},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Learn.},
  title        = {Enhanced route planning with calibrated uncertainty set},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online dimensionality reduction through stacked generalization of spectral methods with deep networks. <em>ML</em>, <em>114</em>(5), 1-40. (<a href='https://doi.org/10.1007/s10994-024-06715-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing large volumes of high-dimensional data poses significant challenges. Dimensionality reduction aims to reveal the most prominent properties of data by embedding them into a low-dimensional representation. Spectral dimensionality reduction methods using kernel matrices have been proven to yield optimal results. Online versions of those methods are desirable to incrementally project new data without recomputing the whole embedding from the complete dataset. In addition, integrating different spectral methods may have a synergistic effect. This paper presents an online dimensionality reduction method based on deep neural networks that integrates embeddings optimized by statistical approximation of neighborhoods and induced by different spectral methods through stacking ensemble learning. In particular, the proposed method first applies a self-supervised stage in order to train a set of deep encoders based on the embeddings induced by different spectral methods applied to a given input dataset. Those basis encoders are optimized and then integrated through a metamodel constituted by a fully connected network. A supervised and an unsupervised approach have been designed depending on whether the final aim is to enforce topological preservation or cluster induction. The proposed method has been experimentally validated on well-known image datasets and compared to some of the most relevant dimensionality reduction techniques by using widely-used quality measures.},
  archive      = {J_ML},
  author       = {Alvarado-Pérez, Juan Carlos and Garcia, Miguel Angel and Puig, Domènec},
  doi          = {10.1007/s10994-024-06715-8},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {Online dimensionality reduction through stacked generalization of spectral methods with deep networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural RELAGGS. <em>ML</em>, <em>114</em>(5), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06753-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-relational databases are the basis of most consolidated data collections in science and industry today. Most learning and mining algorithms, however, require data to be represented in a propositional form. While there is a variety of specialized machine learning algorithms that can operate directly on multi-relational data sets, propositionalization algorithms transform multi-relational databases into propositional data sets, thereby allowing the application of traditional machine learning and data mining algorithms without their modification. One prominent propositionalization algorithm is RELAGGS by Krogel and Wrobel, which transforms the data by nested aggregations. We propose a new neural network based algorithm in the spirit of RELAGGS that employs trainable composite aggregate functions instead of the static aggregate functions used in the original approach. In this way, we can jointly train the propositionalization with the prediction model, or, alternatively, use the learned aggegrations as embeddings in other algorithms. We demonstrate the increased predictive performance by comparing N-RELAGGS with RELAGGS and multiple other state-of-the-art algorithms.},
  archive      = {J_ML},
  author       = {Pensel, Lukas and Kramer, Stefan},
  doi          = {10.1007/s10994-025-06753-w},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {Neural RELAGGS},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DatRel: A noise-tolerant data relocation approach for effective synthetic data generation in imbalanced classifiers. <em>ML</em>, <em>114</em>(5), 1-45. (<a href='https://doi.org/10.1007/s10994-025-06755-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most machine learning algorithms tend to bias towards the majority class when a dataset exhibits a skewed distribution in the class variable. This is called the class imbalance problem and is frequently encountered in real-life applications. One of the most prevalent methods for addressing class imbalance is data resampling, which generates or removes samples to balance the dataset. A well-known issue with oversampling is noise generation. Noise removal or hybrid resampling is used to deal with noise. However, these methods cause imbalance to re-emerge. In this study, a data relocation approach named DatRel is proposed to address the noise generation problem of oversampling without causing imbalance. The proposed approach utilizes pure and proper class cover catch digraphs (P-CCCD) to determine dominant points and cover areas for minority class. Then, new samples from oversampling are drawn to the dominant points until they are covered. This process ensures that newly generated samples never overlap with a negative sample. Imbalance is not affected since no sample is removed by undersampling. The proposed DatRel approach is applied to commonly used oversampling methods, namely SMOTE, ADASYN, and BLSMOTE. Moreover, the performance of the DatRel approach is compared to noise filtering methods such as Tomeklink, ENN, NEATER, and NearMiss after SMOTE. Several baseline classification algorithms are employed, and comparisons are made using various metrics. Results using 49 imbalanced datasets show that DatRel improves classifier performance in oversampling methods and demonstrates its value in comparison to other noise removal techniques according to AUC, BACC, F1, GMEAN, and MCC.},
  archive      = {J_ML},
  author       = {Sağlam, Fatih},
  doi          = {10.1007/s10994-025-06755-8},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-45},
  shortjournal = {Mach. Learn.},
  title        = {DatRel: A noise-tolerant data relocation approach for effective synthetic data generation in imbalanced classifiers},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal ensemble of multiple patterns’ instances for continuous prediction of events. <em>ML</em>, <em>114</em>(5), 1-42. (<a href='https://doi.org/10.1007/s10994-025-06756-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-life data of various domains, such as traffic, meteorology, or healthcare data, events may have varying durations. Moreover, heterogeneous multivariate temporal data may consist of varying samplings, including regular sampling in different frequencies or irregular, as well as events data of different types, having fixed or varying duration. We propose to uniformly represent heterogeneous multivariate temporal data using symbolic time-intervals, from which a model that predicts an occurrence of events early can be learned. We introduce a novel use of time-interval-related patterns (TIRPs), in which patterns that end with an event of interest can be used to continuously estimate the event’s occurrence probability in real-time. Recently, we introduced a model that allows continuous prediction of the completion of a pattern, which is extended in this work, to also predict the expected completion time. This work focuses on predicting the probability and time occurrence of an event based on multiple different instances of patterns that end with the event, for which we propose and evaluate aggregation functions. A rigorous evaluation was conducted on four real-life datasets to assess the effectiveness of the proposed model and the aggregation functions. The proposed model performed better than the baseline models (ResNet, LSTM-FCN, ROCKET, and XGBoost) for all datasets.},
  archive      = {J_ML},
  author       = {Itzhak, Nevo and Jaroszewicz, Szymon and Moskovitch, Robert},
  doi          = {10.1007/s10994-025-06756-7},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-42},
  shortjournal = {Mach. Learn.},
  title        = {Temporal ensemble of multiple patterns’ instances for continuous prediction of events},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive optimization for prediction with missing data. <em>ML</em>, <em>114</em>(5), 1-37. (<a href='https://doi.org/10.1007/s10994-025-06757-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2–10% improvement in out-of-sample accuracy.},
  archive      = {J_ML},
  author       = {Bertsimas, Dimitris and Delarue, Arthur and Pauphilet, Jean},
  doi          = {10.1007/s10994-025-06757-6},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Adaptive optimization for prediction with missing data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An unsupervised adversarial domain adaptation based on variational auto-encoder. <em>ML</em>, <em>114</em>(5), 1-26. (<a href='https://doi.org/10.1007/s10994-025-06760-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collecting a large amount of labeled data in machine learning is always challenging. Often, even with sufficient data, domain differences can cause a shift or bias in data distribution, affecting model performance during testing. Domain adaptation methods, especially adversarial techniques, are effective solutions for these challenges. The goal is to learn a classifier for an unlabeled target dataset using a labeled source dataset, enhancing resistance to domain shifts. However, existing methods sometimes struggle with adapting the joint feature distribution across domains, resulting in negative transfer. To address this, we propose a method that forms class-specific clusters to prevent negative transfer. This method is encapsulated in an unsupervised adversarial domain adaptation framework based on a variational auto-encoder. Our structure is designed to enhance invariant and discriminative feature representation. We process source and target data through a VAE to establish a smooth latent representation. In our method, source and target data are fed into a variational auto-encoder, which produces a smooth latent representation. The feature extractor then plays an adversarial minimax game with the discriminator to learn domain-invariant features, while the feature extractor is shared between the reconstructed source and reconstructed target data. In addition, we proposed a second structure in which the domain discriminator part of the prior structure is eliminated to demonstrate the influence of the variational auto-encoder in domain adaptation. On numerous unsupervised domain adaptation benchmarks, our results indicate that our proposed model outperforms or is comparable to state-of-the-art outcomes.},
  archive      = {J_ML},
  author       = {Hassan Pour Zonoozi, Mahta and Seydi, Vahid and Deypir, Mahmood},
  doi          = {10.1007/s10994-025-06760-x},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {An unsupervised adversarial domain adaptation based on variational auto-encoder},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Nettop: A lightweight-network of orthogonal-plane features for image recognition. <em>ML</em>, <em>114</em>(5), 1. (<a href='https://doi.org/10.1007/s10994-025-06765-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Nguyen, Thanh Tuan and Nguyen, Thanh Phuong},
  doi          = {10.1007/s10994-025-06765-6},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Nettop: A lightweight-network of orthogonal-plane features for image recognition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CPAT: Cross-patch aggregated transformer for time series forecasting. <em>ML</em>, <em>114</em>(5), 1-31. (<a href='https://doi.org/10.1007/s10994-025-06758-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting utilizes historical data to forecast future information over a specific period. It aims to predict forthcoming developmental trends through meticulous statistical analysis and modeling of historical data, addressing real-life challenges like power load prediction, traffic condition prognostication, and extreme weather warnings. Currently, Transformer-based models for time series prediction normally segment the original time series into multiple patches. While this modeling methodology has demonstrated superiority in improving performance, the approach of patch partition based on a fixed length constrains the model’s predictive accuracy when dealing with time series forecasting tasks of varying lengths. To overcome the limitation, this article proposes an innovative Cross-Patch Aggregated Transformer (CPAT), which introduces the Patch Reconstruction module to restructure patches between encoder layers, facilitating cross-patch connections and information interaction. This empowers the model to focus on the correlation among adjacent patches, acquiring effective representations of both global and local features. Consequently, the modeling of time dependency becomes more precise. Extensive experiments conducted on eight publicly available benchmark datasets in real-world scenarios showcase that the proposed CPAT model attains state-of-the-art (SOTA) accuracy overall compared to existing baseline models. Notably, it achieves relative improvement rates of 5.46% and 2.56% for Mean Square Error (MSE) and Mean Absolute Error (MAE), respectively, augmenting the predictive capabilities of Transformer family models in time series tasks.},
  archive      = {J_ML},
  author       = {Liu, Bingyan and Wu, Li and Wang, Xiaoying and Huang, Jianqiang and Zhang, Guojing},
  doi          = {10.1007/s10994-025-06758-5},
  journal      = {Machine Learning},
  month        = {5},
  number       = {5},
  pages        = {1-31},
  shortjournal = {Mach. Learn.},
  title        = {CPAT: Cross-patch aggregated transformer for time series forecasting},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on impact of label noise on synthetic tabular data generation. <em>ML</em>, <em>114</em>(4), 1-17. (<a href='https://doi.org/10.1007/s10994-024-06629-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic data has been actively used for various machine learning-based tasks due to its benefits such as massive reproducibility and privacy enhancement compared to using the original data. The quality of the generated synthetic dataset crucially depends on the quality of the original data, and the latter is often corrupted by label noise. While there have been studies on feature noise, how label noise affects synthetic data generation is under-explored. In this paper, we evaluate the impact of the noisy label on synthetic data generation with a focus on tabular data. One challenge is how to evaluate the quality of synthetic data under label noise. To this end, we design comprehensive experiments to measure the impact of label noise on synthetic data generation in different aspects: synthetic data quality, data utility, and convergence for training synthesizers and machine learning models for downstream tasks. The empirical results cover wide aspects of synthetic data generation under label noise and they show quality and utility degrades with higher noise levels while there is no significant effect on the synthesizer convergence observed.},
  archive      = {J_ML},
  author       = {Kim, Jeonghoon and Huang, Chao and Liu, Xin},
  doi          = {10.1007/s10994-024-06629-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Learn.},
  title        = {An empirical study on impact of label noise on synthetic tabular data generation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing cross-validation variance through seed blocking in hyperparameter tuning. <em>ML</em>, <em>114</em>(4), 1-48. (<a href='https://doi.org/10.1007/s10994-024-06630-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameter tuning plays a crucial role in optimizing the performance of predictive learners. Cross-validation (CV) is a widely adopted technique for estimating the error of different hyperparameter settings. Repeated cross-validation (RCV) is commonly employed to reduce the variability of CV errors. This study investigates the efficacy of blocking cross-validation partitions and algorithm initialization seeds during hyperparameter tuning. The proposed approach, termed Controlled Cross-Validation (CCV), reduces variability in error estimates, enabling fairer and more reliable comparisons of predictive model performance. We provide both theoretical and empirical evidence to demonstrate that this blocking approach lowers the variance of the estimates compared to RCV. Our experiments indicate that the algorithm’s internal random behavior often does not significantly affect CV error variability. We present extensive examples using real-world datasets to compare the effectiveness and efficiency of blocking the CV partitions when tuning the hyperparameters of different supervised predictive learning algorithms.},
  archive      = {J_ML},
  author       = {Merola, Giovanni Maria},
  doi          = {10.1007/s10994-024-06630-y},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-48},
  shortjournal = {Mach. Learn.},
  title        = {Reducing cross-validation variance through seed blocking in hyperparameter tuning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inductive learning of robot task knowledge from raw data and online expert feedback. <em>ML</em>, <em>114</em>(4), 1-33. (<a href='https://doi.org/10.1007/s10994-024-06636-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing level of autonomy of robots poses challenges of trust and social acceptance, especially in human-robot interaction scenarios. This requires an interpretable implementation of robotic cognitive capabilities, possibly based on formal methods as logics for the definition of task specifications. However, prior knowledge is often unavailable in complex realistic scenarios. In this paper, we propose an offline algorithm based on inductive logic programming from noisy examples to extract task specifications (i.e., action preconditions, constraints and effects) directly from raw data of few heterogeneous (i.e., not repetitive) robotic executions. Our algorithm leverages on the output of any unsupervised action identification algorithm from video-kinematic recordings. Combining it with the definition of very basic, almost task-agnostic, commonsense concepts about the environment, which contribute to the interpretability of our methodology, we are able to learn logical axioms encoding preconditions of actions, as well as their effects in the event calculus paradigm. Since the quality of learned specifications depends mainly on the accuracy of the action identification algorithm, we also propose an online framework for incremental refinement of task knowledge from user’s feedback, guaranteeing safe execution. Results in a standard manipulation task and benchmark for user training in the safety-critical surgical robotic scenario, show the robustness, data- and time-efficiency of our methodology, with promising results towards the scalability in more complex domains.},
  archive      = {J_ML},
  author       = {Meli, Daniele and Fiorini, Paolo},
  doi          = {10.1007/s10994-024-06636-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Inductive learning of robot task knowledge from raw data and online expert feedback},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient projection-free online convex optimization using stochastic gradients. <em>ML</em>, <em>114</em>(4), 1-61. (<a href='https://doi.org/10.1007/s10994-024-06640-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Online Convex Optimization (OCO) problems subject to a compact convex set. An important class of projection-free online methods known as Frank–Wolfe-type (FW-type) methods have attracted considerable attention in the machine learning community, as they eschew the expensive projection operation and only require a simple linear minimization oracle in each round. Recently, the stochastic gradient technique has been integrated in FW-type online methods to circumvent the expensive full gradient computation and further reduce the per-round computational cost. However, these methods generally have high regret bounds due to high variance in gradient estimation. Although adopting a large minibatch in stochastic gradients can reduce the variance, it would in turn increase the per-round computational cost. In this paper, we develop efficient FW-type methods that only need stochastic gradients with small minibatch and achieve nearly optimal regret bounds with low per-round costs. We first explore the similarity between gradients of decision variables in consecutive rounds, and construct a lightweight variance-reduced estimator by utilizing historical gradient information. Based on this estimator, we propose a method named OFWRG for smooth problems in the stochastic setting. We prove that OFWRG achieves a nearly optimal regret bound with the lowest $$\mathcal {O}(1)$$ per-round computational cost. OFWRG is the first method with such nearly optimal result in this setting. We further extend OFWRG to OCO problems in other settings, including smooth problems in the adversarial setting and a class of non-smooth problems in the stochastic and adversarial settings. Our theoretical analyses show that these extensions of OFWRG achieve nearly optimal regret bounds and low per-round computational costs under mild conditions. Experimental results demonstrate the efficiency of our methods.},
  archive      = {J_ML},
  author       = {Xie, Jiahao and Zhang, Chao and Shen, Zebang and Qian, Hui},
  doi          = {10.1007/s10994-024-06640-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-61},
  shortjournal = {Mach. Learn.},
  title        = {Efficient projection-free online convex optimization using stochastic gradients},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibrated explanations for regression. <em>ML</em>, <em>114</em>(4), 1-34. (<a href='https://doi.org/10.1007/s10994-024-06642-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) methods are an integral part of modern decision support systems. The best-performing predictive models used in AI-based decision support systems lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature’s importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations, previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is below an arbitrary threshold. The extension for regression keeps all the benefits of Calibrated Explanations, such as calibration of the prediction from the underlying model with confidence intervals, uncertainty quantification of feature importance, and allows both factual and counterfactual explanations. Calibrated Explanations for regression provides fast, reliable, stable, and robust explanations. Calibrated Explanations for probabilistic regression provides an entirely new way of creating probabilistic explanations from any ordinary regression model, allowing dynamic selection of thresholds. The method is model agnostic with easily understood conditional rules. An implementation in Python is freely available on GitHub and for installation using both pip and conda, making the results in this paper easily replicable.},
  archive      = {J_ML},
  author       = {Löfström, Tuwe and Löfström, Helena and Johansson, Ulf and Sönströd, Cecilia and Matela, Rudy},
  doi          = {10.1007/s10994-024-06642-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-34},
  shortjournal = {Mach. Learn.},
  title        = {Calibrated explanations for regression},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforced logical reasoning over KGs for interpretable recommendation system. <em>ML</em>, <em>114</em>(4), 1-27. (<a href='https://doi.org/10.1007/s10994-024-06646-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various domains, traditional recommendation systems have demonstrated significant benefits. However, their "black box" mechanisms have led to a crisis of trust among users. Interpretable recommendation systems have emerged as a solution by providing explanations for recommended items, thus enhancing transparency and user confidence. Another challenge to interpretable recommendation systems is data sparsity, which causes subpar recommendation performance. Addressing the challenges of model interpretability and data sparsity, this paper introduces the Knowledge Graphs-based Logic Reasoning Recommendation (KG-LRR) method, structured around an "encoder-decoder" architecture. The KG-LRR method tackles these issues by leveraging a knowledge graph for items to enhance the representation of users and items during the encoding process. It introduces a propositional logic reasoning model for decoding, rendering explanations in a more comprehensible manner. This dual approach ensures a balance between the recommendation system’s efficiency and interpretability. The KG-LRR method employs a neural network to simulate human-like propositional logical reasoning. This not only mitigates data sparsity issues but also explicates users’ interest in items. It provides deeper insights into users’ preferences and delivers robust interpretability. Experimental results across three public datasets-Yelp2018, Amazon-book, and Amazon-electronics-demonstrate that the KG-LRR model outperforms existing methods in terms of Recall and nDCG in top-k ranking recommendation scenarios. This validates its superior performance compared to prevailing interpretable recommendation techniques. In summary, the KG-LRR method offers a novel approach to enhance transparency and performance through an innovative "encoder-decoder" architecture. Its integration of knowledge graphs and propositional logic reasoning showcases promising outcomes in addressing current challenges within interpretable recommendation systems. Our code is available at https://github.com/siri-ya/KG-LRR .},
  archive      = {J_ML},
  author       = {Wang, Shirui and Xie, Bohan and Ding, Ling and Chen, Jianting and Xiang, Yang},
  doi          = {10.1007/s10994-024-06646-4},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Reinforced logical reasoning over KGs for interpretable recommendation system},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified convergence analysis for adaptive optimization with moving average estimator. <em>ML</em>, <em>114</em>(4), 1-51. (<a href='https://doi.org/10.1007/s10994-024-06650-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although adaptive optimization algorithms have been successful in many applications, there are still some mysteries in terms of convergence analysis that have not been unraveled. This paper provides a novel non-convex analysis of adaptive optimization to uncover some of these mysteries. Our contributions are three-fold. First, we show that an increasing or large enough momentum parameter for the first-order moment used in practice is sufficient to ensure the convergence of adaptive algorithms whose adaptive scaling factors of the step size are bounded. Second, our analysis gives insights for practical implementations, e.g., increasing the momentum parameter in a stage-wise manner in accordance with stagewise decreasing step size would help improve the convergence. Third, the modular nature of our analysis allows its extension to solving other optimization problems, e.g., compositional, min–max and bilevel problems. As an interesting yet non-trivial use case, we present algorithms for solving non-convex min–max optimization and bilevel optimization that do not require using large batches of data to estimate gradients or double loops as the literature do. Our empirical studies corroborate our theoretical results.},
  archive      = {J_ML},
  author       = {Guo, Zhishuai and Xu, Yi and Yin, Wotao and Jin, Rong and Yang, Tianbao},
  doi          = {10.1007/s10994-024-06650-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-51},
  shortjournal = {Mach. Learn.},
  title        = {Unified convergence analysis for adaptive optimization with moving average estimator},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the usefulness of the fit-on-test view on evaluating calibration of classifiers. <em>ML</em>, <em>114</em>(4), 1-75. (<a href='https://doi.org/10.1007/s10994-024-06652-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibrated uncertainty estimates are essential for classifiers used in safety-critical applications. If a classifier is uncalibrated, then there is a unique way to calibrate its uncertainty using the idealistic true calibration map corresponding to this classifier. Although the true calibration map is typically unknown in practice, it can be estimated with many post-hoc calibration methods which fit some family of potential calibration functions on a validation dataset. This paper examines the connection between such post-hoc calibration methods and calibration evaluation. Despite the negative connotations of fitting on test data in machine learning, we claim that fitting calibration maps on test data as part of the calibration evaluation process is a method worth considering, and we refer to this view as fit-on-test. This view enables the usage of any post-hoc calibration method as an evaluation measure, unlocking missed opportunities in development of evaluation methods. We prove that even ECE, which is the most common calibration evaluation method, is actually a fit-on-test measure. This observation leads us to a new method of tuning the number of bins in ECE with cross-validation. Fitting on test data can lead to test-time overfitting, and therefore, we discuss the limitations and concerns with the fit-on-test view. Our contributions also include: (1) enhancement of reliability diagrams with diagonal filling; (2) development of new calibration map families PL and PL3; and (3) an experimental study of which families perform strongly both as post-hoc calibrators and calibration evaluators.},
  archive      = {J_ML},
  author       = {Kängsepp, Markus and Valk, Kaspar and Kull, Meelis},
  doi          = {10.1007/s10994-024-06652-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-75},
  shortjournal = {Mach. Learn.},
  title        = {On the usefulness of the fit-on-test view on evaluating calibration of classifiers},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximum causal entropy inverse constrained reinforcement learning. <em>ML</em>, <em>114</em>(4), 1-44. (<a href='https://doi.org/10.1007/s10994-024-06653-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When deploying artificial agents in real-world environments where they interact with humans, it is crucial that their behavior is aligned with the values, social norms or other requirements specific to that environment. However, many environments have implicit constraints that are difficult to specify and transfer to a learning agent. To address this challenge, we propose a novel method that utilizes the principle of maximum causal entropy to learn constraints and an optimal policy that adheres to these constraints, using demonstrations of agents that abide by the constraints. We prove convergence in a tabular setting and provide a practical implementation which scales to complex environments. We evaluate the effectiveness of the learned policy by assessing the reward received and the number of constraint violations, and we evaluate the learned cost function based on its transferability to other agents. Our method has been shown to outperform state-of-the-art approaches across a variety of tasks and environments, and it is able to handle problems with stochastic dynamics and a continuous state-action space.},
  archive      = {J_ML},
  author       = {Baert, Mattijs and Mazzaglia, Pietro and Leroux, Sam and Simoens, Pieter},
  doi          = {10.1007/s10994-024-06653-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-44},
  shortjournal = {Mach. Learn.},
  title        = {Maximum causal entropy inverse constrained reinforcement learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A prompt-driven framework for multi-domain knowledge tracing. <em>ML</em>, <em>114</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10994-024-06660-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) models students’ knowledge states to predict future performance based on historical interactions. Due to data privacy concerns and budget constraints, the availability of high-quality student data differs across domains and it is essential to effectively utilize KT data from multiple domains. In this work, we propose a novel prompt-enhanced paradigm, i.e., promptKT, to utilize student data from multiple domains to improve KT performance simultaneously. Specifically, a unified Transformer based backbone model is first pre-trained using data from all the KT domains to capture the commonality across domains. Then, we design a novel soft domain prompt module to capture the distinctions among various domains and users. Our promptKT is evaluated on six public real-world educational datasets. The results demonstrate that our approach outperforms the majority of existing KT models in terms of AUC and accuracy. Furthermore, empirical analysis shows the decent transferability and adaptation of promptKT across multiple KT domains. To encourage reproducible research, we make our data and code publicly available at https://github.com/pykt-team/pykt-toolkit .},
  archive      = {J_ML},
  author       = {Liu, Zitao and Huang, Shuyan and Guo, Teng and Hou, Mingliang and Liang, Qianru},
  doi          = {10.1007/s10994-024-06660-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {A prompt-driven framework for multi-domain knowledge tracing},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Capturing the context-aware code change via dynamic control flow graph for commit message generation. <em>ML</em>, <em>114</em>(4), 1-23. (<a href='https://doi.org/10.1007/s10994-024-06671-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commit messages that summarize code changes of each commit in natural language help developers understand code changes without digging into implementation details, thus playing an essential role in comprehending software evolution. In constructing models for automatic commit message generation, prior research has focused on extracting information from the changed code hunks (i.e., code difference), while ignoring the unchanged code hunks (i.e., code context). However, most studies often neglect the fact that the code change is context-aware, that is the semantics of the code difference are heavily dependent on its code context. To take the code context into account, a key challenge arises: the extensive code context may overshadow the minuscule code difference in capturing the changed semantics, which is a disadvantage to commit message generation. In this paper, we propose the dynamic control flow graph (DCFG), which combines both the code contexts and code differences into one dynamic global–local structure. Based on DCFG, we design a novel framework termed capturing the context-aware code change for commit message generation ( $${\text {C}^4\text {MG}}$$ ), which attempts to model the changed semantics of the code change based on the relevant code context, while avoiding being misled by the overwhelming amount of unchanged code context. Extensive experiments demonstrate that benefiting from modeling the context-aware code change, $${\text {C}^4\text {MG}}$$ outperforms not only the state-of-the-art open-source models but also the large language models (e.g., LLaMA3, GPT-4o, and Gemini) on the commit message generation.},
  archive      = {J_ML},
  author       = {Du, Yali and Li, Ying and Ma, Yi-Fan and Li, Ming},
  doi          = {10.1007/s10994-024-06671-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Capturing the context-aware code change via dynamic control flow graph for commit message generation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nettop: A light-weight network of orthogonal-plane features for image recognition. <em>ML</em>, <em>114</em>(4), 1-27. (<a href='https://doi.org/10.1007/s10994-024-06672-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current light-weight CNN-based networks, convolutional operators are principally utilized to extract feature maps for image representation. However, such conventional operation can lead to lack of informative patterns for the learning process. It is because the operators have just been allocated to convolute on the spatial side of an input tensor. To deal with this deficiency, we propose a competent model to efficiently exploit the full-side features of a tensor. The proposed model is based on three novel concepts as follows. i) A novel grouped-convolutional operator is defined to produce complementary features in consideration of three plane-based volumes that have been correspondingly partitioned subject to three orthogonal planes (TOP) of a given tensor. ii) An effective perceptron block is introduced to take into account the TOP-based operator for orthogonal-plane feature extraction. iii) A light-weight backbone of TOP-based blocks (named NetTOP) is proposed to take advantage of the full-side informative patterns for image representation. Experimental results for image recognition on benchmark datasets have proved the prominent performance of the proposals. The code of NetTOP is available at https://github.com/nttbdrk25/NetTOP .},
  archive      = {J_ML},
  author       = {Nguyen, Thanh Tuan and Nguyen, Thanh Phuong},
  doi          = {10.1007/s10994-024-06672-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Nettop: A light-weight network of orthogonal-plane features for image recognition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HorNets: Learning from discrete and continuous signals with routing neural networks. <em>ML</em>, <em>114</em>(4), 1-23. (<a href='https://doi.org/10.1007/s10994-024-06673-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction of neural network architectures suitable for learning from both continuous and discrete tabular data is challenging, as contemporary high-dimensional tabular data sets are often characterized by a relatively small set of instances and the request for efficient learning. We propose HorNets (Horn Networks), a neural network architecture with state-of-the-art performance on synthetic and real-life data sets from scarce-data tabular domains. HorNets are based on a clipped polynomial-like activation function, extended by a custom discrete-continuous routing mechanism that decides which part of the neural network to optimize based on the input’s cardinality. By explicitly modeling parts of the feature combination space or combining whole space in a linear attention-like manner, HorNets dynamically decide which mode of operation is the most suitable for a given piece of data with no explicit supervision. This architecture is one of the few approaches that reliably retrieves logical clauses (including noisy XNOR) and achieves state-of-the-art classification performance on 14 real-life biomedical high-dimensional data sets. HorNets are made freely available under a permissive license alongside a synthetic generator of categorical benchmarks.},
  archive      = {J_ML},
  author       = {Koloski, Boshko and Lavrač, Nada and Škrlj, Blaž},
  doi          = {10.1007/s10994-024-06673-1},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {HorNets: Learning from discrete and continuous signals with routing neural networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TCR: Topologically consistent reweighting for XGBoost in regression tasks. <em>ML</em>, <em>114</em>(4), 1-52. (<a href='https://doi.org/10.1007/s10994-024-06704-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient boosted tree ensembles (GBTEs) such as XGBoost continue to outperform other machine learning models on tabular data. However, the plethora of adjustable hyperparameters can exacerbate optimisation, especially in regression tasks with no intuitive performance measures such as accuracy and confidence. Automated machine learning frameworks alleviate the hyperparameter search for users, but if the optimisation procedure ends prematurely due to resource constraints, it is questionable whether users receive good models. To tackle this problem, we introduce a cost-efficient method to retrofit previously optimised XGBoost models by retraining them with a new weight distribution over the training instances. We base our approach on topological results, which allows us to infer model-agnostic weights for specific regions of the data distribution where the targets are more susceptible to input perturbations. By linking our theory to the training procedure of XGBoost regressors, we then establish a topologically consistent reweighting scheme, which is independent of the specific model instance. Empirically, we verify that our approach improves prediction performance, outperforms other reweighting methods and is much faster than a hyperparameter search. To enable users to find the optimal weights for their data, we provide guides based on our findings on 20 datasets. Our code is available at: https://github.com/montymaxzuehlke/tcr .},
  archive      = {J_ML},
  author       = {Zühlke, Monty-Maximilian and Kudenko, Daniel},
  doi          = {10.1007/s10994-024-06704-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-52},
  shortjournal = {Mach. Learn.},
  title        = {TCR: Topologically consistent reweighting for XGBoost in regression tasks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intramodal consistency in triplet-based cross-modal learning for image retrieval. <em>ML</em>, <em>114</em>(4), 1-29. (<a href='https://doi.org/10.1007/s10994-024-06710-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval requires building a common latent space that captures and correlates information from different data modalities, usually images and texts. Cross-modal training based on the triplet loss with hard negative mining is a state-of-the-art technique to address this problem. This paper shows that such approach is not always effective in handling intra-modal similarities. Specifically, we found that this method can lead to inconsistent similarity orderings in the latent space, where intra-modal pairs with unknown ground-truth similarity are ranked higher than cross-modal pairs representing the same concept. To address this problem, we propose two novel loss functions that leverage intra-modal similarity constraints available in a training triplet but not used by the original formulation. Additionally, this paper explores the application of this framework to unsupervised image retrieval problems, where cross-modal training can provide the supervisory signals that are otherwise missing in the absence of category labels. Up to our knowledge, we are the first to evaluate cross-modal training for intra-modal retrieval without labels. We present comprehensive experiments on MS-COCO and Flickr30k, demonstrating the advantages and limitations of the proposed methods in cross-modal and intra-modal retrieval tasks in terms of performance and novelty measures. We also conduct a case study on the ROCO dataset to assess the performance of our method on medical images and present an ablation study on one of our approaches to understanding the impact of the different components of the proposed loss function. Our code is publicly available on GitHub https://github.com/MariodotR/FullHN.git .},
  archive      = {J_ML},
  author       = {Mallea, Mario and Ñanculef, Ricardo and Araya, Mauricio},
  doi          = {10.1007/s10994-024-06710-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Intramodal consistency in triplet-based cross-modal learning for image retrieval},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain generalization via content factors isolation: A two-level latent variable modeling approach. <em>ML</em>, <em>114</em>(4), 1-33. (<a href='https://doi.org/10.1007/s10994-024-06717-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of domain generalization is to develop models that exhibit a higher degree of generality, meaning they perform better when evaluated on data coming from previously unseen distributions. Models obtained via traditional methods often cannot distinguish between label-specific and domain-related features in the latent space. To confront this difficulty, we propose formulating a novel data generation process using a latent variable model and postulating a partition of the latent space into content and style parts while allowing for statistical dependency to exist between them. In this model, the distribution of content factors associated with observations belonging to the same class depends on only the label corresponding to that class. In contrast, the distribution of style factors has an additional dependency on the domain variable. We derive constraints that suffice to recover the collection of content factors block-wise and the collection of style factors component-wise while guaranteeing the isolation of content factors. This allows us to produce a stable predictor solely relying on the latent content factors. Building upon these theoretical insights, we propose a practical and efficient algorithm for determining the latent variables under the variational auto-encoder framework. Our simulations with dependent latent variables produce results consistent with our theory, and real-world experiments show that our method outperforms the competitors.},
  archive      = {J_ML},
  author       = {Gao, Erdun and Bondell, Howard and Huang, Shaoli and Gong, Mingming},
  doi          = {10.1007/s10994-024-06717-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Domain generalization via content factors isolation: A two-level latent variable modeling approach},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing exchangeability in the batch mode with e-values and markov alternatives. <em>ML</em>, <em>114</em>(4), 1-27. (<a href='https://doi.org/10.1007/s10994-024-06720-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of this paper is testing the assumption of exchangeability, which is the standard assumption in mainstream machine learning. The common approaches are online testing by betting (such as conformal testing) and the older batch testing using p-values (as in classical hypothesis testing). The approach of this paper is intermediate in that we are interested in batch testing by betting; as a result, p-values are replaced by e-values. As a first step in this direction, this paper concentrates on the Markov model as alternative. The null hypothesis of exchangeability is formalized as a Kolmogorov-type compression model, and the Bayes mixture of the Markov model w.r. to the uniform prior is taken as simple alternative hypothesis. Using e-values instead of p-values leads to a computationally efficient testing procedure. Two appendixes discuss connections with the algorithmic theory of randomness; in particular, the test proposed in this paper can be interpreted as a poor man’s version of Kolmogorov’s deficiency of randomness.},
  archive      = {J_ML},
  author       = {Vovk, Vladimir},
  doi          = {10.1007/s10994-024-06720-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Testing exchangeability in the batch mode with e-values and markov alternatives},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel density estimation for multiclass quantification. <em>ML</em>, <em>114</em>(4), 1-38. (<a href='https://doi.org/10.1007/s10994-024-06726-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several disciplines, like the social sciences, epidemiology, sentiment analysis, or market research, are interested in knowing the distribution of the classes in a population rather than the individual labels of the members thereof. Quantification is the supervised machine learning task concerned with obtaining accurate predictors of class prevalence, and to do so particularly in the presence of label shift. The distribution-matching (DM) approaches represent one of the most important families among the quantification methods that have been proposed in the literature so far. Current DM approaches model the involved populations using histograms of posterior probabilities. In this paper, we argue that their application to the multiclass setting is suboptimal since the histograms become class-specific, thus missing the opportunity to model inter-class information that may exist in the data. We propose a new representation mechanism based on multivariate densities that we model via kernel density estimation (KDE). The experiments we have carried out show our method, dubbed KDEy, yields superior quantification performance compared to previous DM approaches and other state-of-the-art quantification systems.},
  archive      = {J_ML},
  author       = {Moreo, Alejandro and González, Pablo and del Coz, Juan José},
  doi          = {10.1007/s10994-024-06726-5},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Kernel density estimation for multiclass quantification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical risk minimization in the interpolating regime with application to neural network learning. <em>ML</em>, <em>114</em>(4), 1-52. (<a href='https://doi.org/10.1007/s10994-025-06738-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common strategy to train deep neural networks (DNNs) is to use very large architectures and to train them until they (almost) achieve zero training error. Empirically observed good generalization performance on test data, even in the presence of lots of label noise, corroborate such a procedure. On the other hand, in statistical learning theory it is known that over-fitting models may lead to poor generalization properties, occurring in e.g. empirical risk minimization (ERM) over too large hypotheses classes. Inspired by this contradictory behavior, so-called interpolation methods have recently received much attention, leading to consistent and optimally learning methods for, e.g., some local averaging schemes with zero training error. We extend this analysis to ERM-like methods for least squares regression and show that for certain, large hypotheses classes called inflated histograms, some interpolating empirical risk minimizers enjoy very good statistical guarantees while others fail in the worst sense. Moreover, we show that the same phenomenon occurs for DNNs with zero training error and sufficiently large architectures.},
  archive      = {J_ML},
  author       = {Mücke, Nicole and Steinwart, Ingo},
  doi          = {10.1007/s10994-025-06738-9},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-52},
  shortjournal = {Mach. Learn.},
  title        = {Empirical risk minimization in the interpolating regime with application to neural network learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compression and restoration: Exploring elasticity in continual test-time adaptation. <em>ML</em>, <em>114</em>(4), 1-32. (<a href='https://doi.org/10.1007/s10994-025-06739-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation is a task that a pre-trained source model is updated during inference with given test data from target domains with different distributions. However, frequent updates in a long time without resetting the model will bring two main problems, i.e., error accumulation and catastrophic forgetting. Although some recent methods have alleviated the problems by designing new loss functions or update strategies, they are still very fragile to hyperparameters or suffer from storage burden. Besides, most methods treat each target domain equally, neglecting the characteristics of each target domain and the situation of the current model, which will mislead the update direction of the model. To address the above issues, we first leverage the mean cosine similarity per test batch between the features output by the source and updated models to measure the change of target domains. Then we summarize the elasticity of the mean cosine similarity to guide the model to update and restore adaptively. Motivated by this, we propose a frustratingly simple yet efficient method called Elastic-Test-time ENTropy Minimization (E-TENT) to dynamically adjust the mean cosine similarity based on the built relationship between it and the momentum coefficient. Combined with the extra three minimal improvements, E-TENT exhibits significant performance gains and strong robustness on CIFAR10-C, CIFAR100-C and ImageNet-C along with various practical scenarios.},
  archive      = {J_ML},
  author       = {Li, Jingwei and Liu, Chengbao and Bai, Xiwei and Tan, Jie and Chu, Jiaqi and Wang, Yudong},
  doi          = {10.1007/s10994-025-06739-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-32},
  shortjournal = {Mach. Learn.},
  title        = {Compression and restoration: Exploring elasticity in continual test-time adaptation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep errors-in-variables using a diffusion model. <em>ML</em>, <em>114</em>(4), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06744-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Errors-in-Variables is the statistical concept used to explicitly model input variable errors caused, for example, by noise. While it has long been known in statistics that not accounting for such errors can produce a substantial bias, the vast majority of deep learning models have thus far neglected Errors-in-Variables approaches. Reasons for this include a significant increase of the numerical burden and the challenge in assigning an appropriate prior in a Bayesian treatment. To date, the attempts made to use Errors-in-Variables for neural networks do not scale to deep networks or are too simplistic to enhance the prediction performance. This work shows for the first time how Bayesian deep Errors-in-Variables models can increase the prediction performance. We present a scalable variational inference scheme for Bayesian Errors-in-Variables and demonstrate a significant increase in prediction performance for the case of image classification. Concretely, we use a diffusion model as input posterior to obtain a distribution over the denoised image data. We also observe that training the diffusion model on an unnoisy surrogate dataset can suffice to achieve an improved prediction performance on noisy data.},
  archive      = {J_ML},
  author       = {Faller, Josua and Martin, Jörg and Elster, Clemens},
  doi          = {10.1007/s10994-025-06744-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Deep errors-in-variables using a diffusion model},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncover the balanced geometry in long-tailed contrastive language-image pretraining. <em>ML</em>, <em>114</em>(4), 1-33. (<a href='https://doi.org/10.1007/s10994-025-06745-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Contrastive Language-Image Pretraining (CLIP) has become the de facto standard for vision-language pretraining tasks, the exploration on the inherent long-tailed pretraining data distribution remains limited. From a neural collapse perspective, we show in principle that the vanilla CLIP training can be vulnerable to the long-tailed distributions, which might distort the representations with reduced inter-class separation and poor discriminative ability. To combat this issue, we propose an improved method, termed as Geometry-Balanced CLIP (GeoCLIP), which automatically constructs pseudo clusters and aligns them with a predefined equiangular geometric structure, thereby enjoying the theoretical merits of better maintaining the uniformity at the semantic level. Furthermore, we enhance GeoCLIP’s generality for real-world complex distributions by incorporating harmonized clusters that integrate both empirically observed data structures and theoretically optimal geometry. Extensive experiments across various benchmarks demonstrate the consistent superiority of GeoCLIP in achieving robust and transferable representation under long-tailed distributions. The source code will be publicly available.},
  archive      = {J_ML},
  author       = {Zhou, Zhihan and Ye, Yushi and Hong, Feng and Zhao, Peisen and Yao, Jiangchao and Zhang, Ya and Tian, Qi and Wang, Yanfeng},
  doi          = {10.1007/s10994-025-06745-w},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Mach. Learn.},
  title        = {Uncover the balanced geometry in long-tailed contrastive language-image pretraining},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning with pre-trained conditional generative models. <em>ML</em>, <em>114</em>(4), 1-38. (<a href='https://doi.org/10.1007/s10994-025-06748-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is crucial in training deep neural networks on new target tasks. Current transfer learning methods always assume at least one of (i) Source and target task label spaces overlap, (ii) Source datasets are available, and (iii) Target network architectures are consistent with source ones. However, holding these assumptions is difficult in practical settings because the target task rarely has the same labels as the source task, the source dataset access is restricted due to storage costs and privacy, and the target architecture is often specialized to each task. To transfer source knowledge without these assumptions, we propose a transfer learning method that uses deep generative models and is composed of the following two stages: pseudo pre-training (PP) and pseudo semi-supervised learning (P-SSL). PP trains a target architecture with an artificial dataset synthesized by using conditional source generative models. P-SSL applies SSL algorithms to labeled target data and unlabeled pseudo samples, which are generated by cascading the source classifier and generative models to condition them with target samples. Our experimental results indicate that our method can outperform the baselines of scratch training and knowledge distillation.},
  archive      = {J_ML},
  author       = {Yamaguchi, Shin’ya and Kanai, Sekitoshi and Kumagai, Atsutoshi and Chijiwa, Daiki and Kashima, Hisashi},
  doi          = {10.1007/s10994-025-06748-7},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Transfer learning with pre-trained conditional generative models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model for intelligible interaction between agents that predict and explain. <em>ML</em>, <em>114</em>(4), 1-40. (<a href='https://doi.org/10.1007/s10994-025-06750-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has emerged as a powerful form of data modelling with widespread applicability beyond its roots in the design of autonomous agents. However, relatively little attention has been paid to the interaction between people and ML systems. In this paper we view interaction between humans and ML systems within the broader context of communication between agents capable of prediction and explanation. We formalise the interaction model by taking agents to be automata with some special characteristics and define a protocol for communication between such agents. We define One- and Two-Way Intelligibility as properties that emerge at run-time by execution of the protocol. The formalisation allows us to identify conditions under which run-time sequences are bounded, and identify conditions under which the protocol can correctly implement an axiomatic specification of intelligible interaction between a human and an ML system. We also demonstrate using the formal model to: (a) identify instances of One- and Two-Way Intelligibility in literature reports on humans interacting with ML systems providing logic-based explanations, as is done in Inductive Logic Programming (ILP); and (b) map interactions between humans and machines in an elaborate natural-language based dialogue-model to One- or Two-Way Intelligible interactions in the formal model.},
  archive      = {J_ML},
  author       = {Baskar, A. and Srinivasan, Ashwin and Bain, Michael and Coiera, Enrico},
  doi          = {10.1007/s10994-025-06750-z},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {A model for intelligible interaction between agents that predict and explain},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A contrastive neural disentanglement approach for query performance prediction. <em>ML</em>, <em>114</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10994-025-06752-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach, referred to as contrastive disentangled representation for query performance prediction (CoDiR-QPP), to estimate search query performance by disentangling query content semantics from query difficulty. Our proposed approach leverages neural disentanglement to isolate the information need expressed in search queries from the complexities that affect retrieval performance. Motivated by empirical observations that varying query formulations for the same information need can significantly impact retrieval outcomes, we hypothesize that separating content semantics from query difficulty can enhance query performance prediction. Utilizing contrastive learning, CoDiR-QPP distinguishes between well-performing and poorly performing query variants, facilitating the estimation of a given query’s performance. Our extensive experiments on four standard benchmark datasets demonstrate that CoDiR-QPP outperforms state-of-the-art baselines in predicting query performance, offering improved semantic similarity computation and higher correlation metrics such as Kendall $$\tau$$ , Spearman $$\rho$$ , and scaled Mean Absolute Ranking Error (sMARE).},
  archive      = {J_ML},
  author       = {Salamat, Sara and Arabzadeh, Negar and Seyedsalehi, Shirin and Bigdeli, Amin and Zihayat, Morteza and Bagheri, Ebrahim},
  doi          = {10.1007/s10994-025-06752-x},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {A contrastive neural disentanglement approach for query performance prediction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Likelihood-ratio-based confidence intervals for neural networks. <em>ML</em>, <em>114</em>(4), 1-28. (<a href='https://doi.org/10.1007/s10994-024-06639-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a first implementation of a novel likelihood-ratio-based approach for constructing confidence intervals for neural networks. Our method, called DeepLR, offers several qualitative advantages: most notably, the ability to construct asymmetric intervals that expand in regions with a limited amount of data, and the inherent incorporation of factors such as the amount of training time, network architecture, and regularization techniques. While acknowledging that the current implementation of the method is prohibitively expensive for many deep-learning applications, the high cost may already be justified in specific fields like medical predictions or astrophysics, where a reliable uncertainty estimate for a single prediction is essential. This work highlights the significant potential of a likelihood-ratio-based uncertainty estimate and establishes a promising avenue for future research.},
  archive      = {J_ML},
  author       = {Sluijterman, Laurens and Cator, Eric and Heskes, Tom},
  doi          = {10.1007/s10994-024-06639-3},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Likelihood-ratio-based confidence intervals for neural networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pairwise learning to rank by neural networks revisited: Reconstruction, theoretical analysis and practical performance. <em>ML</em>, <em>114</em>(4), 1-28. (<a href='https://doi.org/10.1007/s10994-024-06644-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We reevaluate the pairwise learning to rank approach based on neural nets, called RankNet, and present a theoretical analysis of its architecture. We show mathematically that the model can, under certain conditions, learn reflexive, antisymmetric, and transitive relations, enabling simplified training and improved performance. Experimental results on the LETOR MSLR-WEB10K, MQ2007 and MQ2008 datasets show that the model outperforms numerous state-of-the-art methods (including a listwise approach), while being inherently simpler in structure and using a pairwise approach only.},
  archive      = {J_ML},
  author       = {Köppel, Marius and Segner, Alexander and Wagener, Martin and Pensel, Lukas and Karwath, Andreas and Kramer, Stefan},
  doi          = {10.1007/s10994-024-06644-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Pairwise learning to rank by neural networks revisited: Reconstruction, theoretical analysis and practical performance},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on self-supervised methods for visual representation learning. <em>ML</em>, <em>114</em>(4), 1-56. (<a href='https://doi.org/10.1007/s10994-024-06708-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.},
  archive      = {J_ML},
  author       = {Uelwer, Tobias and Robine, Jan and Wagner, Stefan Sylvius and Höftmann, Marc and Upschulte, Eric and Konietzny, Sebastian and Behrendt, Maike and Harmeling, Stefan},
  doi          = {10.1007/s10994-024-06708-7},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-56},
  shortjournal = {Mach. Learn.},
  title        = {A survey on self-supervised methods for visual representation learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring individual direct causal effects under heterogeneous peer influence. <em>ML</em>, <em>114</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06729-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference is central to understanding the effectiveness of policies and designing personalized interventions. Causal inference involves estimating the causal effects of treatments on outcomes of interest after modeling appropriate assumptions. Most causal inference approaches assume that a unit’s outcome is independent of the treatments or outcomes of other units. However, this assumption is unrealistic when inferring causal effects in networks where a unit’s outcome can be influenced by the treatments and outcomes of its neighboring nodes, a phenomenon known as interference. Causal inference in networks should explicitly account for interference. In interference settings, the direct causal effect measures the impact of the unit’s own treatment while controlling for the treatments of peers. Existing solutions to estimating direct causal effects under interference consider either homogeneous influence from peers or specific heterogeneous influence mechanisms (e.g., based on local neighborhood structure). In this work, we define heterogeneous peer influence (HPI) as the general interference that occurs when a unit’s outcome may be influenced differently by different peers based on their attributes and relationships, or when each network node may have a different susceptibility to peer influence. This paper presents IDE-Net, a framework for estimating individual, i.e., unit-level, direct causal effects in the presence of HPI where the mechanism of influence is not known a priori. We first propose a structural causal model for networks that can capture different possible assumptions about network structure, interference conditions, and causal dependence and that enables reasoning about causal effect identifiability and discovery of potential heterogeneous contexts. We then propose a novel graph neural network-based estimator to estimate individual direct causal effects. We show empirically that state-of-the-art methods for individual direct effect estimation produce biased results in the presence of HPI, and that our proposed estimator is robust.},
  archive      = {J_ML},
  author       = {Adhikari, Shishir and Zheleva, Elena},
  doi          = {10.1007/s10994-024-06729-2},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Inferring individual direct causal effects under heterogeneous peer influence},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end explainability framework for spatio-temporal predictive modeling. <em>ML</em>, <em>114</em>(4), 1-47. (<a href='https://doi.org/10.1007/s10994-024-06733-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising adoption of AI models in real-world applications characterized by sensor data creates an urgent need for inference explanation mechanisms to support domain experts in making informed decisions. Explainable AI (XAI) opens up a new opportunity to extend black-box deep learning models with such inference explanation capabilities. However, existing XAI approaches for tabular, image, and graph data are ineffective in contexts with spatio-temporal data. In this paper, we fill this gap by proposing a XAI method specifically tailored for spatio-temporal data in sensor networks, where observations are collected at regular time intervals and at different locations. Our model-agnostic masking meta-optimization method for deep learning models uncovers global salient factors influencing model predictions, and generates explanations taking into account multiple analytical views, such as features, timesteps, and node locations. Our qualitative and quantitative experiments with real-world forecasting datasets show that our approach effectively extracts explanations of model predictions, and is competitive with state-of-the-art approaches.},
  archive      = {J_ML},
  author       = {Altieri, Massimiliano and Ceci, Michelangelo and Corizzo, Roberto},
  doi          = {10.1007/s10994-024-06733-6},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-47},
  shortjournal = {Mach. Learn.},
  title        = {An end-to-end explainability framework for spatio-temporal predictive modeling},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient descent fails to learn high-frequency functions and modular arithmetic. <em>ML</em>, <em>114</em>(4), 1-30. (<a href='https://doi.org/10.1007/s10994-025-06747-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classes of target functions containing a large number of approximately orthogonal elements are known to be hard to learn by the Statistical Query algorithms. Recently this classical fact re-emerged in a theory of gradient-based optimization of neural networks. In the novel framework, the hardness of a class is usually quantified by the variance of the gradient with respect to a random choice of a target function. A set of functions of the form $$x\rightarrow ax \bmod p$$ , where a is taken from $${{\mathbb {Z}}}_p$$ , has attracted some attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of p-periodic functions on $${{\mathbb {Z}}}$$ and is tightly connected with a class of high-frequency periodic functions on the real line. We present a mathematical analysis of limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We highlight that the variance of the gradient is negligibly small in both cases when either a frequency or the prime base p is large. This in turn prevents such a learning algorithm from being successful.},
  archive      = {J_ML},
  author       = {Takhanov, Rustem and Tezekbayev, Maxat and Pak, Artur and Bolatov, Arman and Assylbekov, Zhenisbek},
  doi          = {10.1007/s10994-025-06747-8},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {Gradient descent fails to learn high-frequency functions and modular arithmetic},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized median of means principle for bayesian inference. <em>ML</em>, <em>114</em>(4), 1-38. (<a href='https://doi.org/10.1007/s10994-025-06754-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of robustness is experiencing a resurgence of interest in the statistical and machine learning communities. In particular, robust algorithms making use of the so-called median of means estimator were shown to satisfy strong performance guarantees for many problems, including estimation of the mean, covariance structure as well as linear regression. In this work, we propose an extension of the median of means principle to the Bayesian framework, leading to the notion of the robust posterior distribution. In particular, we (a) quantify robustness of this posterior to outliers, (b) show that it satisfies a version of the Bernstein-von Mises theorem that connects Bayesian credible sets to the traditional confidence intervals, and (c) demonstrate that our approach performs well in applications.},
  archive      = {J_ML},
  author       = {Minsker, Stanislav and Yao, Shunan},
  doi          = {10.1007/s10994-025-06754-9},
  journal      = {Machine Learning},
  month        = {4},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Generalized median of means principle for bayesian inference},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new formulation of lipschitz constrained with functional gradient learning for GANs. <em>ML</em>, <em>114</em>(3), 1-54. (<a href='https://doi.org/10.1007/s10994-024-06633-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a promising alternative method for training Generative Adversarial Networks (GANs) on large-scale datasets with clear theoretical guarantees. GANs are typically learned through a minimax game between a generator and a discriminator, which is known to be empirically unstable. Previous learning paradigms have encountered mode collapse issues without a theoretical solution. To address these challenges, we propose a novel Lipschitz-constrained Functional Gradient GANs learning (Li-CFG) method to stabilize the training of GAN and provide a theoretical foundation for effectively increasing the diversity of synthetic samples by reducing the neighborhood size of the latent vector. Specifically, we demonstrate that the neighborhood size of the latent vector can be reduced by increasing the norm of the discriminator gradient, resulting in enhanced diversity of synthetic samples. To efficiently enlarge the norm of the discriminator gradient, we introduce a novel $${\varvec{\varepsilon }}$$ -centered gradient penalty that amplifies the norm of the discriminator gradient using the hyper-parameter $${\varvec{\varepsilon }}$$ . In comparison to other constraints, our method enlarging the discriminator norm, thus obtaining the smallest neighborhood size of the latent vector. Extensive experiments on benchmark datasets for image generation demonstrate the efficacy of the Li-CFG method and the $${\varvec{\varepsilon }}$$ -centered gradient penalty. The results showcase improved stability and increased diversity of synthetic samples.},
  archive      = {J_ML},
  author       = {Wan, Chang and Fan, Ke and Sun, Xinwei and Fu, Yanwei and Li, Minglu and Jiang, Yunliang and Zheng, Zhonglong},
  doi          = {10.1007/s10994-024-06633-9},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-54},
  shortjournal = {Mach. Learn.},
  title        = {A new formulation of lipschitz constrained with functional gradient learning for GANs},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse and smooth additive isotonic model in high-dimensional settings. <em>ML</em>, <em>114</em>(3), 1-23. (<a href='https://doi.org/10.1007/s10994-024-06641-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smooth additive isotonic models (SAIM) are used in trend analysis to model the response of nonparametric smooth monotone prediction functions. The relationship between multiple environmental indicators and environmental pollution is a typical SAIM case. Previous methods for estimating SAIM are sub-optimal and computationally expensive in high-dimensional settings, where the number of variables is larger than the number of samples. To address these problems, we hybridize a variable selection procedure with smooth additive isotonic models and propose a novel model called Sparse Smooth Additive Isotonic Model (SSAIM). Our model first solves the variable selection to reduce problem complexities and computational costs. Then, the smooth monotone prediction functions are efficiently estimated via a block coordinate gradient descent algorithm. We theoretically show that SSAIM achieves a state-of-the-art error bound. Experiments on multiple simulated and real-world datasets demonstrate that integrating sparsity and smoothness constraints in SSAIM helps improve model prediction accuracy and sparsity of large models. Moreover, SSAIM spends much less time compared with multivariate non-sparse models and the increment of time cost brought about by smoothness constraint is little compared with multivariate sparse models.},
  archive      = {J_ML},
  author       = {Zhang, Jiaqi and Wang, Yiqin and Wang, Meng and Wang, Beilun},
  doi          = {10.1007/s10994-024-06641-9},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Sparse and smooth additive isotonic model in high-dimensional settings},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable optimisation-based approach for hyper-box classification. <em>ML</em>, <em>114</em>(3), 1-40. (<a href='https://doi.org/10.1007/s10994-024-06643-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data classification is considered a fundamental research subject within the machine learning community. Researchers seek the improvement of machine learning algorithms in not only accuracy, but also interpretability. Interpretable algorithms allow humans to easily understand the decisions that a machine learning model makes, which is challenging for black box models. Mathematical programming-based classification algorithms have attracted considerable attention due to their ability to effectively compete with leading-edge algorithms in terms of both accuracy and interpretability. Meanwhile, the training of a hyper-box classifier can be mathematically formulated as a Mixed Integer Linear Programming (MILP) model and the predictions combine accuracy and interpretability. In this work, an optimisation-based approach is proposed for multi-class data classification using a hyper-box representation, thus facilitating the extraction of compact IF-THEN rules. The key novelty of our approach lies in the minimisation of the number and length of the generated rules for enhanced interpretability. Through a number of real-world datasets, it is demonstrated that the algorithm exhibits favorable performance when compared to well-known alternatives in terms of prediction accuracy and rule set simplicity.},
  archive      = {J_ML},
  author       = {Liapis, Georgios I. and Tsoka, Sophia and Papageorgiou, Lazaros G.},
  doi          = {10.1007/s10994-024-06643-7},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-40},
  shortjournal = {Mach. Learn.},
  title        = {Interpretable optimisation-based approach for hyper-box classification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated algorithms for convex and non-convex optimization on manifolds. <em>ML</em>, <em>114</em>(3), 1-24. (<a href='https://doi.org/10.1007/s10994-024-06649-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a general scheme for solving convex and non-convex optimization problems on manifolds. The central idea is that, by adding a multiple of the squared retraction distance to the objective function in question, we “convexify” the objective function and solve a series of convex sub-problems in the optimization procedure. Our proposed algorithm adapts to the level of complexity in the objective function without requiring the knowledge of the convexity of non-convexity of the objective function. We show that when the objective function is convex, the algorithm provably converges to the optimum and leads to accelerated convergence. When the objective function is non-convex, the algorithm will converge to a stationary point. Our proposed method unifies insights from Nesterov’s original idea for accelerating gradient descent algorithms with recent developments in optimization algorithms in Euclidean space. We demonstrate the utility of our algorithms on several manifold optimization tasks such as estimating intrinsic and extrinsic Fréchet means on spheres and low-rank matrix factorization with Grassmann manifolds applied to the Netflix rating data set.},
  archive      = {J_ML},
  author       = {Lin, Lizhen and Saparbayeva, Bayan and Zhang, Michael Minyi and Dunson, David B.},
  doi          = {10.1007/s10994-024-06649-1},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Accelerated algorithms for convex and non-convex optimization on manifolds},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-view data augmentation at subgraph level and graph contrastive learning for sequential recommendation. <em>ML</em>, <em>114</em>(3), 1-13. (<a href='https://doi.org/10.1007/s10994-024-06656-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing sequential recommendation algorithms generally face the problems of not fully utilizing item relationships across sequences, being incapable of effectively capturing users’ global preferences, and being susceptible to data sparsity. To address above problems, we propose a dual-view data augmentation at subgraph level and graph contrastive learning for sequential recommendation (DSGCL). Firstly, we construct a weighted sequential transition global graph and an item correlation global graph based on sequential interaction data to utilize item relationships across sequences. Secondly, in order to mitigate the data sparsity problem, we construct a pair of augmented subgraphs for each global graph by using data augmentation at subgraph level, and capture user’s global preferences by using graph neural networks on the augmented subgraphs. Consistency between the same user preference learnt from different augmented subgraphs is ensured by employing the graph contrastive learning method, which also helps to better distinguish the difference in preferences between users. Experimental results on multiple sequential recommendation datasets show that the DSGCL has a better performance compared to other advanced methods. Besides, ablation experiments validate the effectiveness of each module in the model.},
  archive      = {J_ML},
  author       = {Mu, Caihong and Yu, Haikun and Qin, Lang and Liu, Yi},
  doi          = {10.1007/s10994-024-06656-2},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Learn.},
  title        = {Dual-view data augmentation at subgraph level and graph contrastive learning for sequential recommendation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution aligned semantics adaption for lifelong person re-identification. <em>ML</em>, <em>114</em>(3), 1-22. (<a href='https://doi.org/10.1007/s10994-024-06657-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, person Re-IDentification (Re-ID) systems need to be adaptable to changes in space and time. Therefore, the adaptation of Re-ID models to new domains while preserving previously acquired knowledge is crucial, known as Lifelong person Re-IDentification (LReID). Advanced LReID methods rely on replaying exemplars from old domains and applying knowledge distillation in logits with old models. However, due to privacy concerns, retaining previous data is inappropriate. Additionally, the fine-grained and open-set characteristics of Re-ID limit the effectiveness of the distillation paradigm for accumulating knowledge. We argue that a Re-ID model trained on diverse and challenging pedestrian images at a large scale can acquire robust and general human semantic knowledge. These semantics can be readily utilized as shared knowledge for lifelong applications. In this paper, we identify the challenges and discrepancies associated with adapting a pre-trained model to each application domain, and introduce the Distribution Aligned Semantics Adaption (DASA) framework. It efficiently adjusts Batch Normalization (BN) to mitigate interference from data distribution discrepancy and freezes the pre-trained convolutional layers to preserve shared knowledge. Additionally, we propose the lightweight Semantics Adaption (SA) module, which effectively adapts learned semantics to enhance pedestrian representations. Extensive experiments demonstrate the remarkable superiority of our proposed framework over advanced LReID methods, and it exhibits significantly reduced storage consumption. DASA presents a novel and cost-effective perspective on effectively adapting pre-trained models for LReID.},
  archive      = {J_ML},
  author       = {Wang, Qizao and Qian, Xuelin and Li, Bin and Xue, Xiangyang},
  doi          = {10.1007/s10994-024-06657-1},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Mach. Learn.},
  title        = {Distribution aligned semantics adaption for lifelong person re-identification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PT4Rec: A universal prompt-tuning framework for graph contrastive learning-based recommendations. <em>ML</em>, <em>114</em>(3), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06658-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning-based recommendations have attracted a lot of research attention due to their exceptional performance. However, these approaches, which hinge on the optimization of downstream recommendations, often deviate from the original purpose of graph contrastive learning (i.e., learning embeddings independently of downstream tasks) and result in inconsistent performance. Some researchers have attempted to address this issue through prompt tuning, but the use of single and fixed prompts has shown limited efficacy and a lack of robustness across various benchmarks. To bridge the gap between graph contrastive learning and recommendation tasks, this paper proposes a universal Prompt-Tuning framework called PT4Rec. PT4Rec constructs learnable multi-prompts to capitalize on the flexibility of prompt tuning, thereby enhancing the robustness of graph contrastive learning-based recommendations in different scenarios. Specifically, PT4Rec first employs graph contrastive learning to enhance the pre-training embeddings of user and item nodes. Then it integrates multiple prompts derived from user profile inputs with user embeddings by the attention mechanism for prompt-tuning on downstream recommendation tasks. PT4Rec extends the prompt-tuning technique to adapt to various recommendation scenarios and constructs learnable multi-prompts to achieve better recommendation performance and scalability. Experimental validation on four benchmark datasets demonstrates the effectiveness of PT4Rec. The code is released at https://github.com/xiaowei-i/PT4Rec .},
  archive      = {J_ML},
  author       = {Xiao, Wei and Zhou, Qifeng},
  doi          = {10.1007/s10994-024-06658-0},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {PT4Rec: A universal prompt-tuning framework for graph contrastive learning-based recommendations},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust query performance prediction for dense retrievers via adaptive disturbance generation. <em>ML</em>, <em>114</em>(3), 1-23. (<a href='https://doi.org/10.1007/s10994-024-06659-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces ADG-QPP (Adaptive Disturbance Generation), an unsupervised Query Performance Prediction (QPP) method designed specifically for dense neural retrievers. The underlying foundation of ADG-QPP is to measure query performance based on its degree of robustness towards perturbations. Traditional QPP methods rely on predefined lexical perturbations on the query, which only apply to sparse retrieval methods and fail to maintain consistent performance across different datasets. In our work, we address these limitations by perturbing the query by injecting disturbance leveraged by the focal network-based measurements including node-based, edge-based, and cluster-based metrics, into its neural embedding representation. Rather than applying the same perturbation across all queries, our approach develops an instance-wise disturbance for each query that is then used for its perturbation. Through extensive experiments on three benchmark datasets, we demonstrate that ADG-QPP outperforms state-of-the-art baselines in terms of Kendall $$\tau$$ , Spearman $$\rho$$ , and Pearson’s $$\rho$$ correlations.},
  archive      = {J_ML},
  author       = {Saleminezhad, Abbas and Arabzadeh, Negar and Rad, Radin Hamidi and Beheshti, Soosan and Bagheri, Ebrahim},
  doi          = {10.1007/s10994-024-06659-z},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {Robust query performance prediction for dense retrievers via adaptive disturbance generation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Per-example gradient regularization improves learning signals from noisy data. <em>ML</em>, <em>114</em>(3), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06661-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient regularization, as described in Barrett and Dherin (in: International conference on learning representations, 2021), is a highly effective technique for promoting flat minima during gradient descent. Empirical evidence suggests that this regularization technique can significantly enhance the robustness of deep learning models against noisy perturbations, while also reducing test error. In this paper, we explore the per-example gradient regularization (PEGR) and present a theoretical analysis that demonstrates its effectiveness in improving both test error and robustness against noise perturbations. Specifically, we adopt a signal-noise data model from Cao et al. (Adv Neural Inf Process Syst 35:25237–25250, 2022) and show that PEGR can learn signals effectively while suppressing noise memorization. In contrast, standard gradient descent struggles to distinguish the signal from the noise, leading to suboptimal generalization performance. Our analysis reveals that PEGR penalizes the variance of pattern learning, thus effectively suppressing the memorization of noises from the training data. These findings underscore the importance of variance control in deep learning training and offer useful insights for developing more effective training approaches.},
  archive      = {J_ML},
  author       = {Meng, Xuran and Cao, Yuan and Zou, Difan},
  doi          = {10.1007/s10994-024-06661-5},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Per-example gradient regularization improves learning signals from noisy data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive adapter routing for long-tailed class-incremental learning. <em>ML</em>, <em>114</em>(3), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06662-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our ever-evolving world, new data exhibits a long-tailed distribution, such as emerging images in varying amounts. This necessitates continuous model learning imbalanced data without forgetting, addressing the challenge of long-tailed class-incremental learning (LTCIL). Existing methods often rely on retraining linear classifiers with former data, which is impractical in real-world settings. In this paper, we harness the potent representation capabilities of pre-trained models and introduce AdaPtive Adapter RouTing (Apart) as an exemplar-free solution for LTCIL. To counteract forgetting, we train inserted adapters with frozen pre-trained weights for deeper adaptation and maintain a pool of adapters for selection during sequential model updates. Additionally, we present an auxiliary adapter pool designed for effective generalization, especially on minority classes. Adaptive instance routing across these pools captures crucial correlations, facilitating a comprehensive representation of all classes. Consequently, Apart tackles the imbalance problem as well as catastrophic forgetting in a unified framework. Extensive benchmark experiments validate the effectiveness of Apart.},
  archive      = {J_ML},
  author       = {Qi, Zhi-Hong and Zhou, Da-Wei and Yao, Yiran and Ye, Han-Jia and Zhan, De-Chuan},
  doi          = {10.1007/s10994-024-06662-4},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Adaptive adapter routing for long-tailed class-incremental learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthesizing efficient data with diffusion models for person re-identification pre-training. <em>ML</em>, <em>114</em>(3), 1-25. (<a href='https://doi.org/10.1007/s10994-024-06663-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing person re-identification (Re-ID) methods principally deploy the ImageNet-1K dataset for model initialization, which inevitably results in sub-optimal situations due to the large domain gap. One of the key challenges is that building large-scale person Re-ID datasets is time-consuming. Some previous efforts address this problem by collecting person images from the internet (e.g., LUPerson), but it struggles to learn from unlabeled, uncontrollable, and noisy data. In this paper, we present a novel paradigm Diffusion-ReID to efficiently augment and generate diverse images based on known identities without requiring any cost of data collection and annotation. Technically, this paradigm unfolds in two stages: generation and filtering. During the generation stage, we propose Language Prompts Enhancement (LPE) to ensure the ID consistency between the input image sequence and the generated images. In the diffusion process, we propose a Diversity Injection (DI) module to increase attribute diversity. In order to make the generated data have higher quality, we apply a Re-ID confidence threshold filter to further remove the low-quality images. Benefiting from our proposed paradigm, we first create a new large-scale person Re-ID dataset Diff-Person, which consists of over 777K images from 5,183 identities. Next, we build a stronger person Re-ID backbone pre-trained on our Diff-Person. Extensive experiments are conducted on four person Re-ID benchmarks in six widely used settings. Compared with other pre-training and self-supervised competitors, our approach shows significant superiority. Our codes and datasets will be available at https://github.com/KeNiu042/Diffusion-ReID .},
  archive      = {J_ML},
  author       = {Niu, Ke and Yu, Haiyang and Qian, Xuelin and Fu, Teng and Li, Bin and Xue, Xiangyang},
  doi          = {10.1007/s10994-024-06663-3},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Synthesizing efficient data with diffusion models for person re-identification pre-training},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-aware contrastive learning via multi-prompt alignment. <em>ML</em>, <em>114</em>(3), 1-21. (<a href='https://doi.org/10.1007/s10994-024-06665-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of the sample generation mechanism in contrastive learning is pivotal. It not only determines the pairings of positive and negative samples but also enriches the diversity of the sample pool, thereby substantially affecting the quality of the learned representations. Yet, maintaining semantic consistency within positive sample pairs and amplifying sample diversity remain persistent hurdles. To address these challenges, this paper investigates the potential of synthesizing semantically consistent samples by leveraging multi-source and multi-modal prompts, guided by the capabilities of Large Multimodal Models. Through a concise and elegant design, we construct a framework capable of generating semantic-aware positive sample pairs. Based on this framework, we delve deeper into the crucial role of semantic consistency in representation learning through visualization and ablation experiments. Additionally, we systematically outline the fundamental principles and universal methods for generating synthetic samples in contrastive learning using large model techniques. Extensive experimental results prove the superior performance of our method and help us uncover related patterns. We will make all the code and generated datasets publicly available.},
  archive      = {J_ML},
  author       = {Zhao, Zhuoran and Qin, Hao and Kong, Ming and Chen, Luyuan and Xie, Di and Zhu, Jiang and Zhu, Qiang},
  doi          = {10.1007/s10994-024-06665-1},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {Semantic-aware contrastive learning via multi-prompt alignment},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive semantic learning for unsupervised skeleton-based action recognition. <em>ML</em>, <em>114</em>(3), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06667-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional contrastive learning frameworks for skeleton-based action recognition use data augmentation and memory bank techniques to obtain positive/negative samples required for training, but this instance-level pseudo-label generation mechanism does not take full advantage of the rich cluster-level semantic information contained in human skeleton sequences. In this paper, we propose a Progressive Semantic Learning method (ProSL), which gradually optimizes the pseudo-label generation mechanism in self-supervised contrastive learning through an iterative framework, so that representation learning can effectively capture action semantic information. Specifically, the existing contrastive learning methods can output an initial skeleton encoder. Then, on the basis of this encoder, clustering methods can be applied to generate a Codebook containing the semantic information of human actions, which is further used to improve the pseudo-label generation mechanism. Finally, based on the above two-step iterations, we achieve progressive semantic learning and obtain a more reasonable skeleton encoder. Extensive experiments on four datasets demonstrate that our proposed method achieves SOTA on multiple downstream tasks.},
  archive      = {J_ML},
  author       = {Qin, Hao and Chen, Luyuan and Kong, Ming and Zhao, Zhuoran and Zeng, Xianzhou and Lu, Mengxu and Zhu, Qiang},
  doi          = {10.1007/s10994-024-06667-z},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Progressive semantic learning for unsupervised skeleton-based action recognition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Schema-tune: Noise-driven bias mitigation in transformer-based language models. <em>ML</em>, <em>114</em>(3), 1-25. (<a href='https://doi.org/10.1007/s10994-024-06670-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce Schema-Tune, a zero-shot self-supervised framework for bias mitigation in transformer-based language models. Schema-Tune introduces curated and optimized adaptive noises to the input embeddings of transformer models to challenge the models’ embedded stereotypes. Through continuous fine-tuning steps, these noises prompt the models to change their internal semantic representations towards more socially fair representations. For fine-tuning language models, Schema-Tune relies on very limited input data: a couple of sentences formed by social group terms. Additionally, Schema-Tune defines bias and language model performance measures independently from labeled data. These measures are then used in forming the language model’s fine-tuning objective function and in searching for effective noises in the embedding space. Experimental evaluation over the StereoSet and Crows-Pairs datasets confirms that Schema-Tune is effective in mitigating bias in different social stereotype categories, including gender, race, and religion.},
  archive      = {J_ML},
  author       = {Shokrollahi, Omid and Penumatcha, Ruthvik and Ensan, Faezeh and Noorian, Zeinab},
  doi          = {10.1007/s10994-024-06670-4},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Schema-tune: Noise-driven bias mitigation in transformer-based language models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SketchMLP: Effectively utilize rasterized images and drawing sequences for sketch recognition. <em>ML</em>, <em>114</em>(3), 1-18. (<a href='https://doi.org/10.1007/s10994-024-06677-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a versatile tool for effectively communicating abstract concepts, freehand sketching has garnered significant attention and has undergone extensive exploration in computer vision. Sketch recognition is one of the fundamental and challenging tasks. Sketches are composed of a limited number of sparse and simple lines, rendering them challenging to identify as natural images through the use of texture and color. Consequently, the introduction of information about the drawing order and graph structure represents a significant avenue for improving the performance of the model. The previous deep learning methods often necessitate the deployment of considerable expertise to process the sketches, including the application of specific rules to the coloring of black-and-white sketches, the design of hierarchical graphic representation structures, and the decoupling of strokes. But, does sketch recognition necessarily require expert knowledge when the dataset is sufficient? In this paper, we propose a multi-layer perceptron (MLPs) based model, namely SketchMLP, for sketch recognition, which directly takes sketch images and drawing sequences as inputs avoiding the dependence on complex data processing expert knowledge. SketchMLP utilizes a gated MLP (gMLP) based sequence branch to discover the relationships between points in the drawing sequence and an axial shifted MLP (AS-MLP) based image branch to extract local visual features. The two branches of the model produce their own recognition decisions, which are then combined by a mixture of expert (MoE) block to generate the final result. The MoE block automatically assigns weight for different branches based on the sketch instance. Sufficient experimental results show the effectiveness of SketchMLP. On the largest sketch dataset, QuickDraw, SketchMLP demonstrates superior performance to the state-of-the-art method SketchXAI in terms of recognition accuracy. This is achieved with a significantly reduced parameter count, about only 41.3% of SketchXAI. The code is available at https://github.com/CMACH508/SketchMLP .},
  archive      = {J_ML},
  author       = {Li, Tengjie and Tu, Shikui and Xu, Lei},
  doi          = {10.1007/s10994-024-06677-x},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Mach. Learn.},
  title        = {SketchMLP: Effectively utilize rasterized images and drawing sequences for sketch recognition},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sandbox: Safeguarded multi-label learning through safe optimal transport. <em>ML</em>, <em>114</em>(3), 1-29. (<a href='https://doi.org/10.1007/s10994-024-06678-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning with label noise presents significant real-world challenges due to dependencies among labels, complicating the transition from clean to noisy labels. Mainstream approaches, such as robust loss functions, and noise transition models, often fall short due to their high sensitivity to noise rate or transition matrix estimation, especially on complex datasets. To address these challenges, we introduce a novel Sandbox mechanism, which iteratively estimates multiple labels. Unlike traditional methods that depend on an explicit, often restrictive linear transition matrix, Sandbox mechanism utilizes an implicit optimal transport process, constraining label refinements between noisy labels and model predictions within a predefined polytope, effectively limiting error propagation, enhancing stability, and offering greater flexibility. In each iteration, we develop a simple yet effective method, termed as Safe Optimal Transport (SOT), to refine noisy labels more reliably towards the ground truth. By involving the interpolated references and complementary orientations, SOT effectively estimates true labels using the Sinkhorn-Knopp algorithm. Our extensive evaluations on various benchmark datasets demonstrate that Sandbox consistently outperforms existing state-of-the-art techniques. Comprehensive ablation studies further elucidate its effectiveness.},
  archive      = {J_ML},
  author       = {Zhang, Lefei and Yu, Geng and Yao, Jiangchao and Ong, Yew-soon and Tsang, Ivor W. and Kwok, James T.},
  doi          = {10.1007/s10994-024-06678-w},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Sandbox: Safeguarded multi-label learning through safe optimal transport},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient pareto-optimal utility-fairness between groups in repeated rankings. <em>ML</em>, <em>114</em>(3), 1-18. (<a href='https://doi.org/10.1007/s10994-024-06679-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we tackle the problem of computing an expectation of ranking with the guarantee of the Pareto-optimal balance between (1) maximizing the utility of the consumers and (2) minimizing the exposure unfairness between item producers. Such a multi-objective optimization problem is typically solved using a scalarization method and linear programming on bi-stochastic matrices, representing the distribution of possible rankings of items. However, such an approach relies on Birkhoff-von Neumann (BvN) decomposition, which is computationally impractical for large-scale systems. To address this issue, we introduce a novel approach to the above problem by using the Expohedron—a permutahedron whose points represent all achievable exposures of items. On the Expohedron, we profile the Pareto curve, which captures the trade-off between group fairness and user utility by identifying a finite number of Pareto optimal solutions. We propose an efficient method by relaxing our optimization problem on the Expohedron’s circumscribed n-sphere, significantly improving the running time. Moreover, the approximate Pareto curve is asymptotically close to the natural Pareto optimal curve as the number of substantial solutions increases. Our methods are applicable with different ranking merits that are non-decreasing functions of item relevance. The effectiveness of our strategies is validated through experiments on both synthetic and real-world datasets.},
  archive      = {J_ML},
  author       = {Dinh, Phuong Mai and Le, Duc-Trong and Hoang, Tuan-Anh and Le, Dung Duy},
  doi          = {10.1007/s10994-024-06679-9},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Mach. Learn.},
  title        = {Towards efficient pareto-optimal utility-fairness between groups in repeated rankings},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SA-CAM: Semantic-aware visual explanations for deep convolutional networks. <em>ML</em>, <em>114</em>(3), 1-17. (<a href='https://doi.org/10.1007/s10994-024-06688-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explaining deep convolutional networks is a long-standing problem. However, existing Class Activation Mapping (CAM) based methods often produce saliency maps with insufficient authentic information or involve time-consuming generating processes. In view of these challenges, we propose Semantic-Aware Class Activation Mapping (SA-CAM), an effective and efficient post-hoc visual explanation method that considers the semantic correlation of activation maps during saliency map generation. Additionally, in order to reduce the computational cost of multiplying the activation map with the original feature map, we partition them into semantic-related clusters to preserve the input sub-pixels and subsequently sum the sub-activations as an initial mask to calculate the weights. Extensive experiments on STL-10, ImageNet-1k, ImageNet Segmentation, and MS COCO2017 datasets demonstrate that SA-CAM outperforms current state-of-the-art explanation approaches. Furthermore, we experimentally highlight the potential of SA-CAM as an effective data augmentation strategy for fine-tuning, as it requires only dozens of queries to generate accurate class-specific saliency maps.},
  archive      = {J_ML},
  author       = {Yu, Anni and Zhang, Qing-Long and Rao, Lu and Yang, Yu-Bin},
  doi          = {10.1007/s10994-024-06688-8},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Mach. Learn.},
  title        = {SA-CAM: Semantic-aware visual explanations for deep convolutional networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward finding strong pareto optimal policies in multi-agent reinforcement learning. <em>ML</em>, <em>114</em>(3), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06700-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study the problem of finding Pareto optimal policies in multi-agent reinforcement learning problems with cooperative reward structures. We show that any algorithm where each agent only optimizes their reward is subject to suboptimal convergence. Therefore, to achieve Pareto optimality, agents have to act altruistically by considering the rewards of others. This observation bridges the multi-objective optimization framework and multi-agent reinforcement learning together. We first propose a framework for applying the Multiple Gradient Descent algorithm (MGDA) for learning in multi-agent settings. We further show that standard MGDA is subjected to weak Pareto convergence, a problem that is often overlooked in other learning settings but is prevalent in multi-agent reinforcement learning. To mitigate this issue, we propose MGDA++, an improvement of the existing algorithm to handle the weakly optimal convergence of MGDA properly. Theoretically, we prove that MGDA++ converges to strong Pareto optimal solutions in convex, smooth bi-objective problems. We further demonstrate the superiority of our MGDA++ in cooperative settings in the Gridworld benchmark. The results highlight that our proposed method can converge efficiently and outperform the other methods in terms of the optimality of the convergent policies. The source code is available at https://github.com/giangbang/Strong-Pareto-MARL .},
  archive      = {J_ML},
  author       = {Le, Bang Giang and Ta, Viet Cuong},
  doi          = {10.1007/s10994-024-06700-1},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Toward finding strong pareto optimal policies in multi-agent reinforcement learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformalised data synthesis. <em>ML</em>, <em>114</em>(3), 1-37. (<a href='https://doi.org/10.1007/s10994-024-06701-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of increasingly complicated Deep Learning architectures, data synthesis is a highly promising technique to address the demand of data-hungry models. However, reliably assessing the quality of a ‘synthesiser’ model’s output is an open research question with significant associated risks for high-stake domains. To address this challenge, we propose a unique synthesis algorithm that generates data from high-confidence feature space regions based on the Conformal Prediction framework. We support our proposed algorithm with a comprehensive exploration of the core parameter’s influence, an in-depth discussion of practical advice, and an extensive empirical evaluation of five benchmark datasets. To show our approach’s versatility on ubiquitous real-world challenges, the datasets were carefully selected for their variety of difficult characteristics: low sample count, class imbalance, and non-separability. In all trials, training sets extended with our confident synthesised data performed at least as well as the original set and frequently significantly improved Deep Learning performance by up to 61% points $$\hbox {F}_1$$ -score.},
  archive      = {J_ML},
  author       = {Meister, Julia A. and Nguyen, Khuong An},
  doi          = {10.1007/s10994-024-06701-0},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-37},
  shortjournal = {Mach. Learn.},
  title        = {Conformalised data synthesis},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic scoring lists for interpretable machine learning. <em>ML</em>, <em>114</em>(3), 1-31. (<a href='https://doi.org/10.1007/s10994-024-06705-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A scoring system is a simple decision model that checks a set of features, adds a certain number of points to a total score for each feature that is satisfied, and finally makes a decision by comparing the total score to a threshold. Scoring systems have a long history of active use in safety-critical domains such as healthcare and justice, where they provide guidance for making objective and accurate decisions. Given their genuine interpretability, the idea of learning scoring systems from data is obviously appealing from the perspective of explainable AI. In this paper, we propose a practically motivated extension of scoring systems called probabilistic scoring lists (PSL), as well as a method for learning PSLs from data. Instead of making a deterministic decision, a PSL represents uncertainty in the form of probability distributions, or, more generally, probability intervals. Moreover, in the spirit of decision lists, a PSL evaluates features one by one and stops as soon as a decision can be made with enough confidence. To evaluate our approach, we conduct case studies in the medical domain and on standard benchmark data.},
  archive      = {J_ML},
  author       = {Hanselle, Jonas and Heid, Stefan and Fürnkranz, Johannes and Hüllermeier, Eyke},
  doi          = {10.1007/s10994-024-06705-w},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Mach. Learn.},
  title        = {Probabilistic scoring lists for interpretable machine learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal load prediction with transductive graph autoencoders. <em>ML</em>, <em>114</em>(3), 1-22. (<a href='https://doi.org/10.1007/s10994-024-06713-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting edge weights on graphs has various applications, from transportation systems to social networks. This paper describes a Graph Neural Network (GNN) approach for edge weight prediction with guaranteed coverage. We leverage conformal prediction to calibrate the GNN outputs and produce valid prediction intervals. We handle data heteroscedasticity through error reweighting and Conformalized Quantile Regression (CQR). We compare the performance of our method against baseline techniques on real-world transportation datasets. Our approach has better coverage and efficiency than all baselines and showcases robustness and adaptability.},
  archive      = {J_ML},
  author       = {Luo, Rui and Colombo, Nicolo},
  doi          = {10.1007/s10994-024-06713-w},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Mach. Learn.},
  title        = {Conformal load prediction with transductive graph autoencoders},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid additive modeling with partial dependence for supervised regression and dynamical systems forecasting. <em>ML</em>, <em>114</em>(3), 1-31. (<a href='https://doi.org/10.1007/s10994-025-06740-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning processes by exploiting restricted domain knowledge is an important task across a plethora of scientific areas, with more and more hybrid training methods additively combining data-driven and model-based approaches. Although the obtained models are more accurate than purely data-driven models, the optimization process usually comes with sensitive regularization constraints. Furthermore, while such hybrid methods have been tested in various scientific applications, they have been mostly tested on dynamical systems, with only limited study about the influence of each model component on global performance and parameter identification. In this work, we introduce a new hybrid training approach based on partial dependence, which removes the need for intricate regularization. Moreover, we assess the performance of hybrid modeling against traditional machine learning methods on standard regression problems. We compare, on both synthetic and real regression problems, several approaches for training such hybrid models. We focus on hybrid methods that additively combine a parametric term with a machine learning term and investigate model-agnostic training procedures. Therefore, experiments are carried out with different types of machine learning models, including tree-based models and artificial neural networks. We also extend our partial dependence optimization process for dynamical systems forecasting and compare it to existing schemes.},
  archive      = {J_ML},
  author       = {Claes, Yann and Huynh-Thu, Vân Anh and Geurts, Pierre},
  doi          = {10.1007/s10994-025-06740-1},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Mach. Learn.},
  title        = {Hybrid additive modeling with partial dependence for supervised regression and dynamical systems forecasting},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alignclip: Navigating the misalignments for robust vision-language generalization. <em>ML</em>, <em>114</em>(3), 1-19. (<a href='https://doi.org/10.1007/s10994-025-06742-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of Vision-Language Pretraining models, achieving robust and adaptive representations is a cornerstone for successfully handling the unpredictability of real-world scenarios. This paper delves into two pivotal misalignment challenges inherent to Contrastive Language-Image Pre-training (CLIP) models: attention misalignment, which leads to an overemphasis on background elements rather than salient objects, and predictive category misalignment, characterized by the model’s struggle to discern between classes based on similarity. These misalignments undermine the representational stability essential for dynamic, real-world applications. To address these challenges, we propose AlignCLIP, an advanced fine-tuning methodology distinguished by its attention alignment loss, designed to calibrate the distribution of attention across multi-head attention layers. Furthermore, AlignCLIP introduces semantic label smoothing, a technique that leverages textual class similarities to refine prediction hierarchies. Through comprehensive experimentation on a variety of datasets and in scenarios involving distribution shifts and unseen classes, we demonstrate that AlignCLIP significantly enhances the stability of representations and shows superior generalization capabilities.},
  archive      = {J_ML},
  author       = {Han, Zhongyi and Luo, Gongxu and Sun, Hao and Li, Yaqian and Han, Bo and Gong, Mingming and Zhang, Kun and Liu, Tongliang},
  doi          = {10.1007/s10994-025-06742-z},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Alignclip: Navigating the misalignments for robust vision-language generalization},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CaCOM: Customizing text-to-image diffusion models in the wild via continual active selection. <em>ML</em>, <em>114</em>(3), 1-22. (<a href='https://doi.org/10.1007/s10994-025-06743-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is recently been pointed out that powerful diffusion models perform quite poorly and unstably in generating unseen or unknown concept tokens. Indeed, several solutions—such as LoRA and DreamBooth—were carried out to mitigate this problem. However, in this work, we first identify that these studies have been ubiquitously limited by a preset dataset and fixed number of concept tokens, which is generally impractical to the production setup. This is because the concept tokens, together with the dataset, are generically dynamic and alter all the time. Therefore, in this work, we propose CaCOM to cope with the underlying research challenges in this realistic setup, such as the catastrophic forgetting problem, etc. In brief words, CaCOM conducts careful selection over both training data and the memory bank, based on the data stream continuously given in the wild. We deem CaCOM as (i) a pioneering attempt to bring the customization closer to the production setting and (ii) a provably viable extension to the existing customization schemes. Through extensive empirical experiments, we show that CaCOM can easily be adapted to any of the customization modules while consistently enhancing them.},
  archive      = {J_ML},
  author       = {Yang, Jianan and Zhang, Yanming and Wang, Haobo and Chen, Gang and Wu, Sai and Zhao, Junbo},
  doi          = {10.1007/s10994-025-06743-y},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Mach. Learn.},
  title        = {CaCOM: Customizing text-to-image diffusion models in the wild via continual active selection},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning of PDE correction and mesh adaption without automatic differentiation. <em>ML</em>, <em>114</em>(3), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06746-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has shown promise in solving partial differential equations (PDEs) in computational fluid dynamics, particularly for enhancing solutions from coarse-mesh simulations. However, integrating deep learning with traditional PDE solvers requires these solvers to support automatic differentiation, a feature often unavailable in existing black-box solvers. This study explores a novel training framework for hybrid models combining a black-box PDE solver and a graph neural network. By replacing the gradient of the mesh nodes positions with its estimation, we optimize both mesh parameters and network weights without requiring solver differentiation. Although the method underperforms exact differentiation in some cases, it surpasses models trained on fixed meshes. With a warm-start strategy, we achieve faster convergence and improved generalization. Our approach demonstrates that effective hybrid modeling is possible even with non-differentiable solvers, expanding accessibility to standard PDE correction workflows.},
  archive      = {J_ML},
  author       = {Ma, Shaocong and Diffenderfer, James and Kailkhura, Bhavya and Zhou, Yi},
  doi          = {10.1007/s10994-025-06746-9},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Deep learning of PDE correction and mesh adaption without automatic differentiation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Post-selection confidence bounds for prediction performance. <em>ML</em>, <em>114</em>(3), 1-32. (<a href='https://doi.org/10.1007/s10994-024-06632-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, the selection of a promising model from a potentially large number of competing models and the assessment of its generalization performance are critical tasks that need careful consideration. Typically, model selection and evaluation are strictly separated tasks, splitting the sample at hand into training, validation, and evaluation sets, and only computing a single confidence interval for the prediction performance of the final selected model. We however regard the selection problem as a simultaneous inference problem and propose an algorithm to compute valid lower confidence bounds for multiple models that have been selected based on their prediction performance in the evaluation set. For this, we use bootstrap tilting and a maxT-type multiplicity correction. Various simulation experiments show that this leads to lower confidence bounds for the conditional performance that are at least as good as bounds from standard methods, and that reliably reach the nominal coverage probability. Also, a better performing final prediction model is selected this way, especially when the sample size is small. The approach is universally applicable for any combination of prediction models, any model selection strategy, and any prediction performance measure that accepts weights.},
  archive      = {J_ML},
  author       = {Rink, Pascal and Brannath, Werner},
  doi          = {10.1007/s10994-024-06632-w},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-32},
  shortjournal = {Mach. Learn.},
  title        = {Post-selection confidence bounds for prediction performance},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaling the weight parameters in markov logic networks and relational logistic regression models. <em>ML</em>, <em>114</em>(3), 1-28. (<a href='https://doi.org/10.1007/s10994-024-06635-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extrapolation with domain size has received plenty of attention recently, both in its own right and as part of the broader issue of scaling inference and learning to large domains. We consider Markov logic networks and relational logistic regression as two fundamental representation formalisms in statistical relational artificial intelligence that use weighted formulas in their specification. However, Markov logic networks are based on undirected graphs, while relational logistic regression is based on directed acyclic graphs. We show that when scaling the weight parameters with the domain size, the asymptotic behaviour of a relational logistic regression model is transparently controlled by the parameters, and we supply an algorithm to compute asymptotic probabilities. We show using two examples that this is not true for Markov logic networks. We also discuss using several examples, mainly from the literature, how the application context can help the user to decide when such scaling is appropriate and when using the raw unscaled parameters might be preferable.},
  archive      = {J_ML},
  author       = {Weitkämper, Felix Q.},
  doi          = {10.1007/s10994-024-06635-7},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Scaling the weight parameters in markov logic networks and relational logistic regression models},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal-view member preference contrastive representation learning for group recommendation. <em>ML</em>, <em>114</em>(3), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06655-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group recommendation (GR) plays a crucial role in social platforms, aiming to recommend items to entire groups based on collective interaction behaviors. Existing GR models predominantly focus on aggregating information from the view of spatial graph structure to infer preferences of members and groups. However, these models neglect the temporal relations among interactions that indicate the dynamic evolution of preferences, and fail to further integrate both the spatial and temporal features. To address these limitations, we propose a novel model called SpatioTemporal-view Member Preference Contrastive Representation Learning (STMP-CRL) for GR. The STMP-CRL can explicitly capture dynamic member preferences in the temporal view and seamlessly integrate them with spatial features to enhance member representation quality. Specifically, a GRU-based context dynamic encoder is proposed for the temporal view modeling to capture the dynamic member preferences. Additionally, to co-model member preferences in both the spatial and temporal views, a spatiotemporal-view joint encoding module is carefully devised. Furthermore, we propose a contrastive fusion mechanism based on independence modeling, which effectively integrates the spatial and temporal features via a disentangle-and-fuse strategy, enhancing the overall quality of member representations. Experimental results on two real datasets showcase the superiority of our STMP-CRL model over mainstream GR models, as evidenced by notable improvements in HR and NDCG metrics. Our implementations are available at https://github.com/STMP-CRL/STMP-CRL .},
  archive      = {J_ML},
  author       = {Zhou, Yangtao and Li, Qingshan and Chu, Hua and Li, Jianan and Wei, Biaobiao and Zhang, Shuai and Han, Jialong},
  doi          = {10.1007/s10994-024-06655-3},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Spatiotemporal-view member preference contrastive representation learning for group recommendation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Verification on out-of-distribution detectors under natural perturbations. <em>ML</em>, <em>114</em>(3), 1-21. (<a href='https://doi.org/10.1007/s10994-024-06666-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) detectors play a vital role in distinguishing between OOD data and in-distribution data. However, the vulnerability of OOD detectors to natural perturbations, such as rotation and lighting variations, can potentially lead to catastrophic accidents in safety-critical applications. Current attack techniques lack robustness guarantees for OOD detectors. Neural network (NN) verification methods are limited to standard NN structures and cannot be applied to OOD detectors due to their non-standard structure. To address this issue, we propose a verification framework called Vood that offers robustness guarantees for OOD detectors under natural perturbations. Our approach begins by proving the Lipschitz continuity of most OOD detection functions under natural transformations. We then estimate the Lipschitz constant using Extreme Value Theory, incorporating a dynamically estimated safety factor. Vood transforms the verification problem into an optimization challenge, which is then effectively addressed using space-filling Lipschitz optimization techniques. Additionally, Vood is a black-box verifier, which can tackle natural perturbations on a wide range of OOD detectors. Through empirical analysis, we demonstrate that Vood outperforms baseline methods in both accuracy and efficiency. Our work represents a pioneering effort in establishing robustness verification for OOD detectors with provable guarantees.},
  archive      = {J_ML},
  author       = {Zhang, Chi and Chen, Zhen and Xu, Peipei and Min, Geyong and Ruan, Wenjie},
  doi          = {10.1007/s10994-024-06666-0},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {Verification on out-of-distribution detectors under natural perturbations},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Commonsense knowledge enhanced event graph representation learning for script event prediction. <em>ML</em>, <em>114</em>(3), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06669-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Script event prediction plays an important role in many artificial intelligence applications. A key challenge in this task is accurately understanding the correlation between events and then inferring the subsequent events. Benefiting from exploring the rich connections among events, recently some event graph-based methods have achieved remarkable success. However, limited by the abstract representation of events, it is often difficult to derive the event relationships fully and precisely from the original event graphs. In this paper, we propose a novel framework, called Commonsense knowledge-enhanced event evolutionary graph (CEEG), to remedy this problem. CEEG constructs two core modules, event evolutionary graph and event commonsense graph, the former incorporates connections among the predicate events and object events to find the potential information among events, while the latter utilizes external commonsense knowledge to further enhance the understanding of event relationships. Pre-trained model RoBERTa and Graph Neural Network are also integrated into the framework to obtain more accuracy event nodes depiction and rich event graph information. Comprehensive experimental results on the benchmark dataset show the effectiveness of the proposed framework.},
  archive      = {J_ML},
  author       = {Li, Xiang and Jiang, Xinxi and Zhou, Qifeng},
  doi          = {10.1007/s10994-024-06669-x},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Commonsense knowledge enhanced event graph representation learning for script event prediction},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Re-assessing accuracy degradation: A framework for understanding DNN behavior on similar-but-non-identical test datasets. <em>ML</em>, <em>114</em>(3), 1-22. (<a href='https://doi.org/10.1007/s10994-024-06693-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) often demonstrate remarkable performance when evaluated on the test dataset used during model creation. However, their ability to generalize effectively when deployed is crucial, especially in critical applications. One approach to assess the generalization capability of a DNN model is to evaluate its performance on replicated test datasets, which are created by closely following the same methodology and procedures used to generate the original test dataset. Our investigation focuses on the performance degradation of pre-trained DNN models in multi-class classification tasks when evaluated on these replicated datasets; this performance degradation has not been entirely explained by generalization shortcomings or dataset disparities. To address this, we introduce a new evaluation framework that leverages uncertainty estimates generated by the models studied. This framework is designed to isolate the impact of variations in the evaluated test datasets and assess DNNs based on the consistency of their confidence in their predictions. By employing this framework, we can determine whether an observed performance drop is primarily caused by model inadequacy or other factors. We applied our framework to analyze 564 pre-trained DNN models across the CIFAR-10 and ImageNet benchmarks, along with their replicated versions. Contrary to common assumptions about model inadequacy, our results indicate a substantial reduction in the performance gap between the original and replicated datasets when accounting for model uncertainty. This suggests a previously unrecognized adaptability of models to minor dataset variations. Our findings emphasize the importance of understanding dataset intricacies and adopting more nuanced evaluation methods when assessing DNN model performance. This research contributes to the development of more robust and reliable DNN models, especially in critical applications where generalization performance is of utmost importance. The code to reproduce our experiments will be available at https://github.com/esla/Reassessing_DNN_Accuracy .},
  archive      = {J_ML},
  author       = {Anzaku, Esla Timothy and Wang, Haohan and Babalola, Ajiboye and Van Messem, Arnout and De Neve, Wesley},
  doi          = {10.1007/s10994-024-06693-x},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Mach. Learn.},
  title        = {Re-assessing accuracy degradation: A framework for understanding DNN behavior on similar-but-non-identical test datasets},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new geometry-aware non-euclidean distance metric. <em>ML</em>, <em>114</em>(3), 1-27. (<a href='https://doi.org/10.1007/s10994-024-06702-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many machine learning algorithms use Euclidean distance as a common metric to calculate similarities between data. However, Euclidean distance is not valid when data lie on a manifold with non-zero curvatures. Therefore, we propose a new non-parametric approach that uses curvatures to calculate distances. Curvature is an appealing feature for this purpose since it is not altered by isometries. In this paper, we propose two formulas for measuring distances on a manifold with constant curvature, and their validities are proven using the theorems of differential geometry. Utilizing these formulas, an algorithm is developed to measure the distance between a point and the center of a class. In the proposed algorithm geodesies are divided into equal linear segments, assuming that the curvature remains constant within each segment. This assumption is shown to be valid in many data spaces experimentally. Observed data near each segment are used to estimate curvatures and calculate distances within each segment. Finally, the total distance is computed by summing up the non-Euclidean lengths of all segments. The proposed method is a supervised version of k-means, named non-Euclidean centers. The correctness of the proposed method is validated using the Riemann tensor and its related theorems in differential geometry. Furthermore, experimental results show that our method performs well in real-world data classification applications. The space of symmetric positive definite matrices, which is often endowed with non-Euclidean metrics that induce some curvature, is used for input data representations.},
  archive      = {J_ML},
  author       = {Ghaziasgar, Mehran and Mahvash Mohammadi, Hossein and Adibi, Peyman},
  doi          = {10.1007/s10994-024-06702-z},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {A new geometry-aware non-euclidean distance metric},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep contrastive coordinated multi-view consistency clustering. <em>ML</em>, <em>114</em>(3), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06735-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) aimed at partitioning data samples into coherent clusters by integrating information from multiple perspectives. Recently, deep contrastive learning approaches have exhibited substantial capabilities in feature extraction within MVC frameworks. However, the challenge lies in extracting efficient feature representations while ensuring consistency. Moreover, existing deep clustering methods based on contrastive learning often overlook the consistency of cluster representation during clustering processes. In this study, we address these challenges by proposing a novel deep learning method called Contrastive Coordinated Multi-View consistency Clustering (CCMVC). Our approach leverages contrastive learning to coordinate training across three levels: feature, cluster, and view. Specifically, we enhance clustering performance by implementing an alignment method to ensure consistent information alignment across different views. This method assigns semantically similar representations for clustering tasks and effectively explores shared semantics across views while mitigating view-specific noise. Experimental evaluations conducted on seven datasets demonstrate the efficacy and superiority of our proposed CCMVC method over existing state-of-the-art approaches. Code is available at https://github.com/hulu88/CCMVC .},
  archive      = {J_ML},
  author       = {Shi, Fuhao and Wan, Shaohua and Wu, Shengli and Wei, Hui and Lu, Hu},
  doi          = {10.1007/s10994-025-06735-y},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Deep contrastive coordinated multi-view consistency clustering},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence of adam for non-convex objectives: Relaxed hyperparameters and non-ergodic case. <em>ML</em>, <em>114</em>(3), 1-42. (<a href='https://doi.org/10.1007/s10994-025-06737-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adam is a commonly used stochastic optimization algorithm in machine learning. However, its convergence is still not fully understood, especially in the non-convex setting. This paper focuses on exploring hyperparameter settings for the convergence of vanilla Adam and tackling the challenges of non-ergodic convergence related to practical applications. The primary contributions are summarized as follows: firstly, we introduce precise definitions of ergodic and non-ergodic convergence, which cover nearly all forms of convergence for stochastic optimization algorithms. Meanwhile, we emphasize the superiority of non-ergodic convergence over ergodic convergence. Secondly, we establish a weaker sufficient condition for the ergodic convergence guarantee of Adam, allowing a more relaxed choice of hyperparameters. On this basis, we achieve the almost sure ergodic convergence rate of Adam, which is arbitrarily close to $$o(1/\sqrt{K})$$ . More importantly, we prove, for the first time, that the last iterate of Adam converges to a stationary point for non-convex objectives. Finally, we obtain the non-ergodic convergence rate of O(1/K) for function values under the Polyak-Łojasiewicz (PL) condition. These findings build a solid theoretical foundation for Adam to solve non-convex stochastic optimization problems. Numerical experiments validate the effectiveness of Adam and support our theoretical findings.},
  archive      = {J_ML},
  author       = {Liang, Yuqing and He, Meixuan and Liu, Jinlan and Xu, Dongpo},
  doi          = {10.1007/s10994-025-06737-w},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-42},
  shortjournal = {Mach. Learn.},
  title        = {Convergence of adam for non-convex objectives: Relaxed hyperparameters and non-ergodic case},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate functional linear discriminant analysis for partially-observed time series. <em>ML</em>, <em>114</em>(3), 1-25. (<a href='https://doi.org/10.1007/s10994-025-06741-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The more extensive access to time-series data, especially for biomedical purposes, raises new methodological challenges, particularly regarding missing values. Functional linear discriminant analysis (FLDA) extends Linear Discriminant Analysis (LDA)-mediated multiclass classification and dimension reduction to data in the form of fragmented observations of a univariate function. For large multivariate and partially-observed data, there are two challenges: (i) statistical dependencies between different components of a multivariate function and (ii) heterogeneous sampling times with missing features. We here develop a multivariate version of FLDA, called MUDRA, to tackle these challenges and describe a computationally efficient expectation/conditional-maximisation (ECM) algorithm to infer its parameters without any tensor inversions. We assess its predictive power on the “Articulary Words” dataset and show its improvement over the state-of-the-art, especially in the case of missing data. This advancement in dimension reduction of multivariate functional data holds promise for enhancing classification accuracy in scenarios like partially observed short multivariate time series analysis.},
  archive      = {J_ML},
  author       = {Bordoloi, Rahul and Réda, Clémence and Trautmann, Orell and Bej, Saptarshi and Wolkenhauer, Olaf},
  doi          = {10.1007/s10994-025-06741-0},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Multivariate functional linear discriminant analysis for partially-observed time series},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised variational domain adaptation. <em>ML</em>, <em>114</em>(3), 1-24. (<a href='https://doi.org/10.1007/s10994-025-06751-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims at boosting learning tasks of the target domain (TD) via transferring learned knowledge from the source domain (SD). Feature alignment, as a key point of UDA, is often pursued by adversarial training or minimizing discrepancy of the marginal distributions of the two domains. However, global feature alignment is not sufficient to eliminate the gap between the domains. Most existing approaches often ignore category-level features during the feature alignment, which might lead to mode collapse. To deal with this issue, we propose a cross-domain probabilistic generative model (CPGM), which formulates the category-level feature adaptation as an issue of probabilistic approximation (i.e., the posterior probability of the TD is forced to approximate the prior probability of the SD). We further present theoretical analysis of evidence lower bound (ELBO) based on variational inference to solve the issue of probabilistic approximation. Consequently, we build an unsupervised variational domain adaptation (UVDA) method for classification tasks, which mitigates the mode collapse issue of the traditional global feature alignment method by constructing an ELBO loss, based on the CPGM. Our UVDA adopts an alternative training strategy that adapts category-level and global features via CPGM and adversarial training, respectively. In particular, we propose an effective sample screening module (SSM) to progressively select target samples with high confidence to facilitate the calculation of ELBO for maximizing the capability of CPGM. Experimental results on four popular datasets, namely, Digits, Office31, VisDA-2017 and DomainNet, demonstrate that our UVDA is effective, and outperforms the state-of-the-art methods.},
  archive      = {J_ML},
  author       = {Li, Yundong and Ge, Yizheng and Lin, Chen and Wang, Guan},
  doi          = {10.1007/s10994-025-06751-y},
  journal      = {Machine Learning},
  month        = {3},
  number       = {3},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Unsupervised variational domain adaptation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal large-scale stochastic optimization of NDCG surrogates for deep learning. <em>ML</em>, <em>114</em>(2), 1-70. (<a href='https://doi.org/10.1007/s10994-024-06631-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce principled stochastic algorithms to efficiently optimize Normalized Discounted Cumulative Gain (NDCG) and its top-K variant for deep models. To this end, we first propose novel compositional and bilevel compositional objectives for optimizing NDCG and top-K NDCG, respectively. We then develop two stochastic algorithms to tackle these non-convex objectives, achieving an iteration complexity of $$\mathcal {O}(\epsilon ^{-4})$$ for reaching an $$\epsilon $$ -stationary point. Our methods employ moving average estimators to track the crucial inner functions for gradient computation, effectively reducing approximation errors. Besides, we introduce practical strategies such as initial warm-up and stop-gradient techniques to enhance performance in deep learning. Despite the advancements, the iteration complexity of these two algorithms does not meet the optimal $$\mathcal {O}(\epsilon ^{-3})$$ for smooth non-convex optimization. To address this issue, we incorporate variance reduction techniques in our framework to more finely estimate the key functions, design new algorithmic mechanisms for solving multiple lower-level problems with parallel speed-up, and propose two types of algorithms. The first type directly tracks these functions with the variance reduced estimators, while the second treats these functions as solutions to minimization problems and employs variance reduced estimators to construct gradient estimators for solving these problems. We manage to establish the optimal $$\mathcal {O}(\epsilon ^{-3})$$ complexity for both types of algorithms. It is important to highlight that our algorithmic frameworks are versatile and can optimize a wide spectrum of metrics, including Precision@K/Recall@K, Average Precision (AP), mean Average Precision (mAP), and their top-K variants. We further present efficient stochastic algorithms for optimizing these metrics with convergence guarantees. We conduct comprehensive experiments on multiple ranking tasks to verify the effectiveness of our proposed algorithms, which consistently surpass existing strong baselines.},
  archive      = {J_ML},
  author       = {Qiu, Zi-Hao and Hu, Quanqi and Zhong, Yongjian and Tu, Wei-Wei and Zhang, Lijun and Yang, Tianbao},
  doi          = {10.1007/s10994-024-06631-x},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-70},
  shortjournal = {Mach. Learn.},
  title        = {Optimal large-scale stochastic optimization of NDCG surrogates for deep learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable sample selection framework against numerical label noise. <em>ML</em>, <em>114</em>(2), 1-36. (<a href='https://doi.org/10.1007/s10994-024-06637-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical label noise in regression would misguide the model training and worsen the generalization performance. As a popular technique, noise filtering reduces the noise level by removing mislabeled samples. Some filters care about the noise level so much that a few clean samples are also removed. The existing optimal sample selection (OSS) framework balances the number of removals and the noise level to avoid overcleaning. However, its underlying interpretability is unobvious due to the complicated objective function, and inaccurate parameter estimates or settings may discount the filtering effect. To address these issues, we first propose a novel interpretable sample selection (ISS) framework against numerical label noise. It seeks to maximize the number of available samples while having a relatively low noise level. ISS converges to OSS in $$\mathcal {O}(1/\log n)$$ , guaranteeing the good generalization performance inherited from OSS on large-scale datasets. Also, we prove its adaptability, thus ensuring that ISS is effective in changing noisy environments. Secondly, we propose a robust and low-deviation noise estimator, namely embedded covering distance. Finally, an embedded covering distance filtering (ECDF) algorithm is presented as part of the ISS framework. Experimental results on benchmark and real-world datasets show that the proposed ECDF algorithm outperforms the state-of-the-art filtering approaches against numerical label noise.},
  archive      = {J_ML},
  author       = {Jiang, Gaoxia and Wang, Wenjian},
  doi          = {10.1007/s10994-024-06637-5},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-36},
  shortjournal = {Mach. Learn.},
  title        = {An interpretable sample selection framework against numerical label noise},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RGCVAE: Relational graph conditioned variational autoencoder for molecule design. <em>ML</em>, <em>114</em>(2), 1-26. (<a href='https://doi.org/10.1007/s10994-024-06638-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying molecules that exhibit some pre-specified properties is a difficult problem to solve. In the last few years, deep generative models have been used for molecule generation. Deep Graph Variational Autoencoders are among the most powerful machine learning tools with which it is possible to address this problem. However, existing methods struggle to capture the true data distribution and tend to be computationally expensive. In this work, we propose RGCVAE, an efficient and effective Graph Variational Autoencoder based on: (i) an encoding network exploiting a new powerful Relational Graph Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to several State-of-the-Art VAE methods on two widely adopted datasets, RGCVAE shows State-of-the-Art molecule generation performance while being significantly faster to train. The Python code implementing RGCVAE is openly accessible for download at: https://github.com/drigoni/RGCVAE .},
  archive      = {J_ML},
  author       = {Rigoni, Davide and Navarin, Nicolò and Sperduti, Alessandro},
  doi          = {10.1007/s10994-024-06638-4},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-26},
  shortjournal = {Mach. Learn.},
  title        = {RGCVAE: Relational graph conditioned variational autoencoder for molecule design},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked autoencoder for multiagent trajectories. <em>ML</em>, <em>114</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10994-024-06647-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically labeling trajectories of multiple agents is key to behavioral analyses but usually requires a large amount of manual annotations. This also applies to the domain of team sport analyses. In this paper, we specifically show how pretraining transformer models improves the classification performance on tracking data from professional soccer. For this purpose, we propose a novel self-supervised masked autoencoder for multiagent trajectories to effectively learn from only a few labeled sequences. Our approach builds upon a factorized transformer architecture for multiagent trajectory data and employs a masking scheme on the level of individual agent trajectories. As a result, our model allows for a reconstruction of masked trajectory segments while being permutation equivariant with respect to the agent trajectories. In addition to experiments on soccer, we demonstrate the usefulness of the proposed pretraining approach on multiagent pose data from entomology. In contrast to related work, our approach is conceptually much simpler, does not require handcrafted features and naturally allows for permutation invariance in downstream tasks.},
  archive      = {J_ML},
  author       = {Rudolph, Yannick and Brefeld, Ulf},
  doi          = {10.1007/s10994-024-06647-3},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Mach. Learn.},
  title        = {Masked autoencoder for multiagent trajectories},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interactive sequential generative models for team sports. <em>ML</em>, <em>114</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10994-024-06648-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding spatiotemporal coordination of players in team sports is key to movement models, pattern detection, and computational tactics. Existing generative models propose to capture all stochasticity by a single latent variable and may suffer from entangled representations, or aim to uncover interaction structures of players but then do not focus on their generative ability. As a remedy, we propose a hierarchical latent variable model for predicting trajectories of multiple players. In the generative model, both, discrete role assignments and a latent interaction graph are sampled to allow for different models in subsequent node updates and message passing operations between nodes, where standard Gaussian latent variables are employed per agent and timestep. We cast our approach as a variational autoencoder that provides a disentangled latent space to capture variability in team sport movements and propose a neural architecture for its optimization. We empirically evaluate our approach on tracking data from basketball and soccer and observe that our contribution outperforms the state-of-art in all experiments.},
  archive      = {J_ML},
  author       = {Fassmeyer, Dennis and Cordes, Moritz and Brefeld, Ulf},
  doi          = {10.1007/s10994-024-06648-2},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Mach. Learn.},
  title        = {Interactive sequential generative models for team sports},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing machine learning and data imputation approaches to handle the issue of data sparsity in sports forecasting. <em>ML</em>, <em>114</em>(2), 1-28. (<a href='https://doi.org/10.1007/s10994-024-06651-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparsity is a common characteristic for datasets used in the domain of sports forecasting, mainly derived from inconsistencies in data coverage. Typically, this issue is circumvented by cutting the number of features (depth-focused) or the sample size (breadth-focused) for analysis. The present study uses an experimental approach to analyse the effects of depth- or breadth-focused analyses and data imputation to enable usage of the full sample size and feature wealth. Two forecasting models following a hybrid (i.e., a combination of classical statistical and machine learning) and a full deep learning approach are introduced to perform experiments on a dataset of more than 300,000 soccer matches. In contrast to typical soccer forecasting studies, the analysis was not restricted to one-match-ahead forecasts but used a longer forecasting horizon of up to two months ahead. Systematic differences between the two types of models were identified. The hybrid model based on classical statistical rating models, performs strongly on depth-focused approaches while not or only marginally improving for approaches with high data breadth. The deep learning model, however, performs weakly in a depth-focused approach but profits strongly from data breadth. The improved prediction performance in cases of high data breadth suggests that a rich feature set offers better training opportunities than a less detailed set with a larger sample size. Additionally, we showcase that data imputation can be used to address data sparsity by enabling full data depth and breadth. The presented findings are relevant for advancing predictive accuracy and sports forecasting methodologies, emphasizing the viability of imputation techniques to increase data coverage in different analytical approaches.},
  archive      = {J_ML},
  author       = {Wunderlich, Fabian and Biermann, Henrik and Yang, Weiran and Bassek, Manuel and Raabe, Dominik and Elbert, Nico and Memmert, Daniel and Garnica Caparrós, Marc},
  doi          = {10.1007/s10994-024-06651-7},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {Assessing machine learning and data imputation approaches to handle the issue of data sparsity in sports forecasting},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapting performance metrics for ordinal classification to interval scale: Length matters. <em>ML</em>, <em>114</em>(2), 1-49. (<a href='https://doi.org/10.1007/s10994-024-06654-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of supervised machine learning, accurate evaluation of classification models is a critical factor for assessing their performance and guiding model selection. This paper delves into the domain of ordinal classification and raises the question of adapting ordinal metrics to the interval scale. In scenarios where measurements are recorded at intervals, not only the order but also their length assume significance, and this promotes the adoption of novel performance metrics. Initially, we revisit two existing confusion matrix-based ordinal metrics and introduce a normalization technique to render them comparable and enhance their practical utility. We extend our focus to classification by intervals, proposing a robust framework for adapting ordinal metrics to the interval scale, and applying it to the aforementioned ordinal metrics. We address the challenge of unbounded rightmost intervals, a common issue in practical applications, from both theoretical and simulation perspectives, by providing a solution that enhances the applicability of the proposed metrics. To further explore practical implications, we conducted experiments on real-world datasets. The results reveal a promising trend in the use of interval-scale metrics to guide hyper-parameter tuning for improving model performance.},
  archive      = {J_ML},
  author       = {Binotto, Giulia and Delgado, Rosario},
  doi          = {10.1007/s10994-024-06654-4},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-49},
  shortjournal = {Mach. Learn.},
  title        = {Adapting performance metrics for ordinal classification to interval scale: Length matters},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). State-novelty guided action persistence in deep reinforcement learning. <em>ML</em>, <em>114</em>(2), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06675-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While a powerful and promising approach, deep reinforcement learning (DRL) still suffers from sample inefficiency, which can be notably improved by resorting to more sophisticated techniques to address the exploration–exploitation dilemma. One such technique relies on action persistence (i.e., repeating an action over multiple steps). However, previous work exploiting action persistence either applies a fixed strategy or learns additional value functions (or policy) for selecting the repetition number. In this paper, we propose a novel method to dynamically adjust the action persistence based on the current exploration status of the state space. In such a way, our method does not require training of additional value functions or policy. Moreover, the use of a smooth scheduling of the repeat probability allows a more effective balance between exploration and exploitation. Furthermore, our method can be seamlessly integrated into various basic exploration strategies to incorporate temporal persistence. Finally, extensive experiments on different DMControl tasks demonstrate that our state-novelty guided action persistence method significantly improves the sample efficiency.},
  archive      = {J_ML},
  author       = {Hu, Jianshu and Weng, Paul and Ban, Yutong},
  doi          = {10.1007/s10994-024-06675-z},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {State-novelty guided action persistence in deep reinforcement learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing sharpe ratio: Risk-adjusted decision-making in multi-armed bandits. <em>ML</em>, <em>114</em>(2), 1-21. (<a href='https://doi.org/10.1007/s10994-024-06680-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharpe ratio (SR) is a critical parameter in characterizing financial time series as it jointly considers the reward and the volatility of any stock/portfolio through its mean and standard deviation. Deriving online algorithms for optimizing the SR is particularly challenging since even offline policies experience constant regret with respect to the best expert (Even-Dar et al., 2006). This paper focuses on optimizing the regularized square SR (RSSR) by considering two settings: regret minimization (RM) and best arm identification (BAI). In this regard, we propose a novel multiarmed bandit (MAB) algorithm for RM called UCB-RSSR for RSSR maximization. We derive a path-dependent concentration bound for the estimate of the RSSR. Based on that, we derive the regret guarantees of UCB-RSSR and show that it evolves as $${\mathcal {O}}\left( \log {n}\right)$$ for the two-armed bandit case played for a horizon n. We also consider algorithms for the fixed budget setting of the BAI problems, i.e., sequential halving and successive rejects, and propose SHSR and SuRSR algorithms. We derive the upper bound for the error probability of BAI algorithms. We demonstrate that UCB-RSSR outperforms the only other known SR optimizing bandit algorithm, U-UCB (Cassel et al., 2023). We also study the efficacy of proposed BAI algorithms for 6 different setups and discuss the cases where our proposed algorithms are suitable. Our research highlights that our proposed algorithms will find extensive applications in risk-aware portfolio management problems.},
  archive      = {J_ML},
  author       = {Khurshid, Sabrina and Abdulla, Mohammed Shahid and Ghatak, Gourab},
  doi          = {10.1007/s10994-024-06680-2},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {Optimizing sharpe ratio: Risk-adjusted decision-making in multi-armed bandits},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ShuttleFlow: Learning the distribution of subsequent badminton shots using normalizing flows. <em>ML</em>, <em>114</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10994-024-06682-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces ShuttleFlow, a simple yet effective model designed to forecast badminton shot types and shuttle positions. This tool could be invaluable for coaches, enabling them to identify opponents’ weaknesses and devise effective strategies accordingly. Given the inherent unpredictability of player behaviors, our model leverages conditional normalizing flow to generate the distributions of shot types and shuttle positions. This is achieved by considering the players and their preceding shots on the court. To augment the performance of our model, especially in predicting outcomes for players who have not previously competed against each other, we incorporate a novel regularization term. Additionally, we utilize Poisson disk sampling to reduce sample redundancy when generating the distributions. Compared to state-of-the-art techniques, our results underscore ShuttleFlow’s effectiveness in forecasting shot types and shuttle positions.},
  archive      = {J_ML},
  author       = {Lien, Yun-Hsuan and Lian, Chia-Tung and Wang, Yu-Shuen},
  doi          = {10.1007/s10994-024-06682-0},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Mach. Learn.},
  title        = {ShuttleFlow: Learning the distribution of subsequent badminton shots using normalizing flows},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shift guided active learning. <em>ML</em>, <em>114</em>(2), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06684-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning is a pivotal machine learning paradigm where the algorithm queries data iteratively from an information source and updates itself accordingly. Active learning provides an instrument to investigate data selection and has been proven effective in reducing annotation costs. In a typical active learning framework, the query step only takes information from the current learning cycle and the information between cycles is usually ignored. It turns out that both inner-cycle and inter-cycle information provide crucial insights for learning progression. In this study, we identify the existence of distribution shifts that include both inner-cycle and inter-cycle information. This shift negatively impacts stability and model performance. To counter the impact of such a shift, we propose to integrate them into an active learning framework with specialized models. Our framework, Shift Adaptation via Guided Enquiry (SAGE), is founded on a set of dedicated query strategies guided by the distribution shift. We show that this new framework mitigates distribution shifts and outperforms previous studies on multiple computer vision benchmarks. With extensive experiments, we conclude that SAGE improves the state-of-the-art, with a significant 3.28% absolute accuracy improvement over the previous methods in the field of active learning. This framework is also compatible with semi-supervised (SSL) settings, allowing state-of-the-art SSL methods to attain higher performance.},
  archive      = {J_ML},
  author       = {Yang, Jianan and Tan, Jimin and Wang, Haobo and Chen, Gang and Wu, Sai and Zhao, Junbo},
  doi          = {10.1007/s10994-024-06684-y},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Shift guided active learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive transformer modelling of density function for nonparametric survival analysis. <em>ML</em>, <em>114</em>(2), 1-24. (<a href='https://doi.org/10.1007/s10994-024-06686-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis holds a crucial role across diverse disciplines, such as economics, engineering and healthcare. It empowers researchers to analyze both time-invariant and time-varying data, encompassing phenomena like customer churn, material degradation and various medical outcomes. Given the complexity and heterogeneity of such data, recent endeavors have demonstrated successful integration of deep learning methodologies to address limitations in conventional statistical approaches. However, current methods typically involve cluttered probability distribution function (PDF), have lower sensitivity in censoring prediction, only model static datasets, or only rely on recurrent neural networks for dynamic modelling. In this paper, we propose a novel survival regression method capable of producing high-quality unimodal PDFs without any prior distribution assumption, by optimizing novel Margin-Mean-Variance loss and leveraging the flexibility of Transformer to handle both temporal and non-temporal data, coined UniSurv. Extensive experiments on several datasets demonstrate that UniSurv places a significantly higher emphasis on censoring compared to other methods.},
  archive      = {J_ML},
  author       = {Zhang, Xin and Mehta, Deval and Hu, Yanan and Zhu, Chao and Darby, David and Yu, Zhen and Merlo, Daniel and Gresle, Melissa and van der Walt, Anneke and Butzkueven, Helmut and Ge, Zongyuan},
  doi          = {10.1007/s10994-024-06686-w},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Adaptive transformer modelling of density function for nonparametric survival analysis},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning global object-centric representations via disentangled slot attention. <em>ML</em>, <em>114</em>(2), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06687-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can discern scene-independent features of objects across various environments, allowing them to swiftly identify objects amidst changing factors such as lighting, perspective, size, and position and imagine the complete images of the same object in diverse settings. Existing object-centric learning methods only extract scene-dependent object-centric representations, lacking the ability to identify the same object across scenes as humans. Moreover, some existing methods discard the individual object generation capabilities to handle complex scenes. This paper introduces a novel object-centric learning method to empower AI systems with human-like capabilities to identify objects across scenes and generate diverse scenes containing specific objects by learning a set of global object-centric representations. To learn the global object-centric representations that encapsulate globally invariant attributes of objects (i.e., the complete appearance and shape), this paper designs a Disentangled Slot Attention module to convert the scene features into scene-dependent attributes (such as scale, position and orientation) and scene-independent representations (i.e., appearance and shape). Experimental results substantiate the efficacy of the proposed method, demonstrating remarkable proficiency in global object-centric representation learning, object identification, scene generation with specific objects and scene decomposition.},
  archive      = {J_ML},
  author       = {Chen, Tonglin and Huang, Yinxuan and Shen, Zhimeng and Huang, Jinghao and Li, Bin and Xue, Xiangyang},
  doi          = {10.1007/s10994-024-06687-9},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Learning global object-centric representations via disentangled slot attention},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Warping resilient robust anomaly detection for multivariate time series. <em>ML</em>, <em>114</em>(2), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06689-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in multivariate time series (MTS) data is pivotal for ensuring the integrity and reliability of real-time systems across diverse domains. Existing approaches for anomaly detection often rely on clean normal data to learn temporal and intervariable relationships. However, such datasets is nearly unavailable in real scenarios. Moreover, the presence of temporal distortions like warp variations and their impact on anomaly detection accuracy remains unexplored, potentially leading to more erroneous detections. In this work, we propose a Warping resilient Robust Anomaly Detection (WRADMts) method with two major modules: 1) Warp Aligning Temporal Transformation to eliminate warp distortions and efficiently capture the normal pattern in the data, and 2) Graph Structure and Node Embedding Learning to capture temporal and intervariable dependencies with unique sparse adjacency matrix learning mechanism. Our model is resilient to warp distortions and also robust to noise contamination in the data. We compare our model with eight baselines on five real-world datasets, demonstrating significant improvements, with up to 7% F1 score enhancement over the best baseline.},
  archive      = {J_ML},
  author       = {Abilasha, S. and Bhadra, Sahely},
  doi          = {10.1007/s10994-024-06689-7},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Warping resilient robust anomaly detection for multivariate time series},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient-based causal discovery with latent variables. <em>ML</em>, <em>114</em>(2), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06696-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering causal graphs from observational data is a challenging problem, which has garnered significant attention due to its crucial role in understanding causal relationships. In recent advancements, this problem is cast as a continuous optimization task with structural constraints, through which the great power of gradient-based methods can be exploited to address the causal discovery problem. Despite their statistical validity, these approaches return causal graphs with spurious edges in the presence of latent variables. In this paper, we generalize the gradient-based method to accommodate the existence of latent confounders and latent intermediate variables. Specifically, we propose a causal discovery method based on latent variable reconstruction. This method primarily consists of two stages. In the first stage, we propose a series of causal models that includes latent variables, which can be applied to different data assumptions. However, due to the influence of latent variables, the causal graph inevitably contains reversed edges. In light of this fact, we propose the method to correct these reversed edges on the second stage via variational autoencoder. Theoretical results show that under some mild conditions, our method can correctly identify the causal relations. Experiments on both synthetic and real datasets demonstrate the superiority of our method to existing gradient-based learning algorithms in the presence of latent variables.},
  archive      = {J_ML},
  author       = {Ni, Haotian and Wang, Tian-Zuo and Tao, Hong and Huang, Xiuqi and Hou, Chenping},
  doi          = {10.1007/s10994-024-06696-8},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Gradient-based causal discovery with latent variables},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing data dependency in neural networks: Introducing the knowledge enhanced neural network (KENN) for time series forecasting +. <em>ML</em>, <em>114</em>(2), 1-25. (<a href='https://doi.org/10.1007/s10994-024-06714-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Purely data-driven deep learning methods often require impractical amounts of high-quality data, which is one of their major weaknesses. This particularly impacts their performance in time series domains, that intrinsically have scarcity of input features. Furthermore, deep learning methods lack the ability to incorporate explicitly defined human knowledge, which can be crucial for finding effective solutions. To address these challenges, we propose a novel fusion framework, Knowledge Enhanced Neural Network (KENN), for time series forecasting. KENN combines knowledge- and data-driven approaches in a novel residual fusion scheme, where information in knowledge and data is combined in a complementary manner. We evaluate KENN in a variety of constrained settings with limited data and inaccurate knowledge models. Even when utilizing only 10% of the data for training, KENN outperforms underlying DNN trained on the complete training set. KENN specifically alleviates data and accuracy constraints of the constituent data and knowledge driven domains while, simultaneously, improving the overall accuracy. We also compare KENN with recent State-of-the-Art (SotA) methods on 5 real-world forecasting datasets. KENN outperforms SotA by an average of 42.2%, when utilizing complete training set, and by 39.7%, when utilizing only 50% of the training set. A fusion framework that reduces dependency of DNN on large datasets and enables harnessing benefits of knowledge driven systems will prove useful in many real-world applications.},
  archive      = {J_ML},
  author       = {Chattha, Muhammad Ali and Malik, Muhammad Imran and Dengel, Andreas and Ahmed, Sheraz},
  doi          = {10.1007/s10994-024-06714-9},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Mach. Learn.},
  title        = {Addressing data dependency in neural networks: Introducing the knowledge enhanced neural network (KENN) for time series forecasting +},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ISOUP-SymRF: Symbolic feature ranking with random forests in online multi-target regression and multi-label classification. <em>ML</em>, <em>114</em>(2), 1-24. (<a href='https://doi.org/10.1007/s10994-024-06718-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of feature ranking has received considerable attention across various predictive modelling tasks in the batch learning scenario, but not in the online learning setting. Available methods that estimate feature importances on data streams have so far predominantly focused on ranking the features for the tasks of classification and occasionally multi-label classification. We propose a novel online feature ranking method for online multi-target regression iSOUP-SymRF, which estimates feature importance scores based on the positions at which a feature appears in the trees of a random forest of iSOUP-Trees, and additionally extend it to task of online feature ranking for multi-label classification. By utilizing iSOUP-Trees, which can address multiple structured output prediction tasks on data streams, iSOUP-SymRF promises feature ranking across a variety of online structured output prediction tasks. We examine the ranking convergence of iSOUP-SymRF in terms of the methods’ parameters, the size of the ensemble and the number of selected features, as well as their stability under different random seeds. Furthermore, to show the utility of iSOUP-SymRF and its rankings we use them in conjunction with two state-of-the-art online multi-target regression and multi-label classification methods, iSOUP-Tree and AMRules, and analyze the impact of adding features according to the rankings obtained from iSOUP-SymRF.},
  archive      = {J_ML},
  author       = {Osojnik, Aljaž and Panov, Panče and Džeroski, Sašo},
  doi          = {10.1007/s10994-024-06718-5},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {ISOUP-SymRF: Symbolic feature ranking with random forests in online multi-target regression and multi-label classification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). De-biased two-sample U-statistics with application to conditional distribution testing. <em>ML</em>, <em>114</em>(2), 1-28. (<a href='https://doi.org/10.1007/s10994-024-06719-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some high-dimensional and semiparametric inference problems involving two populations, the parameter of interest can be characterized by two-sample U-statistics involving some nuisance parameters. In this work we first extend the framework of one-step estimation with cross-fitting to two-sample U-statistics, showing that using an orthogonalized influence function can effectively remove the first order bias, resulting in asymptotically normal estimates of the parameter of interest. As an example, we apply this method and theory to the problem of testing two-sample conditional distributions, also known as strong ignorability. When combined with a conformal-based rank-sum test, we discover that the nuisance parameters can be divided into two categories, where in one category the nuisance estimation accuracy does not affect the testing validity, whereas in the other the nuisance estimation accuracy must satisfy the usual requirement for the test to be valid. We believe these findings provide further insights into and enhance the conformal inference toolbox.},
  archive      = {J_ML},
  author       = {Chen, Yuchen and Lei, Jing},
  doi          = {10.1007/s10994-024-06719-4},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-28},
  shortjournal = {Mach. Learn.},
  title        = {De-biased two-sample U-statistics with application to conditional distribution testing},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invariant representation learning via decoupling style and spurious features. <em>ML</em>, <em>114</em>(2), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06730-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the out-of-distribution (OOD) generalization problem under the setting that both style distribution shift and spurious features exist and domain labels are missing. This setting frequently arises in real-world applications and is underlooked because previous approaches mainly handle either of these two factors. The critical challenge is decoupling style and spurious features in the absence of domain labels. We propose a structural causal model (SCM) for the image generation process to address this challenge, considering both style distribution shifts and spurious features. The proposed SCM enables us to design a new framework called IRSS, which can gradually separate style distribution and spurious features from images by introducing adversarial neural networks and multi-environment optimization, thus achieving OOD generalization. Moreover, it does not require additional supervision (e.g., domain labels) other than the images and their corresponding labels. Experiments on benchmark datasets demonstrate that IRSS outperforms traditional OOD methods and solves the problem of Invariant risk minimization degradation, enabling the extraction of invariant features under distribution shift.},
  archive      = {J_ML},
  author       = {Li, Ruimeng and Pu, Yuanhao and Li, Zhaoyi and Wu, Chenwang and Xie, Hong and Lian, Defu},
  doi          = {10.1007/s10994-024-06730-9},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Invariant representation learning via decoupling style and spurious features},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering temporal patterns in visualizations of high-dimensional data. <em>ML</em>, <em>114</em>(2), 1-27. (<a href='https://doi.org/10.1007/s10994-025-06734-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing availability of high-dimensional data, analysts often rely on exploratory data analysis to understand complex data sets. A key approach to exploring such data is dimensionality reduction, which embeds high-dimensional data in two dimensions to enable visual exploration. However, popular embedding techniques, such as t-SNE and UMAP, typically assume that data points are independent. When this assumption is violated, as in time-series data, the resulting visualizations may fail to reveal important temporal patterns and trends. To address this, we propose a formal extension to existing dimensionality reduction methods that incorporates two temporal loss terms that explicitly highlight temporal progression in the embedded visualizations. Through a series of experiments on both synthetic and real-world datasets, we demonstrate that our approach effectively uncovers temporal patterns and improves the interpretability of the visualizations. Furthermore, the method improves temporal coherence while preserving the fidelity of the embeddings, providing a robust tool for dynamic data analysis.},
  archive      = {J_ML},
  author       = {Poličar, Pavlin G. and Zupan, Blaž},
  doi          = {10.1007/s10994-025-06734-z},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1-27},
  shortjournal = {Mach. Learn.},
  title        = {Uncovering temporal patterns in visualizations of high-dimensional data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Deep negative correlation classification. <em>ML</em>, <em>114</em>(2), 1. (<a href='https://doi.org/10.1007/s10994-025-06736-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ML},
  author       = {Zhang, Le and Hou, Qibin and Liu, Yun and Bian, Jia-Wang and Xu, Xun and Zhou, Joey Tianyi and Zhu, Ce},
  doi          = {10.1007/s10994-025-06736-x},
  journal      = {Machine Learning},
  month        = {2},
  number       = {2},
  pages        = {1},
  shortjournal = {Mach. Learn.},
  title        = {Correction to: Deep negative correlation classification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on self-supervised learning for non-sequential tabular data. <em>ML</em>, <em>114</em>(1), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06674-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has become a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups—predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods in each direction. Moreover, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table transferability, and domain knowledge integration. In addition, we elaborate on existing benchmarks and datasets for NS-TD applications to analyze the performance of existing tabular models. Finally, we discuss the challenges of SSL4NS-TD and provide potential directions for future research. We expect our work to be useful in terms of encouraging more research on lowering the barrier to entry SSL for the tabular domain, and of improving the foundations for implicit tabular data.},
  archive      = {J_ML},
  author       = {Wang, Wei-Yao and Du, Wei-Wei and Xu, Derek and Wang, Wei and Peng, Wen-Chih},
  doi          = {10.1007/s10994-024-06674-0},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {A survey on self-supervised learning for non-sequential tabular data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SA-LfV: Self-annotated labeling from videos for object detection. <em>ML</em>, <em>114</em>(1), 1-24. (<a href='https://doi.org/10.1007/s10994-024-06676-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of object detection, the remarkable strides made by deep neural networks over the past decade have been hampered by challenges such as data labeling and the need to capture natural variations in training samples. Existing benchmark datasets are confined with limited set of classes, and natural variations. This paper presents "SA-LfV", a novel framework designed to streamline object detection from videos with minimal human input. By utilizing basic computer vision tasks, such as image classification and tracking single objects, our method generates pseudo-labels for object detection efficiently. To ensure a rich variety of training samples, we introduce two innovative sampling strategies. The first applies density-based clustering, choosing samples that represent a wide range of scenarios. The second analyzes object movements and their mutual information, capturing diverse behaviors and appearances. The proposed object detection data labeling procedure is demonstrated on object-tracking datasets and custom-downloaded videos. Through these methods, our framework has produced a dataset with 70,000 pseudo-labeled bounding boxes across 13 object classes, significantly diversifying the available data for object detection tasks. Our experiments show that the proposed framework can effectively adapt to unlabelled ImageNet classes, indicating its potential to broaden the capabilities of object detection models. Moreover, integrating our self-annotated dataset with standard benchmark datasets leads to a notable improvement in object detection performance. This new approach not only simplifies the traditionally labor-intensive process of manual labeling but also paves the way for expanding object detection to a wider range of classes and applications.},
  archive      = {J_ML},
  author       = {Sivapuram, Arun Kumar and Komuravelli, Prashanth and Gorthi, Rama Krishna Sai},
  doi          = {10.1007/s10994-024-06676-y},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {SA-LfV: Self-annotated labeling from videos for object detection},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ArithmeticGPT: Empowering small-size large language models with advanced arithmetic skills. <em>ML</em>, <em>114</em>(1), 1-23. (<a href='https://doi.org/10.1007/s10994-024-06681-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have shown remarkable capabilities in understanding and generating language across a wide range of domains. However, their performance in advanced arithmetic calculation remains a significant challenge, especially for small-size LLMs. Therefore, in this paper, we propose ArithmeticGPT, a practical framework designed to enhance the advanced arithmetic skills for small-size LLMs. We carefully curate an arithmetic instruction dataset, ArithInstruct, that is able to teach the small-size LLMs to trigger a self-developed internal calculation API for precise computations without explicit instructions. The advanced arithmetic calculation results are seamlessly generated within natural language sentences. Furthermore, we empirically design a practical three-stage strategy for fine-tuning the small-size LLMs with ArithInstruct to enable the advanced arithmetic skills and keep the models’ original abilities such as commonsense reasoning and question answering. We evaluate ArithmeticGPT on six public math related datasets with 17 state-of-the-art LLM baselines and experimental results demonstrate the superiority of our approach. To encourage reproducible research, we make our data and code publicly available at https://github.com/ai4ed/ArithmeticGPT .},
  archive      = {J_ML},
  author       = {Liu, Zitao and Zheng, Ying and Yin, Zhibo and Chen, Jiahao and Liu, Tianqiao and Tian, Mi and Luo, Weiqi},
  doi          = {10.1007/s10994-024-06681-1},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Mach. Learn.},
  title        = {ArithmeticGPT: Empowering small-size large language models with advanced arithmetic skills},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convergence analysis of over-parameterized variational autoencoders: A neural tangent kernel perspective. <em>ML</em>, <em>114</em>(1), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06683-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational Auto-Encoders (VAEs) have emerged as powerful probabilistic models for generative tasks. However, their convergence properties have not been rigorously proven. The challenge of proving convergence is inherently difficult due to the highly non-convex nature of the training objective and the implementation of a Stochastic Neural Network (SNN) within VAE architectures. This paper addresses these challenges by characterizing the optimization trajectory of SNNs utilized in VAEs through the lens of Neural Tangent Kernel (NTK) techniques. These techniques govern the optimization and generalization behaviors of ultra-wide neural networks. We provide a mathematical proof of VAE convergence under mild assumptions, thus advancing the theoretical understanding of VAE optimization dynamics. Furthermore, we establish a novel connection between the optimization problem faced by over-parameterized SNNs and the Kernel Ridge Regression (KRR) problem. Our findings not only contribute to the theoretical foundation of VAEs but also open new avenues for investigating the optimization of generative models using advanced kernel methods. Our theoretical claims are verified by experimental simulations.},
  archive      = {J_ML},
  author       = {Wang, Li and Huang, Wei},
  doi          = {10.1007/s10994-024-06683-z},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {On the convergence analysis of over-parameterized variational autoencoders: A neural tangent kernel perspective},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient federated unlearning under plausible deniability. <em>ML</em>, <em>114</em>(1), 1-18. (<a href='https://doi.org/10.1007/s10994-024-06685-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy regulations like the GDPR in Europe and the CCPA in the US allow users the right to remove their data from machine learning (ML) applications. Machine unlearning addresses this by modifying the ML parameters in order to forget the influence of a specific data point on its weights. Recent literature has highlighted that the contribution from data point(s) can be forged with some other data points in the dataset with probability close to one. This allows a server to falsely claim unlearning without actually modifying the model’s parameters. However, in distributed paradigms such as federated learning (FL), where the server lacks access to the dataset and the number of clients are limited, claiming unlearning in such cases becomes a challenge. An honest server must modify the model parameters in order to unlearn. This paper introduces an efficient way to achieve machine unlearning in FL, i.e., federated unlearning, by employing a privacy model which allows the FL server to plausibly deny the client’s participation in the training up to a certain extent. Specifically, we demonstrate that the server can generate a Proof-of-Deniability, where each aggregated update can be associated with at least x (the plausible deniability parameter) client updates. This enables the server to plausibly deny a client’s participation. However, in the event of frequent unlearning requests, the server is required to adopt an unlearning strategy and, accordingly, update its model parameters. We also perturb the client updates in a cluster in order to avoid inference from an honest but curious server. We show that the global model satisfies $$(\epsilon , \delta )$$ -differential privacy after T number of communication rounds. The proposed methodology has been evaluated on multiple datasets in different privacy settings. The experimental results show that our framework achieves comparable utility while providing a significant reduction in terms of memory ( $$\approx $$ 30 times), as well as retraining time (1.6-500769 times). The source code for the paper is available https://github.com/Ayush-Umu/Federated-Unlearning-under-Plausible-Deniability .},
  archive      = {J_ML},
  author       = {Varshney, Ayush K. and Torra, Vicenç},
  doi          = {10.1007/s10994-024-06685-x},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Learn.},
  title        = {Efficient federated unlearning under plausible deniability},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Misclassification bounds for PAC-bayesian sparse deep learning. <em>ML</em>, <em>114</em>(1), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06690-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been a significant focus on exploring the theoretical aspects of deep learning, especially regarding its performance in classification tasks. Bayesian deep learning has emerged as a unified probabilistic framework, seeking to integrate deep learning with Bayesian methodologies seamlessly. However, there exists a gap in the theoretical understanding of Bayesian approaches in deep learning for classification. This study presents an attempt to bridge that gap. By leveraging PAC-Bayes bounds techniques, we present theoretical results on the prediction or misclassification error of a probabilistic approach utilizing Spike-and-Slab priors for sparse deep learning in classification. We establish non-asymptotic results for the prediction error. Additionally, we demonstrate that, by considering different architectures, our results can achieve minimax optimal rates in both low and high-dimensional settings, up to a logarithmic factor. Moreover, our additional logarithmic term yields slight improvements over previous works. Additionally, we propose and analyze an automated model selection approach aimed at optimally choosing a network architecture with guaranteed optimality.},
  archive      = {J_ML},
  author       = {Mai, The Tien},
  doi          = {10.1007/s10994-024-06690-0},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Misclassification bounds for PAC-bayesian sparse deep learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concentration properties of fractional posterior in 1-bit matrix completion. <em>ML</em>, <em>114</em>(1), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06691-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of estimating a matrix based on a set of observed entries is commonly referred to as the matrix completion problem. In this work, we specifically address the scenario of binary observations, often termed as 1-bit matrix completion. While numerous studies have explored Bayesian and frequentist methods for real-value matrix completion, there has been a lack of theoretical exploration regarding Bayesian approaches in 1-bit matrix completion. We tackle this gap by considering a general, non-uniform sampling scheme and providing theoretical assurances on the efficacy of the fractional posterior. Our contributions include obtaining concentration results for the fractional posterior and demonstrating its effectiveness in recovering the underlying parameter matrix. We accomplish this using two distinct types of prior distributions: low-rank factorization priors and a spectral scaled Student prior, with the latter requiring fewer assumptions. Importantly, our results exhibit an adaptive nature by not mandating prior knowledge of the rank of the parameter matrix. Our findings are comparable to those found in the frequentist literature, yet demand fewer restrictive assumptions.},
  archive      = {J_ML},
  author       = {Mai, The Tien},
  doi          = {10.1007/s10994-024-06691-z},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Concentration properties of fractional posterior in 1-bit matrix completion},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knoop: Practical enhancement of knockoff with over-parameterization for variable selection. <em>ML</em>, <em>114</em>(1), 1-22. (<a href='https://doi.org/10.1007/s10994-024-06692-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection plays a crucial role in enhancing modeling effectiveness across diverse fields, addressing the challenges posed by high-dimensional datasets of correlated variables. This work introduces a novel approach namely Knockoff with over-parameterization (Knoop) to enhance Knockoff filters for variable selection. Specifically, Knoop first generates multiple knockoff variables for each original variable and integrates them with the original variables into an over-parameterized Ridgeless regression model. For each original variable, Knoop evaluates the coefficient distribution of its knockoffs and compares these with the original coefficients to conduct an anomaly-based significance test, ensuring robust variable selection. Extensive experiments demonstrate superior performance compared to existing methods in both simulation and real-world datasets. Knoop achieves a notably higher Area under the Curve (AUC) of the Receiver Operating Characteristic (ROC) Curve for effectively identifying relevant variables against the ground truth by controlled simulations, while showcasing enhanced predictive accuracy across diverse regression and classification tasks. The analytical results further backup our observations. The source codes of this work are available at https://github.com/RubyZhang166/Knoop-Knockoff-Enhancement-with-Overparametrization-for-Feature-Selection .},
  archive      = {J_ML},
  author       = {Zhang, Xiaochen and Cai, Yunfeng and Xiong, Haoyi},
  doi          = {10.1007/s10994-024-06692-y},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Mach. Learn.},
  title        = {Knoop: Practical enhancement of knockoff with over-parameterization for variable selection},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable individual treatment effect estimator for large graphs. <em>ML</em>, <em>114</em>(1), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06694-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference plays a critical role in decision-making processes about whether to provide treatment to individuals across various domains, such as education, medicine, and e-commerce. One of the fundamental tasks in causal inference is to estimate the individual treatment effect (ITE), which represents the effect of a treatment on an individual outcome. Recently, many studies have focused on estimating ITE from graph data taking into account not only the covariates of units but also connections among them. In such a case, the outcome of a unit can be affected by not only its own covariates and treatment but also those of its neighbors, which is referred to as interference. Existing methods have utilized graph neural networks (GNNs) to capture interference and achieved improvements in estimating ITE on graph data. However, these methods are not computationally efficient and therefore cannot be applied to large graph data. To overcome this problem, we propose a novel method that reduces redundant computation in interference modeling while maintaining the prediction performance of ITE estimation. Our key idea is to model the propagation of interference by aggregating the information of neighbors before training and preserve the aggregated results for training our networks. We conduct intensive experiments on graph data consisting of up to a hundred thousand units and millions of edges. We show that the proposed method achieves superior or comparable performance to the existing GNN-based methods in ITE estimation, while the proposed method can be executed much faster than GNN-based methods.},
  archive      = {J_ML},
  author       = {Lin, Xiaofeng and Bao, Han and Cui, Yan and Takeuchi, Koh and Kashima, Hisashi},
  doi          = {10.1007/s10994-024-06694-w},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Scalable individual treatment effect estimator for large graphs},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Let the noise flow away: Combating noisy labels using normalizing flows. <em>ML</em>, <em>114</em>(1), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06695-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce NoiseFlow, a generative network that addresses the issue of noisy labels in classification problems by modeling the entire label distribution based on the input data/image. Unlike previous methods, which assign each input to only one specific class, NoiseFlow generates different labels by considering the image and a random noise drawn from a standard normal distribution. This approach improves generalization performance since it does not require extensive parameter adjustments to fit the unknown data noise. To model the label distribution, we use conditional normalizing flows, which are effective at avoiding mode collapse and ensuring the presence of the correct label in the distribution for accurate classification. Moreover, NoiseFlow can be combined with other training strategies, such as mixup interpolation and contrastive learning, to achieve even better performance. We compared NoiseFlow with baseline methods on several synthetic and real-world datasets, and the experiment results demonstrate its effectiveness.},
  archive      = {J_ML},
  author       = {Su, Kuan-An and Su, YiHao and Lien, Yun-Hsuan and Wang, Yu-Shuen},
  doi          = {10.1007/s10994-024-06695-9},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Let the noise flow away: Combating noisy labels using normalizing flows},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is it still fair? a comparative evaluation of fairness algorithms through the lens of covariate drift. <em>ML</em>, <em>114</em>(1), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06698-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last few decades, machine learning (ML) applications have grown exponentially, yielding several benefits to society. However, these benefits are tempered with concerns of discriminatory behaviours exhibited by ML models. In this regard, fairness in machine learning has emerged as a priority research area. Consequently, several fairness metrics and algorithms have been developed to mitigate against discriminatory behaviours that ML models may possess. Yet still, very little attention has been paid to the problem of naturally occurring changes in data patterns (aka data distributional drift), and its impact on fairness algorithms and metrics. In this work, we study this problem comprehensively by analyzing 4 fairness-unaware baseline algorithms and 7 fairness-aware algorithms, carefully curated to cover the breadth of its typology, across 5 datasets including public and proprietary data, and evaluated them using 3 predictive performance and 10 fairness metrics. In doing so, we show that (1) data distributional drift is not a trivial occurrence, and in several cases can lead to serious deterioration of fairness in so-called fair models; (2) contrary to some existing literature, the size and direction of data distributional drift is not correlated to the resulting size and direction of unfairness; and (3) choice of, and training of fairness algorithms is impacted by the effect of data distributional drift which is largely ignored in the literature. Emanating from our findings, we synthesize several policy implications of data distributional drift on fairness algorithms that can be very relevant to stakeholders and practitioners.},
  archive      = {J_ML},
  author       = {Deho, Oscar Blessed and Bewong, Michael and Kwashie, Selasi and Li, Jiuyong and Liu, Jixue and Liu, Lin and Joksimovic, Srecko},
  doi          = {10.1007/s10994-024-06698-6},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Is it still fair? a comparative evaluation of fairness algorithms through the lens of covariate drift},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to rank aspects and opinions for comparative explanations. <em>ML</em>, <em>114</em>(1), 1-20. (<a href='https://doi.org/10.1007/s10994-024-06699-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative recommendation explanations help to make sense of recommendations by comparing a recommended item along some aspects of interest with one or many items being considered. This work extends the notion of comparative explanations, by going beyond merely better/worse statements, to further incorporate aspect-level opinions for more informative comparisons. To enhance the quality of both the personalized recommendation and the explanation, we incorporate optimization objectives that preserve relative rankings of aspects and opinions, in addition to the classical rankings of overall preferences for items. We integrate the multiple ranking objectives and multi-tensor factorization together. Experiments on datasets of different domains validate the efficacy of our proposed framework in both recommendation and comparative explanation against comparable explainable recommendation baselines.},
  archive      = {J_ML},
  author       = {Le, Trung-Hoang and Lauw, Hady W.},
  doi          = {10.1007/s10994-024-06699-5},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Mach. Learn.},
  title        = {Learning to rank aspects and opinions for comparative explanations},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Credal ensembling in multi-class classification. <em>ML</em>, <em>114</em>(1), 1-62. (<a href='https://doi.org/10.1007/s10994-024-06703-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a formal framework to (1) aggregate probabilistic ensemble members into either a representative classifier or a credal classifier, and (2) perform various decision tasks based on this uncertainty quantification. We first elaborate on the aggregation problem under a class of distances between distributions. We then propose generic methods to robustify uncertainty quantification and decisions, based on the obtained ensemble and representative probability. To facilitate the scalability of the proposed framework, for all the problems and applications covered, we elaborate on their computational complexities from the theoretical aspects and leverage theoretical results to derive efficient algorithmic solutions. Finally, relevant sets of experiments are conducted to assess the usefulness of the proposed framework in uncertainty sampling, classification with a reject option, and set-valued prediction-making.},
  archive      = {J_ML},
  author       = {Nguyen, Vu-Linh and Zhang, Haifei and Destercke, Sébastien},
  doi          = {10.1007/s10994-024-06703-y},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-62},
  shortjournal = {Mach. Learn.},
  title        = {Credal ensembling in multi-class classification},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPA: A poisoning attack framework for graph neural networks through searching and pairing. <em>ML</em>, <em>114</em>(1), 1-31. (<a href='https://doi.org/10.1007/s10994-024-06706-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNN) have played an important role in many fields, while GNNs also suffer from adversarial attacks that aim to malfunction the GNN model by changing the adjacency matrix (i.e. generating adversarial edges) or node features (i.e. generating adversarial features) in graph data. Although the gradient-based adversarial attack methods have achieved remarkable results in DNNs, optimizing discrete adversarial edges in graph data using continuous gradients may lead to sub-optimal solutions. In order to alleviate this situation, we propose a novel Searching and Pairing Attack (SPA) method to effectively generate adversarial edges by treating each adversarial edge as a combination of a pair of adversarial nodes. The proposed SPA method generates the adversarial edges through a Node Searching step and a Node Pairing step. The proposed Node Searching Ant Colony Optimization (NS-ACO) improves the attack effect by using the ability of heuristic algorithm to quickly find the approximate optimal solution, while in the Node Pairing (NP) step we propose a generative graph convolutional network with a novel Aggregate Cooperative (AC) layer to generate a set of nodes that meet the constraints, so as to obtain the perturbation set together with the Node Searching step. The proposed SPA method outperforms the state-of-the-art adversarial attack methods and achieves a misclassification rate of 32.5% in the poisoning attack on Cora dataset with a perturbation rate of 0.5%.},
  archive      = {J_ML},
  author       = {Liu, Xiao and Huang, Jun-Jie and Zhao, Wentao and Wang, Ziyue and Chen, Zihan and Pan, Yi},
  doi          = {10.1007/s10994-024-06706-9},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Mach. Learn.},
  title        = {SPA: A poisoning attack framework for graph neural networks through searching and pairing},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScoreCL: Augmentation-adaptive contrastive learning via score-matching function. <em>ML</em>, <em>114</em>(1), 1-22. (<a href='https://doi.org/10.1007/s10994-024-06707-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised contrastive learning (CL) has achieved state-of-the-art performance in representation learning by minimizing the distance between positive pairs while maximizing that of negative ones. Recently, it has been verified that the model learns better representation with diversely augmented positive pairs because they enable the model to be more view-invariant. However, only a few studies on CL have considered the difference between augmented views, and have not gone beyond the hand-crafted findings. In this paper, we first observe that the score-matching function can measure how much data has changed from the original through augmentation. With the observed property, every pair in CL can be weighted adaptively by the difference of score values, resulting in boosting the performance. We show the generality of our method, referred to as ScoreCL, by consistently improving various CL methods, SimCLR, SimSiam, W-MSE, and VICReg, up to 3%p in image classifcation on CIFAR and ImageNet datasets. Moreover, we have conducted exhaustive experiments and ablations, including results on diverse downstream tasks, comparison with possible baselines, and further applications when used with other augmentation methods. We hope our exploration will inspire more research in exploiting the score matching for CL.},
  archive      = {J_ML},
  author       = {Kim, Jin-Young and Kwon, Soonwoo and Go, Hyojun and Lee, Yunsung and Choi, Seungtaek and Kim, Hyun-Gyoon},
  doi          = {10.1007/s10994-024-06707-8},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Mach. Learn.},
  title        = {ScoreCL: Augmentation-adaptive contrastive learning via score-matching function},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep metric loss for multimodal learning. <em>ML</em>, <em>114</em>(1), 1-19. (<a href='https://doi.org/10.1007/s10994-024-06709-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning often outperforms its unimodal counterparts by exploiting unimodal contributions and cross-modal interactions. However, focusing only on integrating multimodal features into a unified comprehensive representation overlooks the unimodal characteristics. In real data, the contributions of modalities can vary from instance to instance, and they often reinforce or conflict with each other. In this study, we introduce a novel MultiModal loss paradigm for multimodal learning, which subgroups instances according to their unimodal contributions. MultiModal loss can prevent inefficient learning caused by overfitting and efficiently optimize multimodal models. On synthetic data, MultiModal loss demonstrates improved classification performance by subgrouping difficult instances within certain modalities. On four real multimodal datasets, our loss is empirically shown to improve the performance of recent models. Ablation studies verify the effectiveness of our loss. Additionally, we show that our loss generates a reliable prediction score for each modality, which is essential for subgrouping. Our MultiModal loss is a novel loss function to subgroup instances according to the contribution of modalities in multimodal learning and is applicable to a variety of multimodal models with unimodal decisions. Our code is available at https://github.com/DMCB-GIST/MultiModalLoss},
  archive      = {J_ML},
  author       = {Moon, Sehwan and Lee, Hyunju},
  doi          = {10.1007/s10994-024-06709-6},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Mach. Learn.},
  title        = {Deep metric loss for multimodal learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting short-term passenger flow via CBGC-SCI: An in-depth comparative study on shenzhen metro. <em>ML</em>, <em>114</em>(1), 1-38. (<a href='https://doi.org/10.1007/s10994-024-06711-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term forecasts of passenger flow are crucial for the organization and management of metro systems. However, passenger flow in metro networks exhibits unique and complex patterns due to temporal dependencies associated with operating periods, graph-based spatial dependencies among distant stations, and correlation between inflow and outflow. To address these challenges, this paper proposes a novel deep learning approach, Chebyshev Graph Convolutional-Sample Convolution and Interaction Network (CBGC-SCI), for forecasting metro passenger flow. CBGC-SCI comprises two components: a spatial module based on Chebyshev polynomials graph convolution and a temporal module based on a downsample-convolve-interact architecture. The spatial module effectively captures graph-structured adjacency relations among metro stations, particularly those at a distance, by aggregating high-order node information. The temporal module extracts nonlinear passenger flow dynamics and long-term temporal dependencies, offering multi-step forecasting results with high predictive accuracy. Moreover, using smart card data of Shenzhen Metro collected via the Automatic Fare Collection (AFC) systems, we conduct extensive comparative experiments for methods validation, involving different input time steps, the correlation between passenger inflow and outflow, various operating periods, and diverse Graph Neural Network (GNN) modules. The results show that CBGC-SCI can well capture distant graph-based spatial dependencies and temporal dependencies, and outperform state-of-the-art baselines in terms of forecasting accuracy. Besides, considering the correlation between passenger inflow and outflow enhances the model's forecasting accuracy. Considering various metro passenger flow patterns during different operating periods, our comparative analysis of forecasting results highlights the superior performance of our proposed model across all operational conditions.},
  archive      = {J_ML},
  author       = {He, Yuxin and Hong, Weihang and Li, Lishuai and Zhang, Jinlei and Qin, Jin and Luo, Qin},
  doi          = {10.1007/s10994-024-06711-y},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Mach. Learn.},
  title        = {Forecasting short-term passenger flow via CBGC-SCI: An in-depth comparative study on shenzhen metro},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast online feature selection in streaming data. <em>ML</em>, <em>114</em>(1), 1-35. (<a href='https://doi.org/10.1007/s10994-024-06712-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of getting big amounts of high-quality labeled data is compounded by the fact that data labeling is often subjective and requires significant human effort. In many cases, the quality of the labeled data depends entirely on the expertise and experience of human annotators, making it challenging to ensure labeling accuracy in large and dynamic datasets. Moreover, there may be a significant delay between the arrival of a new instance and its manual labeling. This paper explores the use of fully unsupervised feature selection algorithms in non-stationary data streams, where the importance of features may change over time. We introduce a novel feature selection algorithm called Online Fast FEa-ture SELection-OFFESEL, which calculates the feature importance scores in each incoming window based on their mean normalized values and without using any class labels. We evaluate OFFESEL on 17 benchmark data streams, both stationary and non-stationary, using popular online classifiers like PerceptronMask, VFDT, Online Boosting, and Linear SVM. We compare OFFESEL to several other feature selection algorithms, including state-of-the-art supervised ones like FIRES and ABFS, as well as popular unsupervised ones like MCFS, LS, and Max Variance, which we adapted to data streams. Our results indicate that OFFESEL outperforms all supervised and unsupervised feature selection algorithms in terms of classification accuracy. Specifically, OFFESEL preserves the accuracy level of the supervised FIRES algorithm, which proved more accurate than ABFS in our experiments, while maintaining the accuracy level achieved by the unsupervised Max Variance algorithm. Moreover, OFFESEL requires even less computation time than Max Variance and shows high stability on stationary datasets. Overall, our study demonstrates the potential benefits of using unlabeled data for feature ranking and selection in dynamic data streams.},
  archive      = {J_ML},
  author       = {Hochma, Yael and Last, Mark},
  doi          = {10.1007/s10994-024-06712-x},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Mach. Learn.},
  title        = {Fast online feature selection in streaming data},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PerfoRank: Cluster-based performance ranking for improved performance evaluation and estimation in professional cycling. <em>ML</em>, <em>114</em>(1), 1-30. (<a href='https://doi.org/10.1007/s10994-024-06716-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current cycling analytics solutions do not account for the race course profile or the level of the competition. Therefore, this paper develops a unique two-stage clustering-based ranking approach for rider evaluation. Initially, races are segmented into coherent clusters based upon elevation and road surface type. Subsequently, underlying skill levels are determined per cluster through the observed race results using the TrueSkill algorithm which allows to model multi-entrant competitions. The results indicate that our approach uncovers clusters which match the commonly known specializations in road cycling. The ranking methodology generates skill ratings which enable the identification of specialization and can be used in downstream tasks. Our results show that the proposed rankings drastically improve race outcome estimation when adding these rankings as features to the current prediction models.},
  archive      = {J_ML},
  author       = {Janssens, Bram and Bogaert, Matthias},
  doi          = {10.1007/s10994-024-06716-7},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Mach. Learn.},
  title        = {PerfoRank: Cluster-based performance ranking for improved performance evaluation and estimation in professional cycling},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair prediction sets through multi-objective hyperparameter optimization. <em>ML</em>, <em>114</em>(1), 1-29. (<a href='https://doi.org/10.1007/s10994-024-06721-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread implementation of machine learning in safety-critical domains has raised ethical concerns regarding algorithmic discrimination. In such settings, the integration of fairness-aware algorithms with uncertainty quantification tools enables the development of reliable and safe decision-making. In this paper, we introduce a novel methodology that combines conformal prediction, offering rigorous prediction sets, with multi-objective optimization via evolutionary learning. The proposed meta-algorithm optimizes the hyperparameter configuration of classifiers to produce confidence predictors that balance efficiency and equalized coverage guarantees, addressing fairness concerns related to sensitive attributes. We empirically evaluate our methodology with four real-world problems and demonstrate its efficacy in exploring this trade-off and producing a repertoire of Pareto optimal conformal predictors. In this way, our contribution offers different modeling alternatives from which to choose depending on the policy adopted by stakeholders, thus illustrating its capability to enhance equitable decision-making.},
  archive      = {J_ML},
  author       = {García-Galindo, Alberto and López-De-Castro, Marcos and Armañanzas, Rubén},
  doi          = {10.1007/s10994-024-06721-w},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Mach. Learn.},
  title        = {Fair prediction sets through multi-objective hyperparameter optimization},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resolving power: A general approach to compare the distinguishing ability of threshold-free evaluation metrics. <em>ML</em>, <em>114</em>(1), 1-21. (<a href='https://doi.org/10.1007/s10994-024-06723-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting an evaluation metric is fundamental to model development, but uncertainty remains about when certain metrics are preferable and why. This paper introduces the concept of resolving power to describe the ability of an evaluation metric to distinguish between binary classifiers of similar quality. This ability depends on two attributes: 1. The metric’s response to improvements in classifier quality (its signal), and 2. The metric’s sampling variability (its noise). The paper defines resolving power generically as a metric’s sampling uncertainty scaled by its signal. A simulation study compares the area under the receiver operating characteristic curve (AUROC) and the area under the precision–recall curve (AUPRC) in a variety of contexts. It finds that the AUROC generally has greater resolving power, but that the AUPRC is better when searching among high-quality classifiers applied to low prevalence outcomes. The paper also proposes an empirical method to estimate resolving power that can be applied to any dataset and any initial classification model. The AUROC is useful for developing the resolving power concept, but it has been criticized for being misleading. Newer metrics developed to address its interpretative issues can be easily incorporated into the resolving power framework. The best metrics for model search will be both interpretable and high in resolving power. Sometimes these objectives will conflict and how to address this tension remains an open question.},
  archive      = {J_ML},
  author       = {Beam, Colin},
  doi          = {10.1007/s10994-024-06723-8},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {Resolving power: A general approach to compare the distinguishing ability of threshold-free evaluation metrics},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qualitative control learning can be much faster than reinforcement learning. <em>ML</em>, <em>114</em>(1), 1-21. (<a href='https://doi.org/10.1007/s10994-024-06724-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning has emerged as a prominent method for controlling dynamic systems in the absence of a precise mathematical model. However, its reliance on extensive interactions with the environment often leads to prolonged training periods. In this paper, we propose an alternative approach to learning control policies that focuses on learning qualitative models and uses symbolic planning to derive a qualitative plan for the control task, which is executed by an adaptive reactive controller. We conduct experiments utilizing our approach on the cart-pole problem, a standard benchmark in dynamic system control. We additionally extend this problem domain to include uneven terrains, such as driving over craters or hills, to assess the robustness of learned controllers. Our results indicate that qualitative learning offers significant advantages over reinforcement learning in terms of sample efficiency, transferability, and interpretability. We demonstrate that our proposed approach is at least two orders of magnitude more sample efficient in the cart-pole domain than the usual variants of reinforcement learning.},
  archive      = {J_ML},
  author       = {Šoberl, Domen and Bratko, Ivan},
  doi          = {10.1007/s10994-024-06724-7},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Mach. Learn.},
  title        = {Qualitative control learning can be much faster than reinforcement learning},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self explainable graph convolutional recurrent network for spatio-temporal forecasting. <em>ML</em>, <em>114</em>(1), 1-31. (<a href='https://doi.org/10.1007/s10994-024-06725-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is transforming industries and decision-making processes, but concerns about transparency and fairness have increased. Explainable artificial intelligence (XAI) is crucial to address these concerns, providing transparency in AI decision making, alleviating the effect of biases and fostering trust. However, the application of XAI in conjunction with problems with spatio-temporal components represents a challenge due to the small number of options, which when implemented penalize performance in exchange for the explainability obtained. This paper proposes self explainable graph convolutional recurrent network (SEGCRN), a model that seeks to integrate explainability into the architecture itself, seeking to increase the ability to infer the relationship and dependence between the different nodes, proposing an alternative to explainability techniques, which are applied as a second layer. The proposed model has been able to show in different data sets the ability to reduce the amount of information needed to make a prediction, while reducing the impact on the prediction caused by applying an explainability technique, having managed to reduce the use of information without loss of accuracy. Thus, SEGCRN is proposed as a gray box, which allows a better understanding of its behavior than black box models, having validated the model with traffic data, combining both spatial and temporal components, achieving promising results.},
  archive      = {J_ML},
  author       = {García-Sigüenza, Javier and Curado, Manuel and Llorens-Largo, Faraon and Vicent, Jose F.},
  doi          = {10.1007/s10994-024-06725-6},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Mach. Learn.},
  title        = {Self explainable graph convolutional recurrent network for spatio-temporal forecasting},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unmasking deception: A topic-oriented multimodal approach to uncover false information on social media. <em>ML</em>, <em>114</em>(1), 1-22. (<a href='https://doi.org/10.1007/s10994-024-06727-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital landscape, social media has emerged as a prevalent channel for global communication, connecting like-minded individuals worldwide. However, while facilitating information exchange, it is also susceptible to the dissemination of false information, posing a constant challenge to the reliability of online content. To address this issue, this paper introduces a novel methodology called TM-FID (Topic-oriented Multimodal False Information Detection), which combines false information detection and neural topic modeling within a semi-supervised multimodal approach. By jointly leveraging textual and visual information contained in online news, our approach provides insights into how false information influences specific discussion topics, thus enabling a comprehensive and fine-grained understanding of its spread and impact on social media conversation. Experimental evaluation carried out on a set of multimodal gossip-related news demonstrates the quality of the identified topics, assessed through a novel centroid-based metric, as well as the efficacy of the cross-attention mechanism used within TM-FID to accurately identify false information in multimodal news. Overall, the proposed methodology can enable effective strategies to counter the spread of false information, thereby fostering trust and confidence in the information shared on social media platforms.},
  archive      = {J_ML},
  author       = {Cantini, Riccardo and Cosentino, Cristian and Kilanioti, Irene and Marozzo, Fabrizio and Talia, Domenico},
  doi          = {10.1007/s10994-024-06727-4},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Mach. Learn.},
  title        = {Unmasking deception: A topic-oriented multimodal approach to uncover false information on social media},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning multi-axis representation in frequency domain for medical image segmentation. <em>ML</em>, <em>114</em>(1), 1-15. (<a href='https://doi.org/10.1007/s10994-024-06728-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Visual Transformer (ViT) has been extensively used in medical image segmentation (MIS) due to applying self-attention mechanism in the spatial domain to modeling global knowledge. However, many studies have focused on improving models in the spatial domain while neglecting the importance of frequency domain information. Therefore, we propose Multi-axis External Weights UNet (MEW-UNet) based on the U-shape architecture by replacing self-attention in ViT with our Multi-axis External Weights block. Specifically, our block performs a Fourier transform on the three axes of the input features and assigns the external weight in the frequency domain, which is generated by our External Weights Generator. Then, an inverse Fourier transform is performed to change the features back to the spatial domain. We evaluate our model on four datasets, including Synapse, ACDC, ISIC17 and ISIC18 datasets, and our approach demonstrates competitive performance, owing to its effective utilization of frequency domain information.},
  archive      = {J_ML},
  author       = {Ruan, Jiacheng and Gao, Jingsheng and Xie, Mingye and Xiang, Suncheng},
  doi          = {10.1007/s10994-024-06728-3},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Learn.},
  title        = {Learning multi-axis representation in frequency domain for medical image segmentation},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt-based contrastive learning to combat the COVID-19 infodemic. <em>ML</em>, <em>114</em>(1), 1-24. (<a href='https://doi.org/10.1007/s10994-024-06731-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has brought about an influx of misinformation and disinformation online, especially on social media. The World Health Organization has identified combating this infodemic as one of its top priorities, as false and misleading information can lead to negative consequences, such as the spread of conspiracy theories, false remedies, and xenophobia. This study presents a prompt-based contrastive learning approach that can be employed to address this issue. This method was designed to overcome challenges such as data scarcity and class imbalance commonly found in social media. Fighting the infodemic is modeled as a series of text classification problems in which questions relevant to credibility of the texts, their potential harm to society and the necessity of government intervention need to be answered. Experiments show that prompt-based contrastive learning is effective in assessing the accuracy of COVID-19-related online text.},
  archive      = {J_ML},
  author       = {Peng, Zifan and Li, Mingchen and Wang, Yue and Mo, Daniel Y.},
  doi          = {10.1007/s10994-024-06731-8},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Mach. Learn.},
  title        = {Prompt-based contrastive learning to combat the COVID-19 infodemic},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive parameters identification for nonlinear dynamics using deep permutation invariant networks. <em>ML</em>, <em>114</em>(1), 1-36. (<a href='https://doi.org/10.1007/s10994-024-06732-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The promising outcomes of dynamical system identification techniques, such as SINDy (Brunton et al. in Proc Natl Acad Sci 113(15):3932–3937, 2016), highlight their advantages in providing qualitative interpretability and extrapolation compared to non-interpretable deep neural networks (Rudin in Nat Mach Intell 1(5):206–215, 2019). These techniques suffer from parameter updating in real-time use cases, especially when the system parameters are likely to change during or between processes. Recently, the OASIS (Bhadriraju et al. in AIChE J 66(11):16980, 2020) framework introduced a data-driven technique to address the limitations of real-time dynamical system parameters updating, yielding interesting results. Nevertheless, we show in this work that superior performance can be achieved using more advanced model architectures. We present an innovative encoding approach, based mainly on the use of Set Encoding methods of sequence data, which give accurate adaptive model identification for complex dynamic systems, with variable input time series length. Two Set Encoding methods are used: the first is Deep Set (Zaheer et al. in Adv Neural Inf Process Syst 30, 2017), and the second is Set Transformer (Lee et al. in: International conference on machine learning, PMLR, pp 3744–3753 2019). Comparing Set Transformer to OASIS framework on Lotka–Volterra for real-time local dynamical system identification and time series forecasting, we find that the Set Transformer architecture is well adapted to learning relationships within data sets. We then compare the two Set Encoding methods based on the Lorenz system for online global dynamical system identification. Finally, we trained a Deep Set model to perform identification and characterization of abnormalities for 1D heat-transfer problem.},
  archive      = {J_ML},
  author       = {Elaarabi, Mouad and Borzacchiello, Domenico and Bot, Philippe Le and Guennec, Yves L. E. and Comas-Cardona, Sebastien},
  doi          = {10.1007/s10994-024-06732-7},
  journal      = {Machine Learning},
  month        = {1},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Mach. Learn.},
  title        = {Adaptive parameters identification for nonlinear dynamics using deep permutation invariant networks},
  volume       = {114},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
