<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DMKD</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dmkd">DMKD - 90</h2>
<ul>
<li><details>
<summary>
(2025). TSelect: Selecting relevant and non-redundant channels for multivariate time series classification. <em>DMKD</em>, <em>39</em>(6), 1-62. (<a href='https://doi.org/10.1007/s10618-025-01132-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many time series classification tasks, each instance is described by multiple channels (i.e., signals). This introduces an additional computational burden as the time and resources to train a classifier increase as more channels become available. This is problematic as some tasks can be described by a huge number of channels. However, not all channels may be necessary as some could be irrelevant or redundant, and including these can (dramatically) increase run time while yielding no benefit in terms of predictive performance. Therefore, it can be useful to automatically select a subset of the channels to include in the analysis. We propose TSelect, a novel scalable and classifier-agnostic approach that automatically selects a relevant and non-redundant subset of the channels for multivariate time series classification (MTSC). Experimentally, we show on a large benchmark suite that TSelect (1) eliminates on average 62% of the channels, (2) significantly improves a classifier’s run time without sacrificing predictive performance, and (3) outperforms the two state-of-the-art channel selectors (ECS and ECP) on the majority of the experiments.},
  archive      = {J_DMKD},
  author       = {Nuyts, Loren and Perini, Lorenzo and Davis, Jesse},
  doi          = {10.1007/s10618-025-01132-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-62},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TSelect: Selecting relevant and non-redundant channels for multivariate time series classification},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topic-aware influence maximization with deep reinforcement learning and graph attention networks. <em>DMKD</em>, <em>39</em>(6), 1-35. (<a href='https://doi.org/10.1007/s10618-025-01133-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization is a fundamental problem in network analysis, focusing on identifying a subset of nodes in a social network to maximize the spread of influence. In this paper, we present an approach for tackling the Influence Maximization (IM) problem, integrating Deep Reinforcement Learning (DRL) techniques with attentive Graph Neural Networks (GATs). Our study builds upon a prior algorithm (S2V-DQN-IM) and progressively refines it towards IM-GNN, ultimately achieving competitive performance against state-of-the-art methods on classic IM. Through experiments on benchmark datasets, we empirically validate the effectiveness of graph attention mechanisms and positional encoding, using the graph magnetic Laplacian, to reach state-of-the-art performance in terms of influence spread. Building on this success, we extend our IM-GNN framework to incorporate topic-awareness in TIM-GNN, recognizing the inherent topical nature of real-world diffusions. By harnessing probabilistic techniques, we construct topic-aware social graphs using real cascades and assess the effectivenesss of TIM-GNN on them. Our extensive experimental results validate the utility of our topic-aware approach, demonstrating significant advances over existing topic-aware IM methods. Finally, in order to improve upon performance (latency) at query time, we develop a variant of TIM-GNN, called TIM-GNN $$^x$$ , by using cross-attention mechanisms. We show it maintains comparable overall spread performance as its predecessor, while achieving a 10x-20x speed-up.},
  archive      = {J_DMKD},
  author       = {Halal, Taha and Cautis, Bogdan and Groz, Benoît and Gao, Ruize},
  doi          = {10.1007/s10618-025-01133-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Topic-aware influence maximization with deep reinforcement learning and graph attention networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multimodal graph learning for knowledge graph completion. <em>DMKD</em>, <em>39</em>(6), 1-27. (<a href='https://doi.org/10.1007/s10618-025-01139-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal knowledge graphs, which integrate visual and textual data, have been widely utilized in various applications. Despite their potential, they often suffer from incompleteness due to the large amount of undiscovered valuable triple knowledge within the graph. This has led to a surge in research on multimodal knowledge graph completion. However, existing methods often face challenges such as irrelevant noise in multimodal data and limitations of straightforward multimodal fusion, which can lead to suboptimal model performance. In this paper, we propose a novel adaptive multimodal graph learning approach for efficient knowledge graph completion. We first introduce an adaptive multimodal knowledge capture module designed to integrate entity-related multimodal knowledge. This module includes a relation-aware modal view construction process to achieve semantic meaning consistency, along with link-aware point-to-face interactions for coarse-grained multimodal capture and adaptive point-to-point interactions for fine-grained multimodal extraction. We then propose a two-stage multimodal graph fusion module, which includes a cross-modal augmentation module to perform first-stage multimodal fusion between the entity graph and visual/textual graph, as well as a dynamic selection fusion module to conduct the second-stage fusion between entity-visual and entity-textual graphs. Our method demonstrates superior effectiveness through empirical evaluations on three common datasets.},
  archive      = {J_DMKD},
  author       = {Peng, Jie and Shan, Yongxue and Zha, Yongfu and Wang, Xiaodong},
  doi          = {10.1007/s10618-025-01139-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Adaptive multimodal graph learning for knowledge graph completion},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection and evaluation of clusters within sequential data. <em>DMKD</em>, <em>39</em>(6), 1-30. (<a href='https://doi.org/10.1007/s10618-025-01140-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential data is ubiquitous—it is routinely gathered to gain insights into complex processes such as behavioral, biological, or physical processes. Challengingly, such data not only has dependencies within the observed sequences, but the observations are also often high-dimensional, sparse, and noisy. These are all difficulties that obscure the inner workings of the complex process under study. One solution is to calculate a low-dimensional representation that describes (characteristics of) the complex process. This representation can then serve as a proxy to gain insight into the original process. However, uncovering such low-dimensional representation within sequential data is nontrivial due to the dependencies, and an algorithm specifically made for sequences is needed to guarantee estimator consistency. Fortunately, recent theoretical advancements on Block Markov Chains have resulted in new clustering algorithms that can provably do just this in synthetic sequential data. This paper presents a first field study of these new algorithms in real-world sequential data; a wide empirical study of clustering within a range of data sequences. We investigate broadly whether, when given sparse high-dimensional sequential data of real-life complex processes, useful low-dimensional representations can in fact be extracted using these algorithms. Concretely, we examine data sequences containing GPS coordinates describing animal movement, strands of human DNA, texts from English writing, and daily yields in a financial market. The low-dimensional representations we uncover are shown to not only successfully encode the sequential structure of the data, but also to enable gaining new insights into the underlying complex processes.},
  archive      = {J_DMKD},
  author       = {Van Werde, Alexander and Senen–Cerda, Albert and Kosmella, Gianluca and Sanders, Jaron},
  doi          = {10.1007/s10618-025-01140-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Detection and evaluation of clusters within sequential data},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fastere: A fast framework for entity relation extractions. <em>DMKD</em>, <em>39</em>(6), 1-27. (<a href='https://doi.org/10.1007/s10618-025-01146-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity Relation Extraction (ERE) aims to identify semantic relations between entities from unstructured texts. Despite achieving promising results, existing methods face significant efficiency challenges due to the overwhelming presence of irrelevant candidate entity pairs–with over 96% of candidate pairs lacking meaningful relations in typical datasets. This computational burden severely limits the practical applicability of ERE systems. To address these limitations, we propose FastERE, a fast framework that introduces three key innovations: (1) a pruner-based Entity Pairs Selection Module that dynamically filters irrelevant entity pairs before relation extraction, reducing computational overhead while improving sample quality; (2) an efficient task reformulation that frames NER as sequence tagging with adjacent attention mechanisms and RE as grouped triplet prediction with masked parallel packing; and (3) a multi-task learning framework with prefix-enhanced shared encoders that enables effective feature interaction while maintaining task independence. Experimental results on benchmark datasets demonstrate that FastERE achieves 6-20 $$\times $$ speedup over state-of-the-art methods while maintaining competitive accuracy, with F1 scores of 69.9% and 67.0% on ACE05 and ACE04 respectively. Our code is available at https://github.com/xerrors/FastERE .},
  archive      = {J_DMKD},
  author       = {Zhang, Wenjie and Xu, Tianyang and Hua, Yang and Feng, Zhenhua and Song, Xiaoning},
  doi          = {10.1007/s10618-025-01146-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fastere: A fast framework for entity relation extractions},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metadata supported scale space attention networks for multivariate timeseries prediction. <em>DMKD</em>, <em>39</em>(6), 1-46. (<a href='https://doi.org/10.1007/s10618-025-01151-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate two problems faced in detecting/predicting intracranial hypertension (ICH) onsets. The first challenge is that intracranial pressure (ICP) can be measured only invasively, drilling a hole in patients skull. To tackle this challenge, we propose a novel metadata supported scale space attention (SSA) network to accurately and non-invasively predict intracranial hypertension (ICH) onsets, relying on supporting multi-variate data streams, including electroencephalogram (EEG), arterial blood pressure (ABP) and electrocardiogram (ECG). In particular, we propose a coupled network, integrating a convolutional neural network with scale space attention (CNN-SSA) and a long short-term memory (LSTM) network. This, however, opens up a new challenge—EEG readings at different health centers may rely on hardware with different numbers of sensors, which would render models not shareable across centers. To tackle this second challenge, we propose two alternative techniques: the first alternative relies on a multi-variate multi-scale neural network (M2NN) to impute missing EEG channels; the second alternative, on the other hand, identifies a core subset of EEG channels, shared by different centers, that can still provide accurate ICH predictions. We conduct rigorous experiments to substantiate our proposed method’s effectiveness—the results show that ICH can be predicted accurataly by relying on supporting data modalities, if we can properly attend the data at multiple scales and across modalities. Additionally, an ablation study demonstrates that the various components of our approach are essential in providing accurate ICH predictions.},
  archive      = {J_DMKD},
  author       = {Ravindranath, Manjusha and Candan, K. Selçuk and Appavu, Brian},
  doi          = {10.1007/s10618-025-01151-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-46},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Metadata supported scale space attention networks for multivariate timeseries prediction},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overlapping community detection with a new modularity measure in directed weighted networks. <em>DMKD</em>, <em>39</em>(6), 1-37. (<a href='https://doi.org/10.1007/s10618-025-01153-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying overlapping communities in complex networks is an important aspect of network analysis. This task, however, becomes extremely difficult when working with large-scale directed weighted networks, such as social media networks like Facebook, X, LinkedIn, biological systems, collaboration networks, neural networks, etc., which may consist of millions of entities. There are very few algorithms available for this task, and most are merely extensions of well-known algorithms designed for undirected networks. Besides, these algorithms often produce disjoint and low-quality community structures. On the other hand, evaluating overlapping community structures in directed weighted networks is also a major challenge due to the lack of specific quality measure designed for such networks. To address these challenges, we developed an algorithm called Community Detection in Directed And Weighted Networks (CD-DAWN). It is designed to identify overlapping communities in large, complex networks by selecting and expanding seeds, without needing any prior knowledge about the communities. We also developed a directed weighted overlapping modularity ( $$Q_\text {dwo}$$ ) to evaluate overlapping community structures in directed weighted networks, which is the first such measure to the best of our knowledge. To evaluate the effectiveness of CD-DAWN, we performed extensive experiments on both real-world and artificial networks. The results demonstrate that the proposed approach outperforms the baseline algorithms, accurately detecting high-quality and stable overlapping communities in complex networks.},
  archive      = {J_DMKD},
  author       = {Kumar, Abhinav and Kumari, Anjali and Kumar, Pawan and Dohare, Ravins},
  doi          = {10.1007/s10618-025-01153-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Overlapping community detection with a new modularity measure in directed weighted networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stable graph based decision route explanation in siamese neural networks. <em>DMKD</em>, <em>39</em>(6), 1-32. (<a href='https://doi.org/10.1007/s10618-025-01154-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese Neural Networks (SNNs) have shown promise in addressing a variety of tasks, even with limited data availability. However, their adoption is hindered by the lack of transparency in their decision-making processes. A key challenge in explaining SNNs lies in the absence of an inverse mapping between high-dimensional input feature vectors and the low-dimensional embedding space. Therefore, computing direct distances between input features becomes meaningless. Existing autoencoder-based explanation methods face several limitations. These include poor image reconstruction quality due to insufficient data and the omission of final distance layer of the SNN during the explanation process. While the Siamese Network Explainer (SINEX) can explain audio and grayscale images, it does not support RGB images. To overcome these challenges, we propose a method called Features Distance-based eXplanation (FDbX). This approach identifies salient features using ridge regression, trained on perturbed SLIC-segmented images. To enhance the selection of important features, we incorporate Bayesian analysis, which assigns importance scores to features. To provide a comprehensive explanation of the decision route, we construct a mathematical model that represents important features and their Hamming distances as a bipartite graph. In this graph, nodes represent features and edges denote distances between feature pairs. The resulting explanation heatmaps highlight critical image segments, offering more intuitive and visually informative explanations than existing methods. We evaluate stability and faithfulness of our method using stability indices such as $$R^2$$ and mean squared error. To the best of our knowledge, this is the first work to introduce Variable and Coefficient Stability Indices for image datasets.},
  archive      = {J_DMKD},
  author       = {Saleem, Rabia and Anjum, Ashiq and Yuan, Bo and Liu, Lu},
  doi          = {10.1007/s10618-025-01154-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-32},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Stable graph based decision route explanation in siamese neural networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distilcyphergpt: Enhancing large language models for knowledge graph question answering in cypher through knowledge distillation. <em>DMKD</em>, <em>39</em>(6), 1-36. (<a href='https://doi.org/10.1007/s10618-025-01157-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Question Answering (KGQA) systems allow users to interact with knowledge graphs using natural language queries, which are translated into structured database queries like Cypher. Existing KGQA approaches often rely on large language models, leading to high computational costs and slower inference times that impede real-time applications. To address these challenges, DistilCypherGPT is introduced as an efficient KGQA framework employing knowledge distillation in a teacher-student architecture, optimized for Cypher query generation on academic knowledge graphs. DistilCypherGPT significantly reduces computational demands, enabling deployment in resource-constrained environments while retaining high accuracy. Experimental results show that DistilCypherGPT maintains 99.51% accuracy, achieving a 23% reduction in model size and a 30% improvement in inference speed compared to the baseline. These findings corroborate DistilCypherGPT’s potential as a scalable, high-performance solution for KGQA, advancing efficient, real-time query translation with minimal computational overhead.},
  archive      = {J_DMKD},
  author       = {Chong, You Li and Lee, Chin Poo and Lim, Kian Ming},
  doi          = {10.1007/s10618-025-01157-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-36},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Distilcyphergpt: Enhancing large language models for knowledge graph question answering in cypher through knowledge distillation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HRHE: Hybrid relations-guided multi-modal knowledge graph completion using hierarchical embeddings. <em>DMKD</em>, <em>39</em>(6), 1-31. (<a href='https://doi.org/10.1007/s10618-025-01150-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal knowledge graph completion (MKGC) has garnered substantial research interest, particularly in applying multi-modal knowledge graphs (MMKGs). Previous studies have proposed various approaches to address the MKGC problem, including a variety of knowledge graph embedding techniques and approaches based on large language models. However, these methods face two limitations. First, many models embed entities and relations in a regular embedding space, failing to capture multi-modal components’ hierarchical semantics. Second, existing methods struggle to learn hybrid relations between entities of different modalities, which constrains the performance of MKGC models. In this paper, we propose a novel model, Hybrid Relations-guided MKGC with Hierarchical Embeddings (HRHE), to address the aforementioned limitations. Specifically, HRHE projects MMKGs into a polar coordinate system to capture hierarchical semantics and facilitate intra-inter-class interactions. Subsequently, HRHE introduces a general hybrid relations learning approach to discover implicit relations among entities of different modalities. Noting that our work can flexibly employ various neural network architectures, including convolutional neural networks, multi-layer perceptrons, and linear graph networks, to infer the implicit associations of MMKGs. Finally, an objective function is defined to leverage the inferred hybrid relations in guiding the modeling process of MMKGs. We investigate HRHE using two widely evaluated benchmarks and show that HRHE achieves better results than state-of-the-art MKGC baselines. Extensive evaluations demonstrate that constructing hierarchical semantic space and utilizing implicit associations among entities of different modalities can advance MKGC.},
  archive      = {J_DMKD},
  author       = {Lu, Xinyu and Li, Hao and Lu, Wei and Chang, Yanshuo and Xue, Feng},
  doi          = {10.1007/s10618-025-01150-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {HRHE: Hybrid relations-guided multi-modal knowledge graph completion using hierarchical embeddings},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models are zero-shot point-of-interest recommenders. <em>DMKD</em>, <em>39</em>(6), 1-46. (<a href='https://doi.org/10.1007/s10618-025-01148-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point-of-interest (POI) recommendation systems play an important role in various location-based services by improving the user experience. Previous research has leveraged large-scale visit records to predict a user’s next visit POI based on the behavior of similar users. However, with the increasing emphasis on privacy preservation, there is a shift towards zero-shot recommendation that does not require training and only uses individual visit history data. As a better alternative to traditional zero-shot recommender systems, this paper proposes a novel zero-shot recommender system leveraging the ability of pre-trained large language models (LLMs) to understand human behavior called ZeroPOIRec. ZeroPOIRec involves a profiler module that enables LLMs to extract individual user preferences from multiple aspects, including spatio-temporal patterns and individual characteristics, and a recommender module that enhances the zero-shot POI recommendation performance via candidate refinement and prioritization. Through experiments using a benchmark dataset and a newly introduced real-world dataset with semantic variables, we demonstrate that, despite ZeroPOIRec being a zero-shot approach, it outperforms state-of-the-art methods in terms of recommendation performance.},
  archive      = {J_DMKD},
  author       = {Kim, Joeun and Seo, Youngjin and Kim, Yeonsoo and Kang, Junhyeok and Shin, Jeeho and Trirat, Patara and Lee, Jae-Gil},
  doi          = {10.1007/s10618-025-01148-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-46},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Large language models are zero-shot point-of-interest recommenders},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JammyTS: Joint attention and memory network for temporal scoping of facts. <em>DMKD</em>, <em>39</em>(6), 1-25. (<a href='https://doi.org/10.1007/s10618-025-01156-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Scoping of Facts is crucial for completing the temporal dimension of knowledge graphs. Current mainstream methods rely heavily on external resources for mining temporal information. However, the presence of noise in external resources, coupled with limitations in adaptively inferring non-continuous temporal dimensions with multiple temporal ranges, leads to low accuracy in predicting temporal ranges. To address these challenges, a model named JammyTS is proposed, which Joins an attention mechanism and a memory network for Temporal Scoping of facts. Specifically, JammyTS leverages attention to adjust the distribution of weights dynamically in memory networks and builds attention capsule-based networks to reduce the impact of noise in external resources. Furthermore, two linear classifiers are separately trained to infer the end and beginning timestamps of facts for inference of non-continuous temporal ranges. Extensive experiments on three datasets show that JammyTS improves the accuracy by up to 12.29% compared to the state-of-the-art.},
  archive      = {J_DMKD},
  author       = {Hu, Chenxi and Wu, Tao and Liu, Chunsheng and Chang, Chao},
  doi          = {10.1007/s10618-025-01156-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {JammyTS: Joint attention and memory network for temporal scoping of facts},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting and reacting to smart home novelties. <em>DMKD</em>, <em>39</em>(6), 1-32. (<a href='https://doi.org/10.1007/s10618-025-01158-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To be robust, AI systems need to quickly detect and effectively react to novelties in their environments. Novelty is characterized by a sudden change in the environment where this is little time to collect new data and retrain. This is particularly true for smart home systems, where novelties abound and accurate handling is required for reliable health monitoring and home automation that can react quickly and effectively to the novelties. In this paper, we introduce a Bayesian nonparametric method, called OTACON, for novelty handling. OTACON finds surprising situations using change point detection and adapts accordingly. To evaluate this proposed approach, we design a smart home novelty generator that embeds novelties of varying type and difficulty into CASAS real-world smart home datasets. We observe that OTACON outperforms a state-of-the-art method for eight types of novel scenarios, in some cases even outperforming its own pre-novelty baseline. The results provide evidence that this method can boost AI systems in their ability to handle a variety of unexpected situations.},
  archive      = {J_DMKD},
  author       = {Holder, Lawrence B. and Eaves, Baxter and Shafto, Patrick and Pereyda, Christopher and Thomas, Brian and Cook, Diane J.},
  doi          = {10.1007/s10618-025-01158-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-32},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Detecting and reacting to smart home novelties},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optirefine: Densest subgraphs and maximum cuts with k refinements. <em>DMKD</em>, <em>39</em>(6), 1-68. (<a href='https://doi.org/10.1007/s10618-025-01142-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-analysis tasks often involve an iterative process, which requires refining previous solutions. For instance, when analyzing social networks, we may obtain initial communities based on noisy metadata, and we want to improve them by adding influential nodes and removing non-important ones, without making too many changes. However, classic optimization algorithms, which typically find solutions from scratch, potentially return communities that are very dissimilar to the initial one. To mitigate these issues, we introduce the OptiRefine framework. The framework optimizes initial solutions by making a small number of refinements, thereby ensuring that the new solution remains close to the initial solution and simultaneously achieving a near-optimal solution for the optimization problem. We apply the OptiRefine framework to two classic graph-optimization problems: densest subgraph and maximum cut. For the densest-subgraph problem, we optimize a given subgraph’s density by adding or removing k nodes. We show that this novel problem is a generalization of k-densest subgraph, and provide constant-factor approximation algorithms for $$k=\Omega (n)$$ refinements. We also study a version of maximum cut in which the goal is to improve a given cut. We provide connections to the maximum cut with cardinality constraints and provide an optimal approximation algorithm in most parameter regimes under the Unique Games Conjecture for $$k=\Omega (n)$$ refinements. We evaluate our theoretical methods and scalable heuristics on synthetic and real-world data and show that they are highly effective in practice.},
  archive      = {J_DMKD},
  author       = {Tu, Sijing and Stankovic, Aleksa and Neumann, Stefan and Gionis, Aristides},
  doi          = {10.1007/s10618-025-01142-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-68},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Optirefine: Densest subgraphs and maximum cuts with k refinements},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPARAFAC2: Tracking evolving patterns in (incomplete) temporal data. <em>DMKD</em>, <em>39</em>(6), 1-33. (<a href='https://doi.org/10.1007/s10618-025-01122-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor factorizations have been widely used for the task of uncovering patterns in various domains. Often, the input is time-evolving, shifting the goal to tracking the evolution of the underlying patterns instead. To adapt to this more complex setting, existing methods incorporate temporal regularization but they either have overly constrained structural requirements or lack uniqueness which is crucial for interpretation. In this paper, in order to capture the underlying evolving patterns, we introduce t(emporal)PARAFAC2, which utilizes temporal smoothness regularization on the evolving factors. Previously, Alternating Optimization and Alternating Direction Method of Multipliers-based algorithmic approach has been introduced to fit the PARAFAC2 model to fully observed data. In this paper, we extend this algorithmic framework to the case of partially observed data and use it to fit the tPARAFAC2 model to complete and incomplete datasets with the goal of revealing evolving patterns. Our numerical experiments on simulated datasets demonstrate that tPARAFAC2 can extract the underlying evolving patterns more accurately compared to the state-of-the-art in the presence of high amounts of noise and missing data. Using two real datasets, we also demonstrate the effectiveness of the algorithmic approach in terms of handling missing data and tPARAFAC2 model in terms of revealing evolving patterns. The paper provides an extensive comparison of different approaches for handling missing data within the proposed framework, and discusses both the advantages and limitations of tPARAFAC2 model.},
  archive      = {J_DMKD},
  author       = {Chatzis, Christos and Schenker, Carla and Pfeffer, Max and Acar, Evrim},
  doi          = {10.1007/s10618-025-01122-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-33},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TPARAFAC2: Tracking evolving patterns in (incomplete) temporal data},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fed-FUEL: Fairness and utility enhancing agnostic federated learning framework. <em>DMKD</em>, <em>39</em>(6), 1-30. (<a href='https://doi.org/10.1007/s10618-025-01152-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging communication-efficient and collaborative learning paradigm of machine learning with privacy guarantees. As these advancements unfold, adapting FL for fairness-aware learning becomes crucial. In this context, we propose a pre-processing fairness and utility (balanced accuracy) enhancing agnostic federated learning framework (Fed-FUEL) that mitigates discrimination embedded in the non-independent identically distributed data. We contribute a novel adaptive data manipulation method that mitigates discrimination embedded in the data at client side during optimization, resulting in an optimized and fair centralized server. This pre-processing approach abstracts the model architecture from the equation, offering a significant advantage in a federated environment. This abstraction not only facilitates a broader application across diverse model architectures without necessitating modifications but also sidesteps the potential complexities and inefficiencies associated with model-specific in-processing methods. Extensive experiments with a range of publicly available datasets demonstrate that our method outperforms the competing baselines in terms of both discrimination mitigation and predictive performance. Our model effectively adapts to both statistical and causal fairness notions, as shown through our experiments.},
  archive      = {J_DMKD},
  author       = {Badar, Maryam and Younis, Raneen and Sikdar, Sandipan and Nejdl, Wolfgang and Fisichella, Marco},
  doi          = {10.1007/s10618-025-01152-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fed-FUEL: Fairness and utility enhancing agnostic federated learning framework},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RMIDDM: An unsupervised and interpretable concept drift detection method for data streams. <em>DMKD</em>, <em>39</em>(6), 1-28. (<a href='https://doi.org/10.1007/s10618-025-01155-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional machine learning techniques assume that data is drawn from a stationary source. This assumption is challenged in contexts with data streams for presenting constant and potentially infinite sequences whose distribution is prone to change over time. Based on these settings, detecting changes (a.k.a. concept drifts) is necessary to keep learning models up-to-date. Although state-of-the-art detection methods were designed to monitor the loss of predictive models, such monitoring falls short in many real-world scenarios where the true labels are not readily available. Therefore, there is increasing attention to unsupervised concept drift detection methods as approached in this paper. In this work, we present an unsupervised and interpretable method based on Radial Basis Function Networks (RBFN) and Markov Chains (MC), referred to as RMIDDM (Radial Markov Interpretable Drift Detection Method). In our method, RBF performs, in the intermediate layer, an activation process that implicitly produces groups of observations collected over time. Simultaneously, MC models the transitions between groups to support the detection of concept drifts, which happens when the active group changes and its probability exceeds a given threshold. A set of experiments with synthetic datasets and comparisons with state-of-the-art algorithms demonstrated that the proposed method can detect drifts at runtime in an efficient, interpretable, and independent way of labels, presenting competitive results and behavior. Additionally, to show its applicability in a real-world scenario, we analyzed new COVID-19 cases, deaths, and vaccinations to identify new waves as concept drifts and generate Markov models that allow understanding of their interaction.},
  archive      = {J_DMKD},
  author       = {Neto, Ruivaldo and Alencar, Brenno and Gomes, Heitor Murilo and Bifet, Albert and Gama, João and Cassales, Guilherme and Rios, Ricardo},
  doi          = {10.1007/s10618-025-01155-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {RMIDDM: An unsupervised and interpretable concept drift detection method for data streams},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Steering the LoCoMotif: Using domain knowledge in time series motif discovery. <em>DMKD</em>, <em>39</em>(6), 1-31. (<a href='https://doi.org/10.1007/s10618-025-01143-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Motif Discovery (TSMD) identifies repeating patterns in time series data, but its unsupervised nature might result in motifs that are not interesting to the user. To address this, we propose a framework that allows the user to impose constraints on the motifs to be discovered, where constraints can easily be defined according to the properties of the desired motifs in the application domain. We also propose an efficient implementation of the framework, the LoCoMotif-DoK algorithm. We demonstrate that LoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic data, outperforming other TSMD techniques which only support a limited form of domain knowledge.},
  archive      = {J_DMKD},
  author       = {Yurtman, Aras and Van Wesenbeeck, Daan and Meert, Wannes and Blockeel, Hendrik},
  doi          = {10.1007/s10618-025-01143-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Steering the LoCoMotif: Using domain knowledge in time series motif discovery},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-relational knowledge graph contrastive learning for link prediction. <em>DMKD</em>, <em>39</em>(6), 1-21. (<a href='https://doi.org/10.1007/s10618-025-01161-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs are multi-relational data that contain massive entities and relations. Recently, graph neural networks have been reported outstanding performance in modeling knowledge graphs. However, most existing methods based on graph neural networks are limited by expensive labeled information and high-time-space complexity for large-scale Multi-relational Knowledge Graphs (MKGs). In this paper, we propose the Multi-relational Knowledge Graph Contrastive Learning (MKGCL) method, an end-to-end framework with contrastive learning to solve label problems, which considers the local structure of subgraphs and alleviates algorithm complexity. Firstly, MKGCL extracts relational subgraphs according to the relation types of MKGs. The node representations are learned with a graph neural network encoder, and the representations of different relational subgraphs are obtained by pooling related node representations. Secondly, contrastive learning is used to take fully advantage of multi-relational data and heterogeneous structures for MKGs. MKGCL contrasts node-level and subgraph-level embeddings to capture more structural information in MKGs. Moreover, the number of relational subgraphs for model training has a vital impact on time and space complexity for MKGs. By learning small-size samples, the MKGCL method achieves well results while reducing algorithm complexity. Finally, extensive experiments on four benchmark datasets demonstrate that MKGCL yields better link prediction performance than existing methods.},
  archive      = {J_DMKD},
  author       = {Zhao, Wenqian and Yang, Kai and Liu, Yuan and Ding, Peijin and Zhao, Zijuan},
  doi          = {10.1007/s10618-025-01161-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Multi-relational knowledge graph contrastive learning for link prediction},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Indexing demand response potential at multiple temporal granularity using network theory based analysis. <em>DMKD</em>, <em>39</em>(6), 1-28. (<a href='https://doi.org/10.1007/s10618-025-01164-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of renewable energy sources can significantly reduce carbon footprints; however, it also introduces challenges in maintaining uninterrupted energy services, thereby demanding advancements in technology. The increasing availability of energy consumption data from buildings has opened up the possibility of implementing data-driven methods to improve Demand Side Management (DSM) operations in energy utilities. In this context, the current paper proposes a novel indexing approach for buildings and appliances using a multiple network. The multiple network is constructed from a time series dataset, where each network represents a different time granularity. A quasi-clique-based clustering technique is applied to the multiple network to group similar subsequences based on recurring patterns. From the resulting clusters, an indexing measure is derived to quantify the stability of buildings and appliances. This paper presents a detailed analysis of changes in the stability index across various granular levels. A comparative study with existing state-of-the-art clustering techniques establishes the superiority of the proposed method.},
  archive      = {J_DMKD},
  author       = {Mishra, Kakuli and Basu, Srinka and Maulik, Ujjwal},
  doi          = {10.1007/s10618-025-01164-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Indexing demand response potential at multiple temporal granularity using network theory based analysis},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging internal representations of GNNs with shapley values. <em>DMKD</em>, <em>39</em>(6), 1-41. (<a href='https://doi.org/10.1007/s10618-025-01159-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the challenge of identifying the most influential graph structures in the decisions of Graph Neural Networks (GNNs). To tackle this, we propose a novel approach for evaluating the importance of subgraphs in GNN decisions, with a particular emphasis on calculating Shapley values. Unlike existing methods that impose rigid, predefined constraints on subgraph shapes (e.g., egographs or individual nodes), our approach remains flexible, accommodating arbitrary subgraph structures. Our method begins by analyzing activation rules within the representation spaces generated by the GNN, followed by computing Shapley values for these rules to quantify their contributions to model decisions. Using these Shapley values, we produce both instance-level and model-level explanations, offering deeper insights into the reasoning processes of GNNs. Extensive empirical studies across diverse datasets and comparisons with state-of-the-art methods highlight the effectiveness of our approach in delivering interpretable and robust explanations.},
  archive      = {J_DMKD},
  author       = {Kamal, Ataollah and Ragno, Alessio and Plantevit, Marc and Robardet, Céline},
  doi          = {10.1007/s10618-025-01159-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-41},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Leveraging internal representations of GNNs with shapley values},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structured temporal representation in time series classification with ROCKETs and hyperdimensional computing. <em>DMKD</em>, <em>39</em>(6), 1-49. (<a href='https://doi.org/10.1007/s10618-025-01162-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification poses significant challenges due to the inherent temporal order of the data points and the existence of sequential dependencies between them. The ROCKET family, featuring methods like MiniROCKET, MultiROCKET, and HYDRA, is currently a leading approach in this domain, leveraging convolution kernels to aggregate temporal features into encodings for linear classifiers. However, these models encode temporal features over short temporal windows and then aggregate them as an unordered set of encodings over the longer temporal window of the entire data sequence. This prevents these models from capturing any longer sequence structure. To address this design drawback, we propose integrating hyperdimensional computing into ROCKET methods to explicitly incorporate temporal order of the short-term features within the entire time series. This approach enhances the discriminative power of encodings generated by MiniROCKET, MultiROCKET, and HYDRA where longer-term structure exists in the data, leading to increased classification performance with minimal computational overhead. More specifically, we introduce a method to represent time series as high-dimensional vectors through multiplicative binding of ROCKET encodings with encodings representing temporal order, applying this approach across various ROCKET methods. Additionally, we explore different high-dimensional vector representations of temporal order, yielding diverse similarity kernels that enhance classification accuracy. Through experiments on synthetic datasets, we highlight the limitations of ROCKET methods in handling temporal dependencies and show how the methods based on hyperdimensional computing overcome these limitations. Furthermore, our extensive experimental evaluation with real-world datasets included in the recent UCR archive, validates the advantages of our approach, consistently achieving classification improvements across all ROCKET methods that integrate hyperdimensional computing. Notably, our best model achieves a relative error rate reduction of over 50% compared to the best ROCKET model on several UCR datasets.},
  archive      = {J_DMKD},
  author       = {Schlegel, Kenny and Rachkovskij, Dmitri A. and Kleyko, Denis and Gayler, Ross W. and Protzel, Peter and Neubert, Peer},
  doi          = {10.1007/s10618-025-01162-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1-49},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Structured temporal representation in time series classification with ROCKETs and hyperdimensional computing},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Deep anomaly detection with partition contrastive learning for tabular data. <em>DMKD</em>, <em>39</em>(5), 1-2. (<a href='https://doi.org/10.1007/s10618-025-01114-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Li, Yizhou and Wang, Yijie and Xu, Hongzuo and Li, Bin and Zhou, Xiaohui},
  doi          = {10.1007/s10618-025-01114-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-2},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction: Deep anomaly detection with partition contrastive learning for tabular data},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards automated self-supervised learning for truly unsupervised graph anomaly detection. <em>DMKD</em>, <em>39</em>(5), 1-43. (<a href='https://doi.org/10.1007/s10618-025-01115-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) is an emerging paradigm that exploits supervisory signals generated from the data itself, and many recent studies have leveraged SSL to conduct graph anomaly detection. However, we empirically found that three important factors can substantially impact detection performance across datasets: (1) the specific SSL strategy employed; (2) the tuning of the strategy’s hyperparameters; and (3) the allocation of combination weights when using multiple strategies. Most SSL-based graph anomaly detection methods circumvent these issues by arbitrarily or selectively (i.e., guided by label information) choosing SSL strategies, hyperparameter settings, and combination weights. While an arbitrary choice may lead to subpar performance, using label information in an unsupervised setting is label information leakage and leads to severe overestimation of a method’s performance. Leakage has been criticized as “one of the top ten data mining mistakes", yet many recent studies on SSL-based graph anomaly detection have been using label information to select hyperparameters. To mitigate this issue, we propose to use an internal evaluation strategy (with theoretical analysis) to select hyperparameters in SSL for unsupervised anomaly detection. We perform extensive experiments using 10 recent SSL-based graph anomaly detection algorithms on various benchmark datasets, demonstrating both the prior issues with hyperparameter selection and the effectiveness of our proposed strategy.},
  archive      = {J_DMKD},
  author       = {Li, Zhong and Wang, Yuhang and van Leeuwen, Matthijs},
  doi          = {10.1007/s10618-025-01115-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-43},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Towards automated self-supervised learning for truly unsupervised graph anomaly detection},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning a consensus sub-network with polarization regularization and one pass training. <em>DMKD</em>, <em>39</em>(5), 1-20. (<a href='https://doi.org/10.1007/s10618-025-01118-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a polarizing loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and training simultaneously, which saves energy in both the training and inference phases and avoids extra computational overhead from gating modules at inference time. Our results on CIFAR-10, CIFAR-100, and Tiny Imagenet suggest that our scheme can remove $${\approx 50}\%$$ of connections in deep networks with $$ \le 1\%$$ reduction in classification accuracy. Compared to other related pruning methods, our method demonstrates a lower drop in accuracy for equivalent reductions in computational cost.},
  archive      = {J_DMKD},
  author       = {Zhi, Xiaoying and Babbar, Varun and Liu, Rundong and Sun, Pheobe and Silavong, Fran and Shi, Ruibo and Moran, Sean},
  doi          = {10.1007/s10618-025-01118-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Learning a consensus sub-network with polarization regularization and one pass training},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SimHawNet: A modified hawkes process for temporal network simulation. <em>DMKD</em>, <em>39</em>(5), 1-28. (<a href='https://doi.org/10.1007/s10618-025-01119-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal networks allow representing connections between objects while incorporating the temporal dimension. While static network models can capture unchanging topological regularities, they often fail to model the effects associated with the causal generative process of the network that occurs in time. Hence, exploiting the temporal aspect of networks has been the focus of many recent studies. In this context, we propose a new framework for generative models of continuous-time temporal networks. We assume that the activation of the edges in a temporal network is driven by a specified temporal point process. This approach allows to directly model the waiting time between events while incorporating time-varying history-based features as covariates in the predictions. Coupled with a thinning algorithm designed for the simulation of point processes, SimHawNet enables simulation of the evolution of temporal networks in continuous time. Finally, we introduce a comprehensive evaluation framework to assess the performance of such an approach, in which we demonstrate that SimHawNet successfully simulates the evolution of networks with very different generative processes and achieves performance comparable to the state of the art, while being significantly faster.},
  archive      = {J_DMKD},
  author       = {Perez, Mathilde and Romero, Raphaël and Kang, Bo and De Bie, Tijl and Lijffijt, Jefrey and Laclau, Charlotte},
  doi          = {10.1007/s10618-025-01119-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {SimHawNet: A modified hawkes process for temporal network simulation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFC: A time series decomposition attention network with continuous nature for time series analysis. <em>DMKD</em>, <em>39</em>(5), 1-25. (<a href='https://doi.org/10.1007/s10618-025-01121-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series analysis has attracted considerable research interest because of its wide range of applications, but it remains a challenging task. In recent years, deep learning methods such as CNN and RNN have made significant advances in time series classification and forecasting. However, the continuous nature of time series data has often been overlooked. It is crucial to recognize that the time series evolves continuously in real time; therefore, models must effectively capture this seamless continuity. In this context, we propose an encoder-decoder architecture attention network called SFC, which integrates the idea of time series decomposition as a deep learning approach to tackle this problem. The goal is to capture the ongoing evolution of the dynamical system in time series data. SFC features two types of modules, each associated with a specific time series component in time series decomposition. The component is modeled through a differential equation with a corresponding special form. The attention mechanisms within SFC are employed to capture the relationships between different data points within a time series, effectively treat them as periodic components, and enable the process of time series decomposition. The information within each differential equation would be decoded, and the SFC with the encoder-decoder-attention structure is then formulated as a task-dependent neural network for time series analysis tasks. Extensive experiments on a variety of real-world datasets with robust competitive baselines demonstrate the outstanding performance of our model.},
  archive      = {J_DMKD},
  author       = {Chen, Haoyu and Li, Bo and An, Zhiyong and Yu, Yuan and Jia, Ying},
  doi          = {10.1007/s10618-025-01121-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {SFC: A time series decomposition attention network with continuous nature for time series analysis},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Greatmeta: Gradient-aware adaptive meta-learning for cold-start recommendations. <em>DMKD</em>, <em>39</em>(5), 1-29. (<a href='https://doi.org/10.1007/s10618-025-01123-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, optimizer-based meta-learning, i.e., model-agnostic meta-learning (MAML), has emerged as a powerful tool to tackle the cold-start problem in recommender systems. However, existing methods overlook the key fact that user preferences are inherently imbalanced in real-world applications. Learning indiscriminately from imbalanced user preferences may lead to an imbalanced meta-model that introduces systematic bias in subsequent recommendations. In this paper, we explore the impact of imbalanced user preferences on the meta-training process and consequently propose a novel Gradient-aware adaptive Meta-learning (GreatMeta) model for cold-start recommendations. Inspired by prior work, our key idea is to leverage gradient signals to understand a meta-model’s status and guide the learning process. More specifically, we put forward three original gradient-aware factors to measure the state of the meta-model and the level of user imbalance so that we can make use of the correlation between the meta-model and the underlying users to generate a balanced meta-model. Based on these factors, we design a preference-aware scheduler to adaptively adjust each user’s contribution in the meta-training process, which helps the resultant meta-model better generalize to minority cold-start users. We also introduce a personalized encoder that effectively utilizes limited data and accelerates the adaptation process of the balanced meta-model. Moreover, we theoretically justify the rationality of the three proposed gradient-aware factors. Extensive experimental results on public benchmark datasets demonstrate the superiority of GreatMeta over a large number of state-of-the-art recommendation methods, confirming the value of addressing the imbalance of user preferences in MAML-based meta-learning for cold-start recommendations. The code is available at https://github.com/YantongDU/GreatMeta .},
  archive      = {J_DMKD},
  author       = {Du, Yantong and Chen, Rui and Han, Qilong and Tan, Qiaoyu and Song, Hongtao and Zhang, Chi},
  doi          = {10.1007/s10618-025-01123-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-29},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Greatmeta: Gradient-aware adaptive meta-learning for cold-start recommendations},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARM-stream: Active recovery of miscategorizations in clustering-based data stream classifiers. <em>DMKD</em>, <em>39</em>(5), 1-35. (<a href='https://doi.org/10.1007/s10618-025-01124-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-stationary feature-class interaction problem arises in dynamic data stream environments, where online classifiers avoid frequent label requests by relying on feature-class interaction assumptions to obtain class information from unlabeled data, even though these very assumptions may become invalid as this interaction changes unexpectedly. Clustering-based data stream classifiers mitigate this by leveraging active learning strategies. However, because these strategies are integral to the update process, enhancing reliability within the classifier is typically limited to increasing labeling resources, diminishing the classifier’s ability to learn from unlabeled data. In this paper we tackle this problem by decoupling the error-handling concerns from the classifier into a new framework called ARM-Stream, which works as a fail-safe layer that aims to intervene only when the classifier’s update process would fail, otherwise preserving the classifier’s original label resource usage patterns. ARM-Stream’s architecture, defined through abstract modules, ensures easy integration across different classifiers and easy customization and extension of its modules. To the best of our knowledge, ARM-Stream is the first error recovery framework for clustering-based data stream classifiers. As a study case, we test the framework over three classifiers: MINAS, CDSC-AL, and ECHO, chosen for their differing update process. An off-the-shelf source code library for the framework is provided.},
  archive      = {J_DMKD},
  author       = {Cavalcanti, Douglas Monteiro and Cerri, Ricardo and Faria, Elaine Ribeiro},
  doi          = {10.1007/s10618-025-01124-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-35},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ARM-stream: Active recovery of miscategorizations in clustering-based data stream classifiers},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive interactive network component ensemble for streaming data with concept drift. <em>DMKD</em>, <em>39</em>(5), 1-34. (<a href='https://doi.org/10.1007/s10618-025-01125-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is an important issue in streaming data mining. At present, most concept drift processing methods face some challenges such as slow model convergence after drift occurring and weak representation ability for data stream with complex distributions. In this paper, an adaptive interactive network component ensemble method is proposed to solve the problem of streaming data with concept drift. First, the convolution features with different scales are concatenated, and the channel relationship is modelled to construct concatenated features by applying the attention mechanism. Then the shallow input features and concatenated features are fused adaptively by the gating mechanism. The multi-scale feature extraction module, channel attention mechanism and gating mechanism are used to construct a multi-scale feature fusion network component, the features of diversity and discriminability can be obtained from this component. Second, the mutual learning is used to learn from representative components with different properties, which enhances representation ability of shallow components and convergence ability of deep components. The experimental results on simulation data streams and visual object tracking data streams show that the proposed method can effectively deal with concept drift of streaming data and quickly converge to the new distribution when drift occurs. It outperforms traditional adaptive methods in dealing with concept drift and can improve the representation ability of data with complex distribution and real-time generalization performance of the model.},
  archive      = {J_DMKD},
  author       = {Guo, Husheng and Wu, Zhijie and Liu, ZhengQi and Zhang, Shuai and Wang, Wenjian},
  doi          = {10.1007/s10618-025-01125-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-34},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Adaptive interactive network component ensemble for streaming data with concept drift},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OLIVANDER: A counterfactual-based method to generate adversarial windows PE malware. <em>DMKD</em>, <em>39</em>(5), 1-30. (<a href='https://doi.org/10.1007/s10618-025-01127-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is transforming cybersecurity practices thanks to the amazing accuracy performance achieved with several AI-based malware detection systems. However, several recent studies have shown that AI decision models can be vulnerable to adversarial attacks. In malware detection scenarios, adversarial attacks are realistic manipulations of existing malware, which preserve the executable and malicious behaviour but evade the malware detection measures. In this study, we consider Windows Portable Executable (PE) malware, which is currently trending to prominent malware types, and we show that counterfactual explanations can be used to drive the generation of realistic adversarial Windows PE malware to evade AI-based detection. In particular, the proposed method OLIVANDER works in a black-box manner, which is the most restrictive attack option, as the evasion method interacts with the target decision system to evade by merely knowing the model input and output. The evaluation study explores the effectiveness of the proposed evasion method in terms of evasion ability, efficiency of computation, and attack transferability compared to two state-of-the-art evasion methods. In addition, the performed evaluation accounts for performances on commercial anti-malware systems.},
  archive      = {J_DMKD},
  author       = {De Rose, Luca and Andresini, Giuseppina and Appice, Annalisa and Malerba, Donato},
  doi          = {10.1007/s10618-025-01127-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-30},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {OLIVANDER: A counterfactual-based method to generate adversarial windows PE malware},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the number of iterations of the DBA algorithm. <em>DMKD</em>, <em>39</em>(5), 1-35. (<a href='https://doi.org/10.1007/s10618-025-01116-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The DTW Barycenter Averaging (DBA) algorithm is a widely used algorithm for estimating the mean of a given set of point sequences. In this context, the mean is defined as a point sequence that minimises the sum of dynamic time warping distances (DTW). The algorithm is similar to the k-means algorithm in the sense that it alternately repeats two steps: (1) computing an optimal assignment to the points of the current mean, and (2) computing an optimal mean under the current assignment. The popularity of DBA can be attributed to the fact that it works well in practice, despite any theoretical guarantees to be known. In our paper, we aim to initiate a theoretical study of the number of iterations that DBA performs until convergence. We assume the algorithm is given n sequences of m points in $${\mathbb R}^d$$ and a parameter k that specifies the length of the mean sequence to be computed. We show that, in contrast to its fast running time in practice, the number of iterations can be exponential in k in the worst case — even if the number of input sequences is $$n=2$$ . We complement these findings with experiments on real-world data that suggest this worst-case behaviour is likely degenerate. To better understand the performance of the algorithm on non-degenerate input, we study DBA in the model of smoothed analysis, upper-bounding the expected number of iterations in the worst case under random perturbations of the input. Our smoothed upper bound is $$ \widetilde{O} \left( n^2 m^{8\frac{n}{d}+6}d^4k^6\sigma ^{-2} \right) $$ , where $$\sigma $$ is the variance of the perturbation and the $$\widetilde{O}(\cdot )$$ -notation omits logarithmic factors. For our analysis, we adapt the set of techniques that were developed for analysing the k-means method and observe that this set of techniques is not sufficient to obtain tight bounds for general n.},
  archive      = {J_DMKD},
  author       = {Brüning, Frederik and Driemel, Anne and Ergür, Alperen and Röglin, Heiko},
  doi          = {10.1007/s10618-025-01116-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-35},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {On the number of iterations of the DBA algorithm},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unpacking the trend: Decomposition as a catalyst to enhance time series forecasting models. <em>DMKD</em>, <em>39</em>(5), 1-41. (<a href='https://doi.org/10.1007/s10618-025-01120-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the time series forecasting task, several state-of-the-art algorithms employ moving-average decomposition for improved accuracy. However, the potential of decomposition techniques to enhance time series forecasting methods has not been explored in detail. In this work, we comprehensively investigate the use of decomposition methods for the forecasting task, comparing different decomposition techniques and their effect on forecasting accuracy, as well as the possibility of providing model-agnostic interpretability. We rework recent forecasting models to be compatible with any decomposition technique and experimentally evaluate their effectiveness in different forecasting setups. We further propose and assess a model-agnostic framework using decomposition for interpretability. Our results show that decomposition can improve forecasting accuracy, especially for the proposed decomposition-adapted models. Additionally, we demonstrate that the architectural choices of existing forecasting models can be improved by using different decomposition blocks internally. We found that decomposition techniques must be configured with a low number of components to provide model-agnostic interpretability. Our work concludes that decomposition can enhance time series forecasting algorithms, improving both their performance and interpretability.},
  archive      = {J_DMKD},
  author       = {Kreuzer, Tim and Zdravkovic, Jelena and Papapetrou, Panagiotis},
  doi          = {10.1007/s10618-025-01120-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-41},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Unpacking the trend: Decomposition as a catalyst to enhance time series forecasting models},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combating confirmation bias: A unified pseudo-labeling framework for entity alignment. <em>DMKD</em>, <em>39</em>(5), 1-32. (<a href='https://doi.org/10.1007/s10618-025-01128-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment (EA) aims at identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to the same real-world identity. It has been a compelling but challenging task that requires the integration of heterogeneous information from different KGs to expand the knowledge coverage and enhance inference abilities. To circumvent the shortage of prior seed alignments provided for training, recent EA models utilize pseudo-labeling strategies to iteratively add unaligned entity pairs predicted with high confidence to the seed alignments for model training. However, the adverse impact of confirmation bias during pseudo-labeling has been largely overlooked, thus hindering entity alignment performance. To systematically combat confirmation bias, we propose a new Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA) that explicitly alleviates pseudo-labeling errors to boost the performance of entity alignment. UPL-EA achieves this goal through two key innovations: (1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as an effective means to determine entity correspondences and reduce erroneous matches across two KGs. An effective criterion is derived to infer pseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel pseudo-label ensembling refines pseudo-labeled alignments by combining predictions over multiple models independently trained in parallel. The ensembled pseudo-labeled alignments are thereafter used to augment seed alignments to reinforce subsequent model training for alignment inference. The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally validated. Our extensive results and in-depth analyses demonstrate the superiority of UPL-EA over 15 competitive baselines and its utility as a general pseudo-labeling framework for entity alignment.},
  archive      = {J_DMKD},
  author       = {Ding, Qijie and Yin, Jie and Zhang, Daokun and Gao, Junbin},
  doi          = {10.1007/s10618-025-01128-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-32},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Combating confirmation bias: A unified pseudo-labeling framework for entity alignment},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automating mixture model fitting of task durations for process conformance checking. <em>DMKD</em>, <em>39</em>(5), 1-35. (<a href='https://doi.org/10.1007/s10618-025-01131-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process task duration data often exhibit multiple peaks, indicating differences in, for example, customer ages and preferences, resource capabilities or the day/hour of a week. This heterogeneous data, which captures diverse customer patterns, should be represented using different models, resulting in an overall mixture model. This paper introduces gamma mixture models to represent various customer patterns in task duration data, with a focus on automating the fitting process. The approach involves a two-stage procedure: first, divide-and-conquer using peak-, equidistance- and cluster-based techniques to partition data, and automatically fit gamma distributions to each subset. The second stage then improves the fitted mixture model by directly searching the log-likelihood surface. The method is compared with the expectation–maximization (EM) algorithm and an open tool (HyperStar), using both artificially generated datasets and a publicly available hospital billing dataset, demonstrating its effectiveness and time efficiency in modelling heterogeneous process duration data. Furthermore, a case study on process conformance checking is conducted using the hospital billing dataset, highlighting a potential application area for the method in process mining.},
  archive      = {J_DMKD},
  author       = {Yang, Lingkai and McClean, Sally and Faddy, Malcolm and Donnelly, Mark and Khan, Kashaf and Burke, Kevin},
  doi          = {10.1007/s10618-025-01131-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-35},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Automating mixture model fitting of task durations for process conformance checking},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handling new users and items: A comparative study of inductive recommenders. <em>DMKD</em>, <em>39</em>(5), 1-33. (<a href='https://doi.org/10.1007/s10618-025-01134-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Usually, recommender systems are trained on a set of users and items and then used to recommend new user-item pairings among those seen during training. As users and items are added continuously, there is a pressing need to provide recommendations for new users and items, i.e., for users and items not seen during training. Solutions to this problem exploit techniques like meta-learning or auxiliary information encoded in knowledge graphs to learn an “inductive bias”. Yet, most existing works can either recommend for new users or new items not seen during training but not both. Further, existing methods have rarely been compared to each other. Finally, existing evaluations of these methods use a random split of training data, and thus do not consider temporal splits of ratings in training and testing. This setting ensures testing is correctly performed on user interactions that actually occur after the training period. In this paper, we propose a framework for training and testing the methods on three real world datasets, and perform a deeper analysis of each dataset to better understand the effect of emerging popularity trends. As a result, our re-evaluation of state-of-the-art methods identifies strong architectures and solutions for inductive recommendation. We find that inductive methods that perform aggregation are able to outperform non-aggregating methods in all settings; performances vary greatly across settings, pointing to new important research questions.},
  archive      = {J_DMKD},
  author       = {Jendal, Theis E. and Lissandrini, Matteo and Dolog, Peter and Hose, Katja},
  doi          = {10.1007/s10618-025-01134-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Handling new users and items: A comparative study of inductive recommenders},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A enhanced fuzzy clustering algorithm for probability density functions and image clustering using inception resnet-v2 features. <em>DMKD</em>, <em>39</em>(5), 1-34. (<a href='https://doi.org/10.1007/s10618-025-01136-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel fuzzy clustering algorithm for probability density functions (PDFs) utilizing an improved Kullback–Leibler divergence (FCP). The proposed algorithm not only determines the optimal number of clusters and assigns PDFs to these clusters but also defines the fuzzy membership degrees between the PDFs and the identified clusters based on the improved divergence measure. The convergence and stability of the FCP algorithm are theoretically guaranteed through two theorems: Theorem 1 proves the convergence of cluster centers as iterations proceed, while Theorem 2 establishes the optimal update formulas for cluster centers and membership matrix by minimizing the clustering objective function via derivative and Lagrangian methods. The effectiveness of the FCP algorithm is demonstrated through a numerical example, where the improved Kullback–Leibler divergence is compared with both $$L^1$$ and $$L^2$$ distances. Additionally, the algorithm is applied to cluster an image dataset based on features extracted using the Inception Resnet-v2 architecture. The experimental results show that the FCP algorithm outperforms existing models, achieving both accuracy and F1-Score of over 99%, along with a statistically significant improvement according to one-way ANOVA testing.},
  archive      = {J_DMKD},
  author       = {PhamToan, Dinh},
  doi          = {10.1007/s10618-025-01136-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-34},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A enhanced fuzzy clustering algorithm for probability density functions and image clustering using inception resnet-v2 features},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inductive influence estimation and maximization over unseen social networks under two diffusion models. <em>DMKD</em>, <em>39</em>(5), 1-38. (<a href='https://doi.org/10.1007/s10618-025-01137-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence estimation (IE) and influence maximization (IM) are among the most extensively studied problems in social network analysis. Assuming diffusion (i.e., the spread of diseases) within a social network, IE aims to estimate the influence (i.e., the number of infected nodes) for a given set of seeds; and IM aims to identify a given number of seed nodes that maximize the influence. For both IE and IM, widely-adopted strategies involve repeating Monte Carlo (MC) simulations of diffusion over and over for various seed sets, which is computationally expensive. In this work, we present Monte Carlo Simulator+ (MONSTOR+), an inductive machine learning method designed to estimate the influence of given seed-node sets in social networks under two diffusion models—the independent cascade (IC) model and the linear threshold (LT) model. Due to its inductive nature, MONSTOR+ is applicable to seed-node sets and social networks not included in the training data. MONSTOR+, with its ability to accurately estimate influence through a single forward pass, can greatly accelerate existing IM algorithms by replacing repeated MC simulations. In our experiments, MONSTOR+ exhibits high IE accuracy, achieving 0.955 or higher Pearson and Spearman correlation coefficients in unseen real-world social networks. Notably, MONSTOR+ is about 5 to 3000 times faster than repeated MC simulations with similar IE accuracy. For IM problems, IM algorithms equipped with MONSTOR+ are more accurate than state-of-the-art competitors in 81.5 and 77.8% of IM use cases under the IC model and LT model, respectively.},
  archive      = {J_DMKD},
  author       = {Ko, Jihoon and Kim, Sojeong and Lee, Kyuhan and Kang, Shinhwan and Hwang, Dongyeong and Shin, Kijung and Park, Noseong},
  doi          = {10.1007/s10618-025-01137-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-38},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Inductive influence estimation and maximization over unseen social networks under two diffusion models},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relation prediction based on the attention-enhanced fusion of graph strcuture and multi-hop neighborhood information in knowledge graphs. <em>DMKD</em>, <em>39</em>(5), 1-28. (<a href='https://doi.org/10.1007/s10618-025-01141-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation prediction is a critical approach to addressing the incompleteness of knowledge graphs. Research has shown that learning long-term semantic dependency information from multi-hop neighbourhoods is essential for fully capturing the contextual semantics of knowledge graphs. Based on the complex graph structure of knowledge graphs, multi-hop neighbourhood structures include relational paths between entities and the multi-level local neighbourhoods of given entities. However, existing studies often focus on single-dimensional learning, making it prone to overlook specific patterns when predicting missing relations. Prior research relies on static path aggregation or sequence encoding methods, particularly in learning the multi-hop structures of relational paths, neglecting the dynamic interactions between relational paths.In this paper, we propose a model named R-APSC, which aims to learn contextual representations of knowledge graphs from two types of multi-hop topological structures: (i) by constructing relationship path subgraphs to characterize sets of relation paths between entities and proposing an attention-enhanced recursive message-passing scheme to capture the graph structure within relation paths and their dynamic interactions; and (ii) by integrating the local multi-hop neighbourhood information of entities to capture their intrinsic properties and deep contextual semantics. Experimental results demonstrate that the R-APSC model significantly outperforms state-of-the-art relation prediction models on multiple benchmark datasets, validating the importance of understanding relation semantic propagation and dependencies from a global perspective.},
  archive      = {J_DMKD},
  author       = {Liu, Yisi and Tao, Sha},
  doi          = {10.1007/s10618-025-01141-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Relation prediction based on the attention-enhanced fusion of graph strcuture and multi-hop neighborhood information in knowledge graphs},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal attention-based real-time video watermarking. <em>DMKD</em>, <em>39</em>(5), 1-26. (<a href='https://doi.org/10.1007/s10618-025-01129-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As streaming media becomes prevalent, the demand for real-time video copyright protection has increased. Digital watermarking, a common copyright protection technique, has been widely used in copyright validation in various media. However, most of the existing video watermarking schemes follow the paradigm of image watermarking, focusing mainly on the impact of watermark embedding on visual perception and its robustness in channel transmission while neglecting the importance of efficiency. To efficiently protect the digital rights of streaming media, this article proposes an Efficient deep video Watermarking model based on Spatiotemporal Attention mechanism and patch sampling (EWSA). A spatiotemporal attention mechanism is employed to enhance watermark imperceptibility by embedding the watermark into texture and insensitive regions. Additionally, embedding efficiency is improved by sampling patches of video frames rather than embedding watermarking in entire frames. The performance of our model on three datasets through goal-oriented, three-stage training validates the effectiveness of the proposed EWSA, which achieves embedding speed approximately $$2\sim 3$$ times faster than other deep watermarking methods.},
  archive      = {J_DMKD},
  author       = {Yan, Quan and Luo, Yuanjing and Wang, Zhangdong and Xi, Junhua and Xia, Geming and Cai, Zhiping},
  doi          = {10.1007/s10618-025-01129-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Spatiotemporal attention-based real-time video watermarking},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trace alignment algorithm optimization. <em>DMKD</em>, <em>39</em>(5), 1-30. (<a href='https://doi.org/10.1007/s10618-025-01130-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process Mining is a computational discipline aimed at discovering, monitoring, and improving processes. In the business sector, it integrates artificial intelligence and data mining to uncover trends, patterns, and insights within classified data systems. The Trace Alignment algorithm is a key tool in this field, designed to detect anomalies and identify similarities in event sequences. However, most implementations rely on progressive alignment approaches, which are computationally expensive. This paper introduces a novel Trace Alignment implementation with significant theoretical enhancements. By incorporating advanced programming techniques and parallel computing, the algorithm achieves polynomial time complexity, a notable improvement over previous methods. Validation results confirm its efficiency and the alignment’s high quality.},
  archive      = {J_DMKD},
  author       = {González-Montesino, Leandro and Grass-Boada, Darian H.},
  doi          = {10.1007/s10618-025-01130-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-30},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Trace alignment algorithm optimization},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does user-end work? user-item-aware knowledge graph convolutional networks for recommendation. <em>DMKD</em>, <em>39</em>(5), 1-25. (<a href='https://doi.org/10.1007/s10618-025-01138-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep recommender systems with knowledge graphs (KG) as side information have attracted considerable interest recently. Although they have shown good performance in capturing graph structure information and improving explainability, knowledge-based models are unaware of external user knowledge and ignore the difference in importance between different relations. In this paper, we propose a User-Item Knowledge Graph Convolutional Network (UI-KGCN) model that comprehensively considers the importance of user features and relations, and utilizes different aggregators to integrate two-ends knowledge graph representation. The key contribution lies in three facets: (1) introducing user-end feature information and constructing the knowledge graph; (2) proposing a relation selection strategy to constructs a weighted graph by ranking the top k relations with the greatest impact; (3) designing three node aggregation strategies. Comparison experiments show that the proposed UI-KGCN model significantly outperforms state-of-the-art baselines in different recommendation scenarios. The self-neighbor aggregator of UI-KGCN performs best among all methods, especially on the Recall metric, achieving gains of 1.26%, 1.11% and 2.55% respectively compared with the best one among baselines on the Library, Book-Crossing and MovieLens dataset. Further ablation experiments verify the influence of different sub-modules and feature combinations for learning user and item representations, justifying the rationality and effectiveness of UI-KGCN.},
  archive      = {J_DMKD},
  author       = {Gu, Xiao and Jian, Ling},
  doi          = {10.1007/s10618-025-01138-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Does user-end work? user-item-aware knowledge graph convolutional networks for recommendation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PersAD: Adversarial protection against text-based personality inference. <em>DMKD</em>, <em>39</em>(5), 1-22. (<a href='https://doi.org/10.1007/s10618-025-01144-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As text-based personality inference approaches increasingly mature, personality privacy invasion has become a new threat applied to human-central malicious activities in recent years. When facing text-based personality inference attacks, the adversarial example method is a protection pathway that modifies the original texts to convert the personality characteristics. However, most studies focus on the local word or phrase perturbations instead of the global document semantics, making it challenging to ensure content semantic consistency while effectively reversing personality signals. Simultaneously, most methods are difficult to defend against various personality inference models without psychological knowledge. To address these challenges, we propose an adversarial protection method, PersAD for short. In our method, we extract the knowledge from personality psychology books to construct knowledge-enhanced prompts. Then, the forward-backward adversarial generation is implemented to convert original posts to protected posts with high semantic similarity and low personality relevancy via the LLM. Moreover, the protect success rate (PSR) and cumulative flipping rate (CFR) are proposed to validate the PersAD and compare the performance with other adversarial example methods. The extensive experiments demonstrate the protection performance of PersAD over various text-based personality inference models. Under the protection of PersAD, the average PSR improves in a range of 41.75% to 45.91%, exceeding 3 times the performance of traditional methods. For the CFR of two or more personality dimensions, PersAD achieves results between 84.26 and 87.50%, which is over 40% higher than other baseline methods. Our data & code are available at the Github repository ( https://github.com/Fr4n13Z/PersAD ).},
  archive      = {J_DMKD},
  author       = {Qiu, Houjie and Ma, Xingkong and Liu, Bo and Cai, Yiqing and Peng, Baoyun and Huang, Dan},
  doi          = {10.1007/s10618-025-01144-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {PersAD: Adversarial protection against text-based personality inference},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guiding the generation of counterfactual explanations through temporal background knowledge for predictive process monitoring. <em>DMKD</em>, <em>39</em>(5), 1-49. (<a href='https://doi.org/10.1007/s10618-025-01117-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual explanations suggest what should be different in the input instance to change the outcome of an AI system. When dealing with counterfactual explanations in the field of Predictive Process Monitoring, however, control flow relationships among events have to be carefully considered. A counterfactual, indeed, should not violate control flow relationships among activities (temporal background knowledge). Within the field of Explainability in Predictive Process Monitoring, there have been a series of works regarding counterfactual explanations for outcome-based predictions. However, none of them consider the inclusion of temporal background knowledge when generating these counterfactuals. In this work, we address the problem of generating counterfactual explanations that consider a temporal background knowledge in the scenario of outcome predictions in Predictive Process Monitoring. We do so by adapting state-of-the-art techniques for counterfactual generation in the domain of Explainable Artificial Intelligence that are based on genetic algorithms to consider a series of temporal constraints, expressed by means of Declare constraints, at runtime. We assume that this temporal background knowledge is given, and we adapt the fitness function, as well as the crossover and mutation operators, to maintain the satisfaction of the constraints. The proposed methods are evaluated with respect to state-of-the-art genetic algorithms for counterfactual generation and the results are presented. We showcase that the inclusion of temporal background knowledge allows the generation of counterfactuals more conformant to the temporal background knowledge, without however losing in terms of the counterfactual traditional quality metrics.},
  archive      = {J_DMKD},
  author       = {Buliga, Andrei and Di Francescomarino, Chiara and Ghidini, Chiara and Donadello, Ivan and Maggi, Fabrizio Maria},
  doi          = {10.1007/s10618-025-01117-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-49},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Guiding the generation of counterfactual explanations through temporal background knowledge for predictive process monitoring},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained multi-prompt essay scoring with multi-level disentanglement. <em>DMKD</em>, <em>39</em>(5), 1-29. (<a href='https://doi.org/10.1007/s10618-025-01126-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of language models in essay scoring has gained significant attention in recent years, typically on the basis of evaluating a single model across multiple prompts. However, in a multi-prompt setup, it is crucial to understand the varying aspects of different prompts. In such settings, there exist notable variations even in a trait with the same name across prompts. This semantic variation on the same traits underscores the need to treat them differently at a fine-grained level according to each prompt. In this study, we propose a multi-level disentanglement framework for multi-prompt essay scoring, designed to achieve fine-grained disentanglement of semantic differences across such traits. Our method not only improves the quality of the essay scoring, but also reduces memory usage and latency. Experimental results highlight that our framework surpasses seven state-of-the-art essay scoring methods and large language model(LLM)-based zero-shot and few-shot approaches, achieving the highest agreement with human essay ratings.},
  archive      = {J_DMKD},
  author       = {Han, Donghee and Roh, Daeyoung and Han, Euihwan and Song, Hwanjun and Yi, Mun Yong},
  doi          = {10.1007/s10618-025-01126-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-29},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fine-grained multi-prompt essay scoring with multi-level disentanglement},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stratify: Unifying multi-step forecasting strategies. <em>DMKD</em>, <em>39</em>(5), 1-39. (<a href='https://doi.org/10.1007/s10618-025-01135-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key aspect of temporal domains is the ability to make predictions multiple time-steps into the future, a process known as multi-step forecasting (MSF). At the core of this process is selecting a forecasting strategy; however, with no existing frameworks to map out the space of strategies, practitioners are left with ad-hoc methods for strategy selection. In this work, we propose Stratify, a parameterised framework that addresses multi-step forecasting, unifying existing strategies and introducing novel, improved strategies. We evaluate Stratify on 18 benchmark datasets, five function classes, and short to long forecast horizons (10, 20, 40, 80) in the univariate setting. In over 84% of 1080 experiments, novel strategies in Stratify improved performance compared to all existing ones. Importantly, we find that no single strategy consistently outperforms others in all task settings, highlighting the need for practitioners to explore the Stratify space to carefully search and select forecasting strategies based on task-specific requirements. Our results are the most comprehensive benchmarking of known and novel forecasting strategies. We share the code ( https://github.com/zs18656/stratify_unifying_MSF ) to reproduce our results.},
  archive      = {J_DMKD},
  author       = {Green, Riku and Stevens, Grant and Abdallah, Zahraa S. and Silva Filho, Telmo M.},
  doi          = {10.1007/s10618-025-01135-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-39},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Stratify: Unifying multi-step forecasting strategies},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models are few-shot multivariate time series classifiers. <em>DMKD</em>, <em>39</em>(5), 1-28. (<a href='https://doi.org/10.1007/s10618-025-01145-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are widely applied in time series analysis. Yet, their utility in few-shot classification—a scenario with limited training data—remains unexplored. We aim to leverage the pre-trained knowledge in LLMs to overcome the data scarcity problem within multivariate time series. To this end, we propose LLMFew, an LLM-enhanced framework, to investigate the feasibility and capacity of LLMs for few-shot multivariate time series classification (MTSC). We first introduce a Patch-wise Temporal Convolution Encoder (PTCEnc) to align time series data with the textual embedding input of LLMs. Then, we fine-tune the pre-trained LLM decoder with Low-rank Adaptations (LoRA) to enable effective representation learning from time series data. Experimental results show our model consistently outperforms state-of-the-art baselines by a large margin, achieving 125.2% and 50.2% improvement in classification accuracy on Handwriting and EthanolConcentration datasets, respectively. Our results also show LLM-based methods achieve comparable performance to traditional models across various datasets in few-shot MTSC, paving the way for applying LLMs in practical scenarios where labeled data are limited. Our code is available at https://github.com/junekchen/llm-fewshot-mtsc .},
  archive      = {J_DMKD},
  author       = {Chen, Yakun and Li, Zihao and Yang, Chao and Wang, Xianzhi and Xu, Guandong},
  doi          = {10.1007/s10618-025-01145-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Large language models are few-shot multivariate time series classifiers},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient boosted bagging for evolving data stream regression. <em>DMKD</em>, <em>39</em>(5), 1-37. (<a href='https://doi.org/10.1007/s10618-025-01147-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient boosting has been extensively studied in batch learning. Recently, its streaming adaptation, Streaming Gradient Boosted Trees (Sgbt), has surpassed existing state-of-the-art random subspace and random patches methods for streaming classification under various drift scenarios. However, its application in streaming regression remains unexplored. Vanilla Sgbt with squared loss exhibits high variance when applied to streaming regression problems. To address this, we utilize bagging streaming regressors in this work to create Streaming Gradient Boosted Regression (Sgbr). Bagging streaming regressors are employed in two ways: first, as base learners within the existing Sgbt framework, and second, as an ensemble method that aggregates multiple Sgbts. Our extensive experiments on 11 streaming regression datasets, encompassing multiple drift scenarios, demonstrate that the Sgb(Oza), a variant of the first Sgbr category, significantly outperforms current state-of-the-art streaming regression methods in terms of both predictive power and computational cost.},
  archive      = {J_DMKD},
  author       = {Gunasekara, Nuwan and Pfahringer, Bernhard and Gomes, Heitor Murilo and Bifet, Albert},
  doi          = {10.1007/s10618-025-01147-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-37},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Gradient boosted bagging for evolving data stream regression},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crossmamba: Multivariate time series forecasting model for cross-temporal and cross-dimensional dependencies with mamba. <em>DMKD</em>, <em>39</em>(5), 1-29. (<a href='https://doi.org/10.1007/s10618-025-01149-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep models for multivariate time series (MTS) forecasting highlight Transformer-based approaches for capturing long-term (cross-temporal) dependencies. However, most of these models impose uniform transformations across heterogeneous dimensions, failing to capture cross-dimensional dependencies that are crucial for MTS forecasting. Furthermore, the quadratic complexity $$O(L^2)$$ of the attention mechanism creates an unbearable computational bottleneck when processing long historical sequences. To address these challenges, we propose Crossmamba, a novel Mamba-empowered architecture featuring three key innovations: (1) The MamMLP module exploits Mamba’s selective state space paradigm to achieve linear time complexity for MTS processing; (2) The Full Dimensional State Extraction (FDSE) layer combines linear time series modeling with dimension-aware dependency learning; (3) The Spectral Noising (SN) technique injects frequency-band-specific perturbations to simulate real-world non-stationarity and periodicity. The main structure of the architecture is the Spectral Noising Encoder–Decoder (SNED). It starts with Time Cycle Embedding, which vectorizes the original MTS data into a 2D vector array with time and dimension information. Subsequently, the encoder of the SNED, which is built by multiple FDSE layers and SN layers, processes these to generate multi-granular representations enhanced by simulating non-stationary and periodic changes. The decoder of the SNED transforms these multi-granular representations and accumulates them to obtain the final prediction result. Extensive experimental results demonstrate the effectiveness of Crossmamba over previous techniques. Code is available at https://github.com/IHAN-1212/Crossmamba .},
  archive      = {J_DMKD},
  author       = {Lin, Yuhan and Xiong, Liping and Hong, Zhiyong and Zeng, Zhiqiang and Zeng, Jian and Zeng, Guoqiang},
  doi          = {10.1007/s10618-025-01149-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1-29},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Crossmamba: Multivariate time series forecasting model for cross-temporal and cross-dimensional dependencies with mamba},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clustering using GRASP and path relinking for the max k-cut problem. <em>DMKD</em>, <em>39</em>(4), 1-38. (<a href='https://doi.org/10.1007/s10618-025-01099-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The max k-cut problem is a graph partitioning problem with applications in various fields. It involves partitioning the vertices of a graph into k disjoint sets so as to maximize the sum of the weights of edges connecting vertices belonging to different sets. However, due to its $$\mathcal {NP}$$ -hard complexity, it is not possible to solve the problem quickly for large graphs. In this paper, we propose an algorithm using the greedy randomized adaptive search procedure and path relinking heuristics to solve the problem with an approximation ratio of $$\left( 1 - \frac{1}{k}\right)$$ . We then introduce optimizations with a novel contributions matrix, which considerably improves the time complexity of the algorithm. Experiments show that using this method to cluster data results in it outperforming the more commonly used k-means algorithm and Ward’s method of agglomerative clustering. Furthermore, the proposed algorithm uses far less time and memory than a semidefinite programming approximation algorithm while producing solutions of similar quality.},
  archive      = {J_DMKD},
  author       = {Sudarsan, Bharadwaj and Pugalendhi, Ganesh Kumar},
  doi          = {10.1007/s10618-025-01099-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Clustering using GRASP and path relinking for the max k-cut problem},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tractable probabilistic models and computational complexity. <em>DMKD</em>, <em>39</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10618-025-01101-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic models with tractable marginalization are those in which evidence queries involving marginalization of variables are guaranteed to be computable in polynomial time in the size of the model. The tractability of marginalization of current classes of tractable probabilistic models is achieved through structural properties imposed over the functions they decompose to and it is an open question whether there are more general classes of probabilistic models with tractable marginalization than the current ones. That question is settled in this paper by the usage of boolean circuits to express them. On one hand, bounds on the number of simple and local operations required by a circuit to solve a problem are used as measures of complexity and on the other the circuit value problem is guaranteed to have complexity bounded by a polynomial function on the number of its gates. It is shown that choices of the definitions of the variables to use have an impact on the circuit description length as they are paramount to definitions of simplicity and locality. The framework that is put forward builds on a construction of a circuit representation of a dataset and on the definition of problems that take that circuit as an input such that the answers of the problems correspond to values of interest. Based on that, a set of circuit value problems are defined so that given inputs that identify queries of interest, the corresponding output values are obtained as a result of evaluation of those circuits. This approach is more general than current ones, in that, for any algorithm that runs in polynomial time over a model there is a circuit with number of gates bounded by a polynomial function of the time that the algorithm takes to be executed.},
  archive      = {J_DMKD},
  author       = {Cruz, David and Batista, Jorge},
  doi          = {10.1007/s10618-025-01101-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Tractable probabilistic models and computational complexity},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep anomaly detection with partition contrastive learning for tabular data. <em>DMKD</em>, <em>39</em>(4), 1-38. (<a href='https://doi.org/10.1007/s10618-025-01102-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised anomaly detection (AD) methods define transformations and surrogate tasks to deeply learn data “normality”, presenting superior performance. Different from most existing work designed for images, this paper considers self-supervised AD for tabular data, which has two desiderata: (i) transformation operations need to generate diverse transformed samples for downstream contrastive learning; (ii) the learning of surrogate tasks is required to perceive the semantics of tabular data, i.e., local–global patterns including intra-instance regularities and inter-instance structures. Related studies devise applicable transformations, but their surrogate tasks often neglect the inter-instance structures, failing to describe comprehensive data “normality" accurately. To fill these gaps, this paper proposes a novel partition contrastive learning-based anomaly detection method. We first devise a new transformation. Transformed samples are created by representing sub-vectors generated from different partitions of the whole feature space, during which different feature couplings are embedded in the transformed samples to make them sufficiently diverse. To capture intra-instance regularities, our approach learns a representation space where the transformed samples repel each other while concurrently resembling their corresponding spatial centers. A constraint is posed on the frames of these spatial centers to preserve their similarities between the corresponding original instances, facilitating the learning of inter-instance structures. The synergy of these two learning objectives encourages the modeling of tabular data semantics, thereby comprehensively modeling data “normality”. Abnormal degrees of the testing data are obtained by evaluating whether they conform to these learned patterns shared in the majority of data. Extensive experiments show that our approach achieves significant improvements (13% AUC-ROC and 30% AUC-PR) over state-of-the-art AD methods.},
  archive      = {J_DMKD},
  author       = {Li, Yizhou and Wang, Yijie and Xu, Hongzuo and Li, Bin and Zhou, Xiaohui},
  doi          = {10.1007/s10618-025-01102-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Deep anomaly detection with partition contrastive learning for tabular data},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-scale time series forecasting framework with temporal hierarchical information fusion and reconciliation. <em>DMKD</em>, <em>39</em>(4), 1-26. (<a href='https://doi.org/10.1007/s10618-025-01103-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting plays a crucial role in numerous applications. Views at different scale may offer complementary information and contain temporal coherence of a time series, but are largely neglected by existing studies. In this paper, we aim to take advantage of the complementarity and consistency among multiple scales of a time series to enhance accuracy of forecasting models. To accomplish this, we propose a Multi-Scale forecasting Framework (MSF) that comprises two key components: the Temporal Hierarchical Information Fusion (THIF) module and the Reconciliation Network (ReconNet). The THIF module facilitates the exchange of complementary information among scales, enabling a more comprehensive representation of each scale. The ReconNet, in a top-down manner, dynamically revises predictions to ensure coherence and enhance accuracy. The framework is designed to be flexible, such that different forecasting models can be plugged in, and is capable of joint training of the components. Extensive experiments on real-world datasets indicate the effectiveness of our approach.},
  archive      = {J_DMKD},
  author       = {Shi, Keqin and Ding, Zhihua and Chen, Zhen and He, Hao and Sun, Weiqiang and Hu, Weisheng},
  doi          = {10.1007/s10618-025-01103-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A multi-scale time series forecasting framework with temporal hierarchical information fusion and reconciliation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSCN: An efficient snapshot ensemble learning based sparse transformer model for long-range spatial-temporal traffic flow prediction. <em>DMKD</em>, <em>39</em>(4), 1-38. (<a href='https://doi.org/10.1007/s10618-025-01104-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent Transportation Systems aim to alleviate traffic congestion and enhance urban traffic management. Transformer-based methods have shown promise in traffic prediction due to their capability to handle long-range dependencies. However, they disregard local context during parallel processing and can be computationally expensive for large traffic networks. On the other hand, they miss the hierarchical information hidden in regions of large traffic networks. To address these issues, we introduce CSCN, a novel framework that clusters traffic sensors based on data similarity, employs clustered multi-head self-attention for efficient hierarchical pattern learning, and utilizes causal convolutional attention for capturing local temporal trends. In addition to these advancements, we integrate snapshot ensemble learning into CSCN, allowing for the exploitation of diverse snapshots obtained during training to enrich predictive performance. Evaluations of real-world data highlight CSCN’s superiority in traffic flow prediction, showcasing its potential for enhancing transportation systems with improved accuracy and efficiency.},
  archive      = {J_DMKD},
  author       = {Kumar, Rahul and Moreira, João Mendes and Chandra, Joydeep},
  doi          = {10.1007/s10618-025-01104-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-38},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {CSCN: An efficient snapshot ensemble learning based sparse transformer model for long-range spatial-temporal traffic flow prediction},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep graph embedding learning based on multi-variational graph autoencoders for POI recommendation. <em>DMKD</em>, <em>39</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10618-025-01106-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, point-of-interest (POI) recommendation has become a popular research hotspot in heterogeneous location-based social network (LBSN). One major recurring challenge in POI recommendation is that most existing works fail to learn well graph embeddings for user preferences, lacking the capability of fusing multi-typed nodes and their interaction relations, e.g., users’ check-in relations to POIs, following relations to online topics, and the social relations. To address this challenge, we propose a new unified heterogeneous graph embedding framework by leveraging multimodal variational graph autoencoders, called MultiVGAE. Specifically, we first employ multiple GCN-based encoders to learn the modality-specific latent embeddings for different entities in heterogeneous subgraphs of LBSN, with consideration of fusing multi-types of relations and multi-modal node features. And then reconstruct the corresponding subgraph structures through multiple decoders from the learned embeddings. Finally, extensive experiments have been conducted on two real-world datasets (e.g., Foursquare-NYC and Yelp2018), and the experimental results demonstrate that our proposed MultiVGAE achieves superior performance compared to the existing state-of-the-art baselines on POI recommendation.},
  archive      = {J_DMKD},
  author       = {Gong, Weihua and Shen, Genhang and Zhao, Anlun and Yang, Lianghuai and Cheng, Zhen},
  doi          = {10.1007/s10618-025-01106-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Deep graph embedding learning based on multi-variational graph autoencoders for POI recommendation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge graph completion based on asymmetric translation and automatic entity type representation. <em>DMKD</em>, <em>39</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10618-025-01105-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs (KGs) are graphical knowledge bases widely used to represent and organize knowledge, playing a crucial role in various artificial intelligence applications. The quality of a knowledge graph significantly impacts its utility. Knowledge graph completion (KGC) is a critical approach to enhancing the quality of knowledge graphs, typically categorized into structure-based methods and methods leveraging descriptive information. However, structure-based methods often suffer from data sparsity issues due to their reliance solely on the triadic structure of the knowledge graph. On the other hand, methods utilizing descriptive information face challenges in effectively leveraging and modeling such information. To address these issues, this study proposes AutoTransW, a novel model based on asymmetric translation and automatic representation of entity types. AutoTransW embeds entities and relations into a complex space, treating relations as rotational operations from a head entity to a tail entity. Entity-specific triple encoders with hyperplane projection strategies are employed to embed entities and relations into the complex space. Secondly, type-specific triple encoders learn type embeddings through a relation-aware projection mechanism. Additionally, the incorporation of neighboring information about entities also enhances the model’s learning capabilities. Experimental results on the WN11, WN18, FB13, and FB15K datasets demonstrate that AutoTransW outperforms current state-of-the-art baselines in link prediction and triple classification tasks, confirming the model’s effectiveness.},
  archive      = {J_DMKD},
  author       = {Yuan, Man and Xing, Licheng and Yuan, Jingshu and Zhai, Kexin},
  doi          = {10.1007/s10618-025-01105-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Knowledge graph completion based on asymmetric translation and automatic entity type representation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven learning optimal k values for K-nearest neighbour matching in causal inference. <em>DMKD</em>, <em>39</em>(4), 1-24. (<a href='https://doi.org/10.1007/s10618-025-01107-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the realm of causal inference, a pivotal task involves causal effect estimation from observational data when there exist confounding variables. The K-Nearest Neighbour Matching (K-NNM) method is widely applied to handle confounding bias, but its general application sets a uniform K value for all samples, which can lead to suboptimal results in practice. To overcome this limitation, this paper introduces a novel method for causal effect estimation called Dynamic K-Nearest Neighbour Matching (DK-NNM). The DK-NNM method employs a data-driven learning strategy to determine the optimal value of K for each sample. In practice, DK-NNM reconstructs a sparse coefficient matrix for all samples using sparse learning, while simultaneously learning a graph matrix to preserve local information and sample similarity. This approach helps identify the most suitable K-value for each sample. Additionally, DK-NNM utilizes joint propensity and prognostic scores to effectively mitigate confounding bias arising from high-dimensional covariates during the K-NNM process. Experiments performed on various synthetic, semi-synthetic, and real-world datasets conclusively demonstrate that DK-NNM surpasses baseline models in estimating causal effects from observational data and provides significant improvements over traditional methods.},
  archive      = {J_DMKD},
  author       = {Zhang, Yinghao and Xu, Tingting and Cheng, Debo and Li, Jiuyong and Liu, Lin and Xu, Ziqi and Feng, Zaiwen},
  doi          = {10.1007/s10618-025-01107-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Data-driven learning optimal k values for K-nearest neighbour matching in causal inference},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Locality adaptive incomplete multi-view subspace clustering. <em>DMKD</em>, <em>39</em>(4), 1-28. (<a href='https://doi.org/10.1007/s10618-025-01109-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) has significantly been developed with the advances in information acquisition technologies. Most of MVC methods benefit from complete observation of all views so that consistent information and complementary information contained in different views can be extracted effectively. However, not all views of instances are always available in practical applications. In addition, existing self-representation learning methods for subspace clustering do not investigate the local structure of incomplete views to their full extent. This paper introduces a novel approach called Locality Adaptive Incomplete Multi-view Subspace Clustering (LAIMSC) to address the aforementioned challenges. Unlike previous incomplete multi-view subspace clustering that the process of local learning and clustering is often separated into two steps, our LAIMSC method seamlessly integrates local learning and subspace clustering into a unified framework, which can effectively capture complementary information from different incomplete views. In the meantime, due to the incompleteness of views, partitions of different views might be pretty different. Consequently, the proposed LAIMSC method generates an optimal partition for each view and then aligns each partition to form a consensus partition. The LAIMSC method demonstrates superior performance compared to current incomplete multi-view clustering algorithms, as validated by extensive experiments conducted on a variety of challenging datasets.},
  archive      = {J_DMKD},
  author       = {Zhong, Guo and Zhong, Min and Wu, Shengqi and Liang, Yuzhi and Song, Pengfei and Lin, Shixun and Zhu, Xiuyun},
  doi          = {10.1007/s10618-025-01109-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Locality adaptive incomplete multi-view subspace clustering},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data debiasing via causal diffusion model. <em>DMKD</em>, <em>39</em>(4), 1-36. (<a href='https://doi.org/10.1007/s10618-025-01111-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) systems are deployed at scale in real world, there is a growing concern about the harmful biases when addressing target tasks. Such biases originate from unbalanced training data with respect to sensitive attributes, potentially resulting in unfair treatment of specific groups or individuals. To address these challenges, we design a fairness-aware causal diffusion model (FCDM) that leverages neural network-based architectures to generate counterfactual samples. These counterfactual samples are used to augment original dataset to achieve a balance on sensitive attributes for fair model training. Generally, a qualified counterfactual sample generation method in fair model training requires the ability to flip specific sensitive attributes of original sample while keeping other attributes unchanged as much as possible, and addresses the scenarios involving one or multiple sensitive attributes. To achieve this, FCDM identifies the set of attributes that need to be flipped simultaneously, and calculate corresponding attributes weights by causal reasoning. The chosen attributes and their weight will be input into conditional guided diffusion model to generate desired counterfactual samples by adjusting model gradient. The experiments demonstrate that the counterfactual samples generated by FCDM exhibit better validity, similarity, and diversity compared to existing methods. Moreover, the models trained on the augmented dataset generated by FCDM show ideal fairness and accuracy.},
  archive      = {J_DMKD},
  author       = {Zhang, Wenqiong and Li, Yun and Wang, Yikai},
  doi          = {10.1007/s10618-025-01111-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-36},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Data debiasing via causal diffusion model},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential pattern detection: Similarities and differences across various fields. <em>DMKD</em>, <em>39</em>(4), 1-61. (<a href='https://doi.org/10.1007/s10618-025-01110-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting pattern matches underpins key operations across fields, such as complex event processing (CEP), sequential pattern mining (SPM), string pattern matching, pattern mining from a large sequence, and business process mining. These fields employ various notations and definitions for the detected patterns, posing challenges in recognizing their shared underlying concepts. This work aims to bridge these gaps by proposing a unified notation and terminology and then cataloging various pattern queries and constraints identified in different fields into a comprehensive framework. Our analysis reveals substantial similarities among the various pattern types, suggesting a promising avenue for the transfer of techniques between disciplines. This approach paves the way to leverage existing knowledge efficiently and circumvent the redundancy of “reinventing the wheel”.},
  archive      = {J_DMKD},
  author       = {Mavroudopoulos, Ioannis and Tsichlas, Kostas and Gounaris, Anastasios},
  doi          = {10.1007/s10618-025-01110-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-61},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Sequential pattern detection: Similarities and differences across various fields},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep unsupervised domain adaptation for time series classification: A benchmark. <em>DMKD</em>, <em>39</em>(4), 1-25. (<a href='https://doi.org/10.1007/s10618-025-01108-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims to harness labeled source data to train models for unlabeled target data. While research in UDA is extensive in domains like computer vision and natural language processing, it remains under-explored for time series data despite its widespread real-world applications. Our paper addresses this gap by introducing a comprehensive benchmark for evaluating UDA techniques for time series classification, with a focus on deep learning methods. We provide a fair and standardized UDA method assessment with state of the art neural network backbones (e.g. Inception) for time series data, as well as seven extra datasets in addition to the already established ones. This benchmark offers insights into the strengths and limitations of the evaluated approaches while preserving the unsupervised nature of DA, making it directly applicable to real data mining problems. It serves as a beneficial resource for researchers and practitioners, fostering innovation in this critical field. In order to ensure reproducibility, the code and results have been open-sourced.},
  archive      = {J_DMKD},
  author       = {Ismail Fawaz, Hassan and Del Grosso, Ganesh and Kerdoncuff, Tanguy and Boisbunon, Aurélie and Saffar, Illyyne},
  doi          = {10.1007/s10618-025-01108-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Deep unsupervised domain adaptation for time series classification: A benchmark},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How robust is your fair model? exploring the robustness of prominent fairness strategies. <em>DMKD</em>, <em>39</em>(4), 1-35. (<a href='https://doi.org/10.1007/s10618-025-01112-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the introduction of machine learning in high stakes decision-making, ensuring algorithmic fairness has become an increasingly important task. To this end, many mathematical definitions of fairness have been proposed, and a variety of optimisation techniques have been developed, all designed to maximise a given notion of fairness. Fair solutions, however, tend to rely on the quality of training data, and can be highly sensitive to noise. Recent studies have shown that robustness of many such fairness strategies—i.e., their ability to perform well on unseen data—is not a given and requires careful consideration. To address this challenge, we propose robustness ratio, which is a novel criterion to measure the robustness of diverse fairness optimisation strategies. We support our analysis with multiple extensive experiments on five benchmark fairness data sets, using three prominent fairness strategies, in view of four of the most popular definitions of fairness. Our experiments show that while fairness methods that rely on threshold optimisation (post-processing) mostly outperform other techniques, they are acutely sensitive to noise. This is in contrast to two other methods—correlation remover (pre-processing) and exponentiated gradient descent (in-processing)—which become increasingly fairer as the random noise injected into the data becomes larger. Our findings offer a comprehensive overview of fairness strategies that proves invaluable when tasked with choosing the most suitable method for the task at hand. To the best of our knowledge, we are the first to quantitatively evaluate the robustness of fairness optimisation strategies.},
  archive      = {J_DMKD},
  author       = {Small, Edward A. and Shao, Wei and Zhang, Zeliang and Liu, Peihan and Chan, Jeffrey and Sokol, Kacper and Salim, Flora D.},
  doi          = {10.1007/s10618-025-01112-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-35},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {How robust is your fair model? exploring the robustness of prominent fairness strategies},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hamming encoder: Mining discriminative k-mers for discrete sequence classification. <em>DMKD</em>, <em>39</em>(4), 1-34. (<a href='https://doi.org/10.1007/s10618-025-01113-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence classification has numerous applications in various fields. Despite extensive studies in the last decades, many challenges still exist, particularly in pattern-based methods. Existing pattern-based methods measure the discriminative power of each feature individually during the mining process, leading to the result of missing some combinations of features with discriminative power. Furthermore, it is difficult to ensure the overall discriminative performance after converting sequences into feature vectors. To address these challenges, we propose a novel approach called Hamming encoder, which utilizes a binarized 1D-convolutional neural network architecture to mine discriminative k-mer sets. In particular, we adopt a Hamming distance-based similarity measure to ensure consistency in the feature mining and classification procedure. Our method involves training an interpretable CNN encoder for sequential data and performing a gradient-based search for discriminative k-mer combinations. Experiments demonstrate that the Hamming encoder method proposed in this paper exhibits competitive performance when compared to existing state-of-the-art methods in terms of classification accuracy.},
  archive      = {J_DMKD},
  author       = {Junjie, Dong and Mudi, Jiang and Lianyu, Hu and Zengyou, He},
  doi          = {10.1007/s10618-025-01113-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1-34},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Hamming encoder: Mining discriminative k-mers for discrete sequence classification},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilayer horizontal visibility graphs for multivariate time series analysis. <em>DMKD</em>, <em>39</em>(3), 1-42. (<a href='https://doi.org/10.1007/s10618-025-01089-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series analysis is a vital but challenging task, with multidisciplinary applicability, tackling the characterization of multiple interconnected variables over time and their dependencies. Traditional methodologies often adapt univariate approaches or rely on assumptions specific to certain domains or problems, presenting limitations. A recent promising alternative is to map multivariate time series into high-level network structures such as multiplex networks, with past work relying on connecting successive time series components with interconnections between contemporary timestamps. In this work, we first define a novel cross-horizontal visibility mapping between lagged timestamps of different time series and then introduce the concept of multilayer horizontal visibility graphs. This allows describing cross-dimension dependencies via inter-layer edges, leveraging the entire structure of multilayer networks. To this end, a novel parameter-free topological measure is proposed and common measures are extended for the multilayer setting. Our approach is general and applicable to any kind of multivariate time series data. We provide an extensive experimental evaluation with both synthetic and real-world datasets. We first explore the proposed methodology and the data properties highlighted by each measure, showing that inter-layer edges based on cross-horizontal visibility preserve more information than previous mappings, while also complementing the information captured by commonly used intra-layer edges. We then illustrate the applicability and validity of our approach in multivariate time series mining tasks, showcasing its potential for enhanced data analysis and insights.},
  archive      = {J_DMKD},
  author       = {Freitas Silva, Vanessa and Silva, Maria Eduarda and Ribeiro, Pedro and Silva, Fernando},
  doi          = {10.1007/s10618-025-01089-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-42},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Multilayer horizontal visibility graphs for multivariate time series analysis},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forming coordinated teams that balance task coverage and expert workload. <em>DMKD</em>, <em>39</em>(3), 1-37. (<a href='https://doi.org/10.1007/s10618-025-01090-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a new formulation of the team-formation problem, where the goal is to form teams to work on a given set of tasks requiring different skills. Deviating from the classic problem setting where one is asking to cover all skills of each given task, we aim to cover as many skills as possible while also trying to minimize the maximum workload among the experts. We do this by combining penalization terms for the coverage and load constraints into one objective. We call the corresponding assignment problem Balanced-Coverage, and show that it is $$\textbf{NP}$$ -hard. We also consider a variant of this problem, where the experts are organized into a graph, which encodes how well they work together. Utilizing such a coordination graph, we aim to find teams to assign to tasks such that each team’s radius does not exceed a given threshold. We refer to this problem as Network-Balanced-Coverage. We develop a generic template algorithm for approximating both problems in polynomial time, and we show that our template algorithm for Balanced-Coverage has provable guarantees. We describe a set of computational speedups that we can apply to our algorithms and make them scale for reasonably large datasets. From the practical point of view, we demonstrate how to efficiently tune the two parts of the objective and tailor their importance to a particular application. Our experiments with a variety of real-world datasets demonstrate the utility of our problem formulation as well as the efficiency of our algorithms in practice.},
  archive      = {J_DMKD},
  author       = {Vombatkere, Karan and Gionis, Aristides and Terzi, Evimaria},
  doi          = {10.1007/s10618-025-01090-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-37},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Forming coordinated teams that balance task coverage and expert workload},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient outlier detection in numerical and categorical data. <em>DMKD</em>, <em>39</em>(3), 1-46. (<a href='https://doi.org/10.1007/s10618-024-01084-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to spot outliers in a large, unlabeled dataset with both numerical and categorical attributes? How to do it in a fast and scalable way? Outlier detection has many applications; it is covered therefore by an extensive literature. The distance-based detectors are the most popular ones. However, they still have two major drawbacks: (a) the intensive neighborhood search that takes hours or even days to complete in large data, and; (b) the inability to process categorical attributes. This paper tackles both problems by presenting HySortOD: a new, fast and scalable detector for numerical and categorical data. Our main focus is the analysis of datasets with many instances, and a low-to-moderate number of attributes. We studied dozens of real, benchmark datasets with up to one million instances; HySortOD outperformed nine competitors from the state of the art in runtime, being up to six orders of magnitude faster in large data, while maintaining high accuracy. Finally, we also performed an extensive experimental evaluation that confirms the ability of our method to obtain high-quality results from both real and synthetic datasets with categorical attributes.},
  archive      = {J_DMKD},
  author       = {Cabral, Eugênio F. and Sánchez Vinces, Braulio V. and Silva, Guilherme D. F. and Sander, Jörg and Cordeiro, Robson L. F.},
  doi          = {10.1007/s10618-024-01084-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-46},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Efficient outlier detection in numerical and categorical data},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommending the right academic programs: An interest mining approach using BERTopic. <em>DMKD</em>, <em>39</em>(3), 1-40. (<a href='https://doi.org/10.1007/s10618-024-01087-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prospective students face the challenging task of selecting a university program that will shape their academic and professional careers. For decision-makers and support services, it is often time-consuming and extremely difficult to match personal interests with suitable programs due to the vast and complex catalogue information available. This paper presents the first information system that provides students with efficient recommendations based on both program content and personal preferences. BERTopic, a powerful topic modeling algorithm, is used that leverages text embedding techniques to generate topic representations. It enables us to mine interest topics from all course descriptions, representing the full body of knowledge taught at the institution. Underpinned by the student’s individual choice of topics, a shortlist of the most relevant programs is computed through statistical backtracking in the knowledge map, a novel characterization of the program-course relationship. This approach can be applied to a wide range of educational settings, including professional and vocational training. A case study at a post-secondary school with 80 programs and over 5,000 courses shows that the system provides immediate and effective decision support. The presented interest topics are meaningful, leading to positive effects such as serendipity, personalization, and fairness, as revealed by a qualitative study involving 65 students. Over 98% of users indicated that the recommendations aligned with their interests, and about 94% stated they would use the tool in the future. Quantitative analysis shows the system can be configured to ensure fairness, achieving 98% program coverage while maintaining a personalization score of 0.77. These findings suggest that universities could expand student support services by implementing this real-time, user-centered, data-driven system to improve the program selection process.},
  archive      = {J_DMKD},
  author       = {Hill, Alessandro and Goo, Kalen and Agarwal, Puneet},
  doi          = {10.1007/s10618-024-01087-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-40},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Recommending the right academic programs: An interest mining approach using BERTopic},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-neighbor social recommendation with attentional graph convolutional network. <em>DMKD</em>, <em>39</em>(3), 1-22. (<a href='https://doi.org/10.1007/s10618-025-01094-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social network is usually utilized as auxiliary data to alleviate data sparsity and cold start problems of recommender systems. However, the social network obeys the power-law distribution and the social relations are indiscriminately utilized, resulting in providing insufficient and inaccurate information. Early efforts ignore these drawbacks and fail to exploit abundant information fully. In this paper, we propose a novel multi-neighbor social recommendation framework MNGCN based on graph convolutional network. The representations of user and item could be learned by iteratively integrating their multiple types of neighbor information. In addition, we apply a node-level attention mechanism to aggregate the same type of neighbors and a category-level attention mechanism to incorporate different categories of neighbors. A sampler is utilized to accurately select the social neighbors of users with regard to different items. Besides, the interactions and ratings are captured simultaneously in the user-item interactive network. Extensive experiments on two classical datasets illustrate that MNGCN achieves the best performance, and the ablation study demonstrates the necessity and the effectiveness of each component.},
  archive      = {J_DMKD},
  author       = {Zhang, Min and Liao, Xiao and Wang, Xinlei and Wang, Xiaojuan and Jin, Lei},
  doi          = {10.1007/s10618-025-01094-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Multi-neighbor social recommendation with attentional graph convolutional network},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextualization of soccer analysis with tactical periodization and machine learning. <em>DMKD</em>, <em>39</em>(3), 1-34. (<a href='https://doi.org/10.1007/s10618-025-01092-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has become common practice in topflight leagues to track position data of soccer players and the ball. Analyzing sports performance based on this high-resolution data is a non-trivial task due to the great complexity and simultaneous lack of structure of the game. Sports practitioners tackle this problem through tactical periodization, i.e., mapping the course of the game onto different states, so-called match phases. However, creating manual tactical periodizations is a time-consuming task prone to subjective biases. Automatic approaches are thus preferred, but validated and open match phase models are currently lacking. The present study addresses this issue by (1) formalizing a domain-specific, qualitative match phase annotation scheme from related work, (2) creating and validating a multi-annotator set of annotations, (3) training several supervised machine learning architectures to fully automate the task of annotation, and (4) demonstrating the usefulness by conducting a contextualized detection of playing formations with the best model, referred to as FeatGRU. Steps (2) through (4) were performed on a set of real-world soccer data and the best-performing model is made available. FeatGRU is of value to the soccer community as it provides a fully automatic, frame-by-frame match phase annotation that matches domain experts’ opinions with 80% accuracy while being modular extendable for future work. Moreover, we found a strong relation between semantic complexity of matchphases, expert agreements, and classification performance, highlighting the importance of valid label generation. Thus, our approach presents an interesting benchmark to domains where automatic approaches are required while ambiguity between human expert opinions exists.},
  archive      = {J_DMKD},
  author       = {Biermann, Henrik and Memmert, Daniel and Petersen, Niklas and Raabe, Dominik},
  doi          = {10.1007/s10618-025-01092-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-34},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Contextualization of soccer analysis with tactical periodization and machine learning},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ada-context: Adaptive context-aware grid-based approach for curation of data streams. <em>DMKD</em>, <em>39</em>(3), 1-30. (<a href='https://doi.org/10.1007/s10618-025-01095-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing and real-time applications have changed how data is collected and processed. However, data quality is crucial for its usefulness. In this paper, we introduce Ada-Context, an approach that uses external contextual information to improve data quality assessment. It involves offline and online analysis components and uses a grid structure to map streaming data to cells, enhancing performance of quality control in data streams. Results show that contextual data especially external context improves data cleansing accuracy and the grid design boosts quality control effectiveness for data streams.},
  archive      = {J_DMKD},
  author       = {Mirzaie, Mostafa and Behkamal, Behshid and Allahbakhsh, Mohammad and Paydar, Samad and Bertino, Elisa},
  doi          = {10.1007/s10618-025-01095-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Ada-context: Adaptive context-aware grid-based approach for curation of data streams},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting anomalies using rotated isolation forest. <em>DMKD</em>, <em>39</em>(3), 1-31. (<a href='https://doi.org/10.1007/s10618-025-01096-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Isolation Forest (iForest), proposed by Liu et al. at TKDE 2012, has become a prominent tool for unsupervised anomaly detection. However, recent research by Hariri, Kind, and Brunner, published in TKDE 2021, has revealed issues with iForest. They identified the presence of axis-aligned ghost clusters that can be misidentified as normal clusters, leading to biased anomaly scores and inaccurate predictions. In response, they developed the Extended Isolation Forest (EIF), which effectively solves these issues by eliminating the ghost clusters introduced by iForest. This enhancement results in improved consistency of anomaly scores and superior performance. We reveal a previously overlooked problem in the EIF, showing that it is vulnerable to ghost inter-clusters between normal clusters of data points. In this paper, we introduce the Rotated Isolation Forest (RIF) algorithm which effectively addresses both the axis-aligned ghost clusters observed in iForest and the ghost inter-clusters seen in EIF. RIF accomplishes this by randomly rotating the dataset (using random rotation matrices and QR decomposition) before feeding it into the iForest construction, thereby increasing dataset variation and eliminating ghost clusters. Our experiments conclusively demonstrate that the RIF algorithm outperforms iForest and EIF, as evidenced by the results obtained from both synthetic datasets and real-world datasets.},
  archive      = {J_DMKD},
  author       = {Monemizadeh, Vahideh and Kiani, Kourosh},
  doi          = {10.1007/s10618-025-01096-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Detecting anomalies using rotated isolation forest},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proxy-enhanced cross-domain sequential recommendation. <em>DMKD</em>, <em>39</em>(3), 1-27. (<a href='https://doi.org/10.1007/s10618-025-01097-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain sequential recommendation (CDSR) aims to predict the next item that a user is most likely to interact with based on past sequential behavior from multiple domains. Existing works on CDSR usually transfer knowledge across different domains by linking items between domains via common users, which suffers from the following limitations: (1) Due to the inherent differences between the domains, transferring information across domains can be affected by different representations of related items in different domains. (2) User behavior in various domains may not always contribute constructively to the recommendation process in the target domain. (3) The overall semantic characterization of cross-domain sequences tends to be dominated by data-rich domains. In this work, we propose a novel cross-domain sequential recommendation model to address the above challenges. Specifically, we first develop a proxy encoder that uses textual descriptions to pre-train a universal representation for each item across all domains. We then design a feature enhancement module to filter out extraneous information from the textual descriptions, thereby optimizing and enriching this representation. Moreover, we present a contrastive learning auxiliary task to enhance a cross-domain sequence by weighing the importance of the items in the auxiliary domain with respect to the target domain. Finally, in the prediction phase, we construct the pre-trained prompt representations for each domain to balance the training of the two domains. Extensive experiments demonstrate the superiority of our proposed method from various aspects.},
  archive      = {J_DMKD},
  author       = {Xiao, Shitong and Chen, Rui and Song, Hongtao and Lai, Riwei and Han, Qilong and Li, Li},
  doi          = {10.1007/s10618-025-01097-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-27},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Proxy-enhanced cross-domain sequential recommendation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSRF-dist: A novel time series distance based on extremely randomized canonical interval forests. <em>DMKD</em>, <em>39</em>(3), 1-30. (<a href='https://doi.org/10.1007/s10618-025-01098-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents TSRF-Dist, a novel distance between time series based on Random Forests (RFs). We extend to the time-series domain concepts and tools of RF distances, a recent class of robust data-dependent distances defined for vectorial representations, thus proposing the first RF distance for time series. The distance is determined by (i) creating an RF to model a set of time series, and (ii) exploiting the trained RF to quantify the similarity between time series. As for the first step, we introduce in this paper the Extremely Randomized Canonical Interval Forest (ERCIF), a novel extension of Canonical Interval Forests that can model time series and can be trained without labels. We then exploit three different schemes, following ideas already employed in the vectorial case. The proposed distance, in different variants, has been thoroughly evaluated with 128 datasets from the UCR Time Series archive, showing promising results compared with literature alternatives.},
  archive      = {J_DMKD},
  author       = {Azzari, Alberto and Bicego, Manuele and Combi, Carlo and Cracco, Andrea and Sala, Pietro},
  doi          = {10.1007/s10618-025-01098-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TSRF-dist: A novel time series distance based on extremely randomized canonical interval forests},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HyEED: Embedding learning of knowledge graphs with entity description in hyperbolic space. <em>DMKD</em>, <em>39</em>(3), 1-27. (<a href='https://doi.org/10.1007/s10618-025-01100-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Knowledge graph aim to use graph structures to model, identify and infer complex relations and domain knowledge among things. It serves as a crucial cornerstone for achieving cognitive intelligence. Typically, knowledge graphs exhibit hierarchical structures, and many studies have demonstrated that hyperbolic embedding methods exhibit strong learning capabilities in learning data with hierarchical structures. It can effectively capture hierarchical structures in a high-fidelity and simplified form. Nevertheless, current hyperbolic embedding models only focus on learning knowledge triples of relations between entities, making the embedding space relatively one-sided. They ignore the entity description, category and other additional information containing rich features, which can make graph embedding learn richer semantic information. Therefore, to address this limitation, we propose a hyperbolic knowledge graph embedding with entity description (HyEED), which can further enrich and compensate for graph structure information, thus achieving better embedding effects. Specifically, we use hyperbolic space-based word embedding model and graph embedding model to encode entity description text information and graph structure information, respectively, and effectively combine them using hyperbolic pooling techniques. Extensive experimental results have unequivocally validated the efficacy and superiority of HyEED.},
  archive      = {J_DMKD},
  author       = {Bao, Liming and Wang, Yan and Song, Xiaoyu},
  doi          = {10.1007/s10618-025-01100-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {1-27},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {HyEED: Embedding learning of knowledge graphs with entity description in hyperbolic space},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Missing value replacement in strings and applications. <em>DMKD</em>, <em>39</em>(2), 1-50. (<a href='https://doi.org/10.1007/s10618-024-01074-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values arise routinely in real-world sequential (string) datasets due to: (1) imprecise data measurements; (2) flexible sequence modeling, such as binding profiles of molecular sequences; or (3) the existence of confidential information in a dataset which has been deleted deliberately for privacy protection. In order to analyze such datasets, it is often important to replace each missing value, with one or more valid letters, in an efficient and effective way. Here we formalize this task as a combinatorial optimization problem: the set of constraints includes the context of the missing value (i.e., its vicinity) as well as a finite set of user-defined forbidden patterns, modeling, for instance, implausible or confidential patterns; and the objective function seeks to minimize the number of new letters we introduce. Algorithmically, our problem translates to finding shortest paths in special graphs that contain forbidden edges representing the forbidden patterns. Our work makes the following contributions: (1) we design a linear-time algorithm to solve this problem for strings over constant-sized alphabets; (2) we show how our algorithm can be effortlessly applied to fully sanitize a private string in the presence of a set of fixed-length forbidden patterns [Bernardini et al. 2021a]; (3) we propose a methodology for sanitizing and clustering a collection of private strings that utilizes our algorithm and an effective and efficiently computable distance measure; and (4) we present extensive experimental results showing that our methodology can efficiently sanitize a collection of private strings while preserving clustering quality, outperforming the state of the art and baselines. To arrive at our theoretical results, we employ techniques from formal languages and combinatorial pattern matching.},
  archive      = {J_DMKD},
  author       = {Bernardini, Giulia and Liu, Chang and Loukides, Grigorios and Marchetti-Spaccamela, Alberto and Pissis, Solon P. and Stougie, Leen and Sweering, Michelle},
  doi          = {10.1007/s10618-024-01074-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-50},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Missing value replacement in strings and applications},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative evaluation of clustering-based outlier detection. <em>DMKD</em>, <em>39</em>(2), 1-55. (<a href='https://doi.org/10.1007/s10618-024-01086-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform an extensive experimental evaluation of clustering-based outlier detection methods. These methods offer benefits such as efficiency, the possibility to capitalize on more mature evaluation measures, more developed subspace analysis for high-dimensional data and better explainability, and yet they have so-far been neglected in literature. To our knowledge, our work is the first effort to analytically and empirically study their advantages and disadvantages. Our main goal is to evaluate whether or not clustering-based techniques can compete in efficiency and effectiveness against the most studied state-of-the-art algorithms in the literature. We consider the quality of the results, the resilience against different types of data and variations in parameter configuration, the scalability, and the ability to filter out inappropriate parameter values automatically based on internal measures of clustering quality. It has been recently shown that several classic, simple, unsupervised methods surpass many deep learning approaches and, hence, remain at the state-of-the-art of outlier detection. We therefore study 14 of the best classic unsupervised methods, in particular 11 clustering-based methods and 3 non-clustering-based ones, using a consistent parameterization heuristic to identify the pros and cons of each approach. We consider 46 real and synthetic datasets with up to 125k points and 1.5k dimensions aiming to achieve plausibility with the broadest possible diversity of real-world use cases. Our results indicate that the clustering-based methods are on par with (if not surpass) the non-clustering-based ones, and we argue that clustering-based methods like KMeans−− should be included as baselines in future benchmarking studies, as they often offer a competitive quality at a relatively low run time, besides several other benefits.},
  archive      = {J_DMKD},
  author       = {Sánchez Vinces, Braulio V. and Schubert, Erich and Zimek, Arthur and Cordeiro, Robson L. F.},
  doi          = {10.1007/s10618-024-01086-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-55},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A comparative evaluation of clustering-based outlier detection},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain-level relation extraction for informative taxonomy learning. <em>DMKD</em>, <em>39</em>(2), 1-31. (<a href='https://doi.org/10.1007/s10618-024-01088-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the ever-shifting domain of technological advancement, the quest for automatic taxonomy construction is intensifying. This paper confronts the nuanced challenges of distilling synonym and hyponym relationships from diverse, domain-specific scientific literature, treating it as a domain-level relation extraction problem and resulting in the creation of a hierarchical taxonomy through arborescence generation. The proposed Multi-Scale Identity Connection (MSIC) model excels in capturing inter-entity relationships across various scales, demonstrating superior empirical performance compared to existing relation extraction models. To enhance practicality, a two-stage optimization is introduced to improve efficiency without compromising performance. Additionally, the Depth-prioritized Maximum Spanning Arborescence (DMSA) algorithm has been proposed as a highly efficient strategy for generating an informative and well-structured taxonomy tree. We annotated a concise dataset to train and validate the MSIC model, subsequently applying it to a substantial domain-specific dataset for taxonomy induction. The experimental findings indicate that the DMSA efficiently constructs an information-rich taxonomy tree structure by leveraging extensive domain-specific scientific literature. These results not only affirm the efficacy of the approach but also highlight its effectiveness in supporting industrial-grade applications.},
  archive      = {J_DMKD},
  author       = {Hu, Maodi and Song, Donghuan and Qian, Li and Zhang, Zhixiong},
  doi          = {10.1007/s10618-024-01088-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Domain-level relation extraction for informative taxonomy learning},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximity forest 2.0: A new effective and scalable similarity-based classifier for time series. <em>DMKD</em>, <em>39</em>(2), 1-29. (<a href='https://doi.org/10.1007/s10618-024-01085-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) is a challenging task due to the diversity of types of features that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms other state-of-the-art methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series similarity measures — (1) computationally efficient early abandoning and pruning to speedup elastic similarity computations; (2) a new elastic similarity measure, Amerced Dynamic Time Warping ( $${{\,\textrm{ADTW}\,}}$$ ); and (3) cost function tuning. It rationalizes the set of similarity measures employed, reducing the eight base measures of the original PF to four and using the first derivative transform with all similarity measures, rather than a limited subset. It also incorporates HYDRA, a dictionary-based transform. We have re-implemented PF 1.0 and implemented PF 2.0 framework in Java, making the PF framework more efficient.},
  archive      = {J_DMKD},
  author       = {Tan, Chang Wei and Herrmann, Matthieu and Salehi, Mahsa and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-024-01085-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Proximity forest 2.0: A new effective and scalable similarity-based classifier for time series},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TED: Related party transaction guided tax evasion detection on heterogeneous graph. <em>DMKD</em>, <em>39</em>(2), 1-24. (<a href='https://doi.org/10.1007/s10618-025-01091-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tax evasion causes severe losses of government revenues and disturbs the economic order of fair competition. To help alleviate this problem, the latest tax evasion detection solutions utilize expert knowledge to extract features and then train classifiers to determine whether a company is suspected of tax evasion. However, existing solutions mainly focus on the statistical features of the company, but fail to exploit the rich interactive information in tax scenarios, which affect the detection performance. In this paper, we first model the tax scenario as a heterogeneous graph and study the tax evasion detection problem under the heterogeneous graph model. To improve the performance of tax evasion detection, a novel graph neural network model is proposed to extract the comprehensive information of heterogeneous graphs. Specifically, we use heterogeneous and complex related party transaction groups to filter low-level noise information. Moreover, a hierarchical attention mechanism is designed to capture the deeper structure and semantic information hidden in the related party transaction group. We apply our method to the real risk management system of the tax bureau, and evaluate it on two human-labeled real-world tax datasets. The results demonstrate that our method significantly outperforms the state-of-the-art in the tax evasion detection task. The code and data are available at: https://github.com/yimingxu24/TED .},
  archive      = {J_DMKD},
  author       = {Xu, Yiming and Shi, Bin and Dong, Bo and Wang, Jiaxiang and Wei, Hua and Zheng, Qinghua},
  doi          = {10.1007/s10618-025-01091-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TED: Related party transaction guided tax evasion detection on heterogeneous graph},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inferring tie strength in temporal networks. <em>DMKD</em>, <em>39</em>(2), 1-31. (<a href='https://doi.org/10.1007/s10618-025-01093-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring tie strengths in social networks is an essential task in social network analysis. Common approaches classify the ties as weak and strong ties based on the strong triadic closure (STC). The STC states that if for three nodes, A, B, and C, there are strong ties between A and B, as well as A and C, there has to be a (weak or strong) tie between B and C. A variant of the STC called STC+ allows adding a few new weak edges to obtain improved solutions. So far, most works discuss the STC or STC+ in static networks. However, modern large-scale social networks are usually highly dynamic, providing user contacts and communications as streams of edge updates. Temporal networks capture these dynamics. To apply the STC to temporal networks, we first generalize the STC and introduce a weighted version such that empirical a priori knowledge given in the form of edge weights is respected by the STC. Similarly, we introduce a generalized weighted version of the STC+. The weighted STC is hard to compute, and our main contribution is an efficient 2-approximation (resp. 3-approximation) streaming algorithm for the weighted STC (resp. STC+) in temporal networks. As a technical contribution, we introduce a fully dynamic k-approximation for the minimum weighted vertex cover problem in hypergraphs with edges of size k, which is a crucial component of our streaming algorithms. An empirical evaluation shows that the weighted STC leads to solutions that better capture the a priori knowledge given by the edge weights than the non-weighted STC. Moreover, we show that our streaming algorithm efficiently approximates the weighted STC in real-world large-scale social networks.},
  archive      = {J_DMKD},
  author       = {Oettershagen, Lutz and Konstantinidis, Athanasios L. and Italiano, Giuseppe F.},
  doi          = {10.1007/s10618-025-01093-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Inferring tie strength in temporal networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A probabilistic model for API contract specification retrieval focusing on the openAPI standard. <em>DMKD</em>, <em>39</em>(1), 1-24. (<a href='https://doi.org/10.1007/s10618-024-01073-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a new API for a large-scale project requires developers to make strategic design choices that will allow the codebase to evolve sustainably. To create well-structured API components, developers can learn from existing APIs. However, the lack of standardized methods for comparing API designs often makes this learning process inefficient and challenging. To bridge this gap, we introduce API-Miner, which, to our knowledge, is one of the first engines to recommend API-to-API specifications. API-Miner retrieves relevant components from OpenAPI specifications-a widely recognized standard for describing web APIs. The engine introduces several key innovations: (1) novel techniques for processing and extracting critical information from OpenAPI specifications, (2) specialized feature extraction methods tailored to the technical domain of API specifications, and (3) a log-linear probabilistic model that integrates multiple signals to retrieve relevant and high-quality OpenAPI components based on a query specification. Through both quantitative and qualitative evaluations, API-Miner achieves a recall@1 of 91.7% and an F1 score of 56.2%, outperforming baseline models by 15.4 percentage points (pp) in recall@1 and 3.2 pp in F1. API-Miner enables developers to access relevant OpenAPI components from public or internal databases early in the API development cycle, facilitating the learning of best practices and the identification of potential redundancies. This tool helps streamline the development process and supports the creation of maintainable, high-quality APIs. The code for API-Miner is available at https://github.com/jpmorganchase/api-miner .},
  archive      = {J_DMKD},
  author       = {Moon, Saeyoung and Kerr, Gregor and Silavong, Fran and Moran, Sean},
  doi          = {10.1007/s10618-024-01073-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A probabilistic model for API contract specification retrieval focusing on the openAPI standard},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What do anomaly scores actually mean? dynamic characteristics beyond accuracy. <em>DMKD</em>, <em>39</em>(1), 1-59. (<a href='https://doi.org/10.1007/s10618-024-01077-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection has become pervasive in modern technology, covering applications from cybersecurity, to medicine or system failure detection. Before outputting a binary outcome (i.e., anomalous or non-anomalous), most algorithms evaluate instances with outlierness scores. But what does a score of 0.8 mean? Or what is the practical difference compared to a score of 1.2? Score ranges are assumed non-linear and relative, their meaning established by weighting the whole dataset (or a dataset model). While this is perfectly true, algorithms also impose dynamics that decisively affect the meaning of outlierness scores. In this work, we aim to gain a better understanding of the effect that both algorithms and specific data particularities have on the meaning of scores. To this end, we compare established outlier detection algorithms and analyze them beyond common metrics related to accuracy. We disclose trends in their dynamics and study the evolution of their scores when facing changes that should render them invariant. For this purpose we abstract characteristic S-curves and propose indices related to discriminant power, bias, variance, coherence and robustness. We discovered that each studied algorithm shows biases and idiosyncrasies, which habitually persist regardless of the dataset used. We provide methods and descriptions that facilitate and extend a deeper understanding of how the discussed algorithms operate in practice. This information is key to decide which one to use, thus enabling a more effective and conscious incorporation of unsupervised learning in real environments.},
  archive      = {J_DMKD},
  author       = {Iglesias Vázquez, Félix and Marques, Henrique O. and Zimek, Arthur and Zseby, Tanja},
  doi          = {10.1007/s10618-024-01077-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-59},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {What do anomaly scores actually mean? dynamic characteristics beyond accuracy},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the diverse world of SAX-based methodologies. <em>DMKD</em>, <em>39</em>(1), 1-78. (<a href='https://doi.org/10.1007/s10618-024-01075-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic Aggregate Approximation (SAX) is a widely used method for time series data analysis, known for its ability to transform continuous data to discrete symbols. While SAX has demonstrated its effectiveness in various applications, it is not without limitations. SAX method has certain weaknesses that have motivated researchers to further investigate and propose new modifications to address them. In this article, we present a comprehensive review of the published variations of SAX and categorize them into groups based on the weaknesses they aim to overcome. This taxonomy provides a valuable resource for researchers interested in SAX, as it consolidates a wide range of modifications in one work. Through an extensive literature review, we have gathered a diverse collection of all published variations of the SAX method. Each variation is accompanied by a concise description, facilitating readers’ understanding of the key features and benefits of each approach. By systematically presenting these variations, this paper encourages the exploration of the SAX method’s landscape, enabling researchers to identify the most suitable method for their specific needs, revealing research gaps and alongside promoting further investigation in this field.},
  archive      = {J_DMKD},
  author       = {Pappa, Lamprini and Karvelis, Petros and Stylios, Chrysostomos},
  doi          = {10.1007/s10618-024-01075-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-78},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Exploring the diverse world of SAX-based methodologies},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of deep learning for structural geological interpretation. <em>DMKD</em>, <em>39</em>(1), 1-56. (<a href='https://doi.org/10.1007/s10618-024-01079-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that seismic data (or seismic volumes/images) are one of the primary work materials of the oil and gas industry. Nevertheless, the manual interpretation of such data has become increasingly time-consuming and prone to errors. This problem arises due to the massive amount of data and the complexity and variability of geological patterns. Over the last few years, image processing methods from the Artificial Intelligence (AI) sub-field Deep Learning (DL) have become valuable tools for seismic volume interpretation. DL techniques are already incorporated in commercial seismic interpretation software such as GeoTeric and Earth Science Analytics. Overall, DL accelerates the seismic interpretation process and enhances the results’ precision and consistency. This allows for a better understanding of geological structures and reduces the risks associated with exploring and producing natural resources. Despite these facts, a systematic and broad review of DL techniques for different structural geological interpretation tasks has not been made available to researchers and practitioners. This article comprehensively reviews current techniques, models, and practices in DL-based seismic volume interpretation based on three different structural interpretation tasks: Fault interpretation, Horizon estimation, and Relative Geological Time (RGT) estimation, three complementary pillars of the geological framework. Our review is based on 85 relevant research articles, provides a foundational view of DL concepts, and introduces taxonomies for applying DL to structural geological interpretation. Our review also paves the way for further research by presenting the challenges encountered when applying deep learning in geological tasks and additional directions for future endeavors. The major applications of the techniques presented here are identifying architectural depositional elements and petrophysical properties of seismic images for natural oil and gas exploration.},
  archive      = {J_DMKD},
  author       = {Fernandes, Gustavo Lúcius and Figueiredo, Flavio and Hatushika, Raphael Siston and Leão, Maria Luiza and Mariano, Breno Augusto and Monteiro, Bruno Augusto Alemão and de Cerqueira Oliveira, Fernando Tonucci and Panoutsos, Tales and Pires, João Pedro and Poppe, Thiago Martin and Zavam, Frederico},
  doi          = {10.1007/s10618-024-01079-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-56},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A systematic review of deep learning for structural geological interpretation},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARL: Analogical reinforcement learning for knowledge graph reasoning. <em>DMKD</em>, <em>39</em>(1), 1-22. (<a href='https://doi.org/10.1007/s10618-024-01080-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) knowledge graph reasoning aims to predict complete triplets by learning existing relationship paths. This greatly improves the efficiency of prediction because the RL-based methods do not traverse all entities and relations like representation reasoning. Meanwhile, this kind of method increases the interpretability of reasoning. However, due to the necessity of normalizing the entity outdegree matrices for neural network computations in each step of the retrieval process in reinforcement learning, entities with an excessively high number of outdegrees compel the RL-based model to restrict the retrieval space of each path. Consequently, this limitation leads to the omission of some correct answers. Moreover, for some isolated tail entities with sparse connections, this path-based reasoning will lose these island nodes. To solve both problems, we propose an analogy-based reinforcement learning model named Analogical Reinforcement Learning network (ARL). This model features a novel analogy reinforcement learning architecture, dynamic graph attention networks, and our proprietary AODS algorithm. It injects entity analogy information into the model’s reasoning process and employs virtual link generation, which not only enhances the probability of paths getting rewards, but also increases the breadth of path connection and brings more possibilities for island nodes. In the meantime, we analyze and compare various analogy methods in detail. Experimental results show that ARL outperforms existing multi-hop methods on several datasets.},
  archive      = {J_DMKD},
  author       = {Xia, Nan and Wang, Yin and Zhang, Run-Fa and Luo, Xiangfeng},
  doi          = {10.1007/s10618-024-01080-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ARL: Analogical reinforcement learning for knowledge graph reasoning},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Syntheval: A framework for detailed utility and privacy evaluation of tabular synthetic data. <em>DMKD</em>, <em>39</em>(1), 1-25. (<a href='https://doi.org/10.1007/s10618-024-01081-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing demand for synthetic data to address contemporary issues in machine learning, such as data scarcity, data fairness, and data privacy, having robust tools for assessing the utility and potential privacy risks of such data becomes crucial. SynthEval, a novel open-source evaluation framework distinguishes itself from existing tools by treating categorical and numerical attributes with equal care, without assuming any special kind of preprocessing steps. This makes it applicable to virtually any synthetic dataset of tabular records. Our tool leverages statistical and machine learning techniques to comprehensively evaluate synthetic data fidelity and privacy-preserving integrity. SynthEval integrates a wide selection of metrics that can be used independently or in highly customisable benchmark configurations, and can easily be extended with additional metrics. In this paper, we describe SynthEval and illustrate its versatility with examples. The framework facilitates better benchmarking and more consistent comparisons of model capabilities.},
  archive      = {J_DMKD},
  author       = {Lautrup, Anton D. and Hyrup, Tobias and Zimek, Arthur and Schneider-Kamp, Peter},
  doi          = {10.1007/s10618-024-01081-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Syntheval: A framework for detailed utility and privacy evaluation of tabular synthetic data},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-path based proximity learning in heterogeneous information networks. <em>DMKD</em>, <em>39</em>(1), 1-37. (<a href='https://doi.org/10.1007/s10618-024-01076-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PathSim is a widely used meta-path-based similarity in heterogeneous information networks (HINs). Numerous applications rely on the computation of PathSim, including similarity search and clustering. Computing PathSim scores on large HINs is computationally challenging due to its high time and storage complexity. In this paper, we propose to transform the computation of the ground truth PathSim into a learning problem. We design an encoder-decoder based framework, NeuPath, where the algorithmic structure of PathSim is considered. NeuPath has two variants: (1) The static variant learns PathSim scores between two nodes in a static heterogeneous graph. Specifically, the encoder module identifies Top T optimized path instances, which can approximate the ground truth PathSim. (2) The temporal variant predicts the PathSim scores for a pair of nodes at a certain timestamp. Here, the encoder weighs neighbors with the constraint of a specific time encoding on every edge. The decoder transforms each embedding vector into a scalar respectively, which identifies the similarity score. We perform extensive experiments on real-world datasets from different domains. Our results demonstrate that NeuPath performs better than state-of-the-art baselines in the PathSim approximation task and similarity search task in both static and temporal HINs.},
  archive      = {J_DMKD},
  author       = {Xiao, Wenyi and Zhao, Huan and Zheng, Vincent W. and Song, Yangqiu},
  doi          = {10.1007/s10618-024-01076-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Meta-path based proximity learning in heterogeneous information networks},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIRACLE: Malware image recognition and classification by layered extraction. <em>DMKD</em>, <em>39</em>(1), 1-35. (<a href='https://doi.org/10.1007/s10618-024-01078-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annually, over 800,000 individuals are victims of cyberattacks, predominantly through malware, which possesses the capacity to emerge as a formidable instrument of destruction in the realm of cybersecurity. And, it is a challenging task to manually thwart an assault by malware. Hence, it is crucial to properly categorize malware binaries in order to identify their origins. Furthermore, malware structure discovery and analysis through simple feature extraction approaches is time-consuming and challenging. In fact, malware classification was previously explored using machine learning (ML)-based approaches such as SVM and XGBoost. In addition, recently, deep learning (DL) has proven to be effective in finding malicious patterns. Without DL, analysis of the vast amounts of available data tends to be impossible. In addition, existing methods, such as “transfer-learning”, “fusion-methodology”, “and “transformer-techniques”, lack evidence of effectiveness when tested on actual malware binary files. Moreover, hand-crafted features engineering or rudimentary Convolutional Neural Networks (CNNs) do not perform well and take much time to find effective features. To address the aforementioned challenges, we propose a novel approach, Malware Image Recognition & Classification by Layered Extraction (MIRACLE), by implementing our own spatial convolutional neural network (Sp-CNN) with sufficient regularization and data augmentation to identify and classify malware in images effectively and efficiently. Our proposed method is developed based on analyzing malware binary structure, which is segmented as headers and section, symbolic information lies on section segment. Our Sp-CNN can extract that symbolic information from the top of the hidden layer constructively. We have evaluated our model with as MalImg, Microsoft-Big, Malevis and Android Malware dataset. We achieved accuracy of 99.87% for MalImg, 99.81% for Microsoft-Big, and 99.22% for Malevis in our test dataset, respectively. Our proposed method surpasses Google’s InceptionV3, ResNet50, EfficientNetB1, VGG16, VGG19, and other state-of-the-art (SOTA) methods in terms of performance.},
  archive      = {J_DMKD},
  author       = {Alam, Inzamamul and Samiullah, Md. and Asaduzzaman, S M and Kabir, Upama and Aahad, A. M. and Woo, Simon S.},
  doi          = {10.1007/s10618-024-01078-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MIRACLE: Malware image recognition and classification by layered extraction},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new bandit setting balancing information from state evolution and corrupted context. <em>DMKD</em>, <em>39</em>(1), 1-44. (<a href='https://doi.org/10.1007/s10618-024-01082-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new sequential decision-making setting, combining key aspects of two established online learning problems with bandit feedback. The optimal action to play at any given moment is contingent on an underlying changing state that is not directly observable by the agent. Each state is associated with a context distribution, possibly corrupted, allowing the agent to identify the state. Furthermore, states evolve in a Markovian fashion, providing useful information to estimate the current state via state history. In the proposed problem setting, we tackle the challenge of deciding on which of the two sources of information the agent should base its action selection. We present an algorithm that uses a referee to dynamically combine the policies of a contextual bandit and a multi-armed bandit. We capture the time-correlation of states through iteratively learning the action-reward transition model, allowing for efficient exploration of actions. Our setting is motivated by adaptive mobile health (mHealth) interventions. Users transition through different, time-correlated, but only partially observable internal states, determining their current needs. The side information associated with each internal state might not always be reliable, and standard approaches solely rely on the context risk of incurring high regret. Similarly, some users might exhibit weaker correlations between subsequent states, leading to approaches that solely rely on state transitions risking the same. We analyze our setting and algorithm in terms of regret lower bound and upper bounds and evaluate our method on simulated medication adherence intervention data and several real-world data sets, showing improved empirical performance compared to several popular algorithms.},
  archive      = {J_DMKD},
  author       = {Galozy, Alexander and Nowaczyk, Sławomir and Ohlsson, Mattias},
  doi          = {10.1007/s10618-024-01082-3},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-44},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A new bandit setting balancing information from state evolution and corrupted context},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thompson sampling-based recursive block elimination for dynamic assignment under limited budget in pure-exploration. <em>DMKD</em>, <em>39</em>(1), 1-32. (<a href='https://doi.org/10.1007/s10618-024-01083-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate Thompson sampling-based sequential block elimination approaches for dynamic assignment problems in a pure-exploration Multi-Armed Bandit (MAB) setting with limited budget constraints. The problem can be considered as a bandit game-play between the environment and a decision-maker in a metric space. Many instances of problems in fields such as e-commerce, logistics, mobility management, data management and operations research can be framed as dynamic assignment problems with budget constraints. Given an l-dimensional action space representing l variants of an entity and a budget for exploring the action space, the optimal dynamic assignment problem refers to the task of identifying the values to be assigned to different variants of the entity that maximizes the total reward by utilizing at most the given budget of rounds of play. We contribute a class of block elimination-based MAB algorithms specifically designed for the dynamic assignment problem with limited budget. Our algorithms begin by discretizing the continuous action space into a finite set of discrete actions, then proceed with a recursive block elimination procedure to remove sub-optimal actions. The elimination is carried out by calculating confidence bounds over blocks of actions. We explore two different confidence bound estimation techniques. We perform comprehensive experiments on two problem instances from distributed data management and logistics. Our results showcase that our approach yields a lower misidentification probability (i.e., the probability of recommending a non-optimal action) compared to state-of-the-art elimination-based pure-exploration MAB algorithms.},
  archive      = {J_DMKD},
  author       = {Parambath, Shameem Puthiya and Anagnostopoulos, Christos and Alfahad, Saleh Abdullah M.},
  doi          = {10.1007/s10618-024-01083-2},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Thompson sampling-based recursive block elimination for dynamic assignment under limited budget in pure-exploration},
  volume       = {39},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
