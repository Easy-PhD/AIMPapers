<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>KIS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="kis">KIS - 302</h2>
<ul>
<li><details>
<summary>
(2025). Multi-task learning for categorizing road accidents using social media data: A hybrid deep learning framework. <em>KIS</em>, <em>67</em>(10), 9637-9660. (<a href='https://doi.org/10.1007/s10115-025-02517-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road accidents are common and often result in the loss of many lives throughout the world. Analyzing different types of road accidents helps in planning and managing road networks. Social media, like Twitter, is vital for exchanging information globally and is a cost-effective alternative to detecting such accidents. Many challenges remain in the literature, with the primary focus on detecting the category, cause, and severity of the road accident tweet; eventually, this becomes a multi-task learning problem. Also, the language pre-trained model, called Bidirectional Encoder Representations from Transformers (BERT), suffers from out-of-vocabulary (OOV) words, which can be found in tweets for the three downstream tasks, namely category, cause, and severity of road accidents. Motivated by this, in this work, we propose a framework that combines strategies such as multi-task learning, the context-dependent BERT model, and context-free static word embedding called fastText for fine-tuning in detecting three downstream tasks from the road accident tweet. For this, we crawled Twitter to collect the tweets to leverage multi-task learning. Our framework shows that the information across Twitter datasets for related accident tasks can be helpful in understanding task-specific features. We show that our framework outperforms the baselines for the multi-task classification of road accident tweets. The findings of this work may offer crucial direction to road safety organizations, particularly in any given study area, for the proactive deployment of effective remedies so as to ensure vehicle safety.},
  archive      = {J_KIS},
  author       = {Raul, Sanjib Kumar and Rout, Rashmi Ranjan and Somayajulu, D. V. L. N.},
  doi          = {10.1007/s10115-025-02517-z},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9637-9660},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-task learning for categorizing road accidents using social media data: A hybrid deep learning framework},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of sentence similarity detection using machine and deep learning with vectorization techniques. <em>KIS</em>, <em>67</em>(10), 9615-9636. (<a href='https://doi.org/10.1007/s10115-025-02516-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentence similarity detection provides significant advantages across different applications, such as customer support applications, e-commerce customer service, educational platforms, community opportunities and question-answering systems. This study presents a comparative analysis of various machine learning and deep learning models for sentence similarity detection, including cosine similarity, adaptive boosting (AdaBoost), Extreme Gradient Boosting (XGBoost), Convolutional Neural Network with Long Short-Term Memory, and Bidirectional Encoder Representations from Transformers with Long Short-Term Memory. This research also evaluates the impact of various vectorization techniques, such as Term Frequency-Inverse Document Frequency, OpenAI embeddings and Topic Modeling, on the performance of these models. The proposed research validates the effectiveness of these approaches in enhancing the accuracy of similarity detection. The findings offer valuable insights into an optimal combination of vectorization methods and models for improved sentence similarity detection.},
  archive      = {J_KIS},
  author       = {Asalkar, Gayatri Girish and Lal, Bechoo and Korade, Nilesh B.},
  doi          = {10.1007/s10115-025-02516-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9615-9636},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Comparative analysis of sentence similarity detection using machine and deep learning with vectorization techniques},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing molecular property prediction by fusing graph and sequence encoder representations. <em>KIS</em>, <em>67</em>(10), 9587-9614. (<a href='https://doi.org/10.1007/s10115-025-02514-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug discovery through wet lab experimentation is a time-consuming and expensive process. The high cost of drug development is largely due to research and clinical testing of potential drug candidates that may not achieve regulatory approval. Virtual screening methods offer a computational alternative, but existing methods often rely solely on molecular sequences or graph structures, limiting predictive accuracy. This work introduces a novel fusion learning framework that integrates both molecular graph structure information and molecular sequence contextual information of molecules to enhance molecular representation learning. This is done by combining the graph and sequence embeddings to generate a fused representation that improves generalization. Extensive experiments on benchmark datasets demonstrate that our method significantly outperforms graph-based, sequence-based, and prior fusion approaches. The complete implementation of this work is available at https://github.com/vyshakhgnair/TraGT .},
  archive      = {J_KIS},
  author       = {Pillai, Sonika Rajesh and Nair, Madhurya R. and Nair, Vyshakh G. and Mohan, Anuraj},
  doi          = {10.1007/s10115-025-02514-2},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9587-9614},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing molecular property prediction by fusing graph and sequence encoder representations},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized intrusion detection and secure data management in IoT networks using GAO-xgboost and ECC-integrated blockchain framework. <em>KIS</em>, <em>67</em>(10), 9531-9586. (<a href='https://doi.org/10.1007/s10115-025-02513-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of the Internet of Things (IoT) has introduced a wide array of cybersecurity challenges, particularly due to the heterogeneity and resource-constrained nature of IoT devices. Centralized data storage systems, commonly employed in traditional IoT architectures, are increasingly prone to single points of failure, data breaches, and unauthorized access. Despite advancements in blockchain and machine learning, existing solutions often lack scalability, efficient threat detection, and adaptability to diverse IoT environments. To bridge this gap, we propose a novel framework that integrates a Genetic Algorithm-Optimized XGBoost (GAO-XGBoost) model with an Elliptic Curve Cryptography (ECC)-enabled blockchain architecture. In this framework, ECC ensures lightweight yet robust data encryption, while blockchain facilitates secure, immutable, and decentralized storage. The GAO-XGBoost model leverages genetic algorithms for effective feature selection, significantly improving intrusion detection performance in real-time IoT traffic scenarios. Experimental evaluation on a benchmark dataset demonstrates that the proposed system achieves 98% accuracy, a 97% true positive rate (TPR), and 97.4% recall, outperforming existing methods. This framework effectively mitigates advanced cyber threats, offering a secure, scalable, and intelligent IDS tailored for modern IoT and Industrial IoT (IIoT) networks.},
  archive      = {J_KIS},
  author       = {Nandanwar, Himanshu and Katarya, Rahul},
  doi          = {10.1007/s10115-025-02513-3},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9531-9586},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Optimized intrusion detection and secure data management in IoT networks using GAO-xgboost and ECC-integrated blockchain framework},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vector knowledge transfer-driven representation learning for heterogeneous hypernetworks. <em>KIS</em>, <em>67</em>(10), 9501-9529. (<a href='https://doi.org/10.1007/s10115-025-02512-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike traditional networks, the hypernetworks have complex higher-order tuple relationships, i.e. hyperedges. However, most existing hypernetwork representation learning methods fail to adequately capture these complex higher-order tuple relationships. To address this issue, a vector knowledge transfer-driven representation learning method for heterogeneous hypernetworks abbreviated as VTRL is proposed. Firstly, a hyperedge-aware random walk algorithm with node importance preservation is proposed, which assigns greater random walk probabilities to more important nodes without decomposing the hyperedges. Secondly, based on hyperedge-aware random walk algorithm with node importance preservation, a hyperedge-aware topological structure model is proposed to learn pre-trained vectors. Finally, inspired by transfer learning, the pre-trained vectors are transfered to knowledge-enhanced similarity model to the final node representation vectors. The experiments on four real-world hypernetwork datasets demonstrate that as for link prediction tasks, our proposed method outperforms almost all baseline methods. As for hypernetwork reconstruction tasks, on the drug dataset, our proposed method outperforms all baseline methods when the reconstruction ratio is less than 0.8, meanwhile, on the GPS dataset, our proposed method shows competitive performance compared to the best baseline method HPHG.},
  archive      = {J_KIS},
  author       = {Chen, Yijian and Zhu, Yu and Wang, Xiaoying and Huang, Jianqiang and Cao, Tengfei and Wang, Wei},
  doi          = {10.1007/s10115-025-02512-4},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9501-9529},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Vector knowledge transfer-driven representation learning for heterogeneous hypernetworks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based semi-supervised active learning for multi-class tweet classification. <em>KIS</em>, <em>67</em>(10), 9467-9499. (<a href='https://doi.org/10.1007/s10115-025-02511-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a transformer-based semi-supervised active learning framework for tweet categorization. We focus on distinguishing information, emotion, neutral, and irrelevant tweets in case of a crisis. For this work, we considered the event “Fall of Kabul". We introduce a frequency-based sampling algorithm that deals with the temporal variations in tweet frequency for selecting tweets for initial manual annotation. A pre-trained BERT model is utilized for the classification of labelled data. As part of our iterative active learning process, we select representative tweets from the unlabelled pool. We manually annotate tweets with low-confidence scores and assign pseudo-labels to others. These labelled tweets are used to retrain the model over successive iterations. By using active learning, our approach makes full use of both supervised and unsupervised learning with reduced labelling costs while maintaining satisfactory model performance. We present the class-wise performance of the model and show that our model is able to achieve 0.806 accuracy and is effective in identifying the four classes successfully for the crisis with satisfactory F1-scores.},
  archive      = {J_KIS},
  author       = {Khandelwal, Parul and Singh, Preety and Kaur, Rajbir and Chakraborty, Roshni},
  doi          = {10.1007/s10115-025-02511-5},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9467-9499},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Attention-based semi-supervised active learning for multi-class tweet classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aspect term extraction and snake driving training-based optimization-enabled deep learning for facebook sentiment analysis. <em>KIS</em>, <em>67</em>(10), 9437-9466. (<a href='https://doi.org/10.1007/s10115-025-02510-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this modern world, social media plays a vital role in communicating with each other across the globe. Sentiment analysis plays an essential role on social media platforms in tracking online conversations among themselves and with competitors in real time. They also gain measurable insights about how positively or negatively they are viewed by the users. In this research, a Facebook sentiment analysis approach named snake driving training-based optimization-enabled random multimodal deep learning (SDTBO_RMDL) is developed. Initially, the acquisition of Facebook review from the BuzzFeed News dataset is done. Then, to break down the sentence into tokens, BERT tokenization is carried out. After that, the aspect terms in the sentences are identified using the ATE. Next, the extraction of relevant features is done on the raw data using feature extraction. Then, sentiment analysis classification is performed using RMDL, where the network parameters are optimally tuned using the proposed SDTBO. Here, the SDTBO is the integration of driving training-based optimization and snake optimizer. Finally, the classified output comes from the RMDL. Thus, the Facebook reviews are categorized as positive as well as negative. The performance of SDTBO_RMDL is of values 0.948, 0.974, and 0.961 for precision, recall, and F1-score, respectively.},
  archive      = {J_KIS},
  author       = {Kotagiri, Srividya and Owk, Mrudula and Priyanka, Tata and Sowjanya, A. Mary},
  doi          = {10.1007/s10115-025-02510-6},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9437-9466},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Aspect term extraction and snake driving training-based optimization-enabled deep learning for facebook sentiment analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local recovery and partial snapshot in distributed stateful stream processing. <em>KIS</em>, <em>67</em>(10), 9407-9435. (<a href='https://doi.org/10.1007/s10115-025-02509-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data stream applications, a query is created using a series of operators connected by unidirectional pipelines. Since those operators may be deployed in a shared-nothing distributed environment, it is necessary to have a valid restore point in case an operator fails. This ensures consistency of the states among operators after recovering from failures. Typical fault-tolerant approaches periodically capture global snapshots, encompassing all operator states. In the recovery process, all operators must be reinstated to the most recent global snapshot. This global dependency affects the recovery performance and the required computational costs for recovery. We propose fault-tolerant schemes for stateful stream processing that localizes recovery to a subset of operators. Additionally, we introduce a partial snapshot mechanism to capture the states required for local recovery efficiently. This snapshot records only a subset of operator states instead of global states. Our implemented recovery schemes in Apache Flink show improved recovery times by up to more than 50 percent.},
  archive      = {J_KIS},
  author       = {Takdir and Kitagawa, Hiroyuki and Amagasa, Toshiyuki},
  doi          = {10.1007/s10115-025-02509-z},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9407-9435},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Local recovery and partial snapshot in distributed stateful stream processing},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eliminating false positive results to effectively analyze anomaly changes in violent videos. <em>KIS</em>, <em>67</em>(10), 9385-9406. (<a href='https://doi.org/10.1007/s10115-025-02508-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasing incidents of crime and violence, it is important to develop technology to automatically detect the presence of violence in security camera images. Although law enforcement agencies have sufficient images, they do not have the human resources to analyze them and detect violence in a timely manner. In these video footage, there are examples of false recognitions that are labeled as normal in some frames while the abnormal event continues. In this study, we aim to identify the start and end frames of the event with minimum error in indoor or outdoor violent camera images. For this purpose, firstly, a model is created to enrich the sequential video frames containing violence by using MixUp data augmentation method for limited training datasets, so that the system can learn more features and thus increase the training performance. Secondly, with another proposed method, more effective video analysis is realized by filtering the frames containing false positives in the outputs obtained from a deep learning-based system. Experimental results show that the proposed method achieves a remarkable success rate, reaching 97,6% F-1 score and 95,4% IoU score values. In this way, false positives are significantly reduced, and the start, end and action times of violent events that continue for more than one second in consecutive frames can be accurately detected.},
  archive      = {J_KIS},
  author       = {Kutlugün, Esra and Çetin, Ömer},
  doi          = {10.1007/s10115-025-02508-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9385-9406},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Eliminating false positive results to effectively analyze anomaly changes in violent videos},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic resource orchestration in edge computing environments using multi-agent reinforcement learning. <em>KIS</em>, <em>67</em>(10), 9363-9383. (<a href='https://doi.org/10.1007/s10115-025-02507-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a dynamic resource orchestration framework for edge computing environments, utilizing multi-agent reinforcement learning (MARL) to enhance resource allocation and task scheduling. The proposed system consists of edge nodes (E), a centralized resource manager (CRM), and communication infrastructure (CI). Edge nodes execute computational tasks at the network’s periphery, while the CRM oversees resource distribution and task assignment using a global system perspective. The CI supports efficient communication among these components. The MARL framework enables collaborative learning among agents, where each agent selects optimal actions—such as resource allocation, task scheduling, and migration—based on system states that include resource availability, task queue lengths, network conditions, and task priorities. A deep Q-network (DQN)-based training approach is employed, allowing agents to maximize cumulative rewards by balancing task completion efficiency, resource utilization, and latency minimization. The proposed framework is evaluated through comprehensive simulations against traditional heuristic-based and static resource allocation methods. Results demonstrate that our MARL-based approach reduces average task completion latency by 12.3% and improves resource utilization by 8.7% compared to heuristic baselines. Additionally, the framework dynamically adapts to variations in network conditions and workload distribution, ensuring consistent quality of service (QoS) under dynamic edge computing scenarios.},
  archive      = {J_KIS},
  author       = {Liu, Qi and Yang, Jianzheng and Yan, Zhixian},
  doi          = {10.1007/s10115-025-02507-1},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9363-9383},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamic resource orchestration in edge computing environments using multi-agent reinforcement learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel enhanced bayesian classifier with multiple smoothing parameters. <em>KIS</em>, <em>67</em>(10), 9337-9362. (<a href='https://doi.org/10.1007/s10115-025-02506-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The naïve Bayesian classifier (NBC) is based on an assumption that all attributes are independent with each other, and therefore, it is designed according to the estimation of marginal probability density function (PDF). However, this assumption leads to inability of NBC when conditional dependency information exists among attributes. To address this problem, this paper proposes a enhanced Bayesian classifier (EBC) for continuous attributes. To effectively use dependent information among attributes, a multivariate Gaussian kernel function is applied to estimate joint PDF of these attributes using multiple smoothing parameters. Based on this, there are two types of EBC: one uses a single smoothing parameter for the entire attributes, and the other utilizes reinforcement learning method to find smoothing parameter for each attribute. Extensive experiments are conducted to evaluate the performance of the proposed methods on several University of California, Irvine datasets and Tennessee–Eastman Process dataset with continuous attributes. The superior performance shows the effectiveness of EBC and indicates its wide potential applications in data mining.},
  archive      = {J_KIS},
  author       = {Zhao, Yanli and Yang, Guang and Wei, HuiHui},
  doi          = {10.1007/s10115-025-02506-2},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9337-9362},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel enhanced bayesian classifier with multiple smoothing parameters},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-oriented and attractor regularized multi-task learning for environmental spatial–temporal time series prediction. <em>KIS</em>, <em>67</em>(10), 9307-9336. (<a href='https://doi.org/10.1007/s10115-025-02505-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of sensor networks, the spatial–temporal relationships of multivariate time series also bring the challenge for multi-task spatial–temporal time series prediction in environmental datasets. In this paper, we propose the task-oriented and attractor regularized multi-task learning model (TOAR-MTL). In the model, random feature and Nyström approximation are adopted for hierarchical dynamics. Joint sparsity constraints are utilized for task-oriented feature selection, which could explore the dynamic-shared and dynamic-specific features. Simultaneously, the manifold regularization is adopted for attractor priori to constrain the predicted value back onto the attractor manifold. Furthermore, an efficient iterative algorithm based on the alternative direction multiplier method and accelerated proximal gradient method is proposed and a decoupling variable is introduced to handle the complex objective function. Simulations are conducted on the London, Beijing–Tianjin–Hebei, Lorenz and Beijing Air Quality datasets. The experiment results illustrate that the Nyström approximation could speed up the training process by 30–100 times and the task-oriented feature selection could adaptively obtain sparse interpretable weights. Therefore, the TOAR-MTL could effectively promote the multi-step prediction accuracy of environmental spatial–temporal time series.},
  archive      = {J_KIS},
  author       = {Lv, Fei and Si, Shuaizong and Ren, Weijie},
  doi          = {10.1007/s10115-025-02505-3},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9307-9336},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Task-oriented and attractor regularized multi-task learning for environmental spatial–temporal time series prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QUERY2RULE: Automatically generating logical query and reasoning from complex questions over large knowledge graph embedding. <em>KIS</em>, <em>67</em>(10), 9277-9305. (<a href='https://doi.org/10.1007/s10115-025-02504-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning to answer complex questions is a difficult task and is receiving research attention in natural language processing today. The purpose of this study is to propose a model, QUERY2RULE, that automatically generates logic rules representing the reasoning process and learn how to find answer entities from logical reasoning rules in the graph embedding space. Specifically, for generating automatic logical reasoning rules, we improved the REBEL model by combining LSTM to create automatic logical reasoning rules for each complex question. To learn how to find answers to logic rules, we used the node2vec algorithm to convert the knowledge graph to the embedding space. We added descriptive information about each entity to the graph for more semantics. Then, the K-D tree technique indexed embedded entities for fast retrieval. Finally, we combined the question embedding and the logical reasoning rule embedding and gave them input to the decoder layer to rank the highest candidate entities as the answer. We tested the proposed model on benchmark datasets such as FreeBaseQA, WebQSP, CWQ, and FB15k-237. Experimental results showed that the QUERY2RULE model achieved better than previous models when comparing classification, link prediction, and multi-hop reasoning tasks.},
  archive      = {J_KIS},
  author       = {Phan, Truong H. V. and Do, Phuc},
  doi          = {10.1007/s10115-025-02504-4},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9277-9305},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {QUERY2RULE: Automatically generating logical query and reasoning from complex questions over large knowledge graph embedding},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active learning-based multi-armed bandits for recommendation systems. <em>KIS</em>, <em>67</em>(10), 9253-9275. (<a href='https://doi.org/10.1007/s10115-025-02502-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional recommender systems often struggle with challenges such as the cold start problem, data sparsity, and adaptation to changing user preferences. This paper explores the integration of reinforcement learning (RL), specifically multi-armed bandits (MAB), with active learning strategies to address these limitations. MAB-based models dynamically balance exploration and exploitation, allowing continuous learning and enabling the system to adapt to changing user preferences over time. Additionally, active learning enhances personalization by selectively acquiring user feedback, further improving recommendation accuracy. To evaluate the effectiveness of this approach, a comprehensive comparative analysis of various MAB policies and active learning strategies is conducted using the MovieLens ML1M dataset. The results highlight the trade-offs between different strategies, offering valuable insights into various configurations for optimizing recommendation performance. While the integration of MAB and active learning shows significant potential in mitigating data sparsity and cold start issues, the findings highlight the need for further refinement to enhance real-world applicability. This paper contributes to the advancement of reinforcement learning-based recommender systems by demonstrating how continuous learning and active learning mechanisms enhance recommendation efficiency, personalization, and adaptability, highlighting the importance of ongoing innovation in scalable recommendation frameworks.},
  archive      = {J_KIS},
  author       = {Asri, Bachir and Qassimi, Sara and Rakrak, Said},
  doi          = {10.1007/s10115-025-02502-6},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9253-9275},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Active learning-based multi-armed bandits for recommendation systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable prediction for business process activity with transformer neural networks. <em>KIS</em>, <em>67</em>(10), 9221-9252. (<a href='https://doi.org/10.1007/s10115-025-02501-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business process prediction employs event logs to train models for forecasting process states. While deep learning enhances prediction performance, most prediction models remain black boxes with limited ability to explain why a certain business process prediction was made. The lack of model interpretability undermines prediction reliability and reduces decision-makers’ adoption rates. In this paper, we propose an interpretable Transformer-based process prediction model that delivers predictions with explanations, supported by quantitative and qualitative evaluation of interpretation reliability. We analyze how events and attributes independently influence subsequent activity predictions and how individual event and attribute influence mutually to explain model decisions. Experimental results show our explainable prediction model enhances interpretation reliability with high faithfulness and trustworthiness.},
  archive      = {J_KIS},
  author       = {Wu, Budan and Hong, Shiyi and Lin, Rongheng},
  doi          = {10.1007/s10115-025-02501-7},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9221-9252},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Explainable prediction for business process activity with transformer neural networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt templates for argument relation classification using frame semantic parsing. <em>KIS</em>, <em>67</em>(10), 9189-9219. (<a href='https://doi.org/10.1007/s10115-025-02500-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argument relation classification (ARC) between argument components (ACs) has made significant progress in recent years. However, many existing approaches either rely heavily on external knowledge or on linguistic information encoded in Pre-trained Language Models (PLMs) or large language models, often neglecting the extraction of fine-grained, semantic information within ACs. This information is essential for developing strategies tailored to the specific challenges of ARC tasks. To address this, we propose leveraging Frame Semantic Parsing (FSP), an open-source transformer, to extract semantic frames. These frames, consisting of triggers and arguments along with their roles, represent the semantic relationships within ACs. We then design two types of prompt templates: one for triggers and arguments, and another for frames and roles, to generate conceptual information that facilitates ARC. Finally, we utilize the RoBERTa PLM model, training it with the two types of prompt templates using a Siamese network architecture, which encodes two inputs separately, with multi-head attention. Extensive experiments across six domain-specific argument mining datasets demonstrate that our FSP–ARC approach yields competitive results compared to four state-of-the-art baselines in terms of accuracy, precision, recall, and macro Macro F1s.},
  archive      = {J_KIS},
  author       = {Moslemnejad, Somaye and Reed, Chris},
  doi          = {10.1007/s10115-025-02500-8},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9189-9219},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Prompt templates for argument relation classification using frame semantic parsing},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing exploration and exploitation in moth-flame optimization for global optimization and feature selection. <em>KIS</em>, <em>67</em>(10), 9147-9188. (<a href='https://doi.org/10.1007/s10115-025-02498-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moth-Flame Optimization (MFO) is a population-based algorithm with a simple design and few control parameters, making it suitable for optimization tasks. However, its susceptibility to being trapped in local optima limits its effectiveness. To address this limitation and enhance the exploration–exploitation trade-off in MFO, this study introduces two novel strategies: random follower and dispersed foraging. These innovations aim to significantly improve the global optimization capabilities of the original MFO algorithm. We conducted a comprehensive evaluation using 29 benchmark functions from the IEEE CEC 2017 competition, demonstrating that the proposed DRMFO outperforms seven enhanced MFO variants and eleven state-of-the-art optimization algorithms. Statistical tests, including the Wilcoxon signed-rank test and Friedman test, confirm the effectiveness of DRMFO in global optimization. Additionally, experiments focused on balance and diversity confirm that DRMFO significantly enhances the algorithm’s exploration–exploitation trade-off. Furthermore, a binary version of DRMFO was developed using a V-shaped transfer function for feature selection, and its performance was compared with eight binary classification algorithms across 36 public datasets. The results highlight DRMFO’s strong performance in both global optimization and feature selection, demonstrating its potential as a robust optimization method.},
  archive      = {J_KIS},
  author       = {Zhou, Xinsen and Heidari, Ali Asghar and Chen, Yi and Chen, Huiling and Yu, Sudan},
  doi          = {10.1007/s10115-025-02498-z},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9147-9188},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Balancing exploration and exploitation in moth-flame optimization for global optimization and feature selection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid ontology-based feature selection framework for enhancing predictive accuracy in regression models. <em>KIS</em>, <em>67</em>(10), 9111-9145. (<a href='https://doi.org/10.1007/s10115-025-02497-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting firefighter interventions is challenging due to the high dimensionality and complexity of the data. This study introduces a hybrid feature selection framework that combines ontology-based reasoning with machine learning (ML) techniques to improve predictive accuracy and model interpretability. Three ML algorithms—XGBoost, LightGBM, and LSTM—were applied using two feature selection strategies: a traditional ML-based approach and a hybrid method integrating ontology-driven centrality metrics (degree, closeness, betweenness). A domain-specific ontology was developed to capture key factors like environmental and temporal variables, enhancing feature selection for more relevant and interpretable inputs. The hybrid approach consistently outperformed the ML-only method across all models, achieving $$R^2$$ scores of 0.976 (XGBoost), 0.964 (LSTM), and 0.975 (LightGBM), compared to slightly lower scores for ML-only approaches. These findings demonstrate the potential of combining ontology-based reasoning with ML to address high-dimensional data challenges in predictive tasks.},
  archive      = {J_KIS},
  author       = {Ayad, Sarah and Mallouhy, Roxane Elias and Guyeux, Christophe},
  doi          = {10.1007/s10115-025-02497-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9111-9145},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hybrid ontology-based feature selection framework for enhancing predictive accuracy in regression models},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A transformer-based deep reinforcement learning for the dial-a-ride problem. <em>KIS</em>, <em>67</em>(10), 9085-9109. (<a href='https://doi.org/10.1007/s10115-025-02493-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dial-a-Ride problem (DARP) is a specialized variant of the vehicle routing problem that focuses on designing optimal routes for users who specify pick-up and delivery requests between origins and destinations. This problem involves determining the most effective and efficient routes using limited vehicles and resources to offer responsive door-to-door transportation services for users with specific needs. Logistics service providers require systems that can deliver optimal solutions to these combinatorial problems within a reasonable time. Recently, there has been a significant increase in the use of artificial intelligence optimization algorithms such as meta-heuristics or learning-based approaches to solve such problems. Among the learning-based approaches, reinforcement learning has gained prominence for routing and scheduling tasks, owing to its ability to adaptively learn from complex state spaces and dynamically changing environments. In this study, a novel transformer-based deep reinforcement learning method is proposed to solve the Dial and Ride problem for a single service vehicle. In the proposed model, we adopt a modified transformer architecture instead of employing traditional linear layers, we integrate convolutional layers. To validate our approach, we conduct comprehensive experiments comparing our method against four well-known metaheuristic algorithms and a Deep Q-Network algorithm. The results indicate that proposed approach outperforms these techniques in terms of shorter total travel distances. Additionally, the proposed method is tested on a real-world scenario generated in the Buyukdere neighborhood of Eskisehir. The results demonstrate that the proposed method makes it possible to solve the problem within a reasonable time. This study confirms that the proposed deep reinforcement learning method can effectively address the Dial-a-Ride Problems.},
  archive      = {J_KIS},
  author       = {Aslan Yıldız, Özge and Sarıçiçek, İnci and Yazıcı, Ahmet},
  doi          = {10.1007/s10115-025-02493-4},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9085-9109},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A transformer-based deep reinforcement learning for the dial-a-ride problem},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technology feature fusion prediction method based on multidimensional information extraction and deep neural networks. <em>KIS</em>, <em>67</em>(10), 9051-9084. (<a href='https://doi.org/10.1007/s10115-025-02496-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early and accurate predictions of technology fusion can help explore technological innovation opportunities and play a significant role in promoting industrial change and development. However, technology fusion prediction suffers from insufficient data mining and poor prediction accuracy. To improve the accuracy of technology fusion prediction, this paper proposes a method based on patent multidimensional information extraction and deep neural networks. The proposed method first constructs an international patent classification (IPC) co-occurrence network based on existing technology fusion and then transforms the technology fusion prediction problem into a patent IPC co-occurrence network link prediction problem. Second, a fusion knowledge graph based on patent data is constructed and the technical characteristic information in the patent data, semantic information in the patent text, and link information in the IPC co-occurrence network are deeply mined. Third, deep neural networks are used to learn the features of historical data and train the technology fusion prediction model to achieve fine-grained and accurate predictions of future industrial technology fusion. An experimental study was conducted by evaluating the artificial intelligence and green technology of the high-speed rail industry, and the results demonstrate that all the indices of the proposed method are better than those of other comparative models, allowing us to realize the accurate prediction of the fusion trend of artificial intelligence and green technology in the high-speed rail industry. This study also has practical significance for the prediction of technological fusion and innovation in other industries.},
  archive      = {J_KIS},
  author       = {Zhang, Hong and Chen, Yanchun},
  doi          = {10.1007/s10115-025-02496-1},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9051-9084},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Technology feature fusion prediction method based on multidimensional information extraction and deep neural networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LG-VGAE: A local and global collaborative variational graph autoencoder for detecting crypto money laundering. <em>KIS</em>, <em>67</em>(10), 9027-9050. (<a href='https://doi.org/10.1007/s10115-025-02494-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have emerged as a promising method for anti-money laundering (AML) applications. However, existing semi-supervised GNN models often collaborate with downstream classifiers in a monotonous manner and tend to focus solely on local or global information. To address these limitations, we propose an unsupervised graph representation learning model called local and global collaborative variational graph autoencoder (LG-VGAE) for detecting cryptocurrency money laundering. Our model seamlessly integrates GNNs that concentrate on both local information and global structure into a unified framework. In experiments using the Elliptic dataset, the representation learned by LG-VGAE improves the precision, recall, and F1 score of the random forest (RF) classifier by 3.7%, 7.0%, and 5.7%, respectively, achieving state-of-the-art results. The experimental results show that the proposed LG-VGAE can better capture financial anomalies and enhance the performance of downstream classifiers. The construction of this graph representation learning model provides a new way of introducing GNNs into AML. Our code and data are released at https://github.com/AnguoCYF/LG-VGAE .},
  archive      = {J_KIS},
  author       = {Chen, Yifan and Chen, Zhiyuan and Amin, Hafeez Ullah},
  doi          = {10.1007/s10115-025-02494-3},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9027-9050},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {LG-VGAE: A local and global collaborative variational graph autoencoder for detecting crypto money laundering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic PScore: A dynamic method to prioritize user reviews. <em>KIS</em>, <em>67</em>(10), 9007-9025. (<a href='https://doi.org/10.1007/s10115-025-02492-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, with the emergence of mobile apps, user reviews have become of great importance for app developers because they contain user sentiments, bugs, and new requests. Due to the large number of reviews, it is a difficult, time-consuming, and error-prone task to prioritize reviews manually. A tool or method for automated prioritization of reviews could reduce development time and maintenance cycles. Various methods have been presented for prioritizing reviews; most have focused on old features that are no longer valid or ignored new features provided by the store. This study provides a method, called Dynamic PScore, for dynamic prioritizing reviews into five categories of hot, serious, pay attention, getting traction, and not serious in the Google Play Store. In this method, the score is calculated using ThumbsUp features (popularity of each review), extracting sentiment and considering the number of words in the review, and then, prioritization is performed using the obtained score. To evaluate the proposed method experimentally in the PPrior database, the corresponding score was calculated for each review, and prioritization was performed. Finally, the results demonstrate that the proposed method has an accuracy and PSP of 99.94% and 99.86%, respectively, in prioritizing and scoring reviews, and its prioritization accuracy has improved by 13.34% compared to recent studies.},
  archive      = {J_KIS},
  author       = {Dehkordi, Mehrdad Razavi and Rastegari, Hamid and Najafabadi, Akbar Nabiolahi and Gandomani, Taghi Javdani},
  doi          = {10.1007/s10115-025-02492-5},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {9007-9025},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamic PScore: A dynamic method to prioritize user reviews},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RRBoost: A new ensemble method for classifying imbalanced data. <em>KIS</em>, <em>67</em>(10), 8983-9005. (<a href='https://doi.org/10.1007/s10115-025-02490-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a common issue in classification tasks, often causing standard classification models to misclassify instances from minority classes. Recent efforts to address this problem frequently involve the combination of sampling techniques with ensemble models. Extending this trend, we propose a new method called R-ROSE Boosting (RRBoost). This method involves the development of a novel synthetic data generation technique, termed radius-random oversampling examples (R-ROSE), and its integration with a boosting-based ensemble method. This approach offers the advantage of enhancing the diversity of synthetic data in the vicinity of hard-to-classify observations, thereby potentially improving classification accuracy. We demonstrate the effectiveness of RRBoost by comparing it with other ensemble models using 24 real imbalanced datasets. As a result, RRBoost proves to be an effective method for addressing imbalanced data, demonstrating superior classification performance compared to other ensemble models.},
  archive      = {J_KIS},
  author       = {Park, Hyejoon and Kim, Hyunjoong},
  doi          = {10.1007/s10115-025-02490-7},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8983-9005},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {RRBoost: A new ensemble method for classifying imbalanced data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining temporal attack patterns from cyberthreat intelligence reports. <em>KIS</em>, <em>67</em>(10), 8941-8981. (<a href='https://doi.org/10.1007/s10115-025-02491-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyberthreat intelligence (CTI) reports on past cyberattacks describe the sequence of actions of attackers in terms of time. The sequence contains temporal relations among attack actions, such as a malware is first downloaded and then executed. Information related to temporal relations enables cybersecurity practitioners to investigate past cyberattack incidents and analyze attackers’ behavior. However, cybersecurity practitioners must extract such information automatically, in a structured manner, through a common vocabulary to reduce human effort and enable sharing, and collaboration. The goal of this paper is to aid security practitioners in proactive defense against attacks by automatic information extraction of temporal relations among attack actions from cyberthreat intelligence reports. We propose ChronoCTI, an automated pipeline for extracting temporal relations among attack actions from CTI reports. The attack actions are represented as MITRE ATT&CK techniques, and the relations are represented as a knowledge graph. To construct ChronoCTI, we build a ground truth dataset of temporal relations and apply large language models, natural language processing, and machine learning techniques. ChronoCTI demonstrates higher precision but lower recall performance on a real-world dataset of 94 CTI reports. We apply ChronoCTI on a set of 713 CTI reports, where we identify 9 categories of temporal attack patterns consisting of 124 temporal attack patterns. We identify that the most prevalent pattern category is to trick victim users into executing malicious code to initiate the attack, followed by bypassing the anti-malware system in the victim software systems. Based on the observed patterns, we advocate for training users about cybersecurity best practices, introducing appropriate warning messages for end-users, introducing immutable operating systems, and enforcing multi-user authentications. Moreover, we advocate that practitioners leverage the automated mining capability of ChronoCTI and design countermeasures against recurring attack patterns.},
  archive      = {J_KIS},
  author       = {Rahman, Md Rayhanur and Wroblewski, Brandon and Matthews, Quinn and Morgan, Brantley and Menzies, Timothy and Williams, Laurie},
  doi          = {10.1007/s10115-025-02491-6},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8941-8981},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Mining temporal attack patterns from cyberthreat intelligence reports},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted APPR with restart for community extraction from multilayer networks. <em>KIS</em>, <em>67</em>(10), 8919-8939. (<a href='https://doi.org/10.1007/s10115-025-02488-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community extraction is widely used for various applications concerning network analysis, but its scope has mostly been confined to single-layer network. However, the real-world data are frequently represented as multilayer network, and furthermore, each layer frequently exhibits different properties. It is thus critical to be able to control to what degree each layer should be reflected in the overall network, in order to perform a reliable community extraction. In this paper, we propose a superimposition method called WAPPRS (weighted APPR with restart) that can monotonically increase the degree of reflection and handle a wide range of values for reflection. Our experiments demonstrate that our proposed model outperforms previous models when dealing with the properties desirable for reliable community extraction from multilayer networks.},
  archive      = {J_KIS},
  author       = {Nakano, Shuhei and Yamashita, Tsuyoshi and Shin, Andrew and Matsumoto, Naoki and Kaneko, Kunitake},
  doi          = {10.1007/s10115-025-02488-1},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8919-8939},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Weighted APPR with restart for community extraction from multilayer networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated learning with adaptive optimization of local model. <em>KIS</em>, <em>67</em>(10), 8901-8918. (<a href='https://doi.org/10.1007/s10115-025-02486-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) collaboratively generates a single global model from distributed clients without compromising data privacy. However, data heterogeneity has a significant impact on model training in federated learning. Moreover, in the presence of heterogeneity, the global model exhibits poor generalization across clients, often containing information that individual clients do not expect. To address this issue, we propose a personalized federated learning algorithm called personalized federated learning with adaptive optimization of local model (FedAom), which improves the local models on clients that participate in local training. This is achieved by retaining the locally trained models obtained in each communication round to form personalized models, and comparing them with the new global model passed from the server in the next round of training, a more suitable initial model for local data training is generated for the client, thereby improving the effectiveness of local training. To evaluate the effectiveness of the algorithm, we conducted extensive experiments on three benchmark datasets, demonstrating that FedAom achieves a testing accuracy improvement of approximately 3.16% over baseline algorithms. Furthermore, we applied the FedAom algorithm to other federated learning algorithms, resulting in a testing accuracy improvement of around 3.03%.},
  archive      = {J_KIS},
  author       = {Gao, Yingying and He, Xin and Song, Yajie and Shi, Haobin and Chen, Yan},
  doi          = {10.1007/s10115-025-02486-3},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8901-8918},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Personalized federated learning with adaptive optimization of local model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate data imputation in healthcare with optimized class thresholds using enhanced firefly algorithm. <em>KIS</em>, <em>67</em>(10), 8869-8900. (<a href='https://doi.org/10.1007/s10115-025-02489-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data imputation is a technique for predicting suitable data for missing value healthcare datasets that contain patient and healthcare practitioner information. As medical data’s volume and complexity increase, precise data prediction is essential for informed decision-making and comprehensive analysis. This work introduces a novel method for pre-processing incomplete data, Class Center Mean Value Imputation, using the Enhanced Firefly Algorithm with Courtship Learning (CCMVI-EFA-CL). This method is distinguished by its precise estimations for Missing Completely At Random (MCAR) missingness. Using the Minkowski distance, our proposed method calculates the threshold for accurate data imputation for each class. This threshold is then optimized using an enhanced firefly algorithm with courtship learning. The method combines class center mean value imputation with an enhanced firefly algorithm and courtship learning within an iterative learning framework, ensuring accuracy and efficiency in handling medical datasets. Our proposed method has shown significant improvements in Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (R $$^2$$ ) and Mean Absolute Error (MAE). Experimental results from five benchmark healthcare datasets underscore the superior accuracy and robustness of the proposed method. By outperforming established methods such as Class Center Mean Value Imputation (CCMVI), Single Imputation Chained Equations (SICE), Multivariate Imputation by Chained Equations (MICE) and Class Center-based Firefly Algorithm (C3FA) in terms of accuracy, CCMVI-EFA-CL emerges as a promising solution for healthcare data management. The use of Minkowski distance for threshold calculation and its optimization using the enhanced firefly algorithm with courtship learning further enhances the accuracy and robustness of this method.},
  archive      = {J_KIS},
  author       = {Nayak, Subhashish and Khilar, P. M.},
  doi          = {10.1007/s10115-025-02489-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8869-8900},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Accurate data imputation in healthcare with optimized class thresholds using enhanced firefly algorithm},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective optimization-based task offloading with OFDM in large-scale industrial internet of things. <em>KIS</em>, <em>67</em>(10), 8843-8867. (<a href='https://doi.org/10.1007/s10115-025-02487-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Industrial Internet of Things (IIoT) has brought about a profound transformation in industries by facilitating intelligent data exchange among interconnected devices. However, optimizing performance, reliability, and efficiency in IIoT systems involves overcoming several obstacles, including energy consumption, latency, and scalability, which are still open issues. To overcome these challenges, we propose an improvised IIoT-Multiaccess Edge Computing model by tackling task offloading while considering load balancing and dependency of tasks in a heterogeneous environment. The proposed approach addresses load balancing and computation offloading in an orthogonal frequency division multiplexing (OFDM) network with multiple IIoT end devices connected to a single base station with an edge server. We have used ordinal multi-objective optimization (OMOO) for load balancing to effectively identify high-quality solutions for optimized queue span and link quality. The optimal offloading decision algorithm (OODA) is employed to minimize long-term energy utilization while reducing average traffic congestion. The proposed model is evaluated by comparing it with various offloading systems based on multiple metrics, such as energy consumption and traffic delay, to understand how well the model performs relative to others. The simulation results show that the proposed model excels in energy savings, scalability, and latency reduction for IIoT-edge systems. It effectively satisfies quality of service (QoS) specifications and enhances energy conversion efficiency.},
  archive      = {J_KIS},
  author       = {Prathap, C. and Ranjani, S. Siva},
  doi          = {10.1007/s10115-025-02487-2},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8843-8867},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-objective optimization-based task offloading with OFDM in large-scale industrial internet of things},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature refinement for cross-domain aspect-based sentiment analysis: A contrastive learning and domain alignment perspective. <em>KIS</em>, <em>67</em>(10), 8817-8842. (<a href='https://doi.org/10.1007/s10115-025-02483-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain aspect-based sentiment analysis (ABSA) aims to transfer knowledge from labeled source domain data to perform fine-grained sentiment analysis tasks in an unlabeled target domain. To address the issues of model performance degradation due to domain discrepancy and the insufficient quality of generated augmented data, this paper proposes a unified feature refinement approach that dynamically integrates contrastive learning and domain alignment strategies for cross-domain ABSA. Firstly, we design a strategy that dynamically combines contrastive learning modules with a domain adversarial neural network (DANN). Contrastive learning extends the distribution of features from different domains in the reproducing kernel Hilbert space (RKHS), enhancing inter-domain separability and intra-domain compactness. Subsequently, adversarial training is employed to narrow the distribution gap between source and target domain features. Additionally, our framework incorporates Kullback–Leibler (KL) divergence as a regularization measure to optimize the model’s adjustment to different domain probability distributions. After training on source domain data, the model assigns pseudo-labels to unlabeled data in the target domain. Based on these pseudo-labels, we apply graph optimal transport (GOT) theory, framing cross-domain alignment as a graph matching problem to further improve domain alignment efficiency and generate enhanced text data for the target domain. Extensive experiments on several public ABSA datasets demonstrate that our model significantly outperforms competing approaches across four cross-domain ABSA sub-tasks, thereby confirming its effectiveness.},
  archive      = {J_KIS},
  author       = {Yu, Rui and Xia, Hongbin and Liu, Yuan},
  doi          = {10.1007/s10115-025-02483-6},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8817-8842},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature refinement for cross-domain aspect-based sentiment analysis: A contrastive learning and domain alignment perspective},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lossless graph summarization method for hierarchical and overlapping relational patterns mining. <em>KIS</em>, <em>67</em>(10), 8787-8815. (<a href='https://doi.org/10.1007/s10115-025-02482-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advances in graph computing, an increasing amount of real-world data is now stored in computers as graphs. However, the time and resource costs associated with directly processing large graphs are escalating. As a result, graph summarization techniques, which provide a concise representation of graphs, have been extensively studied. While current graph summarization methods are effective in reducing processing time and minimizing storage overhead, we observe that the hierarchical and overlapping structures prevalent in graphs are not adequately explored, and the resulting summaries lack clear meaning. In this study, we present a new graph summarization model that uses a linear iterative hierarchical clustering algorithm combined with a lexicon-based structure extraction algorithm to generate relational patterns with clear hierarchical and overlapping meanings. In addition, we use the Minimum Description Length (MDL) principle for summarization encoding to save storage space. Experiments show that our approach achieves superior compression ratios on most datasets compared to existing methods and is able to uncover a larger number of relational patterns.},
  archive      = {J_KIS},
  author       = {Zhao, Danfeng and Ma, Jian and Gao, Feng and He, Qi and Zheng, Xiaoluo},
  doi          = {10.1007/s10115-025-02482-7},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8787-8815},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A lossless graph summarization method for hierarchical and overlapping relational patterns mining},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing large language models: Alleviating knowledge deficiency with external knowledge and semantically aware reasoning (SAR). <em>KIS</em>, <em>67</em>(10), 8767-8785. (<a href='https://doi.org/10.1007/s10115-025-02485-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the study of the human thought process, which is categorized into two systems reflecting the brain’s balancing act between speed and cognition, we propose a dual-process architecture that augments System 1 with additional knowledge akin to System 2 in human cognition. The methodology is demonstrated through FAQ retrieval task, showcasing the potential for human-like cognitive processing. Challenged by current data-driven large language models (LLMs) in reasoning and knowledge depth, this work presents a novel approach to improving conversational understanding. We leverage advanced text analysis to strategically extract key information from FAQs and utilize LLM-generated questions combined with robust semantic similarity metrics to significantly improve the precision of user query matching. The results indicate better semantic understanding and reasoning, offering a promising pathway to advancing LLM capabilities in conversational contexts. The base LLM (SBERT) enhanced with semantic textual similarity using Sentence-BERT (STS-SBERT) achieves a mean Average Precision (mAP) of 0.6165, compared to 0.3600 for SBERT alone. By strategically integrating key sentence extraction during knowledge preparation, generating questions, and applying semantic textual similarity measures, our model achieves a substantial improvement in user query matching precision. However, the activation of semantically aware reasoning (SAR) remains an issue for future research.},
  archive      = {J_KIS},
  author       = {Sornlertlamvanich, Virach},
  doi          = {10.1007/s10115-025-02485-4},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8767-8785},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing large language models: Alleviating knowledge deficiency with external knowledge and semantically aware reasoning (SAR)},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph data augmentation using multi-label mixup. <em>KIS</em>, <em>67</em>(10), 8751-8766. (<a href='https://doi.org/10.1007/s10115-025-02480-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph learning has achieved impressive advances with the development of deep networks for graph data. However, unlike images, texts or voices, graph datasets are usually in small size, causing the failure of deep network training due to over-fitting and limiting the generalization capacity of trained networks. Moreover, collecting large graph datasets is costly with a lot of effort, and sometimes needs expertise, becoming impossible in many scenarios. To address this issue, data augmentation is a widely-used solution to expand the training sets by applying transformations to original data to generate more diverse samples. Unfortunately, this approach is losing its power for graph data because graph labels can be unexpectedly changed under graph transformation operations, adding more noise to the data. In this paper, we propose a novel data augmentation for graphs to handle the lack of training data by 1) casting the original multi-class graph classification into the harder problem of multi-label classification and thus preventing from over-fitting when data samples are too easy for networks to learn, and 2) applying multi-label mixup to diversify the training samples without introducing generated graphs with wrong labels. Experiments on benchmark datasets demonstrate the significant improvement of our proposed method in comparison with other state-of-the art graph augmentation techniques for graph classification problem.},
  archive      = {J_KIS},
  author       = {Pham, Luong and Thanh, Tuyen Ho Thi and Nguyen-Tran, Duy-Minh and Le, Bac},
  doi          = {10.1007/s10115-025-02480-9},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8751-8766},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Graph data augmentation using multi-label mixup},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data stream clustering with concept drift using fractal dimension. <em>KIS</em>, <em>67</em>(10), 8715-8749. (<a href='https://doi.org/10.1007/s10115-025-02484-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of data-driven applications has underlined the need for strong methods to analyze and cluster streaming data. Data stream clustering is envisioned to uncover interesting knowledge concealed within data streams, which are typically fast, structure- and pattern-evolving. However, most of the current methods suffer from serious challenges regarding the inability to detect arbitrarily shaped clusters, handling outliers, adaptation to concept drift, and reduction of dependency on predefined parameters. To address these challenges, we propose a novel fractal-dimension-based stream clustering algorithm that detects concept drift and recurrence. Fractal dimension is a versatile mathematical tool that enhances efficiency in clustering the intricate dynamics of evolving streams. Our methodology incorporates a grid-density-based clustering model augmented by dual concept drift detection mechanisms—distance-based metrics and fractal dimensions—to improve sensitivity to subtle distributional shifts. By dynamically updating clusters through model reuse or creation, our algorithm ensures adaptability to real-time changes in data distributions. The proposed algorithm was comprehensively evaluated using the KDDCup-99, Cover Type, and Electricity and Eye State datasets, under diverse scenarios, including concept drifts, evolving data distributions, varying cluster sizes, and outlier conditions. Empirical results demonstrated the algorithm’s superiority over baseline approaches such as StreamKM + + , DenStream, CluStream, and ClusTree, achieving perfect performance metrics. These findings emphasize the effectiveness of our algorithm in addressing real-world streaming data challenges, combining high sensitivity to concept drift with computational efficiency, adaptability, and robust clustering capabilities.},
  archive      = {J_KIS},
  author       = {Rezaei, Zahra and Sajedi, Hedieh and Hashemi, Morteza},
  doi          = {10.1007/s10115-025-02484-5},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8715-8749},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Data stream clustering with concept drift using fractal dimension},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new privacy-preserving approach for publishing periodical reporting systems data. <em>KIS</em>, <em>67</em>(10), 8673-8714. (<a href='https://doi.org/10.1007/s10115-025-02479-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spontaneous reporting systems (SRSs) are widely used to collect adverse drug events, which are released periodically for detection and analysis of adverse drug reactions (ADRs). SRS data need to be anonymized before being released, because it is closely related to the privacy of individuals. Data publishing methods can protect privacy while guaranteeing information utility of released data. Unfortunately, traditional data publishing methods are unsuitable for SRS data due to its special features. Moreover, prior methods for periodical SRS data publishing are vulnerable in practice. These schemes oversimplify an individual’s trajectory, which is the pattern of appearance of individual information across periods. Consequently, an attacker’s background knowledge is limited to only a few trajectories, resulting in significant underestimation. To explore these privacy issues, we analyze and formalize an adversary’s background knowledge, presenting six new privacy attacks aimed at SRS data in this paper. Relationships between these known attacks are analyzed, and we point out that previous SRS data publishing methods are susceptible to the new attacks. To address this very issue, we develop a new periodical SRS data publishing method by slicing and integrating individual information from different periods. Simultaneously, to preserve the utility of released tables, the records with similar quasi-identifier attribute values and trajectories are incorporated into the same group, and the remaining information is transmitted to the next release window. Theoretic analysis shows that our method can provide better protection than previous methods, and information utility can be guaranteed. The experimental results on real datasets also demonstrate that the new method improves privacy protection significantly and can thwart various known attacks. Interestingly, our method also has a little improvement in information utility compared with previous methods. Our method is suitable for the scenario that makes great demands on privacy protection while providing the guaranteed strength of ADR signals.},
  archive      = {J_KIS},
  author       = {Yi, Tong and Wang, Jinyan and Shang, Wenqian and Zhu, Haibin and Li, Xianxian},
  doi          = {10.1007/s10115-025-02479-2},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8673-8714},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new privacy-preserving approach for publishing periodical reporting systems data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach for dynamic portfolio management integrating K-means clustering, mean-variance optimization, and reinforcement learning. <em>KIS</em>, <em>67</em>(10), 8599-8671. (<a href='https://doi.org/10.1007/s10115-025-02475-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective portfolio management is crucial in today’s fast-moving and unpredictable financial landscape. This paper introduces a powerful and adaptive investment framework that fuses classical portfolio theory with cutting-edge artificial intelligence (AI) to optimize portfolio performance during volatile market conditions. Our methodology seamlessly integrates K-means clustering to identify asset groupings based on correlation structures of technical indicators, mean-variance optimization (MVO) to achieve an ideal risk-return trade-off, and advanced Machine Learning (ML) and reinforcement learning (RL) techniques to dynamically adjust asset allocations and simulate market behavior. The proposed framework is rigorously evaluated on historical stock data from 60 prominent stocks listed on NASDAQ, NYSE, and S&P 500 indices between 2021 and 2024, a period marked by significant economic shocks, global uncertainty, and structural market shifts. Our experimental results show that our framework consistently outperforms traditional strategies and recent state of the art models, achieving superior metrics including Sharpe ratio, Sortino ratio, annual return, maximum drawdown, and Calmar ratio. We also assess the computational efficiency of the approach, ensuring its feasibility for real-world deployment. This work demonstrates the transformative potential of AI-driven portfolio optimization in empowering investors to make smarter, faster, and more resilient financial decisions amid uncertainty.},
  archive      = {J_KIS},
  author       = {Zouaghia, Zakia and Kodia, Zahra and Ben said, Lamjed},
  doi          = {10.1007/s10115-025-02475-6},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8599-8671},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel approach for dynamic portfolio management integrating K-means clustering, mean-variance optimization, and reinforcement learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extending the scope of case-based reasoning through multiple case-based generalizations. <em>KIS</em>, <em>67</em>(10), 8577-8597. (<a href='https://doi.org/10.1007/s10115-025-02472-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case-based reasoning systems (CBRs) are widely acknowledged for their scalability, as they excel in handling large volumes of cases. Model bases can be considered as more encompassing generalizations of case bases. This article provides a novel viewpoint on how knowledge is represented and how problems are solved in CBRs. It also presents a new approach termed ‘Middle-Out Generalization.’ The novel method of generating patterns using case bases distinguishes itself from previous studies. It does not combine CBR with PBR but instead demonstrates a novel technique for generalizing cases, improving case-based reasoning efficiency, and reducing computing time. Tests on chess endgames are a proof of concept rather than the primary focus; experiments demonstrate the effectiveness of our approach, showing that the system can successfully improve its performance, with no look-ahead, while reducing execution time. Key findings from the experiments highlight that it nearly mirrors Stockfish’s suggested best move 99% of the time.},
  archive      = {J_KIS},
  author       = {Ghalem, Zahira and Zegour, Djamel Eddine and Bouabana-Tebibel, Thouraya and Rubin, Stuart H.},
  doi          = {10.1007/s10115-025-02472-9},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8577-8597},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Extending the scope of case-based reasoning through multiple case-based generalizations},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-effective data fusion in information retrieval. <em>KIS</em>, <em>67</em>(10), 8551-8575. (<a href='https://doi.org/10.1007/s10115-025-02434-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data fusion has demonstrated its effectiveness in enhancing information retrieval across various studies. However, advanced fusion methods typically require a dataset with extensive relevance judgments to train optimal model weights, necessitating labor-intensive and costly manual efforts. This study explores efficient methods for generating training data to facilitate affordable relevance judgments and improve fusion model quality. Experiments conducted on six datasets from TREC’s Precision Medicine and Deep Learning tracks reveal that with careful sampling design, near-optimal fusion weights can be achieved using only 5% of the documents compared to the full TREC judgments. This translates to a dataset comprising 20 queries and 500 relevance-judged documents in total. The findings highlight the potential for sophisticated fusion techniques to become more accessible to researchers and practitioners, delivering substantial performance improvements with minimal judgment effort and cost.},
  archive      = {J_KIS},
  author       = {Sun, Jiahui and Wu, Shengli and Nugent, Chris and Moore, Adrian},
  doi          = {10.1007/s10115-025-02434-1},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8551-8575},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cost-effective data fusion in information retrieval},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the graph-based extension of the set based model: A new approach on graph representation and model ensemble. <em>KIS</em>, <em>67</em>(10), 8521-8549. (<a href='https://doi.org/10.1007/s10115-025-02470-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to present a set of algorithmic improvements upon an extension of the Set-Based Model [14] that focuses on the dependence among document terms employing graph representations. A graph-based approach to document representation can positively affect the information retrieval process due to the ability of graphs to capture the syntactic notion and, in some cases, the semantic relationship among document terms. The aforementioned model generates complete graphs; thus, each document term will be interdependent with the rest. Consequently, an interdependent segment or multiple parts of a document called a window is defined, in which the graph creation algorithms are applied. The proposed methods aim to approximate the window size by exploiting the document length. Moreover, an attempt to create multiple windows is made, considering the relationship between a sentence and a paragraph, which is reflected in the semantic importance of nodes and edges. An attempt to tackle the stop-word detection problem on bridge nodes is made by implementing algorithmic schemes that use core decomposition [41] to identify the importance of such nodes in a sample of the corpus collection. Finally, a simple reranking scheme and an ensemble voting technique are implemented to enhance model performance on queries where the proposed approach lacks performance. The experimental analysis made on multiple document collections exhibits performance improvements and in some queries, our approach outperforms even state-of-the-art models such as BM25 [36] and ColBERT [17].},
  archive      = {J_KIS},
  author       = {Kalogeropoulos, Nikitas Rigas and Skamnelos, Nikolaos and Makris, Christos},
  doi          = {10.1007/s10115-025-02470-x},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8521-8549},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {On the graph-based extension of the set based model: A new approach on graph representation and model ensemble},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensuring the privacy of data in modern healthcare systems using adaptive encryption technique and disease prediction via attention-based residual DenseNet with GRU. <em>KIS</em>, <em>67</em>(10), 8469-8520. (<a href='https://doi.org/10.1007/s10115-025-02464-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease identification services provided by the healthcare cloud in the “mobile healthcare network” are inaccessible due to security issues and a lack of resources on “internet of things” nodes. Secure medical services are facilitated by the emergence of blockchain and artificial intelligence technologies. However, there is a chance for data hacking and privacy breaches. Therefore, an efficient privacy preservation system is developed in this work using the cryptographic algorithm to safeguard the confidentiality of the data and secure the private information of patients. It also used the deep learning model to predict the patient’s disease. Initially, the multimodal data including images, signals, or data are gathered from standard sources. Then, the collected multimodal data are converted into 2D images. Better visualization is achieved by this 2D image conversion. The converted 2D images are encrypted on the transmitter side through an adaptive 2D logistic chaotic model. This technique is capable of generating secret keys to improve the safety of data. The same approach is used for performing decryption at the receiver side. An optimal key is generated during the encryption to safeguard the data using a modified red-billed blue magpie optimizer. Finally, the disease is predicted using the attention-based residual DenseNet with a gated recurrent unit. This technique consumes much less amount of memory and provides fast execution. The experimental evaluation is carried out to demonstrate the effectiveness of the recommended model over traditional models.},
  archive      = {J_KIS},
  author       = {Sharma, Pankaj Kumar and Vijayvergia, Hemant Kumar and Garg, Amit and Saxena, Varun Prakash and Agrawal, Shyam Sundar and Sharma, Meeta},
  doi          = {10.1007/s10115-025-02464-9},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8469-8520},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Ensuring the privacy of data in modern healthcare systems using adaptive encryption technique and disease prediction via attention-based residual DenseNet with GRU},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CKRHE: A hierarchical embedding method for large-scale complex knowledge graphs. <em>KIS</em>, <em>67</em>(10), 8449-8467. (<a href='https://doi.org/10.1007/s10115-025-02425-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale complex knowledge graphs, existing approaches often overlook the hierarchical structure of data, limiting their effectiveness in applications such as information and recommender systems. To address this, we propose CKRHE, a hierarchical embedding technique that integrates entity types to capture the inherent hierarchy in knowledge graphs. Our method builds on translation-based embedding models and utilizes continuous bag-of-words and CNN architectures to improve semantic representation learning. A joint loss function is introduced to balance both descriptive and hierarchical losses, with adaptive equilibrium coefficients to optimize performance. Experimental results on benchmark and custom datasets demonstrate that CKRHE significantly outperforms existing methods in link prediction tasks, achieving 4.1% and 1.4% improvements in Hits@10 and MRR metrics, respectively. Our findings highlight the critical role of hierarchy-aware embedding in enhancing reasoning accuracy for large-scale knowledge graphs.},
  archive      = {J_KIS},
  author       = {Tong, Weiming and Chu, Xu and Zhang, Yungui and Li, Zhongwei and Jin, Xianji},
  doi          = {10.1007/s10115-025-02425-2},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8449-8467},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CKRHE: A hierarchical embedding method for large-scale complex knowledge graphs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized exercise recommendation based on knowledge structure and learners’ attempting preferences. <em>KIS</em>, <em>67</em>(10), 8417-8448. (<a href='https://doi.org/10.1007/s10115-025-02390-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the differences in learners’ knowledge mastery and learning needs, knowledge matching and appropriate exercise difficulty have always been important challenges faced by personalized exercise recommendation research. Furthermore, the logical relation among knowledge concepts is also an important factor that affects learners’ learning efficiency in exercise recommendation. However, most existing exercise recommendation researches only consider learners’ knowledge mastery, without taking into account their personalized feelings toward exercise difficulty. So, it is difficult to achieve precise recommendation for learners. To address these issues, this paper proposes a personalized exercise recommendation based on Knowledge Structure and learners’ Attempting Preferences (KSAP). Firstly, the method uses deep knowledge tracking (DKT) to obtain learners’ current knowledge state based on their interaction records, which is used to obtain learners’ weak knowledge concepts set. Secondly, Apriori association rules are used to mine the knowledge structure and generate knowledge map, which is further improved based on learners’ knowledge mastery to expand and rank their weak knowledge concepts set in order to obtain the corresponding sequence. Finally, in order to recommend the right difficulty level of exercises for learners, deep matrix factorization (DMF) is used to explore the high-order relation between learners and exercises to obtain their implicit exercise preferences. Their personalized explicit exercise preferences are mined from their historical attempting exercise data through statistical analysis. Then personalized exercises are recommended for learners by above three steps. In the experiments, the superiority of KSAP is verified by conducting comparative experiments with baseline methods. Moreover, the interpretability of KSAP is improved by considering the knowledge structure.},
  archive      = {J_KIS},
  author       = {Wang, Xin and Diao, Xiuli and Zeng, Qingtian and Ni, Weijian and Song, Zhengguo},
  doi          = {10.1007/s10115-025-02390-w},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8417-8448},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Personalized exercise recommendation based on knowledge structure and learners’ attempting preferences},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scientific retrieval: An effective heuristic-aided multi-scale adaptive transformer network for information retrieval process from scientific publications. <em>KIS</em>, <em>67</em>(10), 8377-8415. (<a href='https://doi.org/10.1007/s10115-025-02458-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research work presents a novel IR system, which employs a multi-scale-based deep learning model with a heuristic algorithm to address the issues in this experimental field. Initially, the required data is collected manually, which comprises scientific Portable Document Format (PDF) papers with different topics. Further, the PDF file is converted into docx format, whereas multiple details like author, subsections, title, abstract, table, figure, etc., are extracted. During training process, the essential features are identified from the extracted data by parameter optimized multi-scale adaptive transformer network (PO-MATN), in which some of the parameters are optimized using an improved chameleon swarm algorithm (ICSA). These meaningful features enhance the model’s efficacy and also performance rate. These extracted features are further transformed into a more informative form using weighted feature pool formation and are stored in the feature library. During testing stage, once the user sends the query, the MATN is utilized to determine the query features. Finally, the similarity check is performed for both the trained and test features to retrieve the related documents. Thus, the designed framework effectively performs the IR process from scientific publications. Finally, the experiments of this designed framework are conducted by analyzing traditional approaches. From the experimental findings, the developed model shows 85%, 86% and 86.5% precision, recall and F1-score values, thus ensuring the efficacy of the suggested IR process.},
  archive      = {J_KIS},
  author       = {Sabarmathi, K. R. and Kalamani, M.},
  doi          = {10.1007/s10115-025-02458-7},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8377-8415},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Scientific retrieval: An effective heuristic-aided multi-scale adaptive transformer network for information retrieval process from scientific publications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent optimization and inventory control of deteriorating items: An overview. <em>KIS</em>, <em>67</em>(10), 8335-8375. (<a href='https://doi.org/10.1007/s10115-025-02481-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inventory control of deteriorating items plays an important role since it has a direct influence on the supply chain efficiency and profitability of the concerned organizations. Businesses can use metaheuristic algorithms to optimize inventory levels, cut costs, and increase service levels. The present study provides an overview of the use of metaheuristic algorithms in inventory control of deteriorating items. This study presents an integrated view of how applications of various metaheuristic techniques such as genetic algorithms (GA), differential evolution (DE), particle swarm optimization (PSO), grey wolf optimizer (GWO), and others can be effectively used to address the inventory management issues. The present study demonstrates that metaheuristics greatly enhance the efficiency of inventory control by reducing waste as well as the cost associated with the product deterioration. This study concluded that use of metaheuristics to inventory management can increase supply chain profitability and efficiency. Our findings indicate that intelligent algorithms can be used by the organizations to build more efficacious inventory control systems for the deteriorating items. This review article can be treated as a valuable resource for the practitioners and academicians interested in implementing the intelligent optimization strategies for the inventory management of deteriorating items in various contexts.},
  archive      = {J_KIS},
  author       = {Singh, Praveendra and Jain, Madhu},
  doi          = {10.1007/s10115-025-02481-8},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8335-8375},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Intelligent optimization and inventory control of deteriorating items: An overview},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of AI-driven Q&A systems with taxonomy, prospects, and challenges. <em>KIS</em>, <em>67</em>(10), 8311-8334. (<a href='https://doi.org/10.1007/s10115-025-02477-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of AI-driven question answering has significantly evolved with advancements in large language models (LLMs), ranging from transformer-based models like BERT, which excels in understanding and retrieving text, to generative models such as ChatGPT and T5, which generate contextual responses. Within professional domains, AI-driven Q&A systems enable machines to retrieve and generate precise answers by leveraging ML, natural language processing (NLP), and knowledge-based approaches. This survey aims to provide a structured analysis of AI-driven Q&A systems, answering key research questions: (1) How are Q&A systems categorized based on taxonomy, domain, and architecture? (2) What are the recent advancements, particularly in generative AI and knowledge-based approaches? (3) What are the key challenges, such as accuracy, hallucination in generative models, and ethical concerns? A well-defined taxonomy is introduced, classifying Q&A systems into monolingual, cross-lingual, and multilingual frameworks while differentiating between open-domain and closed-domain approaches. This study extends previous surveys by systematically analyzing the impact of retrieval-based, hybrid, and generative models on Q&A performance. Additionally, the survey explores critical challenges, including bias in datasets, reasoning limitations, latency in real-time applications, and the ethical concerns surrounding AI-generated content. Future research directions include enhancing model robustness, mitigating biases, improving explainability, and optimizing generative models for domain-specific tasks. By addressing these challenges and opportunities, this study provides a comprehensive evaluation of state-of-the-art Q&A techniques, their limitations, and potential improvements.},
  archive      = {J_KIS},
  author       = {Albassami, Zahrah and Algarni, Abdulmohsen and Qahmash, Ayman and Ahmad, Zulfiqar},
  doi          = {10.1007/s10115-025-02477-4},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8311-8334},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A comprehensive review of AI-driven Q&A systems with taxonomy, prospects, and challenges},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven road traffic management: A comprehensive review of outlier detection techniques and challenges. <em>KIS</em>, <em>67</em>(10), 8267-8309. (<a href='https://doi.org/10.1007/s10115-025-02503-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road traffic management is crucial in shaping urban mobility using sophisticated technologies and data analytics to enhance road traffic movement, security, and environmental sustainability. ML, DL, and FL approaches are subsets of AI technologies that play a significant role in urban traffic regulation. In road traffic regulation, within the context of traffic patterns, outliers or anomalies are the data points or observations that diverge markedly from the rest of the data collection. These outliers can arise from various factors and have different implications for traffic analysis, prediction, and control. This survey discusses the significant role that ML, DL, and FL play in outlier detection to predict traffic patterns. This comprehensive review not only outlines the current applications but also contributes to understanding and developing future perspectives in road traffic management. This survey provides a detailed examination of these techniques in road traffic management, outlining the methodologies employed in the reviewed studies. It also covers a qualitative assessment of the role of outliers in managing road traffic. This study extends previous work by detailing how outlier detection techniques are utilized within road traffic management. It provides a comparative analysis of outlier detection methods based on various parameters, a perspective not thoroughly explored in earlier studies. Furthermore, this review investigates the impact of distance metrics on the effectiveness of outlier detection in traffic analysis, an area that previous surveys have not adequately covered. The survey offers new insights into optimizing traffic management systems through advanced analytical techniques by addressing these gaps.},
  archive      = {J_KIS},
  author       = {Rathi, Garima and Kamble, Shailesh and Sharma, Nonita},
  doi          = {10.1007/s10115-025-02503-5},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8267-8309},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {AI-driven road traffic management: A comprehensive review of outlier detection techniques and challenges},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment analysis survey: Datasets, techniques, applications, tools, and challenges. <em>KIS</em>, <em>67</em>(10), 8219-8265. (<a href='https://doi.org/10.1007/s10115-025-02499-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of internet usage worldwide has created an open platform for individuals to express their opinions across various online channels, including Twitter, Facebook, mobile applications, and forums. These social interactions generate vast amounts of data daily. Sentiment analysis, a key technique in natural language processing, involves extracting and analyzing people's opinions, emotions, and reactions to a wide range of subjects such as products, services, events, and public figures. This paper presents a comprehensive survey of sentiment analysis techniques, datasets, applications, tools, and the challenges that shape this evolving field. We provide a detailed exploration of the sentiment analysis process, from data collection and preparation to presenting results through visualization techniques. The paper categorizes sentiment analysis methods into four main types: lexicon-based, machine-learning-based, hybrid, and other approaches, and compares them based on classification performance. Finally, we highlight some of the major challenges in sentiment analysis and suggest potential research directions for the future.},
  archive      = {J_KIS},
  author       = {Hankar, Mustapha and Mzili, Toufik and Kasri, Mohammed and Beni-Hssane, Abderrahim},
  doi          = {10.1007/s10115-025-02499-y},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {8219-8265},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sentiment analysis survey: Datasets, techniques, applications, tools, and challenges},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SecPPAccess: Secured privacy protection access control on big data in cloud computing paradigm. <em>KIS</em>, <em>67</em>(9), 8195-8218. (<a href='https://doi.org/10.1007/s10115-025-02443-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of cloud computing, preserving the privacy of big data while also allowing for secure access control is a critical concern. With the increasing adoption of cloud technology, it is imperative to address the challenges associated with safeguarding sensitive data while enabling authorized access. This paper develops an efficient privacy-preserving security model that uses cryptographic techniques to protect sensitive data and ensure that only authorized individuals can access it. The research puts together a secure data authentication technique, named secured privacy protection access control (SecPPAccess), allowing secured communication in cloud computing. For the protection of privacy for sensitive data, the protected transferring of data is commenced among the elements, like a user, cloud server, registration authority, key generation center and data owner, by using many phases mainly the key generation phase, setup phase, server registration, user registration, data upload, data encryption, requester authentication, data access, and data download phase. Here, a method is designed newly for securing data privacy using various operations, like secret keys, hashing, encryption, etc. The study proves that the initiated SecPPAccess model achieves the highest rate of detection of 0.85, the lowest usage for memory of 0.505 MB, and less computation time of 51.50 s.},
  archive      = {J_KIS},
  author       = {Gupta, Lalit Mohan and Garg, Hitendra and Samad, Abdus and Singh, Atul Kumar},
  doi          = {10.1007/s10115-025-02443-0},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8195-8218},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SecPPAccess: Secured privacy protection access control on big data in cloud computing paradigm},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective and lightweight lossy compression of tensors: Techniques and applications. <em>KIS</em>, <em>67</em>(9), 8143-8193. (<a href='https://doi.org/10.1007/s10115-025-02471-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world data from various domains can be represented as tensors, and a significant portion of them is large scale. Thus, tensor compression is crucial for their storage and transmission. Recently, deep learning-based methods have emerged to enhance compression performance. However, they require considerable compression time to fulfill their performance. In this work, to achieve both speed and performance, we develop ELiCiT, an effective and lightweight lossy tensor compression method. When designing ELiCiT, we avoid deep auto-regressive neural networks and index reordering, which incur high computational costs of deep learning-based tensor compression. Specifically, instead of using the orders of indices as parameters, we introduce a feature-based model for indices, which enhances the model’s expressive capacity and simplifies the overall end-to-end training procedure. Moreover, to reduce the size of the parameters and computational cost for inference, we adopt end-to-end clustering-based quantization, as an alternative to deep auto-regressive architecture. As a result, ELiCiT becomes easy to optimize with enhanced expressiveness. We prove that it (partially) generalizes deep learning-based methods and also traditional ones. Using eight real-world tensors, we show that ELiCiT yields compact outputs that fit the input tensor accurately. Compared to the best competitor with similar fitness, it offers 1.51 $$-$$ 5.05 $$\times $$ smaller outputs. Moreover, compared to deep learning-based compression methods, ELiCiT is 11.8 $$-$$ 96.0 $$\times $$ faster with 5–48% better fitness for a similarly sized output. We also demonstrate that ELiCiT is extended to matrix completion, neural network compression , and tensor stream summarization, providing the best trade-offs between model size and application performance.},
  archive      = {J_KIS},
  author       = {Ko, Jihoon and Kwon, Taehyung and Jung, Jinhong and Shin, Kijung},
  doi          = {10.1007/s10115-025-02471-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8143-8193},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Effective and lightweight lossy compression of tensors: Techniques and applications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective feature selection and classification technique for palmprint biometric identification systems. <em>KIS</em>, <em>67</em>(9), 8115-8142. (<a href='https://doi.org/10.1007/s10115-025-02478-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint recognition’s outstanding sanitary, non-invasive, and user-friendliness properties have sparked a lot of research attention. The majority of palmprint recognition techniques used today are deep learning approaches, which typically use palmprint images to learn discriminative features. In most cases, considerable labelled samples are needed to perform well enough for identification. Here, we propose employing a deep learning method for palmprint recognition to get around the problems. First, the original picture should be (a) converted to greyscale, (b) cropped and resized, and (c) contrast-enhanced using the contrast-limited adaptive histogram equalization method. Next, use the ConvNeXt approach to extract the most essential characteristics from contrast-enhanced palm pictures. After removing redundant collected attributes, the improved spotted hyena optimizer algorithm selects the most important features. Finally, the deep fuzzy neural network (DFNN) technique determines if the palmprint image matches. Use the sooty tern optimization algorithm for increased identification and categorization accuracy to increase categorization accuracy. The proposed approach effectively reduces the number of attributes and computation time while increasing the accuracy of the optimized DFNN—an approximation of the proposed methods using Tongji, IITD, and CASIA open palmprint datasets. The approach has better accuracy with 99.62%, 99.72%, and 99.67% on IITD, Tongji, and CASIA palmprint databases. Experiments show that our approach achieves a high identification rate while using a substantially fewer number of features.},
  archive      = {J_KIS},
  author       = {Nalamothu, Aravind and Rayachoti, Eswaraiah},
  doi          = {10.1007/s10115-025-02478-3},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8115-8142},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An effective feature selection and classification technique for palmprint biometric identification systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VCGAE++: Variational collective graph autoEncoder for multi-behavior recommendation. <em>KIS</em>, <em>67</em>(9), 8085-8114. (<a href='https://doi.org/10.1007/s10115-025-02467-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoder (VAE) is known as a classic and effective method in modeling users’ homogeneous behaviors in recommender systems. In recent years, graph neural networks (GNNs) have achieved promising performance in learning users’ preferences by modeling complex relationships between users and items. However, most VAE- and GNN-based methods are for single-behavior recommendation, rather than the more prevalent counterpart in real-world applications, i.e., multi-behavior recommendation. This motivates us to leverage VAE and GNNs to address the more important and challenging problem of multi-behavior recommendation. Traditional multi-behavior recommendation models have not captured the complex transition relationships across different types of behaviors well, ignoring the varying semantic strength of different types of behaviors. In addition, most of them just construct separate behavior-specific subgraphs and learn separate collaborative filtering embeddings, overlooking the information of the global graph. Moreover, existing methods rarely explored how to deal with the sparse data under the target behavior (e.g., purchase). To tackle the above four challenges, we propose a novel multi-behavior recommendation framework named VCGAE++, which inherits the advantages of VCGAE (Variational Collective Graph AutoEncoder) to fully exploit the multi-behavior data and model the high-order relationships to improve the accuracy of the recommendations. Specifically, we design a multi-behavior contrastive learning framework to capture the complex transitions between diverse behaviors. In addition, we design a global graph information fusion network to capture the high-level relationships across different user-item interaction graphs. Extensive experiments on four real-world datasets clearly demonstrate the effectiveness of our VCGAE++ compared with the state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Zhuang, Yingxuan and Liu, Yang and Pan, Weike and Ming, Zhong},
  doi          = {10.1007/s10115-025-02467-6},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8085-8114},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {VCGAE++: Variational collective graph autoEncoder for multi-behavior recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task multi-modal graph neural network for recommender system. <em>KIS</em>, <em>67</em>(9), 8059-8084. (<a href='https://doi.org/10.1007/s10115-025-02456-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of online information, users may also face information overload. To handle this problem, recommender systems have become an effective strategy, which can analyze the characters of users and items to provide valuable information. One of the important types of information is the item’s side information. For example, in Amazon dataset, side information mainly includes visual side information (e.g., image and video), textual side information (e.g., title and description), and auxiliary side information (e.g., brand and category). To analyze various types of side information, some research designed multiple modalities for different types of side information, which can improve the performance of the recommender system. To analyze the deeper relationships between users and items, recent works also use a graph structure to represent the interactions. Existing works on multi-modal recommender systems using graph neural networks largely depend on the interaction records, while little effort focuses on the relationships between interactions and various types of side information. In this paper, we propose a novel multi-task learning model. First, we construct the interaction records to graphs for each modality to gather the representations, and then we analyze the representations of each modality and the specific side information based on the similarities. We design a multi-task multi-modal graph neural network framework built upon message passing with the attention mechanism of graph neural networks, which can generate the representations of users and items from interaction records, and then analyze the relationships between the representations from GNNs and item’s side information. We conduct experiments on three public datasets, Amazon, Modcloth and MovieLens. The results of our model outperform the state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Jiao, Shengzhe and Zhang, Yihong and Hara, Takahiro},
  doi          = {10.1007/s10115-025-02456-9},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8059-8084},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-task multi-modal graph neural network for recommender system},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view multi-label personalized classification via generalized exclusive sparse tensor factorization. <em>KIS</em>, <em>67</em>(9), 8023-8057. (<a href='https://doi.org/10.1007/s10115-025-02449-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification (MLC) assigns multiple relevant labels to each sample simultaneously, while multi-view MLC aims to apply MLC to handle heterogeneous data represented by multiple feature subsets. In recent years, a variety of methods have been proposed to handle these problems and have achieved great success in a wide range of applications. MLC saves global label correlation by building a single model shared by all samples but ignores sample-specific local structures, while personalized learning (PL) is able to preserve sample-specific information by learning local models but ignores the global structure. Integrating PL with MLC is a straightforward way to overcome the limitations, but it still faces three key challenges. (1) capture both local and global structures in a unified model; (2) efficiently preserve full-order interactions between labels, samples and features or multi-view features in heterogeneous data; (3) learn a concise and interpretable model where only a fraction of interactions are associated with multiple labels. In this paper, we propose a novel Multi-label Personalized Classification (MLPC) method and its multi-view extension to handle these challenges. For (1), it integrates local and global components to preserve sample-specific information and global structure shared across samples, respectively. For (2), a multilinear model is developed to capture full-order label-feature-sample interactions, and over-parameterization is avoided by tensor factorization. For (3), exclusive sparsity regularization penalizes factorization by promoting intra-group competition, thereby eliminating irrelevant and redundant interactions during Exclusive Sparse Tensor Factorization (ESTF). Moreover, theoretical analysis generalizes the proposed ESTF and reveals the equivalence between MLPC and a family of jointly regularized counterparts. We develop an alternating algorithm to solve the optimization problem, and demonstrate its effectiveness based on comprehensive experiments on both synthetic and real-world benchmark datasets.},
  archive      = {J_KIS},
  author       = {Fei, Luhuan and Lin, Weijia and Wang, Jiankun and Sun, Lu and Kudo, Mineichi and Kimura, Keigo},
  doi          = {10.1007/s10115-025-02449-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {8023-8057},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-view multi-label personalized classification via generalized exclusive sparse tensor factorization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond pairwise relationships: A transformer-based hypergraph learning approach for fraud detection. <em>KIS</em>, <em>67</em>(9), 7987-8022. (<a href='https://doi.org/10.1007/s10115-025-02476-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraud detection in online networks has become increasingly challenging as fraudsters adopt sophisticated camouflage tactics to evade detection, making it imperative to combat their deceptive strategies. Graph-based fraud detection has gained significant attention in recent years, reflecting its growing potential to mitigate sophisticated fraudulent activities. The main objective of graph-based fraud detection is to distinguish between fraudsters and normal entities within graphs. While real-world networks contain complex, high-order relationships, existing graph-based fraud detection methods focus solely on pairwise interactions, overlooking non-pairwise relationships and the broader dependencies among entities within fraud graphs. Thus, we highlight the importance of exploring non-pairwise relationships to build a more effective fraud detection model. In this paper, we propose TROPICAL, a novel TRansfOrmer-based hyPergraph LearnIng framework for detecting CAmouflaged maLicious actors in online social networks. To capture comprehensive high-order relations, we construct a hypergraph from the original input graph. However, constructing the hypergraph can be computationally intensive. TROPICAL addresses this challenge by carefully selecting moderate hyperparameters, creating a balance between computational efficiency and effectively capturing high-order relationships. TROPICAL learns node representations by processing multiple hyperedge groups and incorporates positional encodings into the aggregated information to enhance their distinctiveness. The aggregated sequential information is then passed through a transformer encoder, enabling the model to generate rich, high-order representations to detect camouflaged fraudsters. Extensive experiments on two real-world datasets demonstrate TROPICAL’s superior performance compared to the state-of-the-art fraud detection models. The source codes and the datasets of our work are available at https://github.com/VenusHaghighi/TROPICAL .},
  archive      = {J_KIS},
  author       = {Haghighi, Venus and Soltani, Behnaz and Shabani, Nasrin and Wu, Jia and Zhang, Yang and Yao, Lina and Yang, Jian and Sheng, Quan Z.},
  doi          = {10.1007/s10115-025-02476-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7987-8022},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Beyond pairwise relationships: A transformer-based hypergraph learning approach for fraud detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid long-range dependency-aware graph convolutional network for node classification. <em>KIS</em>, <em>67</em>(9), 7955-7986. (<a href='https://doi.org/10.1007/s10115-025-02473-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks, despite their effectiveness, face inherent limitations such as over-squashing within the message-passing mechanism. This impedes their ability to effectively capture long-range dependencies, thereby constraining their performance on graph mining tasks, such as node classification. Recent studies attempt to address this issue by developing two-step aggregation-based methods, whose key idea is to seek nodes with high similarity in topology or attribute feature space for information aggregation. Despite effectiveness, we observe that previous studies share two common deficiencies: single-type feature-based strategies of sifting nodes, and limited to preserving low-frequency information. These limitations result in the inadvertent inclusion of irrelevant nodes and the neglect of multi-frequency information within long-range dependencies. To this end, we propose a novel method, called hybrid long-range dependency-aware graph convolutional network (HLDGCN) to overcome the above deficiencies. Specifically, HLDGCN first employs a flexible node selection process that considers both topological and attribute features across diverse graph structures. Subsequently, the original graph is transformed into a long-range dependency-aware graph by incorporating additional edges that connect selected nodes. Contrary to prior approaches that focus solely on high-similarity nodes, HLDGCN incorporates both high- and low-similarity nodes, assigning positive and negative edge weights, respectively. This design ensures the preservation of diverse frequency information within the graph. Furthermore, HLDGCN employs a specialized graph convolution layer that utilizes a separated fusion strategy to aggregate information effectively on the transformed graph. Extensive experiments on various benchmark datasets, including homophily and heterophily graphs, demonstrate the superiority of HLDGCN on the node classification task.},
  archive      = {J_KIS},
  author       = {Chen, Jinsong and Wang, Meng and He, Kun},
  doi          = {10.1007/s10115-025-02473-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7955-7986},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hybrid long-range dependency-aware graph convolutional network for node classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H2LFR: A hybrid two-layered feature ranking approach for enhanced data analysis. <em>KIS</em>, <em>67</em>(9), 7901-7953. (<a href='https://doi.org/10.1007/s10115-025-02463-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is a critical process in machine learning that involves identifying a subset of relevant features from a larger set. The primary objective of FS is to eliminate irrelevant and redundant features, thereby reducing storage requirements and computational costs, enhancing data interpretability, and improving model accuracy. FS methods can be classified as either supervised or unsupervised, depending on the availability of a target variable. Furthermore, FS techniques are broadly categorized into two subtypes: feature subset selection and feature ranking (FR). Typically, various feature subsets are identified through different FR techniques, each of which operates under specific assumptions regarding the regression function that relates input features to output. In this paper, we introduce a novel supervised hybrid two-layered feature ranking technique, referred to as H2LFR, which integrates a learning model, a metaheuristic algorithm, and weighted metrics. The first layer of this approach (H2LFR-L1) focuses on extracting encoded solutions alongside their performance metrics. Subsequently, the second layer (H2LFR-L2) is tasked with ranking the features. To evaluate the proposed method, we utilize twelve publicly available datasets and the results obtained from H2LFR are compared against 16 established FR methods. Our comparative analysis demonstrates that H2LFR consistently outperforms 12 of these methods in many cases, yielding notably robust results.},
  archive      = {J_KIS},
  author       = {Balaha, Hossam Magdy and Hassan, Asmaa El-Sayed and Balaha, Magdy Hassan},
  doi          = {10.1007/s10115-025-02463-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7901-7953},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {H2LFR: A hybrid two-layered feature ranking approach for enhanced data analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit graph neural network for deep graph transformation. <em>KIS</em>, <em>67</em>(9), 7871-7900. (<a href='https://doi.org/10.1007/s10115-025-02468-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel graph neural network architecture for the general problem of attributed graph transformation, where both the input and output are attributed graphs, and the evolution of the output graphs, including the attributes of nodes and edges, is governed by complex interactions that capture the intricate dependencies within the transformation process. Research in this area has been limited due to two key challenges: (1) the complexity of jointly modeling four types of atomic interactions, i.e., node-to-edge, node-to-node, edge-to-node, and edge-to-edge; and (2) the challenge of modeling dependencies between nodes and edges that span distant parts of the graph and develop through multiple iterative steps in the transformation process. To overcome these challenges, we present a scalable equilibrium model, NEC $$^{\infty }$$ , which incorporates both node-to-edge and edge-to-node message passing. Furthermore, we develop an efficient optimization algorithm based on the implicit function theorem [1] and provide a well-posedness analysis of NEC $$^{\infty }$$ . Experiments were conducted on four synthetic random graph datasets, four real-world datasets, and two synthetic dynamical system datasets, employing multiple evaluation metrics for node and edge prediction. The results show that NEC $$^{\infty }$$ consistently outperforms all baseline models, achieving up to a tenfold decrease in MSE on BA random graphs, edge prediction accuracy ranging from 94% to nearly 100% on synthetic datasets, and exceptional results in tasks involving molecular reactions and high-order brain networks, highlighting its strength in modeling complex graph transformations.},
  archive      = {J_KIS},
  author       = {Zhang, Lei and Zhang, Qisheng and Chen, Zhiqian and Sun, Yanshen and Lu, Chang-Tien and Zhao, Liang},
  doi          = {10.1007/s10115-025-02468-5},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7871-7900},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Implicit graph neural network for deep graph transformation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple feature-anisotropic regularization for out-of-domain intent detection. <em>KIS</em>, <em>67</em>(9), 7847-7869. (<a href='https://doi.org/10.1007/s10115-025-02459-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-domain (OOD) intent detection is one of the research hotspots in task-based AI dialog. Aiming at the problems of entangling features and insufficient extraction of discriminative information, multiple feature-anisotropy regularization for OOD intent detection bidirectional encoder representations from transformer (MFAR-BERT) is proposed. The instance-level and class-level features of the intent are decoupled by a multi-head feature anisotropy disentangled strategy. To extract feature information of instance-level intent samples from different perspectives, multi-view KNN contrastive learning is designed. Correlation-matrix regularization is devised to adjust the direction of class-level sentence embeddings. Feature distributions are avoided from being confined in cone space. The experimental results show that the MFAR-BERT model achieves a minimum improvement of 0.67, 0.44, 0.49, and 1.85% on ACC_ALL, F1_ALL, F1_OOD, and F1_IND compared to DCL, SCL, ABD, HybridCL, and KNNBERT models on the publicly available datasets BANKING, OverStackflow, CLINC-SMALL, and CLINCX-FULL},
  archive      = {J_KIS},
  author       = {Wu, Di and Wang, Xiaoyu and Feng, Liming},
  doi          = {10.1007/s10115-025-02459-6},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7847-7869},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multiple feature-anisotropic regularization for out-of-domain intent detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating rule-based and generative data augmentation techniques for legal document classification. <em>KIS</em>, <em>67</em>(9), 7825-7846. (<a href='https://doi.org/10.1007/s10115-025-02454-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated text classification is a fundamental research topic within the legal domain as it is the foundation for building many intelligent legal solutions. There is a scarcity of publicly available legal training data and these classification algorithms struggle to perform in low data scenarios. Text augmentation techniques have been proposed to enhance classifiers through artificially synthesised training data. In this paper we present and evaluate a combination of rule-based and advanced generative text augmentation methods designed to create additional training data for the task of classification of legal contracts. We introduce a repurposed CUAD contract dataset, modified for the task of document level classification, and compare a deep learning distilBERT model with an optimised support vector machine baseline for useful comparison of shallow and deep strategies. The deep learning model significantly outperformed the shallow model on the full training data (F1-score of 0.9738 compared to 0.599). We achieved promising improvements when evaluating the combined augmentation techniques on three reduced datasets. Augmentation caused the F1-score performance to increase by 66.6%, 17.5% and 2.6% for the 25%, 50% and 75% reduced datasets respectively, compared to the non-augmented baseline. We discuss the benefits augmentation can bring to low data regimes and the need to extend augmentation techniques to preserve key terms in specialised domains such as law.},
  archive      = {J_KIS},
  author       = {Duffy, William and O’Connell, Eoin and McCarroll, Niall and Sloan, Katie and Curran, Kevin and McNamee, Eugene and Clist, Angela and Brammer, Andrew},
  doi          = {10.1007/s10115-025-02454-x},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7825-7846},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Evaluating rule-based and generative data augmentation techniques for legal document classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Providing and evaluating a model for big data anonymization streams by using in-memory processing. <em>KIS</em>, <em>67</em>(9), 7791-7824. (<a href='https://doi.org/10.1007/s10115-025-02417-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting valuable information from vast sources of social networks while protecting confidentiality and preventing data disclosure is a significant challenge in big data environments. Traditional anonymization methods often fall short in handling the volume, variety, and velocity of big data, leading to high data loss and inefficiency. This article addresses these challenges by proposing a novel anonymization method based on K-means clustering within the Spark framework, leveraging its in-memory processing capabilities. Our model uses K-means clustering to determine optimal cluster heads, significantly reducing data loss and identity disclosure risks. By utilizing Spark's RDD abilities and the MLlib component, our method achieves faster processing times compared to traditional methods that rely on non-in-memory big data tools. Performance evaluation demonstrates that at k = 9, the cost factor is minimized to 0.20, indicating the efficiency and effectiveness of our approach. The proposed method not only enhances processing speed but also ensures minimal data loss, making it suitable for real-time anonymization of big data streams. This work provides a balanced solution that addresses the critical need for high-speed data anonymization while maintaining data privacy and utility.},
  archive      = {J_KIS},
  author       = {Shamsinejad, Elham and Banirostam, Hamid and BaniRostam, Touraj and Pedram, Mir Mohsen and Rahmani, Amir Masoud},
  doi          = {10.1007/s10115-025-02417-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7791-7824},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Providing and evaluating a model for big data anonymization streams by using in-memory processing},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TIDRec: A novel triple-graph interactive distillation method for paper recommendation. <em>KIS</em>, <em>67</em>(9), 7757-7789. (<a href='https://doi.org/10.1007/s10115-025-02457-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of academic graph networks, over-fitting has become a great challenge for graph-based paper recommendation. Existing methods mainly focus on knowledge distillation to solve above problem by compressing the volume of graph networks. However, incomplete distillation between two graphs would lead to the neglect of author’s diverse interests, resulting in biases for research interests. Therefore, we propose a new triple-graph interactive distillation recommendation (TIDRec) method for paper recommendation. Specifically, we construct a triple-graph interaction to complete the distillation knowledge between graphs to correct biased research interests of authors. First, a main model is built that integrates knowledge from both graphs (i.e., writing–citation relationship and author–author co-authorship graph) to initialize inner product vectors, capturing global research interests. Then, two auxiliary models with single graph knowledge are constructed to generate distinct inner product vectors, mining local research interests, respectively. Next, a triple-graph interactive distillation approach is designed to continuously correct the global research interests with distill vectors of each other. Finally, papers highly relevant to global research interests are recommended to authors. Extensive experiments prove that TIDRec surpasses state-of-the-art approaches, with an average performance improved by 8–25% for all four metrics.},
  archive      = {J_KIS},
  author       = {Xiao, Xia and Liu, Yan and Huang, Jiaying and Jin, Dawei and Shen, Zuwu and Zhang, Chengde},
  doi          = {10.1007/s10115-025-02457-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7757-7789},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {TIDRec: A novel triple-graph interactive distillation method for paper recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity-enhanced conversational recommendation via multi-agent reinforcement learning. <em>KIS</em>, <em>67</em>(9), 7727-7755. (<a href='https://doi.org/10.1007/s10115-025-02455-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-round conversational recommendation (MRCR) system assists users in finding the items they need with the fewest dialogue rounds by inquiring about desired features or making tailored recommendations. Numerous models employ single-agent Reinforcement Learning (RL) to accomplish MRCR and improve recommendation accuracy. However, they overlook the diversity of conversational recommendations and primarily focus on popular features or items. It impacts the fair visibility of the items and results in an unbalanced user experience. We propose a diversity-enhanced conversational recommendation model (DECREC), which is built on our proposed multi-agent RL framework. Compared to single-agent methods, this collaborative approach enables broader exploration of the action space, leading to more diverse decisions and recommendation results. Furthermore, we introduce a dynamic experience replay method that balances long-tail and head data. It ensures that each learning batch includes long-tail samples, keeping the model attentive to these less common but important data. Moreover, we incorporate feature entropy into the value estimation process, encouraging broader feature exploration and ultimately enhancing recommendation diversity. Extensive experiments on four public datasets demonstrate that DECREC reduces bias in MRCR and achieves optimal recommendation diversity and accuracy. Our code is available at https://github.com/wzhwzhwzh0921/DECREC .},
  archive      = {J_KIS},
  author       = {Wang, Zihan and Feng, Shi and Wang, Daling and Song, Kaisong and Wu, Gang and Zhang, Yifei and Zhao, Han and Yu, Ge},
  doi          = {10.1007/s10115-025-02455-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7727-7755},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Diversity-enhanced conversational recommendation via multi-agent reinforcement learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal associated learning with spatial–temporal attention for hot topic detection. <em>KIS</em>, <em>67</em>(9), 7699-7726. (<a href='https://doi.org/10.1007/s10115-025-02444-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosion in the number of web videos, it has become a common practice to detect hot topics with web videos. However, each video clip contains multiple patterns, in which object actions might only appear in specific spatial areas or specific time periods, posing a huge challenge for web video hot topic detection. Fortunately, visual information during a specific time period and area will significantly enhance the rapid capture of key information, which is particularly important for detecting hot topics. Therefore, we propose a cross-modal associated learning method with spatial–temporal attention. It can automatically select discriminative time segments to detect hot topics by focusing on spatial regions with rich information. Firstly, after focusing on important keyframes related to the topic through temporal attention, spatial attention emphasizes the salient regions in the frame, thus incorporating discriminative features at the spatial level. Secondly, after integrating text structure knowledge into text semantic features, it can adaptively learn the weights of text features and visual features. Thirdly, adaptive learning of cross-modal fusion weights, achieving mutual guidance between text and visual information in attention dispersed association models to enhance feature learning. Finally, under the constraint of contrast loss, hot topics are detected with the similarity between features. Extensive experiments conducted on web videos from YouTube indicate that our method outperforms 8 leading state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Zhang, Chengde and Liu, Shiyu and Li, Xinyu and Xiao, Xia},
  doi          = {10.1007/s10115-025-02444-z},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7699-7726},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cross-modal associated learning with spatial–temporal attention for hot topic detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chronological squirrel search algorithm enabled deep recurrent neural network for employability prediction. <em>KIS</em>, <em>67</em>(9), 7669-7698. (<a href='https://doi.org/10.1007/s10115-025-02437-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scholars are still debating the idea of employability, and no agreement has been reached as of yet. A solid theoretical foundation is still not established with the empirical investigation. The ability of the graduate to find fulfilling employment emphasizes the satisfaction that the graduate finds in their job. Therefore, employability prediction models are crucial in assessing a student's ability to find employment. This study aims to create a hybrid optimization-enhanced deep learning model for predicting employability. For that, primarily, input data is given to the pre-processing phase, where quantile normalization is used. Then, feature fusion is done by using Renyi entropy with Generative Adversarial Network. The employability prediction is done by utilizing a Deep Recurrent Neural Network (Deep RNN) in which its weight is trained using the proposed Chronological Squirrel Search Algorithm (CSSA). Here, CSSA is constructed by the combination of the Chronological concept and Squirrel search algorithm to optimize the predicted result. Moreover, the predicted output is noted. Furthermore, the introduced Chronological Squirrel Search Algorithm_Deep Recurrent Neural Network (CSSA_Deep RNN) compared with different algorithms illustrates better performance concerning the evaluation metrics such as Root-Mean-Square Error and Mean Square Error with a minimal error value of 0.458 and 0.210, respectively.},
  archive      = {J_KIS},
  author       = {Kamakshamma, V. and Bharati, K. F.},
  doi          = {10.1007/s10115-025-02437-y},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7669-7698},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Chronological squirrel search algorithm enabled deep recurrent neural network for employability prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SkSpO-L2TDBM: Optimized drug name recognition using a large language-based time-distributed deep learning model. <em>KIS</em>, <em>67</em>(9), 7641-7667. (<a href='https://doi.org/10.1007/s10115-025-02409-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poor handwriting of doctors in medical prescriptions can cause misunderstanding, misreading, misinterpretation risks and cause medical errors. Despite being rare, identifying the correct drugs with the unclear prescription seriously affects patients and also requires a quite lot of time, attention, and effort for recognition. As a result, a drug name recognition system is implemented, but previously developed models are not significant in terms of accuracy, interpretability, and reliability. Therefore, the skill split optimization enabled large language-based time-distributed bidirectional long short-term memory (SkSpO-L2TDBM) model for drug name recognition is proposed in this research. The SkSpO-L2TDBM model exploits deep features concerning bidirectional encoder representations from transformers and bidirectional long short-term memory techniques that are employed to increase the model’s reliability and interpretability for effective recognition. Moreover, the SkSpO algorithm tunes the hyper-parameters of the proposed model based on the effect of skillful learning and sharing ability that makes easy recognition with maximum convergence speed. The major advantages of the proposed model are simplicity, robustness, and endue complex computation for accurate recognition. Compared to other existing techniques, the SkSpO-L2TDBM model achieved a minimal mean squared error rate of 4.46, and minimal root mean squared error rate of 2.11 using the hybrid dataset comprising the ‘Handwritten Medical Prescriptions Collection’ dataset, and a proprietary set of handwritten medical prescriptions collected from various doctors across the cities such as Nagpur, Pune in Maharashtra, India. Moreover, the proposed approach is robust in recognizing the biomedical entities in the text.},
  archive      = {J_KIS},
  author       = {Nair, Sruthi and Sahare, Parul and Peshwe, Paritosh},
  doi          = {10.1007/s10115-025-02409-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7641-7667},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SkSpO-L2TDBM: Optimized drug name recognition using a large language-based time-distributed deep learning model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive hierarchical knowledge distillation from GNNs to MLPs. <em>KIS</em>, <em>67</em>(9), 7619-7639. (<a href='https://doi.org/10.1007/s10115-025-02447-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation has attracted much attention in knowledge and information systems. The latest distillation methods use Kullback–Leibler divergence to distill complex models from simple models, yielding surprising results. However, due to the difference in the capacity of graph neural networks (GNNs) and MLPs models, distilling GNNs through fixed dimensions and layers of MLPs will inevitably cause information loss. In addition, the gap between the features produced by GNNs and the features produced by MLPs gradually becomes larger as the number of layers increases. To this end, we propose an adaptive hierarchical distillation framework that distills GNNs through MLPs with variable dimensions and layers to ensure the integrity of the distilled information. Specifically, we use a neural architecture search to adaptively find MLPs with appropriate dimensions and layers for each layer of GNNs. Then, the graph structure information is distilled from GNNs layer by layer, which makes the student neural network structure can better match with the teacher model, and the features are better aligned, so as to better learn the graph structure information. At last, the student model distilled to the complete information is used in the downstream learning task. Experimental results on various datasets show impressive improvements in node classification tasks compared with previous state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Zhang, Junfeng and Xie, Cheng and Yu, BeiBei and Yang, Rui},
  doi          = {10.1007/s10115-025-02447-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7619-7639},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Adaptive hierarchical knowledge distillation from GNNs to MLPs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conflict-aware influence maximization on hostile-labeled social networks. <em>KIS</em>, <em>67</em>(9), 7597-7618. (<a href='https://doi.org/10.1007/s10115-025-02466-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) is a well-known problem in social network analysis, which aims to identify a strategic set of seeds to maximize the influence propagation. However, a significant downside has emerged: influence propagation can unintentionally activate previously isolated but mutually hostile nodes in cyberspace, increasing the likelihood of online conflicts. To mitigate this issue, we propose the Conflict-Aware Influence Maximization (CAIM) problem, which seeks to maximize influence while minimizing the activation of mutually hostile nodes on hostile-labeled social networks. We demonstrate that CAIM remains NP-hard and #P-hard, making it a complex non-submodular optimization problem. To overcome this challenge, we propose an efficient estimation method for the objective function and design a convergent algorithm with a data-driven approximation of $$z_\lambda ^+ / b^+$$ , where the parameter computations are intricately connected to the solution. Experiments on real-world networks demonstrate that our algorithms outperform multiple baselines in effectively preventing conflicts while maximizing influence.},
  archive      = {J_KIS},
  author       = {Rao, Guoyao and Li, Deying and Zhu, Yuqing},
  doi          = {10.1007/s10115-025-02466-7},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7597-7618},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Conflict-aware influence maximization on hostile-labeled social networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding user satisfaction: Explainable artificial intelligence-based user-centric analysis of mobile health applications adoption. <em>KIS</em>, <em>67</em>(9), 7563-7596. (<a href='https://doi.org/10.1007/s10115-025-02451-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile health applications (mHealth) have revolutionized healthcare sector by leveraging mobile technology to provide personalized services. As a rapidly growing industry, mHealth aligns with the World Health Organization’s goal of empowering patients to take control of their healthcare journey. In the realm of mHealth, ensuring user satisfaction always remains a key concern. Therefore, recognizing crucial factors that determine customer satisfaction levels can help mHealth applications improve their quality of service. Leveraging machine learning techniques for this task can prove to be highly beneficial. However, the existing machine learning methods in this domain are ‘black-box’ and possess several limitations, such as less accuracy, lack of explainability, and many more. To resolve these challenges, the current research introduces a novel approach based on deep transformers and explainable artificial intelligence (EAI). This approach aims to analyze user-generated content to determine mHealth ratings and the factors influencing user satisfaction. The proposed pipeline encompasses several steps, namely, data preprocessing and anonymization, feature extraction, feature selection, transformer architecture building, and evaluating performance using a dataset containing reviews of several different mHealth applications. The sensitivity analysis of the proposed approach is performed by utilizing several feature selection techniques and comparing the prediction performance with existing benchmark solutions available in the literature. From the comparative evaluation, it is observed that the proposed approach outperforms existing techniques by providing 98% accuracy and 99% F1-score, with a 3–5% relative improvement over benchmark solutions. In addition, the proposed method incorporates EAI to determine several crucial factors that affect user satisfaction or app rating scores. This information will be beneficial for the stakeholders in devising better platforms and strategies for enhancing user satisfaction and experience in the mHealth domain.},
  archive      = {J_KIS},
  author       = {Rai, Stuti and Bedi, Jatin and Anand, Ashima},
  doi          = {10.1007/s10115-025-02451-0},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7563-7596},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Decoding user satisfaction: Explainable artificial intelligence-based user-centric analysis of mobile health applications adoption},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature disentanglement, selection, and reaggregation method for multi-task learning. <em>KIS</em>, <em>67</em>(9), 7537-7562. (<a href='https://doi.org/10.1007/s10115-025-02460-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning utilizes the dependencies among tasks to make the tasks boost each other and achieve better results. However, there are not only dependencies but also conflicts among tasks. Task conflict is a critical issue that needs to be addressed to avoid mutual interference among tasks. Previous studies focus on weighting each task to intervene between two or more tasks to reach a compromise. The issue of dependency and conflict among tasks is more complex, where there may be both dependencies and conflicts between two tasks. Simply adjusting weights cannot effectively handle such complex relationships and achieve better results. This paper analyzes the reasons for the task conflicts and discovers that different requirements exist for features in different tasks. For a single task, the features required for other tasks may interfere with the decoder. A new method named feature disentanglement, selection, and reaggregation method is proposed based on the reason for task conflict, which is to disentangle encoder output to obtain high-level features and then select and aggregate high-level features according to the requirements of the task. Experiments show that our method achieves state-of-the-art results on the Multi-Domain Sentiment dataset and 20 Newsgroups dataset. The results prove that our method effectively alleviates conflicts among tasks.},
  archive      = {J_KIS},
  author       = {Liu, Renyuan and Zhang, Xuejie and Wang, Jin and Zhou, Xiaobing},
  doi          = {10.1007/s10115-025-02460-z},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7537-7562},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature disentanglement, selection, and reaggregation method for multi-task learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A priority-based blockchain transaction packaging algorithm in a cloud-edge-end collaboration computing environment. <em>KIS</em>, <em>67</em>(9), 7503-7536. (<a href='https://doi.org/10.1007/s10115-025-02432-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile internet, intelligent IoT, and 5G communication technologies, many IoT devices connect to the Industrial Internet of Things, generating significant data. Blockchain is widely used in identity authentication and trust management due to its reliability. In blockchain technology, transaction packaging is a crucial component of the consensus mechanism and is critical to enhancing fairness and service quality in request processing. However, the flat structure of the blockchain, the necessity for multi-party consensus, and the profit-seeking nature of nodes lead to issues such as unfair transaction processing and prolonged response times in cloud-edge-end architectures, which are critical for empowering intelligent edge applications. To address these challenges, we have refined the blockchain consensus mechanism and introduced a novel packaging algorithm, ITFPA (Improved time-fee packaging algorithm), within the cloud-edge-end environment. This algorithm models the transaction packaging problem as a 0–1 knapsack problem and employs a branch-and-bound method to find the optimal solution. The proposed model considers both the transaction waiting time and transaction fee, using the weighted result of these factors as the priority for the transaction. We compared the proposed algorithm with the WaitTime and TxFee algorithms across four metrics: system fairness, transaction response time, and block priority. The experimental results demonstrate that the proposed algorithm enhances system fairness, reduces transaction response times, and improves service quality to a significant degree.},
  archive      = {J_KIS},
  author       = {Li, Chunlin and Zeng, Haibo and Jiang, Kun and Yang, Kaijun and Yang, Shaohua and Jia, Qingren},
  doi          = {10.1007/s10115-025-02432-3},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7503-7536},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A priority-based blockchain transaction packaging algorithm in a cloud-edge-end collaboration computing environment},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain feature transfer-based multi-domain fake news detection. <em>KIS</em>, <em>67</em>(9), 7473-7501. (<a href='https://doi.org/10.1007/s10115-025-02412-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news is spreading rapidly throughout social media, and is having serious negative consequences on both individuals and society. Currently, fake news detection methods often only predict news for a single domain, neglecting the domain information contained within the text. This may result in an inability to make effective predictions in domains where there is a low quantity or quality of data. With the aid of multi-task learning and transfer learning concepts, this paper presents a domain feature transfer-based multi-domain fake news detection (DFTD). First, we construct a multi-task feature extractor to obtain news text features in different domains. Then, we build an implicit domain gatherer to mine hidden domain information in the news. Next, the domain feature transferor is combined to obtain cross-domain text features. Finally, these features are inputted into the fake news detector for prediction. Our model maintains the extensive association information between domains while segmenting them. Additionally, it employs features from several source domains to aid in determining the authenticity of news in the target domain. Relevant experiments conducted on Weibo21 provide proof of the effectiveness of this model.},
  archive      = {J_KIS},
  author       = {Meng, Xuan and Zhao, Di and Meng, Jiana and Guo, Xu and Ma, Tengfei and Wang, Xiaopei},
  doi          = {10.1007/s10115-025-02412-7},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7473-7501},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Domain feature transfer-based multi-domain fake news detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XLR-KGDD: Leveraging LLM and RAG for knowledge graph-based explainable disease diagnosis using multimodal clinical information. <em>KIS</em>, <em>67</em>(9), 7451-7471. (<a href='https://doi.org/10.1007/s10115-025-02465-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are making a big impact in Artificial Intelligence due to their ability to perform tasks as humans. However, using LLMs in many domain-specific tasks is a relatively unexplored area, specifically in disease diagnosis. This is due to challenges such as multiple modalities in patient clinical information and LLM's high memory and computational power requirements. This study proposes a framework, XLR-KGDD, that overcomes these challenges and performs an LLM-based disease diagnosis. Additionally, XLR-KGDD generates explanations to establish the trust of clinicians and support the diagnosis. These explanations are precise and unambiguous as they adhere to the standard medical guidelines and are presented in natural language. The proposed framework maps multimodal patient clinical information to a patient Knowledge Graph (KG) using the N2K mapper and CheXzero. Prompt Engineering is then applied to create an LLM-compatible input prompt from the patient KG. The framework employs a Parameter Efficient Fine-tuning technique to fine-tune LLM efficiently by optimizing numerical computations and memory requirements. The framework uses Retrieval Augmented Generation to provide standard medical guidelines as context to the LLM, addressing the issue of hallucinations in LLMs and generating coherent explanations. A system based on the XLR-KGDD framework was developed and tested on the multimodal MIMIC-Eye dataset. The LLaMA-3 LLM shows an AUC value of 0.88 and 0.91 in ROC and PR curves, respectively, in diagnosing patients with CHF disease. Furthermore, the system-generated explanations support the diagnosis with evidence from medical guidelines.},
  archive      = {J_KIS},
  author       = {Bedi, Punam and Thukral, Anjali and Dhiman, Shivani},
  doi          = {10.1007/s10115-025-02465-8},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7451-7471},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {XLR-KGDD: Leveraging LLM and RAG for knowledge graph-based explainable disease diagnosis using multimodal clinical information},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic token pruning for LLMs: Leveraging task-specific attention and adaptive thresholds. <em>KIS</em>, <em>67</em>(9), 7431-7450. (<a href='https://doi.org/10.1007/s10115-025-02450-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have achieved state-of-the-art performance across a wide range of natural language processing (NLP) tasks. With their high inference computational costs, deployment is extremely challenging, especially to resource-constrained environments. Dynamic pruning methods, because they are efficient, are likely to assign uniform policies to all tasks and even forget task-specific knowledge and make optimal behavior complicated. To counter this limitation, we propose task-specific dynamic token pruning (TS-DTP), a novel optimization framework that reaches maximum efficiency for LLMs at inference without compromising task-specific performance and, in certain cases, improving upon it. TS-DTP utilizes task-specific knowledge to regulate the token selection process by applying task-specific attention weights and adaptive threshold learning. This approach enables better token importance decision-making through the dynamically adjustable pruning policy according to the downstream task need. It enables very high-grained control, keeping the meaningful contextual information, therefore promoting better performance compared to regular pruning methods. Experimental findings on a variety of NLP tasks (question answering, machine translation, sentiment analysis) validate that TS-DTP achieves extremely large reductions in computational expense and memory demands and similar or marginal gains in accuracy. Our findings are at the forefront of efficient and deployable LLM development and highlight the importance of task adaptation for optimal performance in low-resource settings.},
  archive      = {J_KIS},
  author       = {Ahmadpanah, Seyed Hossein and Sobhanloo, Sanaz and Afsharfarnia, Pania},
  doi          = {10.1007/s10115-025-02450-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7431-7450},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamic token pruning for LLMs: Leveraging task-specific attention and adaptive thresholds},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based advances in sarcasm detection: A study of contextual models and methodologies. <em>KIS</em>, <em>67</em>(9), 7399-7430. (<a href='https://doi.org/10.1007/s10115-025-02469-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm detection is a subset of sentiment analysis and poses significant challenges due to its inherent linguistic complexity and the contextual subtleties required for accurate interpretation. In this paper, we present a comprehensive survey of the current state of sarcasm detection, covering models from traditional machine learning to large language models. Our review examines primary datasets and corpora used for training and evaluation, evaluates the effectiveness of existing sentiment analysis techniques, and highlights predominant methodologies. Additionally, we discuss the challenges and limitations facing sarcasm detection, including issues related to linguistic complexity and contextual interpretation. We compared all datasets, and how they impact the model performance and generalizability, assess the ability of sentiment analysis techniques to capture sarcasm and irony, and explore the strengths and limitations of leading methodologies. By exploring current limitations, including cross-cultural variances and the adaptability of deep learning models, this survey underscores ongoing challenges and highlights future directions in AI-driven sarcasm detection research.},
  archive      = {J_KIS},
  author       = {Bodige, Ramakrishna and Akarapu, Rameshbabu and Poladi, Pramod Kumar},
  doi          = {10.1007/s10115-025-02469-4},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7399-7430},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Transformer-based advances in sarcasm detection: A study of contextual models and methodologies},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative analysis of predictive process monitoring: Object-centric versus classical event logs. <em>KIS</em>, <em>67</em>(9), 7355-7398. (<a href='https://doi.org/10.1007/s10115-025-02461-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive Process Monitoring (PPM) techniques are emerging as part of the more general research scenario of Process Mining (PM). They play a crucial role in the continuously evolving process of digital transformation by constantly supporting the organizational decision-making processes providing (accurate) predictions on the future behavior of processes. The state of the art of PPM application methodologies is mainly focused on Single ID Event Logs, commonly known as Traditional Event Logs or Classical Event Logs. As a matter of fact, the importance of Object-Centric Event Logs (OCEL) is being increasingly recognized as many emerging PPM approaches benefited of the usage of OCEL by obtaining a significative increase of the prediction accuracy. This survey aims to explore the current proposals in the context of OCEL-based PPM approaches. More in detail, we contribute to the state of the art by adding new classification features by differentiating between the approaches based on the input Event Log (Traditional or OCEL). We also analyzed the existing literature considering the prediction task addressed, the methodology used, the specific contribution area they addressed and the application domain.},
  archive      = {J_KIS},
  author       = {Fioretto, Simona and Masciari, Elio},
  doi          = {10.1007/s10115-025-02461-y},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7355-7398},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A comparative analysis of predictive process monitoring: Object-centric versus classical event logs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chatbot development strategies: A review of current studies and applications. <em>KIS</em>, <em>67</em>(9), 7319-7354. (<a href='https://doi.org/10.1007/s10115-025-02462-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chatbots have become increasingly popular by transforming interactions across numerous fields. As the technology behind chatbots has rapidly developed, new methodologies have arisen, each contributing unique strengths and addressing different challenges. This paper systematically reviews the methods used in chatbot development from 2019 to 2024, comprehensively analyzing the studies. We categorize the techniques into three main groups: machine learning (ML)-based, deep learning (DL)-based, and large language model (LLM)-based methods. We present a broad and inclusive survey by exploring the foundational principles of chatbot technologies and their applications across diverse domains such as education, healthcare, and interviews. Our analysis reveals that while traditional ML-based methods remain widely used, DL models are gaining prominence for handling complex tasks, and LLM-based systems are advancing the field by offering more coherent, contextually aware responses. However, challenges remain, especially in ethical concerns like hallucination and privacy-preserving technologies, particularly with LLMs. The paper also identifies gaps in existing research, notably the need for improved privacy-preserving mechanisms and better strategies for mitigating hallucinations in chatbot responses. Future research directions are suggested to address these challenges, particularly in developing LLM-based chatbots, with a focus on enhancing privacy, accuracy, and ethical standards.},
  archive      = {J_KIS},
  author       = {Yigit, Gulsum and Bayraktar, Rabia},
  doi          = {10.1007/s10115-025-02462-x},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {7319-7354},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Chatbot development strategies: A review of current studies and applications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DecodEM-X: Advancing multimodal meme moderation with robust AI frameworks. <em>KIS</em>, <em>67</em>(8), 7295-7317. (<a href='https://doi.org/10.1007/s10115-025-02439-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of memes on social media, blending images with text to convey complex messages, presents unique challenges in content moderation, particularly when identifying harmful content. This paper introduces DecodEM-X, an innovative framework designed to enhance the detection of harmful memes through advanced multimodal analysis. Utilizing the Facebook Hateful Memes dataset, DecodEM-X integrates cutting-edge techniques such as RoBERTa and ResNet50 for robust text and image processing, coupled with a novel cross-attention mechanism that optimizes the synergy between textual and visual modalities. This approach considerably improves the clarification of nuanced, context-dependent interactions within memes. Furthermore, the framework employs SHAP for explainability, ensuring clearness and answerability in automated decision-making processes. Our extensive evaluations demonstrate that DecodEM-X not only outperforms existing baseline models with an accuracy of 89.46% and an AUROC of 90.37% but also provides interpretable insights into the classification decisions. This study highlights the potential of DecodEM-X to set new standards in AI-driven content moderation, promising enhanced reliability and ethical compliance in real-world applications.},
  archive      = {J_KIS},
  author       = {Arslan, Hafiz Muhammad and Zhenhua, Tan},
  doi          = {10.1007/s10115-025-02439-w},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {7295-7317},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {DecodEM-X: Advancing multimodal meme moderation with robust AI frameworks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient healthcare data classification through gazelle-inspired optimization and multimodal deep learning. <em>KIS</em>, <em>67</em>(8), 7263-7293. (<a href='https://doi.org/10.1007/s10115-025-02424-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid digitization of healthcare systems has led to an unprecedented surge in healthcare data, necessitating robust and automated methods for data classification. In this study, an innovative approach to automated healthcare big data classification is presented by combining the gazelle optimization algorithm with multimodal deep learning techniques. The research problem addressed revolves around the need for accurate and efficient classification of diverse healthcare data sources, such as patient records, diagnostic measurements, and clinical notes. Automated classification is crucial for early disease detection, treatment recommendation, and resource allocation. However, this task is inherently challenging because healthcare data are diverse and it needs reliable classification models. The proposed methodology leverages multimodal deep learning models, including deep echo state networks for sequential records and deep fusion models for structured and unstructured text. This multimodal approach enables us to effectively capture and integrate information from diverse healthcare data sources. The DL technique improves the computational efficiency and prevents the vanishing gradient problem. To optimize the models and enhance classification accuracy, the gazelle optimization algorithm is introduced that dynamically adapts model configurations during training. Gazelle optimization algorithm (GOA) optimizes hyperparameters by mimicking the efficient and agile characteristics of the gazelle. This proposed approach yielded 98.57% accuracy, 97.02% recall, 96.89% precision, and 98.31% F1 score in the healthcare classification. The proposed approach paves the way for more accurate and efficient healthcare data classification, ultimately benefiting healthcare providers and patients.},
  archive      = {J_KIS},
  author       = {Jaya, J. Arockia and Mahalakshmi, K.},
  doi          = {10.1007/s10115-025-02424-3},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {7263-7293},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient healthcare data classification through gazelle-inspired optimization and multimodal deep learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fetal health risk prediction using ensemble-based machine learning approaches. <em>KIS</em>, <em>67</em>(8), 7227-7261. (<a href='https://doi.org/10.1007/s10115-025-02442-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2017, WHO reported a rise in neonatal and maternal deaths in developing countries. Congenital impairments, bad maternal lifestyle, hunger, resource shortages, and absence of fetal health check-ups cause newborn, maternal, and postnatal mortality. To tackle the above concerns in a cost-effective and efficient approach, we tried to predict the risks of fetal health based on machine learning (ML) techniques and categorize them into three classes: pathological, suspect, and normal. Consequently, we can make informed decisions if the fetus exhibits critical issues. The study uses a cardiotocography dataset with 2126 cases, 21 independent features, and a multiclass target attribute in a three-step process to predict the health risk of the fetus. The first stage deploys sixteen ML models following the necessary data preprocessing, both with and without sampling methods. In the second stage, we selected eleven of the best performing classifiers to perform hyperparameter tuning using RandomizedSearchCV and GridSearchCV. Finally, we proposed a fine-tuned ensemble learning (EL) approach based on stacking, voting, and bagging for the precise fetal health status prediction. These models allow early diagnosis of fetal health conditions to be identified and treated with minimal time and expense. It was observed that the proposed stacking, bagging, and voting methods gave ten-fold mean accuracies of 99.60%, 99.72%, and 99.78%, respectively. Using ensemble classifiers can improve and eliminate the weakness of one individual classifier, yielding superior performance for the overall model. This study readily offers healthcare providers useful interpretations to aid in their decision-making.},
  archive      = {J_KIS},
  author       = {Mondal, Subhash and Maity, Ranjan and Nag, Amitava and Ghosh, Soumadip},
  doi          = {10.1007/s10115-025-02442-1},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {7227-7261},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fetal health risk prediction using ensemble-based machine learning approaches},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Association analysis using the mining of positive and negative association rules. <em>KIS</em>, <em>67</em>(8), 7193-7225. (<a href='https://doi.org/10.1007/s10115-025-02445-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Association analysis using the mining of positive and negative association rules (PNARs) was so far primarily based on mining rules of forms $$A\Rightarrow B$$ , $$A \Rightarrow \lnot B$$ , $$\lnot A \Rightarrow B$$ , and $$\lnot A \Rightarrow \lnot B$$ . Most existing algorithms for mining PNARs usually exploit the upward closure property of negative itemsets, while few exploit the downward closure property. PNARs mined by algorithms built in the first exploitation method are not really suitable for human thinking and for explaining association analysis. PNARs mined by algorithms built in the second exploitation method are not intuitively described as the PNARs above. Such rules are consistent with human thinking, but generally, they are just positive and negative dependency relationships. Thus, they are confusing and difficult to understand. So far, no existing algorithm can find all valid PNARs. This work proposes an algorithm based solely on (positive) items in transaction databases under the second exploitation to mine PNARs. The algorithm is developed under the support-confidence framework on equivalence classes. Two steps of the association rule (AR) mining process, discovering frequent itemsets and generating ARs from these itemsets, are executed concurrently. The algorithm is sound and complete. Its computational complexity is also estimated. The experiment shows the application prospect of the proposed algorithm in the association analysis of co-occurrence and non-co-occurrence events.},
  archive      = {J_KIS},
  author       = {Do Van, Thanh and Dinh Thi, Ha and Truong Duc, Phúóng},
  doi          = {10.1007/s10115-025-02445-y},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {7193-7225},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Association analysis using the mining of positive and negative association rules},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing recommendation systems with collaborative filtering and sentiment analysis: Dimensionality reduction for improved content-based approaches. <em>KIS</em>, <em>67</em>(8), 7157-7191. (<a href='https://doi.org/10.1007/s10115-025-02452-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recommendation system plays a pivotal role in assisting users by providing personalized suggestions based on their preferences. However, these systems often neglect the crucial aspect of user sentiment. To address this limitation, this work focuses on integrating sentiment analysis into recommendation systems. Using a dataset of 5000 reviews from Yelp, we employ preprocessing tasks such as Stop Words Removal, Tokenization, Part-of-Speech Tagging, WordNet Part-of-Speech Information, and Lemmatization to facilitate sentiment analysis. Leveraging the power of BERT, our sentiment analysis model achieves an impressive accuracy rate of 91%. We then incorporate the sentiment analysis component into collaborative filtering, utilizing cosine similarity and significantly reducing the root mean square error (RMSE) to 1.2478. Additionally, we integrate sentiment analysis into content-based recommendation using clustering, resulting in improved recommendations with a higher silhouette score of 0.2270. To further enhance the system’s performance, we propose a novel approach that combines these sentiment-aware components using NMF with DecisionTreeRegressor, resulting in an even lower RMSE of 1.1955. This integration of sentiment analysis into the recommendation system demonstrates its effectiveness in improving accuracy and personalization, providing users with more meaningful and relevant recommendations based on their sentiments.},
  archive      = {J_KIS},
  author       = {Darraz, Nossayba and Karabila, Ikram and El-Ansari, Anas and Alami, Nabil and El Mallahi, Mostafa},
  doi          = {10.1007/s10115-025-02452-z},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {7157-7191},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing recommendation systems with collaborative filtering and sentiment analysis: Dimensionality reduction for improved content-based approaches},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical dependency analysis (NDA): A new method for estimating the statistical dependence (not correlation) of two variables. <em>KIS</em>, <em>67</em>(8), 7123-7155. (<a href='https://doi.org/10.1007/s10115-025-02436-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dependence and correlation represent distinct statistical concepts. While there are methods to measure linear and nonlinear correlation between two variables, understanding the statistical dependence between them remains a topic of great interest. In this paper, we propose a heuristic, numerical, and algorithmic approach to estimate the dependence coefficient between two variables. With this approach, first, the X–Y scatter plot is transformed into a functional scatter plot using a procedure called “functionalizing.” Next, a novel concept called “successive triangles” is employed to estimate the dependence of Y on X. The proposed method offers several advantages; it is distribution-free, so it is suitable for both Gaussian and non-Gaussian numerical variables. Moreover, it can be used for both numerical and categorical (nominal) variables. This approach can be employed in other applications such as correlation measurement and also template matching for single-dimensional patterns. The presented method has been validated by both the simulated and clinical data with promising results.},
  archive      = {J_KIS},
  author       = {Zanghaei, Abolfazl and Doosti, Hassan and Ameri, Ali},
  doi          = {10.1007/s10115-025-02436-z},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {7123-7155},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Numerical dependency analysis (NDA): A new method for estimating the statistical dependence (not correlation) of two variables},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learn to unlearn: Meta-learning-based knowledge graph embedding unlearning. <em>KIS</em>, <em>67</em>(8), 7101-7122. (<a href='https://doi.org/10.1007/s10115-025-02407-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding (KGE) methods map entities and relations from knowledge graphs to continuous vector spaces, simplifying their representations and enhancing performance across various tasks (e.g., link prediction, question answering). As concerns about personal privacy rise, machine unlearning (MU), an emerging artificial intelligence technology that enables models to eliminate the influence of specific data, has garnered increasing attention from the academic community. The existing KGE unlearning works mainly achieve MU through data obfuscation and adjustments to the model’s training loss. Furthermore, existing approaches lack generalization ability across different unlearning tasks. In this paper, we propose a Meta-Learning-Based Knowledge Graph Embedding Unlearning framework (MetaEU), specifically designed for KGE unlearning. With the help of meta-learning, the model can discover the inherent relationships between different unlearning tasks, thereby avoiding the need to start from scratch for each unlearning task and achieving better generalization across various task scenarios. A thorough experimental study on benchmark datasets shows that MetaEU demonstrates promising performance in the knowledge graph embedding unlearning task.},
  archive      = {J_KIS},
  author       = {Xu, Naixing and Li, Qian and Li, Zhaochuan and Wang, Xu and Liu, Bingchen and Mpofu, Jabulani Brown and Li, Jingchen and Li, Xin},
  doi          = {10.1007/s10115-025-02407-4},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {7101-7122},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learn to unlearn: Meta-learning-based knowledge graph embedding unlearning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DKFM: A novel data and knowledge fusion-driven model for difficulty prediction of mathematical exercise. <em>KIS</em>, <em>67</em>(8), 7079-7099. (<a href='https://doi.org/10.1007/s10115-025-02438-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online education systems, exercise difficulty prediction holds significant importance for various applications, including personalized exercise recommendation and evaluation of learners’ knowledge proficiency. Traditional methods are mainly based on the manual labeling by experts, which is labor-costing and time-consuming. Accurately predicting the difficulty of exercises is a challenging task, since it is hard to simultaneously mine the rich semantic features existed in the texts and the objective knowledge depth information. To address this issue, we propose a novel data and knowledge fusion-driven model (DKFM) for accurately predicting the difficulty of mathematical exercises. Specifically, we first design a large-scale pre-trained model-based layer to extract semantic features for each exercise. Subsequently, we construct a Mathematical Knowledge Base that enables automated extraction of knowledge depth information associated with the exercises. To consider the varying contributions of different types of features toward the final prediction, we propose an attention-based fusion approach to learn the coefficients for diverse features. Experimental evaluations illustrate that compared with GPT-3, our proposed DKFM achieves an improvement of 22.5% on Algebra dataset, 25.5% on Geometry dataset and 38.8% on Counting dataset in terms of F1-score.},
  archive      = {J_KIS},
  author       = {Duan, Zhiyi and Gu, Hengnian and Wang, Di and Zhou, Dongdai},
  doi          = {10.1007/s10115-025-02438-x},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {7079-7099},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {DKFM: A novel data and knowledge fusion-driven model for difficulty prediction of mathematical exercise},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical attention neural network with multi-view fusion for online course recommendation. <em>KIS</em>, <em>67</em>(8), 7057-7077. (<a href='https://doi.org/10.1007/s10115-025-02441-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-evolving landscape of online education has spawned numerous course recommendation methods. Yet, these methods have neglected learners’ preferences for multi-view course data, including the course content, concepts, instructors, institutions hosting the courses, and their relations. Additionally, they have overlooked the influence of various elements within different views on learners’ course selection behavior. Consequently, the integration of multi-view course data and distinct elements within each view into a comprehensive recommendation method remains a complex challenge. To tackle this issue, we propose a novel solution—a hierarchical attention neural network (HANN) for effective online course recommendation. HANN incorporates a course encoder, a user encoder, and a recommendation generator. The course encoder leverages two levels of attention to integrate multiple views of information, thereby generating comprehensive course representations. The user encoder employs bidirectional long short-term memory and another level of attention mechanism to acquire user representations based on their learning records. Lastly, the recommendation generator calculates recommendation probabilities by feeding user and course representations into a multi-layer perceptron. Experimentation with real-world data validates the effectiveness of HANN. Our results show the multi-view course data and attention mechanisms to be useful in online course recommendation.},
  archive      = {J_KIS},
  author       = {Deng, Weiwei and Chen, Han and Lu, Liuxing and Zhu, Peihu and Wu, Jie},
  doi          = {10.1007/s10115-025-02441-2},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {7057-7077},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hierarchical attention neural network with multi-view fusion for online course recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence and machine learning in cybersecurity: A deep dive into state-of-the-art techniques and future paradigms. <em>KIS</em>, <em>67</em>(8), 6969-7055. (<a href='https://doi.org/10.1007/s10115-025-02429-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of artificial intelligence (AI) and machine learning (ML) into cybersecurity has driven a transformational shift, significantly enhancing the ability to detect, respond to, and mitigate complex cyber threats. Traditional defense mechanisms are increasingly inadequate against sophisticated attacks, necessitating the adoption of AI-driven security solutions. This review paper presents a novel, in-depth analysis of state-of-the-art AI and ML techniques applied to intrusion detection, malware classification, behavioral analysis, and threat intelligence. Unlike existing studies, this work not only synthesizes current advancements but also identifies key limitations and emerging research gaps in AI-powered cybersecurity. A key novelty of this paper lies in its comprehensive evaluation of adversarial defense mechanisms, addressing how AI models can be hardened against adversarial attacks and data manipulation techniques. Additionally, we explore the growing role of federated learning in collaborative threat intelligence, offering privacy-preserving security models that enhance real-time cyber defense across decentralized networks. Another major contribution is our discussion on the integration of AI with quantum computing for cryptographic resilience, as well as its convergence with IoT security, shaping the next generation of adaptive cybersecurity frameworks. Furthermore, this paper proposes a forward-looking roadmap for sustainable AI-driven cybersecurity, emphasizing the need for adaptive adversarial defense systems, federated learning for global threat mitigation, and AI-enhanced cyber resilience frameworks. By bridging the gap between current AI-driven security solutions and future paradigms, this work serves as a valuable resource for researchers, cybersecurity professionals, and policymakers aiming to develop intelligent, scalable, and resilient cybersecurity architectures. While AI and ML are reshaping modern cybersecurity, their effectiveness hinges on continuous innovation, adversarial robustness, and interdisciplinary collaboration to combat an ever-evolving threat landscape.},
  archive      = {J_KIS},
  author       = {Mohamed, Nachaat},
  doi          = {10.1007/s10115-025-02429-y},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6969-7055},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Artificial intelligence and machine learning in cybersecurity: A deep dive into state-of-the-art techniques and future paradigms},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label-consistent kernel transform learning-based sparse hashing for cross-modal retrieval. <em>KIS</em>, <em>67</em>(8), 6937-6967. (<a href='https://doi.org/10.1007/s10115-025-02419-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of diverse multi-modal data, cross-modal retrieval systems, which allow users to query and retrieve results across different modalities, have gained significant popularity. Consequently, this field has attracted increased research attention and also has made substantial progress due to advancements in machine learning technologies. However, existing cross-modal retrieval systems face challenges in learning discriminative representations, as they project raw heterogeneous data linearly onto a common projection space and fail to capture the semantic similarity of multi-modal data. To address this, we propose a label-consistent kernel transform learning-based sparse hashing (LcKTLSH) method for cross-modal retrieval. Instead of directly projecting raw data samples, LcKTLSH learns transform and sparse coefficients suited for projection onto the kernel space. The learned sparse coefficients are embedded into pre-learned hash codes to generate the final hash codes. Additionally, the kernel trick accounts for nonlinearity in the data, and the label-consistency constraint provides supervision. We evaluated the proposed LcKTLSH method on four datasets containing text-image pairs and compared it with nine popular baseline approaches. The experimental results strongly support the effectiveness of LcKTLSH, primarily due to the introduced kernelization and transform embedding components, as validated by ablation studies.},
  archive      = {J_KIS},
  author       = {Maggu, Jyoti and Goel, Anurag and Kumar, Rajeev},
  doi          = {10.1007/s10115-025-02419-0},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6937-6967},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Label-consistent kernel transform learning-based sparse hashing for cross-modal retrieval},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining self-supervision and privileged information for representation learning from tabular data. <em>KIS</em>, <em>67</em>(8), 6907-6935. (<a href='https://doi.org/10.1007/s10115-025-02418-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When building predictive models for real-world applications, many data are discarded because conventional learning algorithms cannot utilize it, although such data could be very informative. This paper focuses on representation learning using two types of additional data: privileged information (PI) and unlabeled data. PI refers to data available only during training but not at test time. Existing methods transfer the knowledge embedded in PI via supervised mechanisms, making them unable to use unlabeled data. In contrast, self-supervised learning methods can use unlabeled data but cannot learn from PI. While these techniques appear complementary, as we demonstrate, combining them is non-trivial. This paper introduces the privileged information regularized (PIReg) self-supervised learning framework, which utilizes both PI and unlabeled data to learn better representations.},
  archive      = {J_KIS},
  author       = {Yang, Haoyu and Steinbach, Michael and Melton, Genevieve and Kumar, Vipin and Simon, Gyorgy},
  doi          = {10.1007/s10115-025-02418-1},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6907-6935},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Combining self-supervision and privileged information for representation learning from tabular data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-grained feature gating fusion network for multimodal sentiment analysis. <em>KIS</em>, <em>67</em>(8), 6879-6905. (<a href='https://doi.org/10.1007/s10115-025-02446-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of AI-generated content, enabling machines to accurately infer and recognize human emotions has become crucial for applications such as public opinion monitoring, product optimization, and enhancing user experience. However, existing multimodal sentiment analysis methods predominantly rely on unimodal pre-trained models (e.g., BERT for textual features and ResNet for visual features) and perform feature fusion using simple concatenation or weighted summation. Such approaches fail to effectively model the correlations between modalities, often leading to conflicts in sentiment decisions across different modalities, which ultimately impacts overall performance. To address these limitations, this paper proposes a multi-granularity feature gated fusion network (MFGFN) that combines unimodal and multimodal pre-trained models. The network incorporates a co-attention encoder interaction module to integrate fine-grained unimodal features and coarse-grained cross-modal features, thereby enhancing semantic alignment across modalities. A gated fusion mechanism is introduced to adjust information distribution through similarity-weighted operations, enabling cross-modal interaction and cross-granularity feature integration. Additionally, a mutual information maximization mechanism is employed to enhance information sharing while preserving modality-specific correlations during the fusion process. Experimental results demonstrate that MFGFN significantly outperforms existing methods across multiple datasets. On the MVSA-Single dataset, MFGFN achieves improvements of 0.89% and 1.92% in accuracy and F1-score compared to the best baseline, respectively. Furthermore, it achieves F1-score gains of 0.11% and 0.06% on the MVSA-Multiple and MMSD datasets, respectively, further validating its effectiveness and generalizability in multimodal sentiment analysis tasks.},
  archive      = {J_KIS},
  author       = {Yu, Bengong and Li, Chenyue and Shi, Zhongyu},
  doi          = {10.1007/s10115-025-02446-x},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6879-6905},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-grained feature gating fusion network for multimodal sentiment analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSBC-SNet: A novel graph-aware bidirectional contrastive semantic network for multilabel text classification. <em>KIS</em>, <em>67</em>(8), 6845-6877. (<a href='https://doi.org/10.1007/s10115-025-02431-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel text classification (MLTC) requires capturing the relationships between texts and labels. Unlike single-label tasks, it has a higher label co-occurrence rate. We propose GSBC-SNet, an end-to-end graph convolutional network. It fuses the label information of GCN and the features of RoBERTa through a unique attention mechanism, breaking the traditional fusion approach. Its adaptive adjacency matrix, designed by a novel algorithm, focuses on the actual label correlations in the text rather than just co-occurrence. This matrix can reduce noise and alleviate the long-tail effect in label distribution, adapting to diverse data. The positive–negative sample mechanism optimizes the learning process by leveraging the synergy among labels, context, and the matrix. Experiments on the AAPD, RCV1, and Reuters-21578 datasets demonstrate that GSBC-SNet outperforms existing methods and achieves better scores in some metrics on the AAPD dataset. This research enhances the model performance and provides new insights for handling label correlations, thus promoting the development of the MLTC field.},
  archive      = {J_KIS},
  author       = {Pang, Rui and Zhang, Qiongbing and Lin, Yating and Ouyang, Liang and Cui, Zhangwei},
  doi          = {10.1007/s10115-025-02431-4},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6845-6877},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GSBC-SNet: A novel graph-aware bidirectional contrastive semantic network for multilabel text classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChatGPT struggles to recognize reproducible science. <em>KIS</em>, <em>67</em>(8), 6825-6843. (<a href='https://doi.org/10.1007/s10115-025-02428-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of answers provided by ChatGPT matters with over 100 million users and approximately 1 billion monthly website visits. Large language models have the potential to drive scientific breakthroughs by processing vast amounts of information in seconds and learning from data at a scale and speed unattainable by humans, but recognizing reproducibility, a core aspect of high-quality science, remains a challenge. Our study investigates the effectiveness of ChatGPT (GPT $$-$$ 3.5) in evaluating scientific reproducibility, a critical and underexplored topic, by analyzing the methods sections of 158 research articles. In our methodology, we asked ChatGPT, through a structured prompt, to predict the reproducibility of a scientific article based on the extracted text from its methods section. The findings of our study reveal significant limitations: Out of the assessed articles, only 18 (11.4%) were accurately classified, while 29 (18.4%) were misclassified, and 111 (70.3%) faced challenges in interpreting key methodological details that influence reproducibility. Future advancements should ensure consistent answers for similar or same prompts, improve reasoning for analyzing technical, jargon-heavy text, and enhance transparency in decision-making. Additionally, we suggest the development of a dedicated benchmark to systematically evaluate how well AI models can assess the reproducibility of scientific articles. This study highlights the continued need for human expertise and the risks of uncritical reliance on AI.},
  archive      = {J_KIS},
  author       = {Chang, Jose R. and Nordling, Torbjörn E. M.},
  doi          = {10.1007/s10115-025-02428-z},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6825-6843},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {ChatGPT struggles to recognize reproducible science},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSAD-GNN: Multi-head spectral attention-driven deep propagation graph neural network. <em>KIS</em>, <em>67</em>(8), 6803-6824. (<a href='https://doi.org/10.1007/s10115-025-02406-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limited representational capacity of static topology matrices and graph transformers, existing graph neural networks (GNNs) perform poorly in capturing the complex relationships between nodes and high-frequency features. To overcome these limitations, this paper proposes a multi-head spectral attention-driven deep propagation graph neural network (MSAD-GNN), which opens new possibilities for implementing an adaptive frequency-aware GNN architecture. Firstly, a multi-head adaptive spectral filter is designed to enhance the capturing of high-frequency features by adjusting the frequency response. Subsequently, a spectral-aware matrix is introduced to adapt the node relationship weights, effectively capturing complex dependencies, thereby constructing a more expressive graph convolution. Based on this, node frequency learning is used to dynamically balance spectral information, and adaptive Mixup is integrated to address the generalization issue. Extensive evaluations on real benchmark datasets show that MSAD-GNN achieves comparable or superior performance on both homophilic and heterophilic graphs.},
  archive      = {J_KIS},
  author       = {Lv, Yonghao and Xiong, Hui and Liu, Jinzhen},
  doi          = {10.1007/s10115-025-02406-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6803-6824},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MSAD-GNN: Multi-head spectral attention-driven deep propagation graph neural network},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-strategy chimp optimization algorithm for solving global and constraint engineering problems. <em>KIS</em>, <em>67</em>(8), 6753-6802. (<a href='https://doi.org/10.1007/s10115-025-02422-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The chimp optimization algorithm (ChOA) is a recently introduced metaheuristic algorithm inspired by nature. This algorithm identified four types of chimpanzee groups: attacker, barrier, chaser, and driver, and proposed a suitable mathematical model for them, based on the various intelligence and sexual motivations of chimpanzees. However, this algorithm is not more successful in the convergence rate and escaping of the local optimum trap in solving high-dimensional problems. Although it and some of its variants use some strategies to overcome these problems, it is observed that it is not sufficient. Therefore, in this study, a newly expanded variant is described. In the algorithm, called Ex-ChOA, hybrid models are proposed for position updates of search agents, and a dynamic switching mechanism is provided for transition phases. This flexible structure solves the slow convergence problem of ChOA and improves its accuracy in multi-dimensional problems. Therefore, it tries to achieve success in solving global, complex, and constrained problems. The performance of the proposed algorithm was analyzed on a total of 34 benchmark functions and a total of 17 real-world optimizations, including classical, constrained, and modern engineering problems. According to the results obtained, the proposed algorithm performs better or equivalent performance than the compared algorithms.},
  archive      = {J_KIS},
  author       = {Anka, Ferzat},
  doi          = {10.1007/s10115-025-02422-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6753-6802},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A multi-strategy chimp optimization algorithm for solving global and constraint engineering problems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing link prediction in dynamic social networks through hybrid GCN-LSTM models. <em>KIS</em>, <em>67</em>(8), 6717-6751. (<a href='https://doi.org/10.1007/s10115-025-02430-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks, represented as graphs with users as vertices and relationships as edges, are inherently complex and dynamic. Link prediction that forecasts future connections in social networks is vital for platforms like Facebook, WeChat, and LinkedIn. However, as social networks evolve, they exhibit temporal dynamics that pose significant challenges for link prediction, especially when dealing with large-scale networks and a multitude of parameters. Traditional methods often struggle to simultaneously capture both structural and temporal aspects. To overcome these limitations, this paper proposes a novel hybrid model that integrates graph convolutional networks (GCNs) and long short-term memory (LSTM) networks. The GCNs effectively capture the structural information of the network, while the LSTMs model the temporal evolution of connections, improving prediction accuracy. GCNs provide a robust representation of the network's structure, while LSTMs process these representations over time to model temporal dynamics. Experiments conducted on the MUSAE GitHub social network dataset and the KARTAE dataset demonstrate the model's superior performance, achieving accuracies of 92.8% and 98.7%, respectively. Our approach not only simplifies the link prediction process but also outperforms state-of-the-art models, showcasing its potential for broader applications in dynamic social networks. Additionally, the model’s scalability and adaptability to diverse datasets highlight its versatility for real-world applications. Future work will focus on optimizing the model for larger networks, exploring hybrid architectures, and expanding its application to real-time prediction tasks.},
  archive      = {J_KIS},
  author       = {Choudhary, Sonia and Kumar, Gyanender},
  doi          = {10.1007/s10115-025-02430-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6717-6751},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing link prediction in dynamic social networks through hybrid GCN-LSTM models},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generic framework for mining sequences with various interestingness measures in dynamic attributed graphs. <em>KIS</em>, <em>67</em>(8), 6689-6716. (<a href='https://doi.org/10.1007/s10115-025-02421-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining various patterns in dynamic graphs is a crucial task across many domains, including social networks, web analysis, and epidemiology. This paper addresses the challenge of mining multiple attributes associated with vertices in evolving large graphs over time. Current approaches to mining attributed sequences in graph databases often overlook inter-element correlations or rely on user-defined measures to prune, limiting their universality. To address this, we present a generic framework for extracting graph sequences with various interestingness measures in dynamic attributed graphs by extending support and confidence concepts. The novel support with the anti-monotonic property reduces the search space. Additionally, we introduce adaptable support and confidence measures tailored to graph sequence mining. The proposed IS-Miner algorithm liberates the pruning strategy from specific interestingness measures. Experimental evaluations on both real-life and synthetic databases validate the efficiency and effectiveness of our approach, particularly in scenarios involving multiple attributes.},
  archive      = {J_KIS},
  author       = {He, Cheng and Cai, Jiayu and Chen, Guoting and Gan, Wensheng},
  doi          = {10.1007/s10115-025-02421-6},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6689-6716},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A generic framework for mining sequences with various interestingness measures in dynamic attributed graphs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection for semi-supervised sentiment analysis of e-commerce reviews using CNN and neutrosophic fuzzy parameters. <em>KIS</em>, <em>67</em>(8), 6645-6687. (<a href='https://doi.org/10.1007/s10115-025-02440-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization is at the heart of machine learning refinement in the age of big data and AI. Although natural language processing (NLP) has achieved remarkable progress, sentiment analysis in semi-supervised settings still shows a significant challenge in having limited labeled data. Semi-supervised learning can be used to overcome this limitation as it utilizes both labeled and unlabeled data, yet the success of the semi-supervised model is highly dependent on feature selection that reduces noise, increases interpretability, and improves classification accuracy. Classical feature selection methods struggle with providing a finer clarity as well as redundancy detection, it requires implementing supervised feature refinement approaches for better sentiment classification. To resolve these issues, the proposed work introduces a semi-supervised e-commerce review sentiment analysis framework utilizing Neutrosophic Fuzzy Chi-Square (NFCS) for uncertainty-aware feature selection, as well as the Firefly Algorithm for feature optimization. For text-based opinions, the experimental progress of CNN architectures such as AlexNet and VGGNet (designed for image processing tasks) that use text-based sentiment classification and hierarchical feature representations to learn text-based rules that summarize and localize textual dependencies. This framework consists of self-training and co-training-based methods that improve the process. A conditional probability-based validation mechanism is further used to prevent misclassification during the pseudo-labeling process, so as to verify reliable knowledge transfer from unlabeled data. Extensive experimental comparisons establish the combined use of deep learning-based feature extraction, Neutrosophic Fuzzy feature selection, and optimization using Firefly Algorithm have a significant impact on sentiment classification. The main novelty of this work results from the incorporation of uncertainty-aware feature selection in a metaheuristic optimization framework integrated with deep learning within a semi-supervised scenario that leads to both a scalable and interpretable sentiment analytical approach with low computational cost for e-commerce implementations.},
  archive      = {J_KIS},
  author       = {Jena, Alok Kumar and Gopal, K. Murali and Tripathy, Abinash and Tripathy, Siba Prasada},
  doi          = {10.1007/s10115-025-02440-3},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6645-6687},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature selection for semi-supervised sentiment analysis of e-commerce reviews using CNN and neutrosophic fuzzy parameters},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient federated recommender system based on slimify module and feature sharpening module. <em>KIS</em>, <em>67</em>(8), 6611-6644. (<a href='https://doi.org/10.1007/s10115-025-02433-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of communication networks and smart devices, research in the field of recommender systems has garnered widespread interest among researchers. The prediction of click-through rate (CTR) holds significant importance inside recommender systems. Many current CTR prediction approaches face the following challenges: first, they typically require centralized storage of user behavioral data to create accurate user models. However, users are highly sensitive to privacy, and privacy concerns and related data protection regulations make it difficult to aggregate behavioral data across platforms. Second, most CTR prediction models adopt large-scale deep models, whose substantial size hinders their broader application. To address these issues, we propose the efficient federated recommender system based on Slimify Module and Feature Sharpening Model (FedMS). The novelty of the proposed approach lies in its ability to effectively combine feature representations from several platforms in a manner that ensures privacy preservation, while also minimizing the communication overhead and storage requirements of the model. FedMS comprises two modules. The Slimify Module reduces the model’s size through inner product substitution and parameter reduction achieved by stacking compression layers. The Feature Sharpening Module enhances the importance of different field embeddings using position-based attention mechanisms. Furthermore, we introduce local differential privacy to further mitigate user privacy leakage. The empirical results obtained from conducting extensive experiments on three publicly available datasets provide evidence that FedMS has exceptional performance in predicting CTR, while also requiring a reduced amount of training time. Using RelaImpr as the evaluation metric, it achieves improvements of 11.04%, 3.38%, and 4.82% on the Criteo, Avazu, and MovieLens datasets, respectively.},
  archive      = {J_KIS},
  author       = {Di, Yicheng and Shi, Hongjian and Fan, Jiansong and Bao, Jiayu and Huang, Gaoyuan and Liu, Yuan},
  doi          = {10.1007/s10115-025-02433-2},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6611-6644},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient federated recommender system based on slimify module and feature sharpening module},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hypergraph embedded entropy approach to gene selection. <em>KIS</em>, <em>67</em>(8), 6577-6609. (<a href='https://doi.org/10.1007/s10115-025-02423-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the hypergraph embedded entropy approach for gene selection (HEEGS), an innovative unsupervised algorithm designed to enhance clustering performance in high-dimensional biological data. HEEGS achieves this by preserving local structure through the integration of cluster indicators and entropy measures, enabling efficient data processing and the identification of distinct, biologically meaningful clusters. Unlike conventional methods that rely on the Laplacian matrix for pattern similarity, HEEGS uses the hyper-Laplacian, which provides a more robust similarity measure by capturing higher-order relationships among data points. Comparative experiments on twelve datasets and five state-of-the-art unsupervised algorithms demonstrate that HEEGS consistently outperforms its counterparts, achieving higher normalized mutual information (NMI) scores and clustering accuracy in nearly all cases. Statistical validation using the Nemenyi post hoc test further confirms that the performance improvements achieved by HEEGS are statistically significant. These results highlight HEEGS’s potential for critical applications such as biomarker discovery and disease classification.},
  archive      = {J_KIS},
  author       = {Bhattarai Lamsal, Mamta and Rastogi, Reshma},
  doi          = {10.1007/s10115-025-02423-4},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6577-6609},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hypergraph embedded entropy approach to gene selection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian hierarchical model for orthogonal tucker decomposition with oblivious tensor compression. <em>KIS</em>, <em>67</em>(8), 6553-6575. (<a href='https://doi.org/10.1007/s10115-025-02381-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank representations such as the Tucker decomposition underlie many frequentist methods for tensor analysis. Bayesian analogues, in contrast, have received less attention. Notably missing in the literature is a Bayesian Tucker decomposition with orthogonal factor matrices—a standard interpretability restriction in frequentist settings. We propose a Bayesian hierarchical model for the orthogonal Tucker decomposition, which we implement via conditionally conjugate Gibbs sampler. To reduce the complexity of tensor operations in MCMC estimation, we incorporate a mechanism that uses Johnson–Lindenstrauss embeddings to compress data. Our theoretical analysis bounds change in the full-conditional posterior distributions of tensor components due to compression (with respect to Hellinger distance and KL divergence). We further establish posterior consistency for the decomposition’s factor matrices and core tensor in settings where parameters are shared across tensor observations. Empirical results show that, for large tensor datasets, moderate compression can significantly reduce draw time (by about 50%) with only a moderate increase (up to about 15%) in median reconstruction error. Compression in the proposed model additionally enables analyses of tensor data that are too large to be held wholly in memory, thus making large-scale analyses tractable on even moderate computing resources.},
  archive      = {J_KIS},
  author       = {Pietrosanu, Matthew and Jiang, Bei and Kong, Linglong},
  doi          = {10.1007/s10115-025-02381-x},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6553-6575},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A bayesian hierarchical model for orthogonal tucker decomposition with oblivious tensor compression},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Addressing popularity discrepancy in collaborative filtering. <em>KIS</em>, <em>67</em>(8), 6525-6551. (<a href='https://doi.org/10.1007/s10115-025-02426-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering (CF) has emerged as the most successful type of recommendation algorithm during the past few decades. However, we observe that CF algorithms often exhibit a popularity discrepancy between user-interacted items and recommended items, e.g., CF algorithms may recommend items that are more popular than the ones the user preferred, especially to those who prefer non-popular items. To address this previously overlooked bias, we make three key contributions: (1) We introduce two novel metrics, PopDis_ED and PopDis_JS, to quantitatively measure popularity discrepancy, providing new perspectives beyond traditional bias indicators; (2) we propose an innovative model-agnostic mutual debiasing (MUDE) framework that uniquely combines a holistic model with a specialized long-tail model through a popularity-aware gating mechanism; (3) comprehensive experiments on four real-world datasets demonstrate that MUDE improves both recommendation accuracy and popularity discrepancy reduction, outperforming state-of-the-art debiasing methods. Moreover, MUDE shows strong generalizability across different types of CF algorithms, making it a practical solution for real-world recommender systems.},
  archive      = {J_KIS},
  author       = {Yu, Cizhou and Li, Dongsheng and Gu, Hansu and Zhang, Peng and Gu, Ning and Lu, Tun},
  doi          = {10.1007/s10115-025-02426-1},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6525-6551},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Addressing popularity discrepancy in collaborative filtering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving knowledge graphs via data-based causal structures. <em>KIS</em>, <em>67</em>(8), 6505-6523. (<a href='https://doi.org/10.1007/s10115-025-02414-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs, prevalent in a multitude of domains, have gained significant traction over the years. However, these graphs, often created from noisy sources using imperfect methods, suffer from inaccuracies, inconsistencies, and incompleteness. The imperative to rectify and supplement these issues is frequently addressed by costly domain-expert-based strategies or learning-based techniques, which can prove unreliable in noisy environments. Real-world data, with its inherent objectivity and implicit knowledge, can serve as an external source of knowledge. The ready availability of big data today presents an opportunity for improving knowledge graphs. Yet, the challenge of extracting latent information from such data has often deterred the inclusion of numerical data in current research. This paper introduces a pioneering approach that leverages external numerical data to enhance knowledge graph quality. The proposed method commences by mining statistical relationships, such as correlation and causality from the data. Following a thorough analysis of causal patterns and an evaluation of relation strengths, it identifies and eliminates redundant knowledge. The empirical results from numerous experiments validate the efficacy of our approach. The proposed method outperforms existing learning-based methods, demonstrating superior accuracy and stability in knowledge correction.},
  archive      = {J_KIS},
  author       = {Lyu, Derui and Wang, Xiangyu and Chen, Lyuzhou and Ban, Taiyu and Xiao, Kaiming and Liu, Lihua},
  doi          = {10.1007/s10115-025-02414-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6505-6523},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving knowledge graphs via data-based causal structures},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-KGMQA: Large language model-augmented multi-hop question-answering system based on knowledge graph in medical field. <em>KIS</em>, <em>67</em>(8), 6461-6503. (<a href='https://doi.org/10.1007/s10115-025-02399-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the problems of poor performance of large language models in specific domains, limited research on knowledge graphs and question-answering systems incorporating large language models, this paper proposed a multi-hop question-answering system framework based on a knowledge graph in the medical field, which was fully augmented by large language models (LLM-KGMQA). The method primarily addressed the problems of entity linking and multi-hop knowledge path reasoning. To address the entity linking problem, an entity fast-linking algorithm was proposed, which categorized entities based on multiple attributes. Then, it used user mentions to obtain the target attribute set of attributes and further narrowed the entity search scope through attribute intersection operations. Finally, for entities that remained too numerous after the intersection, the method suggested using a pre-trained model for similarity calculation and ranking, and to determine the final entity through construction instructions. Regarding multi-hop knowledge path reasoning, the paper proposed a three-step reasoning framework that included an $$n$$ -hop subgraph construction algorithm, a knowledge fusion algorithm, and a semantics-based knowledge pruning algorithm. In the entity fast-linking experiments, the maximum computational complexity was reduced by 99.90% through intersection operations. Additionally, an evaluation metric called CRA@n was used alongside the classic nDCG metric. When using the RoBERTa model for similarity calculations, the CRA@n score reached a maximum of 96.40, the nDCG scores reached a maximum of 99.80, and the entity fast-linking accuracy was 96.60%. In multi-hop knowledge path reasoning, the paper first validated the need for knowledge fusion by constructing three different forms of instructions. Subsequently, experiments were conducted with several large language models, concluded that the GLM4 model showed the best performance in Chinese semantic reasoning. The accuracy rates for GLM4 after pruning were 99.90%, 83.30%, and 86.60% for 1-hop, 2-hop, and 3-hop, respectively, compared to 95.00%, 6.60%, and 5.00% before pruning. The average response time was reduced by 1.36 s, 6.21 s and 27.07 s after pruning compared to before pruning.},
  archive      = {J_KIS},
  author       = {Wang, FeiLong and Shi, Donghui and Aguilar, Jose and Cui, Xinyi and Jiang, Jinsong and Shen, Longjian and Li, Mengya},
  doi          = {10.1007/s10115-025-02399-1},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6461-6503},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {LLM-KGMQA: Large language model-augmented multi-hop question-answering system based on knowledge graph in medical field},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defining quality in peer review reports: A scoping review. <em>KIS</em>, <em>67</em>(8), 6413-6460. (<a href='https://doi.org/10.1007/s10115-025-02435-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the challenge of defining quality in peer-review reports, a crucial yet underexplored aspect of academic publishing. Reviewers are vital gatekeepers of scientific knowledge, but unclear skills and a lack of standardized guidelines have led to inconsistent and subjective practices, weakening the overall efficacy of the peer-review process. To address this issue, the primary objective of this paper is to answer the research question: How has literature addressed guidance for producing quality peer-review reports? A scoping review was conducted, utilizing Scopus, Web of Science, SpringerLink, ScienceDirect, PubMed, and SAGE databases to search for records using keywords related to guidelines for scientific peer reviewing. The review identified 111 primary studies offering recommendations on how to review scientific articles. Extracted data were analysed thematically, focusing on approaches to reviewing articles, manuscript evaluation criteria, and report-writing guidelines. The findings revealed six key categories of review criteria for evaluating scientific manuscripts: structural components, research approach, style, ethical conduct, scientific value, and overall suitability. Additionally, the review provides 70 actionable recommendations for writing peer-review reports and highlights eight essential quality features expected in review texts: constructive, specific, fair, thorough, courteous, consistent, objective, and readable feedback. This study contributes to developing a standardized guide for scientific reviewing, with a particular emphasis on supporting early-career reviewers. The findings encourage academic publishers, journal editors, and professional organizations to adopt the proposed guidelines to enhance consistency, reduce bias, and improve the peer-review process. They also provide a foundation for developing new tools to support the reviewing.},
  archive      = {J_KIS},
  author       = {Sizo, Amanda and Lino, Adriano and Rocha, Álvaro and Reis, Luís Paulo},
  doi          = {10.1007/s10115-025-02435-0},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6413-6460},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Defining quality in peer review reports: A scoping review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommender systems in the digital age: A comprehensive review of methods, challenges, and applications. <em>KIS</em>, <em>67</em>(8), 6367-6411. (<a href='https://doi.org/10.1007/s10115-025-02453-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of digital technology and the widespread use of web applications have led to an overwhelming increase in data, necessitating effective recommender systems to filter and present relevant content. Recommender systems have evolved as essential tools in the digital era, providing personalized suggestions by analyzing user interests and behavior patterns. Integration of these systems has become an essential component of digital marketing strategies. They hold significant positions in a wide range of domains, including streaming services (music, movies, and books), social media systems, digital governance, electronic commerce, e-libraries, e-learning systems, traveling and resource services, and many more. Recently, recommender systems have been extensively employed in the healthcare and educational sectors to assist users in identifying and accessing content that aligns with their interests. While these systems offer numerous advantages, they are also confronted with several formidable challenges, including the cold-start problem, data sparsity, the presence of gray sheep users, starvation, and shilling, which can negatively impact their performance. Extensive research has been conducted to address these challenges and improve the accuracy of recommender systems. This review provides a comprehensive overview of the primary methods, including Content-Based Filtering, Collaborative Filtering, Knowledge-Based Systems, Demographic-Based Systems, Community-Based Systems, Hybrid Systems, Cross Domain Systems, and Social Recommender Systems, along with their respective merits and demerits. It also examines the evaluation methods and metrics used to assess recommender systems performance. Additionally, the review explores the application of recommender systems across diverse domains and highlights the tools and technologies supporting their development. Future opportunities and unresolved issues in recommender systems are explored to help promote continued research and innovation.},
  archive      = {J_KIS},
  author       = {Garapati, Rajesh and Chakraborty, Manomita},
  doi          = {10.1007/s10115-025-02453-y},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {6367-6411},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Recommender systems in the digital age: A comprehensive review of methods, challenges, and applications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracking dynamic community evolution based on social relevance and strong events. <em>KIS</em>, <em>67</em>(7), 6333-6365. (<a href='https://doi.org/10.1007/s10115-025-02411-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although incremental methods are widely used in community detection, their error accumulation problem remains unresolved. Additionally, current methods typically identify events only after community detection has been completed for all time snapshots, lacking consideration of the impact of events on community structure during evolution. Therefore, this paper proposes a framework called Tracking dynamic community evolution based on Social Relevance and Strong Events(TranSiEnt). For the first time, TranSiEnt integrates evolution event identification with dynamic community updating, classifying evolution events into ordinary events and Strong Events based on the influence of the relevant communities. During dynamic community updating, TranSiEnt employs a path diffusion strategy to determine core nodes for community detection, establishing the initial community structure. Using an incremental approach, the framework expands the influence range of incremental nodes in communities experiencing Strong Events. It again conducts precise community detection on all affected nodes to reduce error accumulation, ultimately optimizing community partitioning. TranSiEnt was subjected to objective accuracy experiments on real and synthetic datasets, using modularity, NMI, and EMA as performance evaluation metrics. T-tests were used to verify the significance of the performance improvement of the TranSiEnt algorithm. The experimental results show that TranSiEnt performs better in dynamic community detection and evolution event tracking, significantly improving over existing methods.},
  archive      = {J_KIS},
  author       = {Wu, Ling and Xie, Xiaohua and Chen, Chengkai and Yang, Yingjie and Guo, Kun},
  doi          = {10.1007/s10115-025-02411-8},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6333-6365},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Tracking dynamic community evolution based on social relevance and strong events},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FZeroTC: Fully zero-shot text classification for simultaneously discovering and labeling unseen classes. <em>KIS</em>, <em>67</em>(7), 6307-6331. (<a href='https://doi.org/10.1007/s10115-025-02379-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive data growth on the web, there are massive textual data without class labels such that zero-shot text classification has attracted much research attention. However, existing zero-shot text classification models still take the class labels as the weakly supervised signal, which are usually unavailable in the open domain. In this paper, we study the text classification problem in a fully zero-shot setting, in which not only are we not given any training samples for unseen classes, but also the label names and the total number of unseen classes are unknown. We propose a fully zero-shot text classification model (FZeroTC) in a semi-supervised learning framework to simultaneously discover and label unseen classes. In the FZeroTC model, a pairwise loss and a Kullback–Leibler divergence-based regularization term are specially designed for unseen class discovery, and a faraway loss is specially designed for class labeling. We propose three different kinds of learning strategies based on the pretrained language model and prompt learning to train FZeroTC. From extensive experiments on four public text classification datasets, FZeroTC outperforms the state-of-the-art zero-shot text classification models in terms of unseen class discovery performance and can provide high-quality labels for unseen classes.},
  archive      = {J_KIS},
  author       = {Duan, Dongsheng and Lv, Cunchi and Zhang, Cheng and Hou, Wei and Shi, Lei and Li, Yangxi and Zhao, Xiaofang},
  doi          = {10.1007/s10115-025-02379-5},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6307-6331},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {FZeroTC: Fully zero-shot text classification for simultaneously discovering and labeling unseen classes},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolution neural network-AlexNet with gazelle optimization algorithm-based software defect prediction. <em>KIS</em>, <em>67</em>(7), 6285-6306. (<a href='https://doi.org/10.1007/s10115-025-02408-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the size and complexity of software systems have increased dramatically. Software defects are very challenging to prevent because of these characteristics. Therefore, developers may be able to better allocate their limited resources by predicting the number of defects in software modules automatically. There are various approaches presented for identifying and fixing such problems, but none of these give sufficient results. To address these, this paper proposes convolution neural network-AlexNet with gazelle optimization algorithm-based software defect prediction (SWDP-CNN-AlexNet-GAOA). Here, NASA software defect prediction dataset is used. The feature normalization ensures that all features contribute equally to the model. Without normalization, features with larger numerical values would dominate the learning process. Then, software defects are predicted using convolution neural network (CNN)-AlexNet. Finally, gazelle optimization algorithm (GAOA) is proposed to optimize the parameters of CNN-AlexNet. Simulation proves that the SWDP-CNN-AlexNet-GAOA method outperforms existing models. The proposed SWDP-CNN-AlexNet-GAOA approach attains 3.88%, 5.75%, and 4.94% better accuracy and 6.25%, 5.91%, and 11.28% better F-measure compared with the existing methods, like software defect prediction using enhanced CNN (SWDP-EN-CNN), software defect prediction using hybrid swarm intelligence and deep learning (SWDP-HS-DL), and software defect prediction under ant colony optimization (SWDP-ACO), respectively.},
  archive      = {J_KIS},
  author       = {Gayetri Devi, S. V. and Ranjith, V. G. and Ramani, P. and Kavitha, A.},
  doi          = {10.1007/s10115-025-02408-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6285-6306},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Convolution neural network-AlexNet with gazelle optimization algorithm-based software defect prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). C-UCB: Chaos upper confidence bound in reinforcement learning for feature selection. <em>KIS</em>, <em>67</em>(7), 6241-6284. (<a href='https://doi.org/10.1007/s10115-025-02427-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective feature selection becomes increasingly important as medical data volumes increase. Despite this, existing dimensionality reduction techniques typically fail to address the unique challenges posed by medical datasets, such as their high dimensionality, class imbalance, and ethical imperative to use sensitive patient information as little as possible. To improve feature selection for medical data, this paper presents a novel reinforcement learning approach. Unlike traditional wrapper methods, which are prone to local optima and premature convergence, our approach leverages a chaos-based upper confidence bound (C-UCB) algorithm to dynamically adjust key parameters, such as the exploration coefficient (c) and the number of inner loop iterations (N), based on the mean reward from previous iterations. This dynamic adjustment not only improves algorithm performance but also significantly reduces computation time. Our experiments demonstrate that the proposed method outperforms state-of-the-art algorithms, improving the objective function by 51.43%, 42.86%, 88.57%, 71.43%, and 68.57%, compared to simulated annealing (SA), gray wolf optimization (GWO), pathfinder algorithm (PFA), $$\varepsilon $$ -Greedy, and UCB, respectively. This innovative approach has the potential to significantly enhance the accuracy and efficiency of feature selection in medical applications, enabling the development of more reliable and interpretable predictive models while minimizing the use of sensitive patient data.},
  archive      = {J_KIS},
  author       = {Zandvakili, Aboozar and Javidi, Mohammad Masoud and Mansouri, Najme},
  doi          = {10.1007/s10115-025-02427-0},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6241-6284},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {C-UCB: Chaos upper confidence bound in reinforcement learning for feature selection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent secure and efficient workflow scheduling (SEWS) model for heterogeneous cloud computing environment. <em>KIS</em>, <em>67</em>(7), 6193-6239. (<a href='https://doi.org/10.1007/s10115-025-02416-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study recognizes the critical role of the cloud computing platform in scientific workflow applications yet identifies vulnerabilities in existing cloud workflow systems, such as information leaks, unauthorized access, and compromised data integrity during task scheduling. Mainly, attackers exploit the lack of security for intermediate-level task information. To address these security threats, this work introduces the secure and efficient workflow scheduling (SEWS) model for heterogeneous cloud computing environments. The SEWS model identifies malicious attacks on all workflow tasks and focuses explicitly on safeguarding intermediary data. The SEWS model employs intelligent techniques to enhance security and introduces a comprehensive metric to measure the security of workflow tasks, considering factors like integrity, confidentiality, and availability. Beyond security improvements, the SEWS model aims to elevate the overall quality of service (QoS) in workflow scheduling applications. This includes reducing simulation time, enhancing overall power efficiency, and minimizing average energy consumption. Results: Results from the SEWS model demonstrate substantial improvements over the energy-minimized scheduling (EMS) model, with a reduction of 79.41% in average simulation time, 87.92% in average power sum, 41.35% in average power average, and 89.62% in average energy consumption. These findings underscore the SEWS model’s effectiveness in providing enhanced security and improved QoS in cloud workflow scheduling. The overarching goal of this work is to contribute to developing a more secure and efficient cloud workflow scheduling system, aligning with the increasing demands for robust security measures and optimized performance in heterogeneous cloud environments. Findings: Compared to the energy-minimized scheduling (EMS) model, the findings of this study demonstrate that the secure and efficient workflow scheduling (SEWS) model yields superior outcomes across key performance metrics. Specifically, the SEWS model excels in average simulation time, power sum, power average, and energy consumption. These results underscore the effectiveness of the SEWS model in enhancing the efficiency and resource utilization of cloud workflow scheduling. Importantly, the study identifies a notable gap in the existing work related to workflow task scheduling. Many prior studies still need to address the critical aspects of security and QoS in this context. While some jobs have attempted to enhance security, a significant limitation is the failure to extend these security measures to intermediary data. This gap in the literature highlights the unique contribution of the SEWS model, which addresses security concerns comprehensively and prioritizes QoS in the workflow task scheduling process. The observed superiority of the SEWS model in comparison with the EMS model serves as a testament to the model’s efficacy in concurrently addressing security and QoS challenges. By focusing on intermediary data, the SEWS model presents a holistic solution that aligns with the increasing demand for comprehensive security measures in cloud workflow environments. The findings emphasize the significance of integrating security and QoS considerations to establish a more robust and efficient workflow scheduling framework in heterogeneous cloud computing environments.},
  archive      = {J_KIS},
  author       = {Pasha, Fairoz and Natarajan, Jayapandian},
  doi          = {10.1007/s10115-025-02416-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6193-6239},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An intelligent secure and efficient workflow scheduling (SEWS) model for heterogeneous cloud computing environment},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized neural networks for efficient modeling of crude oil production. <em>KIS</em>, <em>67</em>(7), 6171-6192. (<a href='https://doi.org/10.1007/s10115-025-02415-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of crude oil production is crucial for effective management of oil reservoir operations. This paper leverages recent advancements in machine learning techniques and metaheuristic optimization algorithms, specifically deep learning (DL) and metaheuristic (MH) approaches, to construct a robust and efficient oil production prediction model. Real-world datasets from two diverse countries, Yemen and China, are employed in model development. The study focuses on optimizing a multilayer perceptron (MLP) using the Runge–Kutta optimizer (RUN). The primary goal is to enhance the MLP parameters through the application of the RUN algorithm. Rigorous evaluation experiments gauge the efficacy of the resulting prediction model (RUN-MLP), demonstrating impressive performance across three widely recognized evaluation metrics: root-mean-square error (RMSE), mean absolute error (MAE), and coefficient of determination ( $$R^2$$ ). Comparative analyses involve multiple MLP-modified models employing various MH algorithms, with the RUN-MLP consistently exhibiting competitive performance. The findings underscore the computational efficiency of the RUN optimization algorithm. Additionally, the study employs the Friedman test as a statistical tool to elucidate differences between RUN and its competitors.},
  archive      = {J_KIS},
  author       = {Ewees, Ahmed A. and Al-qaness, Mohammed A. A. and Thanh, Hung Vo and AlRassas, Ayman Mutahar and Elaziz, Mohamed Abd},
  doi          = {10.1007/s10115-025-02415-4},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6171-6192},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Optimized neural networks for efficient modeling of crude oil production},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new flow-based approach for enhancing botnet detection efficiency using convolutional neural networks and long short-term memory. <em>KIS</em>, <em>67</em>(7), 6139-6170. (<a href='https://doi.org/10.1007/s10115-025-02410-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the growing research and development of botnet detection tools, an ever-increasing spread of botnets and their victims is being witnessed. Due to the frequent adaptation of botnets to evolving responses offered by host-based and network-based detection mechanisms, traditional methods are found to lack adequate defense against botnet threats. In this regard, the suggestion is made to employ flow-based detection methods and conduct behavioral analysis of network traffic. To enhance the performance of these approaches, this paper proposes utilizing a hybrid deep learning method that combines convolutional neural network (CNN) and long short-term memory (LSTM) methods. CNN efficiently extracts spatial features from network traffic, such as patterns in flow characteristics, while LSTM captures temporal dependencies critical to detecting sequential patterns in botnet behaviors. Experimental results reveal the effectiveness of the proposed CNN-LSTM method in classifying botnet traffic. In comparison with the results obtained by the leading method on the identical dataset, the proposed approach showcased noteworthy enhancements, including a 0.61% increase in precision, a 0.03% augmentation in accuracy, a 0.42% enhancement in the recall, a 0.51% improvement in the F1-score, and a 0.10% reduction in the false-positive rate. Moreover, the utilization of the CNN-LSTM framework exhibited robust overall performance and notable expeditiousness in the realm of botnet traffic identification. Additionally, we conducted an evaluation concerning the impact of three widely recognized adversarial attacks on the Information Security Centre of Excellence dataset and the Information Security and Object Technology dataset. The findings underscored the proposed method’s propensity for delivering a promising performance in the face of these adversarial challenges.},
  archive      = {J_KIS},
  author       = {Asadi, Mehdi and Heidari, Arash and Jafari Navimipour, Nima},
  doi          = {10.1007/s10115-025-02410-9},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6139-6170},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new flow-based approach for enhancing botnet detection efficiency using convolutional neural networks and long short-term memory},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive multi-view clustering based on multi-head attention mechanisms and three-way decision. <em>KIS</em>, <em>67</em>(7), 6115-6138. (<a href='https://doi.org/10.1007/s10115-025-02403-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) methods can improve the clustering accuracy by integrating complementary information from multiple views. In recent years, multi-view clustering based on contrastive learning has become an effective MVC method. However, there are two limitations: (1) using shallow view fusion strategies, which is unable to effectively fuse features for multiple views and (2) using hard clustering method, which cannot effectively solve the uncertainty between objects and clusters. Therefore, we propose a contrastive multi-view clustering based on multi-head attention mechanisms and three-way decision (called TAC-MVC). Firstly, we use auto-encoder to extract the low-level features of each view, based on which contrastive learning is used to maximize the consistency of information among views and learn the high-level features and semantic labels of each view and secondly, introduce the multi-attention mechanism to fuse the high-level features of each view and obtain a more accurate multi-view fusion feature representation. Finally, we perform clustering based on the three-way decision for multi-view fusion features and semantic labels and obtain three soft clustering results for the core, fringe, and the trivial region, to solve the uncertainty between objects and clusters. Experimental results on different datasets validate the effectiveness of the method.},
  archive      = {J_KIS},
  author       = {Lu, Mingyue and Xu, Yi and Chu, Wenke and Gu, Jiaye and Gao, Muyang},
  doi          = {10.1007/s10115-025-02403-8},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6115-6138},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Contrastive multi-view clustering based on multi-head attention mechanisms and three-way decision},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PISRuleGrowth: Mining periodic intra-sequential rules common to multiple sequences. <em>KIS</em>, <em>67</em>(7), 6071-6114. (<a href='https://doi.org/10.1007/s10115-025-02401-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential rule mining remains an important research topic in data mining. Discovering conditioned events in large databases can provide beneficial insights. However, periodic intra-sequential mining has also emerged as an interesting topic, describing patterns that are simultaneously periodic within sequences across different sequences. These patterns have been proven especially useful in sales and marketing, aiding in targeting a group of customers based on their common periodic behavior. Motivated by previous works, we propose a new mining task to discover periodic sequential rules that appear in multiple sequences. The goal is to find causal temporal relationships in periodic intra-sequential patterns, producing more meaningful patterns while retaining the significance of the information they provide. We also emphasize that the periodicity constraint was not introduced in partially-ordered sequential and intra-sequential rules beforehand. In this paper, we propose the periodic intra-sequential RuleGrowth (PISRuleGrowth) algorithm, which is based on the pattern-growth approach and uses a map structure to remain efficient even on large databases. Extensive experiments on real datasets confirmed that the algorithm outperforms the existing algorithms in terms of speed and memory usage.},
  archive      = {J_KIS},
  author       = {Milenković, Nevena and Delibašić, Boris},
  doi          = {10.1007/s10115-025-02401-w},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6071-6114},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PISRuleGrowth: Mining periodic intra-sequential rules common to multiple sequences},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TAMTE: Topic-attentive multimodal transformer encoder for hashtag recommendation. <em>KIS</em>, <em>67</em>(7), 6047-6070. (<a href='https://doi.org/10.1007/s10115-025-02398-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of social media has posed challenges for search and raised concerns about effective classification approaches for this massive amount of data. To manage this influx of information effectively, leveraging hashtags emerges as a valuable option for enhancing information diffusion. This paper introduces TAMTE, a topic-attentive multimodal transformer encoder for hashtag recommendation task. Specifically, introduce a topic leveraging self-attention mechanism for cross-modal representation learning. The proposed model first encodes the text and image inputs separately using dedicated encoders, and a joint embedding is produced from image and text embeddings. Then, the microblog caption text and the image text generated from the image-to-text model are combined and fed into a topic modeling module to capture latent themes. Finally, the proposed approach proceeds using topic-attentive self-attentive mechanism between the topic embedding and the joint feature embedding. This facilitates more contextually relevant hashtag recommendations. Experimental results show the superiority of the proposed TAMTE method in terms of precision, recall, and F1-score compared to the existing state-of-the-art baseline methods. By effectively utilizing self-attention across multimodal and topic word representations, the proposed approach offers an effective solution for hashtag recommendation in social media contexts.},
  archive      = {J_KIS},
  author       = {Khalil, Mian Muhammad Yasir and Chen, Bo and Rauf, Muhammad Arslan and Wang, Weidong and Wang, Qingxian},
  doi          = {10.1007/s10115-025-02398-2},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6047-6070},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {TAMTE: Topic-attentive multimodal transformer encoder for hashtag recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User preferences learning with temporal point process and attention model for next POI recommendation. <em>KIS</em>, <em>67</em>(7), 6021-6045. (<a href='https://doi.org/10.1007/s10115-025-02378-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, next POI (point of interest) recommendation has emerged as a broad research area. The task involves leveraging user check-in data to recommend POIs at specific times. Despite the remarkable progress in next POI recommendation, the complex temporal correlations between POIs as well as dynamics of user preferences make it challenging to leverage user check-in data effectively. Therefore, we propose a novel user preferences learning model (UPLM) for next POI recommendation. This model employs a temporal point process to finely model the time intervals between historical POI check-ins and the user’s current time, and effectively integrate the impact of POI visit frequency on recommendations. Additionally, the model incorporates an attention mechanism to better capture both long-term and short-term user preferences information. Considering the impacts of POI categories, we utilize LSTM (long short-term memory) networks to learn users’ preferences for different categories. By comprehensively considering both POI and category preferences of users, we can obtain the probabilities of all candidate POIs and recommend the POI with the highest probability. Finally, our model is evaluated on two real-world datasets, and the results demonstrate that the proposed model is better than the state-of-the-art next POI recommendation methods. We achieved a recall@10 of 45.76% with the NYC dataset and 41.88% with the Weeplace dataset. We also achieved an MRR of 22.67% with the NYC dataset and 21.89% with the Weeplace dataset.},
  archive      = {J_KIS},
  author       = {Gong, Liupeng and Li, Zheng and Xing, Siqi and Liu, Chun and Yang, Wei},
  doi          = {10.1007/s10115-025-02378-6},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {6021-6045},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {User preferences learning with temporal point process and attention model for next POI recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deconfounding representation learning for mitigating latent confounding effects in recommendation. <em>KIS</em>, <em>67</em>(7), 5999-6020. (<a href='https://doi.org/10.1007/s10115-025-02404-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has gained significant attention in the field of recommender systems due to its ability to learn highly expressive representations with limited labels. However, historical user–item interaction data used for recommender systems often contain confounders, thereby establishing spurious correlations between user preferences and confounders during self-supervised training and misleading recommender systems to use these correlations as shortcuts for generating recommendations. Existing approaches for debiasing usually involve manually identifying observed confounders, but they are often tailored to specific situations and overlook latent confounders. To address this challenging problem, we propose a Deconfounding Graph Contrastive Learning (DeGCL) method to provide deconfounding recommendations by adjusting for a learned deconfounding representation from interaction data, using the back-door adjustment strategy. DeGCL learns the representation to capture latent confounding effects in observational data between users and items. It artificially adds interactions and noise to create contrastive views, which help deconfound the model. By adjusting for the learned representation, DeGCL mitigates latent confounding effects in training downstream recommendation models. Experiments on two real-world datasets demonstrate that our method outperforms state-of-the-art methods, suggesting its potential to provide more effective recommendations in practice.},
  archive      = {J_KIS},
  author       = {Zhang, Guixian and Yuan, Guan and Cheng, Debo and Liu, Lin and Li, Jiuyong and Xu, Ziqi and Zhang, Shichao},
  doi          = {10.1007/s10115-025-02404-7},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5999-6020},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deconfounding representation learning for mitigating latent confounding effects in recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A synthetic over-sampling method with minority and majority classes for imbalance problems. <em>KIS</em>, <em>67</em>(7), 5965-5998. (<a href='https://doi.org/10.1007/s10115-025-02394-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a substantial challenge in classifying many real-world cases. Synthetic over-sampling methods have been effective to improve the performance of classifiers for imbalance problems. However, most synthetic over-sampling methods generate synthetic instances within the convex hull formed by the existing minority instances as they only concentrate on the minority class and ignore the vast information provided by the majority class. They also often do not perform well for extremely imbalanced data, as fewer minority instances mean less information with which to generate synthetic instances. Moreover, existing methods that generate synthetic instances using the majority class distributional information cannot perform effectively when the majority class has a multi-modal distribution. We propose a new method to generate diverse and adaptable synthetic instances using Synthetic Over-sampling with Minority and Majority classes (SOMM). SOMM generates synthetic instances diversely within the minority data space. It updates the generated instances adaptively to the neighbourhood including both classes. Thus, SOMM performs well for imbalance problems. We examine the performance of SOMM for binary multiclass imbalance classification problems for different imbalance levels. The empirical results and nonparametric statistical testing show the superiority of SOMM compared to existing methods. We also discuss the strengths and limitations of SOMM through visualisations.},
  archive      = {J_KIS},
  author       = {Khorshidi, Hadi A. and Aickelin, Uwe},
  doi          = {10.1007/s10115-025-02394-6},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5965-5998},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A synthetic over-sampling method with minority and majority classes for imbalance problems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive prototype source-free adaptation boosted by inherent data structure. <em>KIS</em>, <em>67</em>(7), 5947-5964. (<a href='https://doi.org/10.1007/s10115-025-02385-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free unsupervised domain adaptation (SFDA), which needs only pre-trained source model and unlabeled target data ( $${D}_{t}$$ ), has attracted lots of interests recently.Query Compared with unsupervised domain adaptation, SFDA is more practical since it can be applied to scenarios that lack of source data ( $${D}_{s}$$ ) due to data privacy. Although remarkable progresses have been made in the field of SFDA, it still remains challenge since most of the existing methods do not fully exploit the internal structure information of the $${D}_{t}$$ . To bridge the gap, we propose prototype generative model leveraging inherent data structure mined by graph neural networks and forge a source-free domain adaptation method accordingly. Our method consists of two stages: (1) data mining and prototype generation and (2) domain adaptation. In stage one, we firstly use graph convolutional network and graph autoencoder to model the data relations of the $${D}_{t}$$ , extract features and label the target features in a self-supervised way; then, we generate source-like avatar conditioned on features of the $${D}_{t}$$ , labels and noise. In the second stage, a category-level alignment between the source prototypes and the $${D}_{t}$$ is conducted based on contrastive learning strategy. Extensive experiments on Office-31, Office-Home and VisDA-2017 datasets prove the effectiveness and superiority of our method. Average accuracy rates of 90.7%, 76.1% and 87.4% are achieved, thereby leading to superior performance compared with the state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Guo, Longxia and Li, Yundong},
  doi          = {10.1007/s10115-025-02385-7},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5947-5964},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Contrastive prototype source-free adaptation boosted by inherent data structure},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized similarity regression models based on maximum correntropy criterion for stock series prediction. <em>KIS</em>, <em>67</em>(7), 5925-5945. (<a href='https://doi.org/10.1007/s10115-025-02374-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock price prediction is an important topic in finance and economics. Owing to the frequent occurrence of singularities in stock prices, the accuracy of stock price prediction is seriously affected. To address this issue, a novel method, which is named personalized similarity regression model based on maximum correntropy criterion (PSR-MCC), is proposed in this paper. Specifically, we first employ a similarity measurement to select series from historical data that are similar to the test series. The personalized similarity measurement embeds Canberra distances in dynamic time warping to attenuate the impact of singularity and coping with time shifts and warping. Afterward, for similar series, a new regression model based on maximum correntropy criterion is proposed. It uses maximum correntropy criterion as the constraint condition of regression models instead of the minimum mean square error for weakening the negative influence of singularities further. Experiments based on real data demonstrate that the proposed method can effectively reduce the impact of abnormal data on prediction accuracy, with small prediction errors and strong robustness.},
  archive      = {J_KIS},
  author       = {Zhao, Feng and Liu, Mengyang and Gao, Yating and Qiao, Xiaoyan and Ge, Shiyu and Liu, Pingping and Zhang, Yong},
  doi          = {10.1007/s10115-025-02374-w},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5925-5945},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Personalized similarity regression models based on maximum correntropy criterion for stock series prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text augmentation method with adjustable manipulation intensity based on in-context learning. <em>KIS</em>, <em>67</em>(7), 5901-5923. (<a href='https://doi.org/10.1007/s10115-025-02413-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text augmentation, a technique for generating new samples through various combinations, noise, and manipulations of small datasets, is an essential technique in natural language processing research. This methodology enables the construction of robust models during the training step by enhancing data diversity. However, determining the manipulation level remains a significant challenge. When the manipulation intensity is too low, insufficient data diversity is generated, leading to suboptimal augmentation effects. Conversely, excessive manipulation can compromise label reliability, resulting in a degradation of model performance. To address the challenge of “manipulation level,” we propose a text augmentation technique that can make systematic adjustments. In particular, we introduce a method for flexibly resetting the range of the candidate pool for manipulations, ensuring an optimal level of randomness during the augmentation process. We also introduce an advanced sentence embedding that supports reliable pseudo-labeling across different manipulation levels. Additionally, we utilize ChatGPT model in the final stage to enhance the coherence and expressiveness of the generated text, thereby improving the quality of the output. To evaluate the effectiveness of our approach, we performed comparisons with existing text augmentation approaches. The experimental results show significant performance improvements in almost all test datasets.},
  archive      = {J_KIS},
  author       = {Cha, Yuho and Lee, Younghoon},
  doi          = {10.1007/s10115-025-02413-6},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5901-5923},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Text augmentation method with adjustable manipulation intensity based on in-context learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedTPS: Traffic pattern sharing for personalized federated traffic flow prediction. <em>KIS</em>, <em>67</em>(7), 5873-5899. (<a href='https://doi.org/10.1007/s10115-025-02393-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction plays a critical role in ensuring the efficiency of transportation systems, which has motivated extensive research into capturing spatial-temporal dependencies within road networks. However, most existing approaches depend on centralized data, potentially raising privacy concerns as traffic data is often managed by different traffic administration departments and restricted from distribution. To address this issue, federated learning (FL) allows collaborative model training without exchanging raw data. Nevertheless, traditional FL methods are designed to optimize a model that performs well globally, making them inadequate for handling the naturally non-independent and identically distributed traffic data across different regions. To overcome this limitation, we propose a new framework termed “personalized Federated learning with Traffic Pattern Sharing” (FedTPS), which exploits the sharing of underlying common traffic patterns across regions while preserving region-specific characteristics in a personalized manner. Specifically, discrete wavelet transform is employed to decompose the traffic data and extract low-frequency components in each client that reflect stable traffic dynamics. The clients then learn representative traffic patterns from these stable traffic dynamics and store them in traffic pattern repositories. Afterward, these repositories are shared with a central server, which enables the identification and integration of common traffic patterns to improve global learning. Meanwhile, the model components capturing spatial-temporal dependencies are retained for local training, ensuring adaptation to region-specific data. Intensive experiments on four real-world traffic datasets firmly demonstrate the superiority of our proposed FedTPS over traditional FL methods across various estimation errors.},
  archive      = {J_KIS},
  author       = {Zhou, Hang and Yu, Wentao and Wan, Sheng and Tong, Yongxin and Gu, Tianlong and Gong, Chen},
  doi          = {10.1007/s10115-025-02393-7},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5873-5899},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {FedTPS: Traffic pattern sharing for personalized federated traffic flow prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RKR-GAT: Recurrent knowledge-aware recommendation with graph attention network. <em>KIS</em>, <em>67</em>(7), 5851-5871. (<a href='https://doi.org/10.1007/s10115-025-02391-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph-based recommender systems have attracted increasing attention in recent years. By extracting the semantics of entities and relationships, these recommender systems enable a more comprehensive understanding of user preferences. However, existing methods overlook the impact of low-quality links in knowledge graphs, which leads to user preference propagation bias and affects recommendation accuracy. In this paper, we propose RKR-GAT, a recurrent knowledge-aware recommendation algorithm based on a graph attention network (GAT). Specifically, RKR-GAT integrates user–item graphs and the knowledge graph into a unified graph and learns node embeddings using a graph attention network. To alleviate the issue of user preference propagation bias, we design a new path filtering module to reject low-quality connections and adaptively retain valid reasoning paths between users and items in the knowledge graph. Furthermore, RKR-GAT combines recurrent neural networks with a self-attention mechanism to efficiently analyze the semantics of the paths to generate more accurate recommendations while providing explainability. Ultimately, experimental results on three public datasets demonstrate the better performance of RKR-GAT compared to state-of-the-art baselines.},
  archive      = {J_KIS},
  author       = {Sun, Qifeng and Wang, Yaning and Gong, Faming},
  doi          = {10.1007/s10115-025-02391-9},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5851-5871},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {RKR-GAT: Recurrent knowledge-aware recommendation with graph attention network},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-attention DeepCRNN: An efficient and explainable intrusion detection framework for internet of medical things environments. <em>KIS</em>, <em>67</em>(7), 5783-5849. (<a href='https://doi.org/10.1007/s10115-025-02402-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of cyber threats in healthcare necessitates robust security measures to protect sensitive medical data. This research presents a hybrid security framework integrating blockchain for decentralized, immutable data storage and an intrusion detection system (IDS) leveraging a multi-attention deep convolutional recurrent neural network (MA-DeepCRNN) model for advanced threat detection. The proposed IDS combines convolutional neural networks (CNNs) for spatial feature extraction, recurrent neural networks (RNNs) for temporal pattern recognition, and an attention mechanism to enhance critical data representation. The model is evaluated using the CICIoMT 2024 benchmark dataset. The blockchain architecture achieves a block creation time of 10 s, improving significantly over Bitcoin (~ 600 s) and Ethereum (~ 15 s), while increasing throughput to  ~ 182 transactions per second. Security analysis indicates a low transaction reversal probability of < 0.1%. The IDS demonstrates high classification performance, achieving 99.49% accuracy in binary classification, 99.12% in multiclass (6-class) classification, and 98.56% in large-scale (19-class) classification. Comparative analysis with state-of-the-art approaches highlights improvements in accuracy and F1-score by 3.44 and 3.71%, respectively, for intrusion detection in Internet of Medical Things (IoMT) systems. These results underscore the effectiveness of the proposed framework in enhancing security, scalability, and real-time threat detection in healthcare environments.},
  archive      = {J_KIS},
  author       = {Sharma, Nikhil and Shambharkar, Prashant Giridhar},
  doi          = {10.1007/s10115-025-02402-9},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5783-5849},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-attention DeepCRNN: An efficient and explainable intrusion detection framework for internet of medical things environments},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioral insights of adaptive splitting decision trees in evolving data stream classification. <em>KIS</em>, <em>67</em>(7), 5751-5782. (<a href='https://doi.org/10.1007/s10115-025-02395-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are leading in state-of-the-art architectures for classifying high-speed data streams. Hoeffding-based trees have dominated the field and are more efficient than batch counterparts because they are incremental and distribute the cost of greedy best-split evaluations throughout the stream through periodic evaluation. However, this splitting mechanism is invariant to the state of the stream and the tree, as periodic evaluations cannot capture the state of data or performance of the tree in between assessments. In this work, we significantly outline the main behaviors of a novel decision tree that can deal with high-speed data streams and adapt to performance decays considering the tree state, namely the local adaptive streaming tree (LAST) and Hoeffding-based trees. We also provide a comprehensive benchmark of online decision trees, analyze how split moments affect performance, how the trees react to increases in the number of features and classes, and how the trees behave in concept drift scenarios. Results show that (i) LAST presents the best results, regardless of the change detector selected, (ii) LAST’s strategy of branching the tree based on performance decays is the reason it outperforms other decision trees, (iii) decision trees present similar CPU-Time, but trees with split reevaluation are more costly in memory, and (iv) LAST presents superior results in datasets with abrupt changes, while datasets with gradual changes depend on choosing detectors that are more suitable for gradual changes.},
  archive      = {J_KIS},
  author       = {Nowak Assis, Daniel and Barddal, Jean Paul and Enembreck, Fabrício},
  doi          = {10.1007/s10115-025-02395-5},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5751-5782},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Behavioral insights of adaptive splitting decision trees in evolving data stream classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlation clustering algorithm for dynamic complete signed graphs: An index-based approach. <em>KIS</em>, <em>67</em>(7), 5731-5750. (<a href='https://doi.org/10.1007/s10115-025-02397-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel index-based approach to improve the runtime and extend the applicability of approximation algorithms for correlation clustering on complete signed graphs. Building on prior work, we introduce an indexing structure that enhances runtime efficiency and enables full dynamic updates, including vertex addition, removal, and edge sign flipping. For a complete graph with n vertices and m positively signed edges, our method achieves an amortized runtime of $$\mathcal {O}(m \cdot \alpha (n))$$ for dynamic operations and $$\mathcal {O}(m + n)$$ for clustering queries with a fixed threshold $$\varepsilon $$ , while pre-computation scales as $$\mathcal {O}(m \cdot (\alpha (G) + \log m))$$ where $$\alpha (G)$$ is the arboricity of the graph. The overhead memory cost for the index is $$\mathcal {O}\left( m+n\right) $$ . The proposed method retains the theoretical guarantees of prior algorithms and addresses their computational bottlenecks. Extensive experiments on seven real-world datasets validate the practical efficiency of the approach, showing a 34% decrease in clustering runtime on average compared to non-indexed methods. These results demonstrate the scalability and adaptability of the method for large-scale and dynamic graph clustering scenarios.},
  archive      = {J_KIS},
  author       = {Shakiba, Ali},
  doi          = {10.1007/s10115-025-02397-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5731-5750},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correlation clustering algorithm for dynamic complete signed graphs: An index-based approach},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CREDIFY: Contextualized retrieval of evidence for open-domain fact verification. <em>KIS</em>, <em>67</em>(7), 5699-5729. (<a href='https://doi.org/10.1007/s10115-025-02400-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifying the credibility of claims made in public speeches and articles is crucial in the current digital era due to the widespread prevalence of fake news and its impact on society. Finding authentic evidence to verify these claims efficiently is necessary to counteract their harmful dissemination. To meet this objective, journalists manually verify the claims, which is challenging. Automated claim verification frameworks have emerged as a potential solution to this problem, gaining significant attention in recent years. These frameworks provide an efficient solution to ensure the credibility of information. However, a significant challenge lies in retrieving the most relevant, consistent, and authentic evidence against these claims that considers the context and predicts their veracity. To this end, we propose a contextualized retrieval of evidence for the open-domain fact verification (CREDIFY) framework. This automated open-domain claim verification architecture utilizes context and semantics to retrieve consistent evidence written by journalists and experts. This verified evidence serves as a reliable source for predicting the veracity of claims. CREDIFY consists of three modules: passage retrieval, sentence selection, and veracity prediction for retrieving contextual evidence and performing entailment classification for veracity prediction. To validate our methodology, we propose the retrieval and entailment of claims through inference for the facts’ testimony dataset collected from fact-checking sites. It is composed of 5000 open-domain claims supporting or refuting 3356 consistent and authentic evidence verified by journalists. The proposed framework achieves a significant performance gain on end-to-end systems with a margin of 5.5% over the state-of-the-art work, achieving an accuracy of 96.5%. Moreover, we also analyze the effect of the contrastive claim generation and paraphrasing technique on the veracity prediction module for de-biasing and generalizing our fact verification model. The code for our proposed model, CREDIFY, is available on GitHub https://github.com/SarahNasir54/Credify.git.},
  archive      = {J_KIS},
  author       = {Nasir, Ayesha and Wasim, Muhammad and Nasir, Sarah},
  doi          = {10.1007/s10115-025-02400-x},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5699-5729},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CREDIFY: Contextualized retrieval of evidence for open-domain fact verification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Restless reachability problems in temporal graphs. <em>KIS</em>, <em>67</em>(7), 5651-5697. (<a href='https://doi.org/10.1007/s10115-025-02405-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a family of reachability problems under waiting-time restrictions in temporal and vertex-colored temporal graphs. Given a temporal graph and a set of source vertices, we find the set of vertices that are reachable from a source via a time-respecting path, where the difference in timestamps between consecutive edges is at most a resting time. Given a vertex-colored temporal graph and a multiset query of colors, we find the set of vertices reachable from a source via a time-respecting path such that the vertex colors of the path agree with the multiset query and the difference in timestamps between consecutive edges is at most a resting time. These kinds of problems have applications in understanding the spread of a disease in a network, tracing contacts in epidemic outbreaks, finding signaling pathways in the brain network, and recommending tours for tourists, among others. We present an algebraic algorithmic framework based on constrained multilinear sieving for solving the restless reachability problems we propose. In particular, parameterized by the length k of a path sought, we show that the proposed problems can be solved in $$O(2^k k m \Delta )$$ time and $$O(n \Delta )$$ space, where n is the number of vertices, m the number of edges, and $$\Delta $$ the maximum resting time of an input temporal graph. The approach can be extended to extract paths and connected subgraphs in both static and temporal graphs, thus improving the work of Björklund et al. (in Proceedings of the European symposium on algorithms, 2014) and Thejaswi et al. (Big Data 8:335–362, 2020). In addition, we prove that our algorithms for the restless reachability problems in vertex-colored temporal graphs are optimal under plausible complexity-theoretic assumptions. Finally, with an open-source implementation, we demonstrate that our algorithm scales to large graphs with up to one billion temporal edges, despite the problems being NP-hard. Specifically, we present extensive experiments to evaluate our scalability claims both on synthetic and on real-world graphs. Our implementation is efficiently engineered and highly optimized. For instance, we can solve the restless reachability problem by restricting the path length to 9 in a real-world graph dataset with over 36 million directed edges in less than one hour on a commodity desktop with a 4-core Haswell CPU.},
  archive      = {J_KIS},
  author       = {Thejaswi, Suhas and Lauri, Juho and Gionis, Aristides},
  doi          = {10.1007/s10115-025-02405-6},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5651-5697},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Restless reachability problems in temporal graphs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient stable community search in temporal signed graphs. <em>KIS</em>, <em>67</em>(7), 5619-5649. (<a href='https://doi.org/10.1007/s10115-025-02396-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signed graphs are commonly used to model two opposite relationships between different entities, and finding communities in them has gained much attention. However, previous studies have mainly focused on the structural cohesiveness or robustness of communities in static signed graphs and have not considered the temporal information. To fill this gap, we propose a novel stable span community (SSC) model in temporal signed graphs. This model integrates the desirable properties of the k-core model for cohesiveness measurement and the balanced triangle model for robustness measurement, ensuring a seamless transition in both cohesive and robust structural continuity in temporal signed graphs. Following this, we present the SSC search problem in temporal signed graphs and prove it is NP-hard. To solve this problem, we develop a greedy algorithm by leveraging novel bound pruning techniques and search methods to explore the stable community in the intersection graph over a fixed time sub-interval. Furthermore, we employ an interval-pruning technique to elegantly extend this method to temporal signed graphs, significantly enhancing search efficiency for stable span communities within a query time interval. We conduct extensive experiments on real-world datasets to demonstrate the efficiency and effectiveness of our model and algorithms.},
  archive      = {J_KIS},
  author       = {Chen, Jinyi and Xin, Junchang and Choudhury, Farhana and Zhou, Keqi and Wang, Zhiqiong},
  doi          = {10.1007/s10115-025-02396-4},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5619-5649},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient stable community search in temporal signed graphs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A newly developed multi-strategy optimization algorithm framework based on the adaptive switching approach coupled with grey wolf optimizer. <em>KIS</em>, <em>67</em>(7), 5571-5617. (<a href='https://doi.org/10.1007/s10115-025-02360-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engineering optimization tasks often require tens of thousands of real function evaluations to obtain the optimal solution, which incurs significant computational costs and poses challenges in meeting the requirements of practical engineering optimization problems. To maintain a high level of optimization performance under limited computational budgets, this study proposes a Surrogate-Assisted Grey Wolf Optimization framework based on Query by Committee (SAGWO-QBC). The multi-strategy optimization algorithm framework includes a search strategy based on the maximum uncertainty criterion, a surround strategy utilizing clustering algorithms, and an attack strategy employing cross-validation to construct the mechanism for adding points. The method named Query by Committee is employed to enable adaptive switching between these strategies, effectively filling the solution space and improving algorithm efficiency. To evaluate the performance of the proposed algorithm framework, extensive tests were conducted across various optimization scenarios, leading to the following conclusions: (1) Performance tests on 15 benchmark functions from CEC2005 and CEC2017 (with dimensions ranging from 10 to 30) demonstrate that under limited computational budgets, the proposed algorithm outperforms six state-of-the-art optimization algorithms in over 80% of test cases. (2) Application of the algorithm to three classic constrained engineering optimization problems from the CEC2020 test set reveals that SAGWO-QBC exhibits strong competitive potential compared to the top performing algorithms in the test set. In addition, the internal parameters and optimization process of the algorithm are analyzed and discussed in detail to further understand its operation mechanism. In summary, the SAGWO-QBC algorithm framework demonstrates outstanding performance, delivering high-quality solutions with low computational cost, making it a promising approach for addressing complex engineering optimization problems.},
  archive      = {J_KIS},
  author       = {Lu, Yeming and Zhang, Meina and Chen, Yanmu and Ju, Jinjun and Chen, Fang and Bai, Kunlun and Wang, Xiaofang},
  doi          = {10.1007/s10115-025-02360-2},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5571-5617},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A newly developed multi-strategy optimization algorithm framework based on the adaptive switching approach coupled with grey wolf optimizer},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving out-of-distribution detection by enforcing confidence margin. <em>KIS</em>, <em>67</em>(7), 5541-5569. (<a href='https://doi.org/10.1007/s10115-025-02380-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many critical machine learning applications, such as autonomous driving and medical image diagnosis, the detection of out-of-distribution (OOD) samples is as crucial as accurately classifying in-distribution (ID) inputs. Recently, outlier exposure (OE)-based methods have shown promising results in detecting OOD inputs via model fine-tuning with auxiliary outlier data. However, most of the previous OE-based approaches emphasize more on synthesizing extra outlier samples or introducing regularization to diversify OOD sample space, which is rather unquantifiable in practice. In this work, we propose a novel and straightforward method called Margin-bounded Confidence Scores (MaCS) to address the nontrivial OOD detection problem by enlarging the disparity between ID and OOD scores, which in turn makes the decision boundary more compact facilitating effective segregation with a simple threshold. Specifically, we augment the learning objective of an OE regularized classifier with a supplementary constraint, which penalizes high confidence scores for OOD inputs compared to that of ID and significantly enhances the OOD detection performance while maintaining the ID classification accuracy. Extensive experiments on various benchmark datasets for image classification tasks demonstrate the effectiveness of the proposed method by significantly outperforming state-of-the-art methods on various benchmarking metrics. The code is publicly available at https://github.com/lakpa-tamang9/margin_ood/tree/kais},
  archive      = {J_KIS},
  author       = {Tamang, Lakpa and Bouadjenek, Mohamed Reda and Dazeley, Richard and Aryal, Sunil},
  doi          = {10.1007/s10115-025-02380-y},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5541-5569},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving out-of-distribution detection by enforcing confidence margin},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domestic violence: A survey on norms and impacts, AI detection approaches, and existing datasets. <em>KIS</em>, <em>67</em>(7), 5511-5539. (<a href='https://doi.org/10.1007/s10115-025-02420-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domestic violence (DV) remains a pervasive issue globally, prompting extensive research to find effective solutions. This paper presents a comprehensive survey of significant DV research, focusing on societal norms, impacts, detection approaches, and existing datasets. We systematically reviewed and classified research articles from major repositories, including IEEE Xplore, ACM Digital Library, and Google Scholar. Key findings highlight the primary areas of focus in DV research, such as societal norms and perspectives, societal impact, and detection methods. The survey reveals that machine learning (ML) techniques are the most viable for detecting DV, with algorithms applied to various data forms, including text, audio, and video. We also provide brief explanations of related ML techniques and present recognized datasets, making the survey accessible to readers from diverse backgrounds. This paper may serve as a valuable reference for academicians, developers, and researchers, offering insights into the current state and future opportunities in DV research.},
  archive      = {J_KIS},
  author       = {Mah, Shau Xuan and Jusoh, Shaidah and Al Fawareh, Hejab},
  doi          = {10.1007/s10115-025-02420-7},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5511-5539},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Domestic violence: A survey on norms and impacts, AI detection approaches, and existing datasets},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of meta-heuristic high utility patterns mining methods. <em>KIS</em>, <em>67</em>(7), 5469-5510. (<a href='https://doi.org/10.1007/s10115-025-02392-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High utility patterns mining aims to extract significant information from transaction databases, enabling users to make more informed decisions. Meta-heuristic-based HUPM methods significantly enhance the efficiency of mining in large-scale data. The existing research reviews are mostly focused on comparing method categories, lacking summaries from key technical perspectives such as pruning strategies and population update strategies. This paper provides a review of existing mining methods, including full sets and derived efficient patterns. For full sets HUPM methods, the paper presents a pioneering summary and comparison of pruning strategies, including transaction-weighted utility upper bounds, encoding vectors, probabilistic bit vector compression, and prelarge concepts. It also comprehensively examines population update strategies from multiple perspectives, including particle swarm optimization, ant colony optimization, artificial bee colony, and artificial fish swarm methods. For derived HUPM methods, the review introduces and analyzes approaches based on top-k, closed, high-average, and fuzzy utility patterns, discussing the problems they address and their developmental status. Addressing the limitations of current meta-heuristic HUPM methods, the paper proposes future research directions, including data stream mining based on adaptive sliding windows, cross-level meta-heuristic constraints, and multimodal high utility patterns mining.},
  archive      = {J_KIS},
  author       = {Han, Meng and Yang, Wenyan and Dai, Zhenlong and Yang, Shurong and Zhu, Shineng},
  doi          = {10.1007/s10115-025-02392-8},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5469-5510},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A review of meta-heuristic high utility patterns mining methods},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating beyond backpropagation: On alternative training methods for deep neural networks. <em>KIS</em>, <em>67</em>(7), 5437-5468. (<a href='https://doi.org/10.1007/s10115-025-02370-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backpropagation has long been the de facto algorithm for training deep neural networks due to its effectiveness in optimising network parameters. However, the algorithm is not without its limitations, such as high computational requirement, sensitivity to initialisation, weight transport problem, vanishing and exploding gradient, and convergence issues. Many such issues have been highlighted, underscoring the significance of finding an alternative to train deep neural networks. In this survey paper, we aim to critically assess the commended methodologies that either mitigate the constraints inherent in the backpropagation algorithm or provide alternative strategies that obviate the requirement of utilising backpropagation for training neural models. By categorising these methods, we provide a comprehensive understanding of the advancements made in the respective spheres, which can be a valuable resource for researchers and practitioners seeking to explore alternatives to backpropagation in deep neural network training.},
  archive      = {J_KIS},
  author       = {Birjais, Roshan and Wang, Kevin I-Kai and Abdulla, Waleed},
  doi          = {10.1007/s10115-025-02370-0},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {5437-5468},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Navigating beyond backpropagation: On alternative training methods for deep neural networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing cybersecurity using optimized anti-interference dynamic integral neural network-based intrusion detection system. <em>KIS</em>, <em>67</em>(6), 5413-5435. (<a href='https://doi.org/10.1007/s10115-025-02343-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity has become a critical concern due to the exponential growth of the Internet of Things (IoT), computer networks, and associated applications, which have introduced new vulnerabilities and increased the risk of cyberattacks. Detecting such anomalies and designing an efficient intrusion detection system (IDS) is essential to secure interconnected systems. Therefore, this paper proposes an enhancing cybersecurity using optimized anti-interference dynamic integral neural network-based intrusion detection system (AIDINN-CSD). Here, the input data is collected through CIC IoT 2022 dataset. The input CIC IoT 2022 dataset is preprocessed using smoothing–sharpening filter (SSF) for handling missing values and data normalization. Synthetic minority oversampling technique (SMOTE) is used for data balancing. Then, the tyrannosaurus optimization algorithm (TOA) selects relevant features from the preprocessed data. The selected features are used by anti-interference dynamic integral neural network (AIDINN) for detecting normal and attack class from the data. Then, the weight parameters of AIDINN are optimized using Capuchin search optimization algorithm (CSOA) for improving accuracy and lowering computational time. The results show that the proposed technique attains 99.23% accuracy rate, 98.97% precision and 98.47% detection rate by outperforming existing techniques. These results show the effectiveness of the AIDINN-CSD in addressing the limitations of conventional IDS, particularly its ability to handle imbalanced datasets and reduce false positives thereby offering a promising solution for enhancing IoT network security and mitigating cyber threats.},
  archive      = {J_KIS},
  author       = {Chaudhary, Deevesh and Shekhawat, Deepika and Gupta, Sunita and Kalwar, Anju and Mishra, Neha and Nawal, Meenakshi},
  doi          = {10.1007/s10115-025-02343-3},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5413-5435},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing cybersecurity using optimized anti-interference dynamic integral neural network-based intrusion detection system},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The significance of kappa and F-score in clustering ensemble: A comprehensive analysis. <em>KIS</em>, <em>67</em>(6), 5377-5412. (<a href='https://doi.org/10.1007/s10115-025-02388-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering ensemble techniques have gained significant attention due to their ability to enhance partition results’ accuracy and robustness. Selective clustering ensemble (SCE) and weighted clustering ensemble (WCE) methods further improve performance by selecting and weighting base partitions or clusters based on their diversity and stability. However, striking a balance between these two factors remains challenging. The primary difficulty lies in evaluating the quality of base partitions and clusters. Existing evaluation criteria, such as normalized mutual information (NMI) and its variants, suffer from inherent flaws, including symmetric problem, context meaning problem, and the disregard for small clusters’ importance. To address these limitations, this paper proposes a novel evaluation method that utilizes kappa and F-score. We introduce a new SCE method that employs kappa to select informative base partitions and utilizes F-score to assign weights to clusters based on their stability. Empirical validation on real datasets demonstrates the effectiveness and efficiency of the proposed approach. The code is available at https://github.com/Jarvisyan/DSKF-matlab .},
  archive      = {J_KIS},
  author       = {Yan, Jie and Liu, Xin and Qi, Ji and You, Tao and Zhang, Zhong-Yuan},
  doi          = {10.1007/s10115-025-02388-4},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5377-5412},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {The significance of kappa and F-score in clustering ensemble: A comprehensive analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient distributed co-movement pattern detection framework for streaming trajectory. <em>KIS</em>, <em>67</em>(6), 5355-5375. (<a href='https://doi.org/10.1007/s10115-025-02369-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ubiquitous of GPS-equipped devices has been generating massive trajectory data that record the movements of pedestrians, vehicles, and other moving objects. As one of the fundamental trajectory applications, the mining of co-movement patterns has been used extensively in location-based services (LBS), such as future motion forecasting and social recommendations. These applications require massive real-time data processing capabilities. However, the majority of existing research primarily focuses on historical data, while the rest minority targets the stream scenario, but only works on optimizing efficiency rather than scalability. In this paper, we focus on the distributed real-time co-movement pattern detection on trajectory stream. First, we propose a distributed streaming data processing framework based on Apache Flink. This framework consists of two stages: clustering and pattern mining. To accelerate clustering, we developed partitioning methods such as QGrid structure to achieve effective parallel processing of the spatial aspect of the clustering process. To efficiently perform pattern enumeration, we fully utilize the spatial information carried by cluster to narrow down the search space and perform verification effectively. Compared with the existing research on co-movement, we use spatial information to effectively integrate the two stages of data processing, reduce the search space of the pattern mining process, and improve the overall efficiency of the framework. Extensive experiments compared to existing methods have fully confirmed the efficiency of the proposed framework and its constituent technologies.},
  archive      = {J_KIS},
  author       = {Cheng, Tong and Chao, Pingfu and Zhang, Kenan and Fang, Junhua and Xu, Jiajie},
  doi          = {10.1007/s10115-025-02369-7},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5355-5375},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An efficient distributed co-movement pattern detection framework for streaming trajectory},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CTVAE: Contrastive tabular variational autoencoder for imbalance data. <em>KIS</em>, <em>67</em>(6), 5335-5354. (<a href='https://doi.org/10.1007/s10115-025-02377-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance, where datasets often lack sufficient samples for minority classes, is a persistent challenge in machine learning. Existing solutions often generate synthetic data to mitigate this issue, but they typically struggle with complex data distributions, primarily because they focus on oversampling the minority class while neglecting the relationships with the majority class. To overcome these limitations, we propose the Contrastive Tabular Variational Autoencoder (CTVAE), which integrates conditional Variational Autoencoders with contrastive learning techniques. CTVAE excels at generating high-quality synthetic samples that capture the intricate data distributions of both minority and majority classes. Additionally, it can be seamlessly integrated with variants of the Synthetic Minority Oversampling Technique (SMOTE) for enhanced effectiveness. Experimental results demonstrate that CTVAE substantially improves classification performance on imbalanced datasets, offering a more robust and holistic solution to the class imbalance problem.},
  archive      = {J_KIS},
  author       = {Wang, Alex X. and Le, Minh Quang and Duong, Huu-Thanh and Van, Bay Nguyen and Nguyen, Binh P.},
  doi          = {10.1007/s10115-025-02377-7},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5335-5354},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CTVAE: Contrastive tabular variational autoencoder for imbalance data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated deep learning and blockchain-based framework for cloud manufacturing with improved customer satisfaction. <em>KIS</em>, <em>67</em>(6), 5301-5334. (<a href='https://doi.org/10.1007/s10115-025-02373-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, cloud manufacturing has been employed with blockchain technologies. Many researchers have analyzed the third-party problems between customers and service providers. The cloud manufacturing providers handle the service provided for the customers. Hence, the third-party problem occurs in service-related participants. So, third-party involvement has to be evaded for better client satisfaction. Also, proper supervisions are needed to provide efficient services by the cloud manufacturers. Managing more clouds is a complex task. The security of the data might be at risk if the cloud manufacturing frameworks are not properly managed. Therefore, we developed a deep learning-based customer satisfaction model with Blockchain technology to securely analyze customer reviews for increasing sales growth in companies. Initially, data related to customer reviews is attained from standard sources. The gathered data are given to the prediction stage. Here, we use the cloud manufacturing platform for customer satisfaction. The sentiment analysis is performed by Adaptive Transformer Network (ATN) to predict customer reviews. Here, the parameter present in the ATN is optimized by Hybrid Sewing Training with Osprey Optimization Algorithm (HSTOOA). The developed HSTOOA optimization is used for choosing the cloud manufacturing provider based on customer satisfaction. The performance of the designed customer review prediction method is improved by parameter optimization.},
  archive      = {J_KIS},
  author       = {Ramshankar, N. and Shathik, J. Anvar and Raju, K. and Murugesan, S.},
  doi          = {10.1007/s10115-025-02373-x},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5301-5334},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Integrated deep learning and blockchain-based framework for cloud manufacturing with improved customer satisfaction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QUERY2TREE: A reasoning model for answering logical queries based on knowledge graph embedding and large language model. <em>KIS</em>, <em>67</em>(6), 5271-5300. (<a href='https://doi.org/10.1007/s10115-025-02347-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning to answer logical queries over a knowledge graph has been paid attention to research recently. Previous studies have only used either distance measurement or machine learning approaches processed logical operators such as conjunction, disjunction, and negation but considering the entity description information of knowledge graph yet. This study proposes a model, named QUERY2TREE, to combine a graph representative learning model and large language model to augment semantic for embedded entities of the knowledge graph such that these entities can handle logical operations more accurately during the reasoning process. Specifically, the QUERY2TREE used the graph neural network (GNN) to embed entities and Gemma model, a family of Google Gemini model, to embed the entity's descriptive information into low-dimensional space. Then, these embedded entities were indexed by K-D tree according to each relation over the knowledge graph. Next, we used nearest neighbor search algorithm of K-D tree to build logical operations such as projection, intersection, union, and negation. Finally, we applied these created logical operations to infer the answer to complex questions. We experimented with our model over benchmark knowledge graphs such as FB15k, FB15k-237, and NELL995. As a result, the QUERY2TREE model improved the accuracy better than the baseline models QUERY2BOX, ConE, CQD-CO, MLP, SILR, GNN-QE, CKGR, LACK, and LACT by up to 69.6% of MRR and by up to 69.4% of hits@3.},
  archive      = {J_KIS},
  author       = {Phan, Truong H. V. and Do, Phuc},
  doi          = {10.1007/s10115-025-02347-z},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5271-5300},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {QUERY2TREE: A reasoning model for answering logical queries based on knowledge graph embedding and large language model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ambiguity in information systems: Rows, columns, and the ellsberg paradox. <em>KIS</em>, <em>67</em>(6), 5247-5270. (<a href='https://doi.org/10.1007/s10115-024-02334-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present paper investigates information systems stemming from rough set theory against the backdrop of the Ellsberg Paradox. In the early 1960s, D. Ellsberg introduced and discussed a type of informational uncertainty/ambiguity that could not be adequately modelled by the concept of measurable risk. Twenty years later, Z. Pawlak developed rough set theory—a mathematical framework aimed at modelling and managing different types of informational uncertainty that may be found in data analysis (including various forms of imprecision, vagueness, or indecision). However, as in economics in the 1960s, by uncertainty it has commonly been meant, in default of any other alternative interpretation, a sort of measurable risk. In the present study, we would like to fill this research gap and examine D. Ellsberg’s distinction between measurable risk and unmeasurable ambiguity in the context of various forms of information systems related to rough set theory. It turns out that this distinction brings an interesting reformulation of rough sets and reveals hidden facets of this theory. Specifically, we investigate two versions of ambiguity that can be derived from the Ellsberg paradox and relate them to the row and column oriented representations of information systems.},
  archive      = {J_KIS},
  author       = {Wolski, Marcin and Gomolińska, Anna},
  doi          = {10.1007/s10115-024-02334-w},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5247-5270},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Ambiguity in information systems: Rows, columns, and the ellsberg paradox},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GC-PTransE: Multi-step attack inference method based on graph convolutional neural network and translation embedding. <em>KIS</em>, <em>67</em>(6), 5215-5245. (<a href='https://doi.org/10.1007/s10115-025-02387-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasing complexity of cyberattacks, their varied and complex methods pose risks to individuals, organizations, and governments, making the prediction of complex, multi-step cyberattacks a crucial aspect of cyberdefense. However, current methods for predicting cyberattacks mostly focus on single-query, single-step link scenarios and only consider direct relationships between entities, overlooking the complex inference patterns embedded within cyberattack graphs. Consequently, this paper proposes a multi-step attack inference method called GC-PTransE, based on graph convolutional neural networks and translational embedding. By classifying and embedding knowledge graphs of cyberattacks, it effectively represents entities and their relationships, uncovers hidden associations in multi-hop paths, and accurately predicts cyberattack scenarios. The method first classifies cyberattack data using graph convolutional neural networks (GCN) and then embeds the symbolic and descriptive information of cyberattack entities into a low-dimensional continuous vector space using PTransE. It navigates the graph structure data along the paths of triples to identify entities and relationships in cyberattack scenarios. Using the PCRA algorithm, it assigns a confidence level to each path and selects paths based on their confidence levels. It combines path relationships and scores the energy of new triples to predict outcomes, further completing the cyberattack knowledge graph. Based on our constructed dataset, the proposed method was evaluated, and the experimental results show that this technique significantly improves accuracy in inference prediction compared to other embedding inference models. Comparisons with real cyberattack knowledge demonstrate the effectiveness of this method.},
  archive      = {J_KIS},
  author       = {Ren, Weiwu and Li, Wenjuan and Hong, Yu and Du, Yazhou and Gao, Yuan and Zhang, Hewen and Xia, Mingqi},
  doi          = {10.1007/s10115-025-02387-5},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5215-5245},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GC-PTransE: Multi-step attack inference method based on graph convolutional neural network and translation embedding},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KerGNNT: An interpretable graph neural network recommendation model. <em>KIS</em>, <em>67</em>(6), 5187-5213. (<a href='https://doi.org/10.1007/s10115-025-02376-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recommendation systems, mining the implicit relationship between users and items is the key to achieving accurate recommendations. Graph structures can represent users, items, and even the interactive relationship between the two entities. Graph neural networks (GNNs) have inherent advantages in processing graph-structured data, particularly in capturing the connection relationships between graph nodes and learning representations of graph data. However, calculating the similarity between graph structures has been a persistent challenge in improving the performance of graph neural network-based recommendation systems. We propose a novel recommendation model based on GNNs called KerGNNT, where Transformers are employed to optimize the representation learning method for the graph. We extract node-centered subgraph and use graph kernels to compute kernel values of each node in the input graph to update the subgraph representation. The graph kernel allows for a more precise comparison of the similarity between graph filters and input subgraphs. Furthermore, the proposed model uses Transformer to optimize the graph representation learning. In order to capture the interactive information between nodes, the output of the GNN is connected to the Transformer, dual-encoding the position and structure of subgraphs to further enhance graph representation. We conduct a series of experiments on four different datasets of different scales and categories (COLLAB, IMDB-BINARY, Reddit, and ZINC), and the experimental results demonstrate that our proposed model performs better in terms of mean absolute error (MAE) and mean squared error (MSE), indicating an advantage in the accuracy of recommendations. KerGNNT also exhibits good interpretability. Visualization through GNN filters helps us understand the features that GNN focuses on. The experimental results also verify that the features extracted by our proposed model are positively correlated with improving the recommendation performance. Compared with baseline models, our proposed model reduces the MAE by at least 51.2% and the MSE by at least 51.6%, which indicates the improvement of recommendation performance.},
  archive      = {J_KIS},
  author       = {Liang, Shengbin and Ma, Jinfeng and Sun, Fuqi and Chen, Tingting and Lu, Xixi and Ren, Shuanglong},
  doi          = {10.1007/s10115-025-02376-8},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5187-5213},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {KerGNNT: An interpretable graph neural network recommendation model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal feature tuning model by variants of convolutional neural network with LSTM for driver distract detection in IoT platform. <em>KIS</em>, <em>67</em>(6), 5151-5186. (<a href='https://doi.org/10.1007/s10115-025-02342-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, traffic accidents are caused due to the distracted behaviors of drivers that have been noticed with the emergence of smartphones. Due to distracted drivers, more accidents have been reported in recent years. Therefore, there is a need to recognize whether the driver is in a distracted driving state, so essential alerts can be given to the driver to avoid possible safety risks. For supporting safe driving, several approaches for identifying distraction have been suggested based on specific gaze behavior and driving contexts. Thus, in this paper, a new Internet of Things (IoT)-assisted driver distraction detection model is suggested. Initially, the images from IoT devices are gathered for feature tuning. The set of convolutional neural network (CNN) methods like ResNet, LeNet, VGG 16, AlexNet GoogleNet, Inception-ResNet, DenseNet, Xception, and mobilenet are used, in which the best model is selected using Self Adaptive Grass Fibrous Root Optimization (SA-GFRO) algorithm. The optimal feature tuning CNN model processes the input images for obtaining the optimal features. These optimal features are fed into the long short-term memory (LSTM) for getting the classified distraction behaviors of the drivers. From the validation of the outcomes, the accuracy of the proposed technique is 95.89%. Accordingly, the accuracy of the existing techniques like SMO-LSTM, PSO-LSTM, JA-LSTM, and GFRO-LSTM is attained as 92.62%, 91.08%, 90.99%, and 89.87%, respectively, for dataset 1. Thus, the suggested model achieves better classification accuracy while detecting distracted behaviors of drivers and this model can support the drivers to continue with safe driving habits.},
  archive      = {J_KIS},
  author       = {Farhan, Hameed Mutlag and Türkben, Ayça Kurnaz and Naseri, Raghda Awad Shaban},
  doi          = {10.1007/s10115-025-02342-4},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5151-5186},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Optimal feature tuning model by variants of convolutional neural network with LSTM for driver distract detection in IoT platform},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal aspect-based sentiment analysis based on a dual syntactic graph network and joint contrastive learning. <em>KIS</em>, <em>67</em>(6), 5125-5149. (<a href='https://doi.org/10.1007/s10115-025-02372-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal aspect-based sentiment analysis aims to integrate various modalities of information, such as images and text, through comprehensive analysis to more accurately capture user sentiments across different aspects. Existing multimodal research methods fail to fully utilize rich syntactic information in the text and do not comprehensively align intermodal and intramodal features before downstream tasks, thereby limiting the overall model performance. Therefore, we propose a novel method, multimodal aspect-based sentiment analysis based on a dual syntactic graph network and joint contrastive learning, to enhance the model’s sentiment classification performance. Specifically, we first construct a sentiment knowledge-enhanced graph by combining sentiment knowledge and syntactic features. Then, we utilize the graph convolutional networks and the graph attention networks to capture local syntactic information and global attention weights, respectively. Subsequently, we devise a bidirectional fusion mechanism to integrate dual-channel features. Furthermore, a multisource feature alignment method is designed through contrastive learning to achieve consistency between and within modalities in the feature representation space. Finally, the integrated features are used for multimodal aspect-based sentiment analysis, and contrastive learning is used to improve the overall model’s sentiment classification accuracy. Extensive experiments are conducted on two benchmark datasets, TWITTER-2015 and TWITTER-2017, and the results validate the effectiveness of our proposed model.},
  archive      = {J_KIS},
  author       = {Yu, Bengong and Xing, Yu and Yang, Ying and Cao, Chengwei and Shi, Zhongyu},
  doi          = {10.1007/s10115-025-02372-y},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5125-5149},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multimodal aspect-based sentiment analysis based on a dual syntactic graph network and joint contrastive learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Struct-KGS2S: A structural context-based sequence-to-sequence model for link prediction in knowledge graphs. <em>KIS</em>, <em>67</em>(6), 5105-5124. (<a href='https://doi.org/10.1007/s10115-025-02364-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, description-based approaches are widely employed for the link prediction task on knowledge graphs, largely due to the integration of pre-trained language models (PLMs). These approaches enable the effective exploration and utilization of the rich textual data present in knowledge graphs. However, despite their strengths, models that rely on textual feature descriptions still face certain limitations compared to structure-based methods, particularly in terms of providing additional contextual information for entities. Structure-based methods, by representing entities and their relationships in embedding spaces, deliver stronger performance by allowing entities to carry contextual information through their relative positions to neighboring entities. In this paper, we propose the Struct-KGS2S model for link prediction on knowledge graphs, which integrates structural information from the graph with a seq2seq PLM-based model. To assess our model, we conducted experiments on three widely-used link prediction datasets: FB15k-237, WN18RR, and Wikidata5M, evaluating performance using the MRR and Hit@K metrics. Our experimental results demonstrate that the proposed method not only surpasses existing PLM-based models on these metrics but also achieves competitive results with structure-based models, setting new state-of-the-art performance on several benchmark datasets.},
  archive      = {J_KIS},
  author       = {Dang, Hien and Nguyen, Thu},
  doi          = {10.1007/s10115-025-02364-y},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5105-5124},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Struct-KGS2S: A structural context-based sequence-to-sequence model for link prediction in knowledge graphs},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recurrent-optimized user association representation for multi-target cross-domain sequential recommendation. <em>KIS</em>, <em>67</em>(6), 5077-5103. (<a href='https://doi.org/10.1007/s10115-025-02371-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, research on multi-target cross-domain recommendation and sequential recommendation has advanced significantly, and their combination has attracted increasing attention from researchers. The challenge of cross-domain recommendation lies in how to balance the relationship between domain-common features and domain-specific features. Meanwhile, the difficulty of sequential recommendation is how to integrate item features with user representations to improve performance, which makes cross-domain sequential recommendation face greater challenges. To address the challenges in these two fields, we enhance the reusability of user representations through extracting two categories of user association representations as feature containers for cross-domain sequential recommendation. Based on this, we propose a recurrent-balance learning approach to optimize and balance the two user representations, achieving both fine-grained user modeling in sequential recommendation and balanced feature learning in cross-domain recommendation. When the number of domains increases, our method has good scalability through fine-tuning. Experimental results demonstrate the advantages of recurrent-balance learning, and our model outperforms current state-of-the-art cross-domain recommendation methods.},
  archive      = {J_KIS},
  author       = {Chen, Nuo and Zheng, Lin and Chen, Sentao},
  doi          = {10.1007/s10115-025-02371-z},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5077-5103},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Recurrent-optimized user association representation for multi-target cross-domain sequential recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for neural topic models with variance–invariance–covariance regularization. <em>KIS</em>, <em>67</em>(6), 5057-5075. (<a href='https://doi.org/10.1007/s10115-025-02367-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our study, we propose a self-supervised neural topic model (NTM) that combines the power of NTMs and regularized self-supervised learning methods to improve performance. NTMs use neural networks to learn latent topics hidden behind the words in documents, enabling greater flexibility and the ability to estimate more coherent topics compared to traditional topic models. On the other hand, some self-supervised learning methods use a joint embedding architecture with two identical networks that produce similar representations for two augmented versions of the same input. Regularizations are applied to these representations to prevent collapse, which would otherwise result in the networks outputting constant or redundant representations for all inputs. Our model enhances topic quality by explicitly regularizing latent topic representations of anchor and positive samples. We also introduced an adversarial data augmentation method to replace the heuristic sampling method. We further developed several variation models including those on the basis of an NTM that incorporates contrastive learning with both positive and negative samples. Experimental results on three datasets showed that our models outperformed baselines and state-of-the-art models both quantitatively and qualitatively.},
  archive      = {J_KIS},
  author       = {Xu, Weiran and Hirami, Kengo and Eguchi, Koji},
  doi          = {10.1007/s10115-025-02367-9},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5057-5075},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Self-supervised learning for neural topic models with variance–invariance–covariance regularization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separation axioms in fuzzy multiset topological spaces. <em>KIS</em>, <em>67</em>(6), 5043-5055. (<a href='https://doi.org/10.1007/s10115-025-02352-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key intent of this article is to present the conception of fuzzy multi- $$T_i (i=0,1,s,2,2\frac{1}{2},3,4)$$ spaces within the framework of fuzzy multiset topological spaces, incorporating a fuzzy multipoint structure. It is shown that a fuzzy- $$T_1$$ space is a specific instance of a fuzzy multi- $$T_1$$ space to underscore the pivotal role of multiplicity within this context. Also, we delve into the relationships among those spaces by different examples.},
  archive      = {J_KIS},
  author       = {Ray, Satabdi and Hoque, Md Mirazul and Bhattacharya, Baby and Tripathy, Binod Chandra},
  doi          = {10.1007/s10115-025-02352-2},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5043-5055},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Separation axioms in fuzzy multiset topological spaces},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse dynamic topic model with topic birth and death over time. <em>KIS</em>, <em>67</em>(6), 5019-5041. (<a href='https://doi.org/10.1007/s10115-025-02368-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past twenty years, topic modeling has gradually become popular as a powerful tool, extracting useful and meaningful latent representations from large texts. Research on topic evolution, focusing on the representation of changes in topics over time, has begun to attract extensive attention in the fields of information retrieval and data mining. The dynamic topic model is a classical model for topic evolution. It assumes all topics exist throughout the entire time period, overlooking the fact that topics that were previously important are no longer considered, and new topics can also emerge. To address this issue, we propose a novel Bayesian sparse dynamic topic model, utilizing a spike-and-slab prior distribution to capture topic birth and death. The results demonstrate that our proposed model can effectively estimate both the topic distribution and topic sparsity at the same time. Furthermore, simulations and empirical studies on two real-world datasets demonstrate that our proposed model outperforms the classical dynamic topic model and provides rich semantic information on focused topics.},
  archive      = {J_KIS},
  author       = {Zhou, Rui and Wang, Feifei and Liu, Chang and Lu, Xiaoling},
  doi          = {10.1007/s10115-025-02368-8},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {5019-5041},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sparse dynamic topic model with topic birth and death over time},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label-specific disentanglement and correlation-guided fusion for multi-label classification. <em>KIS</em>, <em>67</em>(6), 4991-5017. (<a href='https://doi.org/10.1007/s10115-025-02359-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label classification, label-specific features have stronger connection with the label than original ones. Popular methods rely on either representative instance-based transformation or shared feature selection. However, the former hardly achieves good coupling between stages, while the latter lacks pertinence in feature generation. In this paper, we propose an end-to-end approach named label-specific disentanglement and correlation-guided fusion (LSDF) to handle these issues. First, the self-attention mechanism is employed to explore shared feature representations with rich semantic information. Second, the specific features are disentangled from the share one via constructing dedicated multilayer perceptron. Third, guided by third-order label correlation, the tailored features of each label are enriched by fusing with that of the relevant two. Finally, the specific features of each label are separately fed into their respective classifiers for prediction. The results of comprehensive experiments on sixteen benchmark datasets validate the superiority of LSDF against other well-established algorithms. The source code is available at https://github.com/FanSmale/LSDF .},
  archive      = {J_KIS},
  author       = {Li, Yang and Wang, Kun and Tan, Leting and Min, Fan},
  doi          = {10.1007/s10115-025-02359-9},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {4991-5017},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Label-specific disentanglement and correlation-guided fusion for multi-label classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A CTO-based GRU model for identifying emotions from textual data. <em>KIS</em>, <em>67</em>(6), 4967-4990. (<a href='https://doi.org/10.1007/s10115-025-02354-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions play a crucial role in human communication, influencing interactions and decision-making processes. Integrating emotional awareness into machines is increasingly important, as it enables more natural and effective interactions between humans and machines. The proposed work focuses on emotion detection using the ISEAR dataset. During preprocessing, word-intensity lexicon is utilized to extract the features, which were then categorized into positive and negative groups. A Class Topper Optimization (CTO) based Gated Recurrent Unit (GRU) model is proposed to classify the sentences into two groups, effectively framing the task as a binary classification problem. The performance of the proposed model is evaluated against other models using various metrics and resulted in 98.87% accuracy, 98.51% precision, 100% recall, 95.58% specificity and 99.25% F1 score, mean squared error 2.23%, root mean squared error 15.99% and mean absolute error 8.57% and outperformed some existing models, providing more accurate and reliable classifications. This advancement in emotion detection is significant, as it enhances the ability to understand and respond to human emotions, which is essential for applications in fields such as mental health, human-computer interactions and social media analysis.},
  archive      = {J_KIS},
  author       = {Soloman, Shyam Sunder Jannu and Seb, Behilo and Baydeti, Nagaraju and Das, Dushmanta Kumar},
  doi          = {10.1007/s10115-025-02354-0},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {4967-4990},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A CTO-based GRU model for identifying emotions from textual data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompting visual dialog with implicit logical knowledge. <em>KIS</em>, <em>67</em>(6), 4949-4966. (<a href='https://doi.org/10.1007/s10115-025-02384-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual dialog using additional knowledge can naturally leverage the reasoning and generalization ability to learn to infer better answers. Traditional visual dialog models rely on extracting explicit, symbolic knowledge from entity-centric knowledge bases. However, in real-world scenarios, it is also necessary to perform implicit logical reasoning regarding events, knowledge, and the states of entities involved in the questions to provide accurate answers. This capability requires models to possess a high reserve of implicit knowledge. Thus, in this paper, we propose to prompt visual dialog with implicit logical knowledge from data and model perspectives. In terms of training data, we focus on augmenting both implicit and explicit knowledge through the Chain-of-Thought strategy based on Large Language Models. Leveraging this knowledge-augmented data, we design a novel Dual-Stream Debiasing Network (DSDN) and apply contrastive decoding to search for strings that maximize a weighted difference in likelihood between the stronger knowledge-based modules and the weaker amateur modules, thus mitigating the impact of undesirable language biases. The experimental results and analyses on VisDial v1.0 dataset, demonstrate the superiority of our proposed model. The code will be available soon.},
  archive      = {J_KIS},
  author       = {Zhang, Zefan and Li, Yanhui and Zhang, Weiqi and Bai, Tian},
  doi          = {10.1007/s10115-025-02384-8},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {4949-4966},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Prompting visual dialog with implicit logical knowledge},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Process mining on sensor data: A review of related works. <em>KIS</em>, <em>67</em>(6), 4915-4948. (<a href='https://doi.org/10.1007/s10115-024-02297-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining is an efficient technique that combines data analysis and behavioural process aspects to uncover end-to-end processes from data. Recently, the application of process mining on unstructured data has become popular. Particularly, sensor data from IoT-based systems allow process mining to uncover novel insights that can be used to identify bottlenecks in the process and support decision-making. However, the application of process mining requires bridging challenges. First, (raw) sensor data must be abstracted into discrete events to be useful for process mining. Second, meaningful events must be distilled from the abstracted events, fulfilling the purpose of the analysis. In this paper, a comprehensive literature study is conducted to understand the field of process mining for sensor data. The literature search was guided by three research questions: (1) what are common and underrepresented sensor types for process mining, (2) which aspects of process mining are covered on sensor data, and (3) what are the best practices to improve the understanding, design, and evaluation of process mining on sensor data. A total of 36 related papers were identified, which were then used as a foundation to structure the field of process mining on sensor data and provide recommendations and future research directions. The findings serve as a starting point for designing new techniques, enhancing the dissemination of related approaches, and identifying research gaps in process mining on sensor data.},
  archive      = {J_KIS},
  author       = {Brzychczy, Edyta and Aleknonytė-Resch, Milda and Janssen, Dominik and Koschmider, Agnes},
  doi          = {10.1007/s10115-024-02297-y},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {4915-4948},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Process mining on sensor data: A review of related works},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Atom search optimization: A systematic review of current variants and applications. <em>KIS</em>, <em>67</em>(6), 4813-4914. (<a href='https://doi.org/10.1007/s10115-025-02389-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atom search optimization (ASO) is a renowned physics-based metaheuristic algorithm which takes inspiration from the fundamental theory of molecular dynamics and imitates the natural atomic movement characteristics in conformity with classical mechanics. ASO has demonstrated its preeminence over many well-known algorithms in solving real-world optimization problems and flexible implementation; hence, it has great recognition in academic communities. Due to this rising attraction, numerous modifications and improved variants of ASO have been designed and ASO finds applications in many fields including engineering, computer science, medicine, health sector, etc. Therefore, this study aims at reviewing and analyzing the ASO along with its variants. In this review, many reported studies based on ASO have been identified, collected, discussed and summed up. We have identified and selected 141 ASO-related papers published in recognized journals. The strengths as well as weaknesses of ASO have been discussed. The performance of ASO and its selected variants in convergence analysis was evaluated on 23 benchmark test functions with dimension 30. Furthermore, statistical analysis was performed to rank and validate the performance of ASO and its variants. The results reveal that QASO exhibits better overall effectiveness of 91.304% and ranks first in dimension 30 based on both Friedman and Kruskal–Wallis tests. The results also illustrate that the conventional ASO is the fastest algorithm amidst the compared algorithms in terms of computational time. In addition, the results show that the ESA-ASO achieves better results regarding the performance index and mean absolute error test. According to analysis of variance (ANOVA) test, Tukey HSD (honestly significant difference) post hoc test and Kruskal–Wallis test, the findings demonstrate that there are no statistically significant differences between ASO and its variants. Finally, the prospects for future trends are suggested.},
  archive      = {J_KIS},
  author       = {Mugemanyi, Sylvère and Qu, Zhaoyang and Rugema, François Xavier and Dong, Yunchang and Wang, Lei and Mutuyimana, Félicité Pacifique and Mutabazi, Emmanuel and Habumuremyi, Providence and Mutabazi, Rita Clémence and Muhirwa, Alexis and Bananeza, Christophe and Nshimiyimana, Arcade and Kagaju, Clarisse and Nsengumuremyi, Jean},
  doi          = {10.1007/s10115-025-02389-3},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {4813-4914},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Atom search optimization: A systematic review of current variants and applications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of manual and dynamic approaches for cybersecurity taxonomy generation. <em>KIS</em>, <em>67</em>(6), 4785-4811. (<a href='https://doi.org/10.1007/s10115-025-02382-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this work is to provide a systematic literature review of techniques for taxonomy generation across the cybersecurity domain. Cybersecurity taxonomies can be classified into manual and dynamic, each one of which focuses on different characteristics and tails different goals. Under this premise, we investigate the current state of the art in both categories with respect to their characteristics, applications and methods. To this end, we perform a systematic literature review in accordance with an extensive analysis of the tremendous need for dynamic taxonomies in the cybersecurity landscape. This analysis provides key insights into the advantages and limitations of both techniques, and it discusses the datasets which are most commonly used to generate cybersecurity taxonomies.},
  archive      = {J_KIS},
  author       = {Spyros, Arnolnt and Kougioumtzidou, Anna and Papoutsis, Angelos and Darra, Eleni and Kavallieros, Dimitrios and Tziouvaras, Athanasios and Tsikrika, Theodora and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
  doi          = {10.1007/s10115-025-02382-w},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {4785-4811},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A comprehensive survey of manual and dynamic approaches for cybersecurity taxonomy generation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in automated program repair: A comprehensive review. <em>KIS</em>, <em>67</em>(6), 4737-4783. (<a href='https://doi.org/10.1007/s10115-025-02383-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review paper presents a comprehensive examination of automated program repair (APR) and its significant contribution to the field of modern software engineering. It elucidates how APR methodologies markedly mitigate manual debugging needs by automating the detection and resolution of software glitches. The study encompasses an in-depth exploration of three primary categories of APR techniques: template-based, machine learning, and deep learning approaches, drawing from an exhaustive evaluation of 41 APR tools. Each category showcases distinct strategies for managing diverse software errors, underscoring the breadth and effectiveness of current APR methodologies. Template-based APR solutions utilize pre-established patterns to efficiently tackle common coding issues, while machine learning-driven approaches dynamically devise repair strategies from historical bug-fix datasets. Deep learning methods extend error rectification boundaries by delving into the semantic context of code, yielding more precise adjustments. The ongoing advancement of APR technologies necessitates researchers to address critical challenges, including the integration of semantic-syntactic analyses, mitigation of data scarcity, optimization of cross-platform tools, development of context-aware approaches, enhancement of fault localization and patch validation processes, and establishment of standardized performance evaluation metrics. This comprehensive analysis underscores the pivotal role of APR in enhancing software efficiency and reliability, representing significant progress in software development and maintenance practices.},
  archive      = {J_KIS},
  author       = {Dikici, Sena and Bilgin, Turgay Tugay},
  doi          = {10.1007/s10115-025-02383-9},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {4737-4783},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Advancements in automated program repair: A comprehensive review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating complexity: A comprehensive review of heterogeneous information networks and embedding techniques. <em>KIS</em>, <em>67</em>(6), 4703-4736. (<a href='https://doi.org/10.1007/s10115-025-02357-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of heterogeneous information networks (HINs) has gained popularity as a means of modeling complex real-world systems with diverse interacting components. Despite the acknowledged advantages of HIN, a considerable number of current studies tend to treat these networks as homogeneous, neglecting the distinctions between different types of components and connections. Recently, there has been a growing recognition among researchers of the importance of treating interconnected data with multiple types as a HIN. They have developed methods to embed such data into a lower-dimensional space while preserving the diverse structures and meanings within the network. These HIN embedding methods offer versatile applications, including fraud detection, system recommendation, disease prediction, etc., leveraging the rich semantics inherent in object types and links within the network. Although HIN provides more structural and semantic information compared to homogeneous networks, it also poses challenges for data mining. In this paper, we present a comprehensive review of the approaches, techniques, and applications employed in HIN, emphasizing the diverse methodologies identified in existing literature.},
  archive      = {J_KIS},
  author       = {Ammar, Khouloud and Inoubli, Wissem and Zghal, Sami and Nguifo, Engelbert Mephu},
  doi          = {10.1007/s10115-025-02357-x},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {4703-4736},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Navigating complexity: A comprehensive review of heterogeneous information networks and embedding techniques},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupled graph neural architecture search with explainable variable propagation operation. <em>KIS</em>, <em>67</em>(5), 4677-4702. (<a href='https://doi.org/10.1007/s10115-024-02329-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most graph neural networks (GNNs) suffer from the over-smoothing problem which limits further improvement of performance. Hence, many studies have decoupled the GNN into two atomic operations, the propagation (P) operation and the transformation (T) operation to propose a paradigm named decoupled graph neural networks (DGNNs) for alleviating this problem. Since manually designing the architecture of DGNNs is time-consuming and expert-dependent, the decoupled graph neural architecture search (DGNAS) methods were proposed and achieved success. However, existing DGNAS methods lack explanation in the design of DGNN architecture with adaptive variable P operation, which hinders researchers from further exploring DGNAS methods. In addition, the naive evolutionary search algorithm used by previous DGNAS methods lacks constraints on the search direction, limiting its search efficiency in exploring the DGNN architecture. To address the above challenges, we propose the decoupled graph neural architecture search with explainable variable propagation operation (DGNAS-EP) method. Specifically, we propose the mean distinguishability (MD) metric to measure the distinguishable state of node representation, which effectively explains the significance of why the DGNAS method should build DGNN architectures with variable P operation. Graphs with different distributions require different P operations in DGNN architecture to adaptively adjust, thus obtaining the optimal MD, which is very important for improving the performance of DGNNs. Furthermore, DGNAS-EP utilizes the explored historical DGNN architectures as prior knowledge to constrain the search direction based on evolutionary state, which effectively improves the search efficiency of the DGNAS method. The experiments on real-world graphs show that our proposed method DGNAS-EP outperforms state-of-the-art baseline methods. Codes are available at https://github.com/frankdoge/DGNAS-EP.git.},
  archive      = {J_KIS},
  author       = {He, Changlong and Chen, Jiamin and Li, Qiutong and Wang, Yili and Gao, Jianliang},
  doi          = {10.1007/s10115-024-02329-7},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4677-4702},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Decoupled graph neural architecture search with explainable variable propagation operation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The antecedents and outcomes of accounting information systems usage: The indirect effect of IT knowledge. <em>KIS</em>, <em>67</em>(5), 4651-4675. (<a href='https://doi.org/10.1007/s10115-024-02306-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the digital transformation revolution has emerged in the world, many entrepreneurial businesses have used digital technology tools such as blockchain technology, artificial intelligence and machine learning in the domain of the financial and accounting functions digitalization to improve business decision-making quality and sustain their performance. Although it is global usage, only a few studies have investigated the related issues of digital technology tools in the Jordanian small and medium-sized enterprises (SMEs) context. Accordingly, the current study aims to explore the role of accounting information systems (AIS) usage on decision-making quality by the moderating role of information technology (IT) knowledge. The suggested research model is empirically tested with 237 responses retrieved from owners and managers of Jordanian SMEs. The results show that the perceived usefulness of AIS is impacted by perceived ease of use. Besides, the perceived usefulness of AIS significantly influences perceived satisfaction. The findings also indicate that AIS usage is impacted by perceived usefulness, perceived satisfaction, perceived innovativeness in IT, perceived self-efficacy and perceived compatibility. Interestingly, the results show that perceived ease of use and perceived convenience do not significantly influence the usage of AIS. The findings also show the critical role of AIS usage in improving decision-making quality. Likewise, the findings showed that IT knowledge moderated the relationship between AIS usage and decision-making quality. Lastly, more discussion regarding the limitations and future research is presented at the end of this paper, which completes the whole paper.},
  archive      = {J_KIS},
  author       = {Al-Okaily, Manaf},
  doi          = {10.1007/s10115-024-02306-0},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4651-4675},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {The antecedents and outcomes of accounting information systems usage: The indirect effect of IT knowledge},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-space topological contrastive learning for knowledge graph-aware issue recommendation. <em>KIS</em>, <em>67</em>(5), 4623-4650. (<a href='https://doi.org/10.1007/s10115-025-02355-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opensource communities utilize issues to promote knowledge sharing and discussions among developers. However, as the community scales, the number of issues increases dramatically. To ensure efficient circulation of issues and improve issues utilization, it is crucial to recommend appropriate issues to the developers. Nevertheless, developers’ participation in resolving issues varies depending on their levels of expertise, leading to long-tail and cold-start problems. Additionally, interactions between developers and issues exhibit diverse topological modalities, which presents a challenge for existing recommendation models which are often built on a single type of embedding space, leading to suboptimal performance. To capture complex topological information, we propose the cross-space topological contrastive learning for knowledge graph-aware issue recommendation method. It combines different sparse interaction signals from collaborative filtering and knowledge graph information in Euclidean space and hyperbolic space for dual-space information aggregation. By performing intra-space contrastive learning between multi-hop subgraphs within each space, the contribution of CF signals and KG information can be effectively balanced. Cross-space contrastive learning avoids the occurrence of representation shift in a single space. CTCK alleviates the noise generated during KG propagation and increases consistency between the representations in both spaces. Its effectiveness is further enhanced with our proprietary issue knowledge graph (ISSUEKG), which can be used as auxiliary information to alleviate the long-tail problem. Through extensive experiments on a real-world dataset, we demonstrate that CTCK significantly outperforms 12 state-of-the-art baselines, beating the best method by 4.74, 7.36, and 2.86% on average in terms of AUC, F1-score, and accuracy, respectively.},
  archive      = {J_KIS},
  author       = {Zhang, Leihong and Shi, Yuliang and Qi, Kaiyuan and Wu, Dong and Wang, Xinjun and Yan, Zhongmin and Chen, Zhiyong},
  doi          = {10.1007/s10115-025-02355-z},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4623-4650},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cross-space topological contrastive learning for knowledge graph-aware issue recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domino drift effect approach for probability estimation of feature drift in high-dimensional data. <em>KIS</em>, <em>67</em>(5), 4597-4621. (<a href='https://doi.org/10.1007/s10115-025-02362-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift (and data drift) is a common phenomenon in machine learning models, where the statistical properties of the input data change over time, leading to a decrease in model performance. Detecting data drift is crucial for maintaining the accuracy and reliability of machine learning models in real-world applications. While previous data drift detector approaches can identify if a drift has occurred, these approaches cannot localize which specific features have caused the drift. Feature drift detectors solve this deficiency, but the required number of detectors is equal to the number of dimensions, which is a resource-intensive solution in high-dimensional data. In this paper, we propose a novel approach for feature drift analysis and drift detection based on a domino effect caused by the correlation of features. Our approach, the so-called Domino drift effect (DDE), is based on the empirically proven assumption that an initial reference correlation can be utilized as a proxy for detecting other drifting features. The method analyzes the correlating and drifting behavior, and by using only a subset of all features, it derives inference about the drifting of the remaining features, if co-drifting phenomena occur in the data stream. At co-drifting phenomena, the DDE method can estimate the probability of feature drift, which is particularly useful in high-dimensional datasets. To evaluate the effectiveness of our approach, we conducted experiments on four real-world datasets. The results show that our approach can effectively be used to predict feature drift in the whole dataset, and it has potential industrial applications.},
  archive      = {J_KIS},
  author       = {Szűcs, Gábor and Németh, Marcell},
  doi          = {10.1007/s10115-025-02362-0},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4597-4621},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Domino drift effect approach for probability estimation of feature drift in high-dimensional data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting learning performance using NLP: An exploratory study using two semantic textual similarity methods. <em>KIS</em>, <em>67</em>(5), 4567-4595. (<a href='https://doi.org/10.1007/s10115-024-02293-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most learning analytics (LA) systems provide generic feedback, because they primarily draw on performance data based on quiz scores. This study explored the potential of student-generated summaries as an alternative method for predicting learning performance. Two hundred and fifty-four undergraduates first watched a series of six short video lectures and then wrote a short summary for each one. Based on their median performance quiz scores, the participants were divided into two performance groups. Sparse and dense text vectorization methods were used to represent the video lectures and student summaries. Three semantic textual similarity features were computed using cosine similarity and were used as input into seven common machine learning algorithms. The results indicated that the sparse similarity features outperformed dense ones in classifying performance. Also, the best classification accuracy was achieved using the K-Nearest Neighbors and Random Forrest algorithms. Overall, the findings suggest that semantic similarity measures can be used as additional proxy measures of learning, thereby enabling the real-time monitoring and evaluation of student understanding in LA contexts.},
  archive      = {J_KIS},
  author       = {Papadimas, C. and Ragazou, V. and Karasavvidis, I. and Kollias, V.},
  doi          = {10.1007/s10115-024-02293-2},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4567-4595},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Predicting learning performance using NLP: An exploratory study using two semantic textual similarity methods},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RDPNet: A multi-stage summary generation network for long chat dialogues. <em>KIS</em>, <em>67</em>(5), 4549-4566. (<a href='https://doi.org/10.1007/s10115-025-02358-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of human–computer interaction and natural language generation technology, chat dialogue summarization has attracted extensive attention from researchers, which aims to obtain significant information from chat history. The current chat dialogue summary models mainly focus on short dialogues due to the limitation of input text length. This paper proposes a RDPNet model, an effective multi-stage network for long chat dialogues. The method first uses a Retriever to select significant sentences from the long input dialogue, which not only can shorten the length of the source text, but also improve the structure of the chat. Secondly, a DialoGPT annotator is adopted to label the extracted important sentences so that it can further improve the structure of the dialogue. Finally, a large-scale pre-trained generation model ProphetNet is adopted to generate a concise dialogue summary. The experimental results demonstrate that RDPNet outperforms the state-of-the-art methods on three long chat summarization datasets DIALSUMM, SAMSum and TWEETSUMM and verify the effectiveness for long chat summarization.},
  archive      = {J_KIS},
  author       = {Yang, Shihao and Zhang, Shaoru and Zhang, Huayu and Hung, Chih-Cheng and Yang, Fengqin and Liu, Shuhua},
  doi          = {10.1007/s10115-025-02358-w},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4549-4566},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {RDPNet: A multi-stage summary generation network for long chat dialogues},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing minority data generation through optimization in imbalanced datasets. <em>KIS</em>, <em>67</em>(5), 4523-4547. (<a href='https://doi.org/10.1007/s10115-025-02361-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary objective of research concerning class imbalance problems revolves around the generation of high-quality data for minority classes. Prior investigations have witnessed various approaches to synthesizing data, resulting in varying data quality. This study introduces a novel oversampling framework, termed the Optimal Oversampling Framework (OOF), which adopts a distinctive perspective. OOF uses optimization algorithms to guide the data generation process, ensuring that new samples are not only similar to the minority class but also exhibit sufficient diversity. Specifically, the method combines initialization and evolutionary strategies to refine the generated samples, while evaluating the similarity of the samples to the minority and majority classes through distance and cosine similarity measures. In addition, OOF prevents premature convergence and ensures that the generated samples maintain uniqueness through diversity judgments and fitness function settings. Finally, OOF selects the best quality samples for oversampling by optimizing the ranking. To demonstrate the effectiveness of OOF, we integrated the Particle Swarm Optimization algorithm with OOF and conducted comparative experiments involving nine different oversampling methods across 21 datasets characterized by high class imbalance ratios. The experimental outcomes validate the success of the OOF approach.},
  archive      = {J_KIS},
  author       = {Song, Jiuxiang and Wang, Chuang and Liu, Jizhong},
  doi          = {10.1007/s10115-025-02361-1},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4523-4547},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing minority data generation through optimization in imbalanced datasets},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-bloom: Unleashing the power of pre-trained language models in extracting knowledge graph with predefined relations. <em>KIS</em>, <em>67</em>(5), 4487-4521. (<a href='https://doi.org/10.1007/s10115-025-02345-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained language models have become popular in natural language processing tasks, but their inner workings and knowledge acquisition processes remain unclear. To address this issue, we introduce K-Bloom—a refined search-and-score mechanism tailored for seed-guided exploration in pre-trained language models, ensuring both accuracy and efficiency in extracting relevant entity pairs and relationships. Specifically, our crawling procedure is divided into two sub-tasks. Using a few seed entity pairs to minimize the need for extensive manual effort or predefined knowledge, we expand the knowledge graph with new entity pairs around these seeds. To evaluate the effectiveness of our proposed model, we conducted experiments on two datasets that cover the general domain. Our resulting knowledge graphs serve as symbolic representations of the source pre-trained language models, providing valuable insights into their knowledge capacities. Additionally, they enhance our understanding of the pre-trained language models’ capabilities when automatically evaluated on large language models. The experimental results demonstrate that our method outperforms the baseline approach by up to 5.62% in terms of accuracy in various settings of the two benchmarks. We believe that our approach offers a scalable and flexible solution for knowledge graph construction and can be applied to different domains and novel contexts.},
  archive      = {J_KIS},
  author       = {Vo, Trung and Luu, Son T. and Nguyen, Le-Minh},
  doi          = {10.1007/s10115-025-02345-1},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4487-4521},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {K-bloom: Unleashing the power of pre-trained language models in extracting knowledge graph with predefined relations},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing the power of indirect attacks against trust prediction via preferential path. <em>KIS</em>, <em>67</em>(5), 4459-4486. (<a href='https://doi.org/10.1007/s10115-024-02327-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks in network security are a growing concern, prompting the need for innovative strategies to enhance both attack and defense mechanisms. This paper explores ways to improve adversarial attacks on the fairness and goodness algorithm (FGA) and review to reviewer (REV2), focusing on predicting trust within signed graphs. Unlike traditional time-based models, FGA and REV2 rely on iterative processes for trust propagation. By analyzing network structures, we identify strong ties and weak ties within FGA and discover preferential paths in REV2 that significantly impact information spread during algorithm iterations. Based on these insights, we propose a new approach called the vicinage attack, which enhances adversarial attacks by strategically targeting edges along these critical pathways. Our work highlights adversarial perturbation patterns that affect trust prediction on signed graphs and emphasizes their wide-reaching impact. These findings not only advance adversarial attack techniques but also deepen our understanding of trust propagation patterns. By clarifying the propagation bias in FGA and REV2, this research provides valuable insights for improving network security and developing better adversarial mitigation techniques in trust prediction.},
  archive      = {J_KIS},
  author       = {Bu, Yu and Zhu, Yulin and Geng, Longling and Zhou, Kai},
  doi          = {10.1007/s10115-024-02327-9},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4459-4486},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Unleashing the power of indirect attacks against trust prediction via preferential path},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human identification through panoramic dental radiographs: A novel matching approach. <em>KIS</em>, <em>67</em>(5), 4431-4458. (<a href='https://doi.org/10.1007/s10115-025-02353-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric person identification systems identify individuals using personal characteristics such as fingerprints, eyes or facial recognition. However, in some critical situations, such as fires, serious traffic accidents, earthquakes or serious injuries, these features can become ineffective. In certain situations, dental characteristics may become the only valid biometric feature for identification. In these cases, forensic dentists work by examining dental structures to establish a person's identity. Currently, studies are being carried out to develop an automated recognition system based on computer vision to assist forensic dentists. However, due to the difficulties in processing panoramic X-ray images and challenges in accessing the data, person matching studies with these images are limited. This paper presents a novel method for matching people based on panoramic X-ray images. Dental person recognition studies can proceed either by investigating the similarity of teeth or by examining the similarity of jaws. In this work, a new approach that uses keypoint descriptors to perform tooth-jaw matching is proposed. This approach offers a high match rate by allowing to search for dental features on a jaw-by-jaw basis and requires less computational complexity than tooth-to-tooth matching. Unlike jaw-to-jaw approaches, it is possible to match individual teeth. The method presented in this study provides a novel approach with significant matching accuracy and efficiency. By evaluating the effectiveness of these methods on panoramic images, the study contributes to forensic dental identification methods in scenarios where traditional biometric features may fall short.},
  archive      = {J_KIS},
  author       = {Bozkurt, Mustafa Hakan and Karagol, Serap and Omezli, Mehmet Melih},
  doi          = {10.1007/s10115-025-02353-1},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4431-4458},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Human identification through panoramic dental radiographs: A novel matching approach},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GDDRec: Graph neural diffusion model for diversified recommendation. <em>KIS</em>, <em>67</em>(5), 4401-4430. (<a href='https://doi.org/10.1007/s10115-025-02348-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network, as a research hotspot in the field of recommendation system, can effectively deal with complex data structures. Although the existing recommendation model has made some progress, it still has obvious limitations. On the one hand, limited information about users and items may lead to insufficient accuracy. On the other hand, maintaining high accuracy while pursuing the diversity of recommended content is also a challenge. To address the above problems, we propose GDDRec, a diversified recommendation model based on graph diffusion, which introduces a self-gating mechanism that can accurately capture and implement the reinforcement of subtle information between users and items, thereby significantly enhancing the accuracy of recommended content. In addition, we further enrich the feature between users and items by generating diffusion heterogeneous graphs and integrating them with the graph aggregation module, while retaining the personalized interaction characteristics of users and improving the diversity of recommendations. To verify the validity of the GDDRec model, we conduct extensive experiments on two real-world datasets. The experimental results demonstrate that our model can significantly enhance recommendation diversity while ensuring accuracy, thus effectively balancing these two aspects to provide a richer and more personalized user experience.},
  archive      = {J_KIS},
  author       = {Li, Ying and Zhao, Muzi and Zhang, Jiawei and Xie, Zhenping and Liu, Yuan and Zhan, Qianyi},
  doi          = {10.1007/s10115-025-02348-y},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4401-4430},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {GDDRec: Graph neural diffusion model for diversified recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel contrastive multi-view framework for heterogeneous graph embedding. <em>KIS</em>, <em>67</em>(5), 4373-4399. (<a href='https://doi.org/10.1007/s10115-025-02346-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graphs, characterized by diverse node types and relational structures, serve as powerful tools for representing intricate real-world systems. Understanding the complex relationships within these graphs is essential for various downstream tasks. However, traditional graph embedding methods often struggle to represent the rich semantics and heterogeneity of these graphs effectively. In order to tackle this challenge, we present an innovative self-supervised multi-view network (SMHGNN) designed for the embedding of heterogeneous graphs. SMHGNN utilizes three complementary views: meta-paths, meta-structures, and the network schema, to thoroughly capture the complex relationships and interactions among nodes in heterogeneous graphs. The proposed SMHGNN utilizes a self-supervised learning paradigm, thereby obviating the necessity for annotated data while simultaneously augmenting the model's capability for generalization. Furthermore, we present an innovative mechanism for the identification of positive and negative samples, which is predicated on a scoring matrix that integrates both node characteristics and graph topology, thereby proficiently differentiating authentic positive pairs from false negatives. Comprehensive empirical evaluations conducted on established benchmark datasets elucidate the enhanced efficacy of our SMHGNN in comparison with cutting-edge methodologies in heterogeneous graph embedding. (The proposed method achieved a 0.75–2.4% improvement in node classification and a 0.78–2.7% improvement in clustering across diverse datasets when compared to previous state-of-the-art methods.).},
  archive      = {J_KIS},
  author       = {Noori, Azad and Balafar, Mohammad Ali and Bouyer, Asgarali and Salmani, Khosro},
  doi          = {10.1007/s10115-025-02346-0},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4373-4399},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel contrastive multi-view framework for heterogeneous graph embedding},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial homogeneity-aware transfer learning for urban flow prediction. <em>KIS</em>, <em>67</em>(5), 4349-4371. (<a href='https://doi.org/10.1007/s10115-025-02363-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precondition for a deep prediction model to achieve high prediction accuracy is sufficient data, which is not always available in practice. Thus, urban transfer learning methods (i.e., fine-tuning-based methods) are successively proposed to mitigate this issue. However, these existing approaches do not estimate source knowledge transferability and therefore easily lead to negative transfer. Spatial homogeneity (i.e., F1 score gained by link prediction) can provide fine-grained topological structure indication for source knowledge transfer. To this end, we propose a spatial homogeneity-aware transfer learning framework named SHTL for urban flow prediction. In particular, SHTL consists of a link prediction model and an urban flow prediction model. Firstly, the link prediction model is used to capture regional road network topology and regional spatial homogeneity is obtained by evaluating model predictability in each region. Secondly, the urban flow prediction model is optimized by selective source training and target fine-tuning based on spatial homogeneity. We evaluate SHTL on real-world taxi and bike datasets and the result shows that SHTL outperforms the state-of-the-art baselines.},
  archive      = {J_KIS},
  author       = {Liu, Yinghui and Shen, Guojiang and Fu, Yanjie and Feng, Zehui and Zhao, Zhenzhen and Kong, Xiangjie},
  doi          = {10.1007/s10115-025-02363-z},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4349-4371},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Spatial homogeneity-aware transfer learning for urban flow prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RoEMF: Rotational embedding multimodal fusion for link prediction. <em>KIS</em>, <em>67</em>(5), 4325-4348. (<a href='https://doi.org/10.1007/s10115-025-02386-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal link prediction aims to identify missing head and tail entities in the relational triples of multimodal knowledge graphs. However, each modality contains distinct information, and how to effectively fuse multimodal data has become a complex challenge. To address this issue, the rotational embedding multimodal fusion (RoEMF) model was proposed based on rotary position encoding (RoPE). The model employs a multi-head cross-attention mechanism, combined with RoPE, to enhance the representation of positional and contextual information, thereby improving the multimodal data fusion. It focuses on integrating information from different subspaces while capturing cross-modal correlations to mitigate potential data loss, enhances feature fusion, and optimizes the heterogeneity of the representation. Additionally, the cross-modal joint decision loss was proposed to reduce the model’s reliance on single-modal data, aiding in the identification of missing head and tail entities, while enhancing the accuracy and generalization ability of multimodal link prediction. Experimental results on three public MMKG benchmarks demonstrate the outstanding performance of RoEMF compared with other methods in link prediction.},
  archive      = {J_KIS},
  author       = {Lu, Shiteng and Li, Xinqiang and Zhang, Wenqi and Tang, Huanling},
  doi          = {10.1007/s10115-025-02386-6},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4325-4348},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {RoEMF: Rotational embedding multimodal fusion for link prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A keyword extraction model study in the movie domain with synopsis and reviews. <em>KIS</em>, <em>67</em>(5), 4301-4323. (<a href='https://doi.org/10.1007/s10115-025-02350-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of keywords is increasingly being applied across diverse domains, including the movie industry, whose main platforms are adopting advanced natural language processing techniques. Algorithms for automatic extraction of keywords can provide relevant information in this domain. The most novel approaches covering several categories (statistics, graphs, word embedding, and hybrid) have been considered in a model study framework. They have been implemented, applied, and evaluated with standard datasets. In addition, a movie dataset with gold standard keywords, based on textual metadata from synopses and reviews, has been specifically developed for this scope. Keyword extraction models have been evaluated in terms of F-score and computation time. Furthermore, content analysis, both quantitative and qualitative, of the extracted keywords in the movie context has been performed. Results show a great variability in model performance and computation time among the different models. Qualitative results, in addition to F-score and computation time, demonstrate that keyword extraction works better with synopses than with reviews. The quantitative content analysis revealed that EmbedRank effectively reduces redundancy and limits the use of proper nouns, leading to high-quality keywords.},
  archive      = {J_KIS},
  author       = {González-Santos, Carlos and Vega-Rodríguez, Miguel A. and Pérez, Carlos J. and Martínez-Sarriegui, Iñaki and López-Muñoz, Joaquín M.},
  doi          = {10.1007/s10115-025-02350-4},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4301-4323},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A keyword extraction model study in the movie domain with synopsis and reviews},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel fuzzy twin support vector machine using mass-based dissimilarity measure. <em>KIS</em>, <em>67</em>(5), 4233-4300. (<a href='https://doi.org/10.1007/s10115-025-02344-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To mitigate the negative impact of noise on twin support vector machines (TWSVM), researchers have integrated fuzzy set theory with TWSVM, utilizing fuzzy membership degrees to characterize the influence of various samples in constructing the optimal hyperplane. This approach leads to the development of a fuzzy twin support vector machine (FTSVM). However, existing fuzzy membership degree assignment strategies have several drawbacks: (1) The geometric distance-based assignment strategy exhibits high computational complexity and neglects the surrounding environment of data points. (2) The information entropy-based assignment strategy is sensitive to sample variations and poses challenges in selecting the appropriate membership functions. To address these issues, this study introduces a mass-based dissimilarity measure into the fuzzy membership degree assignment process. The primary factor for assessing the dissimilarity between two instances is the minimum area probability block that encompasses both instances. Then this assignment strategy is integrated with the TWSVM and a novel fuzzy twin support vector machine using mass-based dissimilarity measure (MDFTSVM) is proposed. Additionally, MDFTSVM employs a coordinate descent strategy with shrinking by an active set to reduce computational complexity and significantly improve model training speed. Experimental evaluations are conducted using artificially constructed datasets as well as UCI datasets, which confirm the effectiveness of MDFTSVM in addressing binary classification problems with noise. The results also demonstrate its superior robustness and generalization performance compared to support vector machine, fuzzy support vector machine, TWSVM, FTSVM, twin bounded support vector machine, fuzzy membership assignment strategy based on dissimilarity measure, and fuzzy twin support vector machine based on affinity and class probability models.},
  archive      = {J_KIS},
  author       = {Wang, Xia and Wu, Gaohao and Hao, Guosheng and Zhang, Zichen},
  doi          = {10.1007/s10115-025-02344-2},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4233-4300},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel fuzzy twin support vector machine using mass-based dissimilarity measure},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving inference via rich path information and logic rules for document-level relation extraction. <em>KIS</em>, <em>67</em>(5), 4207-4231. (<a href='https://doi.org/10.1007/s10115-024-02336-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of document-level relation extraction is the classification of the relations between pairs of entities within a document. The identification of relations between inter-sentence entity pairs is one of the main challenges, as there may be no direct connections between the entities. Previous researches have attempted to address this challenge by leveraging path information between entities in the graph to predict their relations. However, these methods ignore the insufficiency of relations information in the existing paths and the absence of connecting paths between entity pairs. In this paper, we propose an effective inference model that enhances inter-sentence reasoning at both the document and global levels. Our model enhances path information by aggregating features from various sources along the logical reasoning paths between entities within each document. Additionally, the model learns relational inference rules from large graphs created from multiple documents and applies these rules to enrich existing relational knowledge. The experimental results indicate that our model outperforms existing models on three widely used benchmark datasets. Moreover, further analysis highlights that our model is especially effective in document-level relation extraction, particularly for inter-sentence relation extraction.},
  archive      = {J_KIS},
  author       = {Su, Huizhe and Xie, Shaorong and Yu, Hang and Yuan, Changsen and Wang, Xinzhi and Luo, Xiangfeng},
  doi          = {10.1007/s10115-024-02336-8},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4207-4231},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving inference via rich path information and logic rules for document-level relation extraction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the transferability of adversarial robustness to target domains. <em>KIS</em>, <em>67</em>(5), 4139-4206. (<a href='https://doi.org/10.1007/s10115-024-02333-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge transfer is an effective method for learning, particularly useful when labeled data are limited or when training a model from scratch is too expensive. Most of the research on transfer learning focuses on achieving accurate models, overlooking the crucial aspect of adversarial robustness. However, ensuring robustness is vital, especially when applying transfer learning in safety-critical domains. We compare robustness of models obtained by 11 training procedures on source domains and 3 retraining schemes on target domains, including normal, adversarial, contrastive, and Lipschitz constrained training variants. Robustness is analyzed by adversarial attacks with respect to two different transfer learning model outputs: (i) the latent representations and (ii) the predictions. Studying latent representations in correlation with predictions is crucial for robustness of transfer learning models, since they are solely learned on the source domain. Besides adversarial attacks that aim at changing the prediction, we also analyze the effect of directly attacking representations. Our results show that adversarial robustness can transfer across domains, but effective robust transfer learning requires techniques that ensure robustness independent of the training data to preserve them during the transfer. Retraining on the target domain has a minor impact on the robustness of the target model. Representations exhibit greater robustness compared to predictions across both the source and target domain.},
  archive      = {J_KIS},
  author       = {Kopetzki, Anna-Kathrin and Bojchevski, Aleksandar and Günnemann, Stephan},
  doi          = {10.1007/s10115-024-02333-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4139-4206},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Evaluating the transferability of adversarial robustness to target domains},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clusterability test for categorical data. <em>KIS</em>, <em>67</em>(5), 4113-4138. (<a href='https://doi.org/10.1007/s10115-024-02317-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of clusterability evaluation is to check whether a clustering structure exists within the data set. As a crucial yet often-overlooked issue in cluster analysis, it is essential to conduct such a test before applying any clustering algorithm. If a data set is unclusterable, any subsequent clustering analysis would not yield valid results. Despite its importance, the majority of existing studies focus on numerical data, leaving the clusterability evaluation issue for categorical data as an open problem. Here, we present TestCat, a testing-based approach to assess the clusterability of categorical data in terms of an analytical p-value. The key idea underlying TestCat is that clusterable categorical data possess many strongly associated attribute pairs and hence the sum of Chi-squared statistics of all attribute pairs is employed as the test statistic for p-value calculation. We apply our method to a set of benchmark categorical data sets, showing that TestCat outperforms those solutions based on existing clusterability evaluation methods for numeric data. To the best of our knowledge, our work provides the first way to effectively recognize the clusterability of categorical data in a statistically sound manner.},
  archive      = {J_KIS},
  author       = {Hu, Lianyu and Dong, Junjie and Jiang, Mudi and Liu, Yan and He, Zengyou},
  doi          = {10.1007/s10115-024-02317-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4113-4138},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Clusterability test for categorical data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking wisdom: Enhancing biomedical question answering with domain knowledge. <em>KIS</em>, <em>67</em>(5), 4087-4112. (<a href='https://doi.org/10.1007/s10115-025-02338-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical factoid question answering aims to provide factual answers from biomedical articles for questions related to the biomedical or healthcare domain. Recent advances in biomedical factoid question answering primarily involve using pre-trained Language Models (LMs) to retrieve relevant passages or comprehend text snippets for extracting accurate answers. However, due to the relatively smaller scale of biomedical datasets compared to those used in broader domains, fine-tuning LMs for the biomedical domain often results in decreased accuracy. To address this, we introduce the Biomedical Knowledge-enhanced Question Answering Framework (BK-QAF). This framework retrieves, ranks, and employs domain-specific concepts from the Unified Medical Language System (UMLS) to enhance the comprehension and reasoning capabilities of language models. By using Graph Attention Networks (GATs) to analyze the interconnections and relationships between entities in biomedical texts, we identify the most relevant concepts for the query. The framework then ranks these UMLS concepts by relevance, expands the questions with the top-ranked concepts, and processes them using a fine-tuned language model. We evaluated our framework using the BioASQ 6b, 7b, and 8b datasets, which are widely adopted in the field. Empirical evaluations demonstrate the superior performance of the proposed framework over state-of-the-art baselines across metrics such as Strict Accuracy and MRR@5. The framework’s effectiveness is assessed using three distinct GAT architectures, demonstrating robustness across different configurations.},
  archive      = {J_KIS},
  author       = {Azad, Bita and Ali Akbar Alavi, Mahdiyar and Jafarzadeh, Parastoo and Ensan, Faezeh and Androutsos, Dimitri},
  doi          = {10.1007/s10115-025-02338-0},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4087-4112},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Unlocking wisdom: Enhancing biomedical question answering with domain knowledge},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data augmentation with automated machine learning: Approaches and performance comparison with classical data augmentation methods. <em>KIS</em>, <em>67</em>(5), 4035-4085. (<a href='https://doi.org/10.1007/s10115-025-02349-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. State-of-the-art approaches are increasingly relying on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. The focus of this work is on image data augmentation methods. Nonetheless, we cover other data modalities, especially in cases where the specific data augmentations techniques being discussed are more suitable for these other modalities. For instance, since automated data integration methods are more suitable for tabular data, we cover tabular data in the discussion of data integration methods. The work also presents extensive discussion of techniques for accomplishing each of the major subtasks of the image data augmentation process: search space design, hyperparameter optimization and model evaluation. Finally, we carried out an extensive comparison and analysis of the performance of automated data augmentation techniques and state-of-the-art methods based on classical augmentation approaches. The results show that AutoML methods for data augmentation currently outperform state-of-the-art techniques based on conventional approaches.},
  archive      = {J_KIS},
  author       = {Mumuni, Alhassan and Mumuni, Fuseini},
  doi          = {10.1007/s10115-025-02349-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {4035-4085},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Data augmentation with automated machine learning: Approaches and performance comparison with classical data augmentation methods},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review on sentiment analysis techniques, challenges, and future trends. <em>KIS</em>, <em>67</em>(5), 3967-4034. (<a href='https://doi.org/10.1007/s10115-025-02365-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last ten years, social media site mining, including Twitter, Facebook, Instagram, and all other websites, has become a popular area of study. In the world of digital media today, people are getting more and more vocal because they love to share their opinions. User-generated material abounds on social media platforms and apps such as Facebook, WhatsApp, and Twitter, providing wealthy content to gather sentiments. Comments are another way that the most active and social voices can make themselves heard, which in turn gives us a window into their feelings. Sentiment analysis is the task of comprehending the emotions and opinions conveyed in text and other media. It has various applications in domains along with social media such as e-commerce, health, politics, and marketing. This paper delineates the generic process of sentiment analysis and reviews the main methods, challenges, and trends in this field. The main goal of this survey paper is to survey the current state-of-the-art research works on sentiment analysis (SA) techniques and related fields and to compare the performance of different deep learning models for sentiment polarity. The paper also discusses the recent studies that have employed machine learning, deep learning, and hybrid models to address sentiment polarity problems, which is the categorization of text into positive, negative, or neutral sentiments. The paper evaluates the results of different models on a series of datasets. The paper’s main contributions are the elaborate classifications of numerous recent articles and the demonstration of the recent research directions in sentiment analysis and its related fields. The paper aims to provide a comprehensive overview of SA techniques with succinct details.},
  archive      = {J_KIS},
  author       = {Ali, Hafiz Muhammad Usman and Farooq, Qaisar and Imran, Azhar and El Hindi, Khalil},
  doi          = {10.1007/s10115-025-02365-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3967-4034},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A systematic literature review on sentiment analysis techniques, challenges, and future trends},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cybersecurity in the AI era: Analyzing the impact of machine learning on intrusion detection. <em>KIS</em>, <em>67</em>(5), 3915-3966. (<a href='https://doi.org/10.1007/s10115-025-02366-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of machine learning (ML) techniques for intrusion detection systems (IDS) in cybersecurity has become increasingly prevalent, demonstrating substantial advancements and effectiveness. This survey systematically reviews the use of ML techniques in IDS for cybersecurity, highlighting both advancements and associated challenges. By examining 130 recent studies, this survey systematically reviews the use of ML techniques in IDS, categorizing them into traditional ML-based, single-task deep learning (DL)-based, and multi-task DL-based approaches. Among cited works, traditional ML models like the decision tree and Gaussian mixture have achieved accuracies of 99.96% and 99.0%, respectively. However, most DL models outperform these traditional ML models, with some research indicating that AE+GAN models can achieve 100.0% accuracy. Our analysis identifies emerging trends in DL for complex representation learning and highlights challenges like overfitting and adaptability to new threats, especially in IoT environments. Additionally, it identifies several promising research directions, including exploring effective hybrid model architectures, quantifying task correlation for improved knowledge transfer, investigating model deployment in edge and IoT environments, etc.},
  archive      = {J_KIS},
  author       = {Dong, Huiyao and Kotenko, Igor},
  doi          = {10.1007/s10115-025-02366-w},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3915-3966},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cybersecurity in the AI era: Analyzing the impact of machine learning on intrusion detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive review of deep learning-based tiny object detection: Challenges, strategies, and future directions. <em>KIS</em>, <em>67</em>(5), 3825-3913. (<a href='https://doi.org/10.1007/s10115-025-02375-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tiny object detection (TOD) is a pivotal yet challenging area in computer vision, marked by issues like limited pixel representation, extreme scale variations, occlusion, and noisy backgrounds. This survey provides a systematic and comprehensive review of TOD methodologies, tracing the transition from traditional convolutional neural network (CNN)-based models to state-of-the-art transformer-based architectures. Key limitations of CNNs, such as feature loss at smaller scales and sensitivity to cluttered environments, are addressed by attention mechanisms and multi-scale learning, foundational to transformer designs. Applications spanning autonomous driving, aerial surveillance, medical imaging, underwater detection, and security highlight the critical role of TOD in real-world scenarios. Detailed evaluations of models like YOLO, Faster R-CNN, DETR, Vision Transformers (ViTs), and hybrid frameworks across datasets such as MS COCO, TinyPersons, DeepLesion, DOTA, and UAV123 demonstrate notable advancements. Transformer-based models, including MS Transformer and HTDet, outperform their predecessors by capturing fine-grained features, enhancing robustness, and improving computational efficiency. This survey identifies pressing challenges, including dataset limitations like class imbalance, inadequate representation of tiny objects, and incomplete annotations, alongside evaluating TOD-specific metrics and dynamic object tracking. Future directions include the development of balanced and diverse datasets, weakly supervised learning frameworks, semi-supervised learning strategies, and real-time TOD optimization for edge computing. By integrating detailed comparative analyses and highlighting critical gaps, this survey offers a robust foundation for advancing TOD methodologies, driving enhanced detection accuracy, and expanding applicability in complex real-world environments.},
  archive      = {J_KIS},
  author       = {Muzammul, Muhamad and Li, Xi},
  doi          = {10.1007/s10115-025-02375-9},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3825-3913},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Comprehensive review of deep learning-based tiny object detection: Challenges, strategies, and future directions},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative research aspects of big data modelling, management and analytics. <em>KIS</em>, <em>67</em>(5), 3821-3824. (<a href='https://doi.org/10.1007/s10115-024-02171-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Cuzzocrea, Alfredo},
  doi          = {10.1007/s10115-024-02171-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {3821-3824},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Innovative research aspects of big data modelling, management and analytics},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection by utilizing kernel-based fuzzy rough set and entropy-based non-dominated sorting genetic algorithm in multi-label data. <em>KIS</em>, <em>67</em>(4), 3789-3819. (<a href='https://doi.org/10.1007/s10115-025-02341-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning, which involves assigning multiple class labels to each instance, becomes increasingly complex when dealing with large-scale mixed datasets featuring high-dimensional feature spaces. These mixed datasets often involve a combination of numerical and categorical features, which exacerbate the challenges of multi-label learning by introducing additional layers of uncertainty and variability. Traditional classification methods, although effective in simpler scenarios, often fail to address these complexities resulting in significant errors. To overcome this, we have developed an entropy-based objective function that captures the intricate interplay between features and classes, while accounting for the inherent uncertainty of mixed data. This objective function explicitly accounts for the heterogeneous nature of mixed datasets, ensuring robust feature selection across diverse attribute types. To tackle these challenges, we propose a memetic algorithm that integrates fuzzy rough sets with enhancements from kernel fuzzy rough sets (KFRS), and the Non-dominated Sorting Genetic Algorithm II. This synergy enables the extraction of optimal feature subsets that significantly improve classification performance. By leveraging kernel-based similarity measures, KFRS refines the partitions formed by fuzzy set memberships for distinct classes, ensuring precise alignment of data samples with multiple labels, while effectively handling the complexities of mixed-data representation. A key strength of our approach lies in its ability to preserve valuable information through KFRS-driven feature selection. Empirical evaluations on three benchmark datasets highlight the effectiveness of the proposed methodology. The results validate the superiority of our feature selection strategy, grounded in kernel-modulated neighborhoods; furthermore, the implementation demonstrates a notable improvement in both solution quality and search efficiency, establishing it as a highly promising method for multi-label learning tasks.},
  archive      = {J_KIS},
  author       = {Hamidzadeh, Javad and Mehravaran, Zahra and Harati, Ahad},
  doi          = {10.1007/s10115-025-02341-5},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3789-3819},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature selection by utilizing kernel-based fuzzy rough set and entropy-based non-dominated sorting genetic algorithm in multi-label data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new multiattribute decision making method based on interval-valued intuitionistic fuzzy values. <em>KIS</em>, <em>67</em>(4), 3769-3787. (<a href='https://doi.org/10.1007/s10115-024-02328-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing multiattribute decision making (MADM) methods have the drawback that they cannot distinguish the preference order of alternatives in the context of interval-valued intuitionistic fuzzy values (IVIFVs) in some situations or they cannot deal with interval-valued intuitionistic fuzzy weights. Therefore, it is necessary to develop a new MADM method to overcome the drawback of the existing MADM methods. This paper proposes a new MADM method to overcome the drawbacks of the existing MADM methods in the context of IVIFVs, where the weights of attributes given by the decision makers are represented by interval-valued intuitionistic fuzzy values. Firstly, we propose a novel score function (SCFT) of IVIFVs to overcome the drawbacks of the existing SCFTs of IVIFVs. Based on the proposed SCFT of IVIFVs and the decision matrix (DCM) provided by the decision maker (DCMR), a score matrix (SCMX) is constructed. Then, a new nonlinear programming (NLP) model is developed to obtain the optimal weights of the attributes, which are used to calculate the weighted scores of the alternatives to rank the alternatives. The proposed MADM method can overcome the shortcomings of the existing MADM methods in the environment of IVIFVs.},
  archive      = {J_KIS},
  author       = {Chen, Shyi-Ming and Chen, Deng-Cyun},
  doi          = {10.1007/s10115-024-02328-8},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3769-3787},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new multiattribute decision making method based on interval-valued intuitionistic fuzzy values},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parameter-free text classification method based on dual compressors. <em>KIS</em>, <em>67</em>(4), 3737-3767. (<a href='https://doi.org/10.1007/s10115-024-02335-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the information age, the emergence of massive amounts of text data has made effective text classification a critical challenge. Traditional classification methods often underperform or incur high computational costs when dealing with heterogeneous data, limited labeled data, or domain-specific data. To address these challenges, this paper proposes a novel text classification model, GZclassifier, designed to improve both accuracy and efficiency. GZclassifier employs two distinct compressors to handle information data and calculates distances in parallel to facilitate classification. This dual-compressor approach enhances the model’s ability to manage diverse and sparse data effectively. We conducted extensive experimental evaluations on a range of public datasets, including those with few-shot learning scenarios, to assess the proposed method’s performance. The results demonstrate that our model significantly outperforms traditional methods in terms of classification accuracy, robustness, and computational efficiency. The GZclassifier’s ability to handle limited labeled data and domain-specific contexts highlights its potential as an efficient solution for real-world text classification tasks. This study not only advances the field of text classification but also showcases the model’s practical applicability and benefits in various text processing scenarios.},
  archive      = {J_KIS},
  author       = {Mao, Yanxu and Ding, Ying and Cui, Tiehan},
  doi          = {10.1007/s10115-024-02335-9},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3737-3767},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A parameter-free text classification method based on dual compressors},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A healthy and reliable rating profile expansion approach to address data sparsity in food recommendation systems. <em>KIS</em>, <em>67</em>(4), 3699-3735. (<a href='https://doi.org/10.1007/s10115-024-02331-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food recommendation systems have become increasingly popular due to the proliferation of online food service websites. Accordingly, the ratings assigned by users are one of the most important resources in these systems. However, users generally express their opinions about a few foods, which results in data sparsity. Furthermore, food recommendation is a health-critical task, as recommending unhealthy foods to users may threaten their health. In this paper, we developed a novel rating profile expansion approach for food recommenders that considers both health and reliability measures. This approach enhances the efficiency of the user’s rating profile by including healthy and reliable virtual ratings. Specifically, we introduce a probabilistic rating profile evaluation technique to determine whether a profile needs to be expanded. Then, those profiles with an insufficient number of ratings are automatically expanded by adding virtual ratings obtained using the opinions of users who belong to the target user’s community. For this purpose, the users are grouped using a novel time-aware community detection algorithm based on their preferences. Moreover, a health-aware reliability measure is proposed so that only the most reliable virtual ratings are accounted for in the target user’s rating profile expansion. Therefore, the developed approach not only mitigates issues stemming from sparse data in food recommendation systems but also makes them more effective in recommending healthy foods to users. Experiments conducted on two publicly available real-world datasets demonstrated that the developed system is superior to other baseline models.},
  archive      = {J_KIS},
  author       = {Ahmadian, Sajad and Rostami, Mehrdad and Jalali, Seyed Mohammad Jafar and Oussalah, Mourad and Farrahi, Vahid},
  doi          = {10.1007/s10115-024-02331-z},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3699-3735},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A healthy and reliable rating profile expansion approach to address data sparsity in food recommendation systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit knowledge-augmented prompting for commonsense explanation generation. <em>KIS</em>, <em>67</em>(4), 3663-3698. (<a href='https://doi.org/10.1007/s10115-024-02326-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonsense explanation generation refers to reasoning and explaining why a commonsense statement contradicts commonsense knowledge, such as why the statement “My dad grew volleyballs in his garden” is nonsensical. While such reasoning is trivial for humans, it remains a challenge for AI systems. Despite their notable performance in tasks like text generation and reasoning, large language models (LLMs) often fall short of consistently generating coherent and accurate commonsense explanations. To bridge this gap, we propose a novel Two-stage Identification and Prompting (TIP) framework for enhancing LLMs’ ability to handle the task of commonsense explanation generation. Specifically, in the first stage, TIP identifies the nonsensical concept in the given statement, pinpointing the specific element that contradicts commonsense knowledge. In the second stage, TIP generates implicit knowledge based on the identified nonsensical concept and then leverages this implicit knowledge to guide the adopted LLMs in generating explanations. In order to demonstrate the effectiveness of the proposed TIP framework for commonsense explanation generation, we conducted extensive experiments based on the ComVE dataset and a newly constructed CSE dataset, where a variety of LLMs are evaluated. The experimental results show that TIP consistently outperforms all baseline methods across multiple metrics, demonstrating its effectiveness in improving LLMs’ commonsense reasoning and explanation generation capabilities.},
  archive      = {J_KIS},
  author       = {Ge, Yan and Yu, Hai-Tao and Lei, Chao and Liu, Xin and Jatowt, Adam and Kim, Kyoung-sook and Lynden, Steven and Matono, Akiyoshi},
  doi          = {10.1007/s10115-024-02326-w},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3663-3698},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Implicit knowledge-augmented prompting for commonsense explanation generation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection using game shapley improved grey wolf optimizer for optimizing cancer classification. <em>KIS</em>, <em>67</em>(4), 3631-3662. (<a href='https://doi.org/10.1007/s10115-025-02340-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer has become a leading global health challenge, significantly impacting mortality rates worldwide. Early diagnosis of cancer can substantially reduce patient mortality and improve treatment success. In recent years, machine learning models have played a vital role in cancer classification, enhancing the precision and efficiency of diagnostic strategies. However, existing feature selection methods are often wrapper-based and computationally intensive, especially with high-dimensional biomedical data. Addressing this gap, we propose a two-stage hybrid algorithm for efficient and accurate feature selection, combining kernel Shapley Value (kSV) and Improved Grey Wolf Optimization (IGWO). In the first stage, kSV is used, which leverages calculation and combination of feature contributions by considering interactions among features. In the second stage, IGWO optimizes the selection process by modeling it as an optimization problem. The proposed kSV-IGWO algorithm was tested on eight benchmark cancer datasets using a support vector machines (SVM) classifier. Experimental results demonstrate that the kSV-IGWO algorithm outperforms conventional methods, achieving high accuracy on benchmark datasets $$-$$ 98.51% for Colon and 97.99% for Lung. This approach effectively identifies key genes, showing improvements in accuracy, ROC, precision, recall, and F1-score. Our findings highlight the kSV-IGWO algorithm’s potential to advance cancer diagnosis, providing a robust tool for high-precision gene selection and improving diagnostic outcomes. The source code is available at https://github.com/Afreen1996/Feature-selection_kSV.git},
  archive      = {J_KIS},
  author       = {Afreen, Sana and Bhurjee, Ajay Kumar and Aziz, Rabia Musheer},
  doi          = {10.1007/s10115-025-02340-6},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3631-3662},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature selection using game shapley improved grey wolf optimizer for optimizing cancer classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An outlier detection algorithm based on local density feedback. <em>KIS</em>, <em>67</em>(4), 3599-3629. (<a href='https://doi.org/10.1007/s10115-024-02324-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is very important in the field of data mining and is applied to various scenarios, such as financial fraud detection and network intrusion. Traditional outlier detection methods usually detect outliers based on the local density of objects, and have achieved some results. However, some challenges still exist: (1) traditional methods only consider neighbor information when calculating the density of an object and ignore the global information embedded in the dataset, leading to the wrong detection of outliers and normal points that are only detected in the global view; (2) traditional methods focus only on comparing the density of an object to its neighbors, ignoring the similarity to its neighbors, leading to incorrectly detecting normal points in sparse regions as outliers, even if all the neighbors of a given point are normal ones. To address these issues, we propose a novel outlier detection algorithm based on the local density feedback (LDF). Our method utilizes principal component analysis (PCA) and a natural neighbor search for initial density estimation. A feedback mechanism is designed to refine the density iteratively by leveraging neighborhood similarity, and to aggregate global information for a more accurate outlierness depiction. By integrating local and global data characteristics, our method reliably detects outliers across diverse datasets. Experimental results on ten datasets show that the LDF algorithm outperforms the existing methods by 10.7% and 1.89% on average in terms of precision and AUC, respectively.},
  archive      = {J_KIS},
  author       = {Zhang, Zhongping and Hou, Yuehan and Jia, Yin and Zhang, Ruibo},
  doi          = {10.1007/s10115-024-02324-y},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3599-3629},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An outlier detection algorithm based on local density feedback},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning and metaheuristic optimization algorithms for feature selection and botnet attack detection. <em>KIS</em>, <em>67</em>(4), 3549-3597. (<a href='https://doi.org/10.1007/s10115-024-02322-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Botnet attacks are done using a set of vulnerable systems called bots and managed by an administrator called botmaster that they carry out attacks on a large scale. Various methods are used to detect such attacks, such as (1) traffic analysis (2) behavior analysis (3) behavior-based detection (4) intrusion detection systems (IDS) (5) honeypot and honeynets (6) DNS query analysis (7) common threat intelligence (8) artificial intelligence algorithms (9) login analysis (10) endpoint protection. In this paper, a new hybrid IDS based on machine learning and meta-heuristic algorithms is proposed based on three steps: (1) pre-processing, (2) feature selection, and (3) attack detection. In the pre-processing stage, including 3 stages of numericalization, normalization, and removal of outliers, the K-Nearest Neighbor (K-NN) is used. In the feature selection stage, the combined SFO-WOA method is used. First, redundant features are removed using SailFish Optimizer (SFO), and a set of features is provided to the Whale Optimization Algorithm (WOA) as an initial population, and this algorithm selects the best features. In the attack detection stage, the PSO-K-means combined method is used. In this method, the particle swarm algorithm (PSO) is used to detect attacks, and then K-means is used to manage the boundaries of the search space. The proposed hybrid method is called SFO-WOA-PSO-K-means. Its performance is compared using machine learning methods such as Tree Ensemble (TE), Chi-squared Automatic Interaction Detection (CHAID), Iterative DiChaudomiser 3 (ID3), Fuzzy Rules, Probabilistic Neural Network (PNN). The proposed method is evaluated using the BOT-IOT dataset, UNSW-NB15. The results have shown that the proposed SFO-WOA-PSO-K-means method has the maximum detection accuracy of 0.998 and 0.995 with the lowest execution time (training and testing) of 65.02 s and 112.33 s and was able to detect attacks. Also, the BOT-IOT dataset has obtained more optimal results.},
  archive      = {J_KIS},
  author       = {Maazalahi, Mahdieh and Hosseini, Soodeh},
  doi          = {10.1007/s10115-024-02322-0},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3549-3597},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Machine learning and metaheuristic optimization algorithms for feature selection and botnet attack detection},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emerging skycube. <em>KIS</em>, <em>67</em>(4), 3513-3547. (<a href='https://doi.org/10.1007/s10115-024-02320-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining multi-criteria decision analysis and trend reversal discovery make it possible to extract globally optimal, or non-dominated, data in relation to several criteria, and then to observe their evolution according to a decision-making property. Thus, we introduce Emerging Skycube, a concept associating Skycube and emerging datacube. As far as we know, no DBMS-integrated solution exists to compute an emerging Skycube, and hence taking advantage of ROLAP analysis tools. An emerging datacube has only one measure: we propose to use several to comply to multi-criteria decision analysis constraints which requires multiple attributes. A datacube is expensive to compute. An emerging datacube is about twice as expensive. On the other hand, an emerging Skycube is cheaper as the trend reversal is computed after two Skycube calculations, which considerably reduces the relation volume in comparison with the initial one. It is possible to save even more computing time and storage space. To this end, we propose two successive reductions. First, a Skycube lossless partial materialization using Skylines concepts lattice, based on the agree concepts lattice and partitions lattice. Then, either the closed emerging Skycube for an information-loss reduction, or the closed emerging L-Skycube for a smaller but lossless reduction.},
  archive      = {J_KIS},
  author       = {Martin Nevot, Mickaël},
  doi          = {10.1007/s10115-024-02320-2},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3513-3547},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Emerging skycube},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness-constrained multigroup influence maximization. <em>KIS</em>, <em>67</em>(4), 3487-3511. (<a href='https://doi.org/10.1007/s10115-024-02314-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization is a well-explored subject within network science, aiming to maximize the spread of influence from a given set of initial individuals to other nodes in the network. This concept finds applications in various fields such as viral marketing, information propagation, news dissemination, and vaccinations. However, traditional influence maximization objectives often overlook the equitable distribution of influenced nodes concerning sensitive attributes like race or gender. In this paper, we address the issue of fair influence maximization, aiming to achieve more equitable outcomes, particularly for minority groups. Our approach involves formulating the problem as the optimization of a welfare function that explicitly incorporates two crucial aspects of fairness: utility and equity. To tackle this challenge, we propose a novel neural network architecture consisting of two specialized subnetworks designed for handling combinatorial optimization problems on graphs. Our framework encompasses multiple notions of utility and fairness, including maximin egalitarian fairness, regularized maximin egalitarian fairness, and leximin fairness. Through extensive experimentation, we demonstrate that our framework is not only applicable in diverse scenarios but also competes favorably with existing algorithms. The results showcase the effectiveness and competitiveness of our approach in achieving fair influence maximization.},
  archive      = {J_KIS},
  author       = {Zhang, Zizhen and Li, Deying and Wang, Yongcai and Chen, Wenping and Zhu, Yuqing},
  doi          = {10.1007/s10115-024-02314-0},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3487-3511},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fairness-constrained multigroup influence maximization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced multimodal recommendation systems through reviews integration. <em>KIS</em>, <em>67</em>(4), 3459-3486. (<a href='https://doi.org/10.1007/s10115-024-02309-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal recommendation systems aim to capture diverse user preferences through data such as text and images, offering more personalized recommendation services. Accurately grasping user preferences can enhance the precision of recommendations and augment the user experience. Existing multimodal graph learning models enhance item representations by capturing the latent attribute relationships between items. However, multimodal features often suffer from modality missing issues, which hinder the model’s ability to fully capture the connections between item attributes. Meanwhile, review data, as a form of textual information, not only provide descriptions of item attributes but also directly reflect user preferences. However, directly incorporating review data into item representations may introduce additional noise, potentially affecting the model’s performance. Therefore, we propose the Personalized Multi-Preference Recommender (PMPR) model, which integrates reviews with multimodal data to extract multifaceted user preferences, enhancing the personalization of recommendations. Specifically, we designed a heterogeneous graph learning module based on reviews and a homogeneous graph learning module that combines the review features with multimodal features to capture users’ diverse preferences. Considering the varying informational content of reviews, PMPR processes each review individually and utilizes user IDs to generate review attention vectors for aggregating the review features to reduce review noise. Finally, we integrate the Top-K method for recommendation. We compare common review processing methods with PMPR’s approach to validate its effectiveness. In comparative analyses across five publicly available datasets, our enhanced model consistently demonstrated superior performance when benchmarked against seven widely recognized models. The results indicate a noteworthy improvement over the current state-of-the-art (SOTA) model, ranging from 2.76% to 22.52% in terms of average performance.},
  archive      = {J_KIS},
  author       = {Fang, Hong and Liang, Jindong and Sha, Leiyuxin},
  doi          = {10.1007/s10115-024-02309-x},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3459-3486},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhanced multimodal recommendation systems through reviews integration},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IBWC: A user-centric approach to multi-objective cloud task scheduling using improved beluga whale optimization. <em>KIS</em>, <em>67</em>(4), 3423-3457. (<a href='https://doi.org/10.1007/s10115-024-02325-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has transformed the IT sector by providing on-demand, scalable computing resources. Effective task scheduling in cloud environments is vital for maximizing resource efficiency, minimizing operational expenses, and boosting overall system performance. As cloud environments grow increasingly complex, multi-objective optimization has become essential for handling diverse and dynamic workloads. To improve task execution efficiency in cloud systems, numerous metaheuristic algorithms and their variations have been developed for scheduling optimization. This study employs the most recent metaheuristics beluga whale optimization (BWO) algorithm for scheduling in conjunction with a multi-objective optimization model to improve cloud system performance. Based on that premise, this study proposed an innovative method known as improved beluga whale for cloud task scheduling (IBWC), which builds upon the latest metaheuristic techniques to optimize multiple objectives simultaneously, including execution time, load, and monetary cost. The Cauchy mutation strategy, the effective producer’s search operator of sparrow search algorithms (SSA), and quasi-opposition-based learning (QOBL) are all used by IBWC to make it better at optimizing globally, converge, and be robust. Moreover, the IBWC-Scheduler provides customizable, user-customized weights for various objectives, enabling firms to tailor optimization objectives to their specific needs. The experimental results indicate that the improved IBWC method significantly outperforms earlier algorithms, delivering an 8–11% reduction in execution time, a 20–25% improvement in load balancing, and a 20–28% reduction in pricing costs. Additionally, IBWC reduces overall cost by 14–20%, solidifying its effectiveness as a robust solution for modern cloud computing challenges.},
  archive      = {J_KIS},
  author       = {Kumar, Ravi and Vardhan, Manu},
  doi          = {10.1007/s10115-024-02325-x},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3423-3457},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {IBWC: A user-centric approach to multi-objective cloud task scheduling using improved beluga whale optimization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLGT: Label-flipping-robust federated learning via guiding trust. <em>KIS</em>, <em>67</em>(4), 3399-3422. (<a href='https://doi.org/10.1007/s10115-024-02323-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a distributed machine learning framework that enables the efficient construction of a global model using a vast amount of decentralized client data. However, FL has little control over the local data and the training process, which is vulnerable to poisoning attacks. Among them, label flipping attacks significantly affect model robustness due to its low implementation threshold. Several approaches have been proposed to alleviate the risk of label flipping attacks, but their robustness is limited due to dependencies on various factors, such as a specific number of malicious clients or the availability of additional clean validation datasets. In this paper, we propose a method called FLGT, which guides server trust toward clients to robustly defend against label flipping attacks. It can obtain a trustworthy model update by utilizing Principal Component Analysis (PCA) to capture attack-related features from the similarity matrix of gradient angles in the last layer updates of local models. The central server first can assess the trustworthiness of clients to identify malicious ones by comparing the cosine similarity between the trustworthy model update and local model updates. Then, the central server can use the trustworthy model update to standardize the scale of local model updates, thereby enhancing system robustness. We conduct extensive evaluations of FLGT on CIFAR-10 and Fashion-MNIST, demonstrating its effectiveness against label flipping attacks and robustness in detecting malicious clients, particularly when the proportion of malicious clients ranges from 10% to 40%.},
  archive      = {J_KIS},
  author       = {Li, Hongjiao and Shi, Zhenya and Jin, Ming and Yin, Anyang and Zhao, Zhen},
  doi          = {10.1007/s10115-024-02323-z},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3399-3422},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {FLGT: Label-flipping-robust federated learning via guiding trust},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic network embedding and its temporal link prediction via constructing community adaptive temporal walking. <em>KIS</em>, <em>67</em>(4), 3373-3398. (<a href='https://doi.org/10.1007/s10115-024-02318-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world networks are always dynamic. Dynamic network representation learning represents nodes in a network as low-dimensional, dense, real-valued vectors while preserving semantic and inferential information as much as possible, it has achieved significant success in areas such as link prediction. It focuses on capturing changes in both the spatial dimensions of the network (e.g., the addition and deletion of nodes and edges, and internal network characteristics like behavioral patterns) and the temporal dimensions (e.g., historical influences). Static network representation learning cannot promptly reflect spatial and temporal changes. To address this issue, we propose a sequence generation method based on community adaptive temporal walking (DyCATW) to reflect the internal evolution patterns of networks. Furthermore, in consideration of the impact of historical network data on network evolution, we propose a self-attention mechanism combining community attention and temporal attention modules. This approach generates node walking sequences through DyCATW, which aggregates node neighbor, community, and temporal information for dynamic network representation learning. Experimental results for temporal link prediction on several real-world datasets demonstrate the superior performance of DyCATW, with results by up to 5.1%.},
  archive      = {J_KIS},
  author       = {Zhou, Mingqiang and Cai, Weikai and Hu, Zhengpeng and Qian, Zhiyuan},
  doi          = {10.1007/s10115-024-02318-w},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3373-3398},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamic network embedding and its temporal link prediction via constructing community adaptive temporal walking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced smart weather prediction through advanced atmospheric analysis and forecasting techniques using binarized spiking neural networks. <em>KIS</em>, <em>67</em>(4), 3343-3372. (<a href='https://doi.org/10.1007/s10115-024-02332-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weather prediction remains a challenging task for meteorological researchers, despite advancements in forecasting technologies. This manuscript proposes an Enhanced Smart Weather Prediction through Advanced Atmospheric Analysis and Forecasting Techniques (SWP-AAFT-BSNN) using Binarized Spiking Neural Networks (BSNN). The method begins by collecting data from a rain prediction database, which is pre-processed using the Sigma-Mixed Unscented Kalman Filter (SMUKF) to remove noise. Optimal feature selection is performed using the Wader Hunt Optimization Algorithm (WHOA). The selected features are fed into BSNN for differentiating weather forecasting as rain and no rain. The Leaf in Wind Optimization Algorithm (LWOA) is introduced to optimize the BSNN weight parameters. The proposed SWP-AAFT-BSNN approach is evaluated under the performance metrics, like accuracy, precision, sensitivity, F1-score, MAE, RMSE, and computational time. The results demonstrate a significant improvement, with accuracy increasing by 19.46%, 24.77%, and 29.50%, RMSE reducing by 18.21%, 35.82%, and 28.64%, and computational time decreasing by 16.64%, 28.46%, and 30.44% when compared to the existing methods.},
  archive      = {J_KIS},
  author       = {Amanullah, M. and Ananthajothi, K. and Divya, D.},
  doi          = {10.1007/s10115-024-02332-y},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3343-3372},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhanced smart weather prediction through advanced atmospheric analysis and forecasting techniques using binarized spiking neural networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceptive dual-graph semantic integration network for aspect sentiment triplet extraction. <em>KIS</em>, <em>67</em>(4), 3317-3342. (<a href='https://doi.org/10.1007/s10115-024-02313-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within fine-grained Aspect-Based Sentiment Analysis, one of the critical subtasks is Aspect Sentiment Triplet Extraction, which aims to extract all triplets from user comments, including aspect terms, associated opinion terms, and the corresponding sentiment polarity. Previous approaches to the study of Aspect Sentiment Triplet Extraction (ASTE) tasks have primarily used an end-to-end strategy based on filling tables. However, these methods typically overlook the multi-dimensional features of linguistic expressions and the associations between words. As a result, valuable interactive information is lacking between extracting aspect terms and opinion terms. To address the above problem, we propose a perceptive dual-graph semantic integration network (PDGSIN). To achieve deeper semantic extraction of the sentence, we employ two graph convolutional networks (PosGCN and SynGCN) to capture part-of-speech relations and syntactic dependencies from the dual graphs (part-of-speech graphs and syntactic dependency graphs) of a sentence, respectively. Meanwhile, we design a graph feature fusion mechanism (GFFM) to inject part-of-speech and syntactic dependency features into BERT coding so that the coding contains more thorough and more prosperous semantic features. In addition, we apply a structured perception classifier (SPC) to extract triplets by making full use of the text’s semantic and structural information. We conduct numerous experiments on standard datasets. The results reveal that our proposed method has significant performance advantages and high practical value.},
  archive      = {J_KIS},
  author       = {Wen, Ya and Zong, Liansong and Tang, Mingwei and Tao, LinPing and Tang, Qi},
  doi          = {10.1007/s10115-024-02313-1},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3317-3342},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Perceptive dual-graph semantic integration network for aspect sentiment triplet extraction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable adversarial neural pairwise ranking for academic network embedding. <em>KIS</em>, <em>67</em>(4), 3293-3315. (<a href='https://doi.org/10.1007/s10115-024-02311-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian personalized ranking (BPR) has gained prominence as an effective method for pairwise learning, particularly in personalized tasks such as recommendation systems. However, recent developments in adversarial machine learning (AML) have raised concerns about the vulnerability of advanced BPR techniques. While adversarial training has proven effective in enhancing resilience and performance in the recommendation system (RS) domain, its application in network embedding remains underexplored. This study addresses the gap by introducing the adversarial neural Bayesian ranking-based academic network embedding (Adversarial Neural-Brane). The proposed approach integrates a neural network model with BPR to generate latent representations of vertices, incorporating adversarial training to enhance robustness. Moreover, we integrate interpretability into the training process. The notion behind interpretability modeling is that rather than moving in the worst-case direction, the noise is confined to additional examples from current embeddings. We perform experiments on four datasets that demonstrate the superiority of Adversarial Neural-Brane over state-of-the-art algorithms.},
  archive      = {J_KIS},
  author       = {Paul, Agyemang and Wu, Zhefu and Chen, Boyu and Luo, Kai and Fang, Luping},
  doi          = {10.1007/s10115-024-02311-3},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3293-3315},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Interpretable adversarial neural pairwise ranking for academic network embedding},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fake news detection: Comparative evaluation of BERT-like models and large language models with generative AI-annotated data. <em>KIS</em>, <em>67</em>(4), 3267-3292. (<a href='https://doi.org/10.1007/s10115-024-02321-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news poses a significant threat to public opinion and social stability in modern society. This study presents a comparative evaluation of BERT-like encoder-only models and autoregressive decoder-only large language models (LLMs) for fake news detection. We introduce a dataset of news articles labeled with GPT-4 assistance (an AI-labeling method) and verified by human experts to ensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned on this dataset. Additionally, we developed an instruction-tuned LLM approach with majority voting during inference for label generation. Our analysis reveals that BERT-like models generally outperform LLMs in classification tasks, while LLMs demonstrate superior robustness against text perturbations. Compared to weak labels (distant supervision) data, the results show that AI labels with human supervision achieve better classification results. This study highlights the effectiveness of combining AI-based annotation with human oversight and demonstrates the performance of different families of machine learning models for fake news detection.},
  archive      = {J_KIS},
  author       = {Raza, Shaina and Paulen-Patterson, Drai and Ding, Chen},
  doi          = {10.1007/s10115-024-02321-1},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3267-3292},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fake news detection: Comparative evaluation of BERT-like models and large language models with generative AI-annotated data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weight masking in image classification networks: Class-specific machine unlearning. <em>KIS</em>, <em>67</em>(4), 3245-3265. (<a href='https://doi.org/10.1007/s10115-024-02312-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine unlearning focuses on data forgetting, rendering the model unable to recognize previously identifiable data, which holds promise for ensuring data privacy and security. However, current methods are computationally intensive and can negatively impact classification performance on the retained data. In this paper, we propose a new unlearning method that can blur specific classes via weight masking, without retraining and additional computation. The weights, specific to each class by mapping the feature vectors of corresponding class samples, represent the statistics of these feature vectors and indicate the importance distribution of feature activation units for each class. This method has been validated on multiple datasets, showing a negligible ability to recognize forgotten data, with accuracy dropping to 0–0.32%, while increasing by 0.08–18.85% compared to the original networks on retained data.},
  archive      = {J_KIS},
  author       = {Wang, Jiali and Bie, Hongxia and Jing, Zhao and Zhi, Yichen and Fan, Yongkai},
  doi          = {10.1007/s10115-024-02312-2},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3245-3265},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Weight masking in image classification networks: Class-specific machine unlearning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vertical federated learning: A structured literature review. <em>KIS</em>, <em>67</em>(4), 3205-3243. (<a href='https://doi.org/10.1007/s10115-025-02356-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has emerged as a promising distributed learning paradigm with an added advantage of data privacy. With the growing interest in collaboration among data owners, FL has gained significant attention from organizations. The idea of FL is to enable collaborating participants train machine learning (ML) models on decentralized data without breaching privacy. In simpler words, federated learning is the approach of “bringing the model to the data, instead of bringing the data to the model”. Federated learning, when applied to data which is partitioned vertically across participants, is able to build a complete ML model by combining local models trained only using the data with distinct features at the local sites. This architecture of FL is referred to as vertical federated learning (VFL), which differs from the conventional FL on horizontally partitioned data. As VFL is different from conventional FL, it comes with its own issues and challenges. Motivated by the comparatively less explored side of FL, this paper provides a comprehensive overview of existing methods and developments in VFL, covering various aspects such as communication, learning, privacy, and applications. We conclude by identifying gaps in the current literature and proposing potential future directions for research in VFL.},
  archive      = {J_KIS},
  author       = {Khan, Afsana and ten Thij, Marijn and Wilbik, Anna},
  doi          = {10.1007/s10115-025-02356-y},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3205-3243},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Vertical federated learning: A structured literature review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online deep learning’s role in conquering the challenges of streaming data: A survey. <em>KIS</em>, <em>67</em>(4), 3159-3203. (<a href='https://doi.org/10.1007/s10115-025-02351-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era defined by the relentless influx of data from diverse sources, the ability to harness and extract valuable insights from streaming data has become paramount. The rapidly evolving realm of online learning techniques is tailored specifically for the unique challenges posed by streaming data. As the digital world continues to generate vast torrents of real-time data, understanding and effectively utilizing online learning approaches are pivotal for staying ahead in various domains. One of the primary goals of online learning is to continuously update the model with the most recent data trends while maintaining and improving the accuracy of previous trends. Based on the various types of feedback, online learning tasks can be divided into three categories: learning with full feedback, learning with limited feedback, and learning without feedback. This survey aims to identify and analyze the key challenges associated with online learning with full feedback, including concept drift, catastrophic forgetting, skewed learning, and network adaptation, while the other existing reviews mainly focus on a single challenge or two without considering other scenarios. This article also discusses the application and ethical implications of online learning. The results of this survey provide valuable insights for researchers and instructional designers seeking to create effective online learning experiences that incorporate full feedback while addressing the associated challenges. In the end, some conclusions, remarks, and future directions for the research community are provided based on the findings of this review.},
  archive      = {J_KIS},
  author       = {Sulaiman, Muhammad and Farmanbar, Mina and Kagami, Shingo and Belbachir, Ahmed Nabil and Rong, Chunming},
  doi          = {10.1007/s10115-025-02351-3},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3159-3203},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Online deep learning’s role in conquering the challenges of streaming data: A survey},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in the battle against disinformation and misinformation: A systematic review of challenges and approaches. <em>KIS</em>, <em>67</em>(4), 3139-3158. (<a href='https://doi.org/10.1007/s10115-024-02337-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving digital age, the proliferation of disinformation and misinformation poses significant challenges to societal trust and information integrity. Recognizing the urgency of addressing this issue, this systematic review endeavors to explore the role of artificial intelligence (AI) in combating the spread of false information. This study aims to provide a comprehensive analysis of how AI technologies have been utilized from 2014 to 2024 to detect, analyze, and mitigate the impact of misinformation across various platforms. This research utilized an exhaustive search across prominent databases such as ProQuest, IEEE Explore, Web of Science, and Scopus. Articles published within the specified timeframe were meticulously screened, resulting in the identification of 8103 studies. Through elimination of duplicates and screening based on title, abstract, and full-text review, we meticulously distilled this vast pool to 76 studies that met the study’s eligibility criteria. Key findings from the review emphasize the advancements and challenges in AI applications for combating misinformation. These findings highlight AI’s capacity to enhance information verification through sophisticated algorithms and natural language processing. They further emphasize the integration of human oversight and continual algorithm refinement emerges as pivotal in augmenting AI’s effectiveness in discerning and countering misinformation. By fostering collaboration across sectors and leveraging the insights gleaned from this study, researchers can propel the development of ethical and effective AI solutions.},
  archive      = {J_KIS},
  author       = {Saeidnia, Hamid Reza and Hosseini, Elaheh and Lund, Brady and Tehrani, Maral Alipour and Zaker, Sanaz and Molaei, Saba},
  doi          = {10.1007/s10115-024-02337-7},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3139-3158},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Artificial intelligence in the battle against disinformation and misinformation: A systematic review of challenges and approaches},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing the collective intelligence: A comprehensive review of federated learning security attacks and defensive strategies. <em>KIS</em>, <em>67</em>(4), 3099-3137. (<a href='https://doi.org/10.1007/s10115-025-02339-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning holds significant potential as a collaborative machine learning technique, allowing multiple entities to work together on a collective model without the need to exchange data. However, due to the distribution of data across multiple devices, federated learning becomes susceptible to a range of attacks. This paper provides an extensive examination of the different forms of attacks that can target federated learning systems. The attacks discussed include data poisoning attacks, model poisoning attacks, backdoor attacks, Byzantine attacks, membership inference attacks, model inversion attacks, etc. Each attack is examined in detail, with examples from the literature provided. Additionally, potential countermeasures to defend against these attacks are explored. The objective of this review is to provide an in-depth survey of the current landscape in federated learning attacks and corresponding defense mechanisms.},
  archive      = {J_KIS},
  author       = {Kaushal, Vishal and Sharma, Sangeeta},
  doi          = {10.1007/s10115-025-02339-z},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3099-3137},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Securing the collective intelligence: A comprehensive review of federated learning security attacks and defensive strategies},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An introduction to collaborative filtering through the lens of the netflix prize. <em>KIS</em>, <em>67</em>(4), 3049-3098. (<a href='https://doi.org/10.1007/s10115-024-02315-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey is intended to inform non-expert readers about the field of recommender systems, particularly collaborative filtering, through the lens of the impactful Netflix Prize competition. Readers will quickly be brought up to speed on pivotal recommender systems advances through the Netflix Prize, informing their prospective state-of-the-art research with meaningful historic artifacts. We begin with the pivotal FunkSVD approach early in the competition. We then discuss Probabilistic Matrix Factorization and the importance and extensibility of the model. We examine the strategies of the Netflix Prize winner, providing comparisons to the Probabilistic Matrix Factorization framework as well as commentary as to why one approach became extensively used in research while another did not. Collectively, these models help to understand the progression of collaborative filtering through the Netflix Prize era. In each topic, we include ample discussion of results and background information. Finally, we highlight major veins of research following the competition.},
  archive      = {J_KIS},
  author       = {Munson, Jacob and Cummins, Breschine and Zosso, Dominique},
  doi          = {10.1007/s10115-024-02315-z},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {3049-3098},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An introduction to collaborative filtering through the lens of the netflix prize},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: A new hybrid reasoning model based on rules, cases and processes: Application to care of individuals facing autism spectrum disorders. <em>KIS</em>, <em>67</em>(3), 3047-3048. (<a href='https://doi.org/10.1007/s10115-024-02278-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Kaoura, Georgia and Kovas, Konstantinos and Boutsinas, Basilis and Hatzilygeroudis, Ioannis},
  doi          = {10.1007/s10115-024-02278-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {3047-3048},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: A new hybrid reasoning model based on rules, cases and processes: Application to care of individuals facing autism spectrum disorders},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDMIX: An entropy-based dissimilarity measure to cluster mixed data comprising of numerical–nominal–ordinal attributes. <em>KIS</em>, <em>67</em>(3), 3023-3045. (<a href='https://doi.org/10.1007/s10115-024-02319-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing (dis)similarity measures for mixed data often ignore data distribution and ordering information along numerical and categorical features, respectively, during distance computation. Additionally, combining numerical and categorical (dis)similarities is often done naively, failing to judiciously assign relative weights. This work introduces a unified dissimilarity measure for mixed data (EDMIX) that addresses these challenges. It employs an entropy-based measure (SEND) for numerical attributes which utilizes intra-attribute inhomogeneity efficiently. A Boltzmann’s entropy-based measure (EDMD) is adopted for categorical features, which effectively handles both nominal and ordinal attributes. Decay of attribute weight is analyzed to determine the threshold weights for numerical and categorical parts. Number of attributes whose weights are above the threshold value in each part decides the mixing proportion of the respective parts. Notably, EDMIX requires no input parameters and is adaptable to diversified mixed data. Experimental results highlight its superiority in terms of cluster quality, accuracy, discrimination ability, and execution time across diverse mixed datasets for clustering applications.},
  archive      = {J_KIS},
  author       = {Kar, Amit Kumar and Mishra, Amaresh Chandra and Mohanty, Sraban Kumar},
  doi          = {10.1007/s10115-024-02319-9},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {3023-3045},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {EDMIX: An entropy-based dissimilarity measure to cluster mixed data comprising of numerical–nominal–ordinal attributes},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models: A survey of their development, capabilities, and applications. <em>KIS</em>, <em>67</em>(3), 2967-3022. (<a href='https://doi.org/10.1007/s10115-024-02310-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models can generate text, respond to queries, and translate between languages, as recent research demonstrates. As a new and essential part of computational linguistics, LLMs can understand complex speech patterns and respond appropriately and rationally in the given context. Research contributions have significantly increased as a result of LLMs growing popularity. Nevertheless, the rate of increase has been so quick that evaluating the cumulative impact of these developments is challenging. Keeping abreast with all the LLM research that has surfaced in such a short time and gaining a thorough understanding of the present state of the field’s study has become challenging. The scientific community would gain from a succinct but thorough LLM summary covering their history, architecture, applications, issues, influence, and resources. This paper covers many model types while reviewing LLMs’ fundamental principles and notions. It summarizes earlier research, the evolution of LLMs throughout time, transformer history, and the range of tools and training methods applied to these models. The study also looks at the many fields in which LLMs are used, including business, social work, education, healthcare, and agriculture. Furthermore, it illustrates how LLMs impact society, mold AI’s future, and find valuable uses in problem-solving. This review article aims to provide a comprehensive overview of the history of LLMs, pre-trained architectures, applications, problems, and future directions for practitioners, researchers, and experts.},
  archive      = {J_KIS},
  author       = {Annepaka, Yadagiri and Pakray, Partha},
  doi          = {10.1007/s10115-024-02310-4},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2967-3022},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Large language models: A survey of their development, capabilities, and applications},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated cross-domain recommendation system based on bias eliminator and personalized extractor. <em>KIS</em>, <em>67</em>(3), 2935-2965. (<a href='https://doi.org/10.1007/s10115-024-02316-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of big data is driven by advancements in Internet of Things technology. The cross-domain recommendation system is a highly successful approach for obtaining user-interested items from massive data. However, implementing a cross-domain recommendation system on distributed IoT devices faces three challenges. On the one hand, item embeddings used in existing cross-domain recommendation models often lead to an unavoidable popularity bias. On the other hand, to convey user preference information, most existing methods utilize common interest channels. However, it stands to reason that the interest channels for different users should be distinct, as each person has their own unique preferences. Furthermore, collecting raw data from distributed IoT devices may lead to user privacy concerns. Given these challenges, we propose a federated cross-domain recommendation system based on bias eliminator and personalized extractor (FedBP) in this paper to achieve precise recommendations in cold-start scenarios. Firstly, we employ a bias eliminator to unfold all embedding directions during training, ensuring that each direction captures only specific features while maintaining neutrality in popularity. Secondly, personalized extractor is utilized to individualize the distinct preference information of each user from the source domain to the target domain. Then, we utilize a federated framework to collaboratively train the cross-domain recommendation system model, where local differential privacy is employed to ensure data privacy. Experimental results on public benchmarks show that FedBP consistently outperforms baseline models across three cold-start cross-domain recommendation scenarios, with improvements of at least 3.02%, 5.08%, and 3.08%.},
  archive      = {J_KIS},
  author       = {Di, Yicheng and Shi, Hongjian and Wang, Qi and Jia, Shunyuan and Bao, Jiayu and Liu, Yuan},
  doi          = {10.1007/s10115-024-02316-y},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2935-2965},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Federated cross-domain recommendation system based on bias eliminator and personalized extractor},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-aware user multi-interest modeling method for news recommendation. <em>KIS</em>, <em>67</em>(3), 2911-2933. (<a href='https://doi.org/10.1007/s10115-024-02290-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {News recommendation can filter massive news and provide personalized information services, and the key is to better portray news and users. Current news recommendation methods try to integrate the knowledge graph to introduce background knowledge for news features modeling and have achieved good performance. However, when modeling users’ interest features, these methods only consider the personalized information of the interacted news and ignore the fact that some typical entities can help depict multiple topics that users are interested in. In this paper, we propose a method called KMRec (knowledge-aware user multi-interest modeling method for news recommendation) to realize the fine-grained portrayal of users’ multi-interest based on knowledge graph. Specifically, we represent user multi-interest features through four modules: historical text encoder, individual entity encoder, common entity encoder and user encoder. The historical text encoder is for the text modeling of each news and information aggregation. The individual entity encoder achieves the entity feature modeling of each news and information aggregation based on knowledge graph. The common entity encoder realizes the common entity information extraction of users’ historical interacted news based on knowledge graph. The user encoder integrates the text feature, individual entity feature and common entity feature to comprehensively depict the multi-interest of each user. Offline experimental results on both MIND-small and MIND-large datasets show that leveraging KMRec for users’ interest modeling can effectively improve the performance of news recommendation. The idea of introducing the user multi-interest feature is also verified to be effective by comparative experiments with existing news feature representations.},
  archive      = {J_KIS},
  author       = {Zuo, Zong and Lu, Jicang and Tan, Lei and Gong, Daofu and Chen, Jing and Liu, Fenlin},
  doi          = {10.1007/s10115-024-02290-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2911-2933},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Knowledge-aware user multi-interest modeling method for news recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential uncertainty quantification with contextual tensors for social targeting. <em>KIS</em>, <em>67</em>(3), 2881-2910. (<a href='https://doi.org/10.1007/s10115-024-02304-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common question that an online marketer asks is, given a social network, how one decides which users to target with their limited advertisement resources (such as coupon offers). The key technical task toward answering this question is the estimation of the user–user activation probability that quantifies the influence one user may have on another toward their purchasing decisions. In this paper, we propose a novel targeting strategy with sequential uncertainty quantification via probabilistic tensor regression. The proposed framework is designed to capture the heterogeneity in user preferences, product types, campaign strategies, etc. in the form of contextual tensor. For uncertainty quantification, we derive a closed-form online predictive distribution for the user response score, which is used in a bandit-style sequential decision-making on who to receive marketing offers. We empirically confirm that the proposed algorithm achieves significant improvement in influence maximization tasks over benchmarks, which is attributable to its capability of capturing the user–product heterogeneity.},
  archive      = {J_KIS},
  author       = {Idé, Tsuyoshi and Murugesan, Keerthiram and Bouneffouf, Djallel and Abe, Naoki},
  doi          = {10.1007/s10115-024-02304-2},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2881-2910},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sequential uncertainty quantification with contextual tensors for social targeting},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient parallel algorithm for finding strongly connected components based on granulation strategy. <em>KIS</em>, <em>67</em>(3), 2855-2879. (<a href='https://doi.org/10.1007/s10115-024-02299-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strongly connected components (SCCs) are a significant subgraph structure in digraphs. In the previous work, an algorithm based on rough set theory (RST) called KGRSCC was proposed, which can compute SCCs with high efficiency. Notably, KGRSCC utilized a granulation strategy, which was designed based on SCC correlations between vertices. These SCC correlations are confined to the situations that R-related set or upper approximation set only contains one vertex. However, the situations of ’only one’ cannot fully deduce SCCs correlations, which may limit the computation efficiency of SCCs. In this paper, firstly, the graph concept of SCCs is further analyzed in the framework of RST, and then, four ’not only one’ SCC correlations between vertices can be concluded. Secondly, the four SCC correlations can be divided two classes: trivial and nontrivial. Then, two new granulation strategies are proposed based on the two classes of SCC correlations. They can granulate the vertex set to construct two types of vertex granules. Thirdly, with combination of two types of vertex granules, a parallel algorithm named P@KGS is proposed based on KGRSCC. Finally, experimental results show that the P@KGS algorithm performs higher computational efficiency than compared algorithms.},
  archive      = {J_KIS},
  author       = {Xu, Taihua and He, Huixing and Yang, Xibei and Yang, Jie and Song, Jingjing and Cui, Yun},
  doi          = {10.1007/s10115-024-02299-w},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2855-2879},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient parallel algorithm for finding strongly connected components based on granulation strategy},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Factors influencing open science participation through research data sharing and reuse among researchers: A systematic literature review. <em>KIS</em>, <em>67</em>(3), 2801-2853. (<a href='https://doi.org/10.1007/s10115-024-02284-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This systematic literature review investigates the influential factors guiding researchers’ active engagement in open science through research data sharing and subsequent reuse, spanning various scientific disciplines. The review addresses key objectives and questions, including identifying distinct sample types, data collection methods, critical factors, and existing gaps within the body of literature concerning data sharing and reuse in open science. The methodology employed in the review was detailed, outlining a series of systematic steps. These steps encompass the systematic search and selection of relevant studies, rigorous data extraction and analysis, comprehensive evaluation of selected studies, and transparent reporting of the resulting findings. The review’s evaluation process was governed by well-defined inclusion and exclusion criteria, encompassing publication dates, language, study design, and research outcomes. Furthermore, it adheres to the PRISMA 2020 flow diagram, effectively illustrating the progression of records through the review stages, highlighting the number of records identified, screened, included, and excluded. The findings include a concise tabular representation summarizing data extracted from the 51 carefully selected studies incorporated within the review. The table provides essential details, including study citations, sample sizes, data collection methodologies, and key factors influencing open science data sharing and reuse. Additionally, common themes and categories among these influential factors are identified, shedding light on overarching trends in the field. In conclusion, this systematic literature review offers valuable insights into the multifaceted landscape of open science participation, emphasizing the critical role of research data sharing and reuse. It is a comprehensive resource for researchers and practitioners interested in further understanding the dynamics and factors shaping the open science ecosystem.},
  archive      = {J_KIS},
  author       = {Ahmed, Mahfooz and Othman, Roslina and Noordin, Mohamad Fauzan and Ibrahim, Adamu Abubakar and Al-Hussaini, Abulfathi Ibrahim Saleh},
  doi          = {10.1007/s10115-024-02284-3},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2801-2853},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Factors influencing open science participation through research data sharing and reuse among researchers: A systematic literature review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sarcasm detection using optimized bi-directional long short-term memory. <em>KIS</em>, <em>67</em>(3), 2771-2799. (<a href='https://doi.org/10.1007/s10115-024-02210-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current era, the number of social network users continues to increase day by day due to the vast usage of interactive social networking sites like Twitter, Facebook, Instagram, etc. On these sites, users generate posts, whereas the attitude of followers towards factor utilization like situation, sound, feeling, and so on can be analysed. But most people feel difficult to analyse feelings accurately, which is one of the most difficult problems in natural language processing. Some people expose their opinions with different sole meanings, and this sophisticated form of expressing sentiments through irony or mockery is termed sarcasm. The sarcastic comments, tweets or feedback can mislead data mining activities and may result in inaccurate predictions. Several existing models are used for sarcasm detection, but they have resulted in inaccuracy issues, huge time consumption, less training ability, high overfitting issues, etc. To overcome these limitations, an effective model is introduced in this research to detect sarcasm. Initially, the data are collected from publicly available sarcasmania and Generic sarcasm-Not sarcasm (Gen-Sarc-Notsarc) datasets. The collected data are pre-processed using stemming and stop word removal procedures. The features are extracted using the inverse filtering (IF) model through hash index creation, keyword matching and ranking. The optimal features are selected using adaptive search and rescue (ASAR) optimization algorithm. To enhance the accuracy of sarcasm detection, an optimized Bi-LSTM-based deep learning model is proposed by integrating Bi-directional long short-term memory (Bi-LSTM) with group teaching optimization (GTO). Also, the LSTM + GTO model is proposed to compare its performance with the Bi-LSTM + GTO model. The proposed models are compared with existing classifier approaches to prove the model’s superiority using PYTHON. The accuracy of 98.24% and 98.36% are attained for sarcasmania and Gen-Sarc-Notsarc datasets.},
  archive      = {J_KIS},
  author       = {Sukhavasi, Vidyullatha and Sistla, Venkatrama Phani kumar and Dondeti, Venkatesulu},
  doi          = {10.1007/s10115-024-02210-7},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2771-2799},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sarcasm detection using optimized bi-directional long short-term memory},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new way of search query like knowledge graph and its interpretability. <em>KIS</em>, <em>67</em>(3), 2745-2770. (<a href='https://doi.org/10.1007/s10115-024-02242-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online search, it has always been a challenge to provide a way of search query that can help users accurately express search intentions and encourage them to type easily. This paper studies a knowledge graph-like search query and its interpretability. Inspired by the excellent expressive capabilities of the knowledge graph in knowledge reasoning and explainable artificial intelligence, we propose a new way of search query named search query knowledge graph, which can help users express their search requests using concepts such as entities, relationships, and facts. Therefore, people can draw a graph simply and interactively as a search query in a drag-and-drop manner. Compared with traditional search query keyword, the search query knowledge graph is not only fully compatible with it but also enriches and expands it. The search query keyword is a special case of the search query knowledge graph. We take the search query knowledge graph as the search input, obtain the corresponding search results in an online search, and then generate another knowledge graph named search result knowledge graph. According to the structural mapping relationship and the semantic matching relationship between the above two knowledge graphs, we innovatively discuss the structural interpretability and semantic interpretability for the search query knowledge graph. Some judgments or criteria about structural interpretability and semantic interpretability are given, and the sufficiency and necessity of the two interpretabilities are addressed. Extensive experimental results show that the search query knowledge graph proposed in this paper is a more effective and accurate way to express users’ search intentions and can provide help for online search.},
  archive      = {J_KIS},
  author       = {Xie, Ying-jie and Zeng, Guo-sun},
  doi          = {10.1007/s10115-024-02242-z},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2745-2770},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new way of search query like knowledge graph and its interpretability},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-graph-driven environmental monitoring with cross-regions knowledge transfer. <em>KIS</em>, <em>67</em>(3), 2721-2744. (<a href='https://doi.org/10.1007/s10115-024-02294-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is a critical task in intelligent transportation systems. However, cross-city data-driven prediction models encounter numerous challenges. A principal issue is data scarcity in underdeveloped cities, resulting from inadequate data collection mechanisms. Additionally, these models frequently rely exclusively on direct spatio-temporal traffic data, often deficient in thorough extraction and lateral exploration of external information from source cities, thereby introducing risks of negative transfer. This paper presents KGD-Transfer, a knowledge graph-driven cross-city traffic flow prediction framework that utilizes the extensive semantic information embedded within knowledge graphs. To address the challenge of data scarcity, we advocate for the construction of Fused Meta-Knowledge ( $${{\textbf {FM}}_k}$$ ) from multiple source cities to facilitate precise traffic prediction. Our knowledge fusion network (Ka-net) deeply integrates heterogeneous knowledge graphs with traffic data. Following this, we apply Neural Controlled Differential Equations (NCDEs) to extract intricate spatio-temporal features from the integrated knowledge. Furthermore, the Model-Agnostic Meta-Learning (MAML) framework is utilized to efficiently minimize the risk of negative transfer in cross-city learning. This approach enables the transfer of fused source knowledge by employing non-shared parameters to perform deep feature matching across cities, capitalizing on the spatio-temporal commonalities within the $${{\textbf {FM}}_k}$$ . Experimental results from four real datasets illustrate that the inclusion of contextual information from knowledge graphs markedly enhances the prediction model’s understanding and reasoning capabilities. KGD-Transfer outperforms advanced baseline methods in both short-term (10 min) and long-term (60 min) predictions, demonstrating superior accuracy and generalizability.},
  archive      = {J_KIS},
  author       = {Liu, Xiuwen and Xiao, Yang and Zhou, Shaoheng},
  doi          = {10.1007/s10115-024-02294-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2721-2744},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Knowledge-graph-driven environmental monitoring with cross-regions knowledge transfer},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient rumor detection model based on deep learning and flower pollination algorithm. <em>KIS</em>, <em>67</em>(3), 2691-2719. (<a href='https://doi.org/10.1007/s10115-024-02305-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spite of the growing popularity of social media (Twitter, Facebook, etc.) as a source of news and data, its unfiltered nature often facilitates the spread of rumors or pieces of information that cannot be validated at the time they are shared. It is possible for false or unconfirmed information to spread like wildfire on the internet, influencing public opinion and policy in the same way that reliable news would. Some of the most pervasive examples of incorrect and dubious information are fake news and rumors, both need to be identified as soon as possible to prevent potentially destabilizing outcomes such as loss of life, reputation, or financial loss. This paper presents a pioneering study that integrates the flower pollination algorithm (FPA) with convolutional neural networks (CNNs) for enhanced rumor detection on social media platforms. We develop and test a model that leverages FPA to optimize the architecture and hyperparameters of CNNs, which significantly improves the accuracy and efficiency of detecting rumors. Using data from Twitter, the proposed model achieves a benchmark accuracy of 91.24%, outperforming existing state-of-the-art approaches. The novelty of this research lies in the application of a nature-inspired optimization algorithm to automate the fine-tuning of deep learning models, addressing the challenges of manual parameter selection and model scalability in dynamic information environments. This study contributes to the fields of misinformation detection and machine learning by providing a robust framework for real-time, adaptable rumor analysis.},
  archive      = {J_KIS},
  author       = {Ahsan, Mohammad and Sinha, Bam Bahadur},
  doi          = {10.1007/s10115-024-02305-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2691-2719},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An efficient rumor detection model based on deep learning and flower pollination algorithm},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A clustering algorithm for detecting differential deviations in the multivariate time-series IoT data based on sensor relationship. <em>KIS</em>, <em>67</em>(3), 2641-2690. (<a href='https://doi.org/10.1007/s10115-024-02303-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-of-things (IoT) applications involve a large number of sensors reporting data as a set of time series. Often these data are related to each other based on the relationship of the sensors in the actual application. Any small deviations could indicate a change in the operation of the IoT system and potential problems with the IoT application’s goals. It is often difficult to detect such deviations with respect to the relationship between the sensors. This paper presents the clustering algorithm that can efficiently detect all the deviations small or large in the complex and evolving IoT data streams with the help of sensor relationships. We have demonstrated with the help of experiments that our algorithm can efficiently handle high-dimensional data and accurately detect all the deviations. In this paper, we have presented two more algorithms, anomaly detection and outlier detection, that can efficiently categorize the deviations detected by our proposed clustering algorithm into anomalous or normal deviations. We have evaluated the performance and accuracy of our proposed algorithms on synthetic and real-world datasets. Furthermore, to check the effectiveness of our algorithms in terms of efficiency, we have prepared synthetic datasets in which we have increased the complexity of the deviations to show that our algorithm can handle complex IoT data streams efficiently.},
  archive      = {J_KIS},
  author       = {Idrees, Rabbia and Maiti, Ananda and Garg, Saurabh},
  doi          = {10.1007/s10115-024-02303-3},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2641-2690},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A clustering algorithm for detecting differential deviations in the multivariate time-series IoT data based on sensor relationship},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusing graph structural information with pre-trained generative model for knowledge graph-to-text generation. <em>KIS</em>, <em>67</em>(3), 2619-2640. (<a href='https://doi.org/10.1007/s10115-024-02235-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph-to-text generation (KG-to-Text) is a task that involves generating accurate textual descriptions based on a given knowledge graph. Previous efforts have often enhanced pre-trained generative models by incorporating additional auxiliary pre-training tasks to supplement the missing graph structural information. However, such tasks not only require substantial computational resources, but also yield limited improvements. To address this issue, we propose a novel approach that effectively combines the graph structural information from knowledge graphs with pre-trained generative models without altering their fundamental architecture. Our approach involves inputting the original knowledge graph data into a graph convolutional network to acquire graph feature representations enriched with node characteristics. Additionally, the linearized sequence of the knowledge graph is inputted into the pre-trained generative model to exploit its inherent semantic richness. After computing multi-head attention mechanisms, we fuse the acquired graph feature representations into the pre-trained generative model to supplement the missing graph structural information. Experimental results on the WebNLG and EventNarrative datasets show that our approach achieves improved performance while reducing computational overhead.},
  archive      = {J_KIS},
  author       = {Shi, Xiayang and Xia, Zhenlin and Li, Yinlin and Wang, Xuhui and Niu, Yufeng},
  doi          = {10.1007/s10115-024-02235-y},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2619-2640},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fusing graph structural information with pre-trained generative model for knowledge graph-to-text generation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stream mining with integrity constraint learning for event extraction in evolving data streams. <em>KIS</em>, <em>67</em>(3), 2595-2618. (<a href='https://doi.org/10.1007/s10115-024-02300-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An event is a structured interaction of objects, generally involving an agent (actor) who acts on target entities, possibly with associated instruments. The problem of event extraction is to identify events of interest from a variety of information sources to be stored in a knowledge base for subsequent retrieval and analysis. Two major problems in event extraction are (1) adapting to concept drift and (2) maintaining the consistency of the event knowledge base. In this paper, we address these problems in a stream mining framework, making use of an ontology for events that represents background knowledge and integrity constraints derived empirically from an evolving data stream. We introduce a first-order language FLORE (Formal Language for Ontologies and Representing Events), which is used to encode an ontology of event types and objects, and to represent individual events categorized using concepts from the ontology. We present a multi-layered stream mining method for event extraction, where the first layer consists of a pool of simple learners, and the second layer learns an evolving set of integrity constraints to ensure the ongoing consistency of the extracted events. One intended application of this approach is conflict monitoring, and the domain of the Afghanistan conflict is used to illustrate the approach. Experimental results confirm that our multi-layered approach achieves higher recall and F1 than event extraction baselines on the event extraction task and on the subtasks of event detection and argument detection and classification.},
  archive      = {J_KIS},
  author       = {Calvo Martinez, John and Wobcke, Wayne},
  doi          = {10.1007/s10115-024-02300-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2595-2618},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Stream mining with integrity constraint learning for event extraction in evolving data streams},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer topic interaction propagation model and simulation considering opinion interaction. <em>KIS</em>, <em>67</em>(3), 2571-2594. (<a href='https://doi.org/10.1007/s10115-024-02295-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing research on the dissemination and evolution of online public opinion focuses more on a single original or derivative topic or on treating the evolution process of user opinions and information dissemination as independent research objects. However, the evolution of opinions also affects the process of public opinion dissemination. When multiple derivative topics exist, the dissemination of public opinion will have new dynamic evolution characteristics. Thus, we propose a multi-layer topic interaction propagation model (OI-SEIRI model) considering opinion interaction. When multiple derivative topics appear, we divide the public opinion propagation of multiple topics into multiple independent propagation topic layers and construct opinion interaction rules and topic information transmission mechanisms within and between layers. By dynamically setting the probability of state transition and solving the propagation equilibrium point and propagation threshold, we clarified the impact of different factors on the trend of public opinion dissemination. Finally, the experimental results show that the OI-SEIRI model can effectively and more accurately describe the interactive information dissemination of multiple topics. The number of derivative topics, topic correlation relationships, opinion interaction behavior, and social roles can all impact the dissemination process.},
  archive      = {J_KIS},
  author       = {Yao, Cuiyou and Yu, Lin and Wang, Dong and Fu, Dongpu},
  doi          = {10.1007/s10115-024-02295-0},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2571-2594},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-layer topic interaction propagation model and simulation considering opinion interaction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time topic-based sentiment analysis for movie tweets using hybrid approach. <em>KIS</em>, <em>67</em>(3), 2543-2569. (<a href='https://doi.org/10.1007/s10115-024-02298-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to perform sentiment analysis (SA), which deals with dissecting and then extracting the hidden insights from the sentences said or written by a person. The proposed methodology for SA uses a combined and novel framework involving a Lexicon-based approach (LBA) and a deep learning (DL) technique to predict the overall sentiment of the text. Firstly, the LBA segregates the neutral tweets from the polar tweets. Later, a variant of CNN, namely a BERT-based bidirectional long short-term memory-temporal convolutional network (BiLSTM-TCN) grounded by the residual module and the dilated convolutions, is used to identify the type of polarity of the text. The paper also investigates various other BERT-based models like CNN, LSTM, and BiLSTM on the IMDB Movie Review dataset containing 50k movie reviews to show that the suggested model achieves a mean validation accuracy of 0.932 and a mean validation loss of 0.238 for the last three epochs for the polar reviews. After that, the trained model is used to forecast the sentiment of the live-streaming tweets about a particular movie of interest. The Twitter API, Tweepy, fetches tweets crawling over Twitter. The study obtains test accuracy, F1, and ROC–AUC scores of 92.94%, 0.929, and 0.98, respectively, in the least number of epochs, which is better than other models mentioned above, running in the same environment.},
  archive      = {J_KIS},
  author       = {Madan, Anjum and Kumar, Devender},
  doi          = {10.1007/s10115-024-02298-x},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2543-2569},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Real-time topic-based sentiment analysis for movie tweets using hybrid approach},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel ensemble label propagation with hierarchical weighting for semi-supervised learning. <em>KIS</em>, <em>67</em>(3), 2521-2542. (<a href='https://doi.org/10.1007/s10115-024-02245-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, semi-supervised learning is one of the research sports to solve the problem of labeled data. Label propagation (LP) is a popular method to classify graph data by utilizing the characterization of data nodes. However, LP has randomness and cannot effectively employ node attributes. In this paper, to solve the above problem, we propose a novel LP method based on hierarchical weighting (HWLP). Firstly, attribute aggregation and attribute update are executed for each node. Secondly, in the process of LP, for each unlabeled node, the label owned by its neighbors with the highest similarity is selected to avoid arbitrary LP. Finally, maximum voting is adopted to enhance the stability of the results. Experimental results show that the proposed method has better performance and stability than others.},
  archive      = {J_KIS},
  author       = {Zheng, Yifeng and Liu, Yafen and Qing, Depeng and Zhang, Wenjie and Pan, Xueling and Li, Guohe},
  doi          = {10.1007/s10115-024-02245-w},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2521-2542},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel ensemble label propagation with hierarchical weighting for semi-supervised learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E-learning app selection multi-criteria group decision-making problem using einstein operator in linguistic trapezoidal neutrosophic environment. <em>KIS</em>, <em>67</em>(3), 2481-2519. (<a href='https://doi.org/10.1007/s10115-024-02265-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we have defined linguistic trapezoidal neutrosophic numbers (LTNNs) which are a combination of trapezoidal neutrosophic numbers and linguistic variable numbers. Then, we introduced the Einstein sum, product and exponentiation operations for LTNNs and discussed the relationship among these operations. Moreover, two new kinds of aggregation operators, namely Einstein weighted-average operator and Einstein weighted-geometric operator for LTNNs, have been constructed. Based on the above aggregation operators, a multi-criteria group decision-making technique has been designed, which is demonstrated through numerical examples. Finally, sensitivity analysis and comparative study have been accomplished to exhibit the effectiveness and feasibility of the newly developed technique in the linguistic environment.},
  archive      = {J_KIS},
  author       = {Haque, Tipu Sultan and Chakraborty, Avishek and Alam, Shariful},
  doi          = {10.1007/s10115-024-02265-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2481-2519},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {E-learning app selection multi-criteria group decision-making problem using einstein operator in linguistic trapezoidal neutrosophic environment},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for community architectural layout generation. <em>KIS</em>, <em>67</em>(3), 2453-2480. (<a href='https://doi.org/10.1007/s10115-024-02291-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Architectural layout design is of considerable importance in urban design, yet it can be a time-consuming and labor-intensive task for architects. Thus, a highly automated approach would benefit the profession. Different from recently studied methods for indoor floorplan generation, community architectural layout generation faces more challenges due to constraints such as building interval and volume rate while also considering architectural aesthetics. Existing algorithms that rely on predefined rules or heuristic search have difficulties balancing these factors. In addition, a dataset for evaluating the generated plans is lacking. In this paper, we formulate the community architectural layout task as a Markov game by defining the state, action space and reward function. We then propose a multi-agent reinforcement learning method with curriculum learning to generate layouts for the formulation. Besides, we propose a set of metrics for the evaluation of architectural layouts especially for residential area planning. We conduct our experiments on the real-world scene. The results have demonstrated our approach’s superiority in comparison to baseline methods.},
  archive      = {J_KIS},
  author       = {Sheng, Tao and Xiong, Yun and Wang, Haofen and Zhang, Yao and Wang, Siqi and Zhang, Weinan},
  doi          = {10.1007/s10115-024-02291-4},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2453-2480},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep reinforcement learning for community architectural layout generation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of corn futures prices with decomposition and hybrid deep learning models. <em>KIS</em>, <em>67</em>(3), 2427-2452. (<a href='https://doi.org/10.1007/s10115-024-02301-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting corn futures prices is crucial for market participants, policymakers, and agricultural enterprises to manage risks and make informed decisions. Therefore, it is necessary to improve the accuracy and stability of corn futures price predictions. This study proposes a deep mixed model that integrates variational mode decomposition (VMD), particle swarm optimization (PSO), convolutional neural networks (CNNs), long short-term memory (LSTM) networks, and an attention mechanism. First, VMD is used to decompose the corn futures closing price series, thereby reducing data complexity. Then, principal component analysis (PCA) is performed on 21 influencing factors to extract key components affecting price fluctuations. The proposed model combines CNN for local feature extraction, LSTM for time series processing, and an attention mechanism to focus on critical information. In addition, the PSO algorithm optimizes the LSTM hyperparameters, further enhancing the model’s performance. To verify the model’s effectiveness, comprehensive experiments were conducted using historical data on corn futures. The results of four comparative experiments show that, based on multiple metrics such as RMSE, MAE, and $$\text {R}^2$$ , the proposed model significantly outperforms 14 existing models in terms of prediction accuracy and stability. These findings indicate that the model has practical application potential in financial forecasting and decision-making.},
  archive      = {J_KIS},
  author       = {Li, Feng and Tang, Menghe},
  doi          = {10.1007/s10115-024-02301-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2427-2452},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Prediction of corn futures prices with decomposition and hybrid deep learning models},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin: Securing IoT networks using integrated ECC with blockchain for healthcare ecosystem. <em>KIS</em>, <em>67</em>(3), 2395-2426. (<a href='https://doi.org/10.1007/s10115-024-02273-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twin (DT) innovation is becoming increasingly prevalent in numerous areas. This is often particularly genuine in healthcare, which depends increasingly on Internet of Things (IoT) systems. But this combination brings up enormous issues with safety, security, and being able to develop. Since information is private, it is exceptionally critical to keep it secure from individuals who shouldn't have gotten to it and from being stolen. This consideration proposes a modern framework that combines elliptic curve cryptography (ECC) with blockchain innovation to bargain with these issues. The system is implied to ensure IoT systems by making demonstrative apparatuses in healthcare more secure. A genetic algorithm-based random forest (GAO-RF) demonstration is additionally utilized to form include choice work way better, which makes it beyond any doubt that information taking care of and analysis go easily. The GAO-RF show includes the body of unused thoughts by progressing the method of choosing highlights, which is exceptionally critical for proficiently overseeing colossal sums of private information. The show that was put into activity works exceptionally well, as appeared by its execution measurements as F1-score of 97.3%, an exactness of 98.4%, an exactness of 97.3%, a review of 97.4%, an MCC of 97.69%, and a kappa measurement (KS) of 97.31%. These results show an exceptionally strong framework that can handle and protect private health data. The safety and security of understanding information in IoT systems have enormously progressed by including ECC and blockchain in the DT system. A genetic algorithm has been shown to work well in the random forest model for feature selection, which has led to better security methods. This study has big effects on the healthcare field because it gives us a strong way to keep patient information safe. This method creates trust in the healthcare system by making sure that private data are handled in an honest and safe way. It could change how patient data are protected in this digital age.},
  archive      = {J_KIS},
  author       = {Sharma, Vikas and Kumar, Akshi and Sharma, Kapil},
  doi          = {10.1007/s10115-024-02273-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2395-2426},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Digital twin: Securing IoT networks using integrated ECC with blockchain for healthcare ecosystem},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High utility itemset mining in data stream using elephant herding optimization. <em>KIS</em>, <em>67</em>(3), 2357-2394. (<a href='https://doi.org/10.1007/s10115-024-02288-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining high utility itemsets from data stream within limited time and space is a challenging task. Traditional algorithms typically require multiple scans and complex data structures for data connection, storage and update. Moreover, the evaluation of duplicate itemsets generated by overlapping batches leads to low efficiency of the algorithm in terms of time and space. To address these issues, this paper proposes a heuristic-based data stream high utility itemset mining algorithm, termed SHUIM-EHO, designed to effectively solve limited storage space. The SHUIM-EHO algorithm designs a new clan updating strategy, which effectively enhances the convergence speed and reduces itemset loss. Additionally, a hash storage strategy is proposed to avoid the evaluation of duplicate itemsets, thereby further improving the execution efficiency of the algorithm. Experiments on real and synthetic datasets demonstrate the effectiveness of the algorithm, significantly reducing memory consumption and maintaining strong scalability.},
  archive      = {J_KIS},
  author       = {Han, Meng and He, Feifei and Zhang, Ruihua and Li, Chunpeng and Meng, Fanxing},
  doi          = {10.1007/s10115-024-02288-z},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2357-2394},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {High utility itemset mining in data stream using elephant herding optimization},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trustworthy federated learning: Privacy, security, and beyond. <em>KIS</em>, <em>67</em>(3), 2321-2356. (<a href='https://doi.org/10.1007/s10115-024-02285-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While recent years have witnessed the advancement in big data and artificial intelligence, it is of much importance to safeguard data privacy and security. As an innovative approach, federated learning (FL) addresses these concerns by facilitating collaborative model training across distributed data sources without transferring raw data. However, the challenges of robust security and privacy across decentralized networks catch significant attention in dealing with the distributed data in FL. In this paper, we conduct an extensive survey of the security and privacy issues prevalent in FL, underscoring the vulnerability of communication links and the potential for cyber threats. We delve into various defensive strategies to mitigate these risks, explore the applications of FL across different sectors, and propose research directions. We identify the intricate security challenges that arise within the FL frameworks, aiming to contribute to the development of secure and efficient FL systems.},
  archive      = {J_KIS},
  author       = {Chen, Chunlu and Liu, Ji and Tan, Haowen and Li, Xingjian and Wang, Kevin I-Kai and Li, Peng and Sakurai, Kouichi and Dou, Dejing},
  doi          = {10.1007/s10115-024-02285-2},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2321-2356},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Trustworthy federated learning: Privacy, security, and beyond},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially ordered stochastic conformance checking. <em>KIS</em>, <em>67</em>(3), 2291-2319. (<a href='https://doi.org/10.1007/s10115-024-02280-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining aids organisations in improving their operational processes by providing visualisations and algorithms that turn event data into insights. How often behaviour occurs in a process—the stochastic perspective—is important for simulation, recommendation, enhancement and other types of analysis. Although the stochastic perspective is important, the focus is often on control flow. Stochastic conformance checking techniques assess the quality of stochastic process models and/or event logs with one another. In this paper, we address three limitations of existing stochastic conformance checking techniques: inability to handle uncertain event data (e.g. events having only a date), exponential blow-up in computation time due to the analysis of all interleavings of concurrent behaviour and the problem that loops that can be unfolded infinitely often. To address these challenges, we provide bounds for conformance measures and use partial orders to encode behaviour. An open-source implementation is provided, which we use to illustrate and evaluate the practical feasibility of the approach.},
  archive      = {J_KIS},
  author       = {Leemans, Sander J. J. and Brockhoff, Tobias and van der Aalst, Wil M. P. and Polyvyanyy, Artem},
  doi          = {10.1007/s10115-024-02280-7},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2291-2319},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Partially ordered stochastic conformance checking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid sampling algorithm for imbalanced and class-overlap data based on natural neighbors and density estimation. <em>KIS</em>, <em>67</em>(3), 2259-2290. (<a href='https://doi.org/10.1007/s10115-024-02281-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data classification poses a significant challenge in machine learning and data mining, exacerbated by class overlap which adversely affects model performance. Resampling is widely used to tackle imbalanced data. However, most resampling algorithms overlook the complexity of overlapping region samples, resulting in excessive removal of majority class samples and insufficient representation of minority class information. To tackle these issues, this paper proposes a novel approach, namely natural neighbors and density estimation-based hybrid sampling algorithm (NaNDS). NaNDS fully considers the varying impacts of overlapping samples with different characteristics on subsequent classification tasks. First, the density of minority class samples is estimated using a Gaussian kernel function, and high-density minority class samples are selected to construct a set of hyper-spherical structures. These structures facilitate the geometric identification and removal of overlapping majority class samples that negatively impact classification. Then, oversampling weights for minority samples are determined by combining density and information entropy estimates derived from natural neighbors. Afterward, an adaptive oversampling strategy is developed, using differentiation-based generation guided by natural neighbor information. This process appropriately corrects decision boundary by enhancing information in both overlapping and minority class-dominated regions. Experimental validation on the KEEL dataset demonstrates NaNDS outperforming eight state-of-the-art algorithms, highlighting its superior competitiveness and robustness in handling class-overlap imbalanced datasets.},
  archive      = {J_KIS},
  author       = {Li, Xinqi and Liu, Qicheng},
  doi          = {10.1007/s10115-024-02281-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2259-2290},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hybrid sampling algorithm for imbalanced and class-overlap data based on natural neighbors and density estimation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User-based clustering deep model for the sequential point-of-interest recommendation. <em>KIS</em>, <em>67</em>(3), 2233-2258. (<a href='https://doi.org/10.1007/s10115-024-02277-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The point-of-interest (POI) recommendation is a key function of location-based social networks that can help users exploit unfamiliar areas. Due to the massive check-in records accumulated in these location-based applications, the sequential POI recommendation has evolved quickly in the research community. Although the existing sequential POI recommendation models have reached an encouraging performance in predicting the next-N POIs to users, the data sparsity problem is still severe in the sequential POI recommendation task. It is challenging to learn the users’ preferences of POIs under the highly sparse dataset. Meanwhile, many sequential models focus on exploiting users’ interests from the entire dataset, ignoring the effect of collaborative information from similar users. To this end, we propose a user-based clustering deep model (UCDM) for the sequential POI recommendation to deal with these issues. UCDM extracts collaborative information via a user-based intent clustering module and uses a binary self-attention layer to both learn the general preference from the entire dataset and the local preference from the collaborative information. In addition, our proposed model uses a POI candidate filter to control the size of the POI candidate set to reduce the sparsity of the dataset. In the model learning phase, we adopt the Bayesian personalized ranking to train our model. The experiment verifies that our proposed UCDM outperforms the selected baseline models for the sequential POI recommendation on two real-world check-in datasets.},
  archive      = {J_KIS},
  author       = {Wang, Tianxing and Wang, Can and Tian, Hui and Liew, Alan Wee-Chung},
  doi          = {10.1007/s10115-024-02277-2},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2233-2258},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {User-based clustering deep model for the sequential point-of-interest recommendation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio meets text: A loss-enhanced journey with manifold mixup and re-ranking. <em>KIS</em>, <em>67</em>(3), 2195-2231. (<a href='https://doi.org/10.1007/s10115-024-02283-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio–text retrieval is the task of aligning natural language and audio such that instances from different modalities can be compared with a similarity metric. Contrastive representation learning is a popular way to address this problem by finetuning a dual encoder backbone. However, finetuning incurs a heavy computational burden and requires multiple graphics processing units. The main objective of this study is thus to drive enhanced retrieval performance in audio–text retrieval with lower computational burden and resources. Thus, a series of plug-and-play modules are introduced to elicit knowledge from pretrained audio and text encoders. From creating a competitive baseline with early and late fusion first, the study endeavors to improve the performance further by adapting deep metric Circle and Ranked List losses. This study utilizes Manifold Mixup as a strong regularizer and a novel post-processing step of re-ranking to refine the results further. These changes work in symphony to obtain superior performance than other models without the need to finetune encoders. Particularly, an improvement of 1–3% is obtained on the AudioCaps dataset, establishing a new state of the art. The results on the Clotho dataset remain competitive with other finetuning approaches utilizing heavy resources. Also, the model emerges as the best among the frozen encoder models across all the metrics. Moreover, the proposed modules are modality agnostic and hold great potential for other retrieval tasks beyond the domain of audio–text. Overall, this study establishes a strong and competitive baseline for future approaches in audio–text retrieval.},
  archive      = {J_KIS},
  author       = {Suryawanshi, Yash and Shah, Vedanshi and Randar, Shyam and Joshi, Amit},
  doi          = {10.1007/s10115-024-02283-4},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2195-2231},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Audio meets text: A loss-enhanced journey with manifold mixup and re-ranking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MuLoR: A multi-graph contrastive network for logical reasoning. <em>KIS</em>, <em>67</em>(3), 2171-2193. (<a href='https://doi.org/10.1007/s10115-024-02286-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logical reasoning tasks are more challenging than traditional machine reading comprehension tasks. The machine must recognize the logical relationships implicit in the text and use logical reasoning to derive an answer. Logical reasoning tasks currently face two major challenges. The first challenge is the difficulty of capturing the logical relationships implicit in the text. The second involves connecting the divide between distinct logical structures and the continuous space of text embeddings. In this study, we present a contrastive network based on multiple graphs designed to tackle these issues through the combination of both implicit and explicit logical connections, thereby enhancing reasoning capabilities. We use different construction strategies to create logical relationship graphs and logical hypergraph graphs. These graphs are integrated into a multi-graph contrastive network to learn higher-order logical representations, which are then used as inputs to a decoder for final prediction. The evaluation of our designed models was performed on datasets designed to assess reasoning ability, including ReClor, LogiQA and the extended iteration LogiQA 2.0. The experimental results show that our proposed method outperforms the state-of-the-art models. In particular, the results obtained on the LogiQA 2.0 dataset, which contains a larger number of samples, are particularly outstanding. Our model achieved an accuracy rate of 59.16%, outperforming most of the baseline comparisons by at least one percentage point, demonstrating its superior potential in complex reasoning tasks.},
  archive      = {J_KIS},
  author       = {Xiao, Jing and Lin, Guijin and Xiao, Yu and Li, Ping},
  doi          = {10.1007/s10115-024-02286-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2171-2193},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {MuLoR: A multi-graph contrastive network for logical reasoning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource allocation in fog computing: A survey on current state and research challenges. <em>KIS</em>, <em>67</em>(3), 2091-2170. (<a href='https://doi.org/10.1007/s10115-024-02274-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With fog computing, new services and applications are enabled on the internet of things by providing computational services at the network edge. Fog computing is emerging as a transformative paradigm, linking edge devices with centralized cloud resources. It improves network efficiency, lowers latency, and increases computing power. Resource allocation and optimization are critical for fog computing to achieve optimal system performance, efficient resource usage, and smooth user experiences. Throughout the presentation, we discussed the architecture, framework, comparison of fog computing with cloud computing, resource allocation strategies, and the relevance of resource allocation to fog computing. Various methods of optimization and allocation are discussed, along with their application to fog-enhanced vehicle services and vehicular fog computing. For the purpose of allocating resources, minimizing latency, and optimizing quality of service (QoS), a variety of techniques have been applied, including game theory, convex optimization, reinforcement learning, and genetic algorithms. Additionally, we discuss how fog computing environment resource allocation works using game theory. The purpose of this paper is to review several articles in the field of fog environments and to provide a detailed comparison of each from a variety of perspectives. An overview of the main features of the reviewed articles was also presented in the form of a table. This study highlights the effectiveness of these strategies for improving system performance, reducing latency, optimizing resources, and reducing energy consumption. Lastly, we highlight future research directions and potential contributions in fog computing. Management of heterogeneity, ensuring real-time optimization, ensuring QoS and security concerns, promoting energy-efficient computing and sustainability, managing mobility, scheduling and self-adaptive scheduling, load balancing, offloading, reliability, sensor lifetime, multiagent reinforcement learning, optimal resource allocation, and quality of experience are discussed. The purpose of this survey is to give readers a detailed understanding of state-of-the-art methods, challenges, and possible future directions in resource allocation and optimization in fog computing. The aim of this research is to synthesize insights from the literature in order to provide valuable insight for researchers, practitioners, and stakeholders interested in advancing the field of fog computing.},
  archive      = {J_KIS},
  author       = {Nemati, Amir Mohammad and Mansouri, Najme},
  doi          = {10.1007/s10115-024-02274-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2091-2170},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Resource allocation in fog computing: A survey on current state and research challenges},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recommender systems in smart campus: A systematic mapping. <em>KIS</em>, <em>67</em>(3), 2063-2089. (<a href='https://doi.org/10.1007/s10115-024-02240-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are extremely useful tools to provide the user with information that may be of interest. These systems are responsible for performing a series of procedures to filter items from massive databases and return only what the user would be looking for, which can be a product, a song, a movie or series, a website, news, or educational resources. Recommender systems are also intended for educational purposes, returning items such as teaching materials, video classes, books, courses, and short courses, for example. The environments in universities that aggregate these systems are called smart university campus. Sites that make use of multiple technologies, able to relate the virtual environment with the real and provide users with a fully integrated system. From this context, there was a systematic mapping of smart campus areas and recommendation systems. A study was conducted to investigate the relationship between these areas, through the search in four databases, between the years 2017 and 2024, identifying 894 papers, of which 101 were selected for analysis. We also identified some key documents in the area of recommender systems, as well as the technologies applied in each of them. The analysis conducted in this paper identified several research opportunities in the area. However, it was observed that many of the studies do not make clear the information that their applications will be used in conjunction with smart campus.},
  archive      = {J_KIS},
  author       = {Hideki Mensch Maruyama, Martin and Willig Silveira, Luan and da Silva Júnior, Elvandi and Casanova, Gabriel and Palazzo M. de Oliveira, José and Maran, Vinícius},
  doi          = {10.1007/s10115-024-02240-1},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2063-2089},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Recommender systems in smart campus: A systematic mapping},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum computing and quantum-inspired techniques for feature subset selection: A review. <em>KIS</em>, <em>67</em>(3), 2019-2061. (<a href='https://doi.org/10.1007/s10115-024-02282-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature subset selection is essential for identifying relevant and non-redundant features, which enhances classification accuracy and simplifies machine learning models. Given the computational difficulties of determining optimal feature subsets, heuristic and metaheuristic algorithms have been widely used. Recently, the rise of quantum computing has led to the exploration of quantum-inspired metaheuristics and quantum-based approaches for this task. Although various studies have explored quantum-inspired and quantum-based approaches for feature subset selection, a comprehensive review that critically examines their significance, limitations, underlying mechanisms, and future directions remains lacking in the literature. This paper addresses this gap by presenting the first in-depth survey of these approaches. We systematically selected and analyzed relevant studies from prominent research databases, providing a detailed evaluation of quantum-inspired metaheuristics and quantum computing paradigms applied to feature subset selection. Our findings indicate that quantum-inspired metaheuristic approaches often deliver superior performance compared to traditional metaheuristic methods for feature subset selection. Nevertheless, their reliance on classical computing limits their ability to fully realize the advantages offered by quantum computing. The quantum-based feature subset selection methods, on the other hand, show considerable promise but are frequently constrained by the current limitations of quantum hardware, making large-scale feature subset selection challenging. Given the rapid evolution of quantum computing, research on both quantum-inspired and quantum-based feature subset selection remains insufficient to draw definitive conclusions. We are optimistic that this review will provide a foundation for future advancements in feature subset selection as quantum computing resources become more accessible.},
  archive      = {J_KIS},
  author       = {Mandal, Ashis Kumar and Chakraborty, Basabi},
  doi          = {10.1007/s10115-024-02282-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {2019-2061},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Quantum computing and quantum-inspired techniques for feature subset selection: A review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Taxonomy of deep learning-based intrusion detection system approaches in fog computing: A systematic review. <em>KIS</em>, <em>67</em>(2), 2017. (<a href='https://doi.org/10.1007/s10115-024-02206-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_KIS},
  author       = {Najafli, Sepide and Toroghi Haghighat, Abolfazl and Karasfi, Babak},
  doi          = {10.1007/s10115-024-02206-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {2017},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction: Taxonomy of deep learning-based intrusion detection system approaches in fog computing: A systematic review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-class margin climbing with cost-sensitive learning in neural network classification. <em>KIS</em>, <em>67</em>(2), 1993-2016. (<a href='https://doi.org/10.1007/s10115-024-02279-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large margin stands as an intuitive indicator of reliable classifiers, reflecting classifier robustness and generalizability. However, due to the intricate nonlinear mapping, directly defining margin as the optimization objective for multi-layer neural network is always challenging. On the other hand, the cost sensitivity coefficients of individual samples hold promise for shaping decision boundaries of neural network classification. A question arises as to whether optimizing neural network classifier can be guided to achieve larger classification margin by varying instance-level cost sensitivity factor. Inspired by above question, this paper proposes a heuristic hard mining strategy designed to progressively identify challenging samples and amplify the output margin through cost-sensitive learning. The refinement process adjusts the sample distribution when optimization reaches a local minimum to ensure the sustainable optimization, ultimately leading to margin climbing. Two hard mining algorithms are designed for binary and multi-class classification problems, which utilize distinct margin definitions based on different decision-making scenarios. In the proposed method, we focus on establishing individualized margin between distinct categories to more accurately characterize the inter-class margin. Empirical results demonstrate that our proposed methodology enhances both the accuracy and robustness in neural network classification.},
  archive      = {J_KIS},
  author       = {Zhang, Siyuan and Xie, Linbo and Chen, Ying and Zhang, Shanxin},
  doi          = {10.1007/s10115-024-02279-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1993-2016},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Inter-class margin climbing with cost-sensitive learning in neural network classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evidence-based approach for open-domain question answering. <em>KIS</em>, <em>67</em>(2), 1969-1991. (<a href='https://doi.org/10.1007/s10115-024-02269-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain question answering (ODQA) stands at the forefront of advancing natural language understanding and information retrieval. Traditional ODQA systems, which predominantly utilize a two-step process of information retrieval followed by reading module, face significant challenges in aligning retrieved passages with the contextual nuances of user queries. This paper introduces a novel methodology that leverages a semi-structured knowledge graph to enhance both the accuracy and relevance of answers in ODQA systems. Our model employs a threefold approach: firstly, it extracts and ranks evidence from a textual knowledge graph, a semi-structured knowledge graph where the nodes are real-world entities and the edges are sentences that two entities co-occur in, based on the contextual relationships relevant to the question. Secondly, it utilizes this ranked evidence to re-rank initially retrieved passages, ensuring that they align more closely with the query’s context. Thirdly, it integrates this evidence into a generative reading component to construct precise and context-rich answers. We compare our model, termed contextual evidence-based question answering (CEQA), against traditional and state-of-the-art ODQA systems across several datasets, including TriviaQA, Natural Questions, and SQuAD Open. Our extensive experiments and ablation studies show that CEQA significantly outperforms existing methods by improving both the relevance of retrieved passages and the accuracy of the generated answers, thereby establishing a new benchmark in ODQA.},
  archive      = {J_KIS},
  author       = {Jafarzadeh, Parastoo and Ensan, Faezeh},
  doi          = {10.1007/s10115-024-02269-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1969-1991},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An evidence-based approach for open-domain question answering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised multi-hop heterogeneous hypergraph embedding with informative pooling for graph-level classification. <em>KIS</em>, <em>67</em>(2), 1945-1968. (<a href='https://doi.org/10.1007/s10115-024-02259-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In heterogeneous graph analysis, existing self-supervised learning (SSL) methods face several key challenges. Primarily, these approaches are tailored for node-level tasks and fail to effectively capture global graph-level features, a crucial aspect for comprehensive graph understanding. Furthermore, they predominantly rely on meta-path-based techniques to unravel graph structures, a process that can be computationally intensive and often intractable for complex networks. Another significant limitation is their inability to account for nonpairwise relationships, a common characteristic in real-world networks like protein-protein interaction and collaboration networks, limiting their effectiveness in graph-level learning where high-order connectivity is essential. To address these issues, we propose an innovative SSL framework for heterogeneous hypergraph embedding, expressly designed to enhance graph-level classification. Our framework introduces multi-hop attention in hypergraph convolution, a significant leap from existing attention mechanisms specifically for hypergraphs that primarily focus on immediate neighborhoods. This multi-hop approach allows for an expansive capture of relational structures, both near and far, uncovering intricate patterns integral to accurate graph-level classification. Complementing this, we implement an informative graph-level attentive pooling mechanism that surpasses traditional aggregation methods. It intelligently synthesizes features, taking into account their structural and semantic importance within the hypergraph, thereby preserving critical contextual information. Furthermore, we refine our contrastive learning approach and introduce targeted negative sampling strategies, creating a more robust learning environment that excels at discerning nuanced graph-level features. Rigorous evaluation against established graph kernels, graph neural networks, and graph pooling methods on real-world datasets demonstrates our model’s superior performance, validating its effectiveness in addressing the complexities inherent in heterogeneous graph-level classification.},
  archive      = {J_KIS},
  author       = {Hayat, Malik Khizar and Xue, Shan and Yang, Jian},
  doi          = {10.1007/s10115-024-02259-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1945-1968},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Self-supervised multi-hop heterogeneous hypergraph embedding with informative pooling for graph-level classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An oversampling algorithm for high-dimensional imbalanced learning with class overlapping. <em>KIS</em>, <em>67</em>(2), 1915-1943. (<a href='https://doi.org/10.1007/s10115-024-02276-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing standard learning methods suffer from poor performance in high-dimensional imbalanced learning with class overlapping. To tackle this problem, we propose a novel oversampling algorithm that aims to generate a robust ensemble of manifold dimensionality reduction, grid clustering, and information entropy criteria. Instead of simply balancing positive and negative numbers, the algorithm considers the difference in information entropy for interclass, which first reduces the dimensionality by manifold reduction, and then group data utilize grid clustering. Subsequently, calculate the oversampling weight of each group by information entropy and find seed samples based on entropy and neighborhood. Finally, SMOTE based on Beta distribution combined with standard classifiers achieve the rapid and precise classification for high-dimensional imbalanced datasets with class overlapping. Extensive experimental results on 20 real-world imbalanced datasets and compared with eight popular oversampling algorithms show that our proposed algorithm, while achieving good performance in terms of F-measure, G-mean, and AUPRC, can lead to robust performance under high-dimensional and overlapping. It is worth noting that our algorithm substantially reduces the number of synthetic samples against the quantity-balanced oversampling algorithms, and significantly reduces the generation of class overlapping.},
  archive      = {J_KIS},
  author       = {Yang, Xu and Xue, Zhen and Zhang, Liangliang and Wu, Jianzhen},
  doi          = {10.1007/s10115-024-02276-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1915-1943},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An oversampling algorithm for high-dimensional imbalanced learning with class overlapping},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on federated learning system: A new paradigm to machine learning. <em>KIS</em>, <em>67</em>(2), 1811-1914. (<a href='https://doi.org/10.1007/s10115-024-02257-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a machine learning technique that permits clients to train the model at a local site in a collaborative manner. It builds a global shared model on the basis of updates of the local model without exchanging data among multiple devices. Federated learning was introduced in 2016 with the goal of enabling local training as well as distributed machine learning training at the edge node’s level. It plays a vital role in terms of preserving the privacy of data while training the machine learning model on multiple devices. However, the introduction of federated learning into real-world applications exposes certain challenges in the training process, which affect the overall efficacy and efficiency of the federated learning model in real-world scenarios. As a result, an increasing number of researchers are now focusing on tackling the issues of FL and exploring various efficient research approaches to overcome these current obstacles. This paper systematically provides a detailed overview of federated learning, covering its definition, the need behind its development, privacy concepts, characteristics, and brief knowledge regarding different system components of federated learning. Different open-source frameworks that are available and used for implementing and solving problems related to federated learning have also been addressed in this article. Beyond this, the taxonomy of federated learning systems and different architectures for the same have also been discussed. In this paper, a brief comparison of related concepts with federated learning and a comparison among existing and popular federated learning studies proposed in different articles in the area of federated learning have also been summarized. In addition to the above-stated information, this article also provides brief information and a summary of various application areas of federated learning. Lastly, this paper briefly addresses the different challenges and prospects of research that lead to progress in this field.},
  archive      = {J_KIS},
  author       = {Chaudhary, Rajesh Kumar and Kumar, Ravinder and Saxena, Nitin},
  doi          = {10.1007/s10115-024-02257-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1811-1914},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A systematic review on federated learning system: A new paradigm to machine learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMNE: Link prediction using hypergraph motifs and network embedding in social networks. <em>KIS</em>, <em>67</em>(2), 1787-1809. (<a href='https://doi.org/10.1007/s10115-024-02255-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embeddings, which map nodes to low-dimensional vectors, facilitate link prediction, a pivotal aspect of complex network research. However, existing methods often overlook the complexities of hypergraphs and potent structures for modeling intricate relationships among multiple entities. This paper delves into link prediction within hypergraph motifs and network embedding (HMNE), crucial for diverse fields like knowledge graphs and bioinformatics. HMNE employs motifs to perform network embedding, representing nodes as hyper-nodes. HMNE utilizes the skip-gram model to get the embedding vectors by analyzing the sequence generated using a local random walk technique. Additionally, we consider hyper-motifs as super-nodes to highlight structural similarities between nodes. To further refine our methodology, we use the depth and breadth motif random walk strategy on the embedded network with hyper-nodes. This innovative approach enriches our understanding of network dynamics and enhances the predictive power of our model. We have thoroughly experimented the proposed method on several real-world datasets, and the results consistently demonstrate its usefulness.},
  archive      = {J_KIS},
  author       = {Zhang, Yichen and Lai, Shouliang and Peng, Zelu and Rezaeipanah, Amin},
  doi          = {10.1007/s10115-024-02255-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1787-1809},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {HMNE: Link prediction using hypergraph motifs and network embedding in social networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label classification with label clusters. <em>KIS</em>, <em>67</em>(2), 1741-1785. (<a href='https://doi.org/10.1007/s10115-024-02270-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification is the task of simultaneously predicting a set of labels for an instance, with global and local being the two predominant approaches. The global approach trains a single classifier to handle all classes simultaneously, while the local approach breaks down the problem into multiple binary problems. Despite extensive research, effectively capturing label correlations remains a challenge in both methods. In this paper, we introduce an approach that clusters the label space to create hybrid partitions (disjoint correlated label clusters), striking a balance between global and local strategies while leveraging both advantages. Our approach consists of (i) clustering the label space based on correlations, (ii) generating and validating the resulting hybrid partitions, (iii) selecting the best partitions, and (iv) evaluating their performance. We also compare our approach against an oracle, exhaustive search, and random search to assess how closely our hybrid partitions approximate the best possible partitions. The oracle selects the best partition using the test set, while the exhaustive approach relies on validation data. Experiments conducted on multiple multi-label datasets demonstrate that our method, along with random partitions, achieves results that are superior or competitive compared to traditional global and local approaches, as well as the state-of-the-art Ensemble of Classifier Chains. These findings suggest that conventional methods may not fully capture label correlations, and clustering the label space offers a promising solution.},
  archive      = {J_KIS},
  author       = {Gatto, Elaine Cecília and Ferrandin, Mauri and Cerri, Ricardo},
  doi          = {10.1007/s10115-024-02270-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1741-1785},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-label classification with label clusters},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEGODE: A structure-enhanced graph neural ordinary differential equation network model for temporal link prediction. <em>KIS</em>, <em>67</em>(2), 1713-1740. (<a href='https://doi.org/10.1007/s10115-024-02261-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of temporal link prediction is to forecast potential future connections in a network by analyzing its structural underpinnings and tracking its temporal dynamics. However, existing methods for temporal link prediction are overly reliant on the most recent snapshots of the network, thereby limiting their ability to uncover and utilize the fundamental evolutionary patterns for effective dynamical inference. As a result, the predictive prowess of these models tends to be heightened for proximate future scenarios, as opposed to those farther into the horizon. Furthermore, the majority of the current methodologies overlook the influence of intricate higher-order and overarching structural dynamics, which could potentially enhance predictive accuracy. To tackle these challenges, we introduce a structure-enhanced graph neural ordinary differential equation (SEGODE), a comprehensive framework that leverages neural ordinary differential equations integrated with attention mechanisms to facilitate dynamic inference. The framework enhances the ability to snatch higher-order and global structures. To substantiate the viability of our novel model, we embarked on a comprehensive set of experiments conducted on seven real datasets. The outcomes of these rigorous tests demonstrate that our SEGODE approach not just demonstrates commendable performance in the task of link prediction but additionally has good results even when data is sparse.},
  archive      = {J_KIS},
  author       = {Fu, Jiale and Guo, Xuan and Hou, Jinlin and Yu, Wei and Shi, Hongjin and Zhao, Yanxia},
  doi          = {10.1007/s10115-024-02261-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1713-1740},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SEGODE: A structure-enhanced graph neural ordinary differential equation network model for temporal link prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved affinity propagation clustering algorithms: A PSO-based approach. <em>KIS</em>, <em>67</em>(2), 1681-1711. (<a href='https://doi.org/10.1007/s10115-024-02260-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional clustering algorithms such as K-means need to input the number of clusters before the start of the algorithm. Affinity propagation (AP) clustering algorithm solves this problem by considering each data point as a prospective cluster head (exemplar) and finding a set of appropriate exemplars by message passing. However, the AP clustering algorithm requires two parameters: preference and damping factor. Providing the parameters in advance poses the same issue faced in the traditional clustering algorithm. Moreover, all data points are not equally relevant for becoming cluster heads. To overcome these problems, we propose two parameter-free particle swarm optimization-based algorithms, PSO-APver1 and PSO-APver2. Furthermore, we introduce a novel version of mutant PSO where two cluster validity indices are used to judge the quality of the clustering solution. In PSO-APver2, we consider the internal data distribution using the square wave function to determine the initial preference value of data points. We conducted experiments on 8 real-world datasets to show the efficacy of our proposed algorithms over classic algorithms and two AP-based algorithms. We conducted the Friedman test followed by post hoc analysis to exhibit the significance of our work.},
  archive      = {J_KIS},
  author       = {Sinha, Ankita and Jana, Prasanta K.},
  doi          = {10.1007/s10115-024-02260-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1681-1711},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improved affinity propagation clustering algorithms: A PSO-based approach},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Additive consistency analysis for nested probabilistic linguistic preference relations and its application in decision making. <em>KIS</em>, <em>67</em>(2), 1651-1680. (<a href='https://doi.org/10.1007/s10115-024-02231-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making problems often involve complex and uncertain evaluation criteria, making linguistic variables more practical than precise numerical values for describing information. Among various linguistic terms, the nested probabilistic linguistic term excels at expressing multidimensional information through its dual-layer structure, which captures both ordinal and nominal meanings. Using NPLTs to make expressions, this paper introduces nested probabilistic linguistic preference relations (NPLPRs) for effectively conveying preference information in decision-making scenarios and provides new operational rules for quantitative computations. To ensure the consistency of the preference relations, we define a consistency index and threshold to assess the additive consistency of NPLPRs and innovatively propose an automatic iteration algorithm to improve NPLPRs with unacceptable consistency. To reflect the evaluation focus and obtain the final results, we consider the weights of nominal terms when aggregating preferences with acceptable consistency. Finally, an illustrative example of selecting the most environmentally sustainable city demonstrates the application and advantages of our approach, supported by comparative analyses.},
  archive      = {J_KIS},
  author       = {Xiao, Jinglin and Wang, Xinxin and Xu, Zeshui},
  doi          = {10.1007/s10115-024-02231-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1651-1680},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Additive consistency analysis for nested probabilistic linguistic preference relations and its application in decision making},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep learning and similarity measures for requirements-driven composition of semantic web services. <em>KIS</em>, <em>67</em>(2), 1627-1649. (<a href='https://doi.org/10.1007/s10115-024-02244-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The composition of web service is the best chance provided by Service-Oriented Computing and Service-Oriented Architecture as it gives real competitive benefits for some industrial and technological actors via presenting them with the probability to guarantee fast and inexpensive improvement of collaborative and distributed software applications. Here, a novel technique is introduced, which contains several phases for better web services. At first, the requirements specification phase is enabled with a set of requirements such as non-functional and functional requirements. Next to the requirements specification stage, the discovery stage is enabled to choose the suitable web services that have high-matching profiles with the developer’s requirement set. Here, for a semantic matching algorithm, a new hybrid similarity measure is developed. Additionally, among the group of candidate services that the discovery phase returned, the best service is selected during the selection step. Then, hybrid Squeeze_Long Short-Term Memory (Squeeze_LSTM) is used for choosing the best service and it is designed by the formation of SqueezeNet and LSTM. The Semantic Web Services are finally implemented. The efficiency of the Squeeze_LSTM is evaluated and has achieved a superior precision of 0.909, recall of 0.890, and response time of 6.461S.},
  archive      = {J_KIS},
  author       = {Bhuvaneswari, A. and Sumathi, K. and Sarveshwaran, Velliangiri and Sivasangari, A.},
  doi          = {10.1007/s10115-024-02244-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1627-1649},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Hybrid deep learning and similarity measures for requirements-driven composition of semantic web services},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A proposed real-time decision support platform for moroccan fixed mining production systems. <em>KIS</em>, <em>67</em>(2), 1597-1626. (<a href='https://doi.org/10.1007/s10115-024-02271-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the competition in the mining markets and the rapid evolution of customer requirements, the Moroccan mining group OCP (Office Chérifien des Phosphates) has been forced to improve the performance of its production systems. Thus, continuous performance improvement and optimization of production processes are prerequisites to remain competitive. However, in Morocco, data analytics-based mining process improvements do not fully utilize the data generated during process execution. They lack prescriptive methodologies, which is the major goal of this work, to translate analytic results into improvement actions. Indeed, we propose a new platform for optimizing the production processes of a Moroccan mine based on knowledge extraction from data, allowing mine managers to rapidly and continuously improve the performance of their production chains. The platform will be an effective and efficient tool for mining companies to generate prescriptive action recommendations during the execution of the processes.},
  archive      = {J_KIS},
  author       = {Battas, Ilham and Behja, Hicham and El Ouazguiti, Mohamed},
  doi          = {10.1007/s10115-024-02271-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1597-1626},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A proposed real-time decision support platform for moroccan fixed mining production systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving density peak clustering on multi-dimensional time series: Rediscover and subdivide. <em>KIS</em>, <em>67</em>(2), 1573-1596. (<a href='https://doi.org/10.1007/s10115-024-02272-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density peak clustering (DPC) algorithm identifies patterns in high-dimensional data and obtains robust outcomes across diverse data types with minimal hyperparameters. However, DPC may produce inaccurate pattern sizes in multi-dimensional datasets and exhibit poor performance in recognizing similar patterns. To solve these issues, we propose the rediscover and subdivide density peak clustering algorithm (RSDPC), which follows three key strategies. The first strategy, rediscover, iteratively uncovers prominent patterns within the existing data. The second strategy, subdivide, partitions patterns into several similar subclasses. The third strategy, re-sort, rectifies errors from the preceding steps by incorporating critical distance and nearest distance considerations. The experimental results show that RSDPC is feasible and effective in synthetic and practical datasets compared with state-of-the-art works.},
  archive      = {J_KIS},
  author       = {Wang, Huina and Liu, Bo and Zhao, Huaipu and Qu, Guangzhi},
  doi          = {10.1007/s10115-024-02272-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1573-1596},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving density peak clustering on multi-dimensional time series: Rediscover and subdivide},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A customized balanced-objective genetic algorithm for task scheduling in reconfigurable computing systems. <em>KIS</em>, <em>67</em>(2), 1541-1571. (<a href='https://doi.org/10.1007/s10115-024-02268-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfiguration and hardware implementation capabilities in reconfigurable computing (RC) systems make them more appropriate to recent computationally intensive applications. However, reaching optimal resource utilization remained as one of the main challenges in these systems. In order to implement more tasks in each reconfiguration interval, several decisive factors such as execution time, data communication cost, and required hardware resources must be analyzed simultaneously. In this paper, we proposed a novel balanced-objective task selector combined with a genetic algorithm to efficiently pick up the tasks of an application and occupy the resources as more as possible. The multi-objective fitness function of this algorithm adequately partitions the input application and provides the desirable intra and inter-cluster characteristics. Moreover, a new chromosome encoding technique has been developed to prevent precedence constraint violation of invalid solutions by removing forbidden regions in the search space. We classified the input applications with topological features such as first level parallel tasks (FLPT) and critical path length (CPL) for comprehensive evaluation. Several experiments are performed on randomly generated and real-world Directed Acyclic Graphs (DAGs), and the results are more satisfying in DAGs with more FLPTs and shorter CPLs where up to 28.63% makespan and 29.3% resource utilization improvement have been achieved in comparison with previous methods.},
  archive      = {J_KIS},
  author       = {Gholamrezanejad, Milad and Shahhoseini, Hadi Shahriar and Mohtavipour, Seyed Mehdi},
  doi          = {10.1007/s10115-024-02268-3},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1541-1571},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A customized balanced-objective genetic algorithm for task scheduling in reconfigurable computing systems},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSSN: A novel cache placement method based on adapted shannon entropy and simple additive weighting method in named data networking. <em>KIS</em>, <em>67</em>(2), 1507-1540. (<a href='https://doi.org/10.1007/s10115-024-02266-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new strategy called PSSN to address the Cache Placement challenge in NDN. The paper first presents a novel clustering method based on the SAW decision-making approach and dynamically adapted Shannon weighting. Criteria such as the number of neighbors, hop count, router capacity, and CPU power are simultaneously considered for clustering and determining cluster heads. A key innovation in the proposed clustering is storing a copy of each content in each cluster to reduce duplicate content, increase content diversity, and consequently improve hit rate while reducing latency. Subsequently, for content placement, popularity of content, remaining router capacity, and hop count are analyzed concurrently using the SAW method adapted with the proposed approach. This ensures that popular content is placed closer to requesters. Throughout all stages of the method, the dynamic change in the status of content requests from users leads to a dynamic adjustment of the criteria weights. Simulation results using NDNsim demonstrate improvements in key parameters, with average enhancements of 17.8% and 9% for Hit rate and Delivery Time, respectively, as well as a 30.75% improvement in Load Balancing compared to recent methods.},
  archive      = {J_KIS},
  author       = {Soltani, Mohammad and Barekatain, Behrang and Hendessi, Faramarz and Beheshti, Zahra},
  doi          = {10.1007/s10115-024-02266-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1507-1540},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PSSN: A novel cache placement method based on adapted shannon entropy and simple additive weighting method in named data networking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly-aware symmetric non-negative matrix factorization for short text clustering. <em>KIS</em>, <em>67</em>(2), 1481-1506. (<a href='https://doi.org/10.1007/s10115-024-02226-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text clustering is a significant yet challenging task, where short texts generated from the Internet are extremely sparse, noisy, and ambiguous. The sparse nature makes traditional clustering methods, e.g., k-means family and topic modeling, much less effective. Fortunately, recent arts of document distance, e.g., word mover’s distance, and document representation, e.g., BERT, can accurately measure the similarities of short texts, especially their nearest neighbors. Inspired by those arts and observations, we induce short text clusters by directly factorizing the informative affinity matrix of nearest neighbors into the product of the cluster assignment matrix, following the intuition that neighboring short texts tend to be assigned to the same cluster. However, due to the noisy nature of short texts, many of them can be regarded as outliers or near outliers, resulting in many noisy neighboring similarities within the affinity matrix. To further alleviate this problem, we enhance the affinity matrix factorization by (1) incorporating a sparse noisy matrix to directly capture noisy neighboring similarities and (2) regularizing the cluster assignment matrix by $$\ell _{2,1}$$ norm to eliminate hard-to-clustering short texts (called pseudo-outliers), so as to indirectly neglect noisy neighboring similarities corresponding to them. After this factorization for pre-clustering, we train a classifier over the resulting clusters and adopt it to assign each pseudo-outlier to one cluster finally. We call this novel clustering method as anomaly-aware symmetric non-negative matrix factorization ( $$\hbox {A}^{2}$$ snmf). Experimental results on benchmark short text datasets demonstrate that $$\hbox {A}^{2}$$ snmf performs very competitively with the existing baseline methods. The code is available at the website https://github.com/wizardbo/A3SNMF_functions .},
  archive      = {J_KIS},
  author       = {Li, Ximing and Guan, Yuanyuan and Fu, Bo and Luo, Zhongxuan},
  doi          = {10.1007/s10115-024-02226-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1481-1506},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Anomaly-aware symmetric non-negative matrix factorization for short text clustering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CETra: Online cluster tracking for clustering of streaming data sources. <em>KIS</em>, <em>67</em>(2), 1455-1479. (<a href='https://doi.org/10.1007/s10115-024-02267-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data stream clustering tasks may be applied to cluster streaming data objects (clustering by examples) or to cluster streaming data sources based on their temporal behavior (clustering by variables). We focus on the latter problem and propose CETra (Cluster evolution tracker)—the first online cluster tracking technique designed to provide information regarding cluster evolution in a streaming scenario of clustering by variables with efficient processing suitable for real-time problems. CETra can trace different intra and inter-cluster changes by considering not only statistics of interest but also the clusters’ membership, thus allowing a deeper understanding of the clustering results. Experimental evaluation using synthetic datasets and real data from meteorological sensors shows that CETra can track abrupt and gradual cluster transitions, while the competing method misses most of the gradual changes. Furthermore, CETra performs efficiently in a clustering environment for multiple streaming data sources, twice as fast as the related method.},
  archive      = {J_KIS},
  author       = {Sousa Lima, Afonso Matheus and de Sousa, Elaine Parros Machado},
  doi          = {10.1007/s10115-024-02267-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1455-1479},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CETra: Online cluster tracking for clustering of streaming data sources},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A path-based distance computation for non-convexity with applications in clustering. <em>KIS</em>, <em>67</em>(2), 1415-1453. (<a href='https://doi.org/10.1007/s10115-024-02275-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms are essential in data analysis, but evaluating their performance is challenging when the true labels are not available, especially for non-convex clusters. Traditional performance evaluation metrics struggle to identify clustering quality, often assigning higher scores for linearly separated clusters than the true clusters. We propose an original approach to distance computation that accounts for the data structure, thus improving the clustering quality evaluation for non-convex clusters without affecting other shapes of clusters. We also showcase the applicability of this method through a modified version of K-Means using the proposed method that is capable of correctly separating non-convex clusters. The validation included the analysis of performance and time complexity of 3 traditional clustering quality evaluation metrics and the K-Means clustering algorithm against their augmented versions with the proposed approach. This analysis conducted on 7 benchmark synthetic datasets and 6 real datasets with various numbers of examples and features of diverse characteristics and joint complexities: simple convex clusters, overlapped and imbalanced clusters, and non-convex clusters. Through these analyses, we show the ineffectiveness of traditional methods and that the proposed approach overcomes the weaknesses of traditional methods.},
  archive      = {J_KIS},
  author       = {Ardelean, Eugen-Richard and Portase, Raluca Laura and Potolea, Rodica and Dînșoreanu, Mihaela},
  doi          = {10.1007/s10115-024-02275-4},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1415-1453},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A path-based distance computation for non-convexity with applications in clustering},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outlier detection in classification based on feature-selection-based regression. <em>KIS</em>, <em>67</em>(2), 1399-1414. (<a href='https://doi.org/10.1007/s10115-024-02264-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An outlier is a datum that is far from other data points in which it occurs. The appearance of outliers results in a complexity to obtain an accurate classification; numerous statistical and machine learning methods have been proposed to identify the outliers. This paper devotes a regression-based algorithm to the detection and identification of outlier before selecting a suitable classifier. The problem is firstly converted to an high-dimensional regression, then a novel method, based on the combination of multiple-correlation-coefficient-based feature selection for dimensional reduction and t-test for sparsification, is proposed, and an iterated algorithm is also given. Performance on simulated numerical data, low-dimensional iris data and high-dimensional DBWorld E-mail data demonstrate the superiority of the proposed method in outlier identification for classification.},
  archive      = {J_KIS},
  author       = {Su, Jinxia and Liu, Qiwen and Cui, Jingke},
  doi          = {10.1007/s10115-024-02264-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1399-1414},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Outlier detection in classification based on feature-selection-based regression},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An energy-aware migration framework using metaheuristic algorithm in cloud computing. <em>KIS</em>, <em>67</em>(2), 1373-1398. (<a href='https://doi.org/10.1007/s10115-024-02224-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pervasive computing requires dynamic, energy-efficient cloud architecture to deploy services in VMs to distributed computing nodes. This mapping must ensure the service level agreement (SLA) runs VMs without disruption. This paper presents an energy-efficient metaheuristic Rock Hyrax algorithm-based cloud VM migration architecture. The male Rock Hyraxes find food and ensure the colony’s safety. His behavior has been mimicked in our proposed algorithm for finding energy-efficient compute nodes. A multi-objective VM migration function considers job submission deadlines. The proposed method reduces SLA violations and energy utilization while optimizing resource utilization. When evaluating the project, makespan, energy efficiency, and SLA violations were considered. The proposed algorithm is simulated on the CloudSim simulator considering both resources and jobs dynamic in nature. The suggested strategy outperforms ant colony optimization, particle swarm optimization, cuckoo optimization, and modified gray wolf optimization. The migration technique improves resource utilization by 18%, makespan time by 5%, SLA violation by 13%, and energy usage by 15%.},
  archive      = {J_KIS},
  author       = {Singhal, Saurabh and Sharma, Ashish},
  doi          = {10.1007/s10115-024-02224-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1373-1398},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An energy-aware migration framework using metaheuristic algorithm in cloud computing},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A timely personalized comments generation assistant based on LSTM-SNP. <em>KIS</em>, <em>67</em>(2), 1351-1372. (<a href='https://doi.org/10.1007/s10115-024-02198-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Posting personalized comments on an upcoming hot topic in time is very meaningful, which not only likely to attract more users to participate, but also affects other users’ point of view. LSTM-SNP is a variant of long short-term memory (LSTM) inspired by the nonlinear spiking mechanism in nonlinear spiking neural systems. In order to improve the efficiency and diversity of user-edited comments, we design a novel assistant based on the LSTM-SNP model. The assistant consists of two modules, one for predicting topic hotness and the other for generating comments with personalized expression features based on blog post and user information. Experimental results show that, this novel assistant not only predicts the upcoming hot topics accurately, but also outperforms the baseline model in terms of automatic evaluation and human discernment of comment generation. More importantly, the generated comments excel in terms of timeliness and personalization.},
  archive      = {J_KIS},
  author       = {Li, Yixiao and Wu, Yue and Li, Wenjia and Chen, Hui and Chen, Qi and Li, Yuehui},
  doi          = {10.1007/s10115-024-02198-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1351-1372},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A timely personalized comments generation assistant based on LSTM-SNP},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IC-SNI: Measuring nodes’ influential capability in complex networks through structural and neighboring information. <em>KIS</em>, <em>67</em>(2), 1309-1350. (<a href='https://doi.org/10.1007/s10115-024-02262-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influential nodes are the important nodes that most efficiently control the propagation process throughout the network. Among various structural-based methods, degree centrality, k-shell decomposition, or their combination identify influential nodes with relatively low computational complexity, making them suitable for large-scale network analysis. However, these methods do not necessarily explore nodes’ underlying structure and neighboring information, which poses a significant challenge for researchers in developing timely and efficient heuristics considering appropriate network characteristics. In this study, we propose a new method (IC-SNI) to measure the influential capability of the nodes. IC-SNI minimizes the loopholes of the local and global centrality and calculates the topological positional structure by considering the local and global contribution of the neighbors. Exploring the path structural information, we introduce two new measurements (connectivity strength and effective distance) to capture the structural properties among the neighboring nodes. Finally, the influential capability of a node is calculated by aggregating the structural and neighboring information of up to two-hop neighboring nodes. Evaluated on nine benchmark datasets, IC-SNI demonstrates superior performance with the highest average ranking correlation of 0.813 with the SIR simulator and a 34.1% improvement comparing state-of-the-art methods in identifying influential spreaders. The results show that IC-SNI efficiently identifies the influential spreaders in diverse real networks by accurately integrating structural and neighboring information.},
  archive      = {J_KIS},
  author       = {Nandi, Suman and Curado Malta, Mariana and Maji, Giridhar and Dutta, Animesh},
  doi          = {10.1007/s10115-024-02262-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1309-1350},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {IC-SNI: Measuring nodes’ influential capability in complex networks through structural and neighboring information},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised feature selection with minimal redundancy based on group optimization strategy for multi-label data. <em>KIS</em>, <em>67</em>(2), 1271-1308. (<a href='https://doi.org/10.1007/s10115-024-02258-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of intelligence technology, high-dimensional multi-label data exist in practical applications, which makes multi-label learning a significant challenge. Feature selection can obtain more distinguishable features to enhance recognition ability to address high-dimensional problems. Nowadays, most researchers usually evaluate the relevance between labels and features and the similarity between samples. They only focus on the local characteristics of samples without considering the global characteristics. To solve the above problems, in this paper, a novel feature selection approach for semi-supervised learning with minimal redundancy and group optimization strategy (SFGR) in multi-label scenario is proposed. First, a measure based on the Laplacian score and constrain score is utilized to evaluate the relevance between each feature and label. Meanwhile, the global structure of the data is considered via the creation of graphs and a priori information to obtain the feature subset with the highest relevance. Secondly, an optimization iteration algorithm based on a regularization term combining $$\text {l}_1$$ -norm and $$\text {l}_2$$ -norm is employed to ensure the sparsity of the feature weight matrix and minimize the redundancy. Moreover, a group optimal strategy is applied as a global search approach to fusion the feature subsets to obtain an approximate globally optimal feature subset. Eventually, experimental results on various multi-labeled datasets show that SFGR can perform better than other algorithms.},
  archive      = {J_KIS},
  author       = {Qing, Depeng and Zheng, Yifeng and Zhang, Wenjie and Ren, Weishuo and Zeng, Xianlong and Li, Guohe},
  doi          = {10.1007/s10115-024-02258-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1271-1308},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Semi-supervised feature selection with minimal redundancy based on group optimization strategy for multi-label data},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All is attention for multi-label text classification. <em>KIS</em>, <em>67</em>(2), 1249-1270. (<a href='https://doi.org/10.1007/s10115-024-02253-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label text classification(MLTC) is a key task in natural language processing. Its challenge is to extract latent semantic features from text and effectively exploit label-associated features. This work proposes an MLTC model driven solely by attention mechanisms, which includes Graph Attention(GA), Class-Specific Attention(CSA), and Multi-Head Attention(MHA) modules. The GA module examines and records label dependencies by considering label semantic features as attributes of graph nodes. It uses graph embedding to maintain structural relationships within the label graph. Meanwhile, the CSA module produces distinctive features for each category by utilizing spatial attention scores, thereby improving classification accuracy. Then, the MHA module facilitates extensive feature interactions, enhancing the expressiveness of text features and supporting the handling of long-range dependencies. Experimental evaluations conducted on two MLTC datasets show that our proposed model outperforms existing MLTC algorithms, achieving state-of-the-art performance. These results highlight the effectiveness of our attention-based approach in tackling the complexity of MLTC tasks.},
  archive      = {J_KIS},
  author       = {Liu, Zhi and Huang, Yunjie and Xia, Xincheng and Zhang, Yihao},
  doi          = {10.1007/s10115-024-02253-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1249-1270},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {All is attention for multi-label text classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoRBS: A dynamic storytelling algorithm using a novel contextualization approach for documents utilizing BERT features. <em>KIS</em>, <em>67</em>(2), 1213-1248. (<a href='https://doi.org/10.1007/s10115-024-02263-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Storytelling is the process of connecting documents one after another representing the evolution of an event. Existing algorithms for storytelling connect events based on content overlaps between consecutive documents ignoring the role of the same term in different documents and the contemporary contexts (e.g., dynamic embeddings) of documents and terms. Due to the lack of role and contemporary contexts in the designs of the existing storytelling methods, the resultant stories frequently jump to documents with the keywords to form a chain but not a meaningful one. In this paper, we present a novel storytelling algorithm—Contextual Role-Based Storytelling (CoRBS)—that generates a chain of documents explaining the evolution of an event, addressing role and contemporary context issues of existing methods. CoRBS starts with a given document and moves forward temporally, stitching together role and context-driven documents to represent the evolution of the events that appear in the first document. We define the role of a term in a document as a distribution of similarities of the nearest neighbors of the term based on BERT embeddings of all terms of that document. Contemporary contexts are incorporated as a mechanism to discover a coherent next document while the story progresses. Our experiments demonstrate that CoRBS generates more meaningful stories compared to other baseline storytelling techniques.},
  archive      = {J_KIS},
  author       = {Nouri, Alireza and Hossain, M. Shahriar},
  doi          = {10.1007/s10115-024-02263-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1213-1248},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CoRBS: A dynamic storytelling algorithm using a novel contextualization approach for documents utilizing BERT features},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compact lossy compression of tensors via neural tensor-train decomposition. <em>KIS</em>, <em>67</em>(2), 1169-1211. (<a href='https://doi.org/10.1007/s10115-024-02252-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TensorCodec, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions.TensorCodec incorporates three key ideas. The first idea is neural tensor-train decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be exploited by NTTD for improved approximation. In addition, we extend TensorCodec to enable the lossy compression of tensors with missing entries, often found in real-world datasets. Our analysis and experiments on 8 real-world datasets demonstrate that TensorCodec is (a) Concise: it gives up to $$7.38 \times $$ more compact compression than the best competitor with similar reconstruction error, (b) Accurate: given the same budget for compressed size, it yields up to $$3.33\times $$ more accurate reconstruction than the best competitor, (c) Scalable: Its empirical compression time is linear in the number of tensor entries, and it reconstructs each entry in logarithmic time. Our code and datasets are available at https://github.com/kbrother/TensorCodec .},
  archive      = {J_KIS},
  author       = {Kwon, Taehyung and Ko, Jihoon and Jung, Jinhong and Jang, Jun-Gi and Shin, Kijung},
  doi          = {10.1007/s10115-024-02252-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1169-1211},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Compact lossy compression of tensors via neural tensor-train decomposition},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Book recommendation using sentiment analysis and ensembling hybrid deep learning models. <em>KIS</em>, <em>67</em>(2), 1131-1168. (<a href='https://doi.org/10.1007/s10115-024-02250-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An immense volume of user-generated content exists online due to the exponential growth of internet usage among individuals. However, this abundance presents substantial challenges for recommendation systems and online customer services. The diverse data, encompassing consumer emails, reviews, and comments, contain a broad spectrum of information, making it challenging to decipher the underlying emotions and nuances. In social media analytics, sentiment analysis has emerged as a pivotal tool to unveil the emotional context embedded within textual data, offering deeper insights into user attitudes, opinions, and sentiments. This study introduces a novel strategy to strengthen the performance and precision of sentiment analysis-based book recommendation systems through ensemble learning on hybrid deep learning models. These book recommendation models leverage customer ratings and reviews from a vast Amazon books dataset as input. Initially, we used TextBlob to assess the polarity of customer reviews, categorizing them into neutral, negative, and positive sentiments. Subsequently, the input data underwent preprocessing, tokenization, and word embedding using bidirectional encoder representations from transformers (BERT). To effectively analyze and filter processed review comments and ratings, the proposed ensemble model integrates a diverse array of hybrid deep learning architectures, including long short-term memory (LSTM), bidirectional LSTM (BiLSTM), gated recurrent unit (GRU), and convolutional neural network (CNN). Extensive experimentation validated the superiority of the proposed ensemble model, achieving an impressive accuracy and F1-score of 98.21%. The significance of the approach lies in its ability to provide more accurate and contextually relevant book recommendations by considering the nuanced emotions expressed in customer reviews. This contributes to enhancing user satisfaction and engagement with recommendation systems, ultimately improving the overall quality of personalized book suggestions. Evaluation metrics further validate the efficacy of the proposed model, underscoring its practical utility in real-world applications of sentiment-based book recommendation systems.},
  archive      = {J_KIS},
  author       = {Devika, P. and Milton, A.},
  doi          = {10.1007/s10115-024-02250-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1131-1168},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Book recommendation using sentiment analysis and ensembling hybrid deep learning models},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion pattern mining. <em>KIS</em>, <em>67</em>(2), 1101-1129. (<a href='https://doi.org/10.1007/s10115-024-02254-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a diffusion network, some nodes exhibit similar diffusion patterns as they have analogous influence reachabilities to the other nodes. When these nodes are selected as initially infected nodes, they tend to yield similar infection results. Mining diffusion patterns of nodes is of practical significance in various applications, such as online marketing and epidemic prevention. Nonetheless, few existing work has effectively addressed this problem. In this work, we investigate how to find out which nodes in a diffusion network share similar diffusion pattern based only on historical infection results. Toward this, we first reconstruct the structure of influence relationships in the network, and then infer the infection propagation probability on each influence relationship, based on which the influence reachability of each node can be estimated. We present a diffusion pattern similarity metric to quantify the similarity of influence reachabilities, and group nodes that share similar influence reachabilities via hierarchical clustering. Extensive experimental results on both synthetic and real-world networks verify the effectiveness and efficiency of our approach.},
  archive      = {J_KIS},
  author       = {Yan, Qian and Yang, Yulan and Yin, Kai and Gan, Ting and Huang, Hao},
  doi          = {10.1007/s10115-024-02254-9},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1101-1129},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Diffusion pattern mining},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward deep multi-view document clustering using enhanced semantic embedding and consistent context semantics. <em>KIS</em>, <em>67</em>(2), 1073-1100. (<a href='https://doi.org/10.1007/s10115-024-02249-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view document clustering (MVDC) is a sophisticated approach in natural language processing that leverages multiple representations or views of data to improve clustering performance. Existing solutions are challenging due to inconsistency of document views, high dimensions, and sparseness in text documents. On the other hand, existing MVDC-based methods often depend on the performance of bag-of-words and pretrained language models. However, these models usually do not consider contextual semantics and are suitable for single-view document clustering. This paper addresses these challenges by proposing a deep MVDC model that utilizes enhanced semantic embedding and consistent context semantics (SECS). SECS uses semantic embedding to address high-dimensional challenges by considering complementary semantic information. Meanwhile, SECS takes advantage of the potential benefits of view-consistent context semantics based on pretrained language models. The proposed model captures intricate semantic relationships between words and documents through advanced embedding techniques, ensuring a richer and more nuanced representation of textual content. Furthermore, by incorporating consistent context semantics, SECS maintains contextual integrity across multiple views, leading to more coherent and meaningful clusters. Experimental results on benchmark datasets demonstrate the superiority of our model over state-of-the-art MVDC methods, highlighting its effectiveness in improving clustering quality and interpretability.},
  archive      = {J_KIS},
  author       = {Du, Yongsheng and Sun, Hongwei and Abdollahi, MohammadJavad},
  doi          = {10.1007/s10115-024-02249-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1073-1100},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Toward deep multi-view document clustering using enhanced semantic embedding and consistent context semantics},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Twitter sentiment analysis using ensemble of multi-channel model based on machine learning and deep learning techniques. <em>KIS</em>, <em>67</em>(2), 1045-1071. (<a href='https://doi.org/10.1007/s10115-024-02256-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People post a lot of comments on the websites these days, as social media and the Internet are major parts of modern life. With so much information available online related to services, products, politics, stocks, etc., thus, using artificial intelligence to understand the emotions in these comments is highly beneficial for understanding public opinion. In particular, detecting sentiment polarity in customer reviews is crucial for businesses to make informed decisions. Despite the vast amount of information available on the internet, understanding the underlying emotions in user comments remains a challenge. Our work aims to bridge this gap by proposing a sophisticated sentiment analysis model that leverages state-of-the-art deep learning techniques. In this study, we present a sentiment analysis model that combines advanced deep learning neural networks: convolutional neural network, long short-term memory networks (LSTM), Bidirectional LSTM (BiLSTM), and Bidirectional Encoder Representations from Transformers (BERT). Accurate feature extraction plays a pivotal role in sentiment analysis applications. By merging pre-trained BERT with sophisticated neural networks, the devised model achieves an impressive accuracy of 94.95%. We evaluated the proposed model on a publicly available Twitter Sentiment Analysis dataset. The proposed ensemble multi-channel model outperforms several deep learning and machine learning techniques in sentiment analysis. Hence, we suggest the use of the ensemble model to accurately determine sentiments from tweets and other textual data.},
  archive      = {J_KIS},
  author       = {Tembhurne, Jitendra V. and Lakhotia, Kirtan and Agrawal, Anant},
  doi          = {10.1007/s10115-024-02256-7},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1045-1071},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Twitter sentiment analysis using ensemble of multi-channel model based on machine learning and deep learning techniques},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic regex synthesis methods for english: A comparative analysis. <em>KIS</em>, <em>67</em>(2), 1013-1043. (<a href='https://doi.org/10.1007/s10115-024-02232-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular expressions (short form regex) find their application in program script synthesis, machine translation, information extraction and web applications, such as input validations. Their expressiveness and flexibility make them decidedly the best tool for many challenging text extraction tasks. Writing regex manually has been labeled as a laborious, time consuming and error prone task even for skilled programmers. An abundance of regex generation from text queries at online platforms mainly Stackoverflow and Quora signifies the automatic regex synthesis problem. Despite their popularity, a criminal lack of comprehensive literature study on the problem has also been observed. We intend to perform a detailed review of a variety of methods available for regex synthesis, repair, and learn beneficial lessons for appropriate datasets with one earnest goal: to synthesize resource efficient and correct regexes for given textual description.},
  archive      = {J_KIS},
  author       = {Tariq, Sadia and Rana, Toqir Ahmad},
  doi          = {10.1007/s10115-024-02232-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {1013-1043},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Automatic regex synthesis methods for english: A comparative analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chromosome segmentation and classification: An updated review. <em>KIS</em>, <em>67</em>(2), 977-1011. (<a href='https://doi.org/10.1007/s10115-024-02243-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Karyotyping is a study of chromosomes to identify various chromosomal aberrations related to structure and number. Chromosome image analysis involves challenging issues related to overlapping and touching of chromosomes. Chromosome segmentation and classification generally focus on separating overlapping and touching chromosomes. The analysis methods start from conventional image processing methods to advanced machine learning techniques. These methods are broadly classified into low-level and high-level methods. The low-level methods are thresholding-based approaches, edge detection, feature extraction techniques like active contours and watershed approaches and machine learning for classification. The high-level methods are deep learning algorithms like convolutional neural networks (CNNs), U-Net, autoencoder architectures. These methods help in improving accuracy and automate the process of chromosome segmentation and classification. High-level approaches can handle complexity in chromosome overlaps which provides better segmentation results. The approach learns complicated patterns and structures of chromosome images, which helps in achieving better classification accuracy. The challenges are: (i) working on large and annotated dataset for training deep learning models and (ii) suffer issues with new dataset even in they perform better during training phase. The solution for all these can be a hybrid approach that combines conventional method with modern approaches. This survey gives readers a basic understanding of automated karyotyping and future direction in this domain.},
  archive      = {J_KIS},
  author       = {Somasundaram, Devaraj and Madian, Nirmala and Goh, Kam Meng and Suresh, S.},
  doi          = {10.1007/s10115-024-02243-y},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {977-1011},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Chromosome segmentation and classification: An updated review},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotions in recommender systems for discrepant-users. <em>KIS</em>, <em>67</em>(1), 953-976. (<a href='https://doi.org/10.1007/s10115-024-02307-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender system (RS) predicts relevant items to a new user based on the purchase history of existing users. In this area, Collaborative Filtering (CF) is one of the popular approaches, uses the user rating data for recommendations. Generally, users express their opinion on the items purchased through user ratings. Considering the user rating data alone may not be sufficient to identify the user interests that are nearer to the user intent. So, there is a need to look into other data provided in the purchase history such as user reviews, user profile, and helpfulness. In this paper, we propose an approach Elaboration Likelihood Model for Discrepant-users (ELM-D) to address the issue of identifying user interest using user rating data and user review data. In this approach, we use elaboration to identify the correct data to reach the user interests nearer to the user intent for discrepant-users. We observed real-world datasets and found the discrepant-users. The discrepant-users are the users who do not provide user ratings and/or reviews correctly. To understand the user interests from user review data, we proposed an approach to extract the user emotions using emotion detection algorithm by exploiting n-polarity. We built a CF approach to predict the interest of a new user using user rating data and user emotions from user review data. We conducted experiments on the real-world datasets from Amazon and Yelp. We evaluated results using mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean square error (RMSE) metrics. The proposed approach outperforms the existing approaches.},
  archive      = {J_KIS},
  author       = {Aramanda, Amarajyothi and Md Abdul, Saifulla and Vedala, Radha},
  doi          = {10.1007/s10115-024-02307-z},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {953-976},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Emotions in recommender systems for discrepant-users},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HGSMAP: A novel heterogeneous graph-based associative percept framework for scenario-based optimal model assignment. <em>KIS</em>, <em>67</em>(1), 915-952. (<a href='https://doi.org/10.1007/s10115-024-02251-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating pervasiveness of big data applications has incited the development of a multitude of models for the same objectives within the identical scenarios and datasets. Existing methods primarily focus on the allocation of tasks to workers, recommendation of items to users, and model selection in a fixed scenario. However, there currently lacks a unified framework for assigning optimal models to different scenarios. Furthermore, integrating heterogeneous information, uncovering their implicit associations, and conducting comprehensive evaluations of models remain significant challenges. To address this, a novel heterogeneous graph-based scenario and model associative percept framework (HGSMAP) is proposed. This framework incorporates a scoring function with three components: graph embedding, feature embedding, and the cross-feature relationship learning network (CFRLN). The graph embedding transforms the heterogeneous graph into lower-dimensional vectors by utilizing the metapath2vec++ method. The feature embedding learns vectors pertaining to model performance, as well as the node features of datasets and models in the graph by exploiting an embedding block. CFRLN integrates feature vectors derived from the feature embedding with their corresponding graph embedding vectors obtained from the graph embedding, extracts explicit and implicit dependencies among heterogeneous data, and employs an attention fusion block to intelligently fuse them. Six popular traffic scenarios are chosen as study cases and extensive experiments are conducted on a dataset to verify the effectiveness and efficiency of HGSMAP and the score function.},
  archive      = {J_KIS},
  author       = {Qiu, Zekun and Xie, Zhipu and Ji, Zehua and Mao, Yuhao and Cheng, Ke},
  doi          = {10.1007/s10115-024-02251-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {915-952},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {HGSMAP: A novel heterogeneous graph-based associative percept framework for scenario-based optimal model assignment},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised re-identification for online joint multi-object tracking. <em>KIS</em>, <em>67</em>(1), 881-914. (<a href='https://doi.org/10.1007/s10115-024-02237-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the bottleneck of multi-object tracking is shifting from detection performance to association performance. However, research on association algorithms requires a large number of identity labels, which are more expensive than detection labels. To circumvent the need for identity labels, we propose a Self-supervised Re-identification module for online joint Multi-Object Tracking (SR-MOT). Specifically, we design an appearance discriminator to judge identities based solely on detection hypotheses and then associate the same identity with the final trajectory. To train the discriminator without using identity labels, we construct negative pairs by the detections that appear in the same video frame, as they definitely belong to different identities. Positive pairs are naturally constructed through several useful data augmentation strategies at the box level. In addition, our proposed method balances conflicting detection and re-ID tasks by using different output features and dynamically adjusts detection and re-ID loss weights based on the information content of the loss distribution to promote balance between the two tasks from the feature level and optimization methods. In our evaluation on the MOT Challenge benchmark, we show that our SR-MOT performs comparably to supervised methods and is significantly superior to other unsupervised methods. Our proposed method provides a practical solution for multi-object tracking without the need for identity labels, making it more accessible for real-world applications.},
  archive      = {J_KIS},
  author       = {Li, Shuman and Yang, Longqi and Tan, Huibin and Wang, Binglin and Huang, Wanrong and Liu, Hengzhu and Yang, Wenjing and Lan, Long},
  doi          = {10.1007/s10115-024-02237-w},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {881-914},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Self-supervised re-identification for online joint multi-object tracking},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of social media on the cryptocurrency markets during the COVID-19 pandemic and the russia-ukraine conflict. <em>KIS</em>, <em>67</em>(1), 863-880. (<a href='https://doi.org/10.1007/s10115-024-02236-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the impact of social media on the profitability of the cryptocurrency market during the COVID-19 pandemic and the Russia-Ukraine conflict. We employ a daily database spanning from April 22, 2018, to April 23, 2023, focusing on the prominent cryptocurrencies: Bitcoin, Ethereum, and Litecoin. Social Media is approximated using Google Trends, Twitter, and You-Tube. To assess the interconnectedness of the cryptocurrency market, we examine the symmetrical volatility of each cryptocurrency using the GARCH model. This allows us to identify assets that exhibit less vulnerability and can serve as safe-haven and hedging instruments. Additionally, we verify the bullish and bearish movements of this market using the EGARCH specification. Finally, we employ a causality test in the sense of Granger (1969) to determine the impact of Social Media on the dynamics of the cryptocurrency market. This test will elucidate whether Social Media data can effectively predict cryptocurrency price movements.},
  archive      = {J_KIS},
  author       = {Mgadmi, Nidhal and Moussa, Wajdi and Mohammedi, Walid and Abidi, Ameni and Wahada, Majdouline},
  doi          = {10.1007/s10115-024-02236-x},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {863-880},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {The impact of social media on the cryptocurrency markets during the COVID-19 pandemic and the russia-ukraine conflict},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep-transfer learning inspired natural language processing system for software requirements classification. <em>KIS</em>, <em>67</em>(1), 839-861. (<a href='https://doi.org/10.1007/s10115-024-02248-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the software engineering domain, the distinction between functional (FRs) and non-functional requirements (NFRs) is paramount, as it directly influences the design and development of software systems. However, several challenges, such as dealing with limited training data, domain-specific datasets, and high computational costs, have driven the need for innovative solutions, particularly those related to classifying functional and non-functional software requirements. The limited availability of labeled data for training deep learning models and their high computational costs have hindered progress. This study proposes a novel hierarchical transfer learning (HTL) approach to address the challenges of limited training data and high computational costs associated with deep learning models. The HTL model leverages transfer learning techniques, incorporating pre-trained models such as global vectors for word representation (GloVe) for text vectorization and a bidirectional long short-term memory (BiLSTM) architecture. By harnessing knowledge from large text corpora and capturing both high-level semantic relationships and detailed syntactic patterns, the HTL model demonstrates enhanced classification performance. We have evaluated the model’s performance using precision, recall, F1-score, and the area under the receiver operating characteristic curve. For FRs classification, we have observed a 26% improvement in precision, a 9% improvement in recall, and an 18% in F1-score for small datasets. Similarly, for NFRs, classification achieves a 20% improvement in precision, a 38.8% improvement in recall, and a 31.8% improvement in F1-score. For large datasets, we have observed a 25% improvement in precision, a 7% improvement in recall, and a 15% improvement in F1-score for FRs classification. For NFRs classification, it achieves a 24% improvement in precision, a 39.8% improvement in recall, and a 41.8% improvement in F1-score. Our study presents a pioneering HTL approach for FRs and NFRs classification, demonstrating superior performance compared to traditional methods. Furthermore, we identify areas for future research, including improving model interpretability, handling data biases, and fine-tuning hyperparameters, which will further enhance the capabilities and applicability of the HTL model.},
  archive      = {J_KIS},
  author       = {Saqib, Mohd and Mustaqeem, Mohd and Jawed, Md Saquib and Abdulaziz, Alsolami and Khan, Anish and Khan, Jeeshan},
  doi          = {10.1007/s10115-024-02248-7},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {839-861},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep-transfer learning inspired natural language processing system for software requirements classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning robust discriminant features via correntropy-induced functions: From supervised to unsupervised learning. <em>KIS</em>, <em>67</em>(1), 811-837. (<a href='https://doi.org/10.1007/s10115-024-02239-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correntropy-induced (C-) loss function has been successfully employed in many classification and clustering problems due to its good properties such as smoothness and insensitivity to noise and outliers. In this article, we embed the C-loss function into linear discriminant analysis and propose a novel discriminant analysis model that can suppress outliers and noise. The proposed model is a nonlinear optimization problem that belongs to a ratio minimization problem. We use Dinkelbach’s extended algorithm involving parametric optimization subproblems to solve the proposed ratio minimization problem. The half-quadratic optimization algorithm is employed to tackle parametric optimization subproblems. In addition, the proposed model is also modified to suit the clustering problem by introducing additional optimization variables, which gives a robust clustering model from the discriminant criterion. A series of experiments on many datasets are performed, and experimental results demonstrate that the proposed model is superior to some existing models in the presence of outliers and noise.},
  archive      = {J_KIS},
  author       = {Liang, Zhizheng},
  doi          = {10.1007/s10115-024-02239-8},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {811-837},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learning robust discriminant features via correntropy-induced functions: From supervised to unsupervised learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LmGa: Combining label mapping method with graph attention network for agricultural recognition. <em>KIS</em>, <em>67</em>(1), 789-810. (<a href='https://doi.org/10.1007/s10115-024-02234-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of counterfeit agricultural products remains a critical aspect in mitigating risks associated with food safety and hygiene. Apart from employing machine learning methods for detecting counterfeit agricultural products, we aim to implement an efficient real-time analysis using graph attention network (GAT), which is highly effective in detecting the falsification of agricultural products within a graph of labels. The process of identifying counterfeit agricultural products is fraught with two significant challenges. The first challenge arises from the lack of cohesion between the samples and the labels, which can create confusion and impede accurate identification. The second challenge pertains to the significant latency in graph queries and inferences made by the GNN model, which can cause delays of up to hundreds of milliseconds. To address these challenges, we propose a model (termed LmGa) that combines label-linked data with GAT to enable an end-to-end neural network learning process. Specifically, our graph construction is based on the relationship between labeled and unlabeled data. We also demonstrate the efficacy of our proposed method across various datasets such as mini-ImageNet, tieredImageNet, and TLU-States, showcasing that our method surpasses other modern approaches in few-shot learning (FSL) combined with GAT. In widely recognized datasets for graph learning, we have surpassed the accuracy of conventional vanilla few-shot learning algorithms by an improvement ranging from 3 to 5%.},
  archive      = {J_KIS},
  author       = {Tran-Anh, Dat and Vu, Hoai Nam and Bui-Quoc, Bao and Dao Hoang, Ngan},
  doi          = {10.1007/s10115-024-02234-z},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {789-810},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {LmGa: Combining label mapping method with graph attention network for agricultural recognition},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An accuracy analysis of classical and quantum-enhanced K-nearest neighbor algorithm using canberra distance metric. <em>KIS</em>, <em>67</em>(1), 767-788. (<a href='https://doi.org/10.1007/s10115-024-02229-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-nearest neighbor (kNN) algorithm is a widely used machine learning technique for classification tasks. Recent advancements in quantum computing have introduced quantum-enhanced kNN (QKNN) algorithms, which offer potential improvements in computational efficiency. This study aims to perform a comprehensive accuracy analysis of classical and quantum-enhanced kNN algorithms using the Canberra distance metric across a variety of datasets. We evaluated the performance of both classical and quantum kNN algorithms on eight diverse datasets: Wine, Breast Cancer, Diabetes, Seeds, Iris, Raisin, Obesity, and Liver. For each dataset, we compared the accuracy of classical kNN and QKNN models using the Canberra distance metric with k-values of 3 and 5. The Canberra distance metric proved to be effective, yielding high accuracy rates for both classical and quantum models. Notably, the classical kNN achieved higher accuracy rates compared to QKNN, with improvements in accuracy observed as the k-value increased from 3 to 5. Among the eight datasets, the breast cancer and Iris datasets perform with high accuracy of 93.85% and 93.33% for quantum-enhanced K-nearest neighbor with $$k = 5$$ using the Canberra distance. At the end our work is able to figure out the important metric and its accuracy level as well. The detailed results and comparisons done are elaborated in our work.},
  archive      = {J_KIS},
  author       = {Bhaskaran, P. and Prasanna, S.},
  doi          = {10.1007/s10115-024-02229-w},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {767-788},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An accuracy analysis of classical and quantum-enhanced K-nearest neighbor algorithm using canberra distance metric},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARP spoofing detection using machine learning classifiers: An experimental study. <em>KIS</em>, <em>67</em>(1), 727-766. (<a href='https://doi.org/10.1007/s10115-024-02219-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent university data breaches highlight the need to protect sensitive information and enhance centralized security systems like Software-Defined Networking and Intrusion Detection Systems by providing timely data for traffic analysis and attack detection. ARP spoofing, which can facilitate Man-in-the-Middle (MITM) attacks, is a key threat responsible for such breaches. Our work focuses on real-time anomaly detection within host-based systems to improve protection against ARP spoofing-based MITM attacks. The existing intrusion detection methods generally exhibit a gap where ML-based methods often overlook network metrics crucial for assessing real-world impact and system performance, while non-ML approaches struggle with adapting to new attack patterns. Our study introduces a dynamic ARP spoofing detection approach that addresses vulnerabilities in victim ARP caches by continuously updating references and verifying source IP and MAC addresses. The algorithm also cross-verifies gateway values to maintain accurate network integrity. Our research optimizes ML classifiers for ARP spoofing detection in host-based networks by selecting features based on expert insights and literature, utilizing a real-time dataset from our institute’s lab with 12 features, including 6 identified as optimal via PCA, to ensure accuracy and relevance in our specific network conditions. Additionally, we evaluated our models across various machine learning classifiers-including K-Nearest Neighbors, Decision Tree, Random Forest, Artificial Neural Network, Deep Neural Network, Convolutional Neural Network, and hybrid classifiers from continual learning approaches-achieving remarkable performance with 99% F1-Score and accuracy during training, and increased 99.26% F1-Score for real-time attack detection using CNN.},
  archive      = {J_KIS},
  author       = {Majumder, Sharmistha and Deb Barma, Mrinal Kanti and Saha, Ashim},
  doi          = {10.1007/s10115-024-02219-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {727-766},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {ARP spoofing detection using machine learning classifiers: An experimental study},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-to-many two-sided matching decision of logistics O2O platform considering the intermediary benefit. <em>KIS</em>, <em>67</em>(1), 689-725. (<a href='https://doi.org/10.1007/s10115-024-02209-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In existing logistics online-to-offline (O2O) platform decision activities, psychological behavioral behaviors of logistics demanders and suppliers have become increasingly complex, and the required information also involves a great deal of uncertainty and vagueness. Hence, a two-sided matching (TSM) method considering multiple psychological behavioral behaviors and the intermediary benefit is proposed. First, a one-to-many supply–demand matching problem on online platform with multiple fuzzy preferences is described. To solve this problem, a novel weight-solving algorithm considering subjectivity and objectivity is developed. Matching utility values are determined according to the prospect theory and attribute weights. Based on desired matching numbers between logistics supply and demand agents and matching utility values, a unilateral expected matching model is constructed. Then, the expected matching matrix is obtained. By using regret theory, agent satisfaction is calculated according to the expected matching matrix. A novel formula considering risk preference coefficients is developed to calculate the intermediary benefit. Moreover, a one-to-many TSM model is established to maximize agent satisfactions of two-sided agents and the intermediary benefit. To obtain a reasonable logistics service matching scheme, a novel max–min algorithm considering fairness is developed. Finally, the rationality, effectiveness and practicality of the proposed method are verified through a one-to-many matching example on a logistics O2O platform.},
  archive      = {J_KIS},
  author       = {Deng, Zhibin and Yuan, Tao and Yue, Qi and Liu, Xiaohua},
  doi          = {10.1007/s10115-024-02209-0},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {689-725},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {One-to-many two-sided matching decision of logistics O2O platform considering the intermediary benefit},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HGCGE: Hyperbolic graph convolutional networks-based knowledge graph embedding for link prediction. <em>KIS</em>, <em>67</em>(1), 661-687. (<a href='https://doi.org/10.1007/s10115-024-02247-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have received widespread attention in the field of knowledge graph embedding (KGE) due to their powerful graph modeling and neighbor information aggregation capabilities. The GCNs-based embedding methods typically use GCNs to aggregate neighbor entities or relationship information in Euclidean space to enrich the representation of entities and provide initial embeddings for representation learning in KGE methods. However, with the increase of the GCNs convolution layer, the iterative information propagation and feature aggregation may make the entity embeddings in KG become similar, resulting in the inability of the KGE method to distinguish different entities effectively. In addition, embedding hierarchical graph data in Euclidean space will also bring significant distortion. These problems seriously affect the embedded performance of GCNs models. To address these challenges, we propose a hyperbolic graph convolutional network-based knowledge graph embedding (HGCGE) method. Specifically, GCNs convolution operation in hyperbolic space is used to aggregate the adjacent information of entities and capture the deeper relationship between entities, Möbius multiplication and addition are used to embed the entities and relationships to Poincare sphere model to solve the problem of high distortion of embedded representation, and the scoring function based on hyperbolic geometry is designed to increase the attention to entity relationships so as to efficiently identify entities under different relationships. Extensive experiments on four real KG datasets demonstrate that our method outperforms the current strong baseline models on multiple metrics, effectively enhancing the performance of KG embeddings, and it still achieves significant improvements in performance even at low dimensions and low training rounds.},
  archive      = {J_KIS},
  author       = {Bao, Liming and Wang, Yan and Song, Xiaoyu and Sun, Tao},
  doi          = {10.1007/s10115-024-02247-8},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {661-687},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {HGCGE: Hyperbolic graph convolutional networks-based knowledge graph embedding for link prediction},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph partitioning-based hybrid feature selection method in microarray datasets. <em>KIS</em>, <em>67</em>(1), 633-660. (<a href='https://doi.org/10.1007/s10115-024-02292-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection depicts one of the foremost methodologies in dimensionality reduction, with its primary objective being the extraction of pertinent features from an extensive dataset. Its process is driven by two principal objectives: reducing the feature count while simultaneously enhancing classification performance. Furthermore, graph mining techniques enrich the feature selection path by uncovering hidden correlations and facilitating the discovery of the more informative and efficient variables, ultimately improving the classification accuracy of machine learning models. Our research paper introduces an innovative hybrid approach based on graph partitioning and TOPSIS to identify relevant features in high-dimensional datasets. The suggested algorithm, known as Mutual Information Decomposition based on TOPSIS (MIDBT algorithm), can be described in two steps. The initial step, filter phase, aims to eliminate features that do not align with the target. At this stage, we apply the mrMR filter approach to screen out the K-best features from a given dataset, allowing the user to define the value of K. As a second step, we propose an innovative procedure based on graph mining, introducing a new weight between vertices that combines relevance and redundancy. To pinpoint the optimal subset of features, we conceptualize the task as a multi-criteria decision problem by the help of the TOPSIS method, which comprises two objectives to maximize and one objective to minimize simultaneously: modularity, accuracy and diameter. In the final step, applying forward selection algorithm to the optimal subset results in the best features with high classification performance and fewest variables possible. To substantiate and evaluate the efficiency of the hybrid approach (MIDBT algorithm), we benchmark our methodology against several advanced feature selection techniques, using 10 datasets characterized by high dimensionality. The effectiveness of the MIDBT method is validated by experimental results.},
  archive      = {J_KIS},
  author       = {Oubaouzine, Abdelali and Ouaderhman, Tayeb and Chamlal, Hasna},
  doi          = {10.1007/s10115-024-02292-3},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {633-660},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A graph partitioning-based hybrid feature selection method in microarray datasets},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient word segmentation for enhancing chinese spelling check in pre-trained language model. <em>KIS</em>, <em>67</em>(1), 603-632. (<a href='https://doi.org/10.1007/s10115-024-02230-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In existing pre-trained language models, Chinese spelling check (CSC) often considers phonetic and graphic details at the character level, and ignores the essential role of word segmentation. To address this issue, an efficient word segmentation is studied for CSC in this paper, referred to Word Segmentation-Enhanced Speller (WOSES). The WOSES comprises two distinct models, the Word Speller (WSpeller) and the Hierarchical Word Speller (H-WSpeller), designed both for mitigating the often-ignored word boundary errors in CSC. The WOSES framework outperforms existing benchmarks on standard datasets SIGHAN13 and SIGHAN15, attributed to its innovative use of word segmentation and the specialized pre-trained model, W-MLM. Notably, the WSpeller model within the WOSES framework achieves F1 score improvements of 3.3 and 2.1% on SIGHAN13 and SIGHAN15, respectively, compared to existing methods. In this paper, the importance of word segmentation is not only underscored in CSC, but also a novel performance standard is proposed in the domain.},
  archive      = {J_KIS},
  author       = {Li, Fangfang and Jiang, Jie and Tang, Dafu and Shan, Youran and Duan, Junwen and Zhang, Shichao},
  doi          = {10.1007/s10115-024-02230-3},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {603-632},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient word segmentation for enhancing chinese spelling check in pre-trained language model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain recommendation via adaptive bi-directional transfer graph neural networks. <em>KIS</em>, <em>67</em>(1), 579-602. (<a href='https://doi.org/10.1007/s10115-024-02246-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sparsity and the cold start problem significantly impede the advancement of recommendation systems. Cross-domain recommendation (CDR) seeks to alleviate these issues by utilizing knowledge from other domains. One important research direction in CDR is how to effectively transfer knowledge across different domains. Existing methods assume that knowledge between different domains should be transferred among the same users across domains. However, a user may share similar interests with multiple distinct users in other domains, which means that the domain knowledge from one domain should not only be transferred to the same user in another domain but should also be extended to other users with similar interests in that domain. Additionally, a user may experience a phenomenon of interest drift across different domains. The prior assumption often hampers the effective dissemination of domain knowledge, thus limiting the performance of CDR. Therefore we propose AbtCDR, an innovative method that enables adaptive knowledge transfer among different users in different domains. Firstly, we harness the power of graph neural networks to distill user and item embeddings across different domains. Secondly, we define a novel cross-domain user interest transport scheme, meticulously designed to bridge the gap between distinct domains. To further navigate the complex web of user interests, we have proposed two knowledge transfer methods: a coarse-grained method for broad-strokes insight into user interests, and a fine-grained method that delves into the intricate details of user interests. Extensive experiments demonstrate our method outperforms state-of-the-art baselines on real-world datasets. Our code and dataset are available at https://github.com/jujingxin/AbtCDR .},
  archive      = {J_KIS},
  author       = {Zhao, Yi and Ju, Jingxin and Gong, Jibing and Zhao, Jinye and Chen, Mengpan and Chen, Le and Feng, Xinchao and Peng, Jiquan},
  doi          = {10.1007/s10115-024-02246-9},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {579-602},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cross-domain recommendation via adaptive bi-directional transfer graph neural networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preserving data distribution in sampling and instance selection with renyi’s divergence. <em>KIS</em>, <em>67</em>(1), 549-578. (<a href='https://doi.org/10.1007/s10115-024-02302-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel method for sampling based on a distribution function. By utilizing Renyi’s divergence criterion, a recursive formulation is derived directly from the difference between the original and estimated distributions. This recursive equation is solved using the gradient descent algorithm, from which new samples are then generated. Because this method relies on the original data’s distribution, it effectively preserves the data distribution in the sampled dataset. When the original distribution is unknown, kernel density estimation is used for approximation. Experimental results show that the proposed method successfully maintains the data distribution and concept integrity, while also preserving the model’s predictability on selected instances. Specifically, this method has managed to reduce the dataset size by approximately $$70\%$$ , without compromising the accuracy of the learning algorithm.},
  archive      = {J_KIS},
  author       = {Sadoghi-Yazdi, Hadi and Ashkezari-Toussi, Soheila and Ramezanzadeh-Yazdi, Abolfazl},
  doi          = {10.1007/s10115-024-02302-4},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {549-578},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Preserving data distribution in sampling and instance selection with renyi’s divergence},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probability knowledge acquisition from unlabeled instance based on dual learning. <em>KIS</em>, <em>67</em>(1), 521-547. (<a href='https://doi.org/10.1007/s10115-024-02238-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The functionality of machine learning algorithms heavily relies on the abundance and quality of training data accessible. However, the data may originate from diverse data subspaces, making labeled training set typically offers only limited insights, inadequately representing the entirety of potential scenarios. How to safely make use of the unlabeled instances is an emerging and interesting problem for learning Bayesian network classifiers (BNCs), which graphically model the probabilistic relationships among variables in the form of directed acyclic graph (DAG). In this paper, we introduce dual learning into the learning procedure to realize the safe exploitation of the unlabeled instances. We elucidate the mapping between information metric and the local DAG, as well as the distinction between informational (in)dependence and probabilistic (in)dependence. Building upon this foundation, we propose new metrics to accurately measure attribute dependencies within unlabeled instances. The proposed dual learning-based flexible selective k-dependence Bayesian network classifier (DL-FSKDB) employs eager learning to construct the initial model and incorporates lazy learning for personalized fine-tuning and optimization. The extensive experimental evaluations across 36 datasets spanning various domains with distinct properties reveal that the learned BNCs demonstrate competitive classification performance in comparison with state-of-the-art learners in terms of zero–one loss, bias and variance, as well as F1-measure.},
  archive      = {J_KIS},
  author       = {Zhao, Yuetan and Wang, Limin and Zhu, Xinyu and Jin, Taosheng and Sun, Minghui and Li, Xiongfei},
  doi          = {10.1007/s10115-024-02238-9},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {521-547},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Probability knowledge acquisition from unlabeled instance based on dual learning},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learned index for non-key queries. <em>KIS</em>, <em>67</em>(1), 497-519. (<a href='https://doi.org/10.1007/s10115-024-02233-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned indexes have attracted a lot of interest lately due to their superior performance over conventional indexes. When there is a lot of data traffic, the learned index efficiently addresses the issue of the standard index’s large memory usage. In this paper, we concentrate on a well-known learned index, the recursive model index (RMI). Since the machine learning model is unbiased while calculating, when there are too many non-key queried, the model will calculate the position of the key as if it were positive key, which wastes a lot of time on unnecessary calculations. To deal with this condition, we propose a hierarchical learned index structure based on Bloom filter named HBFdex. HBFdex can effectively prune non-keys, which means most non-key return in layer of BF before they get to machine learning model. By lowering the number of layers traversed by non-key and the time spent looking for non-key within the error bound that is provided by machine learning model, HBFdex decreases the average query time of learned index. We compare HBFdex with B-Tree and RMI, and the results prove that our new structure optimizes the performance of RMI in the case of non-key queries.},
  archive      = {J_KIS},
  author       = {Zhu, Rui and Wang, Hongzhi and Xia, Sheng and Zheng, Bo},
  doi          = {10.1007/s10115-024-02233-0},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {497-519},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learned index for non-key queries},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid bat and grey wolf optimizer for gene selection in cancer classification. <em>KIS</em>, <em>67</em>(1), 455-495. (<a href='https://doi.org/10.1007/s10115-024-02225-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA microarray is a technique in which a chip containing numerous DNA codes is used for the expression estimation of an extensive number of genes simultaneously. These genes are arranged in a table or data format. The gene expression data can be employed in pattern recognition algorithms to differentiate between samples obtained from healthy individuals and those with cancer. However, recognizing biomarkers’ patterns from gene selection data is considered challenging because of its huge dimensionality and the presence of noisy, irrelevant, and unwanted genes, leading to mislearning process and, thus, declining in the classification performance. Therefore, in this paper, an intelligent gene selection approach is proposed on the basis of robust minimum redundancy maximum relevancy as the filter and hybrid improved bat algorithm (BA) with grey wolf optimizer (GWO) (BA-GWO). The BA-GWO is introduced to determinate a limited number of biomarker genes that significantly enhance the classification performance. In this approach, the k-nearest neighbor algorithm was employed for the classification task. The proposed BA-GWO is mainly introduced to improve the BA search agents’ performance in searching for the best candidate gene subset that carries the biomarkers for cancer classification. Furthermore, the BA-GWO is designed to enhance both exploitation and exploration capabilities while ensuring a balanced approach and preventing stagnation in local optima. The primary function of this proposed approach is to enhance the solutions acquired through the BA by utilizing them as the initial population for the GWO. The proposed approach is evaluated using ten widely recognized microarray datasets in the experimental stage, including CNS, Colon, Leukemia 3c, Leukemia 4c, Leukemia, Lung Cancer, Lymphoma, MLL, Ovarian, and SRBCT. The performance of the hybridization of BA and GWO, as well as recent and base optimization algorithms, is evaluated. Afterward, the hybrid versions are compared with their individual optimization algorithms. Moreover, the hybridization algorithms are compared with each other. For further validation, the proposed approach performance is compared with twelve state-of-the-art comparative methods in terms of accuracy and the selected genes. The findings indicate that the proposed approach yields superior outcomes in two out of eight datasets, while also delivering highly competitive results in the remaining datasets.},
  archive      = {J_KIS},
  author       = {Tbaishat, Dina and Tubishat, Mohammad and Makhadmeh, Sharif Naser and Alomari, Osama Ahmad},
  doi          = {10.1007/s10115-024-02225-0},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {455-495},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A hybrid bat and grey wolf optimizer for gene selection in cancer classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional network-based relational triple extraction with prior relation mechanism. <em>KIS</em>, <em>67</em>(1), 427-453. (<a href='https://doi.org/10.1007/s10115-024-02241-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relational triple extraction is a fundamental task for extracting semantic information from unstructured text, which is also the cornerstone for constructing large-scale knowledge graphs. In recent years, the relational triple extraction task has achieved significant gains. However, some challenges still need to be addressed in practical applications, such as relational prediction redundancy, error accumulation, and overlapping triples. To address these challenges, we propose a novel bidirectional network with a prior relation mechanism named BPRM in this paper. Our model consists of a prior relational mechanism, a bidirectional network, and a relation-based alignment strategy. Specifically, our model first mines potential relations by the prior relation mechanism, which reduces redundant relations and discovers implied relations. Then, the bidirectional network fuses prior relation features to tag the subject and the object from two opposite directions, which can address the error accumulation and the overlapping triple problem. Moreover, extracting subjects and objects from both directions can reduce the impact of accumulation errors. Finally, the relation-based alignment strategy removes redundant entity pairs. Our proposed model gains the highest F1-scores of 92.7% and 92.4% on two public datasets NYT and WebNLG, respectively. Also, in experiments for complex scenarios, our model obtains consistent performance gain on overlapping triples and multiple triples patterns compared to all baselines.},
  archive      = {J_KIS},
  author       = {Xiao, Youzi and Zheng, Shuai and Tian, Zhiqiang and Shi, Jiancheng and Du, Xiaodong and Hong, Jun},
  doi          = {10.1007/s10115-024-02241-0},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {427-453},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Bidirectional network-based relational triple extraction with prior relation mechanism},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal sarcasm detection using ensemble net model. <em>KIS</em>, <em>67</em>(1), 403-425. (<a href='https://doi.org/10.1007/s10115-024-02227-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, sarcasm is expressed via various verbal and non-verbal words. Various existing works on the detection of sarcasm have been performed in either text or video. With the rapid growth of social media and internet technology, people express their emotions and feelings using text. Therefore, a multi-modal sarcasm detection task is crucial to understanding people’s real feelings and beliefs. However, it is still a challenge to detect sarcasm from multi-modal features. Therefore, this work presents a new hybrid ensemble deep learning approach for multi-modal sarcasm detection. The major goal of this research is to determine the different classes of sarcasm using a multi-modal dataset. Here, imaging modality-based sarcasm detection is performed using Deep Residual Net, and the visual features are extracted. For the generation of text modality, the text data are pre-processed with punctuation removal, and the textual features are extracted using Term Frequency-Inverse Average Document Frequency. The extracted features are used as input for the bidirectional long short-term memory model. The audio (acoustic) elements are extracted to form acoustic modality, which is subsequently sent to the visual geometry group. Furthermore, the weighted fusion modality process is used to combine all of the collected features. The softmax layer acts as the classification layer for performing multi-modal sarcasm detection. Here, the Tent chaotic snack optimization algorithm is employed to tune the hyperparameter and reduce the complexity of the proposed Hybrid EnsembleNet. PYTHON tool is used to evaluate the performance of the proposed classifier. The proposed hybrid EnsembleNet is trained using two datasets: Memotion 7k and MUStARD.},
  archive      = {J_KIS},
  author       = {Sukhavasi, Vidyullatha and Dondeti, Venkatesulu},
  doi          = {10.1007/s10115-024-02227-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {403-425},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-modal sarcasm detection using ensemble net model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new hybrid reasoning model based on rules, cases and processes: Application to care of individuals facing autism spectrum disorders. <em>KIS</em>, <em>67</em>(1), 371-401. (<a href='https://doi.org/10.1007/s10115-024-02228-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining Rule-based reasoning and Case-based reasoning has been widely used, exhibiting quite successful results, since they have complementary capabilities. A system that can utilize both approaches could potentially take advantage of the positive aspects of both while minimizing their negative aspects. In this paper, we propose a hybrid reasoner which combines Rule based reasoning, Case-based reasoning, as well as Process Mining. To our knowledge, this work is the first one that incorporates process modeling in a hybrid reasoner. Process Mining is used in order to take into consideration possible future facts, which are justified by the current ones through a process model extracted by the cases themselves. Each case is considered as an instance of a process and a process model is extracted by applying process mining techniques. We also present PAVEFS, an intelligent system based on the proposed hybrid reasoner, supporting personalized provision of services for individuals facing autism spectrum disorders.},
  archive      = {J_KIS},
  author       = {Kaoura, Georgia and Kovas, Konstantinos and Boutsinas, Basilis and Hatzilygeroudis, Ioannis},
  doi          = {10.1007/s10115-024-02228-x},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {371-401},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new hybrid reasoning model based on rules, cases and processes: Application to care of individuals facing autism spectrum disorders},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic evolution of causal relationships among cryptocurrencies: An analysis via bayesian networks. <em>KIS</em>, <em>67</em>(1), 355-370. (<a href='https://doi.org/10.1007/s10115-024-02222-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the relationships between cryptocurrencies is important for making informed investment decisions in this financial market. Our study utilises Bayesian networks to examine the causal interrelationships among six major cryptocurrencies: Bitcoin, Binance Coin, Ethereum, Litecoin, Ripple, and Tether. Beyond understanding the connectedness, we also investigate whether these relationships evolve over time. This understanding is crucial for developing profitable investment strategies and forecasting methods. Therefore, we introduce an approach to investigate the dynamic nature of these relationships. Our observations reveal that Tether, a stablecoin, behaves distinctly compared to mining-based cryptocurrencies and stands isolated from the others. Furthermore, our findings indicate that Bitcoin and Ethereum significantly influence the price fluctuations of the other coins, except for Tether. This highlights their key roles in the cryptocurrency ecosystem. Additionally, we conduct diagnostic analyses on constructed Bayesian networks, emphasising that cryptocurrencies generally follow the same market direction as extra evidence for interconnectedness. Moreover, our approach reveals the dynamic and evolving nature of these relationships over time, offering insights into the ever-changing dynamics of the cryptocurrency market.},
  archive      = {J_KIS},
  author       = {Amirzadeh, Rasoul and Thiruvady, Dhananjay and Nazari, Asef and Ee, Mong Shan},
  doi          = {10.1007/s10115-024-02222-3},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {355-370},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamic evolution of causal relationships among cryptocurrencies: An analysis via bayesian networks},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep multi-semantic fuzzy K-means with adaptive weight adjustment. <em>KIS</em>, <em>67</em>(1), 325-353. (<a href='https://doi.org/10.1007/s10115-024-02221-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep fuzzy clustering methods employ deep neural networks to extract high-level feature embeddings from data, thereby enhancing subsequent clustering and achieving superior performance compared to traditional methods. However, solely relying on feature embeddings may cause clustering models to ignore detailed information within data. To address this issue, this paper designs a deep multi-semantic fuzzy K-means (DMFKM) model. Our method harnesses the semantic complementarity of various kinds of features within autoencoder to improve clustering performance. Additionally, to fully exploit the contribution of different types of features to each cluster, we propose an adaptive weight adjustment mechanism to dynamically calculate the importance of different features during clustering. To validate the effectiveness of the proposed method, we applied it to six benchmark datasets. DMFKM significantly outperforms the prevailing fuzzy clustering techniques across different evaluation metrics. Specifically, on the six benchmark datasets, our method achieves notable gains over the second-best comparison method, with an ACC improvement of approximately 2.42%, a Purity boost of around 1.94%, and an NMI enhancement of roughly 0.65%.},
  archive      = {J_KIS},
  author       = {Wang, Xiaodong and Hong, Longfu and Yan, Fei and Wang, Jiayu and Zeng, Zhiqiang},
  doi          = {10.1007/s10115-024-02221-4},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {325-353},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep multi-semantic fuzzy K-means with adaptive weight adjustment},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class incremental named entity recognition without forgetting. <em>KIS</em>, <em>67</em>(1), 301-324. (<a href='https://doi.org/10.1007/s10115-024-02220-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class Incremental Named Entity Recognition (CINER) needs to learn new entity classes without forgetting old entity classes under the setting where the data only contain annotations for new entity classes. As is well known, the forgetting problem is the biggest challenge in Class Incremental Learning (CIL). In the CINER scenario, the unlabeled old class entities will further aggravate the forgetting problem. The current CINER method based on a single model cannot completely avoid the forgetting problem and is sensitive to the learning order of entity classes. To this end, we propose a Multi-Model (MM) framework that trains a new model for each incremental step and uses all the models for inference. In MM, each model only needs to learn the entity classes included in corresponding step, so MM has no forgetting problem and is robust to the different entity class learning orders. Furthermore, we design an error-correction training strategy and conflict-handling rules for MM to further improve performance. We evaluate MM on CoNLL-03 and OntoNotes-V5, and the experimental results show that our framework outperforms the current state-of-the-art (SOTA) methods by a large margin.},
  archive      = {J_KIS},
  author       = {Liu, Ye and Huang, Shaobin and Wei, Chi and Tian, Sicheng and Li, Rongsheng and Yan, Naiyu and Du, Zhijuan},
  doi          = {10.1007/s10115-024-02220-5},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {301-324},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Class incremental named entity recognition without forgetting},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral clustering with scale fairness constraints. <em>KIS</em>, <em>67</em>(1), 273-300. (<a href='https://doi.org/10.1007/s10115-024-02183-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is one of the most common unsupervised learning algorithms in machine learning and plays an important role in data science. Fair spectral clustering has also become a hot topic with the extensive research on fair machine learning in recent years. Current iterations of fair spectral clustering methods are based on the concepts of group and individual fairness. These concepts act as mechanisms to mitigate decision bias, particularly for individuals with analogous characteristics and groups that are considered to be sensitive. Existing algorithms in fair spectral clustering have made progress in redistributing resources during clustering to mitigate inequities for certain individuals or subgroups. However, these algorithms still suffer from an unresolved problem at the global level: the resulting clusters tend to be oversized and undersized. To this end, the first original research on scale fairness is presented, aiming to explore how to enhance scale fairness in spectral clustering. We define it as a cluster attribution problem for uncertain data points and introduce entropy to enhance scale fairness. We measure the scale fairness of clustering by designing two statistical metrics. In addition, two scale fair spectral clustering algorithms are proposed, the entropy weighted spectral clustering (EWSC) and the scale fair spectral clustering (SFSC). We have experimentally verified on several publicly available real datasets of different sizes that EWSC and SFSC have excellent scale fairness performance, along with comparable clustering effects.},
  archive      = {J_KIS},
  author       = {Yang, Zhijing and Zhang, Hui and Yang, Chunming and Li, Bo and Zhao, Xujian and Long, Yin},
  doi          = {10.1007/s10115-024-02183-7},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {273-300},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Spectral clustering with scale fairness constraints},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised kernel-based multi-modal bhattacharya distance learning for imbalanced data classification. <em>KIS</em>, <em>67</em>(1), 247-272. (<a href='https://doi.org/10.1007/s10115-024-02223-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned distance metrics measure the difference of the data according to the intrinsic properties of the data points and classes. Distance metric learning approaches are typically used to linearly distinguish the samples of different classes and do not perform well on real-world nonlinear data classes. A kernel-based nonlinear distance metric learning approach is proposed in this article which exploits the density of multimodal classes to properly differentiate the classes while reducing the within-class separation. Here, multimodality refers to the disjoint distribution of a class, resulting in each class having multiple density components. In the proposed kernel density-based distance metric learning approach, kernel trick is applied on the original data and maps the data to a higher-dimensional space. Then, given the possibility of multimodal classes, a mixture of multivariate Gaussian densities is considered for the distribution of each class. The number of components is calculated using a density-based clustering approach, and then the parameters of the Gaussian components are estimated using maximum a posteriori density estimation. Then, an iterative method is used to maximize the Bhattacharya distance among the classes' Gaussian mixtures. The distance among the external components is increased, while the distance among samples of each component is decreased to provide a wide between-class margin. The results of the experiments show that using the proposed approach significantly improves the efficiency of the simple K nearest neighbor algorithm on the imbalanced data set, but when the imbalance ratio is very high, the kernel function does not have a significant effect on the efficiency of the distance metric.},
  archive      = {J_KIS},
  author       = {Jalali Mojahed, Atena and Moattar, Mohammad Hossein and Ghaffari, Hamidreza},
  doi          = {10.1007/s10115-024-02223-2},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {247-272},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Supervised kernel-based multi-modal bhattacharya distance learning for imbalanced data classification},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long short-term search session-based document re-ranking model. <em>KIS</em>, <em>67</em>(1), 223-245. (<a href='https://doi.org/10.1007/s10115-024-02205-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document re-ranking is a core task in session search. However, most existing methods only focus on the short-term session and ignore the long-term history sessions. This leads to inadequate understanding of the user’s search intent, which affects the performance of model re-ranking. At the same time, these methods have weaker capability in understanding user queries. In this paper, we propose a long short-term search session-based re-ranking model (LSSRM). Firstly, we utilize the BERT model to predict the topic relevance between the query and candidate documents, in order to improve the model’s understanding of user queries. Secondly, we initialize the reading vector with topic relevance and use the personalized memory encoder module to model the user’s long-term search intent. Thirdly, we input the user’s current session interaction sequence into Transformer to obtain the vector representation of the user’s short-term search intent. Finally, the user’s search intent and topical relevance information are hierarchically fused to obtain the final document ranking scores. Then re-rank the documents according to this score. We conduct extensive experiments on two real-world session datasets. The experimental results show that our method outperforms the baseline models for the document re-ranking task.},
  archive      = {J_KIS},
  author       = {Liu, Jianping and Wang, Meng and Wang, Jian and Wang, Yingfei and Chu, Xintao},
  doi          = {10.1007/s10115-024-02205-4},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {223-245},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Long short-term search session-based document re-ranking model},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Targeted training for numerical reasoning with large language models. <em>KIS</em>, <em>67</em>(1), 197-221. (<a href='https://doi.org/10.1007/s10115-024-02216-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After recent gains achieved by large language models (LLMs) on numerical reasoning tasks, it has become of interest to have LLMs teach small models to improve on numerical reasoning. Instructing LLMs to generate Chains of Thought to fine-tune small models is an established approach. However, small models are passive in this line of work and may not be able to exploit the provided training data. In this paper, we propose a novel targeted training strategy to match LLM’s assistance with small models’ capacities. The small model will proactively request LLM’s assistance when it sifts out confusing training data. Then, LLM refines such data by successively revising reasoning steps and reducing question complexity before feeding the small model. Experiments show that this targeted training approach remarkably improves the performance of small models on a range of numerical reasoning datasets by 12–25%, making small models even competitive with some LLMs.},
  archive      = {J_KIS},
  author       = {Li, Xiao and Liu, Sichen and Zhu, Yin and Cheng, Gong},
  doi          = {10.1007/s10115-024-02216-1},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {197-221},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Targeted training for numerical reasoning with large language models},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building a comprehensive and multi-dimensional information security ontology: Elicitation process and OWL implementation. <em>KIS</em>, <em>67</em>(1), 167-195. (<a href='https://doi.org/10.1007/s10115-024-02308-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology is an important tool that provides a full representation of domain knowledge. To build ontology from heterogeneous data sources, a comprehensive process is required to extract concepts with their relationships and then conveniently visualize them. In this paper, an ontology construction process is proposed to automatically build a Multi-Dimensional Information Security Ontology, named MDISOnt. The proposed process includes three main stages. The first is the ontology design that gives the hierarchical representation of security concepts. The second starts from ISO/IEC 27000 standard document and enriches the ontology obtained in stage 1 with semantic and synonym relationships. The third consists of the implementation of the obtained ontology by highlighting the security dimensional views and ontology modules. The obtained ontology, which we named MDISOnt helps security specialists and decision-makers remove the ambiguous understanding of the information security domain, decompose security into several perspectives using dimensional views and modules, and efficiently manage information security management in an organization. Compared to the prominent OWL InfoSec ontologies available in the literature, the comparative study demonstrates the outperformance of MDISOnt in terms of accuracy, understandability, and cohesion.},
  archive      = {J_KIS},
  author       = {Meriah, Ines and Ben Arfa Rabai, Latifa and Khedri, Ridha},
  doi          = {10.1007/s10115-024-02308-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {167-195},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Building a comprehensive and multi-dimensional information security ontology: Elicitation process and OWL implementation},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on polyadic chatbots: Trends, challenges, and future research directions. <em>KIS</em>, <em>67</em>(1), 109-165. (<a href='https://doi.org/10.1007/s10115-024-02287-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polyadic chatbots, conversational agents created explicitly for multiparty interactions, are gaining appeal as game-changing technologies in human-AI engagement. Unlike dyadic chatbots, which are designed for one-on-one conversations, there is a lack of studies examining the recent research on polyadic chatbots. This research investigates the roles, design concepts, empirical data, and issues connected with polyadic chatbots. Based on a comprehensive review of 57 studies, this study concludes that chatbots are widely employed for general purposes, particularly to improve group dynamics, decision making, and brainstorming. Polyadic chatbots are used for education, tourism, and social media. While text-based polyadic chatbots are prevalent, there is an increasing trend toward embodied chatbots that use nonverbal cues. Polyadic chatbots serve primarily as discussion facilitators, mediators, collaborators, emotional support, and social companions. They help to address key communication challenges related to improving collaboration and balancing participation. Only a few studies have used theoretical frameworks to create polyadic chatbots, focusing on idea creation, conversation management, and social support. Experimentation is the key approach for testing these chatbots, demonstrating their efficiency in various tasks while also identifying drawbacks such as limited problem-solving ability, user acceptance, and technological limits. Moving ahead, research should focus on improving polyadic chatbots' adaptability in social contexts, understanding the importance of nonverbal clues, and matching their design with user experience theories and cultural sensitivities. Addressing ethical issues regarding transparency, prejudice, and privacy is critical to ensuring responsible and equitable deployment across various interaction settings.},
  archive      = {J_KIS},
  author       = {Kuhail, Mohammad Amin and Taj, Imran and Alimamy, Saifeddin and Abu Shawar, Bayan},
  doi          = {10.1007/s10115-024-02287-0},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {109-165},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A review on polyadic chatbots: Trends, challenges, and future research directions},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive bibliometric analysis on social network anonymization: Current approaches and future directions. <em>KIS</em>, <em>67</em>(1), 29-108. (<a href='https://doi.org/10.1007/s10115-024-02289-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, social network anonymization has become a crucial research field due to its pivotal role in preserving users' privacy. However, the high diversity of approaches introduced in relevant studies poses a challenge to gaining a profound understanding of the field. In response to this, the current study presents an exhaustive and well-structured bibliometric analysis of the social network anonymization field. To begin our research, related studies from the period of 2007–2022 were collected from the Scopus Database and then preprocessed. Following this, the VOSviewer was used to visualize the network of authors’ keywords. Subsequently, extensive statistical and network analyses were performed to identify the most prominent keywords and trending topics. Additionally, the application of co-word analysis through SciMAT and the Alluvial diagram allowed us to explore the themes of social network anonymization and scrutinize their evolution over time. These analyses culminated in an innovative taxonomy of the existing approaches and anticipation of potential trends in this domain. To the best of our knowledge, this is the first bibliometric analysis in the social network anonymization field, which offers a deeper understanding of the current state and an insightful roadmap for future research in this domain.},
  archive      = {J_KIS},
  author       = {Yazdanjue, Navid and Yazdanjouei, Hossein and Gharoun, Hassan and Khorshidi, Mohammad Sadegh and Rakhshaninejad, Morteza and Amiri, Babak and Gandomi, Amir H.},
  doi          = {10.1007/s10115-024-02289-y},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {29-108},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A comprehensive bibliometric analysis on social network anonymization: Current approaches and future directions},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An overview of aggregation methods for social networks analysis. <em>KIS</em>, <em>67</em>(1), 1-28. (<a href='https://doi.org/10.1007/s10115-024-02296-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating systems that can interpret and manage the ambiguity and subjectivity of the representation and retrieval of information is one of the issues facing information systems researchers. With the emergence of social networks, improvement methods have been developed in both traditional and social information research while taking into account the specificity of the information. The most main features of language that directly impact the results of information systems are ambiguity and uncertainty. We show through this article several approaches have been applied in order to take into account the ambiguity of language, especially in social networks. We thus notice a tendency to apply a variety of aggregation tools in order to overcome the weaknesses of social information retrieval systems. In what follows, we will give an overview on other levels of aggregation allowing to solve certain problems of social information analysis such as credibility evaluation, profile categorization, opinion mining, influencer detection, etc. Then, we held a discussion on the ability of uncertainty theory to consider the different degrees of feature importance as well as the heterogeneity of information resources.},
  archive      = {J_KIS},
  author       = {Hlaoua, Lobna},
  doi          = {10.1007/s10115-024-02296-z},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An overview of aggregation methods for social networks analysis},
  volume       = {67},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
