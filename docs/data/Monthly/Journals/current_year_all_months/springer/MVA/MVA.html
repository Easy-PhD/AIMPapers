<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva">MVA - 118</h2>
<ul>
<li><details>
<summary>
(2025). ConcatPhys: A dual-channel path data concatenation network for robust remote heart rate estimation. <em>MVA</em>, <em>36</em>(6), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01737-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial video-based Blood Volume Pulse (BVP) signal extraction technology has demonstrated significant potential in remote health monitoring. However, most current methods are susceptible to interference from lighting changes and have limited generalization ability in dynamic or complex environments. This paper proposes a dual-channel path data concatenation network called ConcatPhys to improve the accuracy and robustness of remote heart rate (HR) estimation. First, a region-focus (RF) block is introduced to concentrate on spatial attention mechanisms, focusing on physiologically relevant regions. This approach effectively uncovers subtle local feature changes and suppresses irrelevant features, reducing sensitivity to background noise and lighting variations. Second, a dual-path framework is constructed for remote photoplethysmography (rPPG) signal prediction. By incorporating dual-path frequency-domain consistency loss and adjacent-frame similarity loss, the network’s anti-interference capability against illumination variations such as lighting changes is strengthened. Finally, leveraging the temporal correlation between adjacent video frames over short intervals, three consecutive feature image segments are concatenated. By averaging the HR values of these three adjacent segments, the video-level HR is computed. This approach enables efficient reconstruction of rPPG signals and accurate HR estimation using only a 6-s facial video segment. Experimental results demonstrate that ConcatPhys achieves state-of-the-art performance across multiple public datasets (VIPL-HR, OBF, UBFC-rPPG), highlighting its significant potential for remote health monitoring applications.},
  archive      = {J_MVA},
  author       = {Ge, Xiaorui and Xing, Jiahe and Li, Bin},
  doi          = {10.1007/s00138-025-01737-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ConcatPhys: A dual-channel path data concatenation network for robust remote heart rate estimation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the potential of deep learning techniques for analyzing athlete movements in competitive athletics sports. <em>MVA</em>, <em>36</em>(6), 1-20. (<a href='https://doi.org/10.1007/s00138-025-01733-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research work, a Deep Learning (DL) approach utilizing Spatial Transformer, Temporal Transformer, and Collaborative Movement Centric (CMC) module is presented to classify sports event based on athlete movements in competitive sports. Primarily, the Spatial Transformer utilizing the Spatial Feature extraction module (SFEM) is introduced to extract detailed spatial information from video frames. Here, the SFEM employs Modulated Moving Average Graph Convolutional Network (MMA-GCN) to extract complex spatial relationships by learning the knowledge from offset and modulation parameters. Secondly, the Temporal Transformer is designed that employs Temporal Feature extraction module (TFEM) module to extract long-range temporal dependencies and model evolution across consecutive frames. Lastly, the CMC module combines spatial and temporal information into a unified representation which is used to perform sports events categorization based on athlete movements. The proposed model is evaluated on Olympic sports dataset and University of Central Florida’s (UCF) Sports dataset across distinct evaluation measures. The model achieved 98.36% accuracy, 99.42% precision, 98.42% recall, 98.91% F1-score on the Olympic Sports dataset and 98.64% accuracy, 98.45% precision, 98.91% recall, and 98.68% F1-score on the UCF Sports dataset. Moreover, this method showed supreme results compared with existing techniques in all metrics, demonstrating its effectiveness and potential for high- applications in sports analytics and athlete monitoring.},
  archive      = {J_MVA},
  author       = {Gao, Yilun and Zou, Jie and Zhang, Yuexin},
  doi          = {10.1007/s00138-025-01733-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Exploring the potential of deep learning techniques for analyzing athlete movements in competitive athletics sports},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained 3D vehicle shape manipulation via latent space editing. <em>MVA</em>, <em>36</em>(6), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01739-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant potential of 3D object editing to impact various industries, recent research in 3D generation and editing has primarily focused on converting text and images into 3D models, often paying limited attention to the need for fine-grained control over existing 3D objects. This paper introduces a framework that uses a pre-trained regressor to enable continuous and attribute-specific modifications of both the stylistic and geometric attributes of 3D vehicle models. Here, “fine-grained control” refers to the ability to adjust specific geometric or stylistic attributes (such as roof length or perceived luxury) in a continuous and independent manner. Our method aims to preserve the identity of vehicle 3D objects and support multi-attribute editing, allowing for extensive customization while maintaining the model’s structural integrity. The framework leverages DeepSDF to obtain latent representations suitable for continuous attribute editing. Experimental results demonstrate the effectiveness of our approach in achieving detailed, controlled edits on a variety of vehicle 3D models. The code is released at https://github.com/JiangDong-miao/Vehicle_LatentEdit .},
  archive      = {J_MVA},
  author       = {Miao, JiangDong and Ikeda, Tatsuya and Raytchev, Bisser and Mizoguchi, Ryota and Hiraoka, Takenori and Nakashima, Takuji and Shimizu, Keigo and Higaki, Toru and Kaneda, Kazufumi},
  doi          = {10.1007/s00138-025-01739-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fine-grained 3D vehicle shape manipulation via latent space editing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ED-ViTTL: Ensemble vision transformer and transfer learning approach for brain tumor classification. <em>MVA</em>, <em>36</em>(6), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01741-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of brain tumors from magnetic resonance imaging (MRI) remains a challenging task due to the inherent heterogeneity of tumor morphology, class imbalance within datasets, and the limitations of individual deep learning models. To address these challenges, we propose ED-ViTTL (Ensembled Deep Vision Transformer and Transfer Learning), a hybrid framework that leverages both local and global feature representations to enhance diagnostic performance. The model integrates five advanced variants of the Vision Transformer (R50-ViT-L/16, ViT-L/16, ViT-L/32, ViT-B/16, and ViT-B/32) alongside a transfer-learned VGG19 convolutional neural network. Feature embeddings extracted from the ViT and CNN branches are fused through fully connected layers, enabling robust classification into four categories: glioma, meningioma, pituitary tumor, and healthy brain. Experiments were conducted on a publicly available dataset comprising 3264 MRI scans, partitioned into training (70%), validation (15%), and testing (15%) sets using stratified sampling. To mitigate class imbalance and improve model generalization, we employed stratified 5-fold cross-validation, class-weighted categorical cross-entropy, and extensive data augmentation. The best-performing ensemble configuration (ViT-B/32 + VGG19) achieved a classification accuracy of 98.67%, with class-specific AUC values exceeding 0.99 and ROC curves demonstrating clear inter-class separability. Performance metrics, including precision, recall, and F1-scores, remained consistently high across folds, with the confusion matrix indicating minimal misclassifications. These findings demonstrate that ED-ViTTL delivers stable and reproducible results, underscoring its potential as a reliable computer-aided diagnostic tool for assessing brain tumors in clinical practice.},
  archive      = {J_MVA},
  author       = {Thakur, Amit and Patnaik, Pawan Kumar and Kumar, Manoj and Choudhary, Chaitali},
  doi          = {10.1007/s00138-025-01741-5},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ED-ViTTL: Ensemble vision transformer and transfer learning approach for brain tumor classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surface multi-spectral reflectivity and texture material recognition in non-constrained environments. <em>MVA</em>, <em>36</em>(5), 1-17. (<a href='https://doi.org/10.1007/s00138-025-01714-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Material recognition is the process of distinguishing various materials based on their inherent physical properties. It plays a pivotal role in numerous applications, including manufacturing, recycling, and robotic handling. Conventional recognition methods predominantly employ sensor- and vision-based approaches. However, these methods often face challenges such as the similarity and variability in material appearance, environmental conditions, and geometric constraints. In this research, we introduce a multimodal, vision-based, attention-driven model for material recognition. Contrary to preceding texture-based and multi-spectral based methods, our approach harnesses both the texture and light reflection distribution characteristics intrinsic to material surfaces. The proposed method features a collocated system that combines depth, RGB, and near-infrared (Near-IR) cameras, along with infrared laser projector. This specific setup was selected to capture reflection distribution and texture across the visible-near-infrared spectrum. Subsequently, the data captured by this setup were processed by a recognition model within a fusion framework. Our results outperform previous methods in terms of accuracy when additional modalities (Depth, Near-IR, laser projectors) are available, while also exhibiting equivalent performance to top RGB-based models when solely reliant on RGB data, thus, proving the complementarity of the added modalities with visible information.},
  archive      = {J_MVA},
  author       = {Dhahri, Ahmed and Amamou, Ali and Faghihi, Usef and Kelouwani, Sousso and Boisclair, Jonathan and Zeghmi, Lotfi},
  doi          = {10.1007/s00138-025-01714-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Surface multi-spectral reflectivity and texture material recognition in non-constrained environments},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scene low-light remote physiological measurement database. <em>MVA</em>, <em>36</em>(5), 1-9. (<a href='https://doi.org/10.1007/s00138-025-01719-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) has garnered significant research attention for its ability to estimate heart rate from facial videos, achieving promising results under controlled lighting conditions. However, in real-world scenarios, the coupling of lighting variations and motion artifacts poses complex challenges that current methods often overlook. To address this gap, we introduce the Multi-Scene Low-Light rPPG Dataset (MSLR), which can simulate realistic conditions where lighting and motion jointly impact rPPG performance. Specifically, MSLR includes three distinct illumination levels, and each level encompasses stable, talking, and head movement scenarios to reflect various real-world situations. In addition, MSLR captures the synchronized infrared video of the subject under the minimum light setting, providing supplementary data to enhance robustness. Based on MSLR, we propose a novel Low-Light rPPG-Preserving Enhancement (LPPE) method, which compensates for brightness disparities between low-light and normal-light videos while preserving the integrity of rPPG signals. Our dataset has been rigorously validated using a variety of traditional handcrafted and deep learning methods, offering valuable insights into rPPG performance under low-light conditions. We believe this work provides a significant step forward in deploying rPPG in real-world applications and serves as a valuable resource for the research community. Our code and dataset are available at https://github.com/wwenmaositu/MSLR .},
  archive      = {J_MVA},
  author       = {Wu, Junjun and He, Zuxian and Wang, Ying and Jiang, Yan and Cheng, Xu},
  doi          = {10.1007/s00138-025-01719-3},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-scene low-light remote physiological measurement database},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple object tracking using weighted graph convolutional neural networks. <em>MVA</em>, <em>36</em>(5), 1-17. (<a href='https://doi.org/10.1007/s00138-025-01722-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Object Tracking (MOT) is unreplaceable in computer vision for its wide applications. Though Graph Neural Networks (GNNs) are used in existing methods, ways to build graphs and use interactive features generated by GNNs are still open problems. We design an MOT model based on weighted Graph Convolutional Neural Networks (GCNNs). Specifically, we first use the Re-Identification module to extract the appearance features of detections. For tracklet features, we use both the average appearance features of objects within the tracklet and the features of the last object in the tracklet. Then, different from GCNNMatch, we calculate the initial-edge feature by adding cosine similarity constraint and multilayer perceptrons to include more features. For feature updating, we design GCNNs with two node-updating layers and two edge-updating layers. Finally, a fusion module fuses multi-features including interactive cosine similarity, updated edge features, cosine similarity between tracklet appearance and detection ones to get affinity scores which are used to calculate matching results by a designed two-step association algorithm. Our tracker achieves MOTA, IDF1, and HOTA of 79.89%, 78.38% and 64.07% on MOT16; 80.27%, 78.17%, and 63.84% on MOT17; and 77.63%, 76.33% and 62.56% on MOT20 demonstrating its robust data association ability.},
  archive      = {J_MVA},
  author       = {Zhang, Yubo and Zheng, Liying and Huang, Qingming},
  doi          = {10.1007/s00138-025-01722-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multiple object tracking using weighted graph convolutional neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel bi-modal deep neural network with handcrafted features for gait emotion recognition. <em>MVA</em>, <em>36</em>(5), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01718-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing emotions from gait is an emerging area of biometric research, which has applications in medicine, robotics, smart home design, risk assessment, and rescue operations. This article presents a novel deep learning architecture for gait emotion recognition. It utilizes a fusion of latent deep features, extracted from an efficient sequential neural network, with discriminative domain-specific handcrafted features. The results from the numerous experiments used to fine-tune the proposed bi-modal deep neural network (BMDNN) are provided. Additionally, the importance of each component of the architecture is validated via an ablation study and the effects of various Laban Movement Analysis (LMA) groups on the performance of the proposed architecture are analyzed. The proposed architecture outperforms state-of-the-art methods in all emotional classes on two benchmark datasets.},
  archive      = {J_MVA},
  author       = {Bhatia, Yajurv and Bari, A. S. M. Hossain and Gavrilova, Marina},
  doi          = {10.1007/s00138-025-01718-4},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A novel bi-modal deep neural network with handcrafted features for gait emotion recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality assessment of synthetic images via spatial distortion recognition. <em>MVA</em>, <em>36</em>(5), 1-16. (<a href='https://doi.org/10.1007/s00138-025-01724-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of generative models necessitates reliable methods for assessing synthetic image quality, particularly when models are trained with limited data and may produce spatial distortions. Current AI-generated image quality assessment techniques often assume high-fidelity generation or target text-to-image models, and many are computationally intensive. This study introduces an efficient approach for the assessment of images synthesized by generative adversarial networks (GANs). We propose a convolutional neural network, trained to recognize spatial distortions. Our model learns from real-world images altered with random spatial transformations to create distorted examples, simulating common artifacts without relying on GAN-produced training data. The model’s confidence in identifying distortions quantifies image quality. We utilized bilateral filtering to emphasize shape information on natural images. Experiments show that our method distinguishes between synthetic and real images and assesses visual quality more effectively than existing non-reference methods. Furthermore, our method aligned with human perception for both natural and illustration-style images from GANs trained on limited data. Our approach offers a computationally efficient framework for evaluating GAN-generated images without relying on text prompts at assessment and any actual synthetic images at training time.},
  archive      = {J_MVA},
  author       = {Sawada, Tomoya and Katsurai, Marie and Okubo, Masashi},
  doi          = {10.1007/s00138-025-01724-6},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Quality assessment of synthetic images via spatial distortion recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). L-VAE: Variational auto-encoder with learnable beta for disentangled representation. <em>MVA</em>, <em>36</em>(5), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01728-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel model called Learnable VAE (L-VAE), which learns a disentangled representation together with the hyperparameters of the cost function. L-VAE can be considered as an extension of $$\beta $$ -VAE, wherein the hyperparameter, $$\beta $$ , is empirically adjusted. L-VAE mitigates the limitations of $$\beta $$ -VAE by learning the relative weights of the terms in the loss function to control the dynamic trade-off between disentanglement and reconstruction losses. In the proposed model, the weight of the loss terms and the parameters of the model architecture are learned concurrently. An additional regularization term is added to the loss function to prevent bias towards either reconstruction or disentanglement losses. Experimental analyses show that the proposed L-VAE finds an effective balance between reconstruction fidelity and disentangling the latent dimensions. Comparisons of the proposed L-VAE against $$\beta $$ -VAE, VAE, ControlVAE, DynamicVAE, and $$\sigma $$ -VAE on datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that L-VAE consistently provides the best or the second best performances measured by a set of disentanglement metrics. Moreover, qualitative experiments on CelebA dataset, confirm the success of the L-VAE model for disentangling the facial attributes.},
  archive      = {J_MVA},
  author       = {Mogultay Ozcan, Hazal and Kalkan, Sinan and Yarman-Vural, Fatos T.},
  doi          = {10.1007/s00138-025-01728-2},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {L-VAE: Variational auto-encoder with learnable beta for disentangled representation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSPhys: Multiscale fusing-based diffusion model for remote physiological measurement. <em>MVA</em>, <em>36</em>(5), 1-18. (<a href='https://doi.org/10.1007/s00138-025-01720-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote Photoplethysmography (rPPG), an emerging non-contact solution for measuring physiological signals by capturing subtle skin color variations from facial videos, has attracted considerable attention for its potential in many applications. While recent deep learning-based approaches primarily leverage CNNs or Transformers to map videos to rPPG signals, they often fail to fully exploit the intrinsic temporal periodicity of rPPG. Moreover, thanks to recent advances in the diffusion model and the inherent periodic distribution of rPPG, remote physiological measurement can also benefit from the diffusion model in estimating rPPG with high quality. In this work, we propose a MultiScale Fusing-based diffusion-based architecture, namely MSPhys, to effectively capture the multiscale temporal contextual and periodic rPPG clues for precise rPPG estimation. At the core of MSPhys, the multiscale temporal fusing block serves as the key denoising module, where input features are first decomposed into multiscale temporal representations via a down-sampling mechanism and subsequently refined through a learnable up-sampling process to integrate multiscale information. Additionally, we introduce a hierarchical loss function to constrain multiscale temporal consistency between the predicted and ground-truth rPPG signals, further improving estimation accuracy. Extensive experiments on four challenging physiological benchmark datasets demonstrate that MSPhys achieves state-of-the-art performance, significantly outperforming existing approaches in remote physiological measurement.},
  archive      = {J_MVA},
  author       = {Su, Gaoji and Qian, Wei and Li, Qi and Wu, YingXu and Feng, Weijia and Guo, Dan},
  doi          = {10.1007/s00138-025-01720-w},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MSPhys: Multiscale fusing-based diffusion model for remote physiological measurement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimal lightweight convolutional bi-LSTM model-based driver behavior detection and classification. <em>MVA</em>, <em>36</em>(5), 1-21. (<a href='https://doi.org/10.1007/s00138-025-01727-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, classifying the behavior of the drivers is considered a real-time requirement in diverse contexts. Distraction during driving occurs when the drivers engage in various non-driving tasks that cause serious dilemmas in road traffic safety as well as road accidents. These non-driving tasks while driving significantly reduce the attention hence, the number of road accidents has increased and transportation is smashed. Therefore, an effective driver behavior detection system has been exploited widely to minimize the risk of road traffic accidents. A novel lightweight convolutional bidirectional long short-term memory-based transient search optimization algorithm is developed to detect the behavior of drivers and classify them based on the severity levels or class labels namely steady-state driving (class I), assertive driving (class II), absent-minded driving (class III), exhausted driving (class IV), Slow (class V), Sudden Acceleration (class VI), Sudden Right Turn (class VII), Sudden Left Turn (class VIII), Sudden Break (class IX) as well as alcoholic driving (class X). The proposed model systematically detects and generates the outcome effectively by focusing on the key aspects of the driver’s body movements. Various types of data namely gravity, speed, acceleration, vehicle throttle as well and Revolutions per minute are collected and provided as input. The proposed technique demonstrates high classification accuracy in detecting diverse driver behaviors under real-world conditionsand the accuracy rate attained by the proposed approach is 98.25%.},
  archive      = {J_MVA},
  author       = {Alajlan, Abrar Mohammed},
  doi          = {10.1007/s00138-025-01727-3},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An optimal lightweight convolutional bi-LSTM model-based driver behavior detection and classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised medical image segmentation with joint pseudo supervision. <em>MVA</em>, <em>36</em>(5), 1-18. (<a href='https://doi.org/10.1007/s00138-025-01723-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of medical image segmentation, annotating data collection is crucial for model training, but it is also a formidable task. The mainstream algorithm to address this issue is semi-supervised learning based segmentation, with one of the key technologies being pseudo label generation. However, incorrect pseudo labels will lead to error accumulation effect during training, thereby decreasing the model’s segmentation performance. To alleviate this problem, we propose a simple but effective pseudo label generation mechanism called joint pseudo supervision (JPS). Our basic framework consists of two pairs of teacher-student architectures with the same structure but independent parameters. The parameters of the student model are obtained through training, and the parameters of the teacher model are updated by the parameters of the corresponding student model through exponential moving average. Inspired by the idea of ensemble learning, JPS uses two independent teacher models to jointly generate pseudo labels with high-confidence, and then supervises the two student models with it, thereby restricting the accumulation of errors. By imposing consistency constraints, JPS encourages the two student models to learn from each other, further reducing the negative impact of incorrect pseudo labels. In the inference phase, the two student models collaborate to generate segmentation results and improve the overall segmentation performance. Experimental results show that the our method performs competitively with several state-of-the-art methods when benchmarked on public datasets, i.e., Automated Cardiac Diagnosis Challenge, Multi-Modality Whole Heart Segmentation and CVC-ClinicDB.},
  archive      = {J_MVA},
  author       = {He, Maolin and Liu, Nian and Bai, Jianbing and Xu, Jianfeng and Tang, Yi and Liu, Yan},
  doi          = {10.1007/s00138-025-01723-7},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Semi-supervised medical image segmentation with joint pseudo supervision},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MACS2_Net: A stochastic depth with antialiasing on an optimized multipath convolutional-SqueezeNet framework for facial emotion recognition. <em>MVA</em>, <em>36</em>(5), 1-25. (<a href='https://doi.org/10.1007/s00138-025-01726-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial emotion recognition (FER) is a prevalent area of research in the field of computer vision, yet detecting a person's emotions in a complex environment remains challenging due to substantial intra-class variances. In current image classification approaches, much research has provided an endwise structure to recognize facial emotions using deep learning algorithms. Consequently, an approach for reducing the computational burden by decreasing the number of feature dimensions while boosting FER efficiency is proposed in this paper. MACS2_Net is a multipath hybrid deep learning framework constructed of two pathways. Path-1 (DA_CNN) is an antialiasing-based CNN model that focuses on capturing local patterns, in which antialiasing is incorporated to eliminate intermediate artifacts arising from downsampling steps. Path-2 (DS_SN) is a StochasticDepth-based SqueezeNet model that relies on capturing more global context. The StochasticDepth layer is chosen for its capacity to effectively cope with sparse and noisy inputs, limiting overfitting while offering effective computing. When merged, they can provide more comprehensive representations to assist with classifying emotions. Davis’s Library (Dlib) is utilized at first to extract facial landmarks with the goal of reducing redundancy in facial data. Following that, these landmarks will be simultaneously delivered as input for both Path-1 and Path-2. Each of these models have then been evaluated on lab-controlled datasets and wild FER dataset to detect eight distinct emotions from image data. The observations from the proposed models will then be contrasted against one another. Furthermore, several performance measures involving accuracy, precision, specificity, sensitivity, Jaccard coefficient, training time, and the overall number of parameters have been analyzed to assess the reliability and efficiency of the proposed methods. The multipath MACS2_Net framework outperforms all other classification models both in terms of reliability and precision while reducing the depth of the input data, alleviating training time, with an accuracy of 99.3%, 83.4%, and 98.6% for the Extended Cohn-Kanade (CK +), Real-world Affective Face (RafD), and Japanese female facial expressions (Jaffee) data, respectively.},
  archive      = {J_MVA},
  author       = {Elsheikh, Reham A. and Mohamed, M. A. and Abou-Taleb, Ahmed Mohamed and Ata, Mohamed Maher},
  doi          = {10.1007/s00138-025-01726-4},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MACS2_Net: A stochastic depth with antialiasing on an optimized multipath convolutional-SqueezeNet framework for facial emotion recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logit scaling for out-of-distribution detection. <em>MVA</em>, <em>36</em>(5), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01730-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The safe deployment of machine learning and AI models in open-world settings hinges critically on the ability to detect out-of-distribution (OOD) data accurately, data samples that contrast vastly from what the model was trained with. Current approaches to OOD detection often require further training the model, and/or statistics about the training data which may no longer be accessible. Additionally, many existing OOD detection methods struggle to maintain performance when transferred across different architectures. Our research tackles these issues by proposing a simple, post-hoc method that does not require access to the training data distribution, keeps a trained network intact, and holds strong performance across a variety of architectures. Our method, Logit Scaling (LTS), as the name suggests, simply scales the logits in a manner that effectively distinguishes between in-distribution (ID) and OOD samples. We tested our method on benchmarks across various scales, including CIFAR-10, CIFAR-100, ImageNet and OpenOOD. The experiments cover 3 ID and 14 OOD datasets, as well as 9 model architectures. Overall, we demonstrate state-of-the-art performance, robustness and adaptability across different architectures, paving the way towards a universally applicable solution for advanced OOD detection. Our code is available at http://github.com/andrijazz/lts .},
  archive      = {J_MVA},
  author       = {Djurisic, Andrija and Liu, Rosanne and Nikolic, Mladen},
  doi          = {10.1007/s00138-025-01730-8},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Logit scaling for out-of-distribution detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a fairer vision: Addressing class imbalance in image gender recognition. <em>MVA</em>, <em>36</em>(5), 1-18. (<a href='https://doi.org/10.1007/s00138-025-01732-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the growing integration of machine learning and computer vision with a particular emphasis on classifying facial features. Although these technologies hold great potential, there are concerns regarding biases that must be mitigated to ensure more impartial results. This study primarily addresses the issue of bias caused by class imbalance, which can be mitigated through algorithmic or data-level approaches. Notably, the literature presents gaps, including the lack of comprehensive studies comparing these two types of mitigation techniques and understanding the circumstances in which each is more suitable. Moreover, the influence of imbalance conditions and data complexity on mitigation methods remains underexplored. To address these gaps, this research formulates three key research questions and conducts experiments using two datasets, UTKFace and PlantVillage, known for varying complexities. Various imbalance scenarios are simulated in these datasets. Additionally, a novel algorithm-level mitigation method named “Diffuse Focal Loss” is introduced. Results indicate the high effectiveness of synthetic oversampling methods, specifically using the Wasserstein variant of Generative Adversarial Networks, compared to algorithm-level approaches. Among the latter, the proposed novel method outperforms others in terms of metrics. However, it is worth noting that algorithmic techniques are more practical and quicker to apply in low-complexity scenarios.},
  archive      = {J_MVA},
  author       = {Barahona, Daniel and Quijano-Sánchez, Lara and Liberatore, Federico},
  doi          = {10.1007/s00138-025-01732-6},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Towards a fairer vision: Addressing class imbalance in image gender recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A zero-shot anomaly detection method based on learnable text query. <em>MVA</em>, <em>36</em>(5), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01725-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot anomaly detection task is to identify anomaly regions without using target domain data. The detection methods based on large vision language model typically rely on text encoder to extract text features, which significantly increases the computational complexity of the model. This paper proposes a zero-shot anomaly detection method based solely on learnable text query. Text features are directly represented by designed learnable text query rather than encoded by the text encoder. Partial tokens from the input space of the image encoder are replaced by designed learnable vision prompt. Image features are extracted by the improved image encoder. Anomaly regions are calculated based on cosine similarity between text query and multiple adjusted image features. Four publicly available datasets are used to evaluate the zero-shot anomaly detection performance. Experimental results show that the proposed method achieves better results than the methods using the text encoder, and it has strong zero-shot generalization ability in different detection scenarios.},
  archive      = {J_MVA},
  author       = {Song, Yanan and Pan, Baisong and Yi, Wenchao and Zhang, Biao},
  doi          = {10.1007/s00138-025-01725-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A zero-shot anomaly detection method based on learnable text query},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight and generalizable detection enhancement method using segmentation feedback. <em>MVA</em>, <em>36</em>(5), 1-16. (<a href='https://doi.org/10.1007/s00138-025-01729-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a vital area of computer vision that has achieved significant advancements in recent years. A common approach to detection involves refining the backbone of the model using supervision from classification tasks during the pre-training phase. However, the differences between classification and detection tasks during the pre-training and fine-tuning stages lead to challenges such as attention bias and a lack of location information. To address these issues, we introduce a lightweight and generalizable module called SegHead in this study. This module leverages segmentation feedback to align the pre-training and fine-tuning phases of object detectors, thereby optimizing the backbone parameters. Notably, our technique relies solely on bounding box-level supervision to achieve weakly supervised semantic segmentation without incurring additional costs for the detection model. We have employed a method called the Combination of GrabCut and Filling Rate (CGFR) to enhance the accuracy of segmentation supervision and generate improved pseudo masks. This approach boosts both detection accuracy and segmentation performance. Using the COCO evaluation metric, our method surpasses the RetinaNet baseline by 2.5 points in Average Precision (AP) on the PASCAL test set. Additionally, we have demonstrated in further experiments that our method can significantly improve the performance of various object detectors with minimal cost.},
  archive      = {J_MVA},
  author       = {Wang, Song and Wei, Wei and Wang, Zi’ang and Liu, Xue},
  doi          = {10.1007/s00138-025-01729-1},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A lightweight and generalizable detection enhancement method using segmentation feedback},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced hyperspectral image reconstruction via parallel 2D/3D convolution with global layer purification and multiscale pooling fusion. <em>MVA</em>, <em>36</em>(5), 1-16. (<a href='https://doi.org/10.1007/s00138-025-01734-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) reconstruction from RGB images offers a cost-effective solution, significantly reducing acquisition costs and expanding application scope. However, existing deep learning-based HSI reconstruction methods struggle with noise, artifacts, and overexposure in RGB images, leading to low reconstruction accuracy and poor local edge preservation. To address these challenges, we propose an Enhanced Hyperspectral Image Reconstruction via Parallel 2D/3D Convolution with Global Layer Purification and Multiscale Pooling Fusion (P2D/3D HRNet). The Global Layer-wise Purification Module (GLPM) effectively eliminates noise in shallow RGB features. The Multiscale Pooling Fusion Module (MPFM) fuses shallow spatial contextual information, local details, and deep spatial-spectral features, enhancing local edge reconstruction. Additionally, an Efficient 3D Convolution Pathway (E3DCP), parallel to the 2D convolution branch, captures sufficient spatial and spectral information, improving spectral accuracy. Experimental results on benchmark datasets demonstrate that our method outperforms most state-of-the-art approaches, achieving a significant reduction in mean relative absolute error 7.82% on the NTIRE2022 dataset and root mean square error 8.15% on the NTIRE2020-Real dataset. This study underscores the potential of our approach for practical hyperspectral imaging applications. The code can be obtained at https://github.com/zhang-yujie/P2D-3D_HRNet .},
  archive      = {J_MVA},
  author       = {Zhang, Yujie and Wang, Suyu and Cui, Yuqing},
  doi          = {10.1007/s00138-025-01734-4},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhanced hyperspectral image reconstruction via parallel 2D/3D convolution with global layer purification and multiscale pooling fusion},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StyleDemorpher: High-quality face demorphing via StyleGAN2’s latent space. <em>MVA</em>, <em>36</em>(5), 1-22. (<a href='https://doi.org/10.1007/s00138-025-01735-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Morphing attacks pose a serious threat to automated border control systems by allowing identity documents to be used by multiple individuals, undermining biometric security. To address this, we propose a novel face demorphing framework that leverages the latent space of StyleGAN2. At its core is ReStyle-ID, an encoder network optimized for identity preservation through improved loss functions and targeted training data, enabling accurate and identity-focused inversion. Combined with StyleDemorpher, a face demorphing network trained on a novel DemorphDB dataset with high-quality morph images that simulate realistic and challenging attack scenarios, the framework reconstructs high-resolution demorphed faces and generalizes well to unseen identities and morphing methods. Together, these components overcome key limitations of prior approaches, such as low resolution, poor robustness, and visual artifacts. This work offers a scalable and effective solution for face demorphing and contributes a comprehensive dataset and framework to support future research in biometric security.},
  archive      = {J_MVA},
  author       = {Ismayilov, Raul and Spreeuwers, Luuk and Batskos, Ilias},
  doi          = {10.1007/s00138-025-01735-3},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {StyleDemorpher: High-quality face demorphing via StyleGAN2’s latent space},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater image stitching algorithm based on point-line dual feature. <em>MVA</em>, <em>36</em>(4), 1-27. (<a href='https://doi.org/10.1007/s00138-025-01691-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limited imaging range of underwater cameras, this paper proposed an underwater image stitching algorithm based on point-line dual feature. Specifically, this paper proposed an underwater image enhancement algorithm based on dynamic thresholding and dark channel prior, which preprocessed the input degraded images to improve subjective perception and feature matching results. Simultaneously analyzed the stitching methods based on optimized alignment and seam optimization, and proposed an image matching strategy with point line dual features for pre-alignment of the target image. Considering the impact of different constraint terms on image alignment, this paper constructs local constraints, global constraints, smoothing constraints, and line feature constraints as the objective energy function based on the idea of grid optimization, in order to further optimize alignment. Finally, seamless underwater image stitching is achieved through preliminary fusion and optimal seam fusion. Our algorithm is significantly ahead of other comparative algorithms in terms of overall performance, both subjectively and objectively.},
  archive      = {J_MVA},
  author       = {Tang, Zhijie and Guo, Tian and Chen, Jiajun and Yan, Siyu and Xu, Congqi},
  doi          = {10.1007/s00138-025-01691-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Underwater image stitching algorithm based on point-line dual feature},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous tracking of objects with loose context constraints from multiple views: Human–human interaction paradigm. <em>MVA</em>, <em>36</em>(4), 1-18. (<a href='https://doi.org/10.1007/s00138-025-01695-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a scene consists of multiple regions of interest related in some context, their tracking can (and often should) incorporate topology constraints that depict the inherent structure properties of the target scene subset. Such principle has been commonly implemented by pictorial structure representations, graph networks, siamese trackers, etc., and, in general, part-based spatio-temporal modeling. This is contrary to the multiple object tracking principle, where all objects of the desired categories are detected and tracked, including objects that do not ‘belong’ to the context. Context topology is often not fixed over time. We use the notion of ‘loose context’ to denote partial conditionality among the targets: preserving relationships with respect to labels and locations within each part set (part-defined entity), while assuming conditionality between part sets only where dictated by the given scenario. An indicative example is human–human interaction, where one person can move independently from the other, while their approximate relative positions are given. We encode context with a small graph, where tracked regions are represented by nodes, and context topology is captured by edges. Instead of using image patches in a fully connected graph representation, we employ region proposals: we decouple the graph definition from the image domain, and the search space consists of a proposal set to be sampled for deriving candidate solutions at each time instance. We use sequences from multiple views to alleviate missing data from occlusions, and the corresponding proposals are also considered, through projection regression, in the candidate graph solution for a reference plane (view). The objective function incorporates spatio-temporal topology information, and appearance similarity of the proposal regions as encoded by a definition of residual loss of a siamese graph attention network (embedding similarity). Our architecture consists of four parts: a region proposal network, a plane-to-plane reciprocal projection regression module, the siamese GAT for evaluating target set appearance similarity between successive instances, and the objective optimizer. We validate our method using a ‘round table’ setup with four subjects and three cameras: one providing bird’s-eye view, where the desired targets are hands, and each of the other cameras captures two subjects with desired targets being faces and hands.},
  archive      = {J_MVA},
  author       = {Vatti, Jay and Tsechpenakis, Gavriil},
  doi          = {10.1007/s00138-025-01695-8},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Simultaneous tracking of objects with loose context constraints from multiple views: Human–human interaction paradigm},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on advances in visual computing 2023. <em>MVA</em>, <em>36</em>(4), 1. (<a href='https://doi.org/10.1007/s00138-025-01696-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  doi          = {10.1007/s00138-025-01696-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Editor’s note: Special issue on advances in visual computing 2023},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobgazenet: Robust gaze estimation mobile network based on progressive attention mechanisms. <em>MVA</em>, <em>36</em>(4), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01690-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze estimation is a fundamental task in computer vision with a wide range of applications. Recently, convolution neural network approaches have made notable progress in inferring gaze from facial images. However, these methods often struggle to capture fine-grained gaze features and reflect spatial contextual relationships, as the most crucial gaze information exists in the eye area, which constitutes only a small portion of the face images. In this paper, we introduce MobGazeNet, an efficient and lightweight network that leverages a progressive combination of attention mechanisms, including squeeze-and-excitation, convolutional block attention module, and coordinate attention. The combination of attention mechanisms helps to emphasize crucial eye features and allows the model to consider both local and global spatial relationships without increasing computational overhead. Furthermore, we introduce the rotation matrix formalism for gaze ground truth to avoid discontinuity and ambiguity in spherical angle representation. Building upon this, we propose a continuous 6D rotation matrix representation to enable efficient and reliable direct regression which we further enhance with a geodesic-based loss. To evaluate our model, we conduct experiments on three popular datasets collected in unconstrained settings. Our proposed model surpasses current SOTA methods in both performance and efficiency, showcasing its superior capability in gaze estimation. Our code is available at: https://github.com/Ahmednull/MobGazeNet .},
  archive      = {J_MVA},
  author       = {Abdelrahman, Ahmed A. and Hempel, Thorsten and Khalifa, Aly and Strazdas, Dominykas and Al-Hamadi, Ayoub},
  doi          = {10.1007/s00138-025-01690-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Mobgazenet: Robust gaze estimation mobile network based on progressive attention mechanisms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using breast density for hybrid region and pixel-level loss function. <em>MVA</em>, <em>36</em>(4), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01684-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer continues to be one of the most lethal cancer types, mainly affecting women. However, thanks to the utilization of deep learning approaches, there has been a considerable boost in the performance of the methods for breast cancer detection. The loss function is a core element of any deep learning architecture with a significant influence on their performance. The loss function is particularly important for tasks such as breast mass segmentation. For this task, challenging properties of input images, such as pixel class imbalance, may result in instability of training or poor detection results due to the bias of the loss function toward correctly segmenting the majority class. We propose a hybrid loss function incorporating both pixel-level and region-level losses, where the breast tissue density is used as a sample-level weighting signal. We refer to the proposed loss as Density-based Adaptive Sample-Level Prioritizing (Density-ASP) loss. Our motivation stems from the observation that mass segmentation becomes more challenging as breast density increases. This observation makes density a viable option for controlling the effect of region-level losses. We also propose to evaluate the method using automated density estimation approaches. To demonstrate the effectiveness of the proposed Density-ASP, we conducted mass segmentation experiments using two publicly available datasets: INbreast and CBIS-DDSM. Our experimental results demonstrate that Density-ASP improves segmentation performance compared to the commonly used hybrid losses across multiple metrics.},
  archive      = {J_MVA},
  author       = {Aliniya, Parvaneh and Nicolescu, Mircea and Nicolescu, Monica and Bebis, George},
  doi          = {10.1007/s00138-025-01684-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Using breast density for hybrid region and pixel-level loss function},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech-aided facial video super resolution with accurate lip motion and enhanced frequency details. <em>MVA</em>, <em>36</em>(4), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01699-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent breakthroughs in face hallucination, video face hallucination remains a challenging task due to the issue of consistency across video frames. The temporal dimension in videos makes it difficult to learn facial motion and maintain color uniformity throughout the sequence. To address these challenges, we propose a novel audio-visual cross-modality support based video face hallucination network. The framework excels in learning fine spatiotemporal motion patterns by leveraging the correlation between movement of the facial structure and associated speech signal. Another significant challenge generic to face hallucination is blurriness around the key facial regions, such as mouth and lips. These areas show higher spatial displacement rendering their recovery in low-resolution images particularly difficult. The proposed approach explicitly defines a lip reading loss to learn the fine-grain motion in these facial regions. Further, during training, GANs show a higher potential to overfit to small frequency bands, which results in missing hard-to-synthesize frequencies. As a remedy, we introduce a frequency based loss function compelling the model to grasp salient frequency features. Visual and quantitative comparisons with state-of-the-art demonstrate significant improvements in visual results as well as higher coherence in the generated outputs across successive frames.},
  archive      = {J_MVA},
  author       = {Sharma, Shailza and Singh, Vivek and Dhall, Abhinav and Kumar, Vinay},
  doi          = {10.1007/s00138-025-01699-4},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Speech-aided facial video super resolution with accurate lip motion and enhanced frequency details},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing underwater image clarity: A GAN-based approach with residual blocks and linear blending. <em>MVA</em>, <em>36</em>(4), 1-22. (<a href='https://doi.org/10.1007/s00138-025-01698-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an advanced approach to underwater image color restoration, building on the FUnIE-GAN network with the integration of residual blocks and a novel image segmentation and linear blending technique. Evaluated on a diverse dataset of 109 underwater images collected at varying depths and conditions, our method consistently outperformed state-of-the-art models, including UICRN, FUnIE-GAN, URanker, and CCL-Net. Key metrics demonstrate the efficacy of our approach, achieving SSIM values of 0.817, 0.885, and 0.830 across scenarios of minimal color deviation, blue–green color cast, and broad blue color shift, respectively. The method also recorded PSNR values of 28.332, 28.970, and 28.977, showcasing superior clarity and noise reduction, and UIQM scores of 2.941, 2.730, and 2.975, highlighting balanced perceptual quality. The incorporation of residual blocks enhanced feature preservation, ensuring fine details and textures were retained during restoration, while image segmentation and linear blending minimized gridline artifacts, achieving seamless image continuity. Notably, our approach reduced Delta E 2000 values to as low as 16.1 in challenging scenarios, underscoring its ability to restore accurate and natural colors in environments where red wavelengths are heavily attenuated. These innovations were validated on both real-world underwater images captured with a 4 K GoPro 8 and simulated datasets generated from atmospheric images, ensuring robust model training and evaluation. By addressing the inherent challenges of underwater imaging, this study provides a reliable and effective solution for restoring color fidelity and structural integrity in underwater imagery. The proposed method holds significant potential for applications in ecological monitoring, marine research, and underwater object recognition, contributing valuable advancements to the field of underwater imaging.},
  archive      = {J_MVA},
  author       = {Tsai, Yu-Shiuan and Chang, Keng-Wei and Lan, YinJie},
  doi          = {10.1007/s00138-025-01698-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Advancing underwater image clarity: A GAN-based approach with residual blocks and linear blending},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Panet: Polarization-aware instance segmentation in autonomous driving. <em>MVA</em>, <em>36</em>(4), 1-12. (<a href='https://doi.org/10.1007/s00138-025-01694-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation in autonomous driving faces significant challenges due to illumination variations, such as strong reflections, shadows, and occlusions. To address these issues, this paper introduces a novel polarization-aware framework that leverages pseudo-polarization information to enhance segmentation robustness under complex illumination conditions. Specifically, an unsupervised method is proposed to generate pseudo-polarization data, including degree of polarization and angle of polarization, directly from RGB images. These pseudo-polarization features are generated using optical physics constraints to ensure their physical plausibility and are further refined through multi-scale and multi-illumination consistency constraints, enhancing their reliability and semantic validity. The proposed polarization-aware network integrates these features into a model-agnostic architecture using an attention-based fusion mechanism, ensuring seamless compatibility with existing instance segmentation models. Extensive experiments demonstrate the effectiveness of the proposed method, achieving superior performance in scenarios with extreme illumination variations. Ablation studies validate the contributions of the pseudo-polarization generation process and consistency constraints, while real-world testing highlights its practical value for autonomous driving applications.},
  archive      = {J_MVA},
  author       = {Wang, Caimei and Xie, Hui and Guo, Fan and Song, Kang},
  doi          = {10.1007/s00138-025-01694-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Panet: Polarization-aware instance segmentation in autonomous driving},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-learning enhanced global–local feature fusion for image quality assessment. <em>MVA</em>, <em>36</em>(4), 1-12. (<a href='https://doi.org/10.1007/s00138-025-01702-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality assessment (IQA) has emerged as a crucial research area in computer vision. Most current IQA methods rely heavily on a large amount of labeled training data. However, labeling distorted images is a difficult and time-consuming task. This complicates the acquisition of sufficient training images for specific distortion types. Moreover, diverse application scenarios introduce various complex distortions, which complicate feature extraction and model training. To address these challenges, this paper proposes a two-stream model that effectively integrates local and global feature of the distorted image. In the local feature extraction module, a convolution-based multi-scale feature network is implemented to effectively extract the local feature. Meanwhile, the global context awareness module based on the Vision Transformer (ViT) captures the global information. Furthermore, a meta-learning framework is utilized to enable the model to acquire prior knowledge of various distortion types from a limited number of samples, which allows the model to rapidly adapt to unknown distortion types. This method reduces the dependence on the number of training samples and adapts to different application scenarios. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on public datasets such as LIVEC and KonIQ-10K. Compared to existing IQA methods, this method gains stronger generalization ability and stability. The code is available at https://github.com/dart-into/MGLIQA .},
  archive      = {J_MVA},
  author       = {Li, Nengxin and Yang, Xichen and Chen, Tianhai and Zhu, Shun and Mao, Zhongyuan and Wang, Tianshu and Shen, Xiaobo},
  doi          = {10.1007/s00138-025-01702-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Meta-learning enhanced global–local feature fusion for image quality assessment},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YG-SLAM: Dynamic environment-based geometric constraint point-line fusion visual SLAM system. <em>MVA</em>, <em>36</em>(4), 1-18. (<a href='https://doi.org/10.1007/s00138-025-01701-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional visual simultaneous localization and mapping technologies are predicated on the assumption of static environments, an assumption that falters in dynamic scenes rife with moving objects. In such scenarios, dynamic elements disrupt the provision of sufficient static texture information, ultimately compromising the robustness of visual simultaneous localization and mapping. To address this issue, this paper introduced YG-SLAM, a point-line fusion visual SLAM algorithm based on geometric constraints in dynamic environments, built upon the ORB-SLAM3 framework. The algorithm integrates an independent semantic segmentation thread using MLANet to detect dynamic objects in input frames. Additionally, it integrated LSD line feature extraction into the tracking thread, leveraging combined geometric constraints, including epipolar and distance constraints, to effectively eliminate dynamic feature points and lines. The efficacy of the proposed MLANet semantic segmentation model and YG-SLAM algorithm was validated using the PASCAL VOC and TUM datasets. Experimental results revealed that, compared to the YOLOv11s-seg network, the constructed MLANet semantic segmentation model achieved a 2.3% increase in detection accuracy on the PASCAL VOC dataset. Furthermore, the proposed YG-SLAM algorithm outperformed ORB-SLAM3 in all metrics on 8 dynamic sequences from the TUM dataset and 9 dynamic sequences from the Boon dataset, with the absolute trajectory RMSE reduced by an average of 79.02% and 89.31%, respectively. In juxtaposition with existing state-of-the-art dynamic visual SLAM algorithms, the YG-SLAM algorithm exhibits enhanced accuracy and robustness.},
  archive      = {J_MVA},
  author       = {Hu, Shuo and Zhao, Liye and Wang, Qing},
  doi          = {10.1007/s00138-025-01701-z},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {YG-SLAM: Dynamic environment-based geometric constraint point-line fusion visual SLAM system},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating visual-semantic relational reasoning for fake news detection on video platforms. <em>MVA</em>, <em>36</em>(4), 1-11. (<a href='https://doi.org/10.1007/s00138-025-01697-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news video detection is a challenge for social media and content platforms. There are two limitations in current method. (1) they only analyze video frames or video objects separately, which lacks information integration; and (2) the key role of semantic knowledge in recognition is ignored, and the video context information is not fully utilized. To solve these problems, we propose the Visual-Semantic Fake News Detection (VS-FND) framework, which aims to achieve deep relational inference of video content. Two types of graph memory modules are designed: (a) visual graph memory module, which focuses on mining key cues in video visual information. (b) Semantic graph memory module, which can use the semantic knowledge of videos to construct a rich semantic space and identify the semantic features related to fake news. Through the collaboration of these two modules, VS-FND build a hierarchical framework to enable visual-semantic relational reasoning from object level to frame level. We experiment on two benchmark datasets and achieve competitive performance compared to state-of-the-art methods, while also achieving significant advantages in the number of parameters and inference speed.},
  archive      = {J_MVA},
  author       = {Huang, Biying and Qu, Jianfeng},
  doi          = {10.1007/s00138-025-01697-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Integrating visual-semantic relational reasoning for fake news detection on video platforms},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-level benchmark dataset for spatial and temporal forensic analysis of videos. <em>MVA</em>, <em>36</em>(4), 1-22. (<a href='https://doi.org/10.1007/s00138-025-01704-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Forensics keeps developing new technologies to verify the authenticity of digital videos. The existing datasets have limitations including unrealistic and poor-quality tampering, small size, few types of forgeries, lack of range of video content, different lighting conditions, and a range of camera models. This paper proposes the COMSATS Structured Video Tampering Evaluation Dataset (CSVTED), a three-level benchmark dataset organized by tampering quality and video complexity. This dataset includes a diversity of tampering in the spatial and temporal domains such as frame duplication, deletion, insertion, copy-move, and splicing. The dataset aims to facilitate the evaluation of video forgery detection methods by providing 1047 videos (133 original and 914 tampered), captured by multiple cameras in different lighting conditions (morning, noon, evening, night, fog). To develop the benchmark dataset, videos are tampered with a variable number of duplicated/deleted/inserted frames as well as Event-Object-Person (EOP) based tampering. Special care has been taken to ensure minimal abrupt changes in tampered videos by using Structural Similarity Index Measure (SSIM) and Optical Flow (OF) to determine the optimal positions for duplication/insertion/deletion in the video. Taking into account the direction of motion of objects in the video, these techniques aid in seamlessly integrating the tampered frames while maintaining visual coherence. Furthermore, the videos in CSVTED depict natural scenes after the tampering process and are in the common formats of avi, mp4, or mov. This dataset will be publicly available for researchers in the domain of video forensic analysis.},
  archive      = {J_MVA},
  author       = {Akhtar, Naheed and Saddique, Mubbashar and Rosin, Paul L. and Sun, Xianfang and Hussain, Muhammad and Habib, Zulfiqar},
  doi          = {10.1007/s00138-025-01704-w},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A three-level benchmark dataset for spatial and temporal forensic analysis of videos},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMU-net: A dual stream multi-scale U-net for image splicing forgery localization. <em>MVA</em>, <em>36</em>(4), 1-16. (<a href='https://doi.org/10.1007/s00138-025-01706-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in image processing and the proliferation of editing software, image splicing forgery has become increasingly facile to execute yet harder to detect, thereby impacting societal security. Effective detection and localization methods are urgently needed. Existing methods, while somewhat effective, often over-rely on semantic features, overlook shallow features, and struggle to adapt to varying tampered region sizes. To address these issues, we propose a two-stream image splicing forgery localization network, named DMU-Net. The network first introduces a noise stream as a supplementary feature alongside the RGB stream to provide a richer feature representation. Subsequently, we improve the Atrous Spatial Pyramid Pooling module by incorporating an attention mechanism that enables the model to obtain feature maps of different scales, effectively use context information, and enhance the ability to capture tampered regions of various sizes. Finally, we employ a dual attention mechanism to fuse features from both the encoder and decoder stages; this approach effectively leverages shallow features for fusing coarse-grained and fine-grained features, thus enhancing the model’s ability to capture features across different dimensions. Extensive experimental results show that the proposed method outperforms the state-of-the-art image splicing localization methods in terms of detection accuracy and robustness.},
  archive      = {J_MVA},
  author       = {Yu, Niankang and Su, Lichao and Wang, Jinli and Huang, Liming},
  doi          = {10.1007/s00138-025-01706-8},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {DMU-net: A dual stream multi-scale U-net for image splicing forgery localization},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GraphLOMO: LOcating multiple objects without visual annotations. <em>MVA</em>, <em>36</em>(4), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01707-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locating multiple objects has become an important task in multimedia research and applications due to the common nature of real-world images. Object localization requires a large number of visual annotations, such as bounding boxes or segmentation, but the annotation process is labour-intensive and sometimes inextricable for human experts in complex domains such as manufacturing and medical fields. Moving beyond single object localization, this paper presents a weakly semi-supervised learning framework based on Graph Transformer Networks using Class Activation Maps to LOcate Multiple Objects (GraphLOMO) in images without visual annotations. Our method overcomes the computational challenges of gradient-based CAM while integrating topological information and prior knowledge into object localization. Moreover, we investigate the higher order of object inter-dependencies with the use of 3D adjacency matrix for better performance. Extensive empirical experiments are conducted on MS-COCO and Pascal VOC to establish a suitable performance measure and baselines, as well as a state-of-the-art for weakly semi-supervised multi-object localization.},
  archive      = {J_MVA},
  author       = {To, Alex and Davis, Joseph G. and Nguyen, Hoang D.},
  doi          = {10.1007/s00138-025-01707-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {GraphLOMO: LOcating multiple objects without visual annotations},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study of PTZ camera-based drone tracking under latency and detector challenges. <em>MVA</em>, <em>36</em>(4), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01708-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of drone technology has unlocked numerous possibilities, but it has also led to a surge in incidents of damage and violation caused by drones. This leads to the increment of the need for technologies preventing drone misuse and ensuring their safe operation. Recent progress in edge devices and computer vision has enabled the development of lightweight and efficient tracking systems. While fixed-camera systems have been proposed for drone detection and tracking, such configurations are inherently limited in their ability to reliably track fast and small drones in dynamic environments. To overcome these limitations, Pan-Tilt-Zoom (PTZ) cameras can be used to track the detected drone with pan and tilt movements. While the PTZ camera is a viable solution for detecting and tracking drones, real-world problems such as network latency, detection accuracy and control stability have rarely been systematically investigated. In this paper, we conduct a comprehensive analysis of a lightweight real-time drone tracking system that integrates image capture, YOLOv8-based detection, and PTZ control via Proportional-Integral-Differential (PID) control feedback, focusing on its performance under the realistic operational constraints. Quantitative analysis reveals the impact of each factor on tracking performance and suggests directions for system optimization. The findings offer practical guidance for the deployment of drone tracking systems and highlight design considerations essential for ensuring robust real-world operation.},
  archive      = {J_MVA},
  author       = {Lee, Hyeyeon and Lim, Dahee and Han, Seok-Ho and Lee, Jongmin and Lee, Sang-ho and Park, Jihun},
  doi          = {10.1007/s00138-025-01708-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An empirical study of PTZ camera-based drone tracking under latency and detector challenges},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Block-recurrent visual transformer for enhanced human detection in thermal imaging. <em>MVA</em>, <em>36</em>(4), 1-26. (<a href='https://doi.org/10.1007/s00138-025-01711-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human detection in thermal imaging is a critical area of research for surveillance systems, especially under challenging conditions such as low visibility and nighttime. This study proposes a novel approach using a Block-recurrent Visual Transformer (BViT) integrated into a Feature Pyramid Network (FPN) for enhanced multi-scale feature extraction. By combining BViTFPN with the Task-aligned One-stage Object Detection (TOOD) head, the model achieves improved detection accuracy and computational efficiency by effectively capturing complex spatial and temporal data in thermal video. Evaluation on a challenging thermal video dataset demonstrates significant improvements in mean Average Precision (mAP), particularly for small and medium-sized objects. The proposed model achieves a mean mAP of 0.5222, with high precision at mAP@50 (0.9319) and mAP@75 (0.5317), while maintaining an efficient processing speed of 35.46 FPS. This research not only advances human detection in thermal imaging but also provides insights into leveraging transformer-based architecture for robust feature extraction and dynamic temporal analysis.},
  archive      = {J_MVA},
  author       = {Vu, Pham Cung Le Thien and Bao, Pham The and Trinh, Tan Dat},
  doi          = {10.1007/s00138-025-01711-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Block-recurrent visual transformer for enhanced human detection in thermal imaging},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual localization based on spatial relative sound order. <em>MVA</em>, <em>36</em>(4), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01700-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound localization is one of the essential tasks in audio-visual learning. Especially, stereo sound localization methods have been proposed to handle multiple sound sources. However, existing stereo-sound localization methods treat sound source localization as a segmentation task and, as a result, require costly annotation of segmentation masks. Another serious problem of the existing stereo-sound localization methods is that they have been trained and evaluated only in a controlled environment, such as a fixed camera and microphone setting with limited variability of scenes. Therefore, their performance on videos recorded in uncontrolled environments, such as in-the-wild videos from the Internet, has not been fully investigated. To address these problems, we propose a weakly supervised method as an extension of a typical stereo-sound localization method by utilizing the spatial relative order of sound sources in recorded videos. The proposed method solves the annotation problem by training the localization model using only sound category labels. Furthermore, our method utilizes the spatial relative order of the sound sources, which is not affected by specific recording settings, and thus can be effectively used for videos recorded in uncontrolled environments. We also collect stereo-recorded videos from YouTube to construct a new dataset to demonstrate the applicability of the proposed method to stereo sounds recorded in various environments. Our method enhances the localization performance by inserting a novel training step that exploits the relative order of sound sources into a typical audio-visual localization method in both existing and newly introduced audio-visual datasets.},
  archive      = {J_MVA},
  author       = {Sato, Tomoya and Sugano, Yusuke and Sato, Yoichi},
  doi          = {10.1007/s00138-025-01700-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Audio-visual localization based on spatial relative sound order},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FIRE-AD: Frequency-dependent image reconstruction error for micro defect detection. <em>MVA</em>, <em>36</em>(4), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01705-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro defects, such as casting pores in industrial products, have been detected by human visual inspection using X-ray CT images and image processing tools. Although recent deep model-based methods achieve high anomaly detection performances, the detection of micro defects is challenging because metrics for anomaly detection are dominated by low-frequency information. To overcome the problem, we propose introducing frequency-dependent losses to capture reconstruction errors appearing around micro defects and frequency-dependent data augmentation to improve the sensitivity against the errors. We demonstrate the effectiveness of the proposed method through experiments with MVTec AD dataset especially on the detection of micro defects.},
  archive      = {J_MVA},
  author       = {Nomura, Yuhei and Hachiya, Hirotaka},
  doi          = {10.1007/s00138-025-01705-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {FIRE-AD: Frequency-dependent image reconstruction error for micro defect detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RPIM-net: Residual channel prior-driven interaction multi-scale network for stereo image deraining. <em>MVA</em>, <em>36</em>(4), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01710-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing stereo image deraining methods do not fully exploit the correlation information between left and right views, as well as the image background prior. This paper introduces a Residual Channel Prior-Driven Interaction Multi-Scale Network (RPIM-Net) for stereo image deraining, comprising feature extraction, parallax interaction, and reconstruction. To enhance deraining performance, we introduce Residual Channel Prior (RCP) to obtain richer background features. Additionally, we design Residual Multi-Path Smooth Dilated convolution Block (ResMSDBlock) to capture multi-scale features while mitigating discontinuities between adjacent pixels caused by the dilated convolution. To further refine the deraining process, we incorporate the bi-directional Parallax Attention Module (biPAM) which leverages correlation between left and right views to enhance view reconstruction. Finally, a channel attention module fuses features after the view interaction, followed by a residual dense block to reconstruct high-quality rain-free images. Experimental results illustrate the superiority of the proposed RPIM-Net algorithm on real-world rain images and synthetic datasets over the existing algorithms. The source code is released at https://github.com/ooowoooyoooh/RPIM-Net/tree/master/stereoDeraining .},
  archive      = {J_MVA},
  author       = {Wang, Yihan and Wang, Yongfang and Li, Mengyao},
  doi          = {10.1007/s00138-025-01710-y},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {RPIM-net: Residual channel prior-driven interaction multi-scale network for stereo image deraining},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing micro-expression recognition via triple diversity feature augmentation in convolutional networks. <em>MVA</em>, <em>36</em>(4), 1-18. (<a href='https://doi.org/10.1007/s00138-025-01712-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions have made it difficult to solve the problems of diversity and balance due to the small dataset, while the properties of local regions of micro-expressions are also one of the problems that cannot be ignored. Therefore, to address these two problems, a convolutional network micro-expression recognition model with triple diversity feature augmentation is proposed. Among them, for the micro-expression local region problem, we combine local and global to construct fusion features by dividing each handcrafted feature into a local and global feature flow network. For the diversity and balance problems of micro-expressions, we first introduce an implicit semantic data augmentation algorithm to construct a diversity augmentation loss function for the semantic direction information of the samples. Meanwhile, considering the diversity value of the original sample, the original diversity cross-entropy loss function is constructed, which can make up for the loss of original diversity due to semantic diversity augmentation and ensure the effectiveness of feature diversity information. Then, considering the effect of class balance on diversity augmentation, the class-weighted mean feature is proposed for the first time, and the balanced diversity class loss function is constructed, which can improve the problems caused by balance. Finally, the three loss functions are fused to construct a triple diversity composite loss function. The experiments on micro-expression datasets and composite databases were conducted for 3- and 5-classification, respectively. The various experiments demonstrate the feasibility and effectiveness of our method while outperforming current state-of-the-art algorithms.},
  archive      = {J_MVA},
  author       = {Zhang, Fan and Chai, Lin},
  doi          = {10.1007/s00138-025-01712-w},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhancing micro-expression recognition via triple diversity feature augmentation in convolutional networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local–global transformer-based point cloud segmentation network for workpiece surface defect grinding. <em>MVA</em>, <em>36</em>(4), 1-17. (<a href='https://doi.org/10.1007/s00138-025-01715-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is extremely useful in high-precision intelligent robotic grinding due to their abundant spatial information. Accurate point cloud segmentation can extract surface defects of workpieces to provide 3D data for robotic grinding. Recently, with the development of computer vision, Transformer-based methods have been widely applied in point cloud segmentation, while most of them only capture features based on global or local attention, resulting in a lack of comprehensive information. In this paper, we propose a novel segmentation network, Local and Global Feature Transformer Network (LGFT-Net), to combine local features and global features based on Transformer module. Specifically, a local feature extraction block with feature embedding module and Local Transformer module is designed to embed features into a higher space and learn feature correlations in local regions, and a global feature extraction block with Global Transformer module is used to obtain global contextual information. We perform extensive experiments on ShapeNet dataset and workpiece point cloud dataset. The results show that: (1) the LGFT-Net achieves segmentation performance comparable to state-of-the-art methods in part segmentation and (2) this model is feasible to segment surface bump defects of workpieces to provide 3D data for robotic grinding.},
  archive      = {J_MVA},
  author       = {Zhang, Qimin and Wang, Qiang and Wang, Ningyuan and Liu, Jiaxuan and Qu, Delin},
  doi          = {10.1007/s00138-025-01715-7},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Local–global transformer-based point cloud segmentation network for workpiece surface defect grinding},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards safer roads: Benchmarking object detection models in complex weather scenarios. <em>MVA</em>, <em>36</em>(4), 1-17. (<a href='https://doi.org/10.1007/s00138-025-01709-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of object detection models in adverse weather conditions remains a critical challenge for intelligent transportation systems. Since advancements in autonomous driving rely heavily on extensive datasets, which help autonomous driving systems be reliable in complex driving environments, this study provides a comprehensive dataset under diverse weather scenarios like rain, haze, nighttime, or sun flares and systematically evaluates the robustness of state-of-the-art deep learning-based object detection frameworks. Our Adverse Driving Conditions Dataset features eight single weather effects and four challenging mixed weather effects, with a curated collection of 50,000 traffic images for each weather effect. State-of-the-art object detection models are evaluated using standard metrics, including precision, recall, and IoU. Our findings reveal significant performance degradation under adverse conditions compared to clear weather, highlighting common issues such as misclassification and false positives. For example, scenarios like haze combined with rain cause frequent detection failures, highlighting the limitations of current algorithms. Through comprehensive performance analysis, we provide critical insights into model vulnerabilities and propose directions for developing weather-resilient object detection systems. This work contributes to advancing robust computer vision technologies for safer and more reliable transportation in unpredictable real-world environments.},
  archive      = {J_MVA},
  author       = {Tran-Le, Ba-Thinh and Patel, Vatsa and Huynh, Viet-Tham and Tran, Mai-Khiem and Agrawal, Kunal and Tran, Minh-Triet and Nguyen, Tam V.},
  doi          = {10.1007/s00138-025-01709-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Towards safer roads: Benchmarking object detection models in complex weather scenarios},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel cauchy mixture modeling combined with the sparse-RCNN architecture for enhanced multi-person pose estimation. <em>MVA</em>, <em>36</em>(4), 1-16. (<a href='https://doi.org/10.1007/s00138-025-01703-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a lot of research in the fields of computer vision and multimedia to analyze human behavior and activities through images. A particular area of focus has been on estimating human pose, also known as skeleton estimation. Deep learning methods have been commonly used for this task, which primarily rely on the keypoint features of the human body. However, this approach can be limiting when there are occluded or incomplete poses, especially when multiple humans are present in a single frame. Other features like visibility conditions and body boundaries can also contribute to pose estimation in such cases. This paper outlines a method for multi-person pose estimation leveraging Sparse-RCNN with mixture models, aiming to reduce computational complexity while enhancing accuracy. The approach integrates innovative techniques of Sparse-RCNN such as learnable proposal boxes, dynamic heads, and an iteration structure to efficiently extract features with mixture models and human body masks. This approach has resulted in significant improvements, as indicated by an increase in average precision and faster processing compared to other state-of-the-art methods on the COCO and CIHP datasets.},
  archive      = {J_MVA},
  author       = {Rizwan, Tahir and Cai, Yunze and Vohra, Rhythm},
  doi          = {10.1007/s00138-025-01703-x},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Novel cauchy mixture modeling combined with the sparse-RCNN architecture for enhanced multi-person pose estimation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SR-BigGAN: Lightweight image super-resolution with priors. <em>MVA</em>, <em>36</em>(4), 1-18. (<a href='https://doi.org/10.1007/s00138-025-01713-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in the field of high-fidelity image synthesis using GANs has shown appealing outcomes, which motivates a series of successful image super-resolution (SR) works. However, most GAN-based SR models apply plain GAN models and are cumbersome compared to CNN-based SR models. In addition, the very advantage of conditional GAN has not yet been explored under the context of SR. In this paper, we develop a lightweight SR-BigGAN with priors for single-image super-resolution (SISR). First, our new model is an extension of BigGAN tailored to deep SR pipeline, retaining both generator and discriminator architectures, but with modifications to accommodate SR tasks. Second, prior knowledge defined as class labels from the low-resolution images, is fully leveraged through the conditional generative model to refine the SR process. Third, the lightweight nature of the model is achieved through knowledge distillation, focusing on reduced computational complexity and memory usage, making it the first practice of this kind in GAN-based SISR modeling. Extensive experiments on DIV2K, Pascal, mini-ImageNet, and SR benchmarks including Set5 and Set14 in an attempt to compare the Structural Similarity (SSIM), Peak Signal-to-Noise Ratio (PSNR) with the state-of-the-art models have shown appealing results. Our model achieves an average PSNR of 34.99 and SSIM of 0.791 across these datasets, demonstrating quantitative improvements over existing methods. The generated high-resolution images offer both perceptual enhancement and improved classification results. Additionally, explicit comparisons with GAN-based SR techniques such as ESRGAN and SRGAN highlight the superiority of our approach in both fidelity and efficiency. In particular, we achieve an average of $$\hbox{PSNR}=34.99$$ and $$\hbox{SSIM}=0.791$$ on several testing datasets.},
  archive      = {J_MVA},
  author       = {Kumar, Deepak and Srinivas Rao, Harshitha and Kumar, Chetan and Shao, Ming},
  doi          = {10.1007/s00138-025-01713-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {SR-BigGAN: Lightweight image super-resolution with priors},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video-based adaptive respiratory rate monitoring for clinical applications. <em>MVA</em>, <em>36</em>(4), 1-16. (<a href='https://doi.org/10.1007/s00138-025-01716-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respiratory rate (RR) is a critical physiological parameter for assessing respiratory function and is widely used in clinical applications. Non-contact monitoring of the RR enables continuous monitoring, avoiding the discomfort often associated with traditional contact methods, making it highly desirable in clinical applications. The existing method [1] could measure the RR from RGB videos by combining the optical flow method and the negative feedback crossover point method. The selection of feature points in the optical flow method significantly affects the accuracy of respiratory signals. To enhance the dynamic tracking performance of feature points, we propose to simplify the feature point evaluation criteria. Additionally, we utilize the temporal information of the feature points to adaptively optimize the feature point space. The crossover point method is prone to erroneous crossover points, while the negative feedback crossover point method can remove erroneous points but may also eliminate valid ones. Therefore, we further optimize the crossover point removal strategy based on the physiological information of the crossover points. In the Large-scale Bedside Respiration Dataset for Intensive Care (LBRD-IC), our proposal method estimates the RR with a mean absolute error (MAE) of 4.034 beats per minute (BPM) and a mean squared error (MSE) of 30.34 $$\text {BPM}^2$$ , while the existing method achieves a MAE of 4.464 BPM and a MSE of 36.75 $$\text {BPM}^2$$ . This demonstrates the effectiveness of our proposal method in clinical applications. The codes are publicly available at https://github.com/ZinChou/RR .},
  archive      = {J_MVA},
  author       = {Zhou, Zhiqin and Shan, Caifeng},
  doi          = {10.1007/s00138-025-01716-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Video-based adaptive respiratory rate monitoring for clinical applications},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSPKD: Multi spatial projectors for knowledge distillation in semantic segmentation. <em>MVA</em>, <em>36</em>(4), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01721-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation involves assigning a class label to every pixel in an image and serves as a key technology in applications such as medical imaging, autonomous driving, and satellite image analysis. While existing deep learning models and transformer-based architectures have demonstrated outstanding performance, their high computational costs and memory demands limit their applicability in resource-constrained environments. To address these challenges, knowledge distillation (KD), which transfers knowledge from a high-performing teacher model to a lightweight student model, has emerged as an effective approach. However, existing logit-based KD methods have shown limitations in fully leveraging spatial context and structural information. Additionally, the distillation process often introduces computational and memory overhead, restricting its practicality. This study proposes a novel framework, Multi Spatial Projectors for Knowledge Distillation (MSPKD), to overcome these limitations. At its core, MSPKD utilizes lightweight $$1 \times 1$$ convolutional layers as multi spatial projectors to maintain spatial integrity while effectively aligning the feature spaces between teacher and student models. The total loss function combines cross-entropy loss, standard KD loss, and projector-based KD loss to effectively support the student model’s learning. Experiments conducted on representative segmentation datasets—PASCAL VOC, Cityscapes, and CamVid—demonstrate that the proposed MSPKD framework achieves performance improvements in both mean intersection over union (mIoU) and pixel accuracy compared to Vanilla KD, while simultaneously proving its efficiency and practicality.},
  archive      = {J_MVA},
  author       = {Park, Yeongje and Hwang, Jeongwon and Jeong, Seung-Min and Lee, Eui Chul},
  doi          = {10.1007/s00138-025-01721-9},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MSPKD: Multi spatial projectors for knowledge distillation in semantic segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enforced clustering for zero-to-one-shot texture anomaly detection. <em>MVA</em>, <em>36</em>(3), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01670-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies on anomaly detection (AD) for industrial products typically address the problem in an unsupervised manner, requiring only normal data for training. This approach alleviates the need for anomalous data but still requires a set of normal samples and often involves demanding computations. More recent methods aim to solve this problem in zero-, one-, or few-shot settings but suffer from performance drops or rely on additional contexts, such as language guidance and text encoding, which add overhead. This paper focuses on homogeneous textures and demonstrates how the problem can be addressed without any training samples or additional training (zero-shot), only requiring one normal sample (one-shot) for hyperparameter selection, which is an additional challenge in unsupervised settings. This is achieved by enforcing K-means clustering with $${K}=2$$ on each of the testing samples independently, distinguishing it from the typical use of clustering methods in outlier detection, which are applied to a set of samples. The confidence score of each locality belonging to the smaller cluster, considered the potential anomalous cluster for evaluation, forms the anomaly map used in anomaly localization, and the maximum values in this map are used in AD. Competitive performance is achieved through the careful selection of the distance metric, feature layers, and clustering method. Experiments show that this zero-to-one-shot method, which facilitates deployment by reducing data dependency, maintains performance comparable to, or even higher than, conventional many-shot methods, all with relatively high speed.},
  archive      = {J_MVA},
  author       = {Amirian Varnousefaderani, Bahar and Rakhmonov, Akhrorjon Akhmadjon Ugli and Kim, Jae-Soo and Kim, Jeonghong},
  doi          = {10.1007/s00138-025-01670-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enforced clustering for zero-to-one-shot texture anomaly detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring filter placement in convolutional layer topologies based on ResNet for image classification. <em>MVA</em>, <em>36</em>(3), 1-11. (<a href='https://doi.org/10.1007/s00138-025-01674-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we investigate the impact that altering the convolutional layer topology has upon the performance of computer vision tasks using a variety of widely used benchmark image datasets. Despite the widespread convention in convolutional neural networks, of incrementally doubling the filter count at each layer, there is little evidence substantiating the superiority of this method over other possible topologies. Our research reveals that a contrarian strategy—reducing the filters by half—can achieve performance on par with, if not superior to, this usual approach. We have extended our investigation to include a variety of novel topological structures. These empirical results challenge the prevailing assumption, that the sequential doubling of number of filters in the network configuration will always yield the best results with all datasets. Our findings advocate for a more nuanced approach to neural network design, incorporating a flexible approach to filter topologies into workflows. This could potentially have a significant impact upon the architectural standards in deep learning for visual recognition tasks.},
  archive      = {J_MVA},
  author       = {Liu, Haixia and Brailsford, Tim and Bull, Larry},
  doi          = {10.1007/s00138-025-01674-z},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Exploring filter placement in convolutional layer topologies based on ResNet for image classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on SLAM and machine learning approaches for indoor autonomous navigation of mobile robots. <em>MVA</em>, <em>36</em>(3), 1-21. (<a href='https://doi.org/10.1007/s00138-025-01673-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex and dynamic nature of indoor environments presents significant challenges for mobile robots autonomous navigation. Traditional navigation methods, reliant on handcrafted features and algorithms, often struggle to adapt to these challenges. Recently, machine learning techniques have emerged as a promising approach for indoor autonomous navigation, offering the ability to learn from data to extract features and develop robust navigation strategies. This survey presents recent strategies for indoor autonomous navigation of mobile robots, providing a comprehensive overview of traditional methods for indoor autonomous mobile robots simultaneous localization and mapping mapping (SLAM), path planning and obstacle avoidance, and machine learning approaches, including deep learning and reinforcement learning. Furthermore, the paper discusses the specific challenges of indoor autonomous navigation for mobile robots and examines the advantages, challenges, and limitations of applying machine learning techniques in this context. Since performance evaluation is crucial for proving the efficiency of each novel developed algorithm and method, the most important performance evaluation metrics are described and mathematically presented with formulas. A systematic review on recent advances in indoor autonomous mobile robot navigation is further supported by presenting relevant patents on the topic of the paper and the field. Additionally, the survey identifies promising future research directions in machine learning-based indoor autonomous navigation. Last but not least, this survey aims to serve as a valuable resource for researchers and engineers interested in developing advanced and machine learning-based indoor autonomous navigation systems for mobile robots.},
  archive      = {J_MVA},
  author       = {Damjanović, Davor and Biočić, Petar and Prakljačić, Stjepan and Činčurak, Dorian and Balen, Josip},
  doi          = {10.1007/s00138-025-01673-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A comprehensive survey on SLAM and machine learning approaches for indoor autonomous navigation of mobile robots},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive fusion attention for enhanced classification and interpretability in medical imaging. <em>MVA</em>, <em>36</em>(3), 1-23. (<a href='https://doi.org/10.1007/s00138-025-01665-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate medical image classification is crucial for effective clinical decision support, improving patient outcomes and reducing healthcare costs. However, developing expert Computer-Aided Diagnosis systems for accurate medical image classification remains a challenging task. Recent advancements in attention mechanisms have revolutionized deep learning-based approaches, leading to improved performance even in applications with limited labeled data. Despite these advances, challenges such as overfitting and poor generalization persist. This work introduces a novel deep learning-based model that incorporates Adaptive Fusion Attention to enhance medical image analysis. The proposed attention module employs a hierarchical fusion of spatial and temporal attention mechanisms, complemented by adaptive refinement. This enables the model to focus on the most discriminative features in medical images, improving its ability to detect abnormalities. Additionally, GRAD-CAM visualizations demonstrate that the model effectively highlights pathological regions while minimizing attention on non-relevant areas. The model is evaluated on three benchmark datasets-APTOS-2019, Figshare, and SARS-CoV-2-demonstrating its effectiveness in Diabetic Retinopathy grading, Brain Tumor Classification, and COVID-19 detection. Experimental results show substantial improvements, achieving 84.56% accuracy for retinopathy grading, 99.60% accuracy for tumor classification, and 99.35% accuracy for COVID-19 detection. These results, along with superior performance across other metrics such as ROC-AUC, and F1-scores, demonstrate the effectiveness of the proposed model with Adaptive Fusion Attention over existing approaches.},
  archive      = {J_MVA},
  author       = {Shaik, Nagur Shareef and Veeranjaneulu, N. and Bodapati, Jyostna Devi},
  doi          = {10.1007/s00138-025-01665-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adaptive fusion attention for enhanced classification and interpretability in medical imaging},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cigarette defect detection algorithm based on attention mechanism and multi-gradient feature fusion. <em>MVA</em>, <em>36</em>(3), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01681-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface defect detection remains a persistent and challenging task. Aiming at the detection of surface defects in cigarettes, we propose an enhanced YOLOX-S model. Firstly, an improved attention mechanism named MS-GCT (Multi-Spectral Gaussian Context Transformer) is introduced into the model’s backbone to enhance the model’s ability of capturing the global context information within images and improve its comprehension of semantic feature information; secondly, we propose the DMG (Dynamic convolution and MS-GCT) module, and combined with the C2f (CSPLayer with 2 convolutions) module to construct the C2f-DMG module,which is introduced into the model to enhance feature interaction and feature extraction ability, to strengthen long-distance dependency ability of global features; finally, we replace the loss function with SIoU to enhance model performance and accelerate model convergence. To validate the effectiveness of our model, we conduct experiments on both the self-made cigarette dataset and the public dataset. The experimental results indicate that the improved model not only ensures the lightweight of the model, but also boosts the model’s mAP by 2.02, while achieving a detection speed of 73.17 frames−1. Furthermore, the proposed algorithm fulfills the real-time detection requirements for cigarette appearance defects.},
  archive      = {J_MVA},
  author       = {Shi, Weiya and Zhang, Shiqiang and Zhang, Shaowen},
  doi          = {10.1007/s00138-025-01681-0},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cigarette defect detection algorithm based on attention mechanism and multi-gradient feature fusion},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D multi-object tracking based on parallel multimodal data association. <em>MVA</em>, <em>36</em>(3), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01675-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel 3D multi-object tracker based on tracking-by-detection (TBD) framework. The system leverages parallel feature extraction and data association methods to process 2D appearance features and 3D spatial information, respectively, to achieve accurate tracking of targets in autonomous vehicles and intelligent transportation systems. By combining the Siamese network-based image feature extractor to extract image appearance features and the kinematic model established using Kalman filtering, our tracker effectively utilizes the appearance and spatial information of the object, thus improving tracker accuracy and reliability. Experimental results demonstrate the competitiveness of our tracker on the KITTI tracking benchmark. Compared to previous methods, A parallel feature extraction algorithm is proposed that can independently extract the spatial and appearance information of the object. A novel data association algorithm is designed that makes full use of the spatial information of the object from the point cloud and the appearance information from the image. This work provides substantial technical underpinnings for the advancement of autonomous driving and intelligent transportation technology. https://github.com/TanShiyu2022/PMTrack .},
  archive      = {J_MVA},
  author       = {Tan, Shiyu and Li, Xu and Xu, QiMin and Zhu, Jianxiao},
  doi          = {10.1007/s00138-025-01675-y},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3D multi-object tracking based on parallel multimodal data association},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature distribution statistics as a loss objective for robust white balance correction. <em>MVA</em>, <em>36</em>(3), 1-20. (<a href='https://doi.org/10.1007/s00138-025-01680-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {White balance (WB) correction is critical for accurate color reproduction in digital images, especially under complex, multi-illuminant lighting conditions. Traditional methods, such as the Gray-World assumption, rely on global statistics and struggle in real-world, non-uniform lighting scenarios. Modern deep learning approaches, including convolutional and attention-based architectures, have significantly advanced WB correction but often fail to explicitly account for higher-order feature distribution statistics, which may limit their robustness in challenging environments. This study introduces a novel framework that leverages Exact Feature Distribution Matching (EFDM) as a loss objective to align feature distributions across multiple moments, including mean, variance, skewness, and kurtosis. By modeling lighting as a style factor, the method explicitly addresses distributional shifts caused by complex illumination, offering a robust solution for WB correction. The framework integrates EFDM with a Vision Transformer architecture, enabling precise handling of global and local lighting variations. Extensive experiments on the large-scale multi-illuminant (LSMI) dataset demonstrate the superiority of the proposed approach over state-of-the-art methods and commonly used loss functions when applied to the same architecture. Qualitative and quantitative evaluations highlight its effectiveness in achieving perceptually accurate WB correction, particularly in multi-illuminant environments. By bridging statistical modeling with modern deep learning, this work establishes the critical role of feature distribution alignment in advancing WB correction and sets a new benchmark for robustness and generalization in complex lighting scenarios.},
  archive      = {J_MVA},
  author       = {Kınlı, Furkan and Kıraç, Furkan},
  doi          = {10.1007/s00138-025-01680-1},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Feature distribution statistics as a loss objective for robust white balance correction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViCap-AD: Video caption-based weakly supervised video anomaly detection. <em>MVA</em>, <em>36</em>(3), 1-12. (<a href='https://doi.org/10.1007/s00138-025-01676-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection becomes increasingly critical amidst rising crime rates and concerns for public safety. Traditional unsupervised video anomaly detection methods primarily focused on normal data, limiting their ability to achieve optimal performance due to their inability to effectively utilize abnormal data. Weakly supervised video anomaly detection methods addressed some of these limitations but still struggled to leverage anomalous video labels effectively, often being susceptible to noise in classification scores. In this paper, we propose a novel approach named Video Caption Anomaly Detector (ViCap-AD), which leverages video captions alongside a combination of BERT and the multiple instance learning (MIL) framework for anomaly detection. ViCap-AD integrates video captions generated using CLIP4Clip with video features within the MIL framework augmented by BERT. In our experimental evaluations on the UCF-Crime and XD-Violence datasets, ViCap-AD achieves state-of-the-art performance, achieving AUC scores of 87.20% and 85.02%, respectively. These results underscore the robustness and effectiveness of our approach across different datasets, demonstrating its powerful performance and stability. This paper contributes a significant advancement in anomaly detection methodologies, highlighting the potential of ViCap-AD to enhance anomaly detection accuracy and reliability in real-world applications.},
  archive      = {J_MVA},
  author       = {Lim, Junwoo and Lee, Juyeob and Kim, Hyunji and Park, Eunil},
  doi          = {10.1007/s00138-025-01676-x},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ViCap-AD: Video caption-based weakly supervised video anomaly detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep representation learning for license plate recognition in low quality video images. <em>MVA</em>, <em>36</em>(3), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01678-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {License plate recognition is an important technology in many application scenarios such as traffic monitoring and vehicle management. Due to variations in viewpoint, illumination, motion-blur, and degradation during the imaging process, it is still a challenging problem to detect and recognize license plates in low quality video images. In this paper, we focus on efficient deep representation learning for license plate recognition, detection and tracking. For license plate recognition, we mainly investigate the configuration of different network structures. We design a novel backbone network structure called SACNN, which combines convolutional neural network (CNN) and self-attention mechanism to learn non-linear representations for the structural patterns of characters in low quality video images. The proposed license plate recognition model employs the SACNN backbone network, a Long Short-Term Memory (LSTM) encoder and a Transformer decoder. For license plate detection, a Transformer encoder–decoder based method is adopted. To tackle the variations in license plate appearances and perspectives, an image rectification method is incorporated by using a spatial transformer network. For license plate tracking, a multi-object tracking method is incorporated by using Kalman filtering and temporal matching to associate detected license plates in video frames. Experiments are mainly carried out on the public large-scale video-based license plate dataset (LSV-LP) to validate the proposed methods.},
  archive      = {J_MVA},
  author       = {Zhao, Kemeng and Peng, Liangrui and Ding, Ning and Yao, Gang and Tang, Pei and Wang, Shengjin},
  doi          = {10.1007/s00138-025-01678-9},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep representation learning for license plate recognition in low quality video images},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting anomalies in human monitoring based on multimodal multiview self-supervised learning. <em>MVA</em>, <em>36</em>(3), 1-26. (<a href='https://doi.org/10.1007/s00138-025-01683-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in human behavior monitoring is a challenging task which requires capturing contextual information. In this paper, we propose a novel anomaly detection method which leverages multimodal (image and caption) and multiview self-supervised learning objectives. Previous works successfully used deep captioning alongside images. However, their reliance on unimodal pre-trained image and text features revealed deficiencies in capturing contextual information across modalities. Our method learns high-quality multimodal feature representations and captures contextual information across modalities by combining contrastive objectives which exploit complementary and consistent information from different modalities and views. We evaluate our method on four real-world datasets for human monitoring anomaly detection. Our extensive experimental results demonstrate substantial improvements compared to the baseline methods. Specifically, our method achieved higher area under the receiver operating characteristic curve (AUC) scores, increasing from 0.967 to 0.99, 0.973 to 0.987, 0.885 to 0.94, and 0.671 to 0.713. Additionally, the area under the precision-recall curve (AUPRC) scores improved from 0.892 to 0.96, 0.90 to 0.905, 0.512 to 0.661, and 0.89 to 0.907.},
  archive      = {J_MVA},
  author       = {Avellaneda Gonzalez, Jose Alejandro and Matsukawa, Tetsu and Suzuki, Einoshin},
  doi          = {10.1007/s00138-025-01683-y},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-26},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Detecting anomalies in human monitoring based on multimodal multiview self-supervised learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TFF-temporal fusion framework for advancing video retrieval through long-range dependencies and multi-modal intent. <em>MVA</em>, <em>36</em>(3), 1-15. (<a href='https://doi.org/10.1007/s00138-025-01677-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently navigating the surge in video content, our paper introduces the Temporal Fusion Framework (TFF) a pioneering approach to video content retrieval and moment detection via natural language queries. TFF utilizes a novel temporal context modeling strategy, adeptly capturing global context with long-range dependencies across diverse time scales. The integration of a specialized encoder-decoder model with multi-modal intent facilitates the extraction of intricate temporal patterns and user intent understanding. Tackling dynamic background changes, a common challenge in moment retrieval, TFF's temporal context modeling ensures accurate content retrieval. Through comprehensive comparative studies on public datasets, employing Recall (R@1, R@5), and mean average precision (mAP) as metrics, our experimental results showcase TFF's superior performance over existing methods. The incorporation of multi-modal intent and innovative temporal context modeling significantly enhances the framework's capability to identify relevant moments, marking a transformative step in video content interaction.},
  archive      = {J_MVA},
  author       = {Singh, Pratibha and Chakrawal, Kashvi and Kushwaha, Alok Kumar Singh},
  doi          = {10.1007/s00138-025-01677-w},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {TFF-temporal fusion framework for advancing video retrieval through long-range dependencies and multi-modal intent},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pothole can be seen with two eyes: An ensemble approach to pothole detection. <em>MVA</em>, <em>36</em>(3), 1-20. (<a href='https://doi.org/10.1007/s00138-025-01679-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of potholes and bad road quality is faced by nearly every developing country. These conditions often lead to accidents and traffic and cause general public inconvenience. Significant research has been performed in the field of effective pothole and road quality detection, but owing to several underlying issues such as relying on only computer vision or inertial sensor-based modeling, or requiring specialized equipment such as LiDAR (Light Detection and Ranging), implementing these solutions in the real world has never been feasible, especially in developing countries where a reliable system is required that works under any condition and, more importantly, can be implemented with minimal resources and changes to the existing infrastructure. The proposed system is an end-to-end robust architecture that uses mobile devices mounted on government vehicles for data collection. It features an innovative ensemble of two object detection models, YOLOS (You Only Look at One Sequence) and YOLOv8 (You Only Look Once), resulting in a 97.34% mAP@0.50 (mean Average Precision) score for pothole detection (a nearly 6% improvement over the best state-of-the-art model) in conjunction with state-of-the-art inertial sensor-based pothole detection and road quality detection models, delivering impressive accuracies of 98.5% and 96.3%, respectively. This two-forked approach to pothole and road quality detection combined with innovative connected applications like an analytics dashboard for the government and a navigation application for the consumer make this entire system highly relevant while being scalable, reliable, and easily deployable using the existing infrastructure in developing countries.},
  archive      = {J_MVA},
  author       = {Patawar, Atharv and Mehdi, Mohammed and Kore, Bhaumik and Saval, Pradnya},
  doi          = {10.1007/s00138-025-01679-8},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A pothole can be seen with two eyes: An ensemble approach to pothole detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibrating uncertainties in human trajectory forecasting. <em>MVA</em>, <em>36</em>(3), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01682-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article tackles the problem of uncertainty quantification and calibration in human trajectory prediction (HTP) tasks. Leveraging precise and meaningful uncertainty quantification is at the heart of risk-aware decision making in many areas relying on estimates provided by machine learning models, for example in autonomous driving. After reviewing the different ways to handle uncertainty representation in HTP, typically cast as a regression problem, we propose and compare three uncertainty calibration methods designed to be used as post-processing steps to any existing stochastic HTP method. Moreover, we present an importance sampling scheme to reflect the calibration effect on the same samples produced by the uncalibrated predictive model. We evaluate and compare these calibration methods combined with different predictive models on standard HTP benchmarking datasets. Our code is available at https://github.com/cimat-ris/trajpred-unc},
  archive      = {J_MVA},
  author       = {Canche, Mario and Morales Quispe, Marcela and Hayet, Jean-Bernard},
  doi          = {10.1007/s00138-025-01682-z},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Calibrating uncertainties in human trajectory forecasting},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion-aware face de-identification with generative adversarial networks. <em>MVA</em>, <em>36</em>(3), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01687-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel end-to-end emotion-preserving face de-identification approach based on Generative Adversarial Networks (GANs), specifically utilizing the StyleGAN architecture. The proposed method generates natural-looking de-identified images by creating a synthetic face dataset and leveraging the DeepFace model for gender classification and representative image selection. To enhance emotion preservation, an improved SimSwap framework is introduced, incorporating a novel loss function designed to maintain emotional expressions. The DeepFace model is further utilized to classify and recognize emotional expressions in both original and de-identified images. Emotion preservation during face swapping is explicitly enforced by minimizing attribute preservation loss. A comprehensive ablation study demonstrates the effectiveness of the proposed GAN components. Experimental results show that the method outperforms recent face de-identification techniques in both accuracy and emotion preservation.},
  archive      = {J_MVA},
  author       = {Shopon, Md and Gavrilova, Marina L.},
  doi          = {10.1007/s00138-025-01687-8},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Emotion-aware face de-identification with generative adversarial networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Projection model-driven image stitching: A novel warping method using epipolar displacement field. <em>MVA</em>, <em>36</em>(3), 1-16. (<a href='https://doi.org/10.1007/s00138-025-01688-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces a novel image stitching method driven by the projection model, utilizing epipolar displacement field (EDF) to ensure geometric consistency in panoramic images. By leveraging the principles of epipolar geometry and infinite homography, this method ensures alignment accuracy and maintains global projectivity across stitched images. The process begins by establishing a pixel warping rule in epipolar geometry through the infinite homography. Then, the epipolar displacement field, which quantifies the displacement of pixels along the epipolar lines, is constructed using thin-plate splines derived from the principle of local elastic deformation. The final panoramic image is produced by inversely warping pixels according to the epipolar displacement field. This method integrates epipolar constraints into the warping rules, ensuring high-quality alignment and preserving projection accuracy. Comparative experiments, both qualitative and quantitative, show that the method effectively reduces parallax artifacts and maintains geometric fidelity, achieving an average SSIM of 0.928 and PSNR of 29.900 across 12 scenes, outperforming existing techniques.},
  archive      = {J_MVA},
  author       = {Yu, Jian and Da, Feipeng},
  doi          = {10.1007/s00138-025-01688-7},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Projection model-driven image stitching: A novel warping method using epipolar displacement field},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hstr-net: Reference based video super-resolution with dual cameras. <em>MVA</em>, <em>36</em>(3), 1-19. (<a href='https://doi.org/10.1007/s00138-025-01685-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-spatio-temporal resolution (HSTR) video recording plays a crucial role in enhancing various imagery tasks that require fine-detailed information. State-of-the-art cameras provide this required high frame-rate and high spatial resolution together, albeit at a high cost. To alleviate this issue, this paper proposes a dual camera system for the generation of HSTR video using reference-based super-resolution (RefSR). One camera captures high spatial resolution low frame rate (HSLF) video while the other captures low spatial resolution high frame rate (LSHF) video simultaneously for the same scene. A novel deep learning architecture is proposed to fuse HSLF and LSHF video feeds and synthesize HSTR video frames. The proposed model combines optical flow estimation and (channel-wise and spatial) attention mechanisms to capture the fine motion and complex dependencies between frames of the two video feeds. Simulations show that the proposed model provides significant improvement over existing reference-based SR techniques in terms of PSNR and SSIM metrics. The method also exhibits sufficient frames per second (FPS) for aerial monitoring when deployed on a power-constrained drone equipped with dual cameras. The source code is publicly available at https://github.com/umutsuluhan/HSTRNet .},
  archive      = {J_MVA},
  author       = {Suluhan, H. Umut and Doruk, Abdullah Enes and Ates, Hasan F. and Gunturk, Bahadir K.},
  doi          = {10.1007/s00138-025-01685-w},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Hstr-net: Reference based video super-resolution with dual cameras},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive adam-based optimizers using second-order weight decoupling and gradient-aware weight decay for vision transformer. <em>MVA</em>, <em>36</em>(3), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01686-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizers play important roles in enhancing the performance of a deep network. A study on different optimizers is necessary to understand the effect of optimizers on the performance of the deep network for a given target task, such as image classification. Several attempts were made to investigate the effect of optimizers on the performance of CNNs. However, such experiments have not been carried out on vision transformers (ViT), despite the recent success of ViT in various image processing tasks. In this paper, we conduct exhaustive experiments with ViT using different optimizers. In our experiments, we found that weight decoupling and weight decay in optimizers play important roles in training ViT. We focused on the concept of weight decoupling and tried different variations of it to investigate to what extent weight decoupling is beneficial for a ViT. We propose two techniques that provide better results than weight-decoupled optimizers: (i) The weight decoupling step in optimizers involves a linear update of the parameter with weight decay as the scaling factor. We propose a quadratic update of the parameter which involves using a linear as well as squared parameter update using the weight decay as the scaling factor. (ii) We propose using different weight decay values for different parameters depending on the gradient value of the loss function with respect to that parameter. A smaller weight decay is used for parameters with a higher gradient value and vice versa. Image classification experiments are conducted over CIFAR-100 and TinyImageNet datasets to observe the performance of these proposed methods with respect to state-of-the-art optimizers such as Adam, RAdam, and AdaBelief. The code is available at https://github.com/Hemanth-Boyapati/Adaptive-weight-decay-optimizers .},
  archive      = {J_MVA},
  author       = {Hemanth Sai, Boyapati and Mukherjee, Snehasis and Dubey, Shiv Ram},
  doi          = {10.1007/s00138-025-01686-9},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adaptive adam-based optimizers using second-order weight decoupling and gradient-aware weight decay for vision transformer},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traversing the subspace of adversarial patches. <em>MVA</em>, <em>36</em>(3), 1-11. (<a href='https://doi.org/10.1007/s00138-025-01689-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite ongoing research on the topic of adversarial examples in deep learning for computer vision, some fundamentals of the nature of these attacks remain unclear. As the manifold hypothesis posits, high-dimensional data tends to be part of a low-dimensional manifold. To verify the thesis with adversarial patches–a special form of adversarial attack that can be used to fool object detectors in the physical world–this paper provides an analysis of a set of adversarial patches and investigates the reconstruction abilities of five different dimensionality reduction methods. Quantitatively, the performance of reconstructed patches in an attack setting is measured and the impact of sampled patches from the latent space during adversarial training is investigated. The evaluation is performed on two publicly available datasets for person detection. The results indicate that more sophisticated dimensionality reduction methods offer no advantages over a simple principal component analysis.},
  archive      = {J_MVA},
  author       = {Bayer, Jens and Becker, Stefan and Münch, David and Arens, Michael and Beyerer, Jürgen},
  doi          = {10.1007/s00138-025-01689-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Traversing the subspace of adversarial patches},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RGMEF: A lightweight multi-exposure image fusion network based on retinex and gradient guidance. <em>MVA</em>, <em>36</em>(3), 1-18. (<a href='https://doi.org/10.1007/s00138-025-01692-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of the multi-exposure image fusion (MEF) is to create a high dynamic range (HDR) image with balanced exposures and more details by extracting and enhancing valid information from multiple low dynamic range images with varied exposures. In recent years, deep learning has provided many solutions for MEF, and they have achieved remarkable effects. However, because of the high network complexity, these approaches often have low efficiency and limit the potential improvement of the effect. In order to fuse valuable information from different images, we introduce a lightweight but robust MEF network, which focus on the image internal characteristics rather than long-distance information. In this study, we first design a decomposed network inspired by retinex theory to extract the illumination map from an image and obtain the gradient map through Sobel operator. Subsequently, we introduce two lightweight feature extraction modules to extract image features with the guidance of illumination and gradient maps respectively, and then fuse these features to produce an HDR result through three fusion blocks. Furthermore, we design a loss function to obtain a more suitable brightness range for the current scene. Numerous comparative experiments demonstrate the superiority of our method in terms of image texture, pixel intensity, and color retention. Moreover, some ablation studies indicate that our approach has a significant adaptability to source images.},
  archive      = {J_MVA},
  author       = {Feng, Yunfei and Zhang, Mei and Zhu, Jinhui},
  doi          = {10.1007/s00138-025-01692-x},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {RGMEF: A lightweight multi-exposure image fusion network based on retinex and gradient guidance},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clean-to-clean: Pretraining vision transformers without additional data. <em>MVA</em>, <em>36</em>(3), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01693-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although self-supervised learning is an effective way to pretrain vision transformers for image classification, it usually requires additional datasets derived from the training dataset, which leads to extra computational cost. In this paper, we investigate the simplest pretext task called clean-to-clean (C2C) task for self-supervised learning, which trains the network to be the identity function for the original training images. We show that this task can be still significantly beneficial for the downstream classification task in various aspects: classification performance, object recognition capability, and dataset-efficiency. In order to identify the main components contributing to the effects of the C2C task, we examine each component of the trained models. As a result, we discover that the patch embeddings and attention scores are the components of the models that are distinguishably determined by the task. Accordingly, when a model is initialized in a way that it produces similar patterns of patch embeddings and attention scores to those of the C2C-pretrained model, this C2C-imitating model shows improved classification performance after finetuning even without the pretext task. Furthermore, in order to identify the main parts learning useful weight parameters from the C2C task, we freeze some parts of the C2C-pretrained model during finetuning and compare their performances for the downstream classification task. We find that the model with the frozen stem part achieves the best downstream classification performance, which indicates that the stem part of the model learns useful weight parameters by the C2C task.},
  archive      = {J_MVA},
  author       = {Lee, Hyeonjin and Kim, Songkuk and Lee, Jong-Seok},
  doi          = {10.1007/s00138-025-01693-w},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Clean-to-clean: Pretraining vision transformers without additional data},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial learning for unguided single depth map completion of indoor scenes. <em>MVA</em>, <em>36</em>(2), 1-30. (<a href='https://doi.org/10.1007/s00138-024-01652-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single depth map completion in the absence of any guidance from color images is a challenging, ill-posed problem in computer vision. Most of the conventional depth map completion approaches rely on information extracted from the corresponding color image and require heavy computations and optimization-based postprocessing functions, which cannot yield results in real time. Successful application of generative adversarial networks has led to significant progress in several computer vision problems including, color image inpainting. However, contrasting local and non-local features of depth maps compared to color images prevents the direct application of deep learning models designed for color image inpainting to depth map completion. Motivated by these challenges, in this work we propose to use deep adversarial learning to derive plausible estimates of missing depth information in a single degraded observation without any guidance from the corresponding RGB frame and any postprocessing. Different types of depth map degradations, such as simulated random and textual missing pixels as well as contiguous large holes found in Kinect depth maps, are effectively handled to reconstruct clean depth maps. An ablation study is also performed to investigate the contribution of our adversarial network architecture towards the recovery of missing scene depth information. We carry out an illustrative experimental analysis on the NYU-Depth V2 dataset and perform zero-shot generalization on the Middlebury and Matterport3D datasets, comparing our proposed method with several state-of-the-art algorithms. The experimental results demonstrate robustness and efficacy of the proposed approach.},
  archive      = {J_MVA},
  author       = {Medhi, Moushumi and Ranjan Sahay, Rajiv},
  doi          = {10.1007/s00138-024-01652-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Adversarial learning for unguided single depth map completion of indoor scenes},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A computer vision system for recognition and defect detection for reusable containers. <em>MVA</em>, <em>36</em>(2), 1-19. (<a href='https://doi.org/10.1007/s00138-024-01636-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small load carriers (SLCs) are standardized reusable containers used to transport and protect customer goods in many manufacturers. Throughout the life cycle of the SLCs, they will be collected, manually checked for defects (wear, cracks, and residue on the surface), and cleaned by specialized logistic companies. Human operators in small to medium-sized companies manually evaluate the defects due to the variety and degree of possible defects and varying customer needs. This manual evaluation is not scalable and prone to errors. This work aims to fill this gap by proposing a computer vision system that can recognize the SLC type for inventory management and perform defect detection automatically. First, we develop a camera portal, consisting of standard components, that capture the relevant surfaces of the SLC. A labeled dataset of 17,530 images of 34 different SLCs with their defect status was recorded using this camera portal. We trained a classification model (ConvNeXt) using our dataset to predict the different types of SLCs achieving 100% class prediction accuracy. For defect detection, we explore eight state-of-the-art (SOTA) anomaly detection models that achieved high rankings in the MVTec industrial anomaly detection benchmark. These models are trained using default hyperparameters and the two highest-scoring models were chosen and fine-tuned. The best-fine-tuned models based on “Area under the Receiver Operating Characteristic Curve (AUROC)” are PatchCore (0.811) and DRAEM (0.748). These results indicate that there is still potential for improvement in the automation of defect detection of SLCs.},
  archive      = {J_MVA},
  author       = {Wahyudi, Vincent and Ziegler, Cedric C. and Frieß, Matthias and Schramm, Stefan and Lang, Constantin and Eberhardt, Lars and Freund, Fabian and Dobhan, Alexander and Storath, Martin},
  doi          = {10.1007/s00138-024-01636-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A computer vision system for recognition and defect detection for reusable containers},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-enhanced feature mapping network for visible-infrared person re-identification. <em>MVA</em>, <em>36</em>(2), 1-17. (<a href='https://doi.org/10.1007/s00138-024-01646-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared Person Re-Identification (VI-ReID) plays a pivotal role in surveillance systems, enabling the accurate identification of individuals across varying times and locations. Traditional methods struggle in low-light conditions, which motivates our research. We introduce an Attention-Enhanced Feature Mapping Network (AEFMNet) that addresses both intra-modal and inter-modal discrepancies. Our AEFMNet employs an Attention-based Feature Fusion Module (AFFM) to enhance global feature representation and a GCN-based Feature Mapping Module (GFMM) to reduce cross-modal feature gaps. The proposed network is further strengthened by a Joint Training Algorithm (JTA) that integrates multi-scale local and global features, enhancing cross-modal matching accuracy. Our approach achieves advanced performance on three large-scale data sets, demonstrating its effectiveness and robustness.},
  archive      = {J_MVA},
  author       = {Liu, Shuaiyi and Han, Ke},
  doi          = {10.1007/s00138-024-01646-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Attention-enhanced feature mapping network for visible-infrared person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Region gradient-guided diffusion model for underwater image enhancement. <em>MVA</em>, <em>36</em>(2), 1-24. (<a href='https://doi.org/10.1007/s00138-024-01647-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) is a critical challenge in marine visual perception and underwater robotics due to complex aquatic environments that severely degrade image quality. This paper introduces the region gradient-guided diffusion model (RGGDM), a novel framework that addresses the fundamental trade-off between local detail preservation and global consistency in UIE. RGGDM innovatively integrates a region gradient-guided mechanism with a hybrid Swin-ConvNeXt architecture, introducing a spatially adaptive denoising process governed by gradient discrepancies between input and target images. We propose a learnable parameter $$\delta $$ that dynamically modulates denoising intensity, focusing computational resources on semantically salient regions. Our approach is underpinned by rigorous mathematical analysis, demonstrating convergence properties under mild assumptions and providing theoretical guarantees for the model’s stability and effectiveness. The synergistic combination of Swin Transformer and ConvNeXt enhances feature representation, significantly improving both perceptual quality and pixel-level accuracy. Extensive experiments on benchmark datasets demonstrate RGGDM’s superior performance, consistently outperforming state-of-the-art methods across multiple evaluation metrics. Notably, RGGDM achieves a peak signal-to-noise ratio (PSNR) of 25.48 dB and an underwater image quality measure (UIQM) of 4.37 on the UIEB dataset. Furthermore, enhanced images show substantial improvements in downstream tasks such as SIFT feature matching, with an average increase of 132.19% in matching points. These results underscore RGGDM’s potential in advancing underwater visual perception and its broader implications for marine robotics and environmental monitoring applications.},
  archive      = {J_MVA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s00138-024-01647-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Region gradient-guided diffusion model for underwater image enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-core token mixer: A novel approach for underwater image enhancement. <em>MVA</em>, <em>36</em>(2), 1-16. (<a href='https://doi.org/10.1007/s00138-024-01651-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) is critical in various applications, including marine biology research, underwater archaeology, and autonomous underwater vehicle (AUV) navigation. The unpredictable nature of underwater environments frequently leads to degradation in contrast, color, and perceptual visual quality. Previous methods using the single receptive field to extract features are not capable of handling varying light conditions, which hinders detail preservation, color correction, and image quality improvement. To address these challenges, we propose Multi Core Token Mixer (MCTM) by introducing a distinctive multi-core mechanism. This mechanism is adept at extracting varied receptive fields, thereby enabling the model to capture the degradation at different scales caused by inhomogeneous underwater conditions. We performed experiments on three datasets (UIEB, EUVP, and UFO-120), and MCTM consistently outperforms existing models in image enhancement, color correction, and perceptual visual quality. Our work sets a new standard in the field and emphasizes the promise held by task-specific architectures that harness the power of Transformer models to tackle domain-specific challenges, particularly in UIE.},
  archive      = {J_MVA},
  author       = {Xu, Tianrun and Xu, Shiyuan and Chen, Xue and Chen, Feng and Li, Hongjue},
  doi          = {10.1007/s00138-024-01651-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-core token mixer: A novel approach for underwater image enhancement},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A concept-aware explainability method for convolutional neural networks. <em>MVA</em>, <em>36</em>(2), 1-17. (<a href='https://doi.org/10.1007/s00138-024-01653-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Convolutional Neural Networks (CNN) outperform the classical models in a wide range of Machine Vision applications, their restricted interpretability and their lack of comprehensibility in reasoning, generate many problems such as security, reliability, and safety. Consequently, there is a growing need for research to improve explainability and address their limitations. In this paper, we propose a concept-based method, called Concept-Aware Explainability (CAE) to provide a verbal explanation for the predictions of pre-trained CNN models. A new measure, called detection score mean, is introduced to quantify the relationship between the filters of the model and a set of pre-defined concepts. Based on the detection score mean values, we define sorted lists of Concept-Aware Filters (CAF) and Filter-Activating Concepts (FAC). These lists are used to generate explainability reports, where we can explain, analyze, and compare models in terms of the concepts embedded in the image. The proposed explainability method is compared to the state-of-the-art methods to explain Resnet18 and VGG16 models, pre-trained on ImageNet and Places365-Standard datasets. Two popular metrics, namely, the number of unique detectors and the number of detecting filters, are used to make a quantitative comparison. Superior performances are observed for the suggested CAE, when compared to Network Dissection (NetDis) (Bau et al., in: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017), Net2Vec (Fong and Vedaldi, in: Paper presented at IEEE conference on computer vision and pattern recognition (CVPR), 2018), and CLIP-Dissect (CLIP-Dis) (Oikarinen and Weng, in: The 11th international conference on learning representations (ICLR), 2023) methods.},
  archive      = {J_MVA},
  author       = {Gurkan, Mustafa Kagan and Arica, Nafiz and Yarman Vural, Fatos T.},
  doi          = {10.1007/s00138-024-01653-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A concept-aware explainability method for convolutional neural networks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end unsupervised learning of latent-space clustering for image segmentation via fully dense-UNet and fuzzy C-means loss. <em>MVA</em>, <em>36</em>(2), 1-16. (<a href='https://doi.org/10.1007/s00138-024-01654-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fundamental unsupervised approach in machine learning for grouping tasks. Image segmentation is one of the main applications of clustering and a preliminary requirement for most high-level applications in computer vision and scene understanding. However, parameter tuning requirements of conventional unsupervised image segmentation approaches limit their application. Deep learning approaches are capable of diverse and discriminate feature learning, however supervised learning paradigm and computational complexity of deep neural networks (DNNs) induces bottlenecks for real-time applications. We present unsupervised learning paradigm for fully dense-UNet (FDU-Net) model training with loss constraints: Semantic loss, Fuzzy C-means Clustering (FCM) loss, and Total Variation (TV) loss. Semantic loss works by selecting maximum activation class for each pixel spatial location and Simple Linear Iterative Clustering (SLIC)-based spatial refinement provides a coherent feature representation for model optimisation. FCM loss is based on the objective function of the conventional unsupervised Fuzzy C-means algorithm loss function. TV loss computes and minimises the spatial discontinuities in the FDU-Net activation maps. Loss constraints operate in tandem to ensure the control of false positives and false negatives. We conduct extensive experiments to compare our proposed method with unsupervised conventional and contemporary deep learning-driven (DL) methods. We experimentally demonstrate that the proposed method yields competitive quantitative and, most importantly, qualitative segmentation results, on the unseen images from the BSDS500 benchmark dataset. During inference, the segmentation quality of the proposed approach results is more significant than the contemporary DL-based and conventional clustering methods while reducing the computation cost by several folds.},
  archive      = {J_MVA},
  author       = {Khan, Zubair and Khan, Tehreem and Sattar, Mohsin and Yang, Jie},
  doi          = {10.1007/s00138-024-01654-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {End-to-end unsupervised learning of latent-space clustering for image segmentation via fully dense-UNet and fuzzy C-means loss},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WTT: Combining wavelet transform with transformer for remote sensing image super-resolution. <em>MVA</em>, <em>36</em>(2), 1-14. (<a href='https://doi.org/10.1007/s00138-024-01655-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, most deep learning-based super-resolution techniques primarily operate in the spatial domain, utilizing similar methods to process high- and low-frequency information in images. However, this often results in edge blurring. To address this issue, this paper introduces a novel structure that integrates wavelet transform and transformer mechanisms. The proposed method effectively segregates high- and low-frequency image information via discrete wavelet transform (DWT) and learns their correlations through a self-attention mechanism to enhance super-resolution outcomes. Specifically, the input image/feature is decomposed into four frequency domain components using DWT, which are concatenated to form a full-frequency domain feature map. A high-frequency feature map is constructed from three of these components. A new feature map is then generated using multi-head self-attention, with the full-frequency domain feature map serving as the query and value, and the high-frequency feature map as the key. The output feature map is produced by applying inverse DWT, with the new feature map serving as the low-frequency component and the original high-frequency components retained. Additionally, a parallel 1 × 1 convolution filter is employed to minimize information loss. Furthermore, a super-resolution network for remote sensing images is constructed by combining wavelet transform and transformer, incorporating hierarchical residual connections to enable the network to focus on learning high-frequency information. Experimental results on a publicly available remote sensing dataset demonstrate the superiority of the proposed method compared to existing approaches.},
  archive      = {J_MVA},
  author       = {Liu, Jingyi and Yang, Xiaomin},
  doi          = {10.1007/s00138-024-01655-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {WTT: Combining wavelet transform with transformer for remote sensing image super-resolution},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environmental factors-aware two-stream GCN for skeleton-based behavior recognition. <em>MVA</em>, <em>36</em>(2), 1-12. (<a href='https://doi.org/10.1007/s00138-024-01656-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of human behavior recognition, modeling human skeletons as spatio-temporal graphs using Graph Convolutional Networks (GCNs) has achieved outstanding performance. Existing GCN-based methods typically focus on the two-dimensional or three-dimensional features of the skeleton. However, the same action may represent different behaviors in different environments, making it suboptimal to consider only skeleton features for behavior recognition tasks with diverse scenarios. To simultaneously account for both skeleton features and environmental factors that influence human behaviors, this study proposed a novel two-stream Graph Convolutional Network, 2S-EGCN, which incorporates environmental factors for human behavior recognition. In this network, we designed an innovative environmental factor sampling strategy that samples fixed-scale environmental factors from variable-scale feature maps. To better integrate environmental factors with skeleton features, we further developed a Skeleton-Environment Interaction Module. This module uses a specific feature fusion method to combine environmental factors with skeleton features, allowing for the modeling of both pure skeleton information and skeleton information fused with environmental factors, thus improving behavior recognition accuracy. Extensive experiments conducted on the large Kinetics dataset demonstrate that our model outperforms the state-of-the-art, improving top-1 accuracy by 1.71–55.61% and achieving top-5 accuracy of 93.41%.},
  archive      = {J_MVA},
  author       = {Li, Zhuoran and Yan, Lianshan and Li, Hua and Wang, Yu},
  doi          = {10.1007/s00138-024-01656-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Environmental factors-aware two-stream GCN for skeleton-based behavior recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetry-induced ambiguity in orientation estimation from RGB images. <em>MVA</em>, <em>36</em>(2), 1-17. (<a href='https://doi.org/10.1007/s00138-024-01657-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of object orientation from RGB images is a core component in many modern computer vision pipelines. Traditional techniques mostly predict a single orientation per image, learning a one-to-one mapping between images and rotations. However, when objects exhibit rotational symmetries, they can appear identical from multiple viewpoints. This induces ambiguity in the estimation problem, making images map to rotations in a one-to-many fashion. In this paper, we explore several ways of addressing this problem. In doing so, we specifically consider algorithms that can map an image to a range of multiple rotation estimates, accounting for symmetry-induced ambiguity. Our contributions are threefold. Firstly, we create a data set with annotated symmetry information that covers symmetries induced through self-occlusion. Secondly, we compare and evaluate various learning strategies for multiple-hypothesis prediction models applied to orientation estimation. Finally, we propose to model orientation estimation as a binary classification problem. To this end, based on existing work from the field of shape reconstruction, we design a neural network that can be sampled to reconstruct the full range of ambiguous rotations for a given image. Quantitative evaluation on our annotated data set demonstrates its performance and motivates our design choices.},
  archive      = {J_MVA},
  author       = {Bertens, Tijn and Caasenbrood, Brandon and Saccon, Alessandro and Jalba, Andrei},
  doi          = {10.1007/s00138-024-01657-6},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Symmetry-induced ambiguity in orientation estimation from RGB images},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ipdm: Identity preserving diffusion model for face sketch and photo synthesis. <em>MVA</em>, <em>36</em>(2), 1-14. (<a href='https://doi.org/10.1007/s00138-024-01658-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face sketch and photo synthesis is widely applied in industry and information fields, such as entertainment business and heterogeneous face retrieval. The key challenge lies in completing a face transformation with both good visual effects and face identity preservation. However, existing methods are still difficult to obtain a good synthesis due to the large model gap between the two different face domains. Recently, diffusion models have achieved great success in image synthesis, which allows us to extend its application in such a face generation task. Thus, we propose IPDM, which constructs a mapping of latent representation for domain-adaptive face features. The other proposed IDP utilizes auxiliary features to correct the latent features through their directions and supplementary identity information, so that the generation can keep face identity unchanged. The various evaluation results show that our method is superior to state-of-the-art methods in both identity preservation and visual effects.},
  archive      = {J_MVA},
  author       = {Tang, Duoxun and Jiang, Xinhang and Zhang, Ying and Dai, Yuhang and Lin, Ye},
  doi          = {10.1007/s00138-024-01658-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Ipdm: Identity preserving diffusion model for face sketch and photo synthesis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personvit: Large-scale self-supervised vision transformer for person re-identification. <em>MVA</em>, <em>36</em>(2), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01659-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-Identification (ReID) aims to retrieve relevant individuals in non-overlapping camera images and has a wide range of applications in the field of public safety. In recent years, with the development of Vision Transformer (ViT) and self-supervised learning techniques, the performance of person ReID based on self-supervised pre-training has been greatly improved. Person ReID requires extracting highly discriminative local fine-grained features of the human body, while traditional ViT is good at extracting context-related global features, making it difficult to focus on local human body features. To this end, this article introduces the recently emerged Masked Image Modeling (MIM) self-supervised learning method into person ReID, and effectively extracts high-quality global and local features through large-scale unsupervised pre-training by combining masked image modeling and discriminative contrastive learning, and then conducts supervised fine-tuning training in the person ReID task. This person feature extraction method based on ViT with masked image modeling (PersonViT) has the good characteristics of unsupervised, scalable, and strong generalization capabilities, overcoming the problem of difficult annotation in supervised person ReID, and achieves state-of-the-art results on publicly available benchmark datasets, including MSMT17, Market1501, DukeMTMC-reID, and Occluded-Duke. The code and pre-trained models of the PersonViT method are released at https://github.com/hustvl/PersonViT to promote further research in the person ReID field.},
  archive      = {J_MVA},
  author       = {Hu, Bin and Wang, Xinggang and Liu, Wenyu},
  doi          = {10.1007/s00138-025-01659-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Personvit: Large-scale self-supervised vision transformer for person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversified image style transfer—approaches, new methods and directed variability control. <em>MVA</em>, <em>36</em>(2), 1-12. (<a href='https://doi.org/10.1007/s00138-025-01660-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of image style transfer is to automatically redraw an input image in the style of another image, such as an artist’s painting. The disadvantage of conventional stylization algorithms is the uniqueness of result. If the user is not satisfied with the way the style was transferred, he has no option to remake the stylization. The paper provides an overview of existing style transfer methods that generate diverse results after each run and proposes two new methods. The first method enables diversity by concatenating a random vector into inner image representation inside the neural network and by reweighting image features accordingly in the loss function. The second method allows diverse stylizations by passing the stylized image through orthogonal transformations, which impact the way the target style is transferred. These blocks are trained to replicate patterns from additional pattern images, which serve as additional input and provide an interpretable way to control stylization variability for the end user. Qualitative and quantitative comparisons demonstrate that both methods are capable to generate different stylizations with higher variability achieved by the second method. The code of both methods is available on github.},
  archive      = {J_MVA},
  author       = {Ustyuzhanin, Alexander and Kitov, Victor and Kitov, Vladimir},
  doi          = {10.1007/s00138-025-01660-5},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Diversified image style transfer—approaches, new methods and directed variability control},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional cascaded multimodal attention for multiple choice visual question answering. <em>MVA</em>, <em>36</em>(2), 1-16. (<a href='https://doi.org/10.1007/s00138-025-01661-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a rapidly advancing field that aims to develop systems capable of answering questions based on image content. Performance of a VQA model largely depends on the effective integration of multimodal data. A sparsity-based Bidirectional Cascaded Multimodal Attention network has been proposed in this paper. This model leverages bidirectional attention between image and text modalities, enabling a deeper contextual understanding of one modality through the other. To encourage the focus of attention mechanism on the most relevant regions in the input, sparsity has been introduced in these interactions. In multiple choice VQA, answer options contain important context, and incorporating them with multimodal features using attention results in a comprehensive feature representation. The performance of the proposed model is assessed using the multiple-choice Visual7W dataset. To test the generalizability of the model, a modified VQAv2 dataset is prepared and evaluated. Through extensive experiments, the model demonstrates competitive performance, effectively handling diverse question types such as “what”, “where”, “who”, “why”, and “how”. A detailed analysis of attention maps for different question types highlights how the model focuses on various input regions. Visualizations of image, text, and cross-modal attention maps reveal the key areas that contributed to the model’s decision-making process.},
  archive      = {J_MVA},
  author       = {Upadhyay, Sushmita and Tripathy, Sanjaya Shankar},
  doi          = {10.1007/s00138-025-01661-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Bidirectional cascaded multimodal attention for multiple choice visual question answering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CorFormer: A hybrid transformer-CNN architecture for corrosion segmentation on metallic surfaces. <em>MVA</em>, <em>36</em>(2), 1-21. (<a href='https://doi.org/10.1007/s00138-025-01663-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of periodic corrosion inspection in steel structures cannot be overstated. However, current manual inspection approaches are fraught with challenges: they are time-consuming, subjective, and pose risks. To address these limitations, extensive research has been conducted over the past decade gauging the feasibility of Convolutional Neural Networks (CNNs) for automation of corrosion inspection. Meanwhile, Transformer networks have recently emerged as powerful tools in computer vision due to their ability to model intricate global relationships. In this paper, a novel hybrid architecture, dubbed CorFormer, is proposed for effective and efficient automation of corrosion inspection. The CorFormer network fuses Transformer and CNN layers at different stages of the encoder, which captures global context through Transformer layers while leveraging the inherent inductive bias of CNNs. To bridge the semantic gap between features generated by Transformer and CNN layers, a Semantic Gap Merger (SGM) module is introduced after each feature merge operation. The encoder is complemented by a hierarchical decoder, able to decrypt complex features at large and small scales. CorFormer is compared against state-of-the-art CNN and Transformer architectures for corrosion segmentation, and is found to outperform the best alternative by 2.7% in terms of Intersection over Union (IoU) across 10 validation data splits. Furthermore, it enables real-time inspection at an impressive rate of 28 frames per second. Rigorous statistical tests provide support for the findings presented in this study, and an extensive ablation study validates all design choices.},
  archive      = {J_MVA},
  author       = {Subedi, Abhishek and Qian, Cheng and Sadeghian, Reza and Jahanshahi, Mohammad R.},
  doi          = {10.1007/s00138-025-01663-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {CorFormer: A hybrid transformer-CNN architecture for corrosion segmentation on metallic surfaces},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretability of fingerprint presentation attack detection systems: A look at the “representativeness” of samples against never-seen-before attacks. <em>MVA</em>, <em>36</em>(2), 1-21. (<a href='https://doi.org/10.1007/s00138-025-01666-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, fingerprint Presentation Attack Detection systems (PADs) are primarily based on deep learning architectures subjected to massive training. However, their performance decreases to never-seen-before attacks. With the goal of contributing to explaining this issue, we hypothesized that this limited ability to generalize is due to the lack of "representativeness" of the samples available for the PAD training. "Representativeness" is treated here from a geometrical perspective: the spread of samples into the feature space, especially near the decision boundaries. In particular, we explored the possibility of adopting three-dimensionality reduction methods to make the problem affordable through visual inspection. These methods enable visual inspection and interpretation by projecting data into two-dimensional spaces, facilitating the identification of weak areas in the decision regions estimated after the training phase. Our analysis delineates the benefits and drawbacks of each dimensionality reduction method and leads us to make substantial recommendations in the crucial phase of the training design.},
  archive      = {J_MVA},
  author       = {Carta, Simone and Casula, Roberto and Orrù, Giulia and Micheletto, Marco and Marcialis, Gian Luca},
  doi          = {10.1007/s00138-025-01666-z},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Interpretability of fingerprint presentation attack detection systems: A look at the “representativeness” of samples against never-seen-before attacks},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Depthanything and SAM for UIE: Exploring large model information contributes to underwater image restoration. <em>MVA</em>, <em>36</em>(2), 1-25. (<a href='https://doi.org/10.1007/s00138-025-01662-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement (UIE) remains a fundamental yet challenging problem in computer vision due to the complex physics of light propagation in aquatic environments. Traditional physics-based or learning-driven approaches often need more prior knowledge and representational capacity to generalize across diverse underwater conditions. This paper presents a novel theoretical framework for leveraging large-scale pre-trained models in UIE, explicitly addressing the fundamental limitations in existing methods through principled integration of depth and semantic priors. Our key contribution is twofold: First, we establish a rigorous information-theoretic foundation that quantifies how auxiliary features from foundation models enhance the representational capacity of UIE systems, providing theoretical guarantees through PAC-Bayesian bounds on generalization performance. Second, we propose a Feature Enhancement Strategy that optimally combines depth information from DepthAnything and semantic priors from the Segment Anything Model, guided by underwater optical physics. We introduce CAB-USRI, a physics-based algorithm for both baseline and theoretical validation. Our extensive experimentation on multiple benchmark datasets demonstrates that our approach consistently outperforms state-of-the-art methods by significant margins while maintaining theoretical interpretability. Our ablation studies reveal the crucial role of depth priors in underwater scenarios, establishing a clear connection between theoretical bounds and empirical performance. This work bridges the gap between foundation models and domain-specific tasks, providing theoretical insights and practical solutions for complex image restoration problems in challenging environments.},
  archive      = {J_MVA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s00138-025-01662-3},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Depthanything and SAM for UIE: Exploring large model information contributes to underwater image restoration},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-based power line cables and pylons detection for low flying aircraft. <em>MVA</em>, <em>36</em>(2), 1-21. (<a href='https://doi.org/10.1007/s00138-025-01664-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power lines are dangerous for low-flying aircraft, especially in low-visibility conditions. Thus, a vision-based system able to analyze the aircraft’s surroundings and to provide the pilots with a “second pair of eyes” can contribute to enhancing their safety. To this end, we develop a deep learning approach to jointly detect power line cables and pylons from images captured at distances of several hundred meters by aircraft-mounted cameras. In doing so, we combine a modern convolutional architecture with transfer learning and a loss function adapted to curvilinear structure delineation. We use a single network for both detection tasks and demonstrate its performance on two benchmarking datasets. We have also integrated it within an onboard system and run it inflight. We show with our experiments that it outperforms the prior distant cable detection method by Stambler et al. (in: International Conference on Robotics and Automation, 2019) on both datasets, while also successfully detecting pylons, given their annotations are available for the data.},
  archive      = {J_MVA},
  author       = {Gwizdała, Jakub and Oner, Doruk and Roy, Soumava Kumar and Shah, Mian Akbar and Eberhard, Ad and Egorov, Ivan and Krüsi, Philipp and Yakushev, Grigory and Fua, Pascal},
  doi          = {10.1007/s00138-025-01664-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Vision-based power line cables and pylons detection for low flying aircraft},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D face parsing based on 2D CPFNet: Conformal parameterized face parsing network. <em>MVA</em>, <em>36</em>(2), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01667-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face parsing is a fundamental component of many advanced face technologies, which assigns labels to each pixel on the face data. Although three-dimensional (3D) face parsing has the potential to outperform its two-dimensional (2D) counterpart, it remains challenging due to the high cost of processing 3D mesh data. Recent works have introduced various methods for 3D surface segmentation, but their performance is still limited and they consume large amounts of memory and computation. In this paper, we propose a “3D–2D–3D” strategy for 3D face parsing. First, we transform 3D face data into a topological disk-like 2D face image containing spatial and textural information via conformal parameterization. Subsequently, we use a specific 2D deep learning network called CPFNet to achieve 2D face image semantic segmentation with multiscale technology and feature aggregation. Finally, the 2D semantic result is inversely remapped to the 3D face data to achieve 3D face parsing. Experimental results show that both CPFNet and our “3D–2D–3D” strategy accomplish high-quality 3D face parsing and outperform some 2D networks and 3D methods in both qualitative and quantitative comparisons.},
  archive      = {J_MVA},
  author       = {Yang, M. and Sun, W. and Wang, Y. and Zhou, G. and Tong, J. and Zhou, P.},
  doi          = {10.1007/s00138-025-01667-y},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3D face parsing based on 2D CPFNet: Conformal parameterized face parsing network},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffic volume measurement using nonlinear count-lines. <em>MVA</em>, <em>36</em>(2), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01668-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic volume measurements are increasingly recognized as important for improving society’s infrastructure, by functions such as managing congestion and enhancing logistics. Dedicated traffic data capture devices that use sensors embedded in the road enable accurate measurements but have the problems of high cost and limited installation locations, which make it difficult to expand the coverage of traffic volume measurements. To address this issue, the approach that combines already deployed Closed-Circuit Television (CCTV) cameras with image recognition technology has attracted attention and offers practical performance in ordinary situations. One remaining problem is that accuracy is degraded by the presence of headlight flare at nighttime and occlusion by large vehicles on busy roads. In this paper, we propose a method for measuring traffic volume that automatically sets count-lines using the Kernel Support Vector Machine (Kernel SVM) at optimal positions less affected by these issues. In addition, to make the proposal robust to illumination changes and occlusion we introduce nonlinear count-lines. Extensive experiments on Japanese road video footage shows that our method improves accuracy by $$5.9\%$$ at night and $$2.1\%$$ in situations prone to occlusion compared to the most basic fixed count-line method. Additionally, experiments on a public dataset, UA-DETRAC, demonstrate the proposal’s effectiveness in countries other than Japan.},
  archive      = {J_MVA},
  author       = {Iwao, Yuwa and Yamamoto, Yota and Yaginuma, Hideki and Taniguchi, Yukinobu},
  doi          = {10.1007/s00138-025-01668-x},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Traffic volume measurement using nonlinear count-lines},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting few-shot learning via selective patch embedding by comprehensive sample analysis. <em>MVA</em>, <em>36</em>(2), 1-14. (<a href='https://doi.org/10.1007/s00138-025-01669-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of limited data samples, few-shot learning continues to pose a significant challenge. A prevalent strategy in recent times has been to pre-train models on extensive datasets and subsequently transfer them to downstream few-shot tasks, which has demonstrated efficacy in enhancing performance. However, a persistent challenge lies in the inadequacy of pre-trained models to capture the essential features of the new downstream dataset. This issue is particularly acute in images containing multiple entities, where crucial features are often overlooked yet play a pivotal role in image classification. To address this challenge, we propose an innovative local information enhancement strategy that harnesses information from all samples to capture important local features in images and integrates them with global features. The objective of our strategy is to enhance class differentiation by ensuring distinct class prototypes in the embedding space through the incorporation of local information. By integrating local information, query samples exhibit closer alignment with the prototype of their respective classes, ultimately resulting in improved classification accuracy. To further bolster the performance of few-shot classification, we have refined the pre-trained model approach and augmented the dataset. Comprehensive ablation experiments demonstrate the specific impact of our approach on enhancing the accuracy of few-shot classification.},
  archive      = {J_MVA},
  author       = {Yang, Juan and Zhang, Yuliang and Wang, Ronggui and Xue, Lixia},
  doi          = {10.1007/s00138-025-01669-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Boosting few-shot learning via selective patch embedding by comprehensive sample analysis},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced normal estimation of point clouds via fine-grained geometric information learning. <em>MVA</em>, <em>36</em>(2), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01671-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud normal estimation is a fundamental task in 3D computer graphics, essential for downstream applications such as surface reconstruction and semantic segmentation. While recent advances in deep learning have significantly improved normal estimation accuracy, existing methods often struggle with capturing fine-grained geometric details. In this study, we propose a novel encoder that integrates a local gradient attention module and positional encoding to better capture subtle geometric variations. By introducing the gradient attention module, we effectively capture fine-grained information along the z-axis, while positional encoding using sine and cosine functions further amplifies these variations. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art methods, achieving up to a 2.53% improvement in accuracy on PCPNet dataset. Our work not only advances normal estimation but also demonstrates its potential for surface reconstruction tasks. The code is available at https://github.com/ABc90/gam-net-normal-main .},
  archive      = {J_MVA},
  author       = {Jin, Wei and Zhou, Jun and Wang, Mingjie and Li, Nannan and Wang, Weixiao and Liu, Xiuping},
  doi          = {10.1007/s00138-025-01671-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhanced normal estimation of point clouds via fine-grained geometric information learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SNFR: Salient neighbor decoding and text feature refining for scene text recognition. <em>MVA</em>, <em>36</em>(2), 1-13. (<a href='https://doi.org/10.1007/s00138-025-01672-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition methods are broadly categorized into serial and parallel. Serial methods achieve superior accuracy but are slower in speed. Parallel methods offer faster speed but may sacrifice accuracy. Current methods struggle to strike a balance between accuracy and inference speed, particularly facing challenges in both accuracy and speed. Therefore, we propose a new scene text recognizer called SNFR. It includes a simple yet efficient decoder, Salient Neighbor Decoder (SND), which achieves high accuracy recognition with lower computational cost for attention map calculation. SND generates a neighbor matrix by selecting salient positions, which guides the generation of all the character attention maps. We also propose a Text Feature Refining Module (TFRM) to capture the contextual relationship of text sequences, enhancing the overall feature representation of scene text. The experimental results demonstrate that our method achieves competitive performance on standard datasets and also shows superior performance on long text recognition.},
  archive      = {J_MVA},
  author       = {Lu, Tongwei and Fan, Huageng and Chen, Yuqian and Shao, Pengyan},
  doi          = {10.1007/s00138-025-01672-1},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {SNFR: Salient neighbor decoding and text feature refining for scene text recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing 3D volumetric asymmetry in facial palsy patients via advanced multi-view landmarks and radial curves. <em>MVA</em>, <em>36</em>(1), 1-21. (<a href='https://doi.org/10.1007/s00138-024-01616-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on facial palsy, a unilateral palsy of the facial nerve, is a complex field with many different causes and symptoms. Even modern approaches to evaluate the facial palsy state rely mainly on stills and 2D videos of the face and rarely on dynamic 3D information. Many of these analysis and visualization methods require manual intervention, which is time-consuming and error-prone. Moreover, they often depend on alignment algorithms or Euclidean measurements and consider only static facial expressions. Volumetric changes by muscle movement are essential for facial palsy analysis but require manual extraction. We propose to extract an estimated unilateral volumetric description for dynamic expressions from 3D scans. Accurate landmark positioning is required for processing the unstructured facial scans. In our case, it is attained via a multi-view method compatible with any existing 2D predictors. We analyze prediction stability and robustness against head rotation during video sequences. Further, we investigate volume changes in static and dynamic facial expressions for 34 patients with unilateral facial palsy and visualize volumetric disparities on the face surface. In a case study, we observe a decrease in the volumetric difference between the face sides during happy expressions at the beginning (13.8 ± 10.0 $$\hbox {mm}^{3}$$ ) and end (12.8 ± 10.3 $$\hbox {mm}^{3}$$ ) of a ten-day biofeedback therapy. The neutral face kept a consistent volume range of 11.8 $$-$$ 12.1 $$\hbox {mm}^3$$ . The reduced volumetric difference after therapy indicates less facial asymmetry during movement, which can be used to monitor and guide treatment decisions. Our approach minimizes human intervention, simplifying the clinical routine and interaction with 3D scans to provide a more comprehensive analysis of facial palsy.},
  archive      = {J_MVA},
  author       = {Büchner, Tim and Sickert, Sven and Volk, Gerd F. and Guntinas-Lichius, Orlando and Denzler, Joachim},
  doi          = {10.1007/s00138-024-01616-1},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Assessing 3D volumetric asymmetry in facial palsy patients via advanced multi-view landmarks and radial curves},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross transformer for LiDAR-based loop closure detection. <em>MVA</em>, <em>36</em>(1), 1-15. (<a href='https://doi.org/10.1007/s00138-024-01629-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loop closure detection, also known as place recognition, a key component of simultaneous localization and mapping (SLAM) systems, aims to recognize previously visited locations and reduce the accumulated drift error caused by odometry. Current vision-based methods are susceptible to variations in illumination and perspective, limiting their generalization ability and robustness. Thus, in this paper, we propose CrossT-Net (Cross Transformer Net), a novel cross-attention based loop closure detection network for LiDAR. CrossT-Net directly estimates the similarity between two frames by leveraging multi-class information maps, including range, intensity, and normal maps, to comprehensively characterize environmental features. A Siamese Encoder Net with shared parameters extracts frame features, and a Cross Transformer module captures intra-frame context and inter-frame correlations through self-attention and cross-attention mechanisms. In the final stage, an Overlap Estimation Module predicts the point cloud overlap between two frames. Experimental results on several benchmark datasets demonstrate that our proposed method outperforms existing methods in precision and recall, and exhibits strong generalization performance in different road environments. The implementation of our approach is available at: https://github.com/Bryan-ZhengRui/CrossT-Net_Pytorch .},
  archive      = {J_MVA},
  author       = {Zheng, Rui and Ren, Yang and Zhou, Qi and Ye, Yibin and Zeng, Hui},
  doi          = {10.1007/s00138-024-01629-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cross transformer for LiDAR-based loop closure detection},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Real estate pricing prediction via textual and visual features. <em>MVA</em>, <em>36</em>(1), 1. (<a href='https://doi.org/10.1007/s00138-024-01627-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  author       = {Yousif, Amira and Baraheem, Samah and Vaddi, Sai Surya and Patel, Vatsa S. and Shen, Ju and Nguyen, Tam V.},
  doi          = {10.1007/s00138-024-01627-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Correction: Real estate pricing prediction via textual and visual features},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on advances in visual computing. <em>MVA</em>, <em>36</em>(1), 1. (<a href='https://doi.org/10.1007/s00138-024-01632-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MVA},
  doi          = {10.1007/s00138-024-01632-1},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Editor’s note: Special issue on advances in visual computing},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-to-face synthesis based on facial landmarks prediction. <em>MVA</em>, <em>36</em>(1), 1-17. (<a href='https://doi.org/10.1007/s00138-024-01624-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human face, being one of the most prominent physical features, plays a crucial role in appearance description and recognition. Consequently, text-to-face synthesis has garnered increasing interest in the research community, with applications in criminal investigation, image editing, and more. Compared to text-to-image synthesis, generating facial images from text requires more specialized knowledge due to the subjectivity and diversity of facial descriptions, which involve more fine-grained appearance features. In this paper, we propose a text-to-face synthesis model based on Facial Landmarks Prediction (FLP-GAN). Specifically, we design two foundational submodules to facilitate the generation task. First, a co-attention mechanism is employed to pretrain the image and text encoders to extract features related to facial information. Second, a facial landmarks prediction model is proposed to generate face segment maps based on descriptive text, providing facial semantic prior knowledge for the subsequent face synthesis process. Conditioned on the semantic features obtained from the submodules, we construct the text-to-face synthesis model, which incorporates a memory network and a segment fuse layer to highlight important text information and refine the features. Additionally, a multi-stage refinement process is designed to generate high-resolution face images. Experimental results on the Face2Text dataset demonstrate that our FLP-GAN model outperforms the state-of-the-art methods in both qualitative and quantitative evaluations. Specifically, our model achieved a 22.7% improvement in Fréchet FaceNet Distence compared to the SOTA models.},
  archive      = {J_MVA},
  author       = {Wang, Kun and Chen, Lei and Cao, Biwei and Liu, Bo and Cao, Jiuxin},
  doi          = {10.1007/s00138-024-01624-1},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Text-to-face synthesis based on facial landmarks prediction},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MYFED: A dataset of affective face videos for investigation of emotional facial dynamics as a soft biometric for person identification. <em>MVA</em>, <em>36</em>(1), 1-15. (<a href='https://doi.org/10.1007/s00138-024-01625-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psychological studies have demonstrated that the facial dynamics play a significant role in recognizing an individual’s identity. This study introduces a novel database (MYFED) and approach for person identification based on facial dynamics, to extract the identity-related information associated with the facial expressions of the six basic emotions (happiness, sadness, surprise, anger, disgust, and fear). Our contribution includes the collection of the MYFED database, featuring facial videos capturing both spontaneous and deliberate expressions of the six basic emotions. The database is uniquely tailored for person identification using facial dynamics of emotional expressions, ensuring an average of ten repetitions for each emotional expression per subject-a characteristic often absent in existing facial expression databases. Additionally, we present a novel person identification method leveraging dynamic features extracted from videos depicting the six basic emotions. Experimental results confirm that dynamic features of all emotional expressions contain identity-related information. Notably, surprise, happiness, and sadness expressions exhibit the highest levels of identity-related data in descending order. To our knowledge, this is the first research that comprehensively analyzes facial expressions of all six basic emotions for person identification. For further research and exploration, the MYFED database is made accessible to researchers via the MYFED database website .},
  archive      = {J_MVA},
  author       = {Saracbasi, Zeynep Nur and Eroglu Erdem, Cigdem and Taskiran, Murat and Kahraman, Nihan},
  doi          = {10.1007/s00138-024-01625-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {MYFED: A dataset of affective face videos for investigation of emotional facial dynamics as a soft biometric for person identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for unambiguous pose estimation of a non-cooperative fixed-wing UAV. <em>MVA</em>, <em>36</em>(1), 1-11. (<a href='https://doi.org/10.1007/s00138-024-01630-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based unmanned aerial vehicle (UAV) pose estimation is carried out by deep learning (DL) to automate the UAV aim-point selection for a laser beam control research testbed (LBCRT). DL models are proposed to estimate the UAV pose from available UAV target images and serve as a new sensor within the testbed. Such models are designed to address the problem of pose ambiguity, which arises from different 3D UAV poses looking similar in a 2D image plane. The models are trained under three datasets: synthetic data, synthetic data combined with real-world data, and real-world data. Afterward, the models are compared based on their ability to infer a real-world UAV trajectory in order to analyze the impact of using synthetic data during training. Quantitative results show that the proposed DL models solve the pose ambiguity problem when trained with appropriate data. In average, the models trained on real-world data had the lowest mean angle error, followed by those trained on combined data, and those trained solely on synthetic data. This shows that synthetic data should be carefully selected to bridge the gap when real-world data is unavailable or scarce. Customized real-world and synthetic data were created for the research. Real-world data are short wave infra-red (SWIR) images of a 3D printed UAV model with varying poses and associated pose labels. In addition, synthetic data are UAV images with varying poses and associated labels created by simulation. For this research, 77,077 real-world and 100,000 synthetic data were created.},
  archive      = {J_MVA},
  author       = {Herrera, Leonardo and Kim, Jae Jun and Agrawal, Brij N.},
  doi          = {10.1007/s00138-024-01630-3},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep learning for unambiguous pose estimation of a non-cooperative fixed-wing UAV},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous sign language recognition based on motor attention mechanism and frame-level self-distillation. <em>MVA</em>, <em>36</em>(1), 1-12. (<a href='https://doi.org/10.1007/s00138-024-01633-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changes in facial expression, head movement, body movement and gesture movement are remarkable cues in sign language recognition, and most of the current continuous sign language recognition(CSLR) research methods mainly focus on static images in video sequences at the frame-level feature extraction stage, while ignoring the dynamic changes in the images. In this paper, we propose a novel motor attention mechanism to capture the distorted changes in local motion regions during sign language expression and obtain dynamic representations of image changes. And for the first time, we apply the self-distillation method to frame-level feature extraction for continuous sign language, which improves the feature expression without increasing the computational resources by self-distilling the features of adjacent stages and using the higher-order features as teachers to guide the lower-order features. The combination of the two constitutes our proposed holistic model of CSLR Based on motor attention mechanism and frame-level self-distillation (MAM-FSD), which improves the inference ability and robustness of the model. We conduct experiments on three publicly available datasets, and the experimental results show that our proposed method can effectively extract the sign language motion information in videos, improve the accuracy of CSLR and reach the state-of-the-art level.},
  archive      = {J_MVA},
  author       = {Zhu, Qidan and Li, Jing and Yuan, Fei and Gan, Quan},
  doi          = {10.1007/s00138-024-01633-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Continuous sign language recognition based on motor attention mechanism and frame-level self-distillation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Naturally constrained reject option classification. <em>MVA</em>, <em>36</em>(1), 1-14. (<a href='https://doi.org/10.1007/s00138-024-01620-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Reject Option Classification formulations typically learn a function (e.g., softmax threshold) to select/reject uncertain classifier predictions. Such approaches leverage a user-defined cost of rejection or constraints on the accuracy or coverage of the selected predictions. We formulate a new objective for applications that have no such costs/constraints by using a natural constraint on the rejected predictions. Our proposed Reject Option Classification formulation eliminates regions of random chance classification in the decision space of any neural classifier and dataset. The goal is to maximize accuracy in the selected region while permitting a reasonable degree of prediction randomness in the rejected region. Optimally, the hope would be to reject more incorrect than correct predictions. We employ a novel selection/rejection function and learn per-class softmax thresholds using a validation set. Results demonstrate the advantages of our proposed method compared to naïvely thresholding calibrated/uncalibrated softmax scores. We evaluate 2-D points, imagery, and text classification datasets using state-of-the-art pretrained and learned models. Source code is available at https://github.com/osu-cvl/learning-idk .},
  archive      = {J_MVA},
  author       = {Kashani Motlagh, Nicholas and Davis, Jim and Anderson, Tim and Gwinnup, Jeremy},
  doi          = {10.1007/s00138-024-01620-5},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Naturally constrained reject option classification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic exposure strategy network for robust visual odometry in environments with high dynamic range. <em>MVA</em>, <em>36</em>(1), 1-14. (<a href='https://doi.org/10.1007/s00138-024-01623-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of visual-based localization requires the image features to be sufficiently captured in well-exposed images and reliably tracked among image sequences. In this paper, a learning-based exposure strategy algorithm is developed to improve the performance of visual odometry or simultaneous localization and mapping in scenes with sudden illumination changes and high dynamic range (HDR). The exposure value is predicted by optimizing a novel image information metric, which is calculated based on a multi-scale image histogram. Experimental observations show that the proposed metric is highly related to the localization performance, and achieves better localization accuracy when compared to other state-of-the-art metrics. Using this novel metric, the exposure control network is designed to predict the exposure value for the next image given a sequence of recently captured images as inputs. For different application scenarios, alternative network frameworks are proposed with the options to include an optimization module providing improved exposure values for feedback learning, but with additional computation complexity. The approach is light-weight for real-time video applications and the images captured using the predicted exposure values are well-exposed with better detected and matched features in a variety of challenging HDR scenes. Code will be available in https://github.com/leejieyu13/HDR-AE .},
  archive      = {J_MVA},
  author       = {Li, Jieyu and Zhen, Ruiwen and Stevenson, Robert},
  doi          = {10.1007/s00138-024-01623-2},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automatic exposure strategy network for robust visual odometry in environments with high dynamic range},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Closing the gap in domain adaptation for semantic segmentation: A time-aware method. <em>MVA</em>, <em>36</em>(1), 1-25. (<a href='https://doi.org/10.1007/s00138-024-01626-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation models need a large number of images to be effectively trained but manual annotation of such images has a high cost. Active domain adaptation addresses this problem by pretraining the model with a synthetically generated dataset and then fine-tuning it with a few selected label annotations (the “budget”) on real images to account for the domain shift. Previous works annotate a percentage of either individual pixels or whole target images. We argue that the first is infeasible in practice, and the second spends part of the budget on classes that the pretrained model may have already learned well. We propose a method based on the annotation of regions computed by Segment Anything, a recently introduced foundation model for class-agnostic image segmentation. The key idea is to assign a ground truth label to each of a tiny subset of regions, those for which the model is more uncertain. In order to increase the number of annotated regions we propagate the ground truth labels to most similar regions according to a hierarchical clustering algorithm that uses the features learned by the pretrained model. Our method outperforms the state-of-the-art on the GTA5 to Cityscapes benchmark by using fewer annotations, almost closing the gap between the synthetically pre-trained model and that obtained with full supervision of the real images. Furthermore, we present competitive results for budgets less than 1% of samples and also for a larger and more challenging target dataset, Mapillary Vistas.},
  archive      = {J_MVA},
  author       = {Serrat, Joan and Gómez, Jose Luis and López, Antonio M.},
  doi          = {10.1007/s00138-024-01626-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Closing the gap in domain adaptation for semantic segmentation: A time-aware method},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid overlapping group sparsity denoising model with fractional-order total variation and non-convex regularizer. <em>MVA</em>, <em>36</em>(1), 1-11. (<a href='https://doi.org/10.1007/s00138-024-01628-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the total variational model can significantly suppress noise and make blurred images sharper, it will unavoidably produce blocky artifacts. In this paper, a new hybrid model based on overlapping group sparsity (OGS) was proposed to effectively prevent the above problems, which combines the advantages of the fractional-order total variational and the non-convex regularizer. The overlapping group sparse fractional-order total variational (OGS-FOTV) regular term can restore complex features in images while reducing noise. Meanwhile, the non-convex regularization term based on the overlapping group sparsity on hyper-Laplacian (OGS-HL) prior can usefully control the staircase artifacts and better protect image edges. To tackle the above denoising model, we employ the alternating direction method of multipliers to decompose our model into several sub-problems and solve them one by one. For images affected by different levels of white Gaussian noise, we conducted a contrast experiment with other advanced methods. The results show that the new model is superior to other related models in denoising.},
  archive      = {J_MVA},
  author       = {Chen, Yaobang and Zhao, Ping},
  doi          = {10.1007/s00138-024-01628-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A hybrid overlapping group sparsity denoising model with fractional-order total variation and non-convex regularizer},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entangled appearance and motion structures network for multi-object tracking and segmentation. <em>MVA</em>, <em>36</em>(1), 1-16. (<a href='https://doi.org/10.1007/s00138-024-01634-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object segmentation mask’s observation sequence shows the trend of changes in the object’s observable geometric form, and predicting them may assist in solving various difficulties in multi-object tracking and segmentation (MOTS). With this aim, we propose the entangled appearance and motion structures network (EAMSN), which can predict the object segmentation mask at the pixel level by integrating VAE and LSTM. Regardless of the surroundings, each EAMSN keeps complete knowledge about the sequence of probable changes in the seen map of the object and its related dynamics. It suggests that EAMSN understands the item meaningfully and is not reliant on instructive examples. As a result, we propose a novel MOTS algorithm. By employing different EAMSNs for each kind of item and training them offline, ambiguities in the segmentation mask discovered for that object may be recovered, and precise estimation of the real boundaries of the object at each step. We analyze our tracker using the KITTI MOTS and MOTS challenges datasets, which comprise car and pedestrian objects, to illustrate the usefulness of the suggested technique. As a result, we developed distinct EAMSNs for cars and pedestrians, trained using the MODELNET40 and Human3.6 M datasets, respectively. The discrepancy between training and testing data demonstrates that EAMSN is not dependent on training data. Finally, we compared our strategy to a variety of other ways. Compared to the published findings, our technique gets the best overall performance.},
  archive      = {J_MVA},
  author       = {Aryanfar, Ehsan and Aliyari Shoorehdeli, Mahdi and Seydi, Vahid},
  doi          = {10.1007/s00138-024-01634-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Entangled appearance and motion structures network for multi-object tracking and segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swin transformer with part-level tokenization for occluded person re-identification. <em>MVA</em>, <em>36</em>(1), 1-17. (<a href='https://doi.org/10.1007/s00138-024-01639-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To determine whether a pedestrian of interest has been captured by another distinct camera across a network of non-overlapping cameras, or by the same camera at a distinct time, is known as the problem of person re-identification and is considered one of the most fascinating challenges in computer vision. When query image of person of interest gets concealed, blocked, obscured or obstructed, the issue becomes considerably more challenging. Termed as occluded person re-identification, this covers scenarios that are closer to the real world crowded scenarios such as market place, airports, commercial malls, university campuses etc. Using a combination of global pedestrian level information along with part-level local feature information has increasingly has been shown to be a successful strategy for dealing with occluded person re-identification as it captures fine grained information from the non-occluded visible part. This paper proposes a swin transformer with part-level tokenization (SwinPLT) model that uses a swin transformer-based backbone enhanced with singular value decomposition (SVD). The proposed model leverages the hierarchical representation learning capabilities of swin transformer, combined with SVD to extract uncorrelated local tokens. Our approach aims to enhance the model’s discriminative ability by effectively handling occlusions in person images. Employing a combination of hard triplet loss and cross-entropy loss, the proposed SwinPLT surpasses the state-of-the-art results by at least 18.14% Rank1-accuracy and 17.28% mAP on the occluded DukeMTMC-reID dataset. On the Occluded-ReID dataset, the proposed SwinPLT model outperforms the other alternative approaches by 9.06% Rank1-accuracy and 7.71% mAP. On P-DukeMTMC-reID dataset, our model shows an improvement of 1.7% Rank1-accuracy and 2.4% mAP, whereas on Partial-iLIDS, it shows an improvement of 11.8% Rank1-accuracy and 4.26% mAP. We will be making the code and the model publically available at https://github.com/Ranjitkm2007/SwinPLT .},
  archive      = {J_MVA},
  author       = {Mishra, Ranjit Kumar and Mondal, Arijit and Mathew, Jimson},
  doi          = {10.1007/s00138-024-01639-8},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Swin transformer with part-level tokenization for occluded person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised monocular depth estimation via joint attention and intelligent mask loss. <em>MVA</em>, <em>36</em>(1), 1-12. (<a href='https://doi.org/10.1007/s00138-024-01640-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation is a crucial and challenging undertaking in computer vision, with applications in visual navigation, autonomous vehicles, and robotics. Traditional depth estimation used pricey LiDAR and depth cameras with sparse data. Recently, using Convolutional Neural Networks (CNNs) to derive depth maps from monocular images has gained significant interest. However, obtaining high-precision depth data with a supervised model requires extensive ground-truth datasets for training, significantly limiting the development of this technique. To overcome the reliance on ground truth data, research on self-supervised depth estimation is of vital importance. In this paper, we introduce a new network structure and loss function aimed at improving the precision of depth estimation. Specifically, we design an attention mechanism called SASE-Block, which simultaneously enhances spatial awareness and channel information. Additionally, we propose an intelligent mask to filter out pixels associated with moving objects or those that violate camera motion assumptions, preventing contamination of the training loss. Experimental results on the KITTI benchmark demonstrate that the model is highly competitive among the latest unsupervised methods and approaches the accuracy of supervised models. We also tested our model in other driving scenarios to evaluate the quality and generalizability of its depth maps.},
  archive      = {J_MVA},
  author       = {Guo, Peng and Pan, Shuguo and Gao, Wang and Khoshelham, Kourosh},
  doi          = {10.1007/s00138-024-01640-1},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Self-supervised monocular depth estimation via joint attention and intelligent mask loss},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Future-proofing class-incremental learning. <em>MVA</em>, <em>36</em>(1), 1-16. (<a href='https://doi.org/10.1007/s00138-024-01635-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exemplar-free class incremental learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning.},
  archive      = {J_MVA},
  author       = {Jodelet, Quentin and Liu, Xin and Phua, Yin Jun and Murata, Tsuyoshi},
  doi          = {10.1007/s00138-024-01635-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Future-proofing class-incremental learning},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pothole segmentation and area estimation with thermal imaging using deep neural networks and unmanned aerial vehicles. <em>MVA</em>, <em>36</em>(1), 1-12. (<a href='https://doi.org/10.1007/s00138-024-01637-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research we tackle a problem common in transportation and autonomous vehicles. To prevent vehicle damage and road accidents efficiently detecting and cataloguing potholes is crucial for road safety. In this study, we examine applying our previously developed method for pothole detection, segmentation and area estimation on RGB and thermal imaging using an unmanned aerial vehicle (UAV). Our prior research generated two datasets: one focused on ground-level views and another utilizing aerial perspectives. After training a total of six deep neural networks, we determined that our YOLOv8 trained model would be the most suitable for area estimation. Our pothole segmentation model in combination with homography, the intrinsic and extrinsic parameters of the UAV cameras, and novel methods, were applied to area estimation which yielded an average area estimation error of 9.71% on vision imaging and 12.7% on thermal imaging. To ensure real-world applicability, we evaluated our method with thermal imaging under various weather and lighting conditions. This analysis allowed us to assess its effectiveness in non-ideal scenarios, as well as times at night with lower vehicle traffic.},
  archive      = {J_MVA},
  author       = {Meier, Joseph and Welborn, Ethan and Diamantas, Sotirios},
  doi          = {10.1007/s00138-024-01637-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Pothole segmentation and area estimation with thermal imaging using deep neural networks and unmanned aerial vehicles},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-layer attentive feature upsampling for low-latency semantic segmentation. <em>MVA</em>, <em>36</em>(1), 1-12. (<a href='https://doi.org/10.1007/s00138-024-01631-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a fundamental problem in computer vision and it requires high-resolution feature maps for dense prediction. Current coordinate-guided low-resolution feature interpolation methods, e.g., bilinear interpolation, produce coarse high-resolution features which suffer from feature misalignment and insufficient context information. Moreover, enriching semantics to high-resolution features requires a high computation burden, so that it is challenging to meet the requirement of low-latency inference. We propose a novel Guided Attentive Interpolation (GAI) method to adaptively interpolate fine-grained high-resolution features with semantic features to tackle these issues. Guided Attentive Interpolation determines both spatial and semantic relations of pixels from features of different resolutions and then leverages these relations to interpolate high-resolution features with rich semantics. GAI can be integrated with any deep convolutional network for efficient semantic segmentation. In experiments, the GAI-based semantic segmentation networks, i.e., GAIN, can achieve 78.8 mIoU with 22.3 FPS on Cityscapes and 80.6 mIoU with 64.5 on CamVid using an NVIDIA 1080Ti GPU, which are the new state-of-the-art results of low-latency semantic segmentation. Code and models are available at: https://github.com/hustvl/simpleseg .},
  archive      = {J_MVA},
  author       = {Cheng, Tianheng and Wang, Xinggang and Liao, Junchao and Liu, Wenyu},
  doi          = {10.1007/s00138-024-01631-2},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cross-layer attentive feature upsampling for low-latency semantic segmentation},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised representation learning for robust fine-grained human hand action recognition in industrial assembly lines. <em>MVA</em>, <em>36</em>(1), 1-14. (<a href='https://doi.org/10.1007/s00138-024-01638-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans are still indispensable on industrial assembly lines, but in the event of an error, they need support from intelligent systems. In addition to the objects to be observed, it is equally important to understand the fine-grained hand movements of a human to be able to track the entire process. However, these deep-learning-based hand action recognition methods are very label intensive, which cannot be offered by all industrial companies due to the associated costs. This work therefore presents a self-supervised learning approach for industrial assembly processes that allows a spatio-temporal transformer architecture to be pre-trained on a variety of information from real-world video footage of daily life. Subsequently, this deep learning model is adapted to the industrial assembly task at hand using only a few labels. Well-known real-world datasets best suited for representation learning of such hand actions in a regression tasks are outlined and to what extent they optimize the subsequent supervised trained classification task. This subsequent fine-tuning is supplemented by concept drift detection, which makes the resulting productively employed models more robust against concept drift and future changing assembly movements.},
  archive      = {J_MVA},
  author       = {Sturm, Fabian and Trat, Martin and Sathiyababu, Rahul and Allipilli, Harshitha and Menz, Benjamin and Hergenroether, Elke and Siegel, Melanie},
  doi          = {10.1007/s00138-024-01638-9},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Self-supervised representation learning for robust fine-grained human hand action recognition in industrial assembly lines},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Twinned attention network for occlusion-aware facial expression recognition. <em>MVA</em>, <em>36</em>(1), 1-18. (<a href='https://doi.org/10.1007/s00138-024-01641-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is a tedious task in image processing for complex real-world scenarios that are captured under different lighting conditions, facial obstructions, and a diverse range of facial orientations. To address this issue, a novel Twinned attention network (Twinned-Att) is proposed in this paper for an efficient FER in occluded images. The proposed Twinned-Att network is designed in two separate modules: Holistic module (HM) and landmark centric module (LCM). The holistic module comprises of dual coordinate attention block (Dual-CA) and the Cross Convolution block (Cross-conv). The Dual-CA block is essential for learning positional, spatial, and contextual information by highlighting the most prominent characteristics in the facial regions. The Cross-conv block learns the spatial inter-dependencies and correlations to identify complex relationships between various facial regions. The LCM emphasizes smaller and distinct local regions while maintaining resilience against occlusions. Vigorous experiments have been undertaken to improve the efficacy of the proposed Twinned-Att. The results produced by the Twinned-Att illustrate the remarkable responses which achieve the accuracies of 86.92%, 85.64%, 78.40%, 69.82%, 64.71%, 85.52%, and 85.83% for the datasets viz., RAF DB, FER PLUS, FER 2013, FED RO, SFEW 2.0, occluded RAF DB and occluded FER Plus respectively. The proposed Twinned-Att network is experimented with various backbone networks, including Resnet-18, Resnet-50, and Resnet-152. It consistently outperforms well and highlights its prowess in addressing the challenges of robust FER in the images captured in complex real-world environments.},
  archive      = {J_MVA},
  author       = {Devasena, G. and Vidhya, V.},
  doi          = {10.1007/s00138-024-01641-0},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Twinned attention network for occlusion-aware facial expression recognition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergizing LiDAR and augmented reality for precise real-time interior distance measurements for mobile devices. <em>MVA</em>, <em>36</em>(1), 1-15. (<a href='https://doi.org/10.1007/s00138-024-01642-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence and widespread usage of mobile devices have revolutionized the field of Augmented Reality (AR), allowing the technology to grow rapidly, and attracting significant attention from different businesses such as e-commerce, healthcare, education, and many more. AR mainly allows spatial recognition and measurements of objects in and of the environment. The integration of AR drastically improves user immersion in contrast to conventional methods. However, the accuracy of spatial measurements with AR is still limited, due to the lack of 3D information. The introduction of light detection and ranging (LiDAR) sensors in Apple’s mobile devices presents a new opportunity to enhance the overall performance of AR applications. In this work, we explore the potential and feasibility of spatial measurements by combining AR technology with LiDAR technology. Hence, we developed a custom AR application using LiDAR data for real-time interior measurements on a mobile device by integrating the point clouds captured by the LiDAR sensor in an AR measurement environment. Further, we carried out a comparison between our custom application and three conventional applications for spatial measurement on the market. With an error value of 0.45 percent, the accuracy during spatial measurements of our application is about one order of magnitude higher than the accuracy of the compared applications. In this work, we provide insights into the development and potential of LiDAR sensors in mobile devices, especially for accurate and real-time operating applications.},
  archive      = {J_MVA},
  author       = {Becker, Robert A. and Do, Minh Dung and Schweiss, Thomas and Albert, Tobias and Bleistein, Thomas and Werth, Dirk},
  doi          = {10.1007/s00138-024-01642-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Synergizing LiDAR and augmented reality for precise real-time interior distance measurements for mobile devices},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Volumetric medical image segmentation via scribble annotations and shape priors. <em>MVA</em>, <em>36</em>(1), 1-18. (<a href='https://doi.org/10.1007/s00138-024-01643-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly-supervised image segmentation, particularly using scribbles, has become increasingly prominent in both computer vision and medical image analysis. This popularity stems from the relative ease of obtaining scribble annotations compared to the extensive effort required for precise pixel- or voxel-level labeling. Despite their advantages, scribble-based methods often struggle with accurate boundary localization due to insufficient structural supervision of regions of interest (ROIs). Additionally, many existing approaches are primarily designed for 2D segmentation and fail to effectively utilize volumetric data when applied directly to individual image slices. In this paper, we propose a scribble-based volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image segmentation and aims to improve boundary prediction. To achieve this, we augment a 2.5D attention UNet with a proposed label propagation module to extend semantic information from scribbles and use a combination of static and active boundary prediction to learn ROI’s boundary and regularize its shape. Also, we propose an optional add-on component, which incorporates the shape prior information from unpaired segmentation masks to improve model accuracy further. Extensive experiments on three public datasets and one private dataset demonstrate our Scribble2D5 achieves state-of-the-art performance on volumetric image segmentation using scribbles and shape prior if available. Our source code is available online: https://github.com/Qybc/Scribble2D5 .},
  archive      = {J_MVA},
  author       = {Chen, Qiuhui and Lyu, Haiying and Hu, Xinyue and Lu, Yong and Hong, Yi},
  doi          = {10.1007/s00138-024-01643-y},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Volumetric medical image segmentation via scribble annotations and shape priors},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond kalman filters: Deep learning-based filters for improved object tracking. <em>MVA</em>, <em>36</em>(1), 1-22. (<a href='https://doi.org/10.1007/s00138-024-01644-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object’s future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on recurrent neural networks, neural ordinary differential equations, and conditional neural processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns—the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.},
  archive      = {J_MVA},
  author       = {Adžemović, Momir and Tadić, Predrag and Petrović, Andrija and Nikolić, Mladen},
  doi          = {10.1007/s00138-024-01644-x},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Beyond kalman filters: Deep learning-based filters for improved object tracking},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of recent techniques for person re-identification. <em>MVA</em>, <em>36</em>(1), 1-29. (<a href='https://doi.org/10.1007/s00138-024-01622-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReId), a crucial task in surveillance, involves matching individuals across different camera views. The advent of Deep Learning, especially supervised techniques like Convolutional Neural Networks and Attention Mechanisms, has significantly enhanced person Re-ID. However, the success of supervised approaches hinges on vast amounts of annotated data, posing scalability challenges in data labeling and computational costs. To address these limitations, recent research has shifted towards unsupervised person re-identification. Leveraging abundant unlabeled data, unsupervised methods aim to overcome the need for pairwise labelled data. Although traditionally trailing behind supervised approaches, unsupervised techniques have shown promising developments in recent years, signalling a narrowing performance gap. Motivated by this evolving landscape, our survey pursues two primary objectives. First, we review and categorize significant publications in supervised person re-identification, providing an in-depth overview of the current state-of-the-art and emphasizing little room for further improvement in this domain. Second, we explore the latest advancements in unsupervised person re-identification over the past three years, offering insights into emerging trends and shedding light on the potential convergence of performance between supervised and unsupervised paradigms. This dual-focus survey aims to contribute to the evolving narrative of person re-identification, capturing both the mature landscape of supervised techniques and the promising outcomes in the realm of unsupervised learning.},
  archive      = {J_MVA},
  author       = {Asperti, Andrea and Fiorilla, Salvatore and Nardi, Simone and Orsini, Lorenzo},
  doi          = {10.1007/s00138-024-01622-3},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A review of recent techniques for person re-identification},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based relational reasoning network for video question answering. <em>MVA</em>, <em>36</em>(1), 1-13. (<a href='https://doi.org/10.1007/s00138-024-01645-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vdeo quesition answering models need to accurately understand video content based on the question and reason out the correct answer. The relations between objects in the video can enhance the ability of the model to comprehend video information. However, due to the complex stereoscopic spatio-temporal structure among objects in the video, current methods still lack sufficient capability to reason about the dynamic spatio-temporal relations between objects. Therefore, this paper proposes a graph-based relational reasoning network for video question answering. For dynamic spatio-temporal relations, a stereoscopic spatio-temporal graph is designed that models the stereoscopic spatio-temporal structure among objects in the video. Spatio-temporal graph convolutional network is employed to synchronously reason about the static spatial relations among objects within the same frame and the dynamic temporal relations among objects in adjacent frames. For appearance relations, globally-aware object-level appearance features are used to construct a fully connected appearance graph. Appearance graph convolutional network is employed to reason about the appearance relations among objects. Our model is evaluated on benchmark datasets and subjected to extensive ablation studies. The experimental results demonstrate the effectiveness of the model presented in this paper.},
  archive      = {J_MVA},
  author       = {Tan, Tao and Sun, Guanglu},
  doi          = {10.1007/s00138-024-01645-w},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Graph-based relational reasoning network for video question answering},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative surface roughness detection method based on white light interference images. <em>MVA</em>, <em>36</em>(1), 1-18. (<a href='https://doi.org/10.1007/s00138-024-01650-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and intelligent real-time roughness detection is essential for industrialized production of high-precision machining and manufacturing industries. The traditional optical inspection methods are designed for offline measurement which is not appropriate for online quick detection. The development of artificial intelligence and machine vision makes online roughness inspection possible. At present most methods utilize deep learning classification tasks, which can only provide one detection result under one field of view and cannot adapt to the inhomogeneous distribution of roughness. To address this issue, we propose a novel semantic segmentation approach processing with white light interference images to segment different roughness values under one field of view at the pixel level, thereby improving the detection results. The strategy and detection mechanism are expressed firstly. The strongest light intensity (0th level) interference fringe is the focus to be segmented. Based on this analysis, a semantic segmentation model, FAMNet, is proposed in this paper for improving surface roughness detection precision. White light interference image is as the input of model. Fusion Attention Mechanism (FAM) algorithm is specifically designed to be embedded into backbone to construct network structure. The channel attention and positional attention are combined together to achieve a balance between detection accuracy and computation time. The channel attention improves the ability of the attention network to perceive contextual information and enhances the connectivity between different channels. The positional attention mechanism reduces the loss of positional information and enhances the scope of utilization of network features. Comparison experiments are executed and the experiment results show that the fusion attention mechanism module algorithm has a better detection ability for white light interference fringe in the balance of accuracy and computing time. The segmentation recognition accuracy MIoU reaches 86.3%, and it improves the performance of computation by about 77.66% compared to the current best-performing model and the amount of parameter is lower than it by about 63.59%. The proposed FAMNet model has a good performance for surface roughness online detection.},
  archive      = {J_MVA},
  author       = {Yang, Huguang and Su, Xiaojing and Li, Botao and Xia, Chenglong and Zheng, Han and Yang, Mingyang and Zhang, Taohong},
  doi          = {10.1007/s00138-024-01650-z},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Innovative surface roughness detection method based on white light interference images},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised domain adaptation by cross-domain consistency learning for CT body composition. <em>MVA</em>, <em>36</em>(1), 1-15. (<a href='https://doi.org/10.1007/s00138-024-01615-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) scans of the abdomen have become the gold standard for assessing body composition (BC). Accurate estimation of skeletal muscle and adipose tissues from CT scan slices is crucial for diagnosis and prognosis. Much research in abdominal image analysis focuses on the third lumbar vertebra (L3) due to its stability and ease of labeling compared to other lumbar vertebrae. This study leverages labeled L3 slices (source domain) to predict unlabeled slices from thoracic T1 to sacrum S5 region (target domain). We proposed a Twin Encoder–Decoder Network (TED-Net) with distinct weight initialization employing Cross-domain Consistency Learning (CDCL) for joint training across the domains. This strategy extends the network’s knowledge by enforcing consistency between predictions from two segmentation networks. The training objective includes supervised loss terms for the source domain and unsupervised loss terms for the target domain. This results in increases of 6.68%, 3.31%, and 4.40% in Precision, Dice Similarity Coefficient, and Intersection over Union, respectively, indicating significant improvement in performance on the target domain, suggesting that domain-invariant feature learning through cross-domain consistency learning enhances a network’s adaptability over unlabeled domains.},
  archive      = {J_MVA},
  author       = {Ali, Shahzad and Lee, Yu Rim and Park, Soo Young and Tak, Won Young and Jung, Soon Ki},
  doi          = {10.1007/s00138-024-01615-2},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Unsupervised domain adaptation by cross-domain consistency learning for CT body composition},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shape related unknown object one-shot learning grasping. <em>MVA</em>, <em>36</em>(1), 1-13. (<a href='https://doi.org/10.1007/s00138-024-01648-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grasping unknown objects without re-training is still a challenging task for robot grasping. The traditional methods, including data-driven and transfer-learning based grasp methods still suffered from the extra data labeling, training on new samples. To this end, this paper proposes a method to quickly predict the grasping position of unknown objects. This method uses a Deep Siamese Network (DSN) as the backbone to estimate the similarity between the query object and the support set objects, and designs a new loss function through the shape feature estimation mechanism to learn the correct grasping position of unknown objects. Our method is compared to the state-of-the-art (SOTA) method on a well-known open grasp dataset: Connell Grasp Dataset, and a practical dataset. Experimental results show that the proposed method significantly surpasses the SOTA methods in the one-shot grasping task and has good generalization ability for unknown objects. Given only one unknown object sample, the proposed method can obtain a reliable grasping position of an unknown object without the need to re-collect data for retraining.},
  archive      = {J_MVA},
  author       = {Liu, Weiwei and Sun, Qi and Yang, Minghao},
  doi          = {10.1007/s00138-024-01648-7},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Shape related unknown object one-shot learning grasping},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A motion direction detecting model for colored images based on the Hassenstein–Reichardt model. <em>MVA</em>, <em>36</em>(1), 1-13. (<a href='https://doi.org/10.1007/s00138-024-01649-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eyes are highly efficient sensors created by nature, capable of perceiving external light signals with remarkable precision. The signals received by the eyes undergo intricate processing in the visual cortex, resulting in the phenomenon of vision. Among the various aspects of visual processing, the detection of motion details holds paramount significance for the survival and navigation of organisms. Extensive research has been conducted over the years to comprehend the complex mechanisms underlying motion direction detection in the visual system. In our previous work, we developed the HRC-based artificial visual system for motion direction detection. However, the model was designed solely for processing images with a single input channel and may not directly apply to colored images. In this paper, we present a novel approach to motion direction detection that supports colored images by integrating photoreceptors with different spectral sensitivities. The experiment demonstrates that incorporating color information enhances motion vision capabilities, aligning with biological theories. In this research, we expanded our model to include colored images. The process of constructing the motion direction detecting model for grayscale and colored images follows a similar trajectory. Our initial step involved constructing the core detector, which employs the HRC model to detect motion in a single direction. According to the HRC model, direction-selective neurons receive signals from two separate photoreceptors to detect motion direction. Subsequently, we developed a contrast-response system that receives input from the same photoreceptors and inhibits motion-direction-detecting neurons based on the contrast information of the input signals. Furthermore, we extended the model to two-dimensional planes to detect eight movement directions. In the two-dimensional model, the contrast-response system receives input from a number of surrounding photoreceptors and outputs an inhibitory signal to the motion-direction-detecting neurons based on the contrast information from the photoreceptors. Finally, we constructed a global motion-direction-detecting model. To demonstrate the practicality of the model, a comprehensive comparison was conducted with four deep learning models, including two types of convolutional neural networks, EfficientNetB0 and ResNet-50. The results of the comparison reveal that the proposed model outperforms the deep learning models in terms of accuracy and noise immunity.},
  archive      = {J_MVA},
  author       = {Qiu, Zhiyu and Yan, Chenyang and Chen, Tianqi and Hua, Yuxiao and Todo, Yuki and Tang, Zheng},
  doi          = {10.1007/s00138-024-01649-6},
  journal      = {Machine Vision and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A motion direction detecting model for colored images based on the Hassenstein–Reichardt model},
  volume       = {36},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
