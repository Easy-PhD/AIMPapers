<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMIR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmir">IJMIR - 37</h2>
<ul>
<li><details>
<summary>
(2025). Ultra fast-inference depth completion with linear attention-based cascaded hourglass network. <em>IJMIR</em>, <em>14</em>(4), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00381-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion is vital for various CV applications such as autonomous driving, robotics, and augmented reality. However, most of the existing approaches suffer from high computational overhead and slow inference speeds, limiting their real-time applicability. In this paper, we present the Linear Attention-based Cascade Hourglass Network (LA-CHN), a lightweight yet robust depth completion model designed for efficient and accurate dense depth prediction. The core of LA-CHN is our Lightweight Linear Attention (LLA) block, which substitutes quadratic self-attention with a ReLU-kernel linear mechanism and a spatial-reduction strategy to maintain a global receptive field at linear cost. These LLA blocks are embedded within a three-stage cascaded hourglass backbone, enabling multiscale feature aggregation and progressive refinement. Experimental results on the outdoor KITTI DC benchmark and indoor NYUv2 dataset show that our approach achieves superior performance compared to previous lightweight depth completion models.},
  archive      = {J_IJMIR},
  author       = {Wu, Zirui and Hao, Yongtao},
  doi          = {10.1007/s13735-025-00381-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Ultra fast-inference depth completion with linear attention-based cascaded hourglass network},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of multimodal visual representation learning: Tracing the evolution from CNNs to transformers and beyond. <em>IJMIR</em>, <em>14</em>(4), 1-30. (<a href='https://doi.org/10.1007/s13735-025-00382-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary goal of multimodal visual representation learning is to generate implicit information that effectively represents multimodal information by exploring the commonalities and characteristics between different modalities. This research report will discuss currently widely used advanced methods in the field of multimodal visual representation learning. This article will discuss these methods in the following order, culminating in multimodal visual learning: (1) pre-trained visual representation learning, (2) generative visual representation learning, (3) contrastive multimodal visual representation learning, and (4) image-text multimodal visual representation learning methods. Each element provides useful clues that ultimately lead to multimodal visual learning. Pre-trained visual representation learning refers to the application of supervised pre-training models in visual representation learning, while generative visual representation learning uses generative models to learn feature representations that can integrate multimodal information. Contrastive multimodal visual representation learning uses contrastive learning methods to compare similar and dissimilar sample pairs, learning feature representations in a self-supervised manner. Image-text multimodal visual representation learning methods, on the other hand, attempt to enhance the capabilities of visual representation learning by fusing visual information (such as images) with textual information. This review report will explain the above research background, the classification of different research methods, commonly used evaluation methods , and future development trends.},
  archive      = {J_IJMIR},
  author       = {Zhang, Dong and Wong, W. K. and Chew, I. M.},
  doi          = {10.1007/s13735-025-00382-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A comprehensive review of multimodal visual representation learning: Tracing the evolution from CNNs to transformers and beyond},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective reinforcement learning for recommender systems: A comprehensive survey of methods, challenges, and future directions. <em>IJMIR</em>, <em>14</em>(4), 1-35. (<a href='https://doi.org/10.1007/s13735-025-00383-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most traditional recommender systems (RSs) prioritize accuracy-based metrics, often favoring popular items while neglecting novelty, diversity, and user engagement. However, real-world recommendation scenarios involve multiple conflicting objectives, requiring advanced optimization techniques. Multi-Objective Optimization (MOO) has been explored in RSs using collaborative filtering and evolutionary algorithms, but these approaches suffer from cold-start issues, data sparsity, and high computational complexity. Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) offer promising alternatives by dynamically learning optimal policies. However, single-objective DRL struggles to balance accuracy with non-accuracy metrics, making it less effective for real-world trade-offs. This has led to the rise of Multi-Objective Reinforcement Learning (MORL), which optimizes conflicting objectives using Pareto-based optimization, scalarization, and policy gradient adaptations. Unlike prior surveys on RL-based RSs, this review specifically examines MORL techniques, providing a structured taxonomy, evaluation of optimization strategies, and analysis of their impact on personalization, diversity, fairness, and engagement. We also highlight unresolved challenges, including scalability, sample efficiency, computational complexity, and real-time applicability. Finally, we propose future research directions to enhance MORL’s scalability, improve policy generalization, and integrate more efficient multi-objective learning techniques for large-scale recommendation systems.},
  archive      = {J_IJMIR},
  author       = {Fatima Ezzahra, Zaizi and Sana, Abakarim and Sara, Qassimi and Said, Rakrak},
  doi          = {10.1007/s13735-025-00383-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-35},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-objective reinforcement learning for recommender systems: A comprehensive survey of methods, challenges, and future directions},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An implicit layout-aware transformer for full-page end-to-end optical music recognition. <em>IJMIR</em>, <em>14</em>(4), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00385-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital preservation and accessibility of musical manuscripts present a significant challenge in musical heritage conservation. Optical Music Recognition (OMR) is a key solution for automating the transcription of sheet music into machine-readable formats, typically relying on multi-stage processing due to the complexity of musical notation. Although the trend in OMR is to reduce the number of processing steps, the latest single-step approaches lose layout information from the original image, which is crucial for accurately interpreting musical documents in practical applications. This paper presents the Layout-Aware Sheet Music Transformer, a novel method based on a single-step transcription strategy that leverages transformer attention mechanisms to extract layout information without explicit annotations. By dynamically analyzing internal model representations during direct image-to-sequence transcription, our approach extracts nuanced structural insights without requiring manual annotations, eliminating manual layout annotations while offering deeper insights into document structures that more faithfully reflect the underlying musical meaning. Experiments in four well-known OMR datasets show that our proposal is both superior in transcription and competitive in layout extraction, making it a solid candidate for a new state of the art in the field.},
  archive      = {J_IJMIR},
  author       = {Rios-Vila, Antonio and Fuentes-Martinez, Eliseo and Castellanos, Francisco J.},
  doi          = {10.1007/s13735-025-00385-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {An implicit layout-aware transformer for full-page end-to-end optical music recognition},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep supervised hashing with multiscale object-driven knowledge distillation for image retrieval. <em>IJMIR</em>, <em>14</em>(4), 1-21. (<a href='https://doi.org/10.1007/s13735-025-00380-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing algorithms are pivotal in large-scale approximate nearest-neighbor searches due to their rapid execution and user-friendly attributes. Recent research underscores the superiority of deep-supervised hashing algorithms over less intricate non-deep-supervised counterparts. These sophisticated algorithms leverage deep learning models to generate hash codes through nonlinear transformations and robust feature extraction, primarily for efficient image retrieval. However, the computational demands and memory requirements of many hashing algorithms are substantial. Compact deep learning models using knowledge distillation (KD) have emerged to address this challenge to alleviate computational burdens. An inherent challenge within this framework is selecting an optimal KD strategy. This paper introduces an innovative approach, “multiscale object-driven knowledge distillation” (MOD-KD), which is designed to enhance optimal deep supervised hashing for image retrieval. MOD-KD enhances student network training by addressing background interference. This involves identifying object locations in training images and segmenting inner layers of teacher and student networks into blocks. This approach preserves object features while discarding background elements, thereby improving student network training quality without misinformation propagation. MOD-KD also improves teacher network training. This approach refines KD by removing background feature interference and utilizing intermediate layer information. Extensive evaluation of CIFAR-10, CIFAR-100, and NUS-WIDE datasets demonstrates MOD-KD’s superiority over existing deep hashing methods.},
  archive      = {J_IJMIR},
  author       = {Hussain, Abid and Li, Heng-Chao and Ali, Muqadar and Hussain, Mehboob and Choo, Ali and Ali, Danish and Rehman, Amir},
  doi          = {10.1007/s13735-025-00380-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Deep supervised hashing with multiscale object-driven knowledge distillation for image retrieval},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-modal fusion with vision transformers for robust action recognition in infrared-visible videos. <em>IJMIR</em>, <em>14</em>(4), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00386-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition in varying illumination and environmental conditions remains challenging, particularly when relying on a single modality. This paper proposes a two-level self-attention fusion framework that integrates visible (RGB) and infrared (IR) video streams using Multi-scale Vision Transformers (MViTs). Each modality’s raw frames and temporal difference frames are processed separately to extract spatial and motion features. Intra-modal fusion is achieved through a multi-head self-attention (MHSA) mechanism, enhancing modality-specific representations. Inter-modal fusion is then performed by applying another MHSA block over concatenated features from the RGB and IR branches, capturing cross-modal dependencies. Experimental results on the Infrared-Visible dataset demonstrate that our dual-stream attention-guided fusion model achieves 96.67% accuracy, significantly outperforming single-modality baselines and conventional fusion techniques. This highlights the effectiveness of our hierarchical fusion strategy and transformer-driven feature learning in multi-modal action recognition.},
  archive      = {J_IJMIR},
  author       = {Imran, Javed and Wasid, Mohammed},
  doi          = {10.1007/s13735-025-00386-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Hierarchical multi-modal fusion with vision transformers for robust action recognition in infrared-visible videos},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing facial beauty prediction via a dual-pathway hybrid architecture integrating vmamba and ViT. <em>IJMIR</em>, <em>14</em>(4), 1-15. (<a href='https://doi.org/10.1007/s13735-025-00387-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Beauty Prediction (FBP) presents a challenging perceptual regression task requiring nuanced understanding of both fine-grained spatial features and high-level semantic concepts. Traditional deep learning models often excel in one area at the expense of the other. Here, we propose the Hybrid Vmamba-ViT architecture, which synergistically combines the strengths of two cutting-edge vision models: a Vmamba branch for fast and efficient long-range spatial feature extraction using State Space Models (SSMs), and a Vision Transformer (ViT) branch leveraging a pretrained backbone for deep semantic awareness through global self-attention. The complementary feature sets from these pathways are intelligently integrated using a multi-head cross-attention fusion mechanism, and the fused representation is passed to a regression head to predict a continuous beauty score. Evaluated on the SCUT-FBP5500 dataset using a rigorous 5-fold cross-validation protocol, our model achieves state-of-the-art performance with a Pearson Correlation (PC) of 0.9261. These results demonstrate the superior capability of our hybrid approach in capturing the complex perceptual cues essential for robust facial beauty assessment. The code is available at https://github.com/DjameleddineBoukhari/Hybrid-Vmamba-ViT-FBP .},
  archive      = {J_IJMIR},
  author       = {Boukhari, Djamel Eddine and Chemsa, Ali},
  doi          = {10.1007/s13735-025-00387-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Enhancing facial beauty prediction via a dual-pathway hybrid architecture integrating vmamba and ViT},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCDINO: Self-supervised learning of masks based on combination of multi-path channel attention and local feature weighting. <em>IJMIR</em>, <em>14</em>(3), 1-13. (<a href='https://doi.org/10.1007/s13735-025-00371-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViT) have made significant progress in computer vision tasks, especially after the introduction of masking operations, which have demonstrated stronger performance in areas such as image classification and detection. However, in self-supervised learning, masking tends to weaken the correlation of local features of an image, which affects the effectiveness of downstream tasks. To address this problem, this paper proposes a new model method, MCDINO, which extends the capability of image mask operation based on the DINO model (based on improved denoising anchor frame DETR). MCDINO introduces a multipath channel attention mechanism to dynamically compute the importance weight of the feature region to capture the key information among local features effectively; meanwhile, it designs a local feature weighted fusion operation in the Block to improve the fineness of local feature expression and correlation modeling capability. Experiments demonstrate that MCDINO significantly outperforms DINO in the ImageNet100 fine-tuning task, and also achieves better performance in downstream tasks such as copy detection. The results show that MCDINO can effectively enhance the feature learning and representation ability under masking conditions and drive the performance of visual tasks. The code has been open-sourced at: https://github.com/wangzy2024/MCDINO},
  archive      = {J_IJMIR},
  author       = {Shao, Yunxue and Wang, Zhiyang and Wang, Lingfeng},
  doi          = {10.1007/s13735-025-00371-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {MCDINO: Self-supervised learning of masks based on combination of multi-path channel attention and local feature weighting},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic fusion and contrastive generation for generalized zero-shot learning. <em>IJMIR</em>, <em>14</em>(3), 1-11. (<a href='https://doi.org/10.1007/s13735-025-00372-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Zero-Shot Learning (GZSL) aims to leverage a classifier trained on seen classes to categorize instances from both seen and unseen classes. Several approaches have been introduced to synthesize visual features that simulate those of unseen classes for training classifiers. However, existing methods only emphasize the distributional relationships between synthesized and real features, while neglecting the inter-class relationships among the synthesized features. Consequently, synthesized visual features exhibit significant loose intra-class distributions and numerous outliers. Furthermore, the generator trained solely on seen classes tend to overfit these classes. In this paper, a Semantic Fusion and Contrastive Generation (SFCG) framework is proposed for GZSL. Specifically, a visual-semantic contrastive generation method and a visual features similarity loss are explored to address the challenges of loose intra-class distribution and outliers in synthesized visual features. Moreover, semantic attributes are fused to create novel and diverse semantic instances for training a balanced generator. The SFCG model is evaluated on four widely-used ZSL benchmark datasets: CUB, FLO, AWA2, and SUN. It achieves harmonic mean accuracies of 68.4% on CUB, 71.3% on FLO, 73.4% on AWA2, and 45.2% on SUN, demonstrating the efficacy of the proposed method.},
  archive      = {J_IJMIR},
  author       = {Yang, Guan and Sun, Weihao and Liu, Xiaoming and Liu, Yang and Wang, Chen},
  doi          = {10.1007/s13735-025-00372-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Semantic fusion and contrastive generation for generalized zero-shot learning},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced YOLOv10 for small object detection with context-aware and adaptive modules. <em>IJMIR</em>, <em>14</em>(3), 1-11. (<a href='https://doi.org/10.1007/s13735-025-00373-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection poses substantial challenges due to limited pixel count and sparse features, yet it holds immense significance in applications like autonomous driving and unmanned aerial vehicles. This paper introduces an algorithm derived from YOLOv10, significantly enhancing the accuracy of detecting small objects. We have devised a novel module, the Context-aware and Enhanced Capture Module (C2A), which addresses the intricacies of small object detection by skillfully integrating multi-scale features and contextual information to bolster recognition and capture capabilities. Additionally, we incorporate the Receptive Field Attention Convolution (RFAConv) mechanism, leveraging attention weights to precisely evaluate the significance of each receptive field position’s information, facilitating the extraction of crucial small object features. Furthermore, we introduce Adaptive Convolution Kernel (AKConv) technology, which dynamically adjusts the convolution kernel to effectively handle small objects of varying sizes and shapes, thereby enhancing detection performance. Evaluations on the VisDrone2021 dataset reveal that our algorithm achieves AP50 and mAP scores of 29.3 and 16.4. This substantial performance uplift underscores the efficacy of our proposed algorithm in small object detection tasks.},
  archive      = {J_IJMIR},
  author       = {Wang, Jian and Su, Jia and Wen, Zonghui and Sun, Yongqing},
  doi          = {10.1007/s13735-025-00373-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Enhanced YOLOv10 for small object detection with context-aware and adaptive modules},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPE-YOLO: Improved low-light object detection using a two-way pyramid enhancement network. <em>IJMIR</em>, <em>14</em>(3), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00374-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low-light conditions, image details will be severely lost due to the lack of illumination and noise interference, resulting in unsatisfactory performance based on existing object detection methods. To address this issue, we propose a Two-Way Pyramid Enhancement Network (TPE-NET) and cascade it with YOLOv3 to construct an end-to-end low-light object detection framework, termed TPE-YOLO. First, TPE-NET uses Laplacian pyramid to divide low-light images into Gaussian low-frequency components and Laplacian high-frequency components of different scales. Specifically, for Gaussian low-frequency components, we design Low-Frequency Detail Enhancement Module (LDEM) to filter out image noise and enhance the semantic and texture information of the image. For Laplacian high-frequency components, High-Frequency Edge Enhancement Module (HEEM) is proposed to capture the edge and global feature information of the image. Secondly, Context Aggregation Module (CAM) is designed to mine and aggregate multi-scale contextual semantics through parallel dilated convolutions to further enhance the feature representation of high- and low-frequency information components. In addition, Color Extraction Module (CEM) and color consistency loss are designed to supplement color information for restored images and reduce image color distortion, thereby making object features more discriminative. We evaluate our proposed method on the low-light object detection benchmark dataset ExDark. Experimental results show that TPE-YOLO performs well under various low-light conditions, achieving 78.4% mAP, which is better than the existing low-light object detection methods. Code and weights are available at https://github.com/zzxf123/TPE-YOLO .},
  archive      = {J_IJMIR},
  author       = {Zhang, Xiaofei and Di, Xiaoguang and Zhu, Runwen},
  doi          = {10.1007/s13735-025-00374-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {TPE-YOLO: Improved low-light object detection using a two-way pyramid enhancement network},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Remote sensing image change captioning: A comprehensive review. <em>IJMIR</em>, <em>14</em>(3), 1-20. (<a href='https://doi.org/10.1007/s13735-025-00375-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing change detection automatically identifies temporal changes in specific geographic areas by analyzing multi-temporal remote sensing images, which locates altered regions but struggles to interpret the underlying semantic implications of these changes. The emergence of Remote Sensing Image Change Captioning (RSICC) has opened new avenues for change interpretation, aiming to understand semantic variations between bi-temporal remote sensing images and express them through natural language. By integrating vision with language, RSICC provides higher-level scene understanding for applications such as environmental monitoring, disaster response, and urban planning. In recent years, despite the continuous emergence of various RSICC datasets and methodologies, this field still lacks systematic review studies. We first categorize and discuss datasets and evaluation metrics. Then, we propose a novel typology that presents the developmental process, similarities and differences, model architecture, and performance comparison of existing RSICC approaches. Furthermore, we discuss the main algorithmic contributions and future research directions based on technical challenges and application requirements. This study fills a critical gap in this emerging field and provides valuable reference for researchers in related disciplines.},
  archive      = {J_IJMIR},
  author       = {Zou, Shiwei and Wei, Yingmei and Xie, Yuxiang and Lao, Mingrui and Luan, Xidao},
  doi          = {10.1007/s13735-025-00375-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Remote sensing image change captioning: A comprehensive review},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted semantic feature based self-supervised deep cross-modal hashing. <em>IJMIR</em>, <em>14</em>(3), 1-22. (<a href='https://doi.org/10.1007/s13735-025-00378-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To fast respond the large-scale cross-modal retrieval task, the deep cross-modal hashing algorithms map different modalities into the low-dimensional Hamming space and measure their similarity degree using Hamming distance. The self-supervised methods train deep hashing network based on the semantic multi-label and preserve the original semantic relationship in the Hamming space. However, most self-supervised methods neglect the semantic weight inconsistency problem among the image, text and multi-label. Moreover, they ignore preserving the relative ranking orders among the retrieval samples. To address the above issues, we propose a novel method termed weighted semantic feature based self-supervised deep cross-modal hashing (WFSCH). Firstly, we design the weighted semantic feature module to adaptively generate the weighted semantic features for the image, text and multi-label. The weighted semantic features improve the accuracy of describing different modal content and enhance the self-supervised semantic constraint. Secondly, to further improve the cross-modal retrieval performance, we preserve the original intra- and inter-modal triplet ranking relationship in the Hamming space. Finally, to minimize the discrepancy between the pairwise hashing similarity and the multi-label semantic relationship, we design the multi-label semantic similarity preserving loss based on the positive-constraint Kullback-Leibler (KL) divergence, which fully explores different modal multi-label semantic information. We conduct extensive experiments on four publicly available datasets including MIRFLICKR-25K, NUS-WIDE, MS COCO2014 and IAPR TC-12. The experimental results demonstrate that the proposed WFSCH algorithm outperforms the state-of-the-art cross-modal hashing algorithms.},
  archive      = {J_IJMIR},
  author       = {Gao, Limeng and Wang, Zhen and Wang, Xinzhong and Zheng, Zhen and Chen, Haixu and Lu, Shihong},
  doi          = {10.1007/s13735-025-00378-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Weighted semantic feature based self-supervised deep cross-modal hashing},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A similarity-optimized and semantic-aligned method for unsupervised cross-modal hashing retrieval. <em>IJMIR</em>, <em>14</em>(3), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00376-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of multimodal data types, unsupervised cross-modal hashing retrieval technology has become a vital solution for efficient storage and fast retrieval by learning universal binary hash codes. However, existing methods often suffer from inaccurate similarity measurements and imbalanced information between modalities, which limit the further improvement of retrieval performance. To address these challenges, this paper introduces a Similarity-Optimized and Semantic-Aligned Method for Unsupervised Cross-modal Hashing Retrieval (SOSAH), which contains a similarity graph optimization strategy and a hierarchical multimodal semantic-aware alignment module. (1) The similarity graph optimization strategy enhances the similarity between closely related samples while suppressing the similarity of unrelated samples. This refinement improves the expressiveness of the similarity matrix, effectively guiding the learning of highly discriminative hash codes. (2) To solve the problem of the imbalance of the modalities, we construct a hierarchical multimodal semantic-aware alignment module. This module employs a graph convolutional network to align intra-modal correlations and a hierarchical similarity aggregation network to align inter-modal relationships. Experimental results on two publicly available datasets demonstrate the effectiveness of the proposed approach and show significant improvements in retrieval performance.},
  archive      = {J_IJMIR},
  author       = {Li, Xin and Li, Xiuyuan and Li, Mingyong and Ge, Mingyuan},
  doi          = {10.1007/s13735-025-00376-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A similarity-optimized and semantic-aligned method for unsupervised cross-modal hashing retrieval},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ETG: The graph convolutional network was enhanced with an EA-transformer for aspect sentiment triplet extraction. <em>IJMIR</em>, <em>14</em>(3), 1-17. (<a href='https://doi.org/10.1007/s13735-025-00377-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Aspect Sentiment Triplet Extraction task involves identifying aspect terms, corresponding opinion terms, and their sentiment polarity within sentences, to comprehensively capture the text's fine-grained information. Existing methods predominantly utilize end-to-end approaches based on graph convolutional networks (GCNs). However, traditional GCN models may encounter long-distance dependency problems when dealing with longer sentences. Although increasing the number of GCN layers can help cover longer-distance dependencies, it leads to over-smoothing, where node features become indistinguishable due to the excessive aggregation of neighboring features as the number of layers increases. Therefore, in this article, we propose a GCN model enhanced by an edge attention transformer (EA-Transformer). We first extract local subgraph structures through GCNs and then capture global graph structures using the EA-Transformer method. Specifically, we use the EA-Transformer to learn different dependency types of adjacent edges; even edges with the same dependency type can obtain different representations and weights. In addition, when constructing edge representations, we incorporate multiple linguistic features and introduce an Additive Context Attention module, which enables the model to better capture and utilize critical contextual information. Extensive experiments on four benchmark datasets demonstrate that our model outperforms existing methods, achieving higher F1 scores while performing excellently in extracting overlapping triplets and handling long-distance dependencies.},
  archive      = {J_IJMIR},
  author       = {Yang, Kun and Gao, Bin and Li, Linlin and Li, Yutong and Liu, Shutian and Liu, Zhengjun},
  doi          = {10.1007/s13735-025-00377-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {ETG: The graph convolutional network was enhanced with an EA-transformer for aspect sentiment triplet extraction},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-NeuS: Neural implicit surface reconstruction using feature multi-view consistency constraint. <em>IJMIR</em>, <em>14</em>(3), 1-15. (<a href='https://doi.org/10.1007/s13735-025-00379-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the emergence of Neural Radiance Fields, neural implicit surface reconstruction methods have achieved remarkable progress. However, the reconstruction process still struggles with precision, often leading to blurry regions in the fine details. To address this challenge, we propose Feature-NeuS, a neural implicit surface reconstruction method that integrates multi-view semantic features consistency. We extract multi-scale features that include high-level semantic representations and low-level visual information, which serve as prior knowledge for the network to refine the fine details in the 3D model. Additionally, we introduce a multi-view consistency loss for surface points, employing multi-view geometric constraints on image level and feature level to enhance the accuracy and sharpness of the reconstruction. Our method aims to better capture and refine the intricate structures of the 3D reconstructed model by utilizing both image-derived features and multi-view consistency. Extensive qualitative and quantitative experiments demonstrate that our method outperforms the state-of-the-art on DTU, BlendedMVS, and Tank & Temple datasets, particularly in recovering fine model details.},
  archive      = {J_IJMIR},
  author       = {Ding, WenYu and Wu, YaHong and Liu, Feng},
  doi          = {10.1007/s13735-025-00379-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Feature-NeuS: Neural implicit surface reconstruction using feature multi-view consistency constraint},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal alignment with synthetic caption for text-based person search. <em>IJMIR</em>, <em>14</em>(2), 1-13. (<a href='https://doi.org/10.1007/s13735-025-00356-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search aims to retrieve target person from a large gallery based on natural language description. Existing methods take it as one-to-one embedding or many-to-many embedding matching problem. The former approach relies on the assumption of the existence of strong alignment between text and images, while the latter inevitably leads to issues of intra-class variation. Rather than being confined to these two approaches, we propose a new strategy that achieves cross-modal alignment with synthetic caption for joint image-text-caption optimization, named CASC. The core of this strategy lies in generating fine-grained captions that are informative for multimodal alignment. To realize this, we introduce two novel components: Granularity Awareness Sensor (GAS) and Conditional Contrastive Learning (CCL). GAS selects relative features through an innovative adaptive masking strategy, endowing the model with an enhanced perception of discriminative features. CCL aligns different modalities through further constraints on the synthetic captions by comparing the similarity of hard negative samples, protecting the disruption from noisy contents. With the incorporation of extra caption supervision, the model has access to learn more comprehensive feature representation, which in turn boosts the retrieval performance during inference. Experiments demonstrate that CASC outperforms existing state-of-the-art methods by 1.20%, 2.35% and 2.29% in terms of Rank@1 on CUHK-PEDES, ICFG-PEDES and RSTPReid datasets, respectively.},
  archive      = {J_IJMIR},
  author       = {Zhao, Weichen and Lu, Yuxing and Liu, Zhiyuan and Yang, Yuan and Jiao, Ge},
  doi          = {10.1007/s13735-025-00356-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Cross-modal alignment with synthetic caption for text-based person search},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMFNet: Geometric multi-scale pixel-level contrastive learning for video salient object detection. <em>IJMIR</em>, <em>14</em>(2), 1-21. (<a href='https://doi.org/10.1007/s13735-025-00361-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For video salient object detection (VSOD) tasks, the geometric variations of object foregrounds and backgrounds across multiple scales pose significant challenges for deep learning models in extracting and integrating semantic features from video streams. Current deep learning approaches, such as recurrent neural networks and transformers, struggle to capture both short- and long-term temporal dependencies at a global level due to their fixed kernel structures. Additionally, these methods are computationally intensive, limiting their practical application. To address these challenges and achieve a balance between accuracy and computational efficiency, a novel lightweight Deformable Multi-scale Fusion Network is proposed, which extracts both attention-based multi-scale features and geometric features together to generate the efficient saliency map. Further, the Geometric Multi-Scale Pixel-level Contrastive Learning (GMPCL) approach, which enhances the geometric representation of features is proposed using GMPCL loss and separates the geometric representations of foreground and background features of objects at the pixel level. The performance evaluation is done on six benchmark datasets and compared with twenty-two state-of-the-art (SOTA) models. The main highlight of this work is that it performs well on most challenging datasets DAVSOD-Difficult as compared to SOTA models and has 6.2 million network parameters, 5.6 G FLOPS, and 90 FPS inference speed.},
  archive      = {J_IJMIR},
  author       = {Singh, Hemraj and Verma, Mridula and Cheruku, Ramalingaswamy},
  doi          = {10.1007/s13735-025-00361-z},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {DMFNet: Geometric multi-scale pixel-level contrastive learning for video salient object detection},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concept-based and embedding-based models in lifelog retrieval: An empirical comparison of performance. <em>IJMIR</em>, <em>14</em>(2), 1-9. (<a href='https://doi.org/10.1007/s13735-025-00359-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many lifelog retrieval systems have been introduced that apply various approaches to their search engines. The traditional method was to match concepts, which are visual objects detected in images and semantic queries. This concept-based approach has been applied in many retrieval systems, achieving the top performance in lifelog search challenges. Many novel embedding-based cross-modality retrieval models, such as CLIP, BLIP, or HADA, have been developed recently and obtained state-of-the-art (SOTA) results in the image-text retrieval task. These models have recently been applied in several lifelog search challenges. However, there is no comprehensive comparison between them since many benchmarking evaluations contain bias factors such as different user interfaces of participated lifelog retrieval systems. In this paper, we conducted non-biased experiments in both automatic (non-interactive) and interactive configurations to evaluate the performance of many SOTA retrieval models, including the traditional concept-based approach, in the lifelog retrieval task. Furthermore, we retrained the models in a lifelog Q&A dataset to assess whether retraining on a small lifelog dataset could improve the performance. The result showed that embedding-based search engines outperformed the concept-based approach by a large margin in both settings. The finding opens the opportunity to apply the embedding-based models as a new generation of lifelog retrieval models instead of the conventional concept-based approach. The source code and detailed result are available online https://github.com/m2man/Comparing-models-in-Lifelog-Retrieval-Task .},
  archive      = {J_IJMIR},
  author       = {Nguyen, Manh-Duy and Nguyen, Binh T. and Gurrin, Cathal},
  doi          = {10.1007/s13735-025-00359-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Concept-based and embedding-based models in lifelog retrieval: An empirical comparison of performance},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view learning for camouflaged object detection with PVTv2. <em>IJMIR</em>, <em>14</em>(2), 1-11. (<a href='https://doi.org/10.1007/s13735-025-00364-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the continuous development in the field of camouflaged object detection (COD), effectively separating objects highly similar to the background has become a focal point of research. Due to the high similarity between camouflaged objects and backgrounds, traditional single visual branch often perform poorly in such scenarios. To address this issue, we propose a multi-view learning detection network based on the Pyramid Vision Transformer, named Multi-view Learning for Camouflaged Object Detection with PVTv2 (MVLNet). By utilizing the information from RGB and noise views, our method can provide a more comprehensive description of the relationship between objects and backgrounds to improve the accuracy and robustness for COD. Inspired by human visual attention during observation, we design a Global Context Aggregation Module by using a U-shaped structure and progressively increasing dilation rates to simulate the human behavior of zooming in and out. Extensive experiments demonstrate that the proposed MVLNet outperforms 23 other representative models on three public datasets.},
  archive      = {J_IJMIR},
  author       = {Yan, Pu and Ruan, Kang and Wang, Lili and Zhao, Yang and Wang, Xu},
  doi          = {10.1007/s13735-025-00364-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-view learning for camouflaged object detection with PVTv2},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep multimodal learning for time series analysis in social computing: A survey. <em>IJMIR</em>, <em>14</em>(2), 1-18. (<a href='https://doi.org/10.1007/s13735-025-00363-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series data, such as sound waves, bio-signals, and user trajectories, are prevalent in social application scenarios. While single-modal time series data often proves inadequate for addressing challenges in complicated environments, it necessitates integrating multiple modalities to understand real-world phenomena. Utilizing multimodal data improves deep learning systems’ effectiveness, generalizability, and robustness. This survey comprehensively reviews recent advancements, especially methodologies for general multimodal time series analysis and efforts in various social computing contexts, including autonomous driving, healthcare, audiovisual speech recognition, and gesture recognition. We highlight the key techniques of existing studies, namely modality selection, feature extraction, and information fusion and detail the solutions under various circumstances. Finally, we discuss the unresolved challenges and suggest potential future research directions. Our survey aims to provide researchers and industries insights into trends, behaviors, and preferences for performing multimodal time series analysis in social computing applications.},
  archive      = {J_IJMIR},
  author       = {Yang, Chao and Chen, Yakun and Li, Zihao and Wang, Xianzhi and Shi, Kaize and Yao, Lina and Xu, Guandong and Guo, Zhongwen},
  doi          = {10.1007/s13735-025-00363-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Deep multimodal learning for time series analysis in social computing: A survey},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal scene-graph matching for cheapfakes detection. <em>IJMIR</em>, <em>14</em>(2), 1-12. (<a href='https://doi.org/10.1007/s13735-025-00365-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of technology and social media platforms has led to the proliferation of fake news, including the cheapfakes problem. Cheapfakes can be produced easily and spread quickly; a common type is out-of-context misinformation. It is worth investigating a proper algorithm that can efficiently detect such types of misinformation. In this work, we study a new approach to the detection problem of cheapfakes by using graphical neural networks to detect out-of-context samples using the dataset from the ICME 23 Grand Challenge on Detecting Cheapfakes. Specifically, our model utilizes scene graph matching and a language model pre-trained for the natural language inference task to solve task 1 of the challenge. We also propose effective methods to generate labeled data from an unlabeled training set of the challenge. Our proposed method achieved an F1 score of 85.69% and an accuracy score of 85.50% on the public testing set, surpassing the baseline by 4.69% and 3.60%, respectively.},
  archive      = {J_IJMIR},
  author       = {Nguyen, Minh-Tam and Nguyen, Quynh T. and Dao, Minh Son and Nguyen, Binh T.},
  doi          = {10.1007/s13735-025-00365-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multimodal scene-graph matching for cheapfakes detection},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMDL: A multi-modal deep learning for video highlight detection in sports. <em>IJMIR</em>, <em>14</em>(2), 1-18. (<a href='https://doi.org/10.1007/s13735-025-00366-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing interest in sports events, the ability to capture highlights has become increasingly important. Traditionally, the process of editing these highlights required significant time and manpower. To address this challenge, this paper introduces an innovative multi-modal deep learning method for highlight detection (MMDL). The proposed MMDL integrates information from multiple modalities, including subtitles, static skeletal features, and video content, to gain a deep understanding of specific behaviors and identify sub-videos containing those highlights. Additionally, the proposed MMDL employed Siamese networks to accurately capture different aspects of behavior by comparing the similarity between input and training videos across different modalities. Experiments conducted on two datasets, MLB-YouTube and ELTA, demonstrate that the proposed MMDL significantly outperforms existing models, achieving at least a 5% improvement in F1-Score compared to the baseline models, such as I3D and NPL.},
  archive      = {J_IJMIR},
  author       = {Zhang, Qiaoyun and Chang, Chih-Yung and Wu, Shih-Jung and Chang, Hsiang-Chuan and Roy, Diptendu Sinha},
  doi          = {10.1007/s13735-025-00366-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {MMDL: A multi-modal deep learning for video highlight detection in sports},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A CNN-transformer hybrid model and a multi-modal multi-stage training strategy for visible-infrared person re-identification. <em>IJMIR</em>, <em>14</em>(2), 1-17. (<a href='https://doi.org/10.1007/s13735-025-00367-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-Infrared Person Re-Identification (VI Re-ID) is designed to match visible pedestrian images with infrared pedestrian images. The modality discrepancy between visible and infrared images is the biggest challenge for VI Re-ID. Existing VI Re-ID approaches mostly weaken the effect of modality discrepancy by extracting discriminative features, while ignoring the ability of models to adapt to modality variations. To address this issue, we construct a CNN-Transformer hybrid model (CTHM) for VI Re-ID, which mainly consists of a dual-stream ResNet-50 network, a dual-stream VIT network, and a multi-modal feature fusion module (MFFM). In particular, we design a multi-modal multi-stage training strategy (MMTS). Given visible images and infrared images, MMTS first uses Cycle GAN to generate fake infrared images and fake visible images. Then, MMTS sequentially employs visible images and fake visible images, infrared images and fake infrared images, fake visible images and fake infrared images, and visible images and infrared images, to train CTHM in stages in order to progressively improve its adaptive ability to modality variations, thus ensuring that CTHM can obtain more discriminative modality-invariant features and better attenuates the influence of modality discrepancy. We conduct extensive experiments on two benchmark datasets, SYSU-MM01 and RegDB, and the results show that our method reaches the current advanced level.},
  archive      = {J_IJMIR},
  author       = {Hao, Xinxin and Du, Haishun and Guo, Jiangtao and Li, Jieru},
  doi          = {10.1007/s13735-025-00367-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A CNN-transformer hybrid model and a multi-modal multi-stage training strategy for visible-infrared person re-identification},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human behavior recognition based on DualBiNet model. <em>IJMIR</em>, <em>14</em>(2), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00369-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A DualBiNet model for human behavior recognition using Channel State Information (CSI) signals is presented in this study. The model aims to provide an effective solution for accurately recognizing diverse human behaviors in specific environments through deep learning techniques. First, we review the relevant knowledge and recent research trends in the field of human behavior recognition utilizing CSI signals, highlighting their potential applications. Second, we detail the preprocessing techniques employed in our study to enhance data quality and model performance. Subsequently, we explore the architecture of the DualBiNet model and its unique advantages, including the effective combination of spatial feature extraction and temporal sequence modeling. Finally, we evaluate the model’s performance on two widely recognized public datasets, achieving recognition accuracies of 99.03% and 98.47%, significantly surpassing other methods. These results underscore the effectiveness of the DualBiNet model for human behavior recognition using CSI signals, providing valuable insights for future research in this domain.},
  archive      = {J_IJMIR},
  author       = {Kan, Lingling and Liu, Ruixuan and Liang, Hongwei and Huo, Fengcai and Wang, Wenfeng},
  doi          = {10.1007/s13735-025-00369-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Human behavior recognition based on DualBiNet model},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FiCo-ITR: Bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis. <em>IJMIR</em>, <em>14</em>(2), 1-16. (<a href='https://doi.org/10.1007/s13735-025-00368-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of Image-Text Retrieval (ITR), recent advancements have leveraged large-scale Vision-Language Pretraining (VLP) for Fine-Grained (FG) instance-level retrieval, achieving high accuracy at the cost of increased computational complexity. For Coarse-Grained (CG) category-level retrieval, prominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency, albeit at the cost of retrieval performance. Due to differences in methodologies, FG and CG models are rarely compared directly within evaluations in the literature, resulting in a lack of empirical data quantifying the retrieval performance-efficiency tradeoffs between the two. This paper addresses this gap by introducing the FiCo-ITR library, which standardises evaluation methodologies for both FG and CG models, facilitating direct comparisons. We conduct empirical evaluations of representative models from both subfields, analysing precision, recall, and computational complexity across varying data scales. Our findings offer new insights into the performance-efficiency trade-offs between recent representative FG and CG models, highlighting their respective strengths and limitations. These findings provide the foundation necessary to make more informed decisions regarding model selection for specific retrieval tasks and highlight avenues for future research into hybrid systems that leverage the strengths of both FG and CG approaches.},
  archive      = {J_IJMIR},
  author       = {Williams-Lekuona, Mikel and Cosma, Georgina},
  doi          = {10.1007/s13735-025-00368-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {FiCo-ITR: Bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chameleon: A multimodal learning framework robust to missing modalities. <em>IJMIR</em>, <em>14</em>(2), 1-14. (<a href='https://doi.org/10.1007/s13735-025-00370-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning has demonstrated remarkable performance improvements over unimodal architectures. However, multimodal learning methods often exhibit deteriorated performances if one or more modalities are missing. This may be attributed to the commonly used multi-branch design containing modality-specific components, making such approaches reliant on the availability of a complete set of modalities. In this work, we propose a robust multimodal learning framework, Chameleon, that adapts a common-space visual learning network to align all input modalities. To enable this, we present the unification of input modalities into one format by encoding any non-visual modality into visual representations thus making it robust to missing modalities. Extensive experiments are performed on multimodal classification task using four textual-visual (Hateful Memes, UPMC Food-101, MM-IMDb, and Ferramenta) and two audio-visual (avMNIST, VoxCeleb) datasets. Chameleon not only achieves superior performance when all modalities are present at train/test time but also demonstrates notable resilience in the case of missing modalities.},
  archive      = {J_IJMIR},
  author       = {Liaqat, Muhammad Irzam and Nawaz, Shah and Zaheer, Muhammad Zaigham and Saeed, Muhammad Saad and Sajjad, Hassan and De Schepper, Tom and Nandakumar, Karthik and Khan, Muhammad Haris and Gallo, Ignazio and Schedl, Markus},
  doi          = {10.1007/s13735-025-00370-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Chameleon: A multimodal learning framework robust to missing modalities},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STCA: An action recognition network with spatio-temporal convolution and attention. <em>IJMIR</em>, <em>14</em>(1), 1-12. (<a href='https://doi.org/10.1007/s13735-024-00350-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution and self-attention mechanisms are two commonly used methods in the field of video understanding. Convolution preserves spatiotemporal relationships in video data while reducing the number of parameters and computations. The self-attention mechanism captures global and long-distance dependencies in sequence data. To address the challenges of low accuracy and excessive parameters in networks for action recognition, we propose a new network that combines convolution and self-attention mechanisms (STCA). STCA consists of two modules: efficient spatiotemporal convolution (ESTConv) and spatiotemporal self-attention (STA). ESTConv extracts local spatiotemporal features of actions, enabling fast reasoning. STA consists of two sub-modules: the spatial self-attention (SA) and the temporal self-attention (TA). SA analyzes the spatial characteristics of actions, while TA analyzes their temporal characteristics. We conducted experiments on the Kinetics400, UCF101, HMDB51, and Something-Something V2 datasets to evaluate our network. Results show that STCA achieves accuracy comparable to the leading action recognition models while reducing parameters by over 20%, making it more lightweight than current best-performing models.},
  archive      = {J_IJMIR},
  author       = {Tian, Qiuhong and Miao, Weilun and Zhang, Lizao and Yang, Ziyu and Yu, Yang and Zhao, Yanying and Yao, Lan},
  doi          = {10.1007/s13735-024-00350-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {STCA: An action recognition network with spatio-temporal convolution and attention},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAMIR: Fine-tuning CLIP and multi-head cross-attention mechanism for multimodal image retrieval with sketch and text features. <em>IJMIR</em>, <em>14</em>(1), 1-15. (<a href='https://doi.org/10.1007/s13735-024-00352-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketches and texts are two input modes of queries that are widely used in image retrieval tasks of different granularities. Text-based image retrieval (TBIR) is mainly used for coarse-grained retrieval, while sketch-based image retrieval (SBIR) aims to retrieve images based on hand-drawn sketches, which pose unique challenges due to the abstract nature of sketches. Existing methods mainly focus on retrieval based on a single modality but fail to explore the connections between multiple modalities comprehensively. In addition, the emerging contrastive language image pre-training (CLIP) model and powerful contrastive learning methods are underexplored in this field. We propose a novel multimodal image retrieval framework (CAMIR) to address these challenges. It obtains sketch and text features through a fine-tuned CLIP model, fuses the extracted features using multi-head cross-attention, and combines contrastive learning for retrieval tasks. In the indexing stage, we introduce Faiss, an open-source similarity search library developed by Meta AI Research, to enhance retrieval efficiency. Comprehensive experiments on the benchmark dataset Sketchy demonstrate the effectiveness of our proposed framework, achieving superior performance compared to existing methods while highlighting the potential of integrating sketch and text features for retrieval tasks.},
  archive      = {J_IJMIR},
  author       = {Yang, Fan and Ismail, Nor Azman and Pang, Yee Yong and Alsayed, Alhuseen Omar},
  doi          = {10.1007/s13735-024-00352-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {CAMIR: Fine-tuning CLIP and multi-head cross-attention mechanism for multimodal image retrieval with sketch and text features},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving skeleton-based action recognition with interactive object information. <em>IJMIR</em>, <em>14</em>(1), 1-13. (<a href='https://doi.org/10.1007/s13735-024-00351-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human skeleton information is important in skeleton-based action recognition, which provides a simple and efficient way to describe human pose. However, existing skeleton-based methods focus more on the skeleton, ignoring the objects interacting with humans, resulting in poor performance in recognizing actions that involve object interactions. We propose a new action recognition framework introducing object nodes to supplement absent interactive object information. We also propose Spatial Temporal Variable Graph Convolutional Networks (ST-VGCN) to effectively model the Variable Graph (VG) containing object nodes. Specifically, in order to validate the role of interactive object information, by leveraging a simple self-training approach, we establish a new dataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more than 2 million additional object nodes. At the same time, we designe the Variable Graph construction method to accommodate a variable number of nodes for graph structure. Additionally, we are the first to explore the overfitting issue introduced by incorporating additional object information, and we propose a VG-based data augmentation method to address this issue, called Random Node Attack. Finally, regarding the network structure, we introduce two fusion modules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the comprehensive performance by effectively fusing and balancing skeleton and object node information. Our method surpasses the previous state-of-the-art on multiple skeleton-based action recognition benchmarks. The accuracy of our method on NTU RGB+D 60 cross-subject split is 96.7%, and on cross-view split, it is 99.2%. The project page: https://github.com/moonlight52137/ST-VGCN .},
  archive      = {J_IJMIR},
  author       = {Wen, Hao and Lu, Ziqian and Shen, Fengli and Lu, Zhe-Ming and Cui, Jialin},
  doi          = {10.1007/s13735-024-00351-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Improving skeleton-based action recognition with interactive object information},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-matrix guided reconstruction hashing for unsupervised cross-modal retrieval. <em>IJMIR</em>, <em>14</em>(1), 1-12. (<a href='https://doi.org/10.1007/s13735-025-00353-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised cross-modal hashing, due to its independence from heavy label information, is more convenient for application to other fields. In recent years, this area has gained widespread attention and achieved great success. However, existing unsupervised cross-modal hashing methods still face some issues, such as simple fusion after feature extraction, the use of a single similarity measure to express data relationships, and guiding hash code learning through a single affinity matrix. To address these problems, we propose a new method called Dual-Matrix Guided Reconstruction Hashing for Unsupervised Cross-Modal Retrieval. We construct an effective matrix from the extracted raw semantic information to guide the generation of reconstructed hash codes for images and texts. Simultaneously, we construct another matrix for the extracted image and text features, guiding the generation of reconstructed hash codes using graph convolution, thus directing hash code learning through dual matrices. In evaluations on three standard datasets, our method achieved an average improvement of approximately 1.3% in MAP@5000 and 1.5% in MAP@50, particularly showing significant performance gains with shorter hash codes.},
  archive      = {J_IJMIR},
  author       = {Lin, Ziyong and Jiang, Xiaolong and Zhang, Jie and Li, Mingyong},
  doi          = {10.1007/s13735-025-00353-z},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Dual-matrix guided reconstruction hashing for unsupervised cross-modal retrieval},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task classification network for few-shot learning. <em>IJMIR</em>, <em>14</em>(1), 1-12. (<a href='https://doi.org/10.1007/s13735-025-00354-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic information provides both internal coherence within categories and distinctiveness between categories that surpass mere visual concepts. Semantic information has been employed in Few-Shot Learning (FSL) to achieve additional performance improvements. Previous methods usually combine support image and semantic information to classify query image. However, in FSL, it is challenging to train a model on a limited base dataset such that the model can effectively fuse or interact with both modalities and obtain better feature representation on the novel dataset. To address this problem, we propose a Multi-task Classification Network (MCN) to decompose the current classification problem into a image-image classification problem and a semantic-image classification problem. Considering the issue that the results of image-image classification and semantic-image classification may not always be trustworthy, we introduce an Uncertainty-Aware Decision Module (UADM) which biases the final classification result towards the result with lower uncertainty in the two types of classification. Extensive experimental results on three datasets have consistently shown that our proposed method achieves impressive results. Particularly, compared to the baseline, we achieved a 2–3% improvement on the CUB, SUN, and Flower datasets in both the 5-way 1-shot and 5-way 5-shot settings.},
  archive      = {J_IJMIR},
  author       = {Ji, Zhong and Liu, Yuanheng and Wang, Xuan and Liu, Jingren and Cao, Jiale and Yu, YunLong},
  doi          = {10.1007/s13735-025-00354-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-task classification network for few-shot learning},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized RT-DETR for accurate and efficient video object detection via decoupled feature aggregation. <em>IJMIR</em>, <em>14</em>(1), 1-13. (<a href='https://doi.org/10.1007/s13735-025-00355-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection (VOD) is a challenging task, and image object detectors are difficult to detect degradation phenomena in certain video frames. However, existing research on VOD mostly trades high computational costs for accuracy, making it difficult to achieve a balance between accuracy and speed. This work proposes an optimized Real-Time Detection Transformer (RT-DETR) model for VOD that introduces a decoupled Feature Aggregation Module (FAM) to separately refine the localization and classification detection heads. This method only requires a minimal increase in the number of parameters to achieve significant improvements in accuracy. Specifically, we insert FAM before the localization detection head and classification detection head, and first freeze all parameters of the feature extractor and classification detection head to train only the parameters of the localization detection head to obtain more accurate localization results. Then, we freeze all parameters of the feature extractor and localization detection head to train only the parameters of the classification detection head to improve the final detection accuracy. We have conducted a large number of ablation experiments to verify the effectiveness of the method. Without using any post-processing methods, we achieved 90.0% mAP on the ImageNet-VID dataset, with only 77.9 M parameters and an average inference speed of 14.1ms.},
  archive      = {J_IJMIR},
  author       = {Chen, Hao and Huang, Wu and Zhang, Tao},
  doi          = {10.1007/s13735-025-00355-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Optimized RT-DETR for accurate and efficient video object detection via decoupled feature aggregation},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAMoE-MSA: Polarity-aware mixture of experts network for multimodal sentiment analysis. <em>IJMIR</em>, <em>14</em>(1), 1-16. (<a href='https://doi.org/10.1007/s13735-025-00362-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) is a challenging task that aims to understand human emotions from text, visual, and audio modalities. Existing studies struggle to capture the monotonic relationship between emotional expressions. This monotonic relationship means that the emotional intensity changes consistently with the expression amplitude when considering emotional polarities, which is a crucial aspect of MSA tasks. To tackle this, we propose a polarity-aware mixture of experts network (PAMoE-MSA). PAMoE-MSA is capable of learning polarity-specific and polarity-common features to capture the monotonic relationship of emotional expressions from multimodal sentiment data. Our model consists of three experts: a positive expert, a negative expert, and a general expert. They are trained through a unique Guide Task, where the positive and negative experts are trained by non-neutral samples, while the general expert is trained by all samples. A gating mechanism is utilized to adaptively perceive the monotonic relationship within emotional expressions. Moreover, the self-supervised labels are introduced to preserve modality-specific information. The experts module is fed with the fusion features, which contain richer emotional information. To enhance model stability during the training phase, we employ multi-side contrastive learning before making predictions. Our evaluation of PAMoE-MSA on the CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets shows notable improvements over state-of-the-art methods, with increases of approximately 1.3% in Acc-7 for CMU-MOSI, 1.2% in Acc-2 for CMU-MOSEI, and 0.8% in F1-score for CH-SIMS.},
  archive      = {J_IJMIR},
  author       = {Huang, Changqin and Lin, Zhenheng and Han, Zhongmei and Huang, Qionghao and Jiang, Fan and Huang, Xiaodi},
  doi          = {10.1007/s13735-025-00362-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {PAMoE-MSA: Polarity-aware mixture of experts network for multimodal sentiment analysis},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFAFD: A few-shot learning method for cascading models with parameter free attention and finite discrete space. <em>IJMIR</em>, <em>14</em>(1), 1-11. (<a href='https://doi.org/10.1007/s13735-025-00357-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on existing learning models, multimodal approaches have demonstrated promising performance in the realm of few-shot learning, owing to contrastive language-image pretraining. However, shortcomings persist in multimodal fusion methods, particularly in aligning textual and visual features across different granularity levels, and in the independent feature extraction by encoders lacking interaction. Therefore, this paper proposes MFAFD: a cascaded model for few-shot learning featuring parameter free attention mechanisms and a finite discrete space. Initially, the model employs a parameter free attention module in the pretraining phase to facilitate cross-modal interactions, enhancing alignment between spatial features of images and generated text prior to extracting global features from images via CLIP. This bidirectional update of textual and visual information addresses the issue of feature alignment. During training, the model leverages a representation based on Finite Discrete Space (FDS), constructing a finite discrete space foundation for textual and image features, effectively bridging modal differences. Ultimately, using text as a baseline, the model predicts image classification based on similarity weights between images and text. Through quantitative and qualitative analyses, this study demonstrates that parameter free attention mechanisms and finite discrete space modules significantly enhance the performance of cascaded multimodal aggregation models. The model exhibits robust performance in few-shot classification across multiple datasets. The code is available at https://github.com/turelove999/MFDFA-dj .},
  archive      = {J_IJMIR},
  author       = {Xue, Lixia and Dong, Jiang and Wang, Ronggui and Yang, Juan},
  doi          = {10.1007/s13735-025-00357-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {MFAFD: A few-shot learning method for cascading models with parameter free attention and finite discrete space},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image forgery classification and localization through vision transformers. <em>IJMIR</em>, <em>14</em>(1), 1-11. (<a href='https://doi.org/10.1007/s13735-025-00358-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the easy availability of software over the Internet, any naive user can tamper the images for entertainment purposes or to defame a personality by circulating over social media networks. The practice of image tampering is a serious issue and can attract legal action if proven guilty. Forensic researchers employ various methods to detect and localize image forgeries. In this research, we use a Vision transformer (ViT) as a method for binary classification of images distinguishing forged and unforged images. Further, we use a pre-trained Segment Anything Model(SAM) which is fine-tuned with custom data to adaptively recognize patterns indicating forged regions within the images. SAM can localize these forged areas and is leveraged to create templates by extracting the identified regions. The proposed method is rigorously tested across various datasets, including CASIA v1.0, CASIA v2.0, MICC-F2000, MICC-F600, and Columbia. Through comprehensive experimentation, our approach showcases considerable promise yielding accuracy in image forgery classification and localization. Our model’s robustness and adaptability make it an attractive tool for forensic analysis in diverse scenarios, contributing to the advancement of multimedia forensics security research.},
  archive      = {J_IJMIR},
  author       = {Pawar, Digambar and Gowda, Raghavendra and Chandra, Krishna},
  doi          = {10.1007/s13735-025-00358-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Image forgery classification and localization through vision transformers},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VPC-VoxelNet: Multi-modal fusion 3D object detection networks based on virtual point clouds. <em>IJMIR</em>, <em>14</em>(1), 1-11. (<a href='https://doi.org/10.1007/s13735-025-00360-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the impact of sparsity and disorder of point clouds on object detection accuracy, this paper proposes a multi-modal fusion network VPC-VoxelNet based on virtual point clouds. Firstly, virtual point clouds are constructed using image detection object information to increase the density of point clouds, thus improving the performance of object features; Secondly, increasing the dimensionality of point cloud features, distinguishing virtual point clouds and avoiding the accumulation of multi model errors; Finally, an optimized loss function such as the scale factor of the virtual point cloud is used to improve the training efficiency of the multi-modal network. The object detection network, VPC-VoxelNet, was tested on the KITTI dataset, and the detection accuracy was better than that of the classical 3D point cloud detection network and certain multi-modal information fusion networks, with a vehicle detection accuracy of 86.9%.},
  archive      = {J_IJMIR},
  author       = {Zhang, Qiang and Shi, Qin and Cheng, Teng and Zhang, Junning and Chen, Jiong},
  doi          = {10.1007/s13735-025-00360-0},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {VPC-VoxelNet: Multi-modal fusion 3D object detection networks based on virtual point clouds},
  volume       = {14},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
