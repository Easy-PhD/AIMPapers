<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRTIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrtip">JRTIP - 193</h2>
<ul>
<li><details>
<summary>
(2025). YOLO-GSD: A real-time pedestrian detection algorithm based on YOLOv8 in dense environments. <em>JRTIP</em>, <em>22</em>(6), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01771-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent advancements in vision intelligence, pedestrian detection in autonomous driving has become a critical research focus within computer vision. Dense pedestrian scenarios present significant challenges from multi-scale variations and occlusions. Traditional detection methods can be used for pedestrian detection in ordinary scenarios, but they face challenges, such as high computational complexity and overly sophisticated models, that prevent effective deployment on mobile devices like in-vehicle cameras, along with unsatisfactory detection accuracy under multi-scale pedestrian scenarios and heavy occlusion conditions. To address these challenges, this paper proposes YOLO-GSD, an improved lightweight real-time pedestrian detection algorithm based on YOLOv8. The algorithm first introduces a dedicated detection layer specifically designed for small-scale targets. It then incorporates lightweight Ghost convolution and designs a DG-C2f module by integrating Ghost convolution and Dynamic Convolution, aiming to reduce computational complexity while enhancing the algorithm’s multi-scale feature fusion capability. Additionally, it employs the ultra-lightweight DySample upsampler for efficient feature reconstruction and integrates the SEAM attention mechanism to improve occlusion-aware detection. Finally, WIoUv3 is used to replace the CIoU loss function, which improves the generalization ability and overall performance of the algorithm. Experimental results demonstrate mAP@0.5 scores of 90.7% on the WiderPerson dataset (1.4% higher than the baseline) and 86.5% on the CrowdHuman dataset (2.2% improvement). The algorithm’s parameter count is reduced to 6.24 M, its FLOPs are lowered to 22.7 G, and its FPS is increased to 106.6. In addition, a homogeneous training comparison was conducted on the small-object dataset RSOD, demonstrating the advantages of YOLO-GSD in small-object detection. Therefore, the YOLO-GSD algorithm proposed in this paper is suitable for real-time pedestrian detection in multi-scale and occlusion scenarios on mobile platforms with limited computational resources.},
  archive      = {J_JRTIP},
  author       = {Zhang, Zuhao and Li, Weiwei and Luo, Lin},
  doi          = {10.1007/s11554-025-01771-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-GSD: A real-time pedestrian detection algorithm based on YOLOv8 in dense environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast coding based on dual-head attention network in 3D-HEVC. <em>JRTIP</em>, <em>22</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01684-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional High-Efficiency Video Coding (3D-HEVC) standard, as the latest 3D video coding standard, has introduced several new coding algorithms for depth map that enables it to have a higher compression ratio compared with previous standards. However, this also leads to a sharp increase in computational complexity, which has become the critical factor to prevent 3D-HEVC from being widely used. To address this problem, a new framework based on Convolutional Neural Networks (CNN) is proposed to achieve fast depth intra-coding in this article. First, we propose an adaptive Quantization Parameter-Attention CNN (QA-CNN) with multiple attention modules to predict the Coding Units (CUs) partition of depth map. After that, according to the relationship between texture map and depth map, we introduce texture map input to form dual-head QA-CNN (DQA-CNN), using texture map CU partition results to correct the depth map results. Finally, we embed DQA-CNN into 3D-HEVC test platform to achieve fast depth intra-coding. Experimental results show that compared with the reference software HTM $$-$$ 16.0, the proposed method can reduce an average of 74.9% of the intra-coding time with the reduction of rate-distortion performance in an acceptable range, outperforming the state-of-the-art methods in reducing coding complexity.},
  archive      = {J_JRTIP},
  author       = {Wu, Yueheng and Jia, Kebin},
  doi          = {10.1007/s11554-025-01684-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast coding based on dual-head attention network in 3D-HEVC},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QLDNet: A lightweight and efficient network for high-robustness aerial human detection in UAV-based remote sensing. <em>JRTIP</em>, <em>22</em>(5), 1-19. (<a href='https://doi.org/10.1007/s11554-025-01707-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of extremely small targets in aerial high-resolution images is critical for military and civilian applications. However, challenges, such as small target size, complex backgrounds, and target deformation, hinder optimal performance. We propose a lightweight network that addresses these issues through a synergistic combination of non-strided convolution and decoupled large-kernel convolutional attention. Specifically, the non-strided convolution constructs a quadruple aggregate and connection feature extractor (QACFE) module, which maps features into the channel dimension while preserving spatial details. The decoupled large kernel convolutional attention (DLKA) then leverages these channel features to effectively extract structural and edge-related low-frequency information, while simultaneously reducing computational costs and model size caused by the increased channel dimensionality after QACFE. A learnable offset mechanism is introduced, transforming the detection head into a deformable detection head (DDH). Additionally, the network incorporates a PAFPN (Path Aggregation Feature Pyramid Network) structure to efficiently extract multi-scale features. During inference, a tailored approach performs pixel-level multi-scale detection. This enhances small target detection by merging features and pixels through dual-scale fusion, integrating multi-scale feature extraction with multilevel feature integration. QLDNet is a dual multiscale network based on quadruple aggregation and connected feature extract (QACFE), decoupled large kernel convolutional channel attention mechanism (DLKA), and deformable decoupled head (DDH). Experimental results demonstrate that QLDNet achieves efficient and accurate small target detection, with an accuracy rate as high as 94.0% and an inference time of 0.12 s per image, meeting the system’s real-time requirements with fewer parameters and satisfying processing speeds. The code can be find at https://github.com/Sjl185721/QLDNet-v1.git .},
  archive      = {J_JRTIP},
  author       = {Zhang, Mandun and Shi, Jiale and Hou, Chenyue and Pang, Yonghui and Zhang, Yiqi and Huang, Xiangsheng},
  doi          = {10.1007/s11554-025-01707-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {QLDNet: A lightweight and efficient network for high-robustness aerial human detection in UAV-based remote sensing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRIME-net: An efficient progressive residual incremental multi-scale estimation network for dynamic ocean wave fields. <em>JRTIP</em>, <em>22</em>(5), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01740-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time estimation of spatio-temporal ocean wave fields is crucial for marine applications but challenged by the limitations of traditional sensors and the computational cost of dense vision-based reconstruction. Existing methods often rely heavily on sparse geometric data, potentially underutilizing rich image information. This paper proposed an efficient deep learning network specifically designed for real-time, sparse-to-dense wave field estimation guided by image features, through the network of Progressive Residual Incremental Multi-scale Estimation (PRIME-Net). PRIME-Net employs an encoder–decoder architecture integrating efficient modules like a Pyramid Pooling Attention Module (PPAM) and modulated decoder blocks (MDBs), along with a multi-scale residual prediction strategy to progressively refine elevation estimates from an input image and an initial sparse or low-quality height map. Experiments conducted on challenging datasets demonstrate that PRIME-Net achieves competitive or superior accuracy in reconstructing wave fields compared to state-of-the-art methods, while exhibiting significant computational efficiency suitable for real-time operation. The results validate the effectiveness of the image-guided approach and the architectural design, positioning PRIME-Net as a promising solution for the awareness of sea state.},
  archive      = {J_JRTIP},
  author       = {Wang, Feng and Qiao, Renjie and Wang, Xiaoyu and Meng, Haiyang},
  doi          = {10.1007/s11554-025-01740-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {PRIME-net: An efficient progressive residual incremental multi-scale estimation network for dynamic ocean wave fields},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and optimization of a new corn–weed detection model with YOLOv8–GAS based on artificial intelligence. <em>JRTIP</em>, <em>22</em>(5), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01742-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crop fertilization, pesticide spraying and weed treatment are crucial links in the process of crop growth, and the effective implementation of these tasks depends on accurate crop and weed identification. The recognition technology based on vision can realize automatic recognition. In this study, a new corn–weed detection model named YOLOv8–GAS-based YOLOv8 was proposed to address the widespread problems of poor recognition performance, difficulty in effectively dealing with complex scenarios, and difficulty in co-existing lightweight detection models and recognition accuracy. First, the newly developed GRCSPESIN (ghost reparameterization cross-stage partial connections stage integration network) module was introduced and integrated into the C2f (channel-to-pixel) feature extraction layer in YOLOv8. Then in the neck of the network, the multi-scale feature extraction by sharing convolution kernels of SPC (Shared Pyramid Convolution) module was introduced and the context-guided feature fusion module, ACFM (Adaptive Context Fusion Module) was employed. Finally, a new corn–weed data set was constructed based on an image acquisition of a complex unmanned farm maize test field, which was used for a comparative analysis of YOLOv8–GAS and the baseline model. The results of comprehensive evaluation of the obtained data set demonstrate that the proposed model has excellent performance, with a 3.2% increase in mAP@0.5, a 10.3% decrease in model parameters, and a 16.05% decrease in calculation amount compared with YOLOv8n.},
  archive      = {J_JRTIP},
  author       = {Li, Li and Sun, Rui and Xu, Yifeng},
  doi          = {10.1007/s11554-025-01742-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Design and optimization of a new corn–weed detection model with YOLOv8–GAS based on artificial intelligence},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot learning with depthwise separable convolution for low-light image enhancement using hybrid perceptual loss. <em>JRTIP</em>, <em>22</em>(5), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01744-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving low-light images is challenging due to insufficient lighting, and the task becomes even more difficult when no reference image is available. This limitation has led to the development of zero-shot Low-light image enhancement (LLIE) methods, which do not require paired reference images, making them suitable for real-world applications such as monitoring and autonomous driving. However, many zero-shot LLIE methods aim to adjust illumination, spatial consistency, exposure, and color balancing across all RGB channels. This multi-channel processing increases model complexity, making it harder to handle intricate lighting conditions efficiently. Moreover, models based on standard convolutions have a large number of parameters, resulting in high computational costs. Many methods also fail to prioritize human perception, which is crucial for ensuring perceptual quality and naturalness in enhanced images. To address these challenges, a novel zero-shot LLIE method is proposed that operates on a single channel (the Value channel) in the Hue, Saturation, Value (HSV) color model, simplifying processing and reducing model complexity. This lightweight deep network uses Depthwise Separable Convolution (DSC) to reduce computational costs, with only 4058 parameters, making it suitable for real-time enhancement on resource-constrained devices. The method employs hybrid perceptual losses that combine both computational metrics and human perceptual criteria to guide the enhancement process. It integrates exposure and illumination losses as computational metrics, while the gram matrix texture loss ensures perceptual criteria that aligns with human visual cognition. Extensive experiments validate that our method outperforms existing state-of-the-art approaches.},
  archive      = {J_JRTIP},
  author       = {Singh, Supriya and Raj, Deepa},
  doi          = {10.1007/s11554-025-01744-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Zero-shot learning with depthwise separable convolution for low-light image enhancement using hybrid perceptual loss},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). USH: An efficient real-time distracted driving detection model. <em>JRTIP</em>, <em>22</em>(5), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01745-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of intelligent transportation systems, driving safety has become a core societal concern. Distracted driving, one of the leading factors of traffic accidents, has been directly responsible for over 60% of such incidents. To effectively reduce these accidents, real-time monitoring and early warning of distracted driving have become crucial tasks in enhancing traffic safety. In this paper, we proposes a low-latency, high-efficiency network called USH, aiming to quickly and accurately detect driver distraction behavior on low-computation devices. USH employs a stage-wise hybrid modeling approach, introducing convolutional operations and self-attention at different stages of the model to achieve a balance between model accuracy and efficiency. By injecting local information modeling layers into a Feed Forward Network (FFN), the network complexity is significantly reduced while maintaining high detection accuracy. Additionally, USH introduces a single-head self-attention, effectively avoiding the head redundancy of multi-head self-attention, making the model more lightweight and efficient, meeting the detection needs in resource-constrained environments. Experimental results demonstrate that USH achieves a top-1 accuracy of 98.5% on the StateFarm dateset, with an inference latency of only 4.7 ms. Compared to the original model, the proposed USH achieves a 21.6% improvement in computational efficiency, a 146% reduction in model size. These results indicate that USH not only demonstrates excellent performance and stability, but also exhibits significant potential in the field of distracted driving detection.},
  archive      = {J_JRTIP},
  author       = {Wang, He and Li, Yuan},
  doi          = {10.1007/s11554-025-01745-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {USH: An efficient real-time distracted driving detection model},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight real-time road defect detection algorithm integrating multi-coordinate aggregation attention and shared convolution. <em>JRTIP</em>, <em>22</em>(5), 1-22. (<a href='https://doi.org/10.1007/s11554-025-01747-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prolonged road use leads to surface defects that, if undetected, degrade road life and pose safety risks. Conventional detection methods are slow and costly. To address this, LAR-YOLO (Lightweight Aggregate Re-param-YOLO), a lightweight model based on YOLOv8, was developed in this paper, featuring the RGCSPELAN (Re-param Ghost CSP ELAN) module to minimize model size and enhance detection speed. It includes the AMCA (Aggregate Multiple Coordinate Attention) mechanism for more accurate feature extraction and an Attention-Enhanced Screening Pyramid Network for improved feature representation, reduced feature loss, and better detection outcomes. Additionally, a Lightweight Shared Convolutional Detection Head (LSCD) was developed. Experimental results showed LAR-YOLO improved mAP50 (Mean Average Precision) by 4.8% over YOLOv8 on the RDD2022 dataset and reduced parameters and computational needs by 46.40% and 41.54%, respectively, achieving 212 FPS for real-time detection. It also outperformed other models on the VOC2007 and NEU-DET datasets, proving its practical value.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yujie and Wang, Tao and Wang, Xueqiu},
  doi          = {10.1007/s11554-025-01747-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-22},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight real-time road defect detection algorithm integrating multi-coordinate aggregation attention and shared convolution},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond obstacles: Feather-light YOLO11-LES for real-time ripeness detection of occluded strawberries in greenhouses. <em>JRTIP</em>, <em>22</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01748-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the strawberry industry expands, the inefficiency and cost of manual harvesting have spurred the development of vision-based automated picking systems. Among them, ripeness detection models are crucial for enabling precise and efficient harvesting. However, existing models often struggle in complex greenhouse environments, particularly under occlusion and small-object conditions, leading to reduced accuracy and excessive model size. To address these challenges, this study proposes YOLO11-LES, a novel lightweight strawberry ripeness detection model based on YOLO11n. It integrates three newly designed modules: (1) the Lightweight Adaptive Weighting Downsampling Structure (LAWDS) module replaces standard downsampling convolutions with adaptive weighting to reduce parameters and computation; (2) the Edge-Intensified Feature Extraction Stem (EIEStem) module enhances edge features and mitigates small-object detail loss during early feature extraction; and (3) the Spatial-Enhanced Attention Module Head (SEAMHead) module improves occlusion awareness through integrated spatial and channel attention. Experiments show that YOLO11-LES achieves a compact model size of 4.6 MB, with a precision of 81.5%, recall of 84.1%, and mAP50 of 86.5%. Compared to the baseline YOLO11n, it improves these metrics by 2.9%, 9.0%, and 3.2%, respectively, while reducing the model size by 0.9 MB. YOLO11-LES effectively balances detection accuracy with lightweight design, making it well suited for deployment on real-time edge devices.},
  archive      = {J_JRTIP},
  author       = {Li, Zheng and Hu, Xiaonan and Zhao, Xiaobei and Ye, Hao and Chen, Feng and Chen, Xin and Li, Xiang},
  doi          = {10.1007/s11554-025-01748-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Beyond obstacles: Feather-light YOLO11-LES for real-time ripeness detection of occluded strawberries in greenhouses},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LAP-net: A lightweight PCB defect detection network combined with attention mechanisms. <em>JRTIP</em>, <em>22</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01749-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the electronics manufacturing industry, the defect detection of printed circuit boards (PCBs) plays a crucial role in ensuring product quality. This paper presents a lightweight PCB defect detection model named LAP-Net. This model mainly overcomes the difficulties of parameter redundancy and low detection accuracy in existing methods. Specifically, while maintaining the overall framework of YOLOv8n, LAP-Net incorporates an enhanced ShuffleNetV2 structure in its backbone for efficient feature extraction. Furthermore, a novel method using dual convolution attention mechanism is introduced to effectively enhance the accuracy of small-sized PCB defect detection. Meanwhile, introducing a lightweight feature detection head eliminates redundant parameters and thus reduces the overall complexity of the algorithm. Massive experiments conducted on the public PCB dataset demonstrate the superiority of the presented LAP-Net model, while the balance between algorithm complexity and detection precision outperforms other algorithms. Compared with the baseline algorithm YOLOv8n, the LAP-Net reduces parameters by 11.7% and FLOPs by 22.2%, while the mAP50 improves by 3.2%. Therefore, the proposed LAP-Net is validated as a reliable, competitive, and lightweight PCB defect detection model.},
  archive      = {J_JRTIP},
  author       = {Li, Ziqiang and Ai, Qing and Peng, Ende and Mao, Shaoyu and Han, Tao},
  doi          = {10.1007/s11554-025-01749-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LAP-net: A lightweight PCB defect detection network combined with attention mechanisms},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRD-YOLO: A faster real-time object detector for aerial imagery. <em>JRTIP</em>, <em>22</em>(5), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01750-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid technological progress, drones, or unmanned aerial vehicles (UAVs), have emerged among the most important artificial intelligence (AI)-powered systems. With their aerial perspective, mobility, and cost-effectiveness, they became crucial for advancing AI-driven visual perception in various sectors. However, implementing generic object detection algorithms on these resource-limited devices remains a complex challenge. Towards efficient and more UAV-adapted systems, this paper introduces the Faster Real-time Detector based on You Only Look Once (FRD-YOLO). FRD-YOLO presents different optimizations on the functional and architectural perspectives. To adapt the model to the vision context of UAVs, our key enhancements include the addition of a new layer for detecting tiny objects and the removal of the detection layer for large objects to emphasize the small and tiny targets. We also introduce a structure-aware integration of C3Ghost blocks, inspired by Ghost Convolutions and Cross Stage Partial Network-based layers, to reduce computational cost, and integrate the Convolutional Block Attention Module to enhance the recall. The FRD-YOLO model demonstrates superior detection performance with reduced size and computations across its five scaled versions. Notably, the evaluation on the challenging VisDroneDet2021 dataset reveals that the FRD-YOLO-x achieves 11.23% higher mean Average Precision (mAP50) than the baseline model, with 25.44% less computational cost, reaching up to 44 Frames Per Second (FPS). Additionally, the FRD-YOLO model showcases reliable embedded inference on the Jetson TX2, with FRD-YOLO-n achieving 25.18 FPS and FRD-YOLO-x reducing model size by 58.1%, confirming the architecture’s strength for UAV deployment.},
  archive      = {J_JRTIP},
  author       = {Ben Rouighi, Ines and Chtioui, Hajer and Jegham, Imen and Alouani, Ihsen and Ben Khalifa, Anouar},
  doi          = {10.1007/s11554-025-01750-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FRD-YOLO: A faster real-time object detector for aerial imagery},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GF-few: A real-time lightweight gaussian feature enhanced few-shot network for industrial defect detection. <em>JRTIP</em>, <em>22</em>(5), 1-18. (<a href='https://doi.org/10.1007/s11554-025-01751-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based image segmentation techniques have emerged as a leading approach for detecting industrial defects. However, training deep learning models necessitates a substantial number of defect samples, and the scarcity of negative industrial defect samples complicates their implementation. Additionally, in real industrial environments, defects are often coupled in complex background patterns, making them difficult to identify. To tackle these issues, we propose a Gaussian Feature Augmented Few-Shot Model (GF-Few) for real-time industrial defect detection tasks. By leveraging the generalization ability of the few-shot network structure for images with a limited number of support sets, we introduce an Adaptive Gaussian Membership Feature Module (GM-FE) and a specialized Gaussian Feature Loss Function (GFloss) to enhance the model’s feature extraction process. This combination uses a Gaussian function to adjust the distribution range of the model in the feature space, thereby improving the feature representation of the image. Furthermore, we design a Defect Prototype Alignment Module (DPAM) to compare the differences in defect features between the support set and the query set, which facilitates defect detection by employing a simpler network architecture. This method fully utilizes prior knowledge from a limited number of support sets, enabling more efficient real-time industrial defect detection. We selected three public industrial datasets and collected a Printing-Packaging-Box dataset to evaluate the model. The experimental results demonstrate that the method strikes a balance between detection accuracy and model size, achieving a detection accuracy of 92.21%, with a parameter count of 9.37 M and a detection speed of 183 frames per second, effectively meeting the requirements for industrial environments.},
  archive      = {J_JRTIP},
  author       = {Ma, Qiurui and Zhang, Erhu and Chen, Yajun and Duan, Jinghong},
  doi          = {10.1007/s11554-025-01751-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GF-few: A real-time lightweight gaussian feature enhanced few-shot network for industrial defect detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight real-time detection transformer with single-head self-attention and feature selection for underwater object detection. <em>JRTIP</em>, <em>22</em>(5), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01752-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection suffers from uneven illumination, low contrast, color distortion, and scattering, while limited onboard computing resources demand a balance between accuracy and speed. However, existing lightweight detectors often sacrifice detection precision in challenging underwater conditions. To address this, we propose a lightweight enhancement of the Real-Time DEtection TRansformer (RT-DETR) tailored for underwater environments. First, a lightweight backbone with enhanced global perception is introduced, leveraging the Cross Stage Partial (CSP) concept to eliminate redundant residual convolutional operations. This is combined with a Single-Head Self-Attention (SHSA) mechanism to strengthen global feature modeling, thereby improving detection accuracy for low-contrast targets. Second, a Feature Selection (FS) module based on Channel-wise Adaptive Attention (CAA) is proposed to filter multi-level features extracted by the backbone within a broader receptive field. In addition, a Select Feature Fusion (SFF) module is introduced for bottom-up feature fusion, simplifying the neck’s feature fusion structure and enhancing real-time performance in underwater object detection. Moreover, a Self-Feature Knowledge Distillation (Self-FKD) strategy is employed to enhance detection performance without increasing inference cost, thereby mitigating the degradation of features caused by underwater image quality. Experimental results demonstrate that the proposed model achieves a superior balance between performance and model size compared to several state-of-the-art object detection models across multiple underwater detection datasets, making it well-suited for real-time underwater object detection scenarios.},
  archive      = {J_JRTIP},
  author       = {Peng, Jiawei and Zhou, Zhiyu and Wang, Haiyan},
  doi          = {10.1007/s11554-025-01752-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight real-time detection transformer with single-head self-attention and feature selection for underwater object detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDet-YOLO: An efficient small object detection algorithm for aerial images. <em>JRTIP</em>, <em>22</em>(5), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01753-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In aerial images, the small size, significant scale variation, and dense object distribution often result in low detection accuracy. Therefore, small object detection in aerial images remains a highly challenging task. This paper proposed EDet-YOLO, a small object detection algorithm designed to improve detection precision. Based on YOLO11, several innovations have been introduced. First, the original C3k2 module is restructured into an Efficient Convolutional Feature Extraction module (ECFE), which incorporates a novel Efficient Convolution Module (ECM) to enhance multi-scale feature extraction. Second, a Spatial Bidirectional Attention Module (SBAM) is proposed to establish a bidirectional attention-guided mechanism between high- and low-resolution feature layers, achieving complementary fusion of semantic and detail information. This design effectively enhanced the discriminability and localization accuracy of small objects in complex backgrounds. In addition, a dynamic head is employed to replace the original detection head, enabling adaptive feature enhancement and multi-level feature integration to boost detection performance. A new small object detection layer is also introduced to further improve accuracy. Experimental results on the VisDrone2019 and HIT-UAV datasets demonstrate that the proposed EDet-YOLO outperforms existing models. Compared to the baseline, EDet-YOLO achieves improvements of 10.2% and 1.7% in mAP@50, and 7.6% and 3.4% in mAP@50–95, respectively. Moreover, the detection speed of EDet-YOLO on Jetson Orin Nano reached 24.9 FPS, and this performance met the requirements of real-time detection.},
  archive      = {J_JRTIP},
  author       = {Xiao, Linsong and Li, Wenzao and Tang, Ran and Li, Hanyun and Wan, Bing and Ren, Dehao},
  doi          = {10.1007/s11554-025-01753-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EDet-YOLO: An efficient small object detection algorithm for aerial images},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimized lightweight YOLO-MRC framework for enhanced obstacle detection in service robots. <em>JRTIP</em>, <em>22</em>(5), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01756-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection, particularly for small and deformed targets in complex backgrounds, remains a significant challenge for service robots. To address this, we propose a novel lightweight YOLO-MRC (You Only Look Once-MobileViT-RepPoints-CTAM) network. First, an improved C3-RepPoints module is proposed for finer localization and classification of objects of varying scales and deformations. Second, a lightweight hybrid architecture is presented for efficient small object detection, combining convolutional neural networks (CNNs) and vision transformers (ViTs). Finally, an improved efficient decoupled head (EDH) is used to enhance detection performance by separating classification and regression tasks. Furthermore, the convolutional triplet attention mechanism (CTAM) is introduced to aggregate crucial semantic information. Experimental results on the ODSR-HIS dataset show that YOLO-MRC reduces model parameters by 21.3% while improving mean average precision (mAP@0.5) by 1.3%, with notable gains for small-scale and deformed objects. YOLO-MRC also demonstrates excellent generalization on PASCAL VOC and COCO datasets, improving mAP@0.5 by 4.4% and 10.7%, respectively, compared to YOLOv5n, all achieved with only 1.4M parameters and 5.5G FLOPs. This underscores YOLO-MRC’s balance between model size, latency, and accuracy, making it well suited for service robots. The code is available at https://github.com/lvyongshjd/YOLO-MRC .},
  archive      = {J_JRTIP},
  author       = {Tian, Junyan and Ma, Ning and Huang, Keya and Lv, Yong and Chi, Wenzheng and Sun, Lining},
  doi          = {10.1007/s11554-025-01756-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An optimized lightweight YOLO-MRC framework for enhanced obstacle detection in service robots},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRT-DETR: Faster real-time end-to-end detector for industrial surface defects based on transformer. <em>JRTIP</em>, <em>22</em>(5), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01702-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time detection of surface defects in industrial products is pivotal for the efficiency of industrial production processes. Over the last decade, deep learning-based object detection algorithms have shown their remarkable performance in this field, especially those recently suggested transformer-based methods. However, they often face challenges related to high computational complexity and substantial memory usage. To address these issues, we propose a Faster Real-Time Detection Transformer (FRT-DETR), designed explicitly for detecting surface defects in industrial products. Our findings highlight that the high computational load of RT-DETR originates from convolutional layers from multiple RepConv blocks within the RepC3 module. To address this issue, we integrate an aggregated SaE layer with the original RepC3 module, aiming to develop a more streamlined SaERepC3 module. Moreover, we introduce the IoU-aware query mechanism to select higher quality queries from boxes generated by the encoder. Additionally, we incorporate a newly designed Multi-Scale Detail Integration (MSDI) module, which employs skip connections to enhance the integration of details across scales, thus enhancing the detection accuracy of the model. From experimental results on the NEU-DET dataset and DeepPCB dataset, our model achieves an impressive inference speed, making it highly effective for real-time surface defect detection tasks.},
  archive      = {J_JRTIP},
  author       = {Xiang, Shuang and Fu, Chong and Song, Wei and Wang, Xingwei and Chen, Junxin and Sham, Chiu-Wing},
  doi          = {10.1007/s11554-025-01702-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FRT-DETR: Faster real-time end-to-end detector for industrial surface defects based on transformer},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TRS-YOLO: A lightweight insulator defect detection method based on enhanced YOLOv12. <em>JRTIP</em>, <em>22</em>(5), 1-18. (<a href='https://doi.org/10.1007/s11554-025-01754-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing transmission line insulator diagnostics confront a tripartite challenge: undetected sub-pixel anomalies, interference from heterogeneous backgrounds, and insufficient computational headroom at edge nodes. To address these issues, this study proposes a lightweight detection model named TRS-YOLO, based on YOLOv12. In the TRS-YOLO model, we introduce three key innovations: (1) a receptive field attention convolution module (RFAConv) is proposed to enhance the feature extraction capability of the backbone network; (2) a spatial and channel synergistic attention module (SCSA) is integrated, effectively boosting the model's perception of critical insulator features—particularly improving robustness for small defect detection in complex backgrounds—through a synergistic mechanism combining multi-semantic spatial attention guidance and progressive channel self-attention recalibration; (3) to enhance the detection head's sensitivity to subtle defects while maintaining model efficiency, a mobile inverted bottleneck block (MBConv) is innovatively introduced. Validated on a dataset where small targets constitute 63.96% of samples, TRS-YOLO achieves 90.7% mAP@50—surpassing YOLOv12 by 2.9% and outperforming state-of-the-art YOLOv13 by 4.2%—all while maintaining ultra-efficient deployment with only 2.95 M parameters and 166 FPS real-time inference speed. This demonstrates TRS-YOLO’s unique capability to deliver superior accuracy for critical small targets and exceptional lightweight adaptability for edge-device deployment.},
  archive      = {J_JRTIP},
  author       = {Wang, Jiao and Li, Bin},
  doi          = {10.1007/s11554-025-01754-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {TRS-YOLO: A lightweight insulator defect detection method based on enhanced YOLOv12},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time multi-object detection and tracking in UAV systems: Improved YOLOv11-EFAC and optimized tracking algorithms. <em>JRTIP</em>, <em>22</em>(5), 1-28. (<a href='https://doi.org/10.1007/s11554-025-01758-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time small-object detection and tracking from UAVs is inherently challenging due to tiny object sizes, rapid viewpoint changes, and stringent accuracy–speed constraints, yet remains essential for mission-critical defense, security, and disaster response applications. We propose YOLOv11-EFAC, using YOLOv11n as baseline, a UAV-oriented detection framework employing a multi-level optimization strategy: an EfficientNet-B0 backbone for lightweight, high-quality feature extraction; hybrid FPN+PANet for enhanced multi-scale fusion; Squeeze-and-Excitation attention for adaptive channel weighting; and a custom loss function tailored for small-object localization. For tracking, we comparatively evaluate multiple state-of-the-art algorithms, including DeepSORT, EKF, ByteTrack, SORT, CenterTrack, FairMOT, and the transformer-based TrackFormer under realistic UAV operational conditions. Additionally, we introduce a hybrid DeepSORT+EKF approach to better handle non-linear motion, achieving 89.9% MOTA, an 11.5% improvement over standalone DeepSORT, with reduced identity switches. A 42,500-image hybrid dataset (58.1% small objects) combining COCO, VisDrone, and UAVDT improves robustness and generalization. Experimental results demonstrate 83.7% mAP@0.5 at 89 FPS on embedded hardware with a 21.4% small-object detection improvement, surpassing YOLOv8, YOLOv12, YOLOv13, and multiple YOLOv11 variants. These results position YOLOv11-EFAC as a robust, real-time solution for mission-critical UAV applications under operational constraints.},
  archive      = {J_JRTIP},
  author       = {Kıratlı, Rabia and Eroğlu, Alperen},
  doi          = {10.1007/s11554-025-01758-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-28},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time multi-object detection and tracking in UAV systems: Improved YOLOv11-EFAC and optimized tracking algorithms},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate insulator defect detection in power transmission lines using semi-supervised hybrid DETR with advanced loss methods. <em>JRTIP</em>, <em>22</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01760-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reliability and safety of power transmission grids critically depend on the condition of insulators, making efficient and accurate defect detection essential. While CNN-based object detection frameworks like YOLO offer a reasonable balance between speed and accuracy, they face limitations in handling class imbalance and complex backgrounds. Transformer-based detectors (e.g., DETR) eliminate the need for Non-Maximum Suppression (NMS) but suffer from high computational costs and slower convergence. To address these challenges, the proposed method is a hybrid Detection Transformer (DETR) framework for insulator defect detection, combining semi-supervised learning with advanced loss methods. Our method introduces Stage-wise Hybrid Matching for generating high-quality pseudo-labels, focal loss to address data imbalance, and Cross-view Query Consistency to enhance feature robustness. Additionally, encoder complexity is optimized, ensuring scalability for UAV- based inspections. Experimental results demonstrate that the proposed hybrid DETR achieves 85 FPS, surpasses state-of-the-art detectors, including YOLO and DETR variants, achieving superior accuracy and speed across diverse and challenging datasets.},
  archive      = {J_JRTIP},
  author       = {Sankuri, Raja Sekhar and Sristy, Nagesh Bhattu and Karri, Sri Phani Krishna},
  doi          = {10.1007/s11554-025-01760-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Accurate insulator defect detection in power transmission lines using semi-supervised hybrid DETR with advanced loss methods},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ELSD-YOLO: An enhanced lightweight ship detection framework based on YOLO11n. <em>JRTIP</em>, <em>22</em>(5), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01757-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship detection presents significant challenges, including limited adaptability to complex maritime environments, the trade-off between lightweight design and accuracy, and the degradation of generalization caused by model compression. To address these issues, we propose ELSD-YOLO, a lightweight ship detection algorithm built upon the YOLO11n architecture. A GhostNet-based backbone is constructed to ensure efficient computation while preserving feature representation, depthwise separable convolutions are employed to accelerate inference, and the StarNet block is integrated to enhance nonlinear modeling, thereby improving feature extraction for small and densely distributed ships. In addition, a SimHead module is designed to eliminate redundant computations while maintaining channel-wise parallelism, achieving a balance between efficiency and accuracy. Structural simplification and quantization further enable deployment on resource-constrained edge devices for real-time operation. Experimental results on the SeaShips dataset demonstrate that ELSD-YOLO not only achieves a 1.1% improvement in mAP50-95 compared with the baseline YOLO11n, alongside an 11% reduction in model parameters and a 19% reduction in computational complexity, but also exhibits superior robustness in detecting small vessels and handling complex background interference. These results validate ELSD-YOLO as an efficient and accurate framework, offering strong technical support for maritime traffic safety.},
  archive      = {J_JRTIP},
  author       = {Meng, Xue and Bi, Zhenbo and Jia, Lei and Wang, Tianyuan and Meng, Xuejian},
  doi          = {10.1007/s11554-025-01757-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ELSD-YOLO: An enhanced lightweight ship detection framework based on YOLO11n},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-DBS: A multi-scale feature fusion-based surface defect detection method of small out-line package. <em>JRTIP</em>, <em>22</em>(5), 1-18. (<a href='https://doi.org/10.1007/s11554-025-01755-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel surface detection model, designated as YOLO-DCFB-BiFPN-SENetV2 (YOLO-DBS), is introduced to address the limitations in detection accuracy of existing methodologies for surface defects in Small Out-line Package (SOP). Initially, the integration of the Coordinate Attention (CA) channel attention mechanism enhances the C2f module within the feature fusion network, thereby facilitating the successful identification of irregularly shaped defect features. Subsequently, to augment the model’s feature expression capability for minor defects while reducing its parameters, a lightweight and efficient multi-scale feature fusion network, termed BiFPN, is employed. Finally, the SENetV2 module is incorporated into the detection head to bolster the model’s ability to discern minor defects against similar backgrounds and mitigate external noise interference. Comparative experiments conducted on a self-constructed SOP dataset demonstrate that YOLO-DBS surpasses more advanced defect detection techniques, achieving a detection accuracy (mAP) of 99.4%. Moreover, the YOLO-DBS model’s parameter count is merely 2.5 million, which is 0.5 million fewer than that of the original model, illustrating how YOLO-DBS effectively balances model complexity and accuracy, thereby providing a reliable method for identifying surface defects in chip packaging within real-world industrial contexts.},
  archive      = {J_JRTIP},
  author       = {Xing, Yikai and Fang, Xin and Zhou, Yongbing and Zhang, Jian and Chen, Haojie},
  doi          = {10.1007/s11554-025-01755-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-DBS: A multi-scale feature fusion-based surface defect detection method of small out-line package},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCC-DETR: A real-time lightweight gesture recognition network for home human–robot interaction. <em>JRTIP</em>, <em>22</em>(5), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01763-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition, an intuitive and efficient human–robot interaction method, shows great potential in smart living applications. However, deployment in home environments faces challenges from complex backgrounds, hand-like distractors, and varying illumination, particularly for lightweight and real-time implementations. To address these limitations in balancing efficiency, speed, and accuracy, a novel lightweight static gesture recognition network, DCC-DETR, based on RT-DETR, is proposed. First, an improved StarNet backbone is developed to enhance feature extraction efficiency, optimizing performance without compromising accuracy. Second, a Cascaded Group Local Attention Network (CGLAN) is designed to improve the perception and processing of local information significantly. Third, a Context-Guided Spatial Reconstruction Feature Pyramid Network (CSRFPN) enhances multi-scale feature representation and robust feature integration. Finally, the Wise Focaler-ShapeIoU loss function enables robust bounding box regression, leading to improved localization precision. Extensive experiments on self-built, 100 Days of Hands, and EgoHands datasets demonstrate DCC-DETR’s superiority in detection accuracy, inference speed, and model efficiency, highlighting its practical utility across diverse scenarios. Compared to RT-DETR, DCC-DETR improves mAP50 by 1.0% and increases FPS by 105.8%, while reducing computation by 80.0%, parameters by 63.2%, and significantly compressing model size, making it highly suitable for resource-limited environments. When deployed on NVIDIA Jetson AGX Orin with TensorRT acceleration, DCC-DETR achieves an end-to-end GPU inference latency of 6.63 ms, 1.62 $$\times$$ faster than RT-DETR (10.71 ms) under identical conditions, demonstrating superior real-time capability.},
  archive      = {J_JRTIP},
  author       = {Chen, Xianyi and Yin, Hao},
  doi          = {10.1007/s11554-025-01763-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DCC-DETR: A real-time lightweight gesture recognition network for home human–robot interaction},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WT-DETR: Wavelet-enhanced DETR for robust tiny object detection via multi-scale feature optimization. <em>JRTIP</em>, <em>22</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01761-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tiny object detection remains a challenging task in computer vision, with broad applications in remote sensing, intelligent transportation, and aerial surveillance. Although recent advancements have improved detection accuracy, DETR-based methods such as RT-DETR still struggle with tiny objects due to limited receptive fields, aliasing artifacts, and the loss of fine-grained details. To address these challenges, we propose WT-DETR, an enhanced version of RT-DETR that incorporates wavelet transform-based optimizations. WT-DETR introduces Wave Field Convolution (WFC) to expand the receptive field while capturing global context and structural features with minimal parameter overhead. Furthermore, Wavelet Anti-Aliasing Downsampling (WTD) replaces conventional downsampling to mitigate aliasing and retain fine details, while maintaining computational efficiency. By integrating these components, WT-DETR improves multi-scale feature representation without sacrificing speed. Extensive experiments on the VisDrone2019 and SIMD datasets demonstrate that WT-DETR achieves $$mAP_{50}$$ scores of 59.65% and 81.0%, respectively, while maintaining an inference speed of 90.3 FPS–striking an effective balance between accuracy and real-time performance, and delivering competitive results compared to state-of-the-art methods.},
  archive      = {J_JRTIP},
  author       = {Shao, Xiaoyan and Diao, Shiqin and Li, Lingling and Zhao, Xuezhuan and Mei, Yang and Zhu, Zonghao},
  doi          = {10.1007/s11554-025-01761-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {WT-DETR: Wavelet-enhanced DETR for robust tiny object detection via multi-scale feature optimization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PGCI-YOLO: A railway fastener detection algorithm based on improved YOLOv8n. <em>JRTIP</em>, <em>22</em>(5), 1-25. (<a href='https://doi.org/10.1007/s11554-025-01762-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Railway fasteners are critical components for maintaining track structural stability and ensuring the safe operation of trains. Defect detection of fasteners plays a vital role in achieving intelligent railway maintenance. To address the challenge of balancing accuracy and efficiency in existing detection models, this paper proposes a lightweight detection algorithm based on the YOLOv8n architecture—PGCI-YOLO. First, a Pyramid Spatial Attention Module (PSAM) is designed, which integrates multi-scale channel grouping and spatial attention to significantly enhance the model’s perception of complex target regions. Second, the GSCSP module is integrated into the Neck. Composed of GSConv and VoV-GSCSP, this module reduces the number of parameters and computational complexity while preserving rich semantic information, thereby improving inference efficiency. Third, the CARAFE upsampling operator replaces traditional interpolation methods to enable adaptive, content-aware feature reconstruction. Finally, a novel regression loss function, Inner-Focaler-MPDIoU (IFM), is constructed by combining sample difficulty weighting, corner modeling, and internal consistency constraints, which improves bounding box localization accuracy and accelerates model convergence. Experimental results show that PGCI-YOLO achieves 97.3% mAP@0.5, 69.3% mAP@0.5:0.95, and an inference speed of 116 FPS on the M-type fastener dataset, with only 2.67 M parameters and 6.7 GFLOPs, comprehensively outperforming other mainstream models. Compared with the original YOLOv8n, PGCI-YOLO improves mAP@0.5 by 2.1%, increases inference speed by 8 FPS, reduces parameters by 11%, decreases computation by 17.3%, and shrinks model size by 12.7%. Further tests on the E-type fastener dataset and edge platforms such as Jetson AGX Orin and KC-7600 demonstrate the model’s strong robustness, generalization capability, and deployment adaptability. PGCI-YOLO achieves an excellent balance between detection accuracy and real-time performance while maintaining a lightweight architecture, making it highly suitable for practical engineering applications.},
  archive      = {J_JRTIP},
  author       = {Ma, Siwei and Li, Ronghua and Hu, Henan},
  doi          = {10.1007/s11554-025-01762-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-25},
  shortjournal = {J. Real-Time Image Process.},
  title        = {PGCI-YOLO: A railway fastener detection algorithm based on improved YOLOv8n},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Backend-free multi-scale feature fusion network for defect detection in printed circuit board images. <em>JRTIP</em>, <em>22</em>(5), 1-18. (<a href='https://doi.org/10.1007/s11554-025-01759-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Printed Circuit Board (PCB) defect detection in industrial scenarios is of great significance to many fields, such as computers and aerospace. In recent years, the YOLO series of algorithms have been widely used in target detection and have achieved great advantages. The detection performance has also been continuously improved with the iteration of versions. However, the YOLO series of algorithms rely on the Non-Maximum Suppression (NMS) processing of the backend, which increases the time and computational cost. In this paper, we propose a PCB defect detection algorithm based on a backend-free processing method. The defect detection method based on backend-free processing avoids the limitation of the anchor box of the traditional YOLO series of algorithms on the detection ability of the model, reducing the time and computational cost. In addition, the proposed attention-based scale fusion network effectively improves the detection performance of the model and enhances the feature extraction ability of small targets. In addition, we superimpose a shallow small-target detection head and introduce an attention mechanism in the detection model to improve the model's solution space ability. We evaluate the proposed detection algorithm on the public Peking University PCB dataset. The results show that our algorithm has significant advantages over other SOTA algorithms in detection accuracy and efficiency. The detection algorithm proposed in this paper achieved an accuracy of 99.7% on Peking University's dataset, representing a 2.3% improvement over YOLO12. Notably, for the "Open_circuit" and "Spurious_copper" categories, the AP values reached 99.9% and 98.9%, respectively, Marking significant increases of 4.1% and 1.2% compared to YOLO12. Furthermore, the algorithm demonstrates exceptional efficiency, with a detection time of merely 0.01195 s per image.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yapin and Guo, Ruiqiang and Li, Min},
  doi          = {10.1007/s11554-025-01759-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Backend-free multi-scale feature fusion network for defect detection in printed circuit board images},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time recognition of miner unsafe behaviors via skeleton-based spatiotemporal modeling with mamba. <em>JRTIP</em>, <em>22</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01764-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring miner safety in underground environments demands vision systems that combine high accuracy with real-time performance under resource constraints. While existing video-based monitoring solutions suffer from limitations where CNNs struggle with long-range modeling and Transformers face high computational complexity, we propose MinerUBR, a compact skeleton sensing framework that converts raw video streams into compact 3D heatmap volumes via pose estimation, replacing traditional graph sequence inputs. By leveraging a novel bidirectional spatiotemporal compression module based on video state space modeling, enhanced via bidirectional scanning, our method achieves efficient joint modeling of skeleton sequences with only 1.9M parameters while retaining spatiotemporal information through spatial and temporal embeddings. Validated on both public datasets including NTURGB+D and our self-constructed dataset featuring 20 unsafe miner behaviors collected using Kinect V2 sensors in simulated mine environments, MinerUBR-B achieves 91.2% accuracy on NTU60 and 92.8% accuracy on the mining dataset with only 1.9M parameters. The system operates at 138 FPS on CPU platforms, demonstrating 10.6 times and 3.8 times faster inference speed compared to PoseC3D and ST-GCN, respectively. This work provides practical insights for implementing lightweight behavior recognition in resource-constrained mining environments, balancing accuracy, and efficiency for industrial deployment.},
  archive      = {J_JRTIP},
  author       = {Li, Biao and Tang, Shoufeng and Li, Wenyi},
  doi          = {10.1007/s11554-025-01764-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time recognition of miner unsafe behaviors via skeleton-based spatiotemporal modeling with mamba},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LMDNet: A lightweight network for infrared small target detection based on mamba and difference convolution. <em>JRTIP</em>, <em>22</em>(5), 1-20. (<a href='https://doi.org/10.1007/s11554-025-01765-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared small target detection faces significant challenges in complex backgrounds and has garnered widespread attention in recent years. To address the trade-off between detection accuracy and computational complexity in current methods, we do not solely pursue higher detection accuracy or IoU values. Instead, we aim to achieve a more lightweight model—one with fewer parameters and reduced computational load—while surpassing the average detection accuracy and IoU values of existing state-of-the-art models. The proposed network introduces a spatial-channel difference convolution module, which efficiently extracts spatial and channel features while maintaining low computational cost. Additionally, considering the characteristics of the SIRST dataset, we design a global feature enhancement Mamba layer that effectively accelerates target localization and ensures precise detection in complex backgrounds. Furthermore, we propose a self-learning joint loss function, which dynamically adjusts weights during training to optimize the contributions of different loss components. The proposed method offers significant advantages in inference speed and parameter size: the FPS is several times higher than that of larger model-based methods, with Params and FLOPs of only 0.024M and 0.168G, respectively, far lower than existing lightweight and full-size models. Its overall performance outperforms current lightweight methods and surpasses some large model-based approaches.},
  archive      = {J_JRTIP},
  author       = {Zang, Dongyuan and Su, Weihua and Song, Zijing and Huang, Jiabao and Yin, Meng and Ma, Jun and Song, Shenao},
  doi          = {10.1007/s11554-025-01765-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-20},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LMDNet: A lightweight network for infrared small target detection based on mamba and difference convolution},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Road crack detection algorithm based on fusion structure re-parameterization with multi-scale parallel convolutions and hybrid attention mechanism. <em>JRTIP</em>, <em>22</em>(5), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01766-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road cracks constitute a predominant form of pavement distress, significantly impacting durability and traffic safety. However, crack detection remains challenging due to interference from significant scale variations and complex backgrounds. Timely detection is a critical prerequisite for effective road maintenance. To address these challenges, this paper presents ReLA-YOLO, an enhanced road crack detection model based on the YOLOv11n architecture. First, the proposed model integrates a structure-reparameterized multi-scale parallel convolution module coupled with a hybrid attention mechanism (ReLA). This integration effectively captures global contextual information of cracks across diverse scales while suppressing background interference. Second, within the backbone network, receptive field attention convolution (RFCAConv) is introduced to optimize the downsampling process, thereby enhancing crack feature extraction. Finally, a simplified spatial pyramid pooling fast module (SimSPPF) is employed in the feature pyramid layer to improve computational efficiency. Experimental validation on the RDD2022 dataset demonstrates that ReLA-YOLO achieves a mean average precision (mAP) of 87.8%, surpassing the baseline model by 2.6%. These results indicate that the proposed ReLA-YOLO model provides an effective solution to the challenges posed by significant scale variations and complex backgrounds in road crack detection, exhibiting high potential for practical engineering applications.},
  archive      = {J_JRTIP},
  author       = {Li, Shanqiang and Lin, Zhiqiang and Shi, Yujing and Lan, Junjie and Zhuo, Yu},
  doi          = {10.1007/s11554-025-01766-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Road crack detection algorithm based on fusion structure re-parameterization with multi-scale parallel convolutions and hybrid attention mechanism},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCF-YOLO: A high-performance traffic sign detection model with hybrid channel fusion and auxiliary box regression. <em>JRTIP</em>, <em>22</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01767-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic signs are critical for road safety and autonomous driving. However, their detection remains challenging due to hardware limitations and the small size of many signs. Existing methods often suffer from low accuracy and poor adaptability. To address these issues, we propose the hybrid channel feature fusion based you only look once framework (HCF-YOLO), a novel traffic sign detection model. HCF-YOLO incorporates a hybrid channel feature fusion module (HCFF) that efficiently combines channel, spatial, local and global information to enhance feature representation with low computational cost. An auxiliary-enhanced minimum point distance IoU (auxiliary-enhanced MPD-IoU) to improve the bounding box regression function based on auxiliary boxes and minimum point distance error is introduced to improve spatial alignment and reduce scale sensitivity. Additionally, an attention scale sequence feature fusion mechanism (ASFF) and hybrid channel feature fusion module are added to the P2 detection layer, improving multi-scale feature extraction and reducing parameter redundancy. We evaluate HCF-YOLO on the Tsinghua-Tencent 100K (TT100K), Chinese Traffic sign detection benchmark (CCTSDB), and German traffic sign detection benchmark (GTSDB) datasets. The model achieves mAP@50 scores of 79.3 $$\%$$ , 88.7 $$\%$$ and 87.3 $$\%$$ , outperforming the attentional scale sequence fusion YOLO (ASF-YOLO) baseline by 5.0 $$\%$$ , 1.3 $$\%$$ and 3.1 $$\%$$ . For mAP@50:95, HCF-YOLO improves performance by 3.5 $$\%$$ , 1.0 $$\%$$ and 1.6 $$\%$$ , respectively. Meanwhile, the FPS remains at 153. In general, HCF-YOLO delivers higher accuracy and efficiency with fewer parameters, showing strong potential for real-world deployment.},
  archive      = {J_JRTIP},
  author       = {Ren, Hongge and Song, Hairui and Liu, Haiqiang and Fan, Anni and Tan, Yingying},
  doi          = {10.1007/s11554-025-01767-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {HCF-YOLO: A high-performance traffic sign detection model with hybrid channel fusion and auxiliary box regression},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient VLSI design for real-time JPEG 2000 EBCOT module with optimized codestream truncation. <em>JRTIP</em>, <em>22</em>(5), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01769-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EBCOT is a critical module in the JPEG 2000 image compression algorithm, where the Tier-2 encoding is responsible for optimizing the truncated bitstream to achieve high-quality streams at various compression ratios. This directly impacts key metrics such as image quality at specified compression levels. However, the commonly used Tier-2 module based on the post-compression rate-distortion (PCRD) algorithm suffers from low encoding efficiency and high hardware resource consumption. This paper converts the division and logarithmic operations required by the PCRD algorithm into addition and lookup table calculations. In addition, it integrates the rate-distortion slope and byte distribution patterns derived from extensive statistics of images, proposing a VLSI architecture of the Tier-2 module based on a compact lookup table that saves significant resources. Experimental results show that the images compressed by this architecture experience a maximum PSNR decrease of up to 0.41 dB at different compression ratios, with a at least 4.43% reduction in the number of encoding cycles. This enhances encoding speed while maintaining encoding quality. Compared to other existing EBCOT architectures, this design achieves a 45% resource savings on FPGA while reducing area and power consumption by over 77% in ASIC implementations compared to traditional Tier-2 architectures. The proposed architecture successfully completes high-quality JPEG 2000 image compression tasks with lower resource consumption and faster encoding times, meeting real-time requirements.},
  archive      = {J_JRTIP},
  author       = {Yu, Ke and Shi, Lin and Dai, Yuzhou and Li, Qitao and Liu, Yanyan},
  doi          = {10.1007/s11554-025-01769-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient VLSI design for real-time JPEG 2000 EBCOT module with optimized codestream truncation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in real-time road damage detection: A comprehensive survey of methodologies and datasets. <em>JRTIP</em>, <em>22</em>(4), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01683-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of road defects like potholes and cracks is essential for timely driver warnings, maintenance prioritization, and accident prevention. However, there is a gap in developing practical real-time road damage detection algorithms. This survey reviews the most efficient deep learning models and available Road Damage Detection (RDD) datasets, comparing them based on accuracy, complexity, and inference rate for real-time application suitability. This survey investigates the available datasets for road damage detection and the evaluation metrics used to assess object detection models. Additionally, it explores recent methods and deep learning models for small object detection, including You Only Look Once (YOLO), Region-based Convolutional Neural Network (R-CNN), and Single Shot Detector (SSD). The survey also provides an overview of Convolutional Neural Networks (CNN) fundamentals and attention mechanisms. A comparative analysis of the models and datasets is conducted to highlight their strengths and limitations. The key findings in road damage detection, particularly those related to the Crowdsensing-based Road Damage Detection Challenge (CRDDC, 2022), are summarized.},
  archive      = {J_JRTIP},
  author       = {Abdelwahed, Salma H. and Sharobim, Bishoy K. and Wasfey, Bishoy and Said, Lobna A.},
  doi          = {10.1007/s11554-025-01683-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Advancements in real-time road damage detection: A comprehensive survey of methodologies and datasets},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PFMultiNet: A progressive fusion ship detection network for underactuated unmanned surface vehicles in complex environments. <em>JRTIP</em>, <em>22</em>(4), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01697-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of underactuated Unmanned Surface Vehicles (USVs) in ship detection and maritime surveillance has highlighted the need for advanced target detection models. However, hardware limitations and environmental constraints impact the perception capability of USVs. This increases the demands on both the speed and accuracy of lightweight detection algorithms. In this study, we adopted a progressive fusion approach to optimize feature information integration, balancing computational resources and accuracy. The network, named PFMultiNet, is based on Yolov7-tiny. It utilizes an interaction strategy between sub-backbones of varying depths in its backbone to enhance the extraction of positional features. Compared to multi-backbone networks with the same depth, this approach leverages the advantages of a multi-backbone architecture while mitigating the computational burden. Feature Interaction Network (FIN) is proposed, combining multi-scale information and cross-layer attention weights, significantly improving the extraction of contour features. Progressive Fusion Network (PFN) is adopted as the Neck network, where a hierarchical fusion strategy is used to effectively reduce redundant information while preserving the expression of key features. The proposed Progressive Fusion Module (PFM) integrates information from different scales layer by layer, optimizing the feature representation capability and enhancing the model’s adaptability and accuracy in handling multi-scale targets. Experimental results indicate that PFMultiNet outperforms the existing state-of-the-art networks, achieving an accuracy of 97.1% with a model size of only 15.7 MB, while maintaining an FPS of 72. Additionally, this paper delves into the optimal combinations of multi-backbone networks, providing valuable insights for research in the field of ship detection.},
  archive      = {J_JRTIP},
  author       = {Zhou, Weina and Wei, Shao and Hu, Wenhua},
  doi          = {10.1007/s11554-025-01697-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {PFMultiNet: A progressive fusion ship detection network for underactuated unmanned surface vehicles in complex environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight object drift verification network based on feature fusion and dual-template for long-term tracking. <em>JRTIP</em>, <em>22</em>(4), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01711-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifying whether the tracking result drifts during long-term tracking is a critical challenge. It’s difficult to select the optimal threshold in traditional threshold-based object drift verification criteria, and existing threshold-free object drift verification networks perform poorly in complex scenarios. To address these issues, we propose an object drift verification network based on multi-scale feature fusion and dual-template, using static and dynamic templates for joint verification. During the feature extraction stage, a multi-scale feature fusion module is introduced to adapt to changes in the object’s scale. Additionally, A template update strategy is devised to obtain high-quality dynamic templates for effective object drift verification. This network doesn’t require manual threshold setting and can be used as a plug-and-play module combined with a short-term visual tracking algorithm and a global re-detection module for long-term tracking. The proposed network forms four long-term tracking algorithms by integrating with four short-term visual tracking algorithms (DiMP50, PrDiMP, TrDiMP, and TransT). Extensive experiments on datasets like LaSOT, UAV20L, VOT2018-LT, and VOT2020-LT demonstrate significant improvements in long-term tracking performance. On the UAV20L dataset, the success rate and precision improved by 8.5% and 9.4%, respectively, compared to the base algorithm TrDiMP. On the VOT2020-LT dataset, the F-score improved by 6.1% compared to the base algorithm PrDiMP. Moreover, the proposed network achieves a verification speed of 220 FPS, with minimal impact on long-term tracking speed.},
  archive      = {J_JRTIP},
  author       = {Hou, Zhiqiang and Chen, Yu and Zhao, Jiaxin and Ma, Sugang and Yu, Wangsheng and Wang, Yunchen},
  doi          = {10.1007/s11554-025-01711-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight object drift verification network based on feature fusion and dual-template for long-term tracking},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv8n-PP: A lightweight pose recognition algorithm for photovoltaic array cleaning robot. <em>JRTIP</em>, <em>22</em>(4), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01713-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular cleaning of the photovoltaic (PV) panel is crucial for maintaining optimal photovoltaic power generation efficiency. However, manual cleaning methods for PV panels are often inadequate and costly, underscoring the need for the introduction of PV cleaning robots. A significant challenge in the development of these robots is the recognition of PV panel poses. We propose the YOLOv8n-Photovoltaic-Pose (YOLOv8n-PP) method, a lightweight pose recognition algorithm, to address this issue. It is specifically designed for PV panel cleaning robots. Built upon the YOLOv8 framework, YOLOv8n-PP incorporates the Mobile-ViT visual module, which is both lightweight and mobile-friendly. This integration helps mitigate the effects of varying target poses from the robot’s mobile perspective. Additionally, we introduce the LMPDIoU boundary box regression loss to enhance the precision of PV panel recognition. Furthermore, we have developed a diverse and comprehensive dataset for PV panel poses to improve the model’s generalization capabilities. Our method shows improvements in both precision and recall, providing an effective solution for PV pose recognition.},
  archive      = {J_JRTIP},
  author       = {Luo, Jidong and Wang, Guoyi and Lei, Yanjiao and Wang, Dong and Zhang, Hongzhou},
  doi          = {10.1007/s11554-025-01713-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLOv8n-PP: A lightweight pose recognition algorithm for photovoltaic array cleaning robot},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fsd-detr: Casting surface defect detection based on improved RT-DETR. <em>JRTIP</em>, <em>22</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01714-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve efficient and accurate detection of casting surface defects, this paper proposes FSD-DETR, a casting surface defect detection model based on real-time detection transformer (RT-DETR). First, the model adopts StarNet as the backbone network, replacing the original ResNet18, which effectively reduces both computational cost and parameter count, while significantly improving object detection accuracy. Second, frequency channel attention (FCA) is introduced during the feature extraction stage of the StarNet backbone to enhance the perception of small-scale defect targets by optimizing feature interaction. Additionally, a Feature Reuse Adaptive (FRA) module is designed and integrated into the CCFF component to strengthen multi-scale feature fusion, thereby improving detection performance. Finally, the GIoU loss is replaced with PIoU, which effectively suppresses false overlaps between background and target boxes, further enhancing detection accuracy and accelerating model convergence. In terms of performance evaluation, FSD-DETR achieves a mAP@50 of 96.1% on the CSDD dataset, representing a 1.7% improvement over the original RT-DETR model and a 2.1% to 6.8% gain compared to other classical detection models. Meanwhile, the model’s parameters and GFLOPs are reduced by 34.2% and 34.9%, respectively, fully meeting the stringent requirements for real-time and high-precision casting surface defect detection. Experimental results on the publicly available NEU-DET dataset further confirm the model’s strong generalization ability, with a 5.3% improvement in detection performance.},
  archive      = {J_JRTIP},
  author       = {Ling, Liuyi and Xu, Shuai and Wei, Liyu and Wei, Guo and Jia, Jixiang and Hong, Bolun},
  doi          = {10.1007/s11554-025-01714-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fsd-detr: Casting surface defect detection based on improved RT-DETR},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLF-YOLO: A novel multiscale feature fusion network for remote sensing small target detection. <em>JRTIP</em>, <em>22</em>(4), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01716-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting small targets in aerial and remote sensing imagery remains challenging due to factors such as cluttered backgrounds, dense distributions, and low resolution of small objects. Existing detection networks often rely on simple multi-scale fusion strategies, which cause shallow features to be overwhelmed by high-level semantics, limiting the ability to identify fine-grained targets. Additionally, downsampling operations in deep layers result in the loss of spatial details, further reducing detection accuracy. Traditional loss functions also struggle with error sensitivity from small annotation boxes, making precise localization difficult in dense environments. To address these issues, we propose MLF-YOLO, a novel small target detection model tailored for complex remote sensing scenes. Specifically, we design a multiscale local fusion (MLF) module guided by attention, which effectively integrates deep semantic information into shallow layers while preserving local details. This module enhances the model’s ability to detect small targets in complex backgrounds and dense environments. To model global context and refine spatial associations, we introduce the feature aware contextual encoding transformer (FACET) module, which extracts contextual information and further improves detection accuracy in dense small target scenarios. Additionally, we design an IW-IoU loss function, which enhances localization robustness for small targets under occlusion and dense scenarios. We evaluate MLF-YOLO on the RSOD, NWPU VHR-10, VisDrone2-019, and large-scale DOTAv2.0 datasets. Our model achieves improvements of 2%, 26.4%, and 7.7% in the mAP50 metric, respectively, while maintaining excellent performance and real-time capabilities on the large-scale DOTAv2.0 dataset. In summary, the proposed innovations comprehensively enhance the overall performance of MLF-YOLO in remote sensing small object detection tasks, especially in complex backgrounds and dense small target scenarios.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yanshan and Wu, Chengjun and Fan, Yuanzhang},
  doi          = {10.1007/s11554-025-01716-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {MLF-YOLO: A novel multiscale feature fusion network for remote sensing small target detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HGO-YOLO: Advancing anomaly behavior detection with hierarchical features and lightweight optimized detection. <em>JRTIP</em>, <em>22</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01717-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate, real-time object detection on resource-constrained hardware is critical for anomaly behavior monitoring. We introduce HGO-YOLO, a lightweight detector that combines GhostHGNetv2 with an optimized parameter-sharing head (OptiConvDetect) to deliver an outstanding accuracy–efficiency trade-off. By embedding GhostConv into the HGNetv2 backbone with multi-scale residual fusion, the receptive field is enlarged while redundant computation is reduced by 50%. OptiConvDetect shares a partial-convolution layer for the classification and regression branches, cutting detection-head FLOPs by 41% without accuracy loss. On three anomaly datasets (fall, fight, smoke), HGO-YOLO attains 87.4% mAP@0.5 and 81.1% recall at 56 FPS on a single CPU with just 4.3 GFLOPs and 4.6 MB—surpassing YOLOv8n by + 3.0% mAP, − 51.7% FLOPs, and 1.7 × speed. Real-world tests on a Jetson Orin Nano further confirm a stable throughput gain of 42 FPS.},
  archive      = {J_JRTIP},
  author       = {Zheng, Qizhi and Luo, Zhongze and Guo, Meiyan and Wang, Xinzhu and Wu, Renqimuge and Meng, Qiu and Dong, Guanghui},
  doi          = {10.1007/s11554-025-01717-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {HGO-YOLO: Advancing anomaly behavior detection with hierarchical features and lightweight optimized detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ELA: Efficient location attention for deep convolution neural networks. <em>JRTIP</em>, <em>22</em>(4), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01719-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanisms are pivotal for enhancing deep convolutional neural networks (CNNs) in computer vision, yet existing methods often inadequately leverage spatial information, reduce channel dimensions, or escalate network complexity—challenges exacerbated by Transformer-like quadratic costs. This paper introduces efficient localization attention (ELA), a novel, lightweight module that achieves highly competitive performance among lightweight attention mechanisms across image classification, object detection, and semantic segmentation, while maintaining computational efficiency. By analyzing coordinate attention (CA), we identify its limitations, including batch normalization’s generalization issues, channel dimensionality reduction’s negative impact, and complex attention generation. To overcome these, ELA employs 1D convolutions and group normalization (GN) to encode spatial features without channel reduction, capturing long-range dependencies and position-sensitive information effectively. We developed four versions of ELA-T, ELA-B, ELA-S, and ELA-L—optimized for different visual tasks. Extensive experiments on ImageNet, MS COCO, and Pascal VOC demonstrate ELA’s superiority, achieving up to 2.4% Top-1 accuracy gain, 1.96% mAP improvement, and 1.08% Mean IoU increase over state-of-the-art methods, making it ideal for resource-constrained, real-world applications. Project page: https://github.com/Xuwei86/ELA_Code},
  archive      = {J_JRTIP},
  author       = {Xu, Wei and Wan, Yi and Zhao, Weina},
  doi          = {10.1007/s11554-025-01719-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ELA: Efficient location attention for deep convolution neural networks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FAN-YOLO: Real-time driver behavior detection based on full-layer aggregation network of YOLO. <em>JRTIP</em>, <em>22</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01720-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driver behavior detection is crucial to improving traffic safety and protecting driver health. Traditional driver behavior monitoring methods often rely on complex models with high computing resources, which are difficult to meet the requirements of real-time and lightweight. This study proposes a full-layer aggregation network (FAN) based on the YOLO (You Only Look Once) architecture, and constructs a lightweight driver behavior detection algorithm called FAN-YOLO. FAN-YOLO combines the YOLO object detection framework and lightweight network structure. Based on the standard YOLO framework, it changes the network structure of the neck layer path aggregation network (PAN) and adopts FAN to alleviate the problems of complex PAN path interactions and information redundancy. At the same time, a new feature fusion module is proposed in the backbone network, which we call RC2A, to extract better feature information. Then, the spatial pyramid pooling (SPP) network is optimized to obtain SPPF_WD, so that FAN-YOLO can detect driver behavior more accurately. FAN-YOLO is experimented on two datasets of abnormal driver behavior. The experimental results show that the number of parameters of FAN-YOLO is only 2.2M, which is 18.5% lower than the SOTA model YOLOv8-N in YOLO. The calculation amount is 6.2G, which is 10% lower. On the improved datasets State Farm v2 and YawDD v2, $$AP_{50}^{val}$$ can reach 86.8% and 97.5%, 1.3% higher than YOLOv8-N. The response speed is 1.9ms, which is 29.6% higher.},
  archive      = {J_JRTIP},
  author       = {Gan, Xinsu and Huang, Lidong and Yuan, Yan and Deng, Yan},
  doi          = {10.1007/s11554-025-01720-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FAN-YOLO: Real-time driver behavior detection based on full-layer aggregation network of YOLO},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on real-time obstacle detection algorithm for driverless electric locomotive in mines based on RSAE-YOLOv11n. <em>JRTIP</em>, <em>22</em>(4), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01721-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many challenges, such as low light, high noise, and complex obstacle distributions, when driverless electric locomotives operate in complex underground coal mine environments. These factors often result in low detection accuracy, especially for multiple or small targets. This paper proposes an improved real-time obstacle detection model, RSAE-YOLOv11n. First, the model is optimized by integrating an RCM into the C3k2 structure, this facilitates the capture of a broader spectrum of contextual information, enhancing the representation of intricate features and improving the extraction of multi-scale characteristics of the target. Second, the SPPF module is refined with SENetv2 to enhance the feature extraction performance of minor target detail features. Third, the ADown module substitutes conventional convolution to realize the model’s lightweight further. Finally, the EMA is implemented to improve the generalization and robustness of the model effectively. Experimental findings demonstrate that RSAE-YOLOv11n achieves a mAP of 90.7% on a self-constructed coal mine obstacle data set, surpassing the original YOLOv11n by 2%, with a 4.2% in P, a 1.2% in R, a 12% reduction in the parameter count, a 14.3% decrease in GFLOPs, an 11% diminution in model size, and the FPS reaches 85.7. Compared to other target detection algorithms, such as YOLOv8 and YOLOv10, RSAE-YOLOv11n exhibits significant advantages in detection accuracy, detection speed, computational efficiency, and model lightweight, providing a strong theoretical foundation for the intelligent perception of driverless electric locomotives in underground coal mines.},
  archive      = {J_JRTIP},
  author       = {Bai, Yun and Zhou, Xinqiang and Hu, Song},
  doi          = {10.1007/s11554-025-01721-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Research on real-time obstacle detection algorithm for driverless electric locomotive in mines based on RSAE-YOLOv11n},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-MSDD: A model for detecting road defects in rainy conditions. <em>JRTIP</em>, <em>22</em>(4), 1-20. (<a href='https://doi.org/10.1007/s11554-025-01723-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of road defect images captured on rainy days is often degraded, making defect recognition a significant challenge. Detection models trained under normal weather conditions typically perform poorly in rainy conditions. This paper introduces a road defect detection model, YOLO-MSDD, specifically tailored for rainy-day scenarios to tackle the identified issue. Employing the advanced rain-removal model NeRD-Rain, synthetic rain streaks are eliminated from images. Additionally, a real rainy-day road defect dataset has been developed. The proposed YOLO-MSDD model utilizes the MobileViT network as its backbone, enabling it to learn both local features and global representations comprehensively. Strip Pooling has been integrated to concentrate on elongated defects and eliminate irrelevant information. Furthermore, the deformable large kernel attention module is improved, resulting in a new C2f_DLKA attention module, which enhances the fusion of multi-scale road defect features. Dynamic Convolution replaces standard convolution to adjust convolution kernels based on different defect types dynamically. Experimental results show that the YOLO-MSDD algorithm achieves significantly higher accuracy on the De-raining Road Damage Detection dataset, the real rainy-day road defects dataset, and the Crack-Forest dataset, with mAP50 improvements of 4.8%, 11.1%, and 3.1%, respectively, compared to the YOLOv8n model. Additionally, mAP50-90 improved by 4.1%, 7.3%, and 2%, respectively. The YOLO-MSDD model also outperforms other state-of-the-art object detection algorithms, demonstrating its high applicability and reliability for road defect identification in rainy conditions.},
  archive      = {J_JRTIP},
  author       = {Xu, Kesheng and Sun, Rong},
  doi          = {10.1007/s11554-025-01723-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-MSDD: A model for detecting road defects in rainy conditions},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RV-YOLO: Real-time object detection algorithm for rail transit platform scenarios. <em>JRTIP</em>, <em>22</em>(4), 1-19. (<a href='https://doi.org/10.1007/s11554-025-01718-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the special characteristics of the rail transit station scene, common object detection algorithms have false alarms and omissions when detecting targets such as carriers in this scene, and have relatively poor real-time performance. In this paper, we innovatively propose a real-time object detection algorithm combining image depth information and multi-scale attention mechanism, notated as RV-YOLO (Rail Vehicle YOLO). First, RV-YOLO designs the IDIM (image depth information module), which can design the sampling strategy according to the depth information of the image, and in the preprocessing module, the improved backbone module and head module process the image data containing different depth information differently; second, the multi-scale attention module is designed to improve the accuracy of the model in recognizing different angles as well as the occluded targets; then, we propose a new bounding box regression loss function for use in RV-YOLO, which enables the model to gradually focus on more useful information and can compensate for the influence of the shape and scale of the bounding box itself on the regression results as much as possible. Finally, we produced a private dataset ( https://github.com/ChinaZhangPeng/Carrier-Datasets ) for the station scenario, which compensates for the lack of the existing public dataset for the rail transit scenario. The experimental results show that RV-YOLO has an AP50 of 97.0% on LVD (Large Vehicle Dataset), 69.8% on MS COCO 2017 dataset, and an FPS of 202 on PASCAL VOC dataset. Compared with object detection models such as YOLOv10 and H-DETR, it provides higher detection accuracy and real-time performance.},
  archive      = {J_JRTIP},
  author       = {Dong, Ke and Li, Dongyang and Zhang, Jinjing and Zhao, Xinlei and Dong, Lijia and Zhang, Peng},
  doi          = {10.1007/s11554-025-01718-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {RV-YOLO: Real-time object detection algorithm for rail transit platform scenarios},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMC-YOLO: Efficient object detector for underwater small sonar target. <em>JRTIP</em>, <em>22</em>(4), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01724-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sonar target detection is crucial for ensuring maritime safety and supporting resource exploration. However, the complex and dynamic underwater environment presents significant challenges in accurately detecting multiscale targets. To balance detection performance and accuracy, we propose a novel sonar target detector based on You Only Look Once version 10 (YOLOv10). Our approach utilizes a redesigned backbone network that integrates Space-to-Depth Convolution (SPD-Conv) and a Mixed Local Channel Attention (MLCA) module to enhance detection performance in complex underwater environments. We also introduce the Content-Guided Feature Pyramid Network (CFPN), which combines a pyramid structure and the Content-Guided Attention Fusion (CGA-Fusion) module to enhance feature fusion and improve detection of small underwater targets. Extensive experiments on publicly available forward-looking sonar image datasets show that our model achieves a 2.4% improvement in mean Average Precision (mAP), a 1.6% increase in precision and a 3% boost in recall rate. Additionally, the inference time remains largely unchanged.},
  archive      = {J_JRTIP},
  author       = {Li, Bingru and Zhang, Runze and Xu, Xudong},
  doi          = {10.1007/s11554-025-01724-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SMC-YOLO: Efficient object detector for underwater small sonar target},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSH-YOLO: A lightweight framework with enhanced multi-scale features fusion for water surface object detection. <em>JRTIP</em>, <em>22</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01726-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenges of limited detection accuracy for small objects and inefficient model deployment in complex water surface scenarios by proposing an enhanced object detection algorithm based on YOLOv8n, referred to as DSH-YOLO. This algorithm achieves a balanced optimization of accuracy and efficiency through a lightweight architecture design and multi-scale feature enhancement strategies. By integrating the DualConv module into the backbone, it leverages parallel $$3\times 3$$ and $$1\times 1$$ convolution kernels to combine group and heterogeneous convolutions, enabling lightweight feature extraction with reduced parameters while preserving feature representation. The Slim-neck in the neck network incorporates generalized spatial convolution and lightweight bottleneck structures, constructing a multi-scale feature fusion mechanism that reduces computational load and maintains high detection accuracy. To enhance small-object detection, a high-resolution detection head is added to capture fine-grained low-level features. Additionally, a dataset tailored for water surface environments with diverse scales, lighting conditions, and wave interference, providing precise annotations, is introduced. Experimental results on this dataset show that the proposed algorithm outperforms baseline models across multiple metrics while maintaining real-time detection speed, validating its effectiveness.},
  archive      = {J_JRTIP},
  author       = {Chen, Hang and Tang, Xinlei and Xiang, Qian and Shi, Zhichao and Liu, Jun and Dai, Lihong and Ye, Song and Xiao, Zufa and Zhang, Lei},
  doi          = {10.1007/s11554-025-01726-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DSH-YOLO: A lightweight framework with enhanced multi-scale features fusion for water surface object detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Journal of real-time image processing: Fourth issue of volume 22. <em>JRTIP</em>, <em>22</em>(4), 1-2. (<a href='https://doi.org/10.1007/s11554-025-01727-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Kehtarnavaz, Nasser},
  doi          = {10.1007/s11554-025-01727-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-2},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Journal of real-time image processing: Fourth issue of volume 22},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Walsh–Hadamard spectra computation on GPU with tensor cores by Cooley–Tukey and constant geometry algorithms. <em>JRTIP</em>, <em>22</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01725-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Walsh–Hadamard transform (WHT) is widely applied in numerous scientific fields, including logic design, system analysis, and signal and image processing. In image processing, WHT is commonly used for image compression, pattern recognition, sequence filtering, etc. Consequently, optimizing algorithms for efficient WHT computation remains an important research area. Many WHT computation algorithms are based on the fast Fourier transform (FFT), particularly the Cooley–Tukey approach. Over time, these methods have been improved to accommodate different representations of logical functions and to take advantage of diverse hardware architectures. Recent advancements in GPU technology, specifically the introduction of tensor cores, offer new possibilities for accelerating WHT computations. Tensor cores provide specialized hardware support for matrix–matrix multiplication, significantly enhancing the efficiency of arithmetically intensive operations. This paper presents an optimized implementation of WHT on GPUs equipped with tensor cores by adapting both the Cooley–Tukey and constant geometry algorithms. Performance is evaluated against traditional implementations on single-core and multi-core CPUs, as well as standard CUDA-based GPU approaches. The experimental results demonstrate that both tensor-core-optimized WHT algorithms outperform all conventional implementations. Furthermore, the reduction in computation time achieved by the proposed algorithms grows exponentially with an increasing number of input variables. When comparing these two tensor-core-optimized algorithms, the results indicate that the constant geometry approach achieves shorter execution time.},
  archive      = {J_JRTIP},
  author       = {Marković, Ivica and Stojković, Suzana and Nikolić, Tatjana},
  doi          = {10.1007/s11554-025-01725-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Walsh–Hadamard spectra computation on GPU with tensor cores by Cooley–Tukey and constant geometry algorithms},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PL-YOLO: A lightweight method for real-time detection of pomegranates. <em>JRTIP</em>, <em>22</em>(4), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01729-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of pomegranate fruits is crucial for advancing smart harvesting and promoting orchard automation. Current fruit detection faces challenges such as small fruit size, color similarity to the background, and occlusion. To overcome these issues, we have designed the Pomegranate Lightweight You Only Look Once (PL-YOLO) model for precise pomegranate fruit detection in complex environments. The PL-YOLO model features four key innovations. First, the Image Edge Feature Extraction (IEFE) module combines Sobel and standard convolution to extract spatial information, enabling the model to learn rich, deep features. Second, the Feature Pyramid Shared Fusion (FPSF) module addresses information loss during Spatial Pyramid Pooling Fast (SPPF) pooling. Third, the Context Guide Attention Feature Pyramid Network (CGAFPN) efficiently guides the model to focus on critical feature information, enhancing recognition accuracy. Finally, the Detail Enhanced Shared Head (DESH) strengthens the detection head’s detail-capturing ability while reducing parameters. Experimental results show that PL-YOLO achieves a mean Average Precision (mAP) of 93.3% and a Frames Per Second (FPS) of 149.5. It outperforms the baseline model, with a 10.8% increase in FPS, along with respective reductions of 18.6% in parameters, 17.5% in Giga Floating Point Operations (GFLOPs), and 14.8% in model size. Overall, PL-YOLO provides robust technical support for intelligent pomegranate harvesting and effectively advances orchard automation.},
  archive      = {J_JRTIP},
  author       = {Chen, Xi and Wang, Guohui},
  doi          = {10.1007/s11554-025-01729-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {PL-YOLO: A lightweight method for real-time detection of pomegranates},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A real-time detection method for small-sized solder joint defect detection. <em>JRTIP</em>, <em>22</em>(4), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01722-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection plays a crucial role in chip production and is essential for ensuring the quality of electronic products. However, existing methods still face challenges when detecting small-sized targets. To address this issue, we propose a chip solder joint defect detection model, SSF-YOLO, based on YOLOv8. A lightweight feature extraction module, StarNet-SimAM Block, is designed to enhance feature extraction capability while reducing the complexity of the model’s feature extraction network, thereby improving detection efficiency. We also introduce a SCAM Fusion module that incorporates a separate channel attention mechanism to fuse feature information from different levels of the backbone network, thereby mitigating the loss of small defect features during feature fusion. A position regression loss function, NWD-CIOU, is employed to calculate the distance between the bounding box of small targets and the ground truth, effectively reducing the missed detection rate for small defects. Experiments conducted on two datasets, SMT chip solder joint defects dataset and PKU-Market-PCB dataset, demonstrate that SSF-YOLO improves the mAP metric by 1.5% and 2.3%, respectively, compared with YOLOv8. The detection speed on SMT chip solder joint defects dataset reaches 128 FPS, representing a 6.7% improvement over YOLOv8, indicating enhanced detection accuracy and real-time detection capability.},
  archive      = {J_JRTIP},
  author       = {Ang, Li and Hamzah, Raseeda and Quanxing, Wan},
  doi          = {10.1007/s11554-025-01722-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time detection method for small-sized solder joint defect detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient shadow removal using LSFormer with DiNA attention and dual-scale gated feedforward network. <em>JRTIP</em>, <em>22</em>(4), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01728-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow removal, a pivotal task in computer vision, significantly impacts subsequent processing tasks such as image segmentation and object detection. To achieve efficient shadow removal with minimal computational overhead and model size, this paper presents LSFormer, a lightweight network. LSFormer introduces the DiNA attention mechanism to model contextual relationships between shadow and non-shadow regions. Furthermore, it proposes a dual-scale gated enhancement feedforward network (DsGeFF) to effectively improve removal accuracy while reducing computational cost. To preserve shadow edges and texture details, a structure loss function is employed based on the Pearson correlation coefficient. Experimental results on the ISTD+ and SRD datasets show that, compared to ShadowFormer, LSFormer reduces the computational load from 63.1 GFLOPs to 20.76 GFLOPs and the number of parameters from 11.4M to 4.52M, while improving the PSNR on the ISTD+ dataset from 35.46 to 36.00. This method achieves a well-balanced trade-off between performance and efficiency, surpassing existing mainstream models and demonstrating strong practical applications.},
  archive      = {J_JRTIP},
  author       = {Hou, Shuainan and Han, Tao and Yu, Shuaishuai and Pang, Jiale and Huang, Yourui},
  doi          = {10.1007/s11554-025-01728-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient shadow removal using LSFormer with DiNA attention and dual-scale gated feedforward network},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rbpsd-yolov8: Weld surface defect detection based on improved YOLOv8. <em>JRTIP</em>, <em>22</em>(4), 1-18. (<a href='https://doi.org/10.1007/s11554-025-01730-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the limitations in weld image defect detection caused by complex backgrounds, blurred defect boundaries, and diverse defect morphologies, this paper proposes the RBPSD-YOLOv8 algorithm tailored for weld surface defect detection, which enhances detection performance through multi-faceted optimizations including replacing conventional Conv layers in the backbone and neck with RepConv modules to leverage multi-parameter branching structures for multi-scale feature extraction and reduce false detections for spatter and porosity defects while maintaining inference efficiency via reparameterization, introducing the Bottleneck Transformer (BoTNet Transformer) module to enhance the model’s capability to capture features of defects with varying shapes and sizes and address challenges from significant variability in weld-defect characteristics, adopting the Powerful-Iou version 2(PIoU v2) loss function in place of the original CIoU to accelerate model convergence and improve target localization accuracy, adding a dedicated small-target detection head optimized for micro-scale anomalies, such as porosity, cracks, and spatters, and replacing YOLOv8’s detection head with the lightweight DWDetect module to further boost detection efficiency. Experimental results demonstrate that the enhanced RBPSD-YOLOv8 model achieves improvements of 4.9% in precision (P), 3.9% in recall (R), and 5.0% in mean average precision (mAP), while reducing model parameters by 0.36M, computational complexity (GFLOPs) by 0.9 GFLOPs, and single-image processing time to 2.3ms, with comparative experiments on the NEU-DET and WELD-DEFECT.v1i.yolov8 datasets validating that the algorithm not only significantly enhances weld-defect detection performance but also exhibits strong generalization capabilities.},
  archive      = {J_JRTIP},
  author       = {Ling, Liuyi and Wei, Guo and Liu, Yuwen and Xu, Shuai and Wei, Liyu and Hong, Bolun},
  doi          = {10.1007/s11554-025-01730-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rbpsd-yolov8: Weld surface defect detection based on improved YOLOv8},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-FastestDet: A real-time and ultra-lightweight model for detection. <em>JRTIP</em>, <em>22</em>(4), 1-20. (<a href='https://doi.org/10.1007/s11554-025-01731-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strip steel is widely used as an important material in various fields, but it is easy to cause surface damage due to various factors in production. Although the existing model takes into account the accuracy and parameters, the number of parameters is still huge. From the perspective of reference quantity, this paper proposes an ultra-lightweight model called Edge-FastestDet for monitoring strip surface defects. Based on ShuffleNetV2 architecture, the model significantly improves feature fusion capability and defect detection accuracy by introducing SiLU activation function and “split-transform-splice" structure. In addition, a lightweight spatial pyramid pool module (LSPP) and dynamic enhanced detection head are proposed to further reduce the number of parameters and improve feature discrimination. The results show that the mAP50 of Edge-FastestDet on the NEU-DET and GC10 datasets is 49.0% and 33.4% respectively, the FPS is 148.2 and 163.2 respectively, and the number of model parameters is only 0.29M. The model successfully detects defects in a simulated industrial environment. This model has very low computing cost and high real-time performance, which can provide a reference for resource-limited industrial scenarios. The code can be found at https://github.com/01WineCool/Strong-FastestDet .},
  archive      = {J_JRTIP},
  author       = {Wu, Meishun and Peng, Jinmin and Yu, Xinyi and Xu, Heng and Zhang, Liulu and Jiang, Chaoqi and Dong, Wenkai and Chen, Liangshen},
  doi          = {10.1007/s11554-025-01731-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Edge-FastestDet: A real-time and ultra-lightweight model for detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware accelerator for VVC/H.266 residual syntax elements: A unified and resource-efficient architecture. <em>JRTIP</em>, <em>22</em>(4), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01733-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for video in recent years has skyrocketed due to the massive usage of streaming providers, remote work, etc. Thus, efficient measures to comply with this scenario are necessary. The Versatile Video Coding (VVC) is the state-of-the-art standard for video coding, developed to deliver high-quality video with excellent compression capabilities. Nevertheless, the high coding efficiency comes at the cost of a considerable complexity increase. Hence, the traditional solution to this burdensome issue is to design hardware accelerators for the bottleneck steps. In recent video coding formats, the so-called Residual Coding tends to generate most of the data (called Syntax Elements—SEs) that will compose the input of the entropy coding step (the last stage of the process) and, thus, the final coded bitstream. Therefore, one may find it important to speed up the residual data processing to avoid idleness in the video coding flow, which, in turn, may hinder video encoding real-time capabilities. With that in mind, this work proposes an architectural exploration of the VVC Residual SEs (RSEs) for both Transform-based and Transform-skip modes, based on statistical analysis that corroborates the RSEs’ importance for the VVC coding. Finally, a complete and efficient hardware accelerator, able to generate the RSEs in both Transform-based and Transform-skip modes, is introduced, fully compliant with the VVC standard and capable of producing enough data to avoid idleness in the VVC entropy encoder with fewer resources when compared to analogous designs from the literature.},
  archive      = {J_JRTIP},
  author       = {Cardoso, Gabriel Bitencourt and Huarachi, Heitor Mauro Chavez and Tomm, Daniel Felipe and Gomes, Jiovana Sousa and Würdig, Rodrigo Nogueira and de Moraes, Marcelo Romero and Bampi, Sergio and Ramos, Fábio Luís Livi},
  doi          = {10.1007/s11554-025-01733-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hardware accelerator for VVC/H.266 residual syntax elements: A unified and resource-efficient architecture},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FGDA-DETR: A lightweight and real-time enhanced algorithm for wire rope defect detection with improved accuracy and efficiency. <em>JRTIP</em>, <em>22</em>(4), 1-17. (<a href='https://doi.org/10.1007/s11554-025-01737-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues of inadequate accuracy, excessive model complexity, and insufficient real-time performance in wire rope defect identification, an enhanced RT-DETR-based algorithm for wire rope defect detection is suggested. The backbone network of the original model is optimised and substituted with FasterNet, enhancing feature information extraction whilst decreasing computational expenses. The feature fusion component is enhanced by substituting standard convolution with GhostConv and RepC3 with DRBC3, both of which augment the model's detection capability for small objects and enhance real-time detection performance. Finally, the coding section of the model is enhanced by fusing the ATM module into AIFI, which improves the connection between the model feature extraction network and the feature fusion network and improves the detection accuracy. Experiments on the wire rope defect dataset reveal that the FGDA-DETR method enhances average accuracy by 2.1% compared to the RT-DETR technique, reduces the number of parameters by 56.8%, and increases FPS by 12.5.The number of parameters in the FGDA-DETR algorithm is lower than that of common object detection algorithms such as the YOLO series and other algorithms. Compared to YOLOv3-tiny, YOLOv5m, YOLOv8m, YOLOv9m, YOLOv10m, and YOLOv11m by 28.9%, 65.6%, 66.7%, 57.4%, 43.8%, and 64.5%, respectively, and compared to Faster R-CNN, SSD, Deformable DETR, DETR, and DION, it reduces them by 79.4%, 64.2%, 78.6%, 79.3%, and 81.9%, respectively. In terms of real-time performance, except for YOLOv3-tiny, YOLOv5m, and YOLOv10m, the FPS of the FGDA-DETR algorithm surpasses that of other algorithms, with increases of 6.4, 12.5, and 8.4 compared to YOLOv8m, YOLOv9m, and YOLOv11m, respectively. Compared to Faster R-CNN, SSD, Deformable DETR, DETR, and DION, the FPS was higher by 52.6%, 56.1%, 59%, 37.1%, and 63.1%, respectively. Although YOLOv3-tiny, YOLOv5m, and YOLOv10m have slightly higher real-time performance than FGDA-DETR, YOLOv3-tiny, and YOLOv10m are inferior to FGDA-DETR in terms of average accuracy and parameter count, whilst YOLOv5m is inferior to FGDA-DETR in terms of parameter count. FGDA-DETR achieves a balance between detection accuracy, lightweight design, and real-time performance, providing an effective solution for steel rope defect detection.},
  archive      = {J_JRTIP},
  author       = {Li, Xin and Xu, Qianqian and Shen, Wenqing},
  doi          = {10.1007/s11554-025-01737-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FGDA-DETR: A lightweight and real-time enhanced algorithm for wire rope defect detection with improved accuracy and efficiency},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WAC-YOLO: A real-time traffic sign detection model for complex weather with feature degradation suppression and occlusion awareness. <em>JRTIP</em>, <em>22</em>(4), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01739-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic sign recognition under complex weather conditions is critical for ensuring road safety and enabling robust perception modules in all-weather intelligent driving systems. Adverse environments, such as rain, snow, and low-light nighttime conditions, often lead to occlusion, texture degradation, and increased image noise, which significantly reduce detection accuracy and hinder a vehicle’s ability to respond in real time to key information such as speed limits and directional instructions. To address these challenges, we propose WAC-YOLO, an improved traffic sign recognition model based on YOLOv8. First, we integrate a Bidirectional Weighted Feature Pyramid Network (BiFPN) to enhance multi-scale feature fusion, and replace traditional downsampling and upsampling operations with WaveletPool to improve accuracy while maintaining a lightweight design. Second, a novel C2f-WT module is introduced in the backbone to enhance textural extraction under low visibility, and a C2f-AC module is introduced to enhance the model’s perception of local details in partially occluded traffic signs. Finally, Inner-ShapeIoU is employed as the loss function, integrating auxiliary box constraints and geometry-aware weighting to enhance the accuracy of bounding box regression. Experiments on the enhanced TT100K dataset show that WAC-YOLO outperforms YOLOv8s by 3.3% in mAP50 and 3.9% in mAP50:95, while reducing parameters by 43.6% and computational cost by 25.2%, achieving real-time performance at 125 FPS.},
  archive      = {J_JRTIP},
  author       = {Dang, Yuanyuan and Wang, Hemin and Fan, Yifeng},
  doi          = {10.1007/s11554-025-01739-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {WAC-YOLO: A real-time traffic sign detection model for complex weather with feature degradation suppression and occlusion awareness},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GEF-YOLO: An enhanced model for detecting drowning risks at sea. <em>JRTIP</em>, <em>22</em>(4), 1-19. (<a href='https://doi.org/10.1007/s11554-025-01736-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned intelligent maritime Search and Rescue (SAR) devices are increasingly being used for monitoring and rescue operations in coastal and remote sea areas. Although existing deep learning methods are effective, their large number of parameters and high computational requirements make deployment on resource-constrained rescue devices difficult. To address this issue, we designed the GEF-YOLO model, an improved version based on YOLOv8, for detecting drowning risks at sea. This method integrates the lightweight GhostConv and replaces the C2f modules in the backbone with C2f-EMBC modules containing EMBConv, ensuring that the model remains lightweight while retaining sufficient feature extraction capability. A PConv layer is introduced before the detection head to improve computational efficiency. In addition, a Feature-Focused module is designed to integrate cross-scale features, and a stronger feature fusion capability pyramid structure, FGDPN, guided by this module, is proposed. The SEAM attention mechanism is integrated into the detection head to alleviate the impact of occlusion. Finally, the LAMP pruning method is applied to further compress the model. Experimental results show that GEF-YOLO improves accuracy by 2.8%, increases mAP by 0.7%, reduces computational cost by 44.8%, improves inference speed by 31.2%, and decreases parameter count and model size by 64.1% and 58.7%, respectively, fully demonstrating the potential of the model in drowning risk detection tasks.},
  archive      = {J_JRTIP},
  author       = {Yuan, JianKang and Liu, Fanghua and Jiang, Longyi and Jiang, Song and Liu, Shuo and Cai, Bingbing},
  doi          = {10.1007/s11554-025-01736-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GEF-YOLO: An enhanced model for detecting drowning risks at sea},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient object detection with dynamic task-aligned feature pyramids for resource-limited mobile devices. <em>JRTIP</em>, <em>22</em>(4), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01735-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient object detection is essential for recognition algorithms deployed on mobile devices with limited computational resources. To address this challenge, we propose a lightweight object detection algorithm incorporating a Dynamic Task-Aligned Feature Pyramid Network (DyTA-FPN). Our approach introduces three key innovations: (1) a Ghost-Shuffle Hierarchical Block (GSHB) introduces a main-branch and auxiliary-branch design within a hierarchical graph structure, which preserves both local details and global semantics while effectively reducing redundant computations; (2) A Dynamic Decomposition and Modulation Module (DyDeMod) separates and optimizes the classification and regression tasks through task decomposition mechanism, avoiding optimization conflicts and effectively improving detection accuracy; and (3) a Multiscale Hybrid Fusion (MSHF) module adopts a segmentation-and-cross-fusion strategy on feature maps, which effectively reduces information redundancy and computational overhead. Evaluated on the VOC dataset, our method surpasses YOLOv8n, achieving a 37.5% reduction in parameters and 5.20 ms of inference time, while improving precision by 2.1% and mAP50:95 by 1.4%. These results highlight the effectiveness of our lightweight algorithm for resource-constrained environments.},
  archive      = {J_JRTIP},
  author       = {Chen, Yuqing and Xie, Shiwen and Li, Qingxin and Hu, Huosheng},
  doi          = {10.1007/s11554-025-01735-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient object detection with dynamic task-aligned feature pyramids for resource-limited mobile devices},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Light-YOLO: A lightweight framework for multi-scale contraband detection in X-ray security images via channel-decoupled feature learning. <em>JRTIP</em>, <em>22</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01734-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Light-YOLO, a novel lightweight framework, for efficient contraband detection in X-ray security images. It proposes three core innovations: the Channel Separation Convolution (CSC) module for enhanced feature learning, the Lightweight Shared CSC Convolutional Detection (LSCCD) head for eliminating cross-scale redundancy, and an Enhanced StarNet Backbone for hardware-aligned optimization. Light-YOLO is designed with multiple scalable variants (n, s, m, l), offering flexible deployment options across diverse hardware platforms. Extensive evaluations on the challenging OPIXray dataset demonstrate Light-YOLO’s superior efficiency–accuracy trade-off. Our models achieve substantial reductions in parameter count and GFLOPs across all scales compared to YOLOv8, YOLOv9, YOLOv10, YOLOv11, and YOLOv12. Crucially, Light-YOLO maintains highly competitive detection accuracy on OPIXray. For instance, Light-YOLON achieves the lowest GFLOPs (3.8) and parameters (1.6M) among all tested models with competitive accuracy, while Light-YOLOS delivers higher mAP50-95 (0.413) at significantly lower computational cost (15 GFLOPs) than YOLOv10s and YOLOv11s. These results affirm Light-YOLO’s practical viability for real-world, resource-constrained security inspection systems, highlighting the importance of synergistic backbone–neck–head co-design.},
  archive      = {J_JRTIP},
  author       = {Wang, Ran and Zhou, Yang and Hu, Guanghuan and Xu, Xianghua},
  doi          = {10.1007/s11554-025-01734-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Light-YOLO: A lightweight framework for multi-scale contraband detection in X-ray security images via channel-decoupled feature learning},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient real-time license plate recognition using deep learning on edge devices. <em>JRTIP</em>, <em>22</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01738-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time automatic license plate recognition (ALPR) is essential for smart-traffic, tolling, parking, and policing, yet roadside cameras must run on $$<\!10$$ W hardware with limited memory and patchy connectivity, ruling out cloud off-loading. These constraints demand compact, fast models resilient to oblique views, motion blur, glare, and diverse plate styles. We introduce Light-Edge, a single-pass deep network that jointly localizes plates and recognizes characters. It shares a ResNet-18 + FPN backbone, removes 28 % of convolutions with a $$1\times 1$$ channel-fusion block, and replaces anchors with an anchor-free head followed by a CTC decoder. After mixed-precision compilation in Torch-TensorRT, the 38 MB model sustains 14 FPS on a Jetson Nano—73 % faster than the anchor-free AF-Net (8.1 FPS) and 49 % faster than YOLOv8-MobileLPR (9.5 FPS)—while keeping competitive accuracy (90.2 % mAP) and halving AF-Net’s power consumption (4.8 W vs 8.8 W). Light-Edge therefore satisfies the stringent speed–accuracy envelope required for large-scale, privacy-preserving ALPR on resource-constrained edge devices.},
  archive      = {J_JRTIP},
  author       = {Sonnara, Fedi and Chihaoui, Hamadi and Filali, Fethi},
  doi          = {10.1007/s11554-025-01738-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient real-time license plate recognition using deep learning on edge devices},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MkT-yolo_SKD_P: A lightweight real-time object detector for foreign object retrieval in pressurized water reactors. <em>JRTIP</em>, <em>22</em>(4), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01732-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the challenging conditions of industrial underwater imaging, the variability of target geometries, and stringent computational constraints, real-time foreign object detection in pressurized water reactors poses a significant challenge for nuclear power plant monitoring systems. This paper proposes MkT-yolo_SKD_P, an optimized lightweight detection model, to address these limitations. The model incorporates the MDIC-KAN (Mix Dynamic Inception Conv—Group Rational-Kolmogorov–Arnold Networks) module, which achieves dynamic multi-scale feature fusion through adaptive weighted deformable convolution and combines GR-KAN (Group Rational-Kolmogorov–Arnold Networks) for efficient nonlinear modeling, thereby enhancing feature representation while reducing computational load. Furthermore, the proposed model introduces TDAH (Task Dynamic Alignment Head), a lightweight detection head with task interaction capabilities. It unifies localization and classification tasks through task decomposition and dynamic dependency learning, adaptive spatial alignment using DCNv2 deformable convolution, and the CRCS (Classification Refinement with Context Selection) module. Additional optimization is achieved via BCSKD (Bridging Cross-task Protocol Inconsistency Self-Knowledge Distillation) and LAMP (Layer-Adaptive Magnitude-based Pruning). Experimental results demonstrate that the proposed model outperforms YOLOv11s, achieving higher detection accuracy and significantly improved processing speed while reducing parameter count by 72% and GFlops by 52.6%. When deployed on a GeForce RTX 4070 Laptop, MkT-yolo_SKD_P achieves an FPS ( $$batchsize = 8$$ ) of 146.32, making it suitable for underwater robotic foreign object retrieval tasks under low-power GPU conditions.},
  archive      = {J_JRTIP},
  author       = {Duan, Xingguang and Zhang, Zhongyue and Wu, Tong and Li, Changsheng and Huan, JiaLe and Jing, Haibo and Wang, Jiapeng and Zhang, Chuan and Cui, Tengfei},
  doi          = {10.1007/s11554-025-01732-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {MkT-yolo_SKD_P: A lightweight real-time object detector for foreign object retrieval in pressurized water reactors},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMT-NET: A lightweight multi-modal fusion network for UAV target detection in adverse environments. <em>JRTIP</em>, <em>22</em>(4), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01741-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenge of insufficient target detection accuracy for UAVs operating in adverse conditions, such as low illumination, dense fog, and extreme weather, this paper proposes a lightweight multi-modal fusion detection network named MMT-NET, designed to enhance UAV perception capabilities in challenging environments. Built upon the RT-DETR framework, the proposed method employs a dual-branch MobileNetV4 backbone to independently extract features from infrared and visible images. Moreover, a lightweight multi-modal feature interaction module is designed to strengthen the interaction between different modalities, and a lightweight cross-modal attention fusion module is designed to efficiently fuse cross-modal features via a spatial attention mechanism with minimal computational overhead. Extensive experiments on the public multi-modal dataset M3FD demonstrate that MMT-NET achieves 89.9% mAP@50 and 60.3% mAP@50:95, validating its effectiveness in multi-modal detection tasks while maintaining a lightweight architecture. Furthermore, qualitative evaluations under diverse real-world and simulated scenarios—including nighttime, fog, snow, and occlusion—confirm the robustness and generalization capability of the proposed method in complex environments. The source code of this work will be publicly available at: https://github.com/UAVSwarm/MMT-NET.},
  archive      = {J_JRTIP},
  author       = {Wang, Chuanyun and Zhou, Mingqi and Sun, Dongdong and Gao, Qian and Li, Zhaokui and Wang, Tian},
  doi          = {10.1007/s11554-025-01741-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {MMT-NET: A lightweight multi-modal fusion network for UAV target detection in adverse environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LMC-YOLO: A lightweight underwater benthic organism detection network with multi-scale feature extraction. <em>JRTIP</em>, <em>22</em>(4), 1-17. (<a href='https://doi.org/10.1007/s11554-025-01743-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of science and technology, underwater robots have become the standard means for harvesting benthic marine organisms. Accurate identification of these organisms is a critical prerequisite for the proper functioning of such robots. However, most existing object detection models are too computationally intensive to be fully deployed on resource-constrained underwater platforms. To address this issue, we propose a lightweight object detection network tailored for benthic marine organisms, named LMC-YOLO, which incorporates multi-scale feature extraction. To begin with, we introduce a lightweight and efficient feature extraction module designed to enhance the model’s ability to capture salient features while reducing its overall complexity. Additionally, we propose a multi-branch semantic enhancement module that leverages branches with diverse receptive fields to capture the multi-scale characteristics of benthic marine organisms. Furthermore, we incorporate a channel-spatial attention mechanism to improve the model’s capacity to distinguish semantic information and increase sensitivity to critical features. Finally, we apply pruning techniques to reduce both the computational load and the number of parameters in the model, enabling more efficient deployment on embedded systems. Experimental results on the URPC data set demonstrate that, compared to the baseline model YOLOv7s, the proposed LMC-YOLO achieves a 3.2 $$\%$$ improvement in mAP50, a 1.9 $$\%$$ increase in accuracy, a 74.8 $$\%$$ reduction in parameter count, and a 67.1 $$\%$$ increase in FPS. Moreover, evaluations conducted on the RUOD marine biology data set show that LMC-YOLO outperforms other existing algorithms, providing strong evidence of its generalization capability.},
  archive      = {J_JRTIP},
  author       = {Zhang, Kangye and Li, Zhanying and Gao, Yu and Liu, Longhui and Liu, Junjie},
  doi          = {10.1007/s11554-025-01743-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LMC-YOLO: A lightweight underwater benthic organism detection network with multi-scale feature extraction},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMSPENet: An efficient multi-scale perceptual enhancement network for aluminum detection. <em>JRTIP</em>, <em>22</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01746-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection of aluminum quality is crucial to guarantee production quality. In this paper, an efficient multi-scale perceptual enhancement network (EMSPENet) for aluminum defect detection is proposed to achieve a balance between performance and efficiency. First, the InResC2f module is designed to enhance the feature extraction capability, and the VoVGSCSP module is introduced to alleviate the feature dispersion. Second, the Contextual Feature Fusion Module (CFFM) is designed to compensate for the deep feature loss and reduce the background interference to enhance the network sensory field. Finally, in light of the fact that the feature mapping in a PAN encompasses data pertaining to two distinct paths, yet the network exhibits varying degrees of sensitivity to the bottom and top feature mappings, a detail-aware aggregation network (Dp-PAN) is put forth as a means of enhancing the feature information and gradient flow information associated with the various paths, thereby facilitating a more optimal fusion of the local and positional data. The experimental results show that the mAP50 and mAP50–90 of the method were 93.8% and 77.2%, respectively, which were 1.7% and 6.1% higher and FLOPs were 17.7% lower than those of YOLOv8s. The method significantly outperformed the SOTA model in all evaluation metrics.},
  archive      = {J_JRTIP},
  author       = {Yang, Li and Jiang, Zhaofei and Wang, Tingting and Wang, Lishen},
  doi          = {10.1007/s11554-025-01746-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EMSPENet: An efficient multi-scale perceptual enhancement network for aluminum detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dal-yolo: A multi-target detection model for UAV-based road maintenance integrating feature pyramid and attention mechanisms. <em>JRTIP</em>, <em>22</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01685-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routine road maintenance is essential to ensure traffic safety. This study proposes DAL-YOLO, a novel multi-object detection model specifically designed for UAV-based road maintenance. First, the feature extraction module is enhanced markedly in small object detection accuracy by integrating Deformable Attention into the C3k2 module. Second, an Adaptive Scaled Pyramid Network is introduced, dynamically fusing high-, mid-, and low-level features with adaptive fusion weights, excelling in dense targets and complex scenarios. Furthermore, a lightweight detection head, the Lightweight Shared Convolution BatchNorm Head, combines shared convolution layers with independent BatchNorm structures, improving precision and recall for dense and occluded object detection substantially. Experimental results on the road maintenance dataset show that DAL-YOLO achieves an AP of 39.8% and an AP50 of 58.8%, improving the accuracy of small, medium, and large objects notably. This model offers an efficient and accurate solution for UAV-based routine road maintenance tasks.},
  archive      = {J_JRTIP},
  author       = {Lan, Xuerui and Liu, Lijun and Wang, Xuyang},
  doi          = {10.1007/s11554-025-01685-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dal-yolo: A multi-target detection model for UAV-based road maintenance integrating feature pyramid and attention mechanisms},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crack detection with minimal labels: A mixed supervision approach with multiscale transformers. <em>JRTIP</em>, <em>22</em>(3), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01678-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack detection plays a vital role in ensuring the structural integrity and safety of infrastructures, such as roads and buildings. However, traditional manual inspection methods are inefficient, and current machine learning approaches require large amounts of labeled data, which is costly and time-consuming to obtain. In this paper, we propose a Mixed Supervised Learning (MSL) algorithm that reduces the need for extensive labeled data sets by combining several learning techniques, and an Efficient Multiscale Transformer version 2 (EMTv2) model, that incorporates multiscale feature extraction and novel attention mechanisms to enhance crack classification and segmentation performance. The MSL algorithm achieves competitive segmentation results using only $$25\%$$ of the image-level labeled data required by traditional weakly supervised learning methods, outperforming existing models on four data sets in multiple metrics. Our approach demonstrates significant improvements in both efficiency, and segmentation quality, offering a more practical solution for real-world crack detection applications.},
  archive      = {J_JRTIP},
  author       = {Al-maqtari, Omar and Peng, Bo and Al-Huda, Zaid and Rahman, Ali},
  doi          = {10.1007/s11554-025-01678-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Crack detection with minimal labels: A mixed supervision approach with multiscale transformers},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Yolo-based power-efficient object detection on edge devices for USVs. <em>JRTIP</em>, <em>22</em>(3), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01682-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in Artificial Intelligence, the Internet of Things, and Computer Vision have introduced challenges in minimizing resource usage-economically, energetically, and environmentally. This work presents a vision system for unmanned surface vehicles (USVs) focused on object detection and autonomous navigation. The system leverages hardware and software acceleration to enhance model performance while also evaluating energy efficiency. In this paper, we analyze the Ultralytics models across various platforms, including MYRIAD VPU, Intel CPUs/GPUs, and NVIDIA Jetson AGX Orin and Orin Nano. The results show that the Orin Nano is especially suitable for real-time detection, consuming less than 2 watts. To increase efficiency, optimization techniques, such as quantization and pruning, are performed. We also compare our models with related studies and assess YOLOv6 to YOLO11 in terms of inference time and FPS. YOLOv8-based models consistently deliver the best results, confirming their suitability for USV applications.},
  archive      = {J_JRTIP},
  author       = {Mela, Jose Luis and Sánchez, Carlos García},
  doi          = {10.1007/s11554-025-01682-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Yolo-based power-efficient object detection on edge devices for USVs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LFE-YOLO: An algorithm suitable for recognizing small target in blurred high-voltage line equipment. <em>JRTIP</em>, <em>22</em>(3), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01686-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development and intelligent transformation of power systems, the safety monitoring and maintenance of high-voltage line equipment has become a key aspect in ensuring the stable operation of the power grid. Currently, the identification of high-voltage line equipment relies on image acquisition from drones; however, image blurriness poses challenges for small target recognition, leading to false detections, missed detection, inaccurate localization, and slow detection speeds. To address this issue, this paper proposes a lightweight feature extraction model, LFE-YOLO, based on YOLOv8-N. The model uses C2BCE modules to replace some C2f modules in the backbone network to enhance feature capture capabilities and introduces ADown modules to process feature maps, reducing the number of parameters to improve operational efficiency and avoid false detections or missed detections. In the neck network, ADown and MSC modules further enhance the model’s feature capture ability, while the introduction of the narrow-neck structure VOVGLFC module reduces computational burden and parameter count, thereby improving detection accuracy for small targets while meeting lightweight requirements. In the head network, to address the issue of excessive parameters in the YOLOv8-N detection head that cannot handle multi-scale targets, the LPIH module is designed to ensure model accuracy while reducing computational load and parameter count. Experiments on public and private datasets have shown that compared to the baseline model, the mAP50 of the LFE-YOLO model has increased by 4.1% and 0.9%, respectively, while the number of parameters has decreased by 43.33% and 28.57%, fully verifying the feasibility and effectiveness of the model. Importantly, this article tested the inference speed using the JOINT framework on the MAIX-III AXera Pi, reaching 26 frames/s, demonstrating efficient detection output.},
  archive      = {J_JRTIP},
  author       = {Zhu, Changsheng and Zhao, Yanan and Li, Tianyu and Li, Jingjie and Wang, Yongxin and Wang, Suchao},
  doi          = {10.1007/s11554-025-01686-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LFE-YOLO: An algorithm suitable for recognizing small target in blurred high-voltage line equipment},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time fall detection algorithm based on FFD-AlphaPose and CTR–GCN. <em>JRTIP</em>, <em>22</em>(3), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01687-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing prevalence of an aging population, falls present a substantial risk to the health of older adults, making fall detection and prevention a primary societal concern. In response to the challenges of inadequate real-time performance and low accuracy in existing methods, this paper proposes a lightweight AlphaPose based on FFD-YOLO. Additionally, it incorporates the channel topology refinement graph convolutional network (CTR-GCN) to improve fall detection capabilities. To address the bottlenecks in efficiency and accuracy, this paper first presents an innovative C2fPDR module aimed at enhancing the processing capabilities of long sequence data and expanding the feature receptive field. This approach maintains efficiency while reducing the parameter count, thereby ensuring the stability and accuracy of detection and fully demonstrating the unique advantages of a lightweight design. Furthermore, the neck component has been innovatively restructured by employing a gather-and-distribute (GD) mechanism to optimize the fusion of multi-layer features. Additionally, the integration of MobileNetV4 enhances the backbone network, significantly improving detection speed. The experimental results indicate that the F-FD-YOLO model proposed in this paper reduces the parameter count by 43.0% compared to the original network, increases the frames per second (FPS) by 11.9, achieves a mean average precision (mAP) of 94.3%, and outperforms other classical object detection algorithms that have been adapted for AlphaPose. After embedding AlphaPose, the pose estimation average precision (AP) reaches 74.3%, demonstrating improvements of 0.7, 1.0, and 0.8% compared to the most recent literature (Liang et al., J Supercomput 81:1–20, 2025; Xu et al., Neurocomputing 619:129154, 2025; Miao et al., Adv Neural Inf Process Syst 37:44791–44813, 2025), respectively. The frames per second (FPS) on the GPU reaches 45.8, which is 32.4 FPS faster than OpenPose. When combined with CTR-GCN for action recognition, the precision reaches 98.62%, representing improvements of 8.57, 2.20, and 1.61% over the most recent literature (Cheng et al., Multimed Syst 31:67, 2025; Raza et al., Eng Appl Artif Intell 143:109809, 2025; Yu et al., Pervasive Mob Comput 107:102016, 2025). These experiments validate the substantial advantages of the proposed algorithm for fall detection.},
  archive      = {J_JRTIP},
  author       = {Yang, Xuecun and Wang, Yixiang and Dong, Zhonghua and Li, Jiayu and Zhang, Qingyun and Qiang, Shushan},
  doi          = {10.1007/s11554-025-01687-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time fall detection algorithm based on FFD-AlphaPose and CTR–GCN},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient single-stage ISP for smartphones using global context residual dense and residual channel attention modules. <em>JRTIP</em>, <em>22</em>(3), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01690-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of smartphone cameras to capture photographs is immensely popular in the world. Smartphone image signal processors are used to produce high-quality images. The field of Image Signal Processing (ISP) in smartphone cameras involves the application of various techniques and algorithms that process the raw image data acquired by the smartphone camera sensor into a high-quality Red, Green, and Blue (RGB) image. The visual difference between images captured by smartphone cameras and Digital Single Lens Reflex (DSLR) cameras can be attributed to the constrained sizes of smartphone camera sensors and lenses. To address the existing disparity in visual differences of these devices, there is a need to redesign the smartphone ISP with the aim of reconstructing the good quality of the captured images in real time. This work proposes a single-stage end-to-end deep-learning model that can replace most complex ISP pipelines of smartphone cameras. The training of the proposed model is independent of the sensor and optics employed in a specific device. The proposed single-stage ISP pipeline for smartphone cameras uses the Global Context Residual Dense (GCRD) module, the Multiple Convolution Block (MCB) module, and the Residual Channel Attention (RCA) module. The GCRD module is used to learn the residual information which helps in color mapping of the raw image to the corresponding RGB image. At the same time, the MCB module with multiple convolution blocks consisting of layers of different kernel sizes focuses on the fine-grained details of the image. Further, the RCA module used in the proposed work consists of a very high deep trainable network that adaptively learns more beneficial channel-wise features simultaneously. By combining these modules, the pipeline achieves a synergistic effect that helps in balancing global context, local refinement, and feature prioritization, enabling superior performance in complex ISP operations. This work evaluates the proposed model on two datasets, i.e., the Zurich RAW-RGB (ZRR) dataset consisting of the Huawei P20 smartphone captured raw images and the Well-Aligned dataset, which consists of two smartphones, iPhone 6 S (high-end) and Mi-Z6 (low-end) camera captured images. The proposed model is a lightweight model in terms of the number of hyperparameters, which makes it suitable for real-time processing in smartphones. The experimental results show that the proposed model outperformed the benchmark models in terms of output image quality and computational time. This makes the proposed model highly suitable for real-time smartphone ISP. Experimental evaluation ensures the proposed model’s robustness, scalability, adaptability, and resource efficiency in various scenarios. Additionally, this work also evaluates the qualitative and quantitative results along with the ablation study of the modules.},
  archive      = {J_JRTIP},
  author       = {Bansal, Roli and Pal, Anjali and Sehgal, Priti},
  doi          = {10.1007/s11554-025-01690-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient single-stage ISP for smartphones using global context residual dense and residual channel attention modules},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multispectral imaging and enhanced YOLOv10n for efficient coal gangue detection in complex mining environments. <em>JRTIP</em>, <em>22</em>(3), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01691-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues in existing coal gangue recognition methods, such as large detection models that are difficult to deploy on embedded devices, long detection times, and low accuracy, this paper proposes a method that collects multispectral images of coal gangue based on spectral technology and integrates an improved lightweight YOLOv10n model for target recognition and detection. This approach effectively reduces the model size, decreases detection time, and improves the accuracy and performance of coal gangue recognition. To reduce the model parameters, this study adopts the lightweight neural network architecture MobileNetV4 to replace the original backbone and further optimizes it for efficiency. Additionally, GhostConv layers are used in the head of the model to replace traditional Conv layers, reducing the computational burden. However, the model’s accuracy in multi-target recognition was still not ideal, so a Spatial Group-wise Enhancement (SGE) module was added after the C2fCIB (Contextual Feature Fusion and Inference Block) module in the head, significantly improving detection accuracy. The coal gangue dataset in this study was collected using a multispectral data acquisition system, resulting in 4500 sets of multispectral data. Through Principal Component Analysis (PCA), spectral images from the 4th, 6th, 22th, and 25th bands were selected from a total of 25 bands to generate pseudo-RGB images, forming the dataset for this study. A total of 974 initial sample images of coal gangue were collected. After applying Gaussian filtering to reduce noise and implementing data augmentation on certain images, a dataset of 4000 coal gangue images was generated. These images were then split into training and testing sets with a ratio of 4:1. The final improved model, M4-GHOST-SGE-YOLOv10n, further enhanced precision and recall. Compared to the original model, the M4-GHOST-SGE-YOLOv10n model reduced parameters and GFLOPs by 22.45% and 26.8%, respectively, while increasing detection speed (FPS) by 52.7%. In conclusion, the optimized model preserves strong detection accuracy while achieving superior portability, making it highly applicable for coal gangue detection in challenging underground environments.},
  archive      = {J_JRTIP},
  author       = {Yan, Pengcheng and Sun, Hao and Zhao, Yuanjun and Wang, Pinghong and Wu, Zhiqi},
  doi          = {10.1007/s11554-025-01691-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multispectral imaging and enhanced YOLOv10n for efficient coal gangue detection in complex mining environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concurrent execution of lossy compression and anomaly detection of hyperspectral images on FPGA devices. <em>JRTIP</em>, <em>22</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01692-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral sensors capture a wide range of spectral data, making them crucial for Earth observation applications, but this fact poses significant challenges for embedded systems with limited resources. Nevertheless, most studies only perform one application at the same time, so multi-applications in the same device are not considered since high-performance and low hardware resources are limited. In this sense, this paper presents a hardware-friendly algorithm for concurrently execution of anomaly detection and lossy compression for hyperspectral imaging. The proposed algorithm reuses a hardware platform to perform both tasks in parallel, offering a validated hardware architecture designed for deployment on a cost-optimized FPGA device. The experimental results show that our hardware component can process hyperspectral images with a resolution of $$825\times 1024$$ pixels and 160 bands in 0.53 s (486 MB/s), with a power consumption of 1.08 watts (399 MB/W).},
  archive      = {J_JRTIP},
  author       = {Caba, Julián and Barba, Jesús and Díaz, María and Mira, José Luis and López, Sebastián and López, Juan Carlos},
  doi          = {10.1007/s11554-025-01692-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Concurrent execution of lossy compression and anomaly detection of hyperspectral images on FPGA devices},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALF-YOLO: A modified YOLOv8n algorithm for precise emotion detection via facial expressions. <em>JRTIP</em>, <em>22</em>(3), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01688-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting facial expressions plays a vital role in medical rehabilitation and human–computer interaction. This work proposes the attention-enhanced lightweight fusion (ALF-YOLO) framework, a modified version of YOLOv8n, to address challenges such as limited precision in emotion categorizing and inadequate real-time performance in traditional methods. The investigation introduces the adaptive multi-branch convolution (AMConv) module as an alternate for particular typical convolutional modules in the original model, thus improving its ability to recognize multi-scale facial information and complicated details. The large separable kernel attention (LSK-Attention) module is integrated into the backbone to emphasize critical spatial features, enhancing the model’s sensitivity to facial information. The interlayer feature injection (IFI) structure is also demonstrated, dramatically decreasing information loss and optimizing the incorporation of multi-scale face features, therefore, improving the model’s emotion recognition capability. Findings from experiments on the AffectNet dataset demonstrate that the proposed ALF-YOLO model achieves an optimal balance between real-time performance and precision compared with the original YOLOv8n. The model improves the mean average precision at 50% (mAP50) by 3.7%, achieves a detection speed of 282 FPS, and has a parameter size of roughly 2.81 M. The precision and generalization ability of the ALF-YOLO model has been demonstrated on the challenging BCCD and Mar20 datasets. The ALF-YOLO framework provides a successful approach for precision emotion detection by face expression.},
  archive      = {J_JRTIP},
  author       = {Ma, Yijia and Lu, Ruizhi and Ren, Wanchun and Huang, Yujie and Li, Wen and Wang, Yaozu},
  doi          = {10.1007/s11554-025-01688-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ALF-YOLO: A modified YOLOv8n algorithm for precise emotion detection via facial expressions},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficiency and complexity analysis of video-based and geometry-based point cloud encoders. <em>JRTIP</em>, <em>22</em>(3), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01689-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point clouds are increasingly used in applications, such as 3D remapping, cultural heritage preservation, and virtual/augmented reality. Given their large data volume, efficient compression is crucial. The MPEG group has introduced two standards: Geometry-based Point Cloud Compression (G-PCC), for static and dynamic-acquired point clouds, and Video-based Point Cloud Compression (V-PCC), specifically for dynamic point clouds. Although both approaches achieve effective data reduction, their high computational complexity limits real-time use, especially on resource-constrained devices. This paper analyzes the computational cost and coding efficiency of G-PCC and V-PCC, identifying the most time-consuming steps. In G-PCC, the Octree-RAHT configuration offers the best trade-off between efficiency and encoding time, with recoloring alone accounting for up to 70%. In V-PCC, video encoding dominates, consuming about 92% of the total time. These findings lay the groundwork for future optimizations to reduce the complexity of more efficient implementations.},
  archive      = {J_JRTIP},
  author       = {Santos, Cristiano and Rehbein, Gustavo and Costa, Eduardo and Corrêa, Guilherme and Porto, Marcelo},
  doi          = {10.1007/s11554-025-01689-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficiency and complexity analysis of video-based and geometry-based point cloud encoders},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight small object detection network for aerial images based on cross-attention and information injection. <em>JRTIP</em>, <em>22</em>(3), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01699-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerial image object detection has increasingly become a popular research direction within the field of small object detection. Unlike natural scene images, objects in aerial images occupy a very small pixel area, making them easily susceptible to background noise interference. In addition, most aerial images are taken from an overhead perspective, providing limited and highly similar information from the tops of objects, which poses significant challenges for object localization and classification. To address these issues, this paper proposes a more efficient aerial image object detection network, which is mainly composed of a Differential Information Injection Network (DIIN), a Multi-scale Dilated Object Perception Module (MDOP), and a Cross-attention Feature Fusion Module (CFFM). First, the Difference Information Injection Network is designed to capture asymmetric information in features. Through semantic information interaction and spatial information perception across spatial and channel dimensions, it can effectively achieve cross-feature difference information injection. Next, the Multi-scale Dilated Object Perception module is employed, which uses parallel dilated convolutions to integrate feature context from different receptive fields, thereby enhancing the model’s ability to perceive small objects. Finally, to efficiently fuse features at different levels, a cross-attention-based feature fusion module is proposed, which adaptively merges the detailed and semantic information between features, strengthening their representation capability and ultimately improving detection performance. To validate the effectiveness of our method, we conducted extensive experimental evaluations on the challenging VisDrone2019 dataset. The experimental results show that compared with other mainstream models, our proposed method achieves better detection performance in aerial image object detection tasks. At the same time, a balance between computational efficiency and detection accuracy is achieved, making it more suitable for real-time small object detection tasks.},
  archive      = {J_JRTIP},
  author       = {Wang, Dong and Liu, Junnan and Jin, Shengyi},
  doi          = {10.1007/s11554-025-01699-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight small object detection network for aerial images based on cross-attention and information injection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time semantic segmentation network via bidirectional feature alignment. <em>JRTIP</em>, <em>22</em>(3), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01693-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a real-time semantic segmentation network based on bidirectional feature alignment, termed BFASNet. Existing methods, such as BiSeNet, learn semantic and detailed information through context and spatial paths, respectively. However, they often fail to adequately address the feature alignment issue between these paths, leading to feature misalignment during progressive downsampling and context information fusion, which adversely affects segmentation performance. To address this, we introduce a novel network that achieves feature alignment between paths by learning flow fields. Our approach incorporates two key modules: the Gated Bidirectional Feature Alignment Module (GBFAM) and the Gated Context Feature Alignment Module (GCFAM). GBFAM employs a learnable interpolation strategy and a gating mechanism to bidirectionally align high- and low-resolution features, reducing noise and semantic discrepancies during fusion. GCFAM adaptively selects personalized contextual information for each pixel, enhancing the alignment of contextual features. Extensive experiments on the Cityscapes and ADE20K datasets demonstrate that BFASNet achieves mean Intersection over Union (mIoU) scores of 80.47% and 45.47%, respectively, validating its effectiveness. The network also achieves a frame rate of 33 FPS on the Cityscapes validation set, meeting the demands of real-time segmentation.},
  archive      = {J_JRTIP},
  author       = {Xiang, Yunjie and Du, Congliu and Zhang, Liang and Mei, Yan and Liu, Xiaoming and Zong, Yang and Du, Yutong},
  doi          = {10.1007/s11554-025-01693-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time semantic segmentation network via bidirectional feature alignment},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time autonomous underwater and aerial exploration with limited FOV sensors. <em>JRTIP</em>, <em>22</em>(3), 1-10. (<a href='https://doi.org/10.1007/s11554-025-01694-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous underwater exploration using Autonomous Mobile Robots (AMRs) is increasingly important as marine resource exploitation intensifies. However, limited computing power and the inherent complexity of unfamiliar environments often causes existing exploration frameworks to produce hazardous maneuvers or mission failures resulting from untimely planning. In this paper, we address bottlenecks in viewpoint generation and multi-goal path planning to improve the real-time performance of AMRs during exploration. We propose an exploration framework that integrates (1) an FOV-based viewpoint generation method to accelerate frontier clustering and viewpoint sampling, (2) a sparse graph construction technique guided by frontier clusters to reduce unnecessary node generation and minimize the search space, and (3) a two-stage multi-goal path planning algorithm that combines heuristic strategies for local rapid decision-making with Traveling Salesman Problem-based methods to balance real-time responsiveness with overall exploration performance. Experimental evaluations indicate that our framework reduces computation time by over 90% compared with state-of-the-art techniques while maintaining exploration performance comparable. The proposed framework significantly enhances the real-time capabilities of AMRs, thereby facilitating efficient and safe underwater exploration in complex environments. This work provides a robust and efficient solution for rapid underwater coverage and mapping tasks, while also being applicable to challenging aerial environments.},
  archive      = {J_JRTIP},
  author       = {Huang, Haiyu and Zhang, Shu and Fan, Hao and Wang, Ting and Jing, Yanguo and Dong, Junyu},
  doi          = {10.1007/s11554-025-01694-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time autonomous underwater and aerial exploration with limited FOV sensors},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rejuvenating efficient convolutional neural networks. <em>JRTIP</em>, <em>22</em>(3), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01696-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, efficient convolutional neural network (CNN) models have received limited attention due to their relatively inferior performance compared to other popular efficient models. In this paper, we propose a novel efficient CNN-based model, dubbed MoConv, which retains many advantages of CNNs. Our approach begins with an analysis of the Transformer encoder structure, leading to the Comprehensive Convolution Attention (CCA) mechanism. Using CCA with the Inverted Residual Block (IRB), we designed the fundamental MoConv block analogous to the Transformer encoder and constructed the MoConv model for various computer vision tasks. The effectiveness of MoConv is demonstrated through benchmarks like ImageNet, MS COCO, and ADE20K. For instance, MoConv-S/T/N models, trained with one or two RTX4090 GPUs on ImageNet, achieve top-1 accuracy rates of 78.6%, 75.7%, and 72.2%, respectively, on ImageNet-1k, with computational overheads of only 5.6M/1.4GFLOPs, 2.3M/0.5GFLOPs, and 1.2M/0.3GFLOPs. This clearly surpasses the performance of SwiftFormer, ConvMLP, EMO, MobileViG, and EfficientFormer, which are among the most outstanding performers in the same class. Consequently, our models stand out as excellent choices among mainstream lightweight models.},
  archive      = {J_JRTIP},
  author       = {Xu, Wei and Wan, Yi and Wang, Kaiyuan},
  doi          = {10.1007/s11554-025-01696-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rejuvenating efficient convolutional neural networks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight and rapid method for detecting silica ores based on yolov5. <em>JRTIP</em>, <em>22</em>(3), 1-17. (<a href='https://doi.org/10.1007/s11554-025-01695-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the purpose of tackling the issue of sluggish identification speed and prolonged processing time resulting from the extensive network architecture of deep learning algorithms, this paper proposes Mix SwiftYOLOV5, a lightweight silicon ore recognition network based on YOLOv5, to decrease processing parameters and boost detection speed. Specifically, we introduced the GhostConv module, and using GhostConv to replace the original convolution can significantly reduce the amount of computation and the model’s size. In addition, we also introduced an innovative Mix Binary Attention (MBA) module and Spatial Dilation Pyramid (SDP) to improve network performance and feature representation capabilities. To examine the impact of the introduced method, we evaluated Mix-SwiftYOLOv5 using a self-built silicon ore dataset (a total of 4525 silicon ore images) and a PASCAL VOC2012 dataset. The experimental outcomes reveal that the proposed algorithm, when evaluated on the self-collected ore dataset, outperforms YOLOv5s by achieving a 1.61% higher mAP (0.5), a 47.5% reduction in FLOPs, a 44.2% reduction in the model parameters, and a 65.7% improvement in FPS. Through example analysis and comparison, the effectiveness and practicality of the algorithm in silicon ore detection tasks are proved.},
  archive      = {J_JRTIP},
  author       = {Liu, Jun and Jiang, Tao and Guo, Liang and Liu, Runjun and Chen, Zhihua},
  doi          = {10.1007/s11554-025-01695-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight and rapid method for detecting silica ores based on yolov5},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight network and adaptive spatial fusion-based detection algorithm for mine aerial passenger devices. <em>JRTIP</em>, <em>22</em>(3), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01698-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the safety detection issue of workers using aerial cable transport devices in mines, this paper proposes a lightweight real-time detection algorithm named MOOD. Considering the challenge of balancing detection speed and accuracy in dynamic working conditions, this study redesigns the network model based on FasterNet, GSCSP, and ELAN-P, incorporating parts of the YOLOv7 Head structure. This redesign significantly reduces the number of parameters and computational cost, thereby improving detection speed. Furthermore, to compensate for the loss of feature extraction information caused by network lightweighting, the CA attention mechanism is introduced into the FasterNet module to enhance feature extraction capabilities and mitigate underground lighting interference. In addition, an adaptively designed spatial ASFF structure is reconstructed to further strengthen multi-level feature interactions, improving detection accuracy in complex backgrounds and multi-scale pedestrian. Experiments show that MOOD achieves a mean average precision (mAP) of 96.6% in mining scenarios, with an inference speed of 121 FPS and a model size of only 23.3 MB. Compared to existing algorithms, MOOD demonstrates significant improvements in both detection accuracy and real-time performance, providing an effective technical solution for safety monitoring.},
  archive      = {J_JRTIP},
  author       = {Gao, Ruxin and Jin, Haiquan and Li, Xinyu and Wang, Tengfei and Liu, Qunpo},
  doi          = {10.1007/s11554-025-01698-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight network and adaptive spatial fusion-based detection algorithm for mine aerial passenger devices},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved RAD-YOLOv8s deep learning algorithm for personnel detection in deep mining workings of mines. <em>JRTIP</em>, <em>22</em>(3), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01700-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time detection of personnel is a significant component of the construction of intelligent mines, especially for the personnel safety pre-warning in underground excavation workface. The underground excavation environment faces many critical challenges, including noise interference, uneven illumination, and the occlusion of mechanical equipment. Due to traditional detection algorithms' low detection accuracy and significant resource consumption, this study proposes an accurate and lightweight RAD-YOLOv8s (You Only Look Once v8) personnel detection algorithm for personnel safety pre-warning in underground excavation workface. First, a parameterized backbone network based on HGNetv2 (Hierarchical Graph Network) is employed to reduce the algorithm’s complexity while collecting richer feature information. Second, according to the different targets, the C2f-AKConv (Alterable Kernel Convolution) module is introduced to the algorithm’s neck network to flexibly adjust the size and shape of the convolution kernel, enhancing the algorithm's capacity to adapt to target deformation. Finally, a novel DCNV4-Dyhead (Deformable Convolutional Network v4 -Dynamic Head)module is developed to compensate for the limitations of classic standard convolution in long-range modeling and adaptive spatial aggregation to improve model detection performance. The proposed RAD-YOLOv8s detection algorithm was verified on a dataset of underground excavation workface people. The results showed that the mAP@0.5 and GFlops achieved 91% and 21.1%, which were 2.1% and 25.7% higher than YOLOv8s, respectively. Thus, the algorithm demonstrates improved accuracy and efficiency for real-time personnel safety monitoring in smart mines.},
  archive      = {J_JRTIP},
  author       = {Zhu, Yan song and Wang, Xing and Tang, Chao hong and Liu, Cun yu},
  doi          = {10.1007/s11554-025-01700-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improved RAD-YOLOv8s deep learning algorithm for personnel detection in deep mining workings of mines},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RPDet: A re-parameterized efficient object detection network for UAV edge platforms. <em>JRTIP</em>, <em>22</em>(3), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01701-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving efficient object detection on unmanned aerial vehicle (UAV) platforms is challenging due to the small scale of targets, complex backgrounds, and limited on-board resources. This paper proposes a novel re-parameterized object detection network named RPDet for UAV edge platforms. A key component of RPDet is the re-parameterizable feature extraction block, Rep-DCSA. During training, Rep-DCSA excels in feature extraction and aggregation. In inference, it simplifies the model structure through re-parameterization, significantly reducing model parameters and computational complexity. To advance feature extraction and interaction, we introduce the inverted bottleneck feature enhancement and aggregation (IFEA) module to enhance important channel features and the feature fusion module (FFM) to integrate multi-level information. In addition, we incorporate the global context (GC) module in the detection head, which leverages contextual information to effectively improve the detection performance of small objects in complex backgrounds. Extensive experiments on the public VisDrone and self-constructed HC-UAV datasets demonstrate that RPDet achieves a better trade-off between accuracy and speed than state-of-the-art methods. When deployed on edge platforms, RPDet’s accuracy on Orin Nano and RK3588-RT platforms improved by 3.4% and 5.8% compared to EdgeYOLO. Moreover, RPDet exhibited superior real-time performance and energy efficiency.},
  archive      = {J_JRTIP},
  author       = {Zhang, Buhong and Wang, Zhigang and Lv, Meibo and Liu, Xiaodong and Zhang, Lei},
  doi          = {10.1007/s11554-025-01701-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {RPDet: A re-parameterized efficient object detection network for UAV edge platforms},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient vision-based vehicle speed estimation. <em>JRTIP</em>, <em>22</em>(3), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01704-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a computationally efficient method for vehicle speed estimation from traffic camera footage. We build upon a previous state-of-the-art method that detects 3D bounding boxes of vehicles using vanishing point geometry and a modified RetinaNet object detector. We show that replacing RetinaNet with a better performing YOLOv6 architecture results in significant improvements in terms of computational efficiency without compromising vehicle speed estimation accuracy. We evaluate the modified method in several variants in terms of vehicle detection and speed estimation accuracy. Our extensive evaluation across various hardware platforms, including edge devices, demonstrates significant gains in frames per second (FPS) compared to the prior state-of-the-art, while maintaining comparable or improved speed estimation accuracy. We analyze the trade-off between accuracy and computational cost, showing that smaller models utilizing post-training quantization offer the best balance for real-world deployment. On the BrnoCompSpeed dataset, our best-performing model achieves better speed estimation accuracy, detection precision and recall than the previous state-of-the-art, while being eight times faster. The transferability of our approach is further confirmed by an evaluation on a different dataset.},
  archive      = {J_JRTIP},
  author       = {Macko, Andrej and Gajdošech, Lukáš and Kocur, Viktor},
  doi          = {10.1007/s11554-025-01704-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient vision-based vehicle speed estimation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-aware deep learning for real-time video analysis through pruning, quantization, and hardware optimization. <em>JRTIP</em>, <em>22</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01703-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence has dramatically changed real-time video application use across a variety of fields, from surveillance and autonomous systems. However, these AI models have high computational demands, and thus high energy consumption, limiting scalability in power-constrained environments. This paper systematically addresses energy optimization in real-time video inference by exploring energy-performance trade-offs with sophisticated AI models, i.e., YOLOv8 and MobileNet. A variety of techniques, such as model pruning, quantization, and hardware-aware optimizations, were stringently evaluated. Experimental results indicate that model pruning resulted in a reduction in energy expenditure by up to 35% without compromising detection accuracy to 92%. Moreover, quantization resulted in a further energy saving by 18% and improved inference acceleration by 25% with minimal reduction in accuracy. Moreover, reducing frame rates from 30 frames per second (FPS) to 15 FPS resulted in a power reduction by 40% with a mere reduction in detection performance by 3%. Benchmarking across different hardware setups showed that power savings by optimized lightweight models running in edge devices were by as much as 50% compared to GPUs. These results highlight feasible directions for energy efficient AI system design in real-time video applications, which prove to be particularly useful in edge computing and Internet of Things environments.},
  archive      = {J_JRTIP},
  author       = {Isenkul, M. Erdem},
  doi          = {10.1007/s11554-025-01703-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Energy-aware deep learning for real-time video analysis through pruning, quantization, and hardware optimization},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced safety helmet detection through optimized YOLO11: Addressing complex scenarios and lightweight design. <em>JRTIP</em>, <em>22</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01708-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limited storage and computational capabilities of equipment at construction sites and the complex on-site conditions that can lead to target misdetections and omissions, it is necessary to minimize computational overhead and storage usage as much as possible while maintaining detection performance. This paper proposes an enhanced safety helmet detection method based on the optimized YOLO11 model, addressing two key challenges: detection accuracy in complex backgrounds and model deployment on edge devices. We introduced PConv in the C3K2 module to reduce computational costs and improve feature extraction efficiency, providing high-quality input for multi-scale processing. SPP-ELAN integrates multi-scale information extraction with feature aggregation, capturing contextual information across scales and enhancing local features with global context, offering optimal input for ECA. ECA optimizes the channel weight distribution of SPP-ELAN output features, capturing channel dependencies, highlighting important features, suppressing background noise, and enhancing multi-scale feature expression and sensitivity to small targets. In addition, we propose the NWD–CIoU loss function, combining NWD and CIoU to inherit CIoU’s positioning advantages and address its limitations in non-overlapping targets and scale variations, improving small target localization and multi-scale detection for a more robust and efficient model. Experimental results show that our improved model achieves a 1.7% increase in recall, a 24.5% reduction in parameter count, and a 6.7% improvement in detection speed while maintaining precision and mAP metrics. These improvements enhance the model’s performance in complex environments and small target detection, providing a viable solution for practical applications in safety helmet real-time monitoring systems and edge device deployment.},
  archive      = {J_JRTIP},
  author       = {Li, Xin and Ji, Hua},
  doi          = {10.1007/s11554-025-01708-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhanced safety helmet detection through optimized YOLO11: Addressing complex scenarios and lightweight design},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCR-DETR: A real-time lightweight DETR model for weed detection. <em>JRTIP</em>, <em>22</em>(3), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01709-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time weed detection is a crucial technology for enabling smart agriculture and precise weed management, helping to boost crop yields and reduce pesticide use. However, field-based weed recognition still faces several practical challenges, such as inadequate model lightweight and real-time performance, background interference, high similarity between weed species, and difficulty detecting objects at multiple scales. To address these issues, this study proposes a lightweight, real-time weed detection model called SCR-DETR. The model incorporates a Split Reparameterized Convolution (SRC) module into the backbone to reduce computational redundancy and improve detection efficiency. Additionally, a Unified Statistical Attention Reduction (USAR) module based on subspace optimization is introduced to enhance the model’s global representation and robustness in complex environments. Furthermore, a context-guided scale reconstruction (CGSR) framework is introduced to fuse multi-scale semantic features and local details, improving detection accuracy for visually similar weeds. Experiments on the CottonWeedDet3 and CottonWeedDet12 datasets show that the proposed model reduces FLOPs and parameter count by 40% and 33.5%, respectively, while improving mAP@0.5 by 2.9% on one dataset and maintaining performance on the other. The proposed model achieves a real-time inference speed of 15.4 FPS on the edge computing device Jetson Nano, demonstrating its practical potential for real-time deployment in weed detection applications.},
  archive      = {J_JRTIP},
  author       = {Zhang, YaJun and Xu, Yu and Ma, Chong and Jiang, Ying and Song, YanHai},
  doi          = {10.1007/s11554-025-01709-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SCR-DETR: A real-time lightweight DETR model for weed detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An object detection model for ochotona curzoniae based on the dual-attention mechanism. <em>JRTIP</em>, <em>22</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01710-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In natural scenes, the target pixel ratio in Ochotona curzoniae images is low, and the target features are not prominent, leading to reduced accuracy in feature extraction. Moreover, traditional Ochotona curzoniae object detection models suffer from high memory consumption and limited real-time performance. To address these challenges, we propose an improved object detection model based on YOLOv8. First, we introduce an enhanced channel–spatial attention (ECSA) mechanism that captures cross-channel interactions through one-dimensional convolutions, minimizing information loss due to dimensionality reduction and ensuring a balanced distribution of spatial semantic features. In addition, convolutional kernels of varying sizes are employed to enhance the network’s capability in extracting multi-scale features. Subsequently, a context-guided block (CGB) is integrated into the C2f module of the neck network to improve the localization accuracy and reduce computational complexity. Finally, we propose a context-guided fusion (CGF) module, which calibrates channel weights using a content-guided attention (CGA) mechanism, enabling efficient fusion of shallow and deep features. Experimental results on the Ochotona curzoniae dataset demonstrate that our proposed model achieves a mean average precision (mAP) of 96.4% with a real-time detection speed of 117.1 frames per second (FPS). Compared to the original YOLOv8s model, our approach improves mAP and precision by 2.9% and 8.3%, respectively, while reducing the number of parameters and computational complexity by 8.35% and 5.28%. These results confirm that the proposed model offers both high detection accuracy and real-time performance.},
  archive      = {J_JRTIP},
  author       = {Chen, Haiyan and Hao, Jie},
  doi          = {10.1007/s11554-025-01710-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An object detection model for ochotona curzoniae based on the dual-attention mechanism},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multi-scale detection of construction workers and vehicles based on deep learning. <em>JRTIP</em>, <em>22</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01712-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collision accidents involving heavy construction machinery remain a leading cause of injuries in the building industry, making real-time detection of site equipment and workers imperative. To address the challenges of multi-scale object detection in complex construction environments, this study proposes MSP-YOLO, an improved object detection framework based on YOLOv12n. The proposed model introduces a small object enhancement pyramid (SOEP), which incorporates high-resolution features from shallow layers into the feature pyramid through SPDConv and CSP_Omnikernel, thereby enhancing fine-grained representation and alleviating the degradation of small targets. A lightweight multi-axis gated coordinate attention (MGCA) mechanism is embedded within the neck to refine spatial feature encoding along height, width, and channel dimensions, facilitating robust localization under occlusion and background clutter. Additionally, a dynamic selective weighted fusion (DSWFusion) module connects the backbone and neck via hierarchical skip connections and integrates a pixel-augmented convolution and attention mechanism (PACAM) with dynamic weighting, promoting effective multi-scale feature fusion. These architectural components jointly improve the detection of both small and large objects while maintaining real-time performance. Experimental results on our self-built construction site dataset demonstrate that MSP-YOLO achieves 64.2% AP and 82.5% AP50 on the test set, surpassing YOLOv12n by 4.5 and 3.7%, respectively. It also shows notable improvements in detecting small objects (APs: +5.3%) and medium-sized objects (APm: +8.0%). Furthermore, MSP-YOLO delivers real-time performance with 304.3 FPS on an NVIDIA RTX 4090, validating the effectiveness of our model.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yushi and Lin, Conggong and Chen, Guodong},
  doi          = {10.1007/s11554-025-01712-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient multi-scale detection of construction workers and vehicles based on deep learning},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LfePy: A python package for local feature extraction with CPU and GPU compatibility. <em>JRTIP</em>, <em>22</em>(3), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01705-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local features identify and describe distinct patterns or features in images at a localized level. However, extracting features from images is crucial for image analysis, as it enables models to acquire knowledge and identify patterns. Therefore, we introduce a novel Python package, LfePy (Local Feature Extractors for Python), that utilizes several local descriptors to extract features from grayscale images, ensuring compatibility with both Central Processing Units (CPUs) and Graphical Processing Units (GPUs). The package encompasses a range of techniques for addressing computer vision and image processing challenges. The LfePy package contains twenty-seven histogram-based descriptors and other essential image-processing methods. The package achieves a fast processing time for extracting features from images, as it includes a Graphical Processing Unit (GPU)-based version that outperforms related packages. This package is designed to advance the field of image analysis and related areas. It offers versatility, enhances performance, facilitates research, and supports a wide range of applications.},
  archive      = {J_JRTIP},
  author       = {Mohammed, Mahmoud A. and Hosny, Khalid M.},
  doi          = {10.1007/s11554-025-01705-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LfePy: A python package for local feature extraction with CPU and GPU compatibility},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight defect detection algorithm for wire and arc additive manufacturing based on modified YOLOv8 model. <em>JRTIP</em>, <em>22</em>(3), 1-22. (<a href='https://doi.org/10.1007/s11554-025-01706-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface defects that affect the quality of parts are a particularly significant issue in wire arc additive manufacturing processes (WAAM). Therefore, how to effectively control their surface quality has become a focus of researchers’ attention. However, due to limited computing power and storage space of terminal devices, it is difficult to deploy defect detection models. Therefore, We present a lightweight WAAM weld surface defect detection algorithm based on YOLOv8n, called high-alternative novel YOLO (HAN-YOLO). Specifically, a novel lightweight adaptive Inverted bottleneck (NLAIB) is designed to optimize lightweight network architectures while significantly improving inference speed and computational efficiency. Subsequently, a lightweight alternative alterable kernel convolution (AAKConv) is employed to improve detection accuracy while reducing model parameters and complexity. Furthermore, the High-Level Screening Feature Fusion Pyramid (HS-FPN) was integrated to achieve multi-scale object detection, enhancing the model’s feature selection and fusion capabilities. Finally, experiments on the 3440-WAAM weld surface defect dataset, NEU-DET dataset and Weld dataset are made to test the validity of HAN-YOLO. The experimental results show that, compared with YOLOv8n, the model parameters and GFLOPs of HAN-YOLO are reduced by 44.1% and 39%, respectively. Moreover, HAN-YOLO achieves an increase of 1%, 6.3%, and 38.6% in mAP@0.5, mAP@0.5:0.95, and real-time detection speed (FPS), respectively. These results demonstrate that HAN-YOLO is effective, and provides a lightweight detection scheme for the weld defects in WAAM.},
  archive      = {J_JRTIP},
  author       = {Huang, Yunli and Zhou, Xiangman and Wang, Guilan and Bai, Xingwang},
  doi          = {10.1007/s11554-025-01706-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-22},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight defect detection algorithm for wire and arc additive manufacturing based on modified YOLOv8 model},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time personal protective equipment detection and classification with YOLOv8 multi-scale fusion. <em>JRTIP</em>, <em>22</em>(3), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01715-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex and ever-changing mining environments, frequent safety accidents highlight the growing importance of accurately detecting personal protective equipment (PPE). However, traditional detection methods struggle to adapt to diverse equipment types and complex environmental conditions, often resulting in low accuracy and real-time performance due to human factors such as monitoring fatigue-induced inconsistency, subjectivity in visual inspection, and limited coverage of manual supervision. To address these challenges, this paper proposes a real-time PPE detection method based on YOLOv8, aiming to enhance detection performance in complex environments. By introducing EfficientViT as the backbone network, we improve real-time processing efficiency while maintaining high detection accuracy. The self-correcting illumination network (SCINet) is integrated into the bacbone to adaptively enhance image contrast under low-light conditions and mitigate glare interference. In the neck network, multi-scale dilated attention (MSDA) captures multi-scale features through dilated convolutions, reducing redundancy in global attention mechanisms. To strengthen shape perception for irregular PPE, standard convolutions are replaced with dynamic snake convolution (DSConv). Finally, spatial and channel reconstruction convolution (SCConv) is incorporated into the detection head to suppress feature redundancy, further improving both detection accuracy and real-time performance. Experimental results demonstrate that our proposed model achieves 94.7% accuracy (4.1% improvement) and 85 FPS (23% improvement) with only 2.5 million parameters (22% reduction) on the self-constructed dataset, effectively improving real-time detection of PPE in complex mining environments.},
  archive      = {J_JRTIP},
  author       = {Wang, Zheng and Zhang, Yingjie and Zhang, Shilong},
  doi          = {10.1007/s11554-025-01715-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time personal protective equipment detection and classification with YOLOv8 multi-scale fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time detection method of intelligent classification and defect of transmission line insulator based on LightWeight-YOLOv8n network. <em>JRTIP</em>, <em>22</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01627-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the difficulty in distinguishing different types of insulators in transmission lines and the need for intelligent identification of insulator defects, a real-time detection method for intelligent classification and defect identification of transmission line insulators based on a LightWeight-YOLOv8n network is proposed. This method is used to efficiently detect isolator defects in complex environments and to quickly identify isolator classes. The LightWeight network model MobileNetV3 is used as the backbone network and is based on the YOLOv8n model. The feature extraction ability of the model is enhanced, the redundancy of model parameters is reduced, and the detection speed is improved by adding the SimAM attention mechanism at the end of the backbone network. The LightWeight CSPPC module is used to enhance the feature fusion component of YOLOv8n, reducing the computation load and network complexity while maintaining the accuracy of the model. To further improve the performance of the algorithm, the WIoUv3 bounding box regression loss function is used to replace the original CIoU loss function, and the SlideLoss is used to replace ClsLoss as the category loss function. The experimental results show that the detection accuracy reaches 92.4%, the recall rate reaches 86.6%, and the mAP50 reaches 90.6%. Meanwhile, the training speed is increased by 40.54%, floating-point operation is reduced by 28.57%, and the model parameters are reduced by 34%. Heatmap visualization analysis also showed that the improved models exhibited greater concentration and confidence than the baseline models. Compared to other algorithms, LightWeight-YOLOv8n showed significant advantages in overall performance, accuracy, and real-time target detection. After a random test of 100 images from the test set, the LightWeight-YOLOv8n model exhibited the lowest detection speed and was able to achieve real-time detection.},
  archive      = {J_JRTIP},
  author       = {Tan, Guoguang and Ye, Yongsheng and Chu, Jiawei and Liu, Qiang and Xu, Li and Wen, Bin and Li, Lili},
  doi          = {10.1007/s11554-025-01627-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time detection method of intelligent classification and defect of transmission line insulator based on LightWeight-YOLOv8n network},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight YOLOv7 for bushing surface defects detection. <em>JRTIP</em>, <em>22</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01630-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bushings have a wide range of applications in industry. Once the surface of the bushing is defective, it will affect the assembly between bearings resulting in mechanical inefficiency. At present, due to the different target sizes of the different types of defects on the bushing surface, it is difficult to balance inspection accuracy and speed. This paper proposes lightweight You Only Look Once (YOLO) v7 networks to cope with this problem. In this paper, we use a lightweight network, MobileNetv3, as the backbone network, in which a Residual edges CBAM block (RC-block) is designed to retain feature information while focusing on small-scale targets; finally, we use a bi-directional feature pyramid network (BiFPN) to perform feature fusion to further improve the detection accuracy. The experimental results show that the improved model reduces the Mean Average Precision (mAP) by only $$0.7\%$$ compared with the traditional YOLOv7 model, but the detection speed is increased by $$29.4\%$$ , and the model volume is reduced by $$29.9\%$$ , effectively improves the detection accuracy and speed of all kinds of defects on the surface of the bushings. The improved model was trained under the publicly available dataset NEU-DET, and the results showed the generalisability of the model.},
  archive      = {J_JRTIP},
  author       = {Cheng, Wenjun and Zeng, Pengfei and Hao, Yongping},
  doi          = {10.1007/s11554-025-01630-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight YOLOv7 for bushing surface defects detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ESF-DETR: A real-time and high-precision detection model for cigarette appearance. <em>JRTIP</em>, <em>22</em>(2), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01632-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of cigarettes is greatly affected by appearance defects. Achieving automatic detection of these defects with high precision and speed has been a critical concern for cigarette factories. To meet the needs of manufacturers in detecting appearance defects in cigarettes, this paper proposes a model based on DETR (DEtection TRansformer) for detecting cigarette appearance defects. The model integrates EfficientViT, SENetV2, and FuNet (Full-scale Feature Fusion Network), called ESF-DETR. First, EfficientViT serves as the backbone feature extraction network, substantially reducing model parameters and enhancing feature extraction efficiency. Second, SENetV2 is introduced at the end of the backbone network to improve feature expression accuracy and global information integration capability. Third, the Full-scale Feature Fusion Network (FuNet) is proposed as the encoder, further reducing model parameters while increasing spatial location and high-level semantic information across each feature layer. The proposed ESF-DETR model achieves a mAP of 96.0% with a parameter count of 10.1M. Compared to the original model, the mAP has increased by 4.4%, while the number of parameters has decreased by 49.8%. Additionally, the detection speed reaches 500 FPS, satisfying cigarette production lines’ accuracy and speed requirements.},
  archive      = {J_JRTIP},
  author       = {Ding, Yingchao and Yuan, Guowu and Zhou, Hao and Wu, Hao},
  doi          = {10.1007/s11554-025-01632-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ESF-DETR: A real-time and high-precision detection model for cigarette appearance},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drone object detection incorporating multi-head mixed self-attention and dynamic regression mapping loss function. <em>JRTIP</em>, <em>22</em>(2), 1-17. (<a href='https://doi.org/10.1007/s11554-025-01633-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drone object detection is a critical area within computer vision, facing challenges such as low accuracy in detecting large occluded objects and high rates of missed detection for small objects. These issues arise from inadequate emphasis on the distinctive features of the target area during detection and the regression enhancement of low-quality training examples. To tackle these challenges, this paper introduces a novel method for drone object detection that integrates multi-head mixed self-attention with a dynamic regression mAPping loss function. Initially, a Multi-head Mixed Self-Attention mechanism (MMSA) is developed, tailored to the characteristics of occluded object images. This mechanism is embedded into the backbone and neck components of YOLOv8n to bolster feature extraction and fusion. Subsequently, a dedicated layer for small object detection is incorporated into YOLOv8n to enhance its capability in detecting small objects. A new loss function, Focaler-WIoU, is formulated by merging Focaler-IoU and WIoU, aiming to improve detection across various object scales and accelerate the convergence of bounding box regression loss while enhancing localization accuracy. Additionally, Soft NMS is employed to refine candidate bounding boxes, mitigating missed detections in scenarios with overlapping similar targets. Evaluations on the public dataset VisDrone2019 using standard metrics, including ablation and model comparison experiments, reveal an average precision (mAP @0.5) improvement of 10.2% over the baseline YOLOv8n. The proposed method outperforms other algorithms such as Drone-YOLO (nano), YOLOv11n, Faster RCNN, and FE-YOLOv5 in detection accuracy. Further validation on datasets like CityPersons and CrowdHuman underscores the versatility of the improved algorithm. The experimental outcomes confirm that the MMSA attention mechanism significantly enhances the detection of occluded objects, achieving superior accuracy compared to established object detection algorithms. This suggests that the proposed method holds substantial practical and general applicability for drone image detection in natural settings. Detailed code is available at https://github.com/CodeSworder/MMSA .},
  archive      = {J_JRTIP},
  author       = {Su, Qinghua and Mu, Jianhong and Xu, Sheng and Wan, Kaizheng and Qi, Xiangyu and Zhang, Zhichao and Li, Juntao},
  doi          = {10.1007/s11554-025-01633-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Drone object detection incorporating multi-head mixed self-attention and dynamic regression mapping loss function},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time dynamic scale-aware fusion detection network: Take road damage detection as an example. <em>JRTIP</em>, <em>22</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01634-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV)-based Road Damage Detection (RDD) is important for daily maintenance and safety in cities, especially in terms of significantly reducing labor costs. However, current UAV-based RDD research still faces many challenges. For example, the damage with irregular size and direction, the masking of damage by the background, and the difficulty of distinguishing damage from the background significantly affect the ability of UAV to detect road damage in daily inspection. To solve these problems and improve the performance of UAV in real-time road damage detection, we design and propose three corresponding modules: a feature extraction module that flexibly adapts to shape and background; a module that fuses multiscale perception and adapts to shape and background; an efficient downsampling module. Based on these modules, we designed a multi-scale, adaptive road damage detection model with the ability to automatically remove background interference, called Dynamic Scale-Aware Fusion Detection Model (RT-DSAFDet). Experimental results on the UAV-PDD2023 public dataset show that our model RT-DSAFDet achieves a mAP50 of 54.2%, which is 11.1% higher than that of YOLOv10-m, an efficient variant of the latest real-time object detection model YOLOv10, while the amount of parameters is reduced to 1.8M and FLOPs to 4.6G, with a decrease by 88% and 93%, respectively. Furthermore, on the large generalized object detection public dataset MS COCO2017 also shows the superiority of our model with mAP50–95 is the same as YOLOv9-t, but with 0.5% higher mAP50, 10% less parameters volume, and 40% less FLOPs.},
  archive      = {J_JRTIP},
  author       = {Pan, Weichao and Wang, Xu and Huan, Wenqing},
  doi          = {10.1007/s11554-025-01634-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time dynamic scale-aware fusion detection network: Take road damage detection as an example},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced underwater object detection with YOLO-LDFE: A model for improved accuracy with balanced efficiency. <em>JRTIP</em>, <em>22</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01628-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In underwater image analysis, challenges such as complex environments, low model performance, and slow processing efficiency hinder effective object detection, which is crucial for real-time monitoring. To address these issues, we propose a high-precision underwater object detection model named YOLO-LDFE. To overcome the limitations of existing datasets, we have developed a comprehensive multi-species underwater biological dataset, MCUA, containing 9327 labeled images across 14 categories. The primary contributions of this work are as follows: (1) SPPF_SCSA, which integrates multi-semantic information with channel-space attention mechanisms to enhance performance; (2) the substitution of traditional convolutions with GSConv in C2f, reducing model size while maintaining feature extraction; (3) LEDF, which improves performance through multi-level, dense connections. YOLO-LDFE achieves exceptional results, with an average precision of 93% on the URPC2021 dataset, outperforming existing algorithms while maintaining high detection speed, demonstrating its potential for real-time underwater monitoring.},
  archive      = {J_JRTIP},
  author       = {Liu, JiaXin and Zhou, RiGui and Li, YaoChong and Ren, PengJu},
  doi          = {10.1007/s11554-025-01628-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhanced underwater object detection with YOLO-LDFE: A model for improved accuracy with balanced efficiency},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive receptive field and attention-guided feature enhancement for ceramic tile surface defect detection. <em>JRTIP</em>, <em>22</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01636-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of diverse defect shapes, small defect sizes, and real-time detection on ceramic tile surfaces, this paper proposes an efficient anchor-free detector that integrates adaptive receptive fields and feature enhancement. First, the anchor-free YOLOv8 is proposed as the detection framework to eliminate the need for setting anchor-related hyperparameters, thus avoiding their impact on performance. Second, an adaptive receptive field module (ARFM) is introduced, which extracts defect features from multiple scales through constructing several parallel branches and fuses the feature maps from different receptive fields, thereby dynamically adjusting the receptive field to detect various defects. Finally, a feature enhancement module (FEM) is added to the neck part, enhancing feature representation from both global and spatial dimensions to reduce feature information loss and improve defect detection performance. Experiments show that our detector achieves a mean average precision of 70.4%, an improvement of 6.1% over YOLOv8, while maintaining a high detection speed of 121.2 frames per second. This performance surpasses the state-of-the-art detection methods. These results indicate that our proposed model can achieve satisfactory defect detection performance, meeting the industry’s real-time detection needs.},
  archive      = {J_JRTIP},
  author       = {Yu, Songsen and Wang, Zhemeng},
  doi          = {10.1007/s11554-025-01636-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Adaptive receptive field and attention-guided feature enhancement for ceramic tile surface defect detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCR-YOLOv8: An enhanced algorithm for target detection in sonar images. <em>JRTIP</em>, <em>22</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01637-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sonar imaging plays a pivotal role in the detection of targets underwater. However, the performance of most sonar detection methods is suboptimal due to the complexity of the underwater environment and its susceptibility to noise interference. To address these issues, this paper proposes a SCR-YOLOv8 algorithm based on an enhanced YOLOv8 architecture. This algorithm aims to overcome the challenges associated with traditional sonar target detection. The proposed approach involves two primary modifications: first, the Conv module in the trunk and neck networks is replaced with the more efficient SPDConv. Second, the CCFM module is introduced to reduce the model size and number of parameters. Subsequently, the Spatial Channel Reconstruction Module (SCRM) is employed. The design of the feature extraction and fusion stage of the model is intended to enhance the model’s efficiency in extracting contextual information from both spatial and channel dimensions. Finally, the Inner-CIoU is employed in lieu of the CIoU to achieve regression results that are both faster and more effective. The experimental results demonstrate that, in comparison with the baseline model, SCR-YOLOv8 enhances precision, recall, and mAP50 by 2.9%, 5.8%, and 2.3%, respectively. Concurrently, the model size and the computational complexity are diminished by 40.2% and 21.4%, respectively.Moreover, a frame rate of 91 FPS is attained, meeting the criteria for real-time detection.},
  archive      = {J_JRTIP},
  author       = {Weng, Youlei and Xiang, Xiaodong and Ma, Linghang},
  doi          = {10.1007/s11554-025-01637-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SCR-YOLOv8: An enhanced algorithm for target detection in sonar images},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight outdoor drowning detection based on improved YOLOv8. <em>JRTIP</em>, <em>22</em>(2), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01638-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite abundant global water resources, drowning remains one of the complex challenges to tackle worldwide. To solve the problem of complicated outdoor remote water environments, where people in the water have different morphologies and scale variations, and the existing detection models are highly complex and computationally intensive, we propose a lightweight drowning detection model, YOLOv8-REH, based on YOLOv8n. First, the C2f-RVB-ELA feature extraction module (effective fusion of C2f and RVB-ELA) is designed to improve the C2f module of the YOLOv8n backbone network, reducing the model’s parameters and computation effectively. Second, in the Neck section, we incorporate the ELA-HSFPN feature fusion module, which consists of the Hierarchical Scale-based Feature Pyramid Network (HSFPN) module and the Efficient Local Attention (ELA) mechanism. This helps us gather multi-scale and spatial information perception more comprehensively and efficiently, enhancing the feature fusion capability of the model. Then, we introduce the Powerful-IoUv2 loss function to enhance bounding box regression along the effective path, consequently improving the model’s convergence speed and detection performance. Finally, we use a pruning method based on layer-adaptive magnitude-based pruning scoring to prune and remove unimportant redundant parameters from the improved model, further compressing the model complexity and achieving a better lightweight effect. The final compressed YOLOv8-REH model is compared with the current mainstream algorithms for comparison experiments as well as ablation experiments. The experimental results indicate that the YOLOv8-REH model sustains an average detection accuracy while the computational volume, parameter count, and model size reach 3.7GFLOPs, 0.72 M, and 1.8 MB, and the FPS is improved by 22.9, which achieves a significant improvement in the model’s lightweight performance compared with the existing methods.},
  archive      = {J_JRTIP},
  author       = {Liu, Xiangju and Shuai, Tao and Liu, Dezeng},
  doi          = {10.1007/s11554-025-01638-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight outdoor drowning detection based on improved YOLOv8},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMC-net: A lightweight network for real-time surface defect segmentation. <em>JRTIP</em>, <em>22</em>(2), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01639-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial applications, surface defect segmentation is a critical task. However, facing challenges such as diverse defect scales, low contrast between defects and background, high interclass similarity and real-time detection in defect inspection, we propose an efficient lightweight network, named DMC-Net, for real-time surface defect segmentation. The structural optimization of DMC-Net includes the following components: (1) depthwise separable convolution attention module, a lightweight and efficient feature extraction module for extracting multi-scale defect features. (2) Multi-scale feature enhancement module, providing long-range information capture and local information focusing to enhance defect localization capability. (3) Channel shuffle group convolution, enhancing feature interaction and information propagation while reducing the parameter quantity. Based on the experimental results, DMC-Net achieved an mIoU of 73.74% on the NEU-SEG dataset, while achieving an FPS of 211.7. This indicates that we have successfully reduced the complexity and computational cost of the model while improving performance, providing a feasible solution for industrial applications. The relevant code can be obtained at https://github.com/Michaelzyb/DMC-Net.git.},
  archive      = {J_JRTIP},
  author       = {Zuo, Haiqiang and Zheng, Yubo and Huang, Qizhou and Du, Zehao and Wang, Hao},
  doi          = {10.1007/s11554-025-01639-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DMC-net: A lightweight network for real-time surface defect segmentation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time recognition research for an automated egg-picking robot in free-range duck sheds. <em>JRTIP</em>, <em>22</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01640-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving efficient and accurate detection and localization of duck eggs in the unstructured environment of free-range duck sheds is crucial for developing automated egg-picking robots. This paper proposes an improved YOLOv5s-based model (YOLOv5s-MNKS) designed to enhance detection performance, reduce model complexity, and improve the robot’s adaptability and operational efficiency in complex environments. The model utilizes MobileNetV3 as the backbone network, reducing the number of parameters and increasing detection speed. The Squeeze-and-Excitation Module is replaced with a Normalization-based Attention Module to improve feature extraction capability. Group Shuffle Convolution and Bidirectional Feature Pyramid Network are introduced in the Neck layer, enhancing multi-scale feature fusion while reducing parameter count. A Soft-CIoU-NMS loss function is also designed, which improves detection accuracy in scenarios involving dense stacking and occlusion by lowering the confidence of overlapping bounding boxes instead of directly eliminating them. Experimental results demonstrate that the mAP of YOLOv5s-MNKS reaches 95.6%, representing a 0.3% improvement over the original model, while the model size is reduced to 5.7 MB, approximately 40% of the original size. When deployed on the Jetson Nano embedded platform with TensorRT acceleration, the model achieves a detection frame rate of 22.3 frames per second. In simulated and real-world duck shed scenarios, the improved model accurately and quickly identifies and locates duck eggs in complex environments, including occlusion, stacking, and low lighting, demonstrating strong robustness and applicability. This research provides technical support for the future development of duck egg-picking robots.},
  archive      = {J_JRTIP},
  author       = {Jie, Dengfei and Wang, Jun and Wang, Hao and Lv, Huifang and He, Jincheng and Wei, Xuan},
  doi          = {10.1007/s11554-025-01640-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time recognition research for an automated egg-picking robot in free-range duck sheds},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EF-RT-DETR: A efficient focused real-time DETR model for pavement distress detection. <em>JRTIP</em>, <em>22</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01641-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification and localization of road distress play a crucial role in intelligent road health monitoring systems. To address the challenges of complex road backgrounds, diverse shapes of distress objects, and high computational resource requirements, this paper proposes an efficient focusing real-time road distress detection model (EF-RT-DETR). Based on RT-DETR, the model designs a new backbone network aimed at accurately capturing fine features and optimizes the attention mechanism to effectively reduce interference from complex backgrounds while enhancing the processing of detailed information. Additionally, an innovative fusion module is introduced in the feature fusion stage to further enhance the interaction between local and global features, while also reducing computational costs. Experiments conducted on the China_Motorbike subset of the RDD2022 dataset include ablation studies to validate the effectiveness of the proposed modules. The experimental results show that EF-RT-DETR reduced background false positives compared to the baseline model, with $${\hbox {mAP}}_{50}$$ and $${\hbox {mAP}}_{50:95}$$ improving by 9.5 and 7.1%, respectively, while reducing computation by 35.4% and the number of parameters by 25.7%.},
  archive      = {J_JRTIP},
  author       = {Han, Tao and Hou, Shuainan and Gao, Can and Xu, Shanyong and Pang, Jiale and Gu, Hai and Huang, Yourui},
  doi          = {10.1007/s11554-025-01641-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EF-RT-DETR: A efficient focused real-time DETR model for pavement distress detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FPGA-based 1D-CNN accelerator for real-time arrhythmia classification. <em>JRTIP</em>, <em>22</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01642-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, cardiovascular diseases are prevalent, real-time arrhythmia detection through electrocardiogram (ECG) is a vital aspect of health monitoring. Consequently, there has been growing interest in wearable edge devices capable of real-time ECG classification. Current convolutional neural networks (CNN) for arrhythmia classification often involve a large number of parameters and have high computational complexity. This work introduces a one-dimensional lightweight convolutional neural network model (LW-CNN), which leverages residual connections and one-dimensional depthwise separable convolution (DSC). The proposed network shows accuracy achieving 99.59% on software-implementation with fewer parameters and lower computational complexity. A model compression method combining unstructured pruning and incremental network quantization (INQ) is implemented to further reduce the model complexity. Additionally, a neural network accelerator based on multiplication-free convolutional processing unit is designed with high level synthesis (HLS) to reduce resource consumption and achieve real-time ECG classification. The entire system is implemented on Xilinx Zynq 7Z020 board leveraging PS-PL synergy design and achieves classification accuracy of 96.55%, a latency of 63 ms under 50-MHZ and a power consumption of 1.78W with resource consumption of 13726 LUT, 9 DSP, and 5.5 BRAM, which improves resource efficiency.},
  archive      = {J_JRTIP},
  author       = {Liu, Zheming and Ling, Xiaofeng and Zhu, Yu and Wang, Nan},
  doi          = {10.1007/s11554-025-01642-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-based 1D-CNN accelerator for real-time arrhythmia classification},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Egc-yolo: Strip steel surface defect detection method based on edge detail enhancement and multiscale feature fusion. <em>JRTIP</em>, <em>22</em>(2), 1-18. (<a href='https://doi.org/10.1007/s11554-025-01644-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to imperfect manufacturing crafts and external factors, steel often produces surface defects during manufacturing, seriously influencing its lifespan and availability. It is therefore crucial that surface defects are detected in industrial production. Nevertheless, conventional detection techniques are vulnerable to background interference and feature scale variations when employed to identify defects on strip surfaces. Therefore, we propose an EGC-YOLO model based on YOLOv8 for steel surface defect detection. First, an edge detail enhancement module (EDEM), based on Sobel convolution (SobelConv), is designed and embedded into C2f to capture defective edges and texture better. Second, the generalized dynamic feature pyramid network (GDFPN) is introduced in the neck structure to enhance the multiscale feature fusion. This enables the model to adapt to defects of different sizes and shapes. Finally, the content-guided attention fusion (CGA Fusion) module is employed to optimize the fusion of shallow and deep features for more detection precision. The extensive experimental results illustrate that the accuracy of EGC-YOLO reaches 80.2% mAP on NEU-DET and improves by 3.7% over YOLOv8. The model’s inference speed reached 136.4 frames per second (FPS). EGC-YOLO outperforms other models in accuracy and speed for detecting steel surface defects, showcasing its industrial application potential.},
  archive      = {J_JRTIP},
  author       = {Ni, Yunfeng and Zi, Dexing and Chen, Wei and Wang, Shouhua and Xue, Xinyi},
  doi          = {10.1007/s11554-025-01644-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Egc-yolo: Strip steel surface defect detection method based on edge detail enhancement and multiscale feature fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight real-time object detection method for complex scenes based on YOLOv4. <em>JRTIP</em>, <em>22</em>(2), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01645-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object detection network can achieve real-time performance on high-performance computers with ease, but the large number of parameters and limited computational resources of mobile devices pose significant challenges, leading to suboptimal detection performance. As application scenarios expand to embedded devices, such as autonomous vehicles and unmanned aerial vehicles, higher requirements on the real-time performance and resource consumption of object detection algorithms have been proposed. The traditional YOLOv4 model has huge parameters and computations, resulting in low detection efficiency in complex environments. To address it, we propose an improved YOLOv4-lite lightweight network based on depthwise over-parameterized convolutional layer (DO-Conv). Firstly, we replace CSPDarknet53 backbone network in YOLOv4 with MobileNetV3. The parameter quantity of YOLOv4-lite is only 62.4 $$\%$$ of YOLOv4. Secondly, we use DO-Conv to replace the traditional convolution network in YOLOv4 to promote feature extraction effectiveness without extending layers. Meanwhile, we use ReLU6 to replace original Leaky ReLU to improve detection efficiency and obtain good numerical resolution. Our method achieved 70.38 $$\%$$ mean average precision (mAP) on Pascal VOC07+12 dataset and 27.63 $$\%$$ average precision (AP) on MS COCO 2017 dataset. Its model size is only 34MB. The running speed on Titan X reaches 41.82 frames per second (fps), which is 1.7 times that of YOLOv4. The experimental results demonstrate that the proposed method achieves a well-balanced trade-off between speed and accuracy, thereby meeting the real-time requirements for object detection in practical applications.},
  archive      = {J_JRTIP},
  author       = {Ding, Peng and Li, Tong and Qian, Huaming and Ma, Lin and Chen, Zhongfei},
  doi          = {10.1007/s11554-025-01645-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight real-time object detection method for complex scenes based on YOLOv4},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective geometrical feature extraction method for scale and rotational invariant multi-lingual character recognition. <em>JRTIP</em>, <em>22</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01646-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new feature extraction technique for Optical Character Recognition (OCR) that achieves state-of-the-art results in the recognition of multilingual characters, overcoming scale, rotation, and distortion challenges. Our method leverages polar coordinates to incorporate two innovative features: Extended Ellipse-Based Features (EEB) and Crossing Count Measure (CCM), which provide inherent scale and rotational invariance. From the benchmark datasets, the proposed technique was tested on character datasets such as ISI Bengali and Chars74K, yielding accuracy rates of 98.82% and 98.69% respectively. These statistics also depict high precision and recall values coupled with a high F1-score, indicating that the method has been sound and robust. Interestingly, our approach exhibits far less computational overhead compared to traditional CNN-based methods and is, therefore, a good candidate to be deployed on resource-constrained edge devices. This work fills the gap between high-performance OCR systems and practical deployment needs by providing a scalable and efficient solution for multilingual character recognition in diverse and challenging contexts.},
  archive      = {J_JRTIP},
  author       = {Mohammed, Sharfuddin Waseem and Murugan, Brindha},
  doi          = {10.1007/s11554-025-01646-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An effective geometrical feature extraction method for scale and rotational invariant multi-lingual character recognition},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating convolutional neural networks on FPGA platforms: A high-performance design methodology using OpenCL. <em>JRTIP</em>, <em>22</em>(2), 1-19. (<a href='https://doi.org/10.1007/s11554-025-01647-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) are among the most promising algorithms, outperforming traditional methods in classification tasks with superior accuracy. They have been widely applied across various deep learning domains, including computer vision, speech recognition, image processing, and object detection. However, many CNNs require substantial computational resources, particularly within their convolutional layers. As high-performance CNNs continue to evolve, their processing and memory requirements are also increasing. To address these challenges, this paper proposes an effective design methodology for accelerating CNN algorithms on Field-Programmable Gate Array (FPGA) hardware architectures. The proposed methodology introduces a novel approach for accelerating CNN algorithms using FPGAs, addressing the significant processing and memory demands associated with CNNs. The implementation is based on Open Computing Language (OpenCL), which provides rapid implementation flows. This approach was chosen for its efficiency in reducing development time and eliminating the need to manually write hardware description language (HDL) code. The MNIST and the CIFAR-10 datasets on the Xilinx ZYNQ 7000 device were used to evaluate our approach. Our method achieved a 97% recognition rate on MNIST and an 86% recognition rate on CIFAR-10. We compared the execution time of our accelerated CNN kernel on the FPGA with that of a single-core Central Processing Unit (CPU). The experimental results demonstrate that our proposed design is 10 times faster than a standard CPU, validating its effectiveness. Our model optimizes power consumption and performance, exceeding previous studies in accuracy and efficiency. It is well suited for real-world applications that demand both precision and energy efficiency.},
  archive      = {J_JRTIP},
  author       = {Gdaim, Soufien and Mtibaa, Abdellatif},
  doi          = {10.1007/s11554-025-01647-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Accelerating convolutional neural networks on FPGA platforms: A high-performance design methodology using OpenCL},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ship detection algorithm based on structural reparameterize dilated large-kernel convolution and spatial selective kernel mechanism. <em>JRTIP</em>, <em>22</em>(2), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01649-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the issues of large target scale variations and complex background environments in ship detection, this paper presents a single-stage ship detection algorithm named DrbLSK, which is an improvement on YOLOv5. First, to enhance the model’s ability to model the context of targets with different scales, a kernel selection module is introduced into the backbone network. By dynamically selecting the receptive field, environmental interference is reduced. Simultaneously, the combination of large-kernel dilated convolution and structural reparameterization is employed to reduce model parameters and enhance the feature expression capability of the backbone network. Next, a decoupled detection head is utilized to alleviate the conflict between classification and regression tasks in the target detection task. Moreover, CIoU is used in the detection head to replace the original loss function, thereby accelerating the convergence of the network. Experimental results show that, on the ABOships dataset, the improved model reduces the number of parameters while increasing accuracy by 2.5% compared to YOLOv5, with a mean average precision of 63.9.},
  archive      = {J_JRTIP},
  author       = {Shi, Yujing and Wang, Haicheng and Li, Shanqiang},
  doi          = {10.1007/s11554-025-01649-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Ship detection algorithm based on structural reparameterize dilated large-kernel convolution and spatial selective kernel mechanism},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ES-YOLOv8: A real-time defect detection algorithm in transmission line insulators. <em>JRTIP</em>, <em>22</em>(2), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01651-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insulators are essential components for protecting transmission lines, yet they are vulnerable to defects caused by harsh environments, which can compromise their performance and transmission stability. Regular inspections are therefore crucial. However, traditional manual inspection methods are time-consuming, error-prone, and pose safety risks when working under adverse conditions. In contrast, the combination of drone inspections and image processing technology enables the safe and efficient detection of insulator defects. In this study, the YOLOv8 model was used to detect insulators. Although YOLOv8 exhibits strong detection performance, it encounters challenges in terms of detection speed and computational efficiency in real-time tasks. To address these challenges, this study introduces EfficientViT from the Transformer framework to enhance feature extraction and improve the model’s computational efficiency. Additionally, a spatial context pyramid was incorporated into the model’s header module to bolster its feature analysis capabilities, particularly for small targets and intricate background details, yielding notable results. After several enhancements, the proposed network is named ES-YOLOv8s. The experimental results reveal that compared with YOLOv8, ES-YOLOv8s improved the precision, recall, mean average precision, and harmonic mean of the precision and recall scores on the Insulator Defect Image Dataset by 2%, 3%, 2.7%, and 3%, respectively. Furthermore, the ES-YOLOv8s exhibited notable advantages in terms of giga floating point operations per second and frames per second, highlighting substantial improvements in both detection performance and computational efficiency. The study findings show that the improved model can perform better in real-time detection tasks.},
  archive      = {J_JRTIP},
  author       = {Song, Xiaoyang and Sun, Qianlai and Liu, Jiayao and Liu, Ruizhen},
  doi          = {10.1007/s11554-025-01651-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ES-YOLOv8: A real-time defect detection algorithm in transmission line insulators},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced YOLOv5 for micro-defect detection on KDP crystal surfaces: A fusion of EfficientNetV2 and normalized wasserstein distance. <em>JRTIP</em>, <em>22</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01654-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of micro-defects on potassium dihydrogen phosphate (KDP) crystal surfaces is crucial for the efficient operation of high-energy laser equipment. However, traditional defect detection methods suffer from low accuracy and efficiency. To address this, we propose an enhanced YOLOv5 model specifically tailored for micro-defect detection on KDP crystals. Our approach integrates EfficientNetV2 as the backbone for feature extraction, reducing parameters by 12.68% and computational cost by 10.8%. Furthermore, we introduce the XIoU loss function and incorporate the normalized Wasserstein distance (NWD) theory, forming a novel loss function that improves the smoothness of bounding boxes and enhances the model's ability to learn and distinguish micro-defect features. We created a dataset comprising micro-defect images of KDP crystal surfaces. Experimental results on this dataset show that our method achieves a mean average precision (mAP) of 96.9% and an F1 score of 0.944, outperforming other mainstream models. This study presents a novel and effective approach for micro-defect detection on KDP crystal surfaces, contributing to advancements in ultra-precision machining technology. For further details and to access the dataset and model, please visit our repository: https://github.com/Good-he/EXN-yolo/tree/master .},
  archive      = {J_JRTIP},
  author       = {Feng, Kai and He, Shuhao and Wu, Xinlong and Jiang, Peidong and Huang, Shuai},
  doi          = {10.1007/s11554-025-01654-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhanced YOLOv5 for micro-defect detection on KDP crystal surfaces: A fusion of EfficientNetV2 and normalized wasserstein distance},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low brightness PCB image enhancement algorithm for FPGA. <em>JRTIP</em>, <em>22</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01635-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of low defect detection rate of PCB images captured by cameras in industrial scenarios under low-light environments, an MGIE (Mean–Gamma Image Enhancement) image brightness enhancement algorithm and the corresponding FPGA design scheme are proposed. Firstly, the RGB image is converted into the YCrCb color space, and the illumination component Y is separated. Then, the illumination component Y is enhanced by the MSR (Multi-Scale Retinex) algorithm based on multi-scale mean filtering, and the Gamma correction algorithm is used to adjust the brightness. Subsequently, the processed Y channel is fused with the Cr and Cb channels to obtain the final output. Secondly, after algorithm research, this paper elaborates on the algorithm design and deployment scheme based on FPGA. The MGIE IP core is designed in the HLS (High-Level Synthesis) environment, and optimization and acceleration are carried out by means of creating look-up tables and constructing PIPELINE. Significantly, this research is capable of real-time processing of images in video. Specifically, images are captured in real time by the OV5640 camera, and the processed images are immediately displayed on the LCD screen. The experimental results show that the MGIE algorithm has remarkable effectiveness in processing low-light PCB images, with a PSNR (Peak Signal-to-Noise Ratio) reaching 17.34 and an SSIM (Structural Similarity Index Measure) reaching 0.79. After the end-to-end deployment, the processing speed of 1280 × 720 and 640 × 640 pixel images reaches 30fps/s and 70fps/s, respectively, meeting the needs of real-time processing.},
  archive      = {J_JRTIP},
  author       = {Han, Jin and Zheng, Meijuan and Dong, Jianye},
  doi          = {10.1007/s11554-025-01635-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low brightness PCB image enhancement algorithm for FPGA},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FPGA architecture-based front-end processing for SLAM applications. <em>JRTIP</em>, <em>22</em>(2), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01650-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping is intended for robotic and autonomous vehicle applications. These targets require an optimal embedded implementation that respects real-time constraints, limited hardware resources, and energy consumption. SLAM algorithms are computationally intensive to run on embedded targets, and often, the algorithms are deployed on CPUs or CPU–GPGPU architectures. With the growth of embedded heterogeneous computing systems, research work is increasingly interested in the algorithm–architecture mapping of existing SLAM algorithms. The latest trend is pushing processing closer to the sensor. FPGAs constitute the perfect architecture for designing smart sensors by providing low latency suitable for real-time applications, such as video streaming, as they supply data directly into the FPGA without needing a CPU. In this work, we propose the implementation of the HOOFR-SLAM front end on a CPU–FPGA architecture, including both feature extraction and matching processing blocks. A high-level synthesis (HLS) approach based on OpenCL paradigm has been used to design a new system architecture. The performance of the FPGA-based architecture was compared to a high-performance CPU. This innovative architecture delivers superior performance compared to existing state-of-the-art systems.},
  archive      = {J_JRTIP},
  author       = {El Bouazzaoui, Imad and Rodríguez Flórez, Sergio and El Ouardi, Abdelhafid},
  doi          = {10.1007/s11554-025-01650-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA architecture-based front-end processing for SLAM applications},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV inspection insulator defect detection method based on dynamic adaptation improved YOLOv8. <em>JRTIP</em>, <em>22</em>(2), 1-17. (<a href='https://doi.org/10.1007/s11554-025-01660-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insulators play an important role in ensuring the electrical stability of transmission systems, and timely detection of insulator status can effectively guarantee the safe and stable operation of power systems. However, the generalized YOLOv8 model still faces two challenges in the complex application scenario of power inspection by Unmanned Aerial Vehicles (UAVs) for high-altitude insulator defects: one is the standard convolution’s inability to efficiently extract insulator edge features, which leads to a large number of convolution operations and results in parameter redundancy; the other is the distortion of feature spatial information fusion due to complex background interference. To solve these challenges, this paper proposes a lightweight and dynamically adaptive model named DVW-YOLO (Dynamic VoV Wise YOLO, based on YOLOv8). First, a Dynamic Elliptic Convolution is designed to make the model more efficient in focusing on elliptic features, significantly reducing redundant feature extraction and multi-size fusion, and thus significantly reducing the model parameters. Then, a VoV-DE-GSCSP module is designed, and a Dynamic Adaptive Fusion Network is developed in combination with GSConv. This network restricts the cross-fusion of complex background interference information and enhances the effective feature fusion range. Finally, Wise-IoU is used to balance the performance between lightweight and average accuracy. Experimental results demonstrate that, compared with the standard YOLOv8 model, the lightweight performance of the model proposed in this paper has been greatly improved, with a reduction of 11.87% in parameter quantity and a 2.3% increase in $$\hbox {mAP}_{0.75}$$ . Meanwhile, compared with the same level lightweight model (YOLOv5), the F1-score increased by 4%, and $$\hbox {mAP}_{0.75}$$ increased by 7.4%, with a significant improvement in detection performance.},
  archive      = {J_JRTIP},
  author       = {Hu, Cong and Lv, Lingfeng and Zhou, Tian},
  doi          = {10.1007/s11554-025-01660-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {UAV inspection insulator defect detection method based on dynamic adaptation improved YOLOv8},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification of rice disease based on MFAC-YOLOv8. <em>JRTIP</em>, <em>22</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01661-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rice has an important place as food for more than half of the world’s population, but its yield and stability are severely affected by rice diseases. Current rice disease detection methods are inefficient and costly. To address this issue, this study collected 1505 data points and images from rice fields and the web and annotated them for training frameworks. After that, this study developed a new rice detection framework, MFAC-YOLOv8, based on the YOLOv8 network. The framework integrates the MobileNetv4 network and the Focal Modulation module and uses them as the backbone network of the improved YOLOv8 to improve the detection accuracy of the network. In addition, AKConv and the Context Guided block are introduced and used to further improve the neck network of YOLOv8, which simplifies the framework and further enhances the detection. The experimental results show that the MFAC-YOLOv8 framework exhibits excellent performance in all evaluation metrics, with 8.1 $$\%$$ , 2.2 $$\%$$ , and 3.4 $$\%$$ improvements in accuracy, recall, and mean average precision, respectively, compared with the baseline framework. In addition, the framework achieves a high frame rate of 166 frames per second (FPS) and a relatively compact parameter size of 18.4 million. These results suggest that the proposed method has great potential for effective rice disease detection.},
  archive      = {J_JRTIP},
  author       = {Wang, Bingyang and Zhou, Huibo and Xie, Hui and Chen, Ruolan},
  doi          = {10.1007/s11554-025-01661-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Identification of rice disease based on MFAC-YOLOv8},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vaccine-YOLOv10: Real-time QR code detection model for complex light condition. <em>JRTIP</em>, <em>22</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01631-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {QR code is not only an information storage approach, but also a spatial localization sign. Compared to other spatial localization signs, QR code is more accurate and more efficient to be detected. To achieve spatial localization by QR code, detection is the essential procedure. Existing approaches perform well in regular light condition, but perform badly in complex light condition, because frame quality is extremely damaged by complex light condition. In the real world, complex light condition is very common but always unavoidable. Therefore, it is necessary and worthwhile to improve the under-complex-light QR code detection. In this paper, Vaccine-YOLOv10 (VCY) is proposed to enhance QR code detection capability in complex light condition. First, GhostConv and FasterC2f are introduced to replace the corresponding original modules of YOLOv10n. Second, Simulative Data Augment Algorithm (SDA) is proposed to simulate 5 types of complex light condition. Third, self-built Multi-Scene QR Code Dataset (MSQ) is augmented by SDA for VCY training. Compared to the baseline model YOLOv10n, VCY is improved on both lightweight and accuracy. Specifically, FPS reaches 150; GFLOPs reduces from 8.2 to 5.3; mAP50 increases from 0.877 to 0.905. Code: https://github.com/AlexTraveling/Vaccine-YOLOv10 .},
  archive      = {J_JRTIP},
  author       = {Zhao, Xiaobei and Li, Xiang},
  doi          = {10.1007/s11554-025-01631-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Vaccine-YOLOv10: Real-time QR code detection model for complex light condition},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FPGA implementation of double-head SalsaNext: A CNN-based model for LiDAR point cloud segmentation. <em>JRTIP</em>, <em>22</em>(2), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01643-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study details the adaptation and deployment of a customized SalsaNext model for semantic segmentation of LiDAR point clouds on edge devices, benchmarked using the SemanticKITTI and Waymo Open datasets. We introduce an innovative multi-dataset training framework designed specifically for range image-based segmentation models. Central to this approach is our double-head SalsaNext model, which features two output heads to facilitate simultaneous training and inference on the Waymo and SemanticKITTI datasets. Following training, the model is streamlined by removing the head dedicated to Waymo, resulting in a compact, single-headed version optimized for SemanticKITTI. This simplified model is then quantized to employ fixed-point arithmetic, significantly enhancing computational efficiency and enabling real-time operation on the Xilinx Kria KV260 board. The quantization process markedly reduces resource consumption while preserving competitive accuracy. Our deployment on this low-power, FPGA-based platform underscores the potential of energy-efficient systems for advanced 3D semantic segmentation, with promising applications in autonomous systems and robotics. Experimental results validate the effectiveness of our training schema and the success of the optimized implementation of the double-head model on resource-constrained hardware.},
  archive      = {J_JRTIP},
  author       = {Adiyaman, Muhammed Yasin and Baskaya, Faik},
  doi          = {10.1007/s11554-025-01643-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA implementation of double-head SalsaNext: A CNN-based model for LiDAR point cloud segmentation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-VG: An efficient real-time recyclable waste detection network. <em>JRTIP</em>, <em>22</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01655-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of sorting waste using robots, it is necessary to determine the location and category information of waste. The key to the efficient work of the waste sorting robot lies in the accuracy of the target recognition. Traditional waste detection algorithms suffer from high computational cost, low detection accuracy, and poor adaptability, which cannot meet the actual detection needs. Therefore, this paper proposes an efficient and lightweight algorithm YOLO-VG based on the current mainstream algorithm YOLOv8s for waste detection and classification. The algorithm replaces the conventional convolution in the backbone network with GSConv to reduce redundant information and improve the model’s inference process. ODConv replaces the regular convolution in the neck network to enhance the model’s adaptability and generalization ability, thereby reducing the risk of overfitting. Additionally, the VoV-Ghost structure is introduced in the neck network to replace the original C2f, making the model more lightweight and efficient, meeting the requirements of real-time object detection in embedded devices or resource-constrained environments. Finally, the ECA attention mechanism is introduced to improve the ability of information interaction between feature channels in the model, thereby better capturing important information between images or features. Experimental results on the recyclable waste dataset demonstrate that the proposed YOLO-VG achieves a 24.6% improvement in computational efficiency, a 20.4% reduction in model size, and an mAP0.5 of 88.4%, surpassing the performance of the original YOLOv8s. These results indicate that YOLO-VG not only demonstrates excellent detection performance and stability, but also exhibits significant potential for widespread application in the field of waste sorting.},
  archive      = {J_JRTIP},
  author       = {Song, Limei and Yu, Haibo and Yang, Yangang and Tong, Yu and Ren, Siyuan and Ye, Chenchao},
  doi          = {10.1007/s11554-025-01655-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-VG: An efficient real-time recyclable waste detection network},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Study on end-to-end detection method for surface defects of automotive sheet metal parts. <em>JRTIP</em>, <em>22</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01656-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sheet metal parts account for more than 60% of the total automotive parts, and their defects can seriously affect the safety of automobile operations. Therefore, it is very important to detect defects in sheet metal parts during the production process. Due to the small size of defects in sheet metal parts, and high detection precision required, the traditional detection method cannot meet the requirements. And the factory production speed is fast, if the detection speed is low, it will cause defects to escape. Therefore, we propose an end-to-end detection method for automotive sheet metal parts surface defects. To effectively improve the detection speed, the dual regression classification strategy is proposed, which removes the NMS post-processing. Gradient information branch is added to provide rich gradient information for the model and mitigate the information loss during long convolution. Use the SPD-Conv module, optimized for small-size defects detection, to retain complete space information. Finally, the model is evaluated on the automotive sheet metal parts defect dataset. The experimental results show that the proposed method is superior to the benchmark methods in precision and speed, with mAP of 92.32% and FPS of 39.06, which achieves end-to-end detection.},
  archive      = {J_JRTIP},
  author       = {Dai, Wei and lv, Juncheng and Xiang, Rui and Jin, Sun},
  doi          = {10.1007/s11554-025-01656-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Study on end-to-end detection method for surface defects of automotive sheet metal parts},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LR-DETR: A lightweight real-time traffic sign detection model based on improved RT-DETR. <em>JRTIP</em>, <em>22</em>(2), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01659-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time detection of traffic signs is crucial for safe autonomous driving and intelligent transportation systems. The current key challenge in the field of traffic sign detection is to achieve model lightweighting and improve real-time performance while maintaining effectiveness. To address these challenges, this paper proposes a lightweight real-time traffic sign detection model called LR-DETR. LR-DETR is based on the end-to-end object detection model of RT-DETR. By redesigning the core modules in RT-DETR, LR-DETR significantly improves the lightweighting level of the model while maintaining high detection accuracy. A PCIR-Block module based on Partial Convolution and Inverted Residual Structure is proposed to more fully extract multi-scale features in the backbone network. A Context-Guided Feature Fusion Module (CGFFM) is proposed to utilize contextual information between features of different scales to enhance the effectiveness of feature representation and subsequently improve the fusion performance of multi-scale features. In addition, with the help of dilated convolution and reparameterization techniques, LR-DETR designed a DRBC3 module for feature re-extraction to further enhance the model’s ability to capture features at different scales, while effectively reducing the number of parameters and floating-point operations. The experimental results on the CCTSDB 2021 dataset show that compared with the state-of-the-art baseline models, LR-DETR performs better in increasing the value of mAP@0.5 by 0.5%, decreasing the value of FLOPs by 28.1%, decreasing the value of Params by 23.7%, and decreasing the value of Latency by 11.6%. The experimental results on the TT100K traffic sign dataset show that the precision and recall of the LR-DETR model are 87.1% and 81.6%, respectively, outperforming other baseline models. LR-DETR significantly reduces the number of parameters and floating-point operations while maintaining high detection performance, improving the detection speed of the model. This will provide a constructive contribution to achieving real-time detection of traffic signs.},
  archive      = {J_JRTIP},
  author       = {Zhang, Longzhen and Wang, Mingyang and Zhao, Xianhao and Wang, Xianjie},
  doi          = {10.1007/s11554-025-01659-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LR-DETR: A lightweight real-time traffic sign detection model based on improved RT-DETR},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time traffic light detection based on lightweight improved RT-DETR. <em>JRTIP</em>, <em>22</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01652-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing traffic light detection algorithms suffer from high computational overhead and low detection speeds, making it difficult to meet the real-time demands. Therefore, reducing computational overhead and increasing detection speed, while maintaining accuracy, becomes a critical challenge. To tackle these, this paper proposes GAD-DETR, an enhanced RT-DETR-based network. First, inspired by the approach of GhostNet to minimize computational redundancy and integrate reparameterized convolution (RepConv), the GRELAN module is developed to restructure the backbone network which significantly decreases model size and parameters while enhancing detection speed. To improve the recognition of small objects, whose features tend to be diluted as the network deepens, ADown is introduced to replace standard convolution for downsampling. Finally, a lightweight feature fusion module, DGSFM, is designed to further reduce computational costs and enhance efficiency. Experimental results indicate that GAD-DETR achieves a detection precision of 95.9% while significantly optimizing efficiency. The model size is reduced by 50.3%, with parameters and computations decreased by 50.8% and 51.2%, respectively. FPS increases from 76.7 to 117.8, demonstrating that the proposed algorithm achieves lightweight, real-time traffic light detection.},
  archive      = {J_JRTIP},
  author       = {Tang, Chaoli and Li, Yun and Wang, Lei and Li, Wenyan},
  doi          = {10.1007/s11554-025-01652-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time traffic light detection based on lightweight improved RT-DETR},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FDDC-YOLO: An efficient detection algorithm for dense small-target solder joint defects in PCB inspection. <em>JRTIP</em>, <em>22</em>(2), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01664-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays printed circuit board plays a vital role in communication, computer, electronics and other industries. Existing PCB welding defect detection algorithms have the problems of low accuracy and poor real-time performance in identifying small or irregular targets and dense solder joints. This is due to the limited receptive field of standard convolutional kernels, which hinder global feature extraction and focus on local details. Moreover, the effects of kernel count and feature extraction dimensions are often overlooked, leading to the loss of important features. Conventional upsampling methods, such as nearest-neighbor interpolation, can further degrade critical information. To address these challenges, we propose FDDC-YOLO, a novel defect detection network. First, we introduce a new full-dimensional dynamic convolution module FDDC, which integrates full-dimensional dynamic convolution with the newly designed od_ottleneck structure to enhance the feature extraction ability by using the four dimensions of the convolution kernel. Secondly, the CECA attention module in the neck improves the ability of the model to detect small defects by enhancing the local interaction between channels. Third, the Dy-Up module is used to improve image resolution and prevent the loss of detailed information during the detection process. Finally, we replace the CIoU loss with IShapeIoU to reduce the overlap of detection boxes in densely packed solder joints, improving both localization accuracy and convergence speed.The mAP of FDDC-YOLO is improved by 5.4% on the PCBSP_dataset, and a Frame Per Second (FPS) of 189. It improves by 3.8% on the public PCB Defect-Augmented dataset, which proves its good generalization ability.},
  archive      = {J_JRTIP},
  author       = {Zheng, Haoyu and Peng, Jinmin and Yu, Xinyi and Wu, Meishun and Huang, Qiufang and Chen, Liangshen},
  doi          = {10.1007/s11554-025-01664-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FDDC-YOLO: An efficient detection algorithm for dense small-target solder joint defects in PCB inspection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved YOLOv8 model for prohibited item detection with deformable convolution and dynamic head. <em>JRTIP</em>, <em>22</em>(2), 1-17. (<a href='https://doi.org/10.1007/s11554-025-01665-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray security inspection is critical for maintaining public safety and transportation security. However, traditional manual inspection methods are often ineffective due to the challenges posed by complex backgrounds and severe occlusions in X-ray images, resulting in false positives and negatives. This study proposes an enhanced object detection framework based on the YOLOv8 model to address these challenges. Key improvements include the integration of the ADown downsampling module to reduce computational complexity while enhancing detection accuracy and the incorporation of Deformable Convolutional Networks v2 (DCNv2) to improve deformable feature extraction. To strengthen feature representation, the Spatial Pyramid Pooling-Fast with ReLU and Efficient Local Attention (SPPF_RE) module is introduced to effectively integrate global and local features. Additionally, the Dynamic Head (DyHead) module is employed to enhance detection in complex backgrounds, while the Pixels-IOU (PIoU) loss function improves the detection accuracy of rotated objects. Experimental results on the OPIXray and HIXray datasets demonstrate that the proposed framework significantly outperforms the baseline model, achieving notable improvements in detection accuracy. The code can be accessed via the following link: https://github.com/Guanfj2024/x-ray-detection.git},
  archive      = {J_JRTIP},
  author       = {Guan, Fangjing and Zhang, Heng and Wang, Xiaoming},
  doi          = {10.1007/s11554-025-01665-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An improved YOLOv8 model for prohibited item detection with deformable convolution and dynamic head},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-branch evidential framework fusing hard example mining for abdominal organ segmentation. <em>JRTIP</em>, <em>22</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01648-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {U-Net variants commonly encounter limitations due to overconfidence in predictions, impeding their clinical applicability. Quantifying model uncertainty accurately is vital, but obtaining sufficient and reliable evidence remains challenging. This paper introduces the $$\textbf{D}$$ ual-Branch $$\textbf{Evi}$$ dential Framework Fusing $$\textbf{H}$$ ard $$\textbf{E}$$ xample $$\textbf{M}$$ ining for Abdominal Organ Segmentation (DEvi-HEM). It is a novel dual-branch framework integrating hard example mining (HEM) at region and pixel levels. By applying higher penalty weights to hard examples, HEM improves fine-grained prediction. The dual-branch structure enhances the model’s expressiveness by learning from both region-level and pixel-level representations. Furthermore, the introduction of dual-branch consistency learning and adversarial learning-based variational distributions captures the cognitive variability across branches. This ensures precise segmentation and reliable uncertainty estimation. DEvi-HEM improves segmentation performance, cuts computational cost, and outperforms uncertainty-based methods, with 3.292 GFLOPs on FLARE22 and 2.486 GFLOPs on Synapse.},
  archive      = {J_JRTIP},
  author       = {Yu, Xiangchun and Wu, Tianqi and Zhang, Dingwen and Liang, Miaomiao and Yu, Lingjuan and Zheng, Jian},
  doi          = {10.1007/s11554-025-01648-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dual-branch evidential framework fusing hard example mining for abdominal organ segmentation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-PDC: Algorithm for aluminum surface defect detection based on multiscale enhanced model of YOLOv7. <em>JRTIP</em>, <em>22</em>(2), 1-18. (<a href='https://doi.org/10.1007/s11554-025-01658-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address challenges multi-scale variability, category imbalance, and high background similarity in aluminum surface defect detection, this paper proposes a YOLO-PDC model. First, Partial Convolution (PConv) and Deformable ConvNetsv2 (DCNv2) replace traditional convolution in the ELAN and MaxPool modules of the YOLOv7 backbone. This configuration forms the PD-ME module, which mitigates the issue of non-uniform scale variations among different defect types in aluminum dataset. It also reduces computational redundancy and memory access, enabling efficient extraction of spatial features and improving inference speed. Next, a 3D attention module (SimAM) is incorporated into YOLOv7 detection head after two up-sample steps and within two MaxPool structures, creating the Sim-CM Attention Mechanism. This addition enhances detection accuracy without introducing additional parameters. Additionally, during training, the Focal loss function replaces CIoU loss function. Focal loss dynamically decreases the weight of easily distinguishable samples through a scaling factor, allowing the model to focus on hard-to-distinguish samples and addressing low detection accuracy caused by sample imbalance. Experimental results demonstrate that the proposed YOLO-PDC model achieves a high mean Average Precision (mAP) of 87.7% and a real-time detection speed of 114 frames per second. Compared to the original YOLOv7, mAP50 and mAP50:90 improve by 5.2% and 12.2%, respectively, while the number of parameters and computations decrease by 2.18 million and 22.2 billion, respectively. Furthermore, compared to the latest defect detection models DETR, Swin-T, and ConvNeXt-T, the mAP50 of YOLO-PDC is higher by 15.2%, 17.9%, 16.2%, respectively. YOLO-PDC also surpasses existing state-of-the-art detection methods in terms of detection accuracy.},
  archive      = {J_JRTIP},
  author       = {Li, Na and Wang, Zhiwen and Zhao, Runxing and Yang, Kaiqi and Ouyang, Rongyi},
  doi          = {10.1007/s11554-025-01658-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-PDC: Algorithm for aluminum surface defect detection based on multiscale enhanced model of YOLOv7},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-cost real-time traffic situational awareness system based on modified YOLO v8 and GWO-LSTM for edge deployment. <em>JRTIP</em>, <em>22</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01657-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a traditional traffic situational awareness system (TSAS), its “Road-side unit (RSU) + cloud-based analysis” structure is difficult to meet the demands of rapidly expanding urban areas. Relatively high costs of microwave speed detection modules and bandwidth requirements of information systems significantly increase construction costs. By computer vision (CV) and edge computing technologies, traffic situational awareness tasks can be integrated into cheaper edge devices (roadside surveillance, RSS), effectively addressing such challenges. In this study, we present a low-cost TSAS developed based on YOLO v8 and grey wolf optimizer-long short-term memory (GWO-LSTM) neural network. Proposed system can automatically perform vehicle and license plate recognition, speed measurement, and data recording within the field of view of RSSs. Additionally, it accurately predicts the future traffic conditions of monitored roads using recorded information. Experimental results demonstrate that the proposed TSAS achieves a license plate recognition accuracy of 97.7%, vehicle type recognition accuracy of 98.1%, and speed measurement error of less than 0.45 km/h, with R2 of 0.8971 for GWO-LSTM predictions. This system is sufficiently effective for traffic monitoring and situational awareness tasks but enforcement forensic applications.},
  archive      = {J_JRTIP},
  author       = {Liu, Jianwen and Gong, Ruyue and Gong, Yi and Li, Zeqin and Chen, Zhiwei},
  doi          = {10.1007/s11554-025-01657-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-cost real-time traffic situational awareness system based on modified YOLO v8 and GWO-LSTM for edge deployment},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stme-net: Spatio-temporal motion excitation network for action recognition. <em>JRTIP</em>, <em>22</em>(2), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01662-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video action recognition, as one of the fundamental tasks in video understanding, relies crucially on accurate temporal modeling. However, accurately modeling the temporal information of videos remains a challenging task. To address this problem, we design two new modules: the Spatial Motion Extraction (SME) module and the Spatio-temporal Motion Excitation (STME) module. The SME module features two branches for extracting motion and spatial features. The motion branch refines pixel differences between neighboring frames through a channel attention module, enhancing detailed motion features. These features are fused with spatial information to yield fine-grained local spatio-temporal features. The STME module, comprising the multi-motion excitation (MME), temporal excitation (TE), and spatio-temporal excitation (STE) sub-modules, efficiently captures long-range motion, temporal, and global spatio-temporal features. The MME introduces a bi-directional, multi-scale structure for effective long-range motion extraction, while the TE module employs a hierarchical pyramid with residual connectivity for fine-grained long-range temporal extraction. The STE module utilizes 3D convolutional layers for global spatio-temporal feature extraction. The seamless integration of these sub-modules within a standard ResNet network forms the Spatio-temporal Motion Excitation Network. Extensive evaluations on Something V1 and V2 and HMDB51 datasets against state-of-the-art methods demonstrate the effectiveness of our approach in achieving accurate recognition of both simple and complex video actions.},
  archive      = {J_JRTIP},
  author       = {Zhao, Qian and Su, Yanxiong and Zhang, Hui},
  doi          = {10.1007/s11554-025-01662-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Stme-net: Spatio-temporal motion excitation network for action recognition},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dilated-convolutional feature modulation network for efficient image super-resolution. <em>JRTIP</em>, <em>22</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01663-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of image super-resolution (SR), deep learning-based models have achieved remarkable success. However, these models often face compatibility issues with low-power devices due to their computational and memory constraints. To address this challenge, numerous lightweight and efficient models have been proposed. While these models typically employ smaller convolutional kernels and shallower architectures to reduce parameter counts and computational complexity, they often neglect the importance of capturing global receptive fields. In this paper, we propose a simple yet effective deep network, termed the dilated-convolutional feature modulation network (DCFMN), to tackle these limitations. Specifically, we introduce a dilated separable modulation unit (DSMU) to aggregate spatial information from diverse large receptive fields. To complement the DSMU, which processes features from a long-range perspective, we further design a local feature enhancement module (LFEM) to extract local contextual information for effective channel fusion. Additionally, by leveraging reparameterization techniques, we ensure that the model incurs no additional computational overhead during inference. Extensive experimental results demonstrate that our DCFMN achieves competitive performance among existing efficient SR methods, while maintaining a compact model size and low computational complexity.},
  archive      = {J_JRTIP},
  author       = {Wu, Lijun and Li, Shan and Chen, Zhicong},
  doi          = {10.1007/s11554-025-01663-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dilated-convolutional feature modulation network for efficient image super-resolution},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-MFX: Lightweight YOLO with improved flame detection for small targets. <em>JRTIP</em>, <em>22</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01666-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A lightweight flame detection model based on YOLOv8n enhancement is presented to address the current issue of many flame detection parameters and low flame recognition accuracy, which is unsuitable for embedded devices. Using the MobileNetV3 model as the backbone network, the model reduces the number of parameters while improving the information about flame features. It then employs a new module called IGSConv, which has a Triplet Attention mechanism that enhances the model’s perception of the flame texture and reduces the flame’s exposure to smoke interference. Afterward, it uses AIFI to enhance the ability of intra- and inter-scale feature interactions to prevent needless feature interactions and to lighten the flame detection model further. The enhanced network model gets 62.8% mAP on the dataset, 3.4% higher than the original model.},
  archive      = {J_JRTIP},
  author       = {Yao, Qingan and Xu, Han and Feng, Yuncong and Wang, Xuexiao and Zhang, Congmin},
  doi          = {10.1007/s11554-025-01666-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-MFX: Lightweight YOLO with improved flame detection for small targets},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). L-DEYO: An optimized lightweight model for intelligent coal gangue recognition. <em>JRTIP</em>, <em>22</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01667-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the escalating demand for precise coal gangue detection in industrial applications, the development of efficient and robust object detection architectures is imperative. In this study, we introduce L-DEYO, an innovative lightweight deep learning model specifically designed for intelligent coal gangue recognition in resource-constrained environments. L-DEYO integrates a hierarchical feature extraction mechanism with a progressive training paradigm, optimizing both detection accuracy and inference speed without necessitating additional annotated datasets. Comprehensive evaluations reveal that L-DEYO attains a mean average precision (mAP) of 37.6% on the COCO benchmark, achieving real-time performance at 497 frames per second (FPS) on an NVIDIA Tesla T4 GPU. Notably, the model's modular design facilitates efficient training on a single 8 GB RTX 4060 GPU, resulting in a substantial reduction in computational overhead. These findings underscore L-DEYO's efficacy and scalability, offering a viable solution for large-scale deployment in industrial coal gangue detection systems. Access to our method is available at https://github.com/srcuyan/L-DEYO.git .},
  archive      = {J_JRTIP},
  author       = {Yan, Sitong and Liu, Wei and Yang, Ziyi and Zhang, Enqi and Chang, Yasheng},
  doi          = {10.1007/s11554-025-01667-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {L-DEYO: An optimized lightweight model for intelligent coal gangue recognition},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FMS-YOLO: A lightweight safety belt detection algorithm for high-altitude workers based on attention mechanism and efficient architecture. <em>JRTIP</em>, <em>22</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01669-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In construction site accidents, the fatality rate from falls due to not wearing safety belts remains high. To address this issue, we propose a lightweight safety belt detection model, FMS-YOLO, based on a fused mixed local channel attention (MLCA) mechanism. FMS-YOLO integrates the FasterBlock module, MLCA mechanism, and Shape-IoU loss function into the YOLOv5s model to enhance detection accuracy and reduce computational load. Experimental results show that FMS-YOLO achieves an mAP@0.5 of 95.4% on the private dataset and 96.7% on the public dataset, with only 5.4 M parameters and a model size of 11.2 MB. Compared to the YOLOv5s model, FMS-YOLO improves mAP@0.5 by 3% and reduces the number of parameters by 1.2 M. This model meets the real-time detection requirements for monitoring the safety belt usage of workers operating at heights in construction site surveillance videos.},
  archive      = {J_JRTIP},
  author       = {Lu, Fangfang and Yao, Sangyu and Sun, Guxue and Zhou, Tong and Huang, Yijie},
  doi          = {10.1007/s11554-025-01669-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FMS-YOLO: A lightweight safety belt detection algorithm for high-altitude workers based on attention mechanism and efficient architecture},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSC-YOLO: A lightweight model for urban road instance segmentation. <em>JRTIP</em>, <em>22</em>(2), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01668-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time instance segmentation in urban environments remains a critical challenge for autonomous driving systems, where occluded objects, cluttered backgrounds, and dynamic scales demand both high accuracy and computational efficiency. Traditional methods often sacrifice precision for speed or vice versa, failing to address the dual demands of urban scene understanding. Motivated by the need to bridge this gap, we propose PSC-YOLO, a lightweight framework driven by two core design principles: (1) enhancing multi-scale feature learning to resolve occlusion ambiguities and (2) enabling real-time interaction without compromising segmentation quality. Simultaneously, inspired by the adaptability of the Segment Anything Model (SAM), we streamline its mask decoding via architectural, enabling efficient pixel-level reasoning crucial for real-time urban perception. Experiments on urban road datasets demonstrate that PSC-YOLO outperforms YOLOv8n-seg by 2.0% in mask average precision while operating at 91 FPS-4 $$\times$$ faster than FastSAM. This work prioritizes the intrinsic requirements of urban perception systems: balancing precision for safety-critical tasks and speed for real-time decision-making, thereby advancing deployable solutions for autonomous vehicles and smart city infrastructure.},
  archive      = {J_JRTIP},
  author       = {Gu, Xiaolin and Zhang, Guofeng},
  doi          = {10.1007/s11554-025-01668-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {PSC-YOLO: A lightweight model for urban road instance segmentation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient NPU–GPU scheduling for real-time deep learning inference on mobile devices. <em>JRTIP</em>, <em>22</em>(2), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01670-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the need for on-device artificial intelligence (AI) has increased in recent years, mobile devices tend to be equipped with multiple heterogeneous processors, including CPU, GPU, and Neural Processing Unit (NPU). While NPUs can offer low-cost and real-time AI processing capabilities for Deep Neural Network (DNN) inference, its limited resources often lead to a trade-off between performance and accuracy, potentially resulting in a non-trivial accuracy drop. To address this problem, we propose a new NPU–GPU Scheduling (NGS) framework for DNN-based video analytics. The main challenge lies in determining when and how to execute inference on the NPU/GPU to satisfy the performance objectives. To make more precise scheduling decisions, we first propose a new image complexity assessment model to replace the existing normalized edge density metric. Then, we formulate the scheduling problem with the objective of maximizing inference accuracy under the given latency constraint, and introduce an adaptive solution based on dynamic programming to determine which frames should be processed on the GPU and when to exit from inference for each of them. Extensive experiments conducted on a real mobile device show that our NGS framework substantially outperforms other solutions, and achieves a close-to-oracle performance.},
  archive      = {J_JRTIP},
  author       = {Yu, Chengwu and Wang, Meng and Chen, Shan and Wang, Wanqi and Fang, Weiwei and Chen, Yanming and N.Xiong, Neal},
  doi          = {10.1007/s11554-025-01670-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient NPU–GPU scheduling for real-time deep learning inference on mobile devices},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lw4s: A lightweight semantic sigmentation model of urban street scene. <em>JRTIP</em>, <em>22</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01671-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation in urban street scenes is essential for autonomous driving, enabling accurate perception of complex road environments. DeepLabV3+ is a widely used segmentation framework, but its high computational complexity and limited utilization of shallow features hinder its efficiency in real-time applications. In addition, class imbalance in urban street scenes leads to suboptimal segmentation performance of the model on small target categories. To address these challenges, this paper proposes a new LightWeight Semantic Sigmentation model for urban Street Scene(LW4S). LW4S uses GhostConv instead of the standard convolution in DeepLabV3+, which reduces the model’s parameters and computational costs while maintaining feature richness. LW4S constructed the LMCF module, which integrates GhostConv with an attention mechanism to enhance the fusion of shallow and deep features, and improve the model’s recognition of fine-grained objects with minimal overhead. LW4S proposes a weighted combination loss function that combines Focal Loss with Cross-Entropy Loss (CE Loss) to address the class imbalance problem in urban street view semantic segmentation tasks. The experimental results show that compared with the baseline models, LW4S achieves a better balance between segmentation accuracy and model computational complexity, demonstrating its effectiveness in urban street scene segmentation.},
  archive      = {J_JRTIP},
  author       = {Zhao, Xianhao and Wang, Mingyang and Xin, Chaoqun and Zhang, Longzhen and He, Huixin},
  doi          = {10.1007/s11554-025-01671-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lw4s: A lightweight semantic sigmentation model of urban street scene},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hcl-yolo: A lightweight and efficient underwater object detection algorithm. <em>JRTIP</em>, <em>22</em>(2), 1-17. (<a href='https://doi.org/10.1007/s11554-025-01674-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the ocean economy and the continuous in-depth development of ocean resources, object detection technology is becoming increasingly important in fields such as ocean scientific research, ecological protection, fisheries management, and ocean engineering. However, the complexity of the underwater background environment, overlap between organisms, small objects have difficulty being detected, and practical application issues associated with large models all increase the difficulty of detection. An HCL-YOLO detection algorithm based on an improved YOLOv10n is proposed to solve the above problem. First, the localization enhancement module (LE) was proposed to solve the problem of covered objects. By extracting positional information in the space and channel, the model’s focus on global information is improved, thereby enhancing the model’s reasoning ability for covered objects. Second, the cross-scale feature fusion (CSFF) neck structure is proposed to enhance the model’s feature interaction ability and multi-scale object feature fusion ability, thereby improving the detection accuracy of small objects. Finally, HetConv was introduced and the C2f module was redesigned so that the backbone network takes the form of a heterogeneous kernel. This ensures the backbone’s feature extraction capabilities and reducing the model parameters and computational requirements. The proposed model was compared with multiple sets of experiments on the URPC, DUO, and ROUD datasets. The HCL-YOLO model has 40.7% fewer parameters and 27.4% fewer floating-point calculations than YOLOv10n, with a 24 increase in FPS. The $$\hbox {mAP}_{0.5}$$ reached 83.9%, 86.6%, and 84.3% respectively. The experimental results show that the HCL-YOLO model performs excellently in underwater detection tasks.},
  archive      = {J_JRTIP},
  author       = {Liang, Xiuman and Zhang, Teng and Yu, Haifeng and Liu, Zhendong},
  doi          = {10.1007/s11554-025-01674-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hcl-yolo: A lightweight and efficient underwater object detection algorithm},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rcf-yolo: An underwater object detection algorithm based on improved YOLOv10n. <em>JRTIP</em>, <em>22</em>(2), 1-18. (<a href='https://doi.org/10.1007/s11554-025-01677-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a key technology for marine exploration. The detection effect is not ideal because of factors such as the biodiversity and overlapping shadows in the underwater environment. Therefore, a new underwater object detection algorithm called RCF-YOLO is proposed. First, a coordinate enhancement (CE) attention module is designed. Depth-separable convolutions are used to extract the location information of the channel and combine it with spatial information to improve the model’s ability to infer global features. Second, we have redesigned the neck with the BiFPN concept, which enhances feature interaction capabilities and optimizes the inference structure. The convolutional operation in the neck path is improved to enhance cross-scale connections, effectively integrating shallow and deep features, achieving a good balance between efficiency and accuracy. Finally, the receptive field convolution (RFAConv) is introduced to solve the parameter sharing problem in complex convolution processing, making the model more flexible in adjusting the convolution kernel weights and more effectively capturing the information in the image. The proposed model was compared with several sets of experiments on the URPC, DUO, and ROUD datasets. With a decrease in both the number of parameters and the complexity of the calculation, the accuracy reached 85.3%, 87.9%, and 84.9%. The experimental results show that the RCF-YOLO model has excellent performance in the underwater detection task.},
  archive      = {J_JRTIP},
  author       = {Liang, Xiuman and Zhang, Teng and Yu, Haifeng and Liu, Zhendong},
  doi          = {10.1007/s11554-025-01677-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rcf-yolo: An underwater object detection algorithm based on improved YOLOv10n},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high-speed hardware accelerator for lightweight dehazing neural network based on SFA-net. <em>JRTIP</em>, <em>22</em>(2), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01672-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In adverse weather conditions such as haze, a large number of suspended particles are present in the atmosphere, which may interfere with the real-time and high-quality acquisition of visual images in applications such as remote sensing, monitoring, and automotive systems. Existing neural network-based algorithms require high computational power and storage, resulting in poor real-time performance. To address these issues, this paper proposes a lightweight spanning fusion attention convolutional neural network (SFA-Net), which adopts an autoencoder architecture and utilizes feature attention fusion to manipulate the interaction between Channel Attention (CA) and Pixel Attention (PA) information, and connects the feature information of the front and back convolutional layers through an adaptive mixup fusion module to train the defogging model and reconstruct clear images. In addition, a configurable and efficient hardware acceleration circuit based on the network is designed, which can be applied to different indoor and outdoor scenarios. Experimental results show that the proposed method has a good dehazing ability with only 720 parameters, and is better than the indicators of other hardware implementation algorithms in the comparison of objective data sets. In terms of hardware, compared with the brightness improved by light-dehazeNet (BILD-Net), the resource is reduced by 48%, and the processing speed can reach a maximum frame rate of 120 frames per second, which meets the requirements of real-time processing.},
  archive      = {J_JRTIP},
  author       = {Chen, Zhe and Du, Gaoming and Li, Zhenmin and Wang, Xiaolei and Yin, Yongsheng},
  doi          = {10.1007/s11554-025-01672-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A high-speed hardware accelerator for lightweight dehazing neural network based on SFA-net},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object detection based on logistic objects in context (LOCO) dataset: An improved dataset split and performance on NVIDIA jetson nano. <em>JRTIP</em>, <em>22</em>(2), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01673-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Logistics Objects in Context (LOCO) dataset is the first and only public dataset that integrates multiple logistic objects in a real logistics context. It is dedicated to object detection applications. However, the dataset is small and has unbalanced classes annotations problem. In this study, we present significant advancements to use YOLO models on the LOCO dataset, emphasizing improved dataset management and optimized inference performance. We introduced a novel balanced dataset splitting, ensuring equitable annotation distribution, which is detailed through a comprehensive dataset analysis. Our results demonstrate enhanced accuracy for YOLOv4 and YOLOv8, while we present additional preliminary results for YOLOv9 and YOLOv10, enabled by the identified experimental parameters. Notably, we achieved state-of-the-art inference times for both YOLOv4-Tiny and YOLOv8n when deployed on the NVIDIA Jetson Nano. Additionally, we provide preliminary findings on accuracy and inference times for YOLOv9 and YOLOv10, revealing important performance overlaps among various small-scale YOLO models on the same platform. Finally, we propose the optimal model configuration for achieving superior real-time performance on the NVIDIA Jetson Nano using the LOCO dataset, contributing valuable insights for future developments in real-time logistics object detection.},
  archive      = {J_JRTIP},
  author       = {Tadjine, Chaouki and Ouafi, Abdelkrim and Taleb-Ahmed, Abdelmalik and El Hillali, Yassin and Rivenq, Atika},
  doi          = {10.1007/s11554-025-01673-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Object detection based on logistic objects in context (LOCO) dataset: An improved dataset split and performance on NVIDIA jetson nano},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deferred demosaicking: Efficient first-person view drone video encoding. <em>JRTIP</em>, <em>22</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01675-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pursuit of effective real-time video transmission for First-Person View (FPV) drone systems, optimizing the encoding process is paramount. Traditional encoding methods, reliant on pre-encoding demosaicking, often fall short in balancing the trade-off between video quality and latency, essential for seamless real-time feedback. This work proposes a novel approach by deferring the demosaicking process to the decoder side, thereby encoding the rearranged Bayer pattern (RGGB) data directly. This deferment significantly reduces the input data size, to the tune of a threefold reduction, thereby achieving a more expeditious encoding process. The tailored encoder and decoder architecture ensures the accurate reconstruction of the full-color image on the decoder side. Through a comprehensive evaluation, leveraging a specialized video quality assessment framework designed for FPV drone footage, our findings illuminate the substantial benefits of our proposed method. Specifically, it achieves faster encoding times and reduced computational overhead, pivotal for low-latency applications. Furthermore, this study opens avenues for integrating advanced encoding techniques into commercial FPV drone systems, potentially enriching user experiences across various applications. Our research not only addresses a critical gap in real-time video transmission but also sets the stage for future exploration into optimizing encoding methodologies for the next generation of FPV drone technologies.},
  archive      = {J_JRTIP},
  author       = {Benjak, Jakov and Hofman, Daniel and Mlinarić, Hrvoje},
  doi          = {10.1007/s11554-025-01675-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deferred demosaicking: Efficient first-person view drone video encoding},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EggYOLOPlant: Optimized YOLOv8 for real-time eggplant seedling center detection. <em>JRTIP</em>, <em>22</em>(2), 1-16. (<a href='https://doi.org/10.1007/s11554-025-01676-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current level of mechanization in the eggplant planting industry is low, with most planting processes still carried out manually. In the context of using agricultural machinery for automated operations, such as automatic pesticide spraying, the real-time detection of eggplant seeding centers is a crucial step. An EggYOLOPlant real-time detection model is designed to address the challenges of detecting the seedling centers by the diverse appearance features of the eggplant in complex planting environments, where there are different lighting conditions on sunny days, cloudy days, and evenings, as well as the presence of many weeds. We constructed a small dataset using real-world images of eggplant seedlings captured in their natural environment, which was used to train and evaluate EggYOLOPlant as well as other object detection models. The EggYOLOPlant model is based on an enhanced YOLOv8 architectural framework. The original backbone was replaced with an Mblock-based architecture, which originates from MobileNetV3’s feature extraction module. Additionally, we replaced the C2f model in the Neck layer with the C2f-faster model. The Grad-CAM visualization of the backbone layer outputs revealed that EggYOLOPlant’s feature activations were more focused on target regions than YOLOv8, demonstrating superior background suppression. The experimental results demonstrate that EggYOLOPlant achieves 94.7% precision (P) and 95.9% mAP50 on the test set, representing a + 4.0% (P) and + 3.8% (mAP50) improvement compared to the baseline YOLOv8 model (90.7% P, 92.1% mAP50). Additionally, the number of parameters has been reduced from 2.7 to 1.9 M (a 30% decrease), while the FPS has increased from 267 to 294, achieving a 10.1% improvement in speed. In comparison to Faster-RCNN, YOLOv5s, RT-DETR, and YOLO11s, the mAP50 of this model demonstrates an improvement of 3.2%, 1.8%, 3.5% and 1.5%, respectively. Moreover, the detection speed exhibits a notable enhancement, reaching approximately 24 times, 1.59 times, 1.95 times, and 1.61 times that of the aforementioned models, respectively. Furthermore, we deployed EggYOLOPlant on the Honor X10 smartphone using the NCNN framework, achieving an average runtime speed of 25.2 FPS, thereby preliminarily validating the model’s real-time performance on mobile devices.},
  archive      = {J_JRTIP},
  author       = {Song, Huabin and Zeng, Yingming and Wen, Tianyi and Li, Xiaomin and Liu, Yongxin},
  doi          = {10.1007/s11554-025-01676-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EggYOLOPlant: Optimized YOLOv8 for real-time eggplant seedling center detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A real-time text recognition based on involution operators and graph convolutional networks. <em>JRTIP</em>, <em>22</em>(2), 1-13. (<a href='https://doi.org/10.1007/s11554-025-01679-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ambiguity and stickiness of text in natural scenarios make text recognition models incorrectly recognize certain characters as others. To address the above problems and the computational scale of the model, we propose a real-time effective text recognition method (IGTR). The method introduces the involution operator in extracting feature sequences within a larger sensory field using a smaller number of parameters. Due to the dependency of the characters before and after the text line, after obtaining the text feature sequence, IGTR uses the gate recurrent unit (GRU) algorithm to obtain richer contextual semantic information and pairs it with the graph convolutional neural network (GCN) to further enhance the semantics of the text features on the basis of the structure of the graph. Finally, the CTC algorithm is used to optimize the computation process of the loss function, and the dynamic planning of the search path is adopted to further reduce the computation amount of the text recognition model. Comparative experiments on ICDAR 2013, ICDAR 2015 and SVT datasets show that the method can effectively recognize multi-scale fuzzy text and sticky text within 75ms. With no dictionary supervision, the $$\text {P}_{\text {recog}}$$ reaches 94.2%, 89.7%, and 93.8%, respectively, which is competitive with other compared methods.},
  archive      = {J_JRTIP},
  author       = {Tong, Guoxiang and Li, Yang and Dong, Ming and Peng, Dunlu},
  doi          = {10.1007/s11554-025-01679-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time text recognition based on involution operators and graph convolutional networks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight YOLOv8-based real-time detection of rice blast for edge computing devices. <em>JRTIP</em>, <em>22</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01681-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In rice cultivation, manually identifying rice blast disease is time-consuming and labor-intensive. Computer vision technology enables real-time and accurate disease detection in complex rice field environments. However, conventional object detection models face challenges in direct deployment on embedded edge devices due to their high computational demands. To address this issue, this paper proposes a lightweight rice blast detection model based on YOLOv8n. First, a Star Operation is introduced to optimize the C2f module in YOLOv8, enhancing the model’s ability to extract multi-scale features of rice blast disease. Second, a lightweight High-level Screening Feature Pyramid Network (HSFPN) is adopted to improve feature fusion efficiency, and a detection head incorporating shared parameters and Detail Enhancement Convolution (DEConv) is designed to reduce computational costs while improving detection accuracy. Finally, the LAMP pruning algorithm is applied to remove redundant parameters, further lightweighting the model. Experimental results demonstrate that, compared to YOLOv8n (3.01M parameters, 8.1 GFLOPs), the proposed model reduces parameters by 88% (from 3.01M to 0.36M) and decreases computational cost by 59.2% (from 8.1 GFLOPs to 3.3 GFLOPs), while maintaining an mAP@50 of 78.6% (an improvement of 0.2%). Furthermore, the model achieves a real-time performance of 22.5 FPS on the Jetson Nano, providing practical value for rice blast disease detection and rice crop protection.},
  archive      = {J_JRTIP},
  author       = {Yang, Song and Ma, Xiaoyun and Liu, Dayang and Zhu, Liangkuan},
  doi          = {10.1007/s11554-025-01681-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Lightweight YOLOv8-based real-time detection of rice blast for edge computing devices},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DHF-UNet: Enhancing blind road segmentation in complex environments via multi-scale feature extraction and fusion. <em>JRTIP</em>, <em>22</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01680-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address challenges such as insufficient feature expression, detail loss, and inadequate multi-scale feature fusion in blind road segmentation under complex environments, we propose DHF-UNet, a segmentation model based on multi-scale feature enhancement. This model introduces the Dense-MultiScale Attention Module (DMAM), which combines multi-scale convolution and attention mechanisms to improve feature extraction, capturing both global context and local details of blind road targets. This significantly enhances the model’s adaptability to varying-scale features. Additionally, we designed a Self-Hierarchical Feature Fusion Block (Self-HFF) and incorporated a learnable activation function derived from Kolmogorov–Arnold Networks (KAN) to strengthen key feature recognition in blind lane areas. To ensure efficiency, we employed Depthwise Separable Convolution, which reduces computational complexity while maintaining expressive capability. Experimental results on a self-constructed blind road dataset showed an average intersection over union (mIoU) of 92.97% and a detection speed of 38.26 FPS, demonstrating strong real-time performance. The model also achieved competitive results on public datasets, confirming its generalization and robustness.},
  archive      = {J_JRTIP},
  author       = {Yang, Xuecun and Dong, Zhonghua and Wang, Yixiang and Li, Jiayu and Zhang, Qingyun and Gao, Jingkun},
  doi          = {10.1007/s11554-025-01680-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DHF-UNet: Enhancing blind road segmentation in complex environments via multi-scale feature extraction and fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CFP-PSPNet: A lightweight unmanned vessel water segmentation algorithm. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-024-01603-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate water segmentation is the first prerequisite for unmanned vessels to navigate safely and perform other operations. Aiming at the problems of low utilization rate of unmanned boat water image features and low accuracy of contour edge segmentation in complex inland river scenarios, this paper proposes a lightweight water segmentation algorithm: channel feature pyramid-pyramid scene parsing network (CFP-PSPNet), which realizes efficient and accurate segmentation of water area in complex scenarios. First, a cross transformation-channel feature pyramid (CT-CFP) is proposed, which improves the utilization of the original feature information by cross-fertilizing the feature information between different layers and realizes the improvement of the water segmentation accuracy; Secondly, a parallel semantic segmentation network CFP-PSPNet is designed, which extracts the image information by pyramid pooling module (PPM) and CT-CFP dual pyramid, which solves the problem of loss of detail information and edge information, so as to achieve the purpose of improving the accuracy; finally, Mobilenetv2 after the introduction of encoder-context-attention (ECA) is used as a feature extraction network, which reduces the number of parameters and computation of the network without affecting the segmentation accuracy and realizes the lightweight design of the network. Experiments are conducted on the open-source dataset USVInland, and the experimental results show that our CFP-PSPNet algorithm has a significant reduction in the number of parameters, an increase in detection speed by 81FPS, and mean intersection over union (MIoU) and accuracy rates of 97.71% and 98.75%, respectively, which are 1.41% and 0.74% higher than that of the original network. It is superior to other classical semantic segmentation algorithms.},
  archive      = {J_JRTIP},
  author       = {Yang, Xuecun and Song, Yijing and He, Lintao and Xue, Hang and Dong, Zhonghua and Zhang, Qingyun},
  doi          = {10.1007/s11554-024-01603-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {CFP-PSPNet: A lightweight unmanned vessel water segmentation algorithm},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECDet: Efficient oriented object detection on the aerial image with cross-layer attention. <em>JRTIP</em>, <em>22</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11554-024-01617-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in horizontal object detection models, oriented object detection models have also made significant strides. Yet existing rotated object detectors often struggle to maintain high accuracy while processing high-resolution remote sensing images in real-time. To address these challenges, we propose a new lightweight model specifically for oriented object detection, named the efficient cross-layer attention detector (ECDet). ECDet integrates several efficient modules, including an efficient reparameterized Transformer-like backbone (ERepViT) to reduce computational costs, and the efficient cross-layer fusion neck (CLF-Neck), a lightweight alternative to traditional pyramid networks for feature fusion with attention mechanism. Additionally, we introduce the lightweight task interaction decoupled (LTID) head, which enhances task-specific performance by providing more detailed, task-aligned information for classification and regression with minimal computational cost. Furthermore, an ensemble loss combined with the phase shifting coder (PSC) mitigates the angle discontinuity issue in regression-based methods. Evaluations on the DOTAv1 and HRSC datasets show that ECDet runs 32% faster than RTMDet-S with higher accuracy, demonstrating its strong potential for practical application. The source code will be release at https://github.com/tianlianghai/ECDet .},
  archive      = {J_JRTIP},
  author       = {Lyu, Xueqiang and Tian, Lianghai and Teng, Shangzhi},
  doi          = {10.1007/s11554-024-01617-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ECDet: Efficient oriented object detection on the aerial image with cross-layer attention},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixel-level attention based data compression for spike camera. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-024-01618-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of autonomous driving applications, there is an increasing demand for high-speed vision sensors like spike cameras. However, the data transmission and storage requirements are becoming increasingly burdensome due to the high temporal resolution, such as 40,000 Hz. To resolve this problem, we propose a pixel-level attention-based data compression method for the spike camera. First the input spike data are partitioned into two block types by pixel-level attention-based method. Then, the two blocks are condensed using different methods and side information marking is transmitted for spike decoding. Finally, the condensed spike and side information marking are compressed into a binary stream for storage. The experimental results show that our method achieves higher compression efficiency than conventional methods. The decompressed spike can also reconstruct the image with better visual quality.},
  archive      = {J_JRTIP},
  author       = {Li, Yansong and Huang, Xiaofeng and Li, Shangqia and Cui, Yan and Zhou, Yang and Song, Jian and Yin, Haibing},
  doi          = {10.1007/s11554-024-01618-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Pixel-level attention based data compression for spike camera},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rocknet: Lightweight network for real-time segmentation of martian rocks. <em>JRTIP</em>, <em>22</em>(1), 1-11. (<a href='https://doi.org/10.1007/s11554-024-01619-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rock segmentation on the Martian is particularly critical for rover navigation, obstacle avoidance, and scientific target detection. We propose a lightweight network for real-time semantic segmentation of Martian rocks (RockNet). First, we propose the cross-dimension channel attention (CDCA) model to replace traditional downsample and upsample operation, which gives more weight to the channels with more useful information by adjusting the weight of each channel. Second, we modify the short-term dense concatenate model, we adopt dilated convolution to learn the feature with a larger receptive field, and through the skip connection structure, the degradation of the network can be reduced. Finally, we propose a feature fusion module (FFM) to fully fuse different levels of features. With only 0.86M parameters, our model gets 82.37% mIoU and 105.7 FPS running speed on the dataset of TWMARS.},
  archive      = {J_JRTIP},
  author       = {Wei, Pengfei and Sun, Zezhou and Tian, He},
  doi          = {10.1007/s11554-024-01619-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rocknet: Lightweight network for real-time segmentation of martian rocks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time underwater target detection based on improved YOLOv7. <em>JRTIP</em>, <em>22</em>(1), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01621-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater target detection is crucial for ocean exploration, but existing methods struggle to achieve satisfactory results due to the complexity of the underwater environment. To enhance the accuracy and real-time performance of underwater detection models, we propose an improved YOLOv7 model. We introduce a multi-granularity feature attention method based on the Efficient Channel Attention (ECA) to help the model better adapt to the diverse conditions in the underwater environment, reducing focus on redundant information. Utilizing coordinate convolution provides the network with spatial awareness of input image coordinates, enabling more effective localization of target objects and reducing interference from similar background elements. To accommodate the features of small and dense underwater targets, we use normalized Wasserstein distance to measure the similarity of bounding boxes. On the Underwater Robot Picking Contest 2019 (URPC 2019) dataset, the mean Average Precision (mAP) of our improved network has reached 86.19%, which represents a 1.57% increase compared to the original YOLOv7 network. Additionally, the frames per second (fps) has achieved 124, surpassing the performance of the original network. This improvement is significantly superior to conventional target detection models, providing a faster and more accurate advantage for underwater target detection tasks in complex underwater environments.},
  archive      = {J_JRTIP},
  author       = {Wu, Qingqi and Cen, Lihui and Kan, Shichao and Zhai, Yongping and Chen, Xiaofang and Zhang, Hong},
  doi          = {10.1007/s11554-025-01621-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time underwater target detection based on improved YOLOv7},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LKR-DETR: Small object detection in remote sensing images based on multi-large kernel convolution. <em>JRTIP</em>, <em>22</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01622-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection in remote sensing imagery remains a challenging problem in computer vision. To address the inherent limitations of aerial imagery, such as densely packed objects with insufficient detail and occlusions caused by complex backgrounds, this study proposes LKR-DETR, an innovative object detection in remote sensing imagery framework based on RT-DETR. We propose a lightweight and efficient feature extraction module with large kernel convolution, which expands the receptive field while reducing parameters and computational costs. Furthermore, we present a novel multi-scale feature fusion structure based on wavelet transform convolution that effectively utilizes low-frequency information from low-level feature maps. Additionally, we introduce a lightweight image restoration module utilizing large kernel convolutions, which effectively recovers previously undetected details of small objects. To improve bounding box regression accuracy, the original GIoU loss is replaced with a Focaler-DIoU loss function. Compared to the benchmark model RT-DETR, the LKR-DETR model achieves a 2.5% improvement in $$mAP_{0.5}$$ and a 2.0% improvement in $$mAP_{0.5:0.95}$$ on the VisDrone2019-DET dataset, a 1.7% and 4.4% improvement on the DOTAv1.5 dataset, and a 3.4% and 2.4% improvement on the HIT-UAV dataset, while also reducing the parameter count and model size. Relative to other cutting-edge models, LKR-DETR attains superior detection accuracy while maintaining relatively low computational complexity, establishing it as an efficient solution for small object detection in remote sensing imagery.},
  archive      = {J_JRTIP},
  author       = {Dong, Ying and Xu, Fucheng and Guo, Jiahao},
  doi          = {10.1007/s11554-025-01622-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LKR-DETR: Small object detection in remote sensing images based on multi-large kernel convolution},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DHNet: A surface defect detection model utilizing multi-scale convolutional kernels. <em>JRTIP</em>, <em>22</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11554-025-01623-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting micro-defects in densely populated printed circuit boards (PCBs) with complex backgrounds is a critical challenge. To address the problem, the DHNet, a small object detection network based on YOLOv8 employing multi-scale convolutional kernels is proposed for feature extraction and fusion. The lightweight VOVGSHet module is designed for feature fusion and a pyramid structure to efficiently leverage feature map relationships while minimizing model complexity and parameters. Otherwise, to optimize the original extraction structure and enhance multi-scale defect detection, convolutional kernels of varying sizes process the same input channels. Additionally, the incorporation of the Wise-IoU loss function improves small defect detection accuracy and efficiency. Moreover, extensive experiments on a custom PCB dataset demonstrate DHNet's effectiveness, achieving an outstanding mean Average Precision (mAP) of 84.5%, surpassing the original YOLOv8 network by 4.0%, with parameters only of 2.85 M. Model demonstrates a latency of 3.6 ms on NVIDIA 4090. However, YOLOv8n has a latency of 4.4 ms. Validation on public DeepPCB and NEU datasets further confirms DHNet's superiority, which can reach 99.1% and 79.9% mAP, respectively. Finally, successful deployment on the NVIDIA Jetson Nano platform validates DHNet's suitability for real-time defect detection in industrial applications.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yingying and Wang, Shuo and Wang, Jinhai and Zhao, Yu and Chen, Zhiwei},
  doi          = {10.1007/s11554-025-01623-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DHNet: A surface defect detection model utilizing multi-scale convolutional kernels},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSBF-YOLO: A lightweight model for tomato ripeness detection in natural environments. <em>JRTIP</em>, <em>22</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11554-025-01624-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate tomato ripeness detection is essential for optimizing harvest timing and maximizing yield. Deep learning-based object detection has proven effective in this task. However, many existing algorithms have numerous parameters and substantial computational demands, making them unsuitable for agricultural environments with limited computational resources. Additionally, accurate detection becomes challenging with overlapping fruits, leaf occlusion, or complex backgrounds. To address these issues, this paper proposes a lightweight detection model, GSBF-YOLO. This model designs the GSim module to reduce parameters while maintaining detection accuracy. The C3Ghost module further reduces parameter count by replacing the traditional C3 module. The PANet multi-scale feature fusion network in the neck is replaced with the Bi-directional Feature Pyramid Network (BiFPN), which adjusts weights based on the importance of input features. Lastly, the fine-tuned FocalEIOU Loss function is used to calculate the bounding box regression loss, enhancing the model’s ability to adjust the weights of high-quality anchor boxes for better detection of targets in occlusion scenarios. Experimental results show that GSBF-YOLO reduces parameters and computational load by 42% and 45%, respectively, while mean Average Precision (mAP) increases by 1.9% and 1.6% on two datasets. The model achieves 110 Frames Per Second (FPS), meeting real-time detection requirements, and has fewer parameters and higher accuracy compared to models like YOLOv8. The research indicates that the proposed lightweight model can effectively detect tomato ripeness in natural environments.},
  archive      = {J_JRTIP},
  author       = {Hao, Fengqi and Zhang, Zuyao and Ma, Dexin and Kong, Hoiio},
  doi          = {10.1007/s11554-025-01624-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GSBF-YOLO: A lightweight model for tomato ripeness detection in natural environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A real-time and general method for converting offline skeleton-based action recognition to online ones. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-025-01625-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many motion sensors can directly acquire human skeletal data, and then extract features on the skeletal data through GCNs (graph convolutional networks) to perform action recognition. However, almost all state-of-the-art (SOTA) methods are offline methods, cannot perform online inference, wasting computational resources. The existing approach to transforming offline action recognition into online action recognition is to reconstruct the network structure of the offline method. This requires developers to have a deep understanding of the algorithm’s network structure and make extensive modifications, which results in slow development. To address the above issue, this paper points out that to convert offline methods to online ones, the key is removing outdated frame features and fusing new frame features. Furthermore, we propose a general and simple model called Encode One Frame (EOF), which achieves feature removal and fusion by a correlation matrix and the guidance of a teacher model. The EOF model has online inference capabilities, requiring only the input of the new frame of the current sample and the features encoded from the old sample. Based on the EOF model, we further propose the You Only Encode One Frame (YOEOF) algorithm to correct the cumulative errors generated during EOF model online inference. By coupling these proposals, YOEOF achieves online inference and outperforms some SOTA methods on public datasets. The deployment at the application level indicates that our method meets the requirements of high accuracy and real-time performance for dangerous action recognition.},
  archive      = {J_JRTIP},
  author       = {Dong, Liheng and He, Guiqing and Zhang, Zhaoxiang and Xu, Yuelei and Hui, Tian and Xu, Xin and Tao, Chengyang and Li, Huafeng},
  doi          = {10.1007/s11554-025-01625-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time and general method for converting offline skeleton-based action recognition to online ones},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid architecture for video frame prediction: Combining convolutional LSTM and 3D CNN. <em>JRTIP</em>, <em>22</em>(1), 1-18. (<a href='https://doi.org/10.1007/s11554-025-01626-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame prediction represents a fundamental challenge in computer vision, necessitating precise modeling of both spatial and temporal dynamics within video sequences. This computational task holds substantial implications across diverse domains, including video compression optimization, robust object tracking systems, and advanced motion forecasting applications. In this investigation, we present a novel hybrid architecture that synthesizes the complementary strengths of Convolutional Long Short-Term Memory (ConvLSTM) networks and three-dimensional Convolutional Neural Networks (3D CNN) for enhanced frame prediction capabilities. Our methodological framework incorporates a ConvLSTM component that fundamentally augments the traditional LSTM architecture through the integration of convolutional operations, thereby facilitating sophisticated modeling of sequential dependencies. Concurrently, the 3D CNN component employs volumetric convolutional layers to extract rich spatio-temporal features from the input sequences. Rigorous empirical evaluation demonstrates the superior performance of the ConvLSTM architecture, which consistently yields reduced validation errors and elevated coefficients of determination. Specifically, the ConvLSTM model achieves a validation Mean Squared Error (MSE) of 0.0237 and an $${\textrm{R}}^{2}$$ value of 0.6951, substantially outperforming the 3D CNN model, which exhibits a validation MSE of 0.0471 and an $${\textrm{R}}^{2}$$ value of 0.3939. These empirical findings substantiate the efficacy of the ConvLSTM architecture in addressing the inherent complexities of video frame prediction, while simultaneously illuminating its considerable potential for deployment across various video processing and predictive modeling applications. The results provide compelling evidence for the advantages of incorporating convolutional operations within recurrent architectures for sequential visual data processing.},
  archive      = {J_JRTIP},
  author       = {Aravinda, C. V. and Al-Shehari, Taher and Alsadhan, Nasser A. and Shetty, Shashank and Padmajadevi, G. and Reddy, K. R. Udaya Kumar},
  doi          = {10.1007/s11554-025-01626-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel hybrid architecture for video frame prediction: Combining convolutional LSTM and 3D CNN},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OMAL-YOLOv8: Real-time detection algorithm for insulator defects based on optimized feature fusion. <em>JRTIP</em>, <em>22</em>(1), 1-9. (<a href='https://doi.org/10.1007/s11554-025-01629-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of misdetection and missed detection caused by the diversity of insulator types and the complexity of defect textures in power transmission lines, and to meet the demands of collaborative inspection, we propose a real-time detection algorithm for insulator defects based on YOLOv8. First, considering the characteristics of the dataset sample sizes, we designed a lightweight OMAL-Neck structure that optimizes feature fusion, enhancing the utilization of feature information and improving detection performance for medium and large targets. Second, to address the issue of large parameter and computation requirements in the YOLOv8 detection head, we designed a lightweight and efficient detection head. This redesigned detection head incorporates PConv, further accelerating model inference speed. Lastly, to counteract the decline in detection accuracy due to model lightweighting, we integrated the C2f module with DySnakeConv, enhancing the feature extraction capability for tubular structures and complex textures, thereby preventing information loss. Experimental results demonstrate that compared to the baseline YOLOv8s, the proposed model increases FPS from 44 to 78 frames/s, reduces the number of parameters and computational complexity by 27 and 38%, respectively, and improves the mAP by 1.7%. The improved model offers significant advantages in both detection accuracy and real-time performance, enabling rapid and precise identification of insulators and their defects, thereby improving the efficiency of power line inspections.},
  archive      = {J_JRTIP},
  author       = {Ru, Hongfang and Zhang, Wenhao and Wang, Guoxin and Ding, Luyang},
  doi          = {10.1007/s11554-025-01629-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {OMAL-YOLOv8: Real-time detection algorithm for insulator defects based on optimized feature fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time recognition method for PCB chip targets based on YOLO-GSG. <em>JRTIP</em>, <em>22</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11554-024-01616-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industrial settings, the identification of chips on PCB boards is crucial for quality control and efficiency. However, achieving both speed and accuracy in chip detection remains a significant challenge. To address this issue, we propose the YOLO-GSG deep network model, which incorporates several novel modifications to the standard YOLO architecture. The key innovations include the replacement of the ELAN module with the C3Ghostnet module in the backbone network, improving feature extraction and reducing model complexity, and the introduction of the SE attention mechanism to minimize feature loss. Additionally, the GSnet module and GSConv convolution are integrated into the neck network to enhance feature fusion. The experimental results indicate that the YOLO-GSG algorithm achieves a mAP of 99.014%, with precision and recall improvements of 1.080% and 1.446% over the baseline YOLOv7 model. Additionally, the improved model has 24.478M parameters, 61.4 GFLOPs, and a model size of 50.8 MB. The model achieves an FPS of 231.55, representing a 12.8% speedup over the baseline. These results indicate that the YOLO-GSG model offers a superior balance of speed and accuracy for chip identification in industrial applications. This study contributes to the advancement of deep learning applications in industrial environments, providing a more efficient and effective tool for quality control in PCB manufacturing.},
  archive      = {J_JRTIP},
  author       = {Yue, Zeang and Li, Xun and Zhou, Huilong and Wang, Gaopin and Wang, Wenjie},
  doi          = {10.1007/s11554-024-01616-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time recognition method for PCB chip targets based on YOLO-GSG},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FANet: Focus-aware lightweight light field salient object detection network. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-024-01581-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection and segmentation of salient objects in light field scenes pose significant challenges due to redundant and noisy information. We introduce FANet, a novel light field salient object detection (LF SOD) network. This network leverages focus-aware techniques to enhance both performance and efficiency. FANet employs a focus-aware module (FAM) that merges low-level local information with high-level semantic information. This enables precise capture of contours and boundaries. It also incorporates a lightweight cross-modal analysis module (CMAM), which utilizes a grouped multi-head self-attention mechanism for efficient hierarchical cross-modal analysis. This design significantly improves the model’s ability to distinguish between foreground and background while optimizing computational resources. FANet demonstrates outstanding performance on the HFUT-Lytro dataset, achieving state-of-the-art results. Notably, it operates at 97.4 frames per second on a single NVIDIA RTX 4090 GPU, with only 5.3 million parameters and minimal GPU memory usage. This emphasizes its suitability for real-time applications and showcases an optimal balance between speed and detection accuracy.},
  archive      = {J_JRTIP},
  author       = {Fu, Jiamin and Chen, Zhihong and Zhang, Haiwei and Gao, Yuxuan and Xu, Haitao and Zhang, Hao},
  doi          = {10.1007/s11554-024-01581-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FANet: Focus-aware lightweight light field salient object detection network},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent detection of safety helmets and reflective vests based on deep learning. <em>JRTIP</em>, <em>22</em>(1), 1-18. (<a href='https://doi.org/10.1007/s11554-024-01573-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearing a safety helmet and a reflective vest is essential for ensuring worker safety. While YOLO-based object detectors have demonstrated significant accuracy in detecting dress code violations, they often struggle with detecting small targets and maintaining a global focus. To address these challenges, we propose MSCG-YOLO, a novel algorithm based on YOLO networks for worker detection. Our approach integrates multi-head self-attention (MHSA) into the backbone network and neck connections, enhancing the model’s global field of view and its ability to detect small and obscured targets. To further improve small target detection, we designed a new neck structure called consolidative informative systematic neck (CISNeck), which includes additional layers and an enhanced detection head. We also developed the superficial feature fusion module (SFFM) to optimize the high-resolution features of the fourth detection head. Generalized intersection over union (GIoU) was used as the loss function. Experimental results on custom datasets show that MSCG-YOLO outperforms existing methods, achieving AP and AP50 values of 52% and 91.6% on the validation set, and 53.6% and 91% on the test set. Compared to YOLOv8n, MSCG-YOLO improves AP50 scores by 3.4% on the validation set and 2.7% on the test set. In conclusion, this study effectively addresses the practical needs of dress code detection in construction scenarios.},
  archive      = {J_JRTIP},
  author       = {Lin, Conggong and Zhang, Yushi and Chen, Guodong},
  doi          = {10.1007/s11554-024-01573-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Intelligent detection of safety helmets and reflective vests based on deep learning},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rigid tank guide fault detection algorithm based on improved YOLOv7. <em>JRTIP</em>, <em>22</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11554-024-01576-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the problems of difficult target detection and recognition and low accuracy caused by factors such as uneven illumination, poor working conditions, complex structure of tank guide and narrow space in coal mine. This paper simulates the complex working environment of the underground mine to carry out different fault conditions experiments, and establishes four categories of channel fault picture data sets. In order to improve the detection accuracy and speed, the following improvements are made on the basis of the YOLOv7 algorithm, and our algorithm is constructed: (1) attention mechanisms are added at different locations of the network; (2) replacement loss function; (3) the original coupling detection head of YOLOv7 is replaced by an efficient decoupled head with implicit knowledge learning. The experimental results show that the mean average precision (mAP) of our algorithm model proposed in this paper reaches 93.2% when the Intersection over Union (IoU) threshold is 0.5, which is 3.2% higher than that of YOLOv7 itself, and the detection speed is also relatively improved by 15.76 frames per second (FPS), reaching 107.50 FPS. While solving the problem of unbalanced improvement of detection accuracy and speed, it also effectively reduces the number of parameters and calculation of the network, which verifies the feasibility of the improved algorithm in this paper.},
  archive      = {J_JRTIP},
  author       = {Du, Fei and Mo, Dandan and Ma, Tianbing and Fang, Jiaxin and Shu, Jinxin and Long, Jitao},
  doi          = {10.1007/s11554-024-01576-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Rigid tank guide fault detection algorithm based on improved YOLOv7},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved real-time object detection method based on YOLOv8: A refined approach. <em>JRTIP</em>, <em>22</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11554-024-01585-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of deep learning, researchers have proposed numerous advanced object detection algorithms. However, these often face challenges such as high parameter count and complexity. While some existing methods have reduced model parameters and complexity, they may sacrifice accuracy, reducing practical value. Therefore, achieving an optimal balance between maintaining detection accuracy and real-time performance has become a paramount concern for researchers. This paper introduces an enhanced YOLOv8 object detection model to tackle this issue. We propose the Faster-C2f module, inspired by RepViT, and the Rep-Fasterblock network based on FasterNet. By reparameterizing the FasterNetBlock module with Partial convolution and Depthwise separable convolution, our model effectively reduces parameters while efficiently handling spatial and channel information in the feature map. We enhance SPPF by combining AdaptiveMaxPool2d and AdaptiveAvgPool2d at different scales, introducing the SimAM attention mechanism to obtain Sim-SPPF. This improvement enables the network to capture features at various scales while increasing sensitivity towards key features. Additionally, we propose two neck network structures: enhanced PANet and CNN-based Cross-scale Feature PANet (CCFPANet). The PANet model is enhanced with our novel convolution PsConv and three CSPCx of varying complexity levels to reduce parameters while maintaining efficiency. Additionally, CCFPANet effectively combines detailed features with contextual information across scales. Furthermore, we introduce a decoupled detection head where Partial convolution replaces the traditional 3 $$\times$$ 3 Conv operation to further reduce parameters. Our proposed YOLOv8PANet and YOLOv8CCFPANet have different neck network structures and achieve a 21%/54.5% reduction in parameter count compared to YOLOv8 while maintaining detection accuracy of 84.2%/83.3% on the PASCAL VOC07+12 dataset and achieving detection accuracies of 42.1%/40.0% on the MSCOCO dataset respectively; furthermore, our experiments demonstrate high-speed detection trials reaching 100 FPS/ 108 FPS. These results indicate that our method achieves a superior balance between detection accuracy and real-time performance compared to existing methods, positioning it as a competitive solution in the field of real-time object detection.},
  archive      = {J_JRTIP},
  author       = {Zhong, Jiaqi and Qian, Huaming and Wang, Huilin and Wang, Wenna and Zhou, Yipeng},
  doi          = {10.1007/s11554-024-01585-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Improved real-time object detection method based on YOLOv8: A refined approach},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCP-YOLO: A lightweight underwater object detection model based on YOLOv7. <em>JRTIP</em>, <em>22</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11554-024-01586-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of technology, underwater biological detection tasks are generally conducted using mobile devices. This paper proposes the GCP-YOLO model based on YOLOv7 to address the challenges of deploying large detection models on mobile devices in the field of underwater object detection, particularly the issues of difficulty in underwater object detection and resource constraints. First, the GhostNetV2 module is used to make the ELAN module lightweight in the Neck part, reducing the model’s parameter count and computational complexity. Second, to address potential issues such as feature loss and low accuracy when collecting feature information in the lightweight network, we incorporate the CA Attention module after the improved ELAN module to prevent feature loss caused by the lightweighting process. Finally, we perform pruning on the overall improved model with a pruning rate of 50 $$\%$$ , further reducing the model’s parameter count and computational complexity. Compared to the YOLOv7 model, the GCP-YOLO underwater object detection model reduces the parameter count and computational complexity by a factor of 4, while improving accuracy by 2.8 $$\%$$ .},
  archive      = {J_JRTIP},
  author       = {Gao, Yu and Li, Zhanying and Zhang, Kangye and Kong, Lingyan},
  doi          = {10.1007/s11554-024-01586-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GCP-YOLO: A lightweight underwater object detection model based on YOLOv7},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Syflo: Augmenting yolo for real-time health monitoring of electric assets in power transmission lines. <em>JRTIP</em>, <em>22</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11554-024-01566-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable transmission of electrical energy to consumers across regions relies heavily on the integrity of power transmission lines and continuous monitoring of assets is crucial for maintaining system reliability. Unmanned aerial vehicles have revolutionized defect identification in real-time and accessibility, even in difficult-to-reach geographical landscapes, thereby improving image-based inspections. This work introduces semisupervised Yolo with focal loss function (SYFLo), a novel method that augments YOLO for real-time health monitoring of electric assets in power transmission lines. SYFLo integrates the focal loss function with semi-supervised learning to effectively address the lack of abundant labeled data, data imbalances and enhance detection accuracy. Additionally, it improves data generalizability across a wide range of images, ensuring robust performance despite varied image backgrounds. By leveraging YOLOv8, SYFLo significantly improves fault identification, achieving a detection accuracy of 96.5% and an FPS of 16.39. Experimental results demonstrate the impact of the proposed approach, highlighting its potential to enhance the reliability of power transmission line monitoring. These findings underscore the importance of integrating advanced deep learning techniques with innovative loss functions to address common challenges in real-time health monitoring systems.},
  archive      = {J_JRTIP},
  author       = {Sankuri, Raja Sekhar and Sristy, Nagesh Bhattu and Karri, Sri Phani Krishna},
  doi          = {10.1007/s11554-024-01566-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Syflo: Augmenting yolo for real-time health monitoring of electric assets in power transmission lines},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized YOLOv8 for multi-scale object detection. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-024-01582-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is considered one of the main tasks in computer vision and finds wide application in various fields, including medical imaging, face detection, object recognition, and many others. With the advent of AI, most contemporary object detection approaches rely on CNN methods such as Faster R-CNN and YOLO. YOLOv8 is one of the most renowned object detection algorithms, acclaimed for its ability to quickly and accurately detect objects of varying sizes, from small to large. YOLOv8 offers five variants, the smallest comprising 225 layers. Utilizing YOLOv8 for specific object sizes and resource-constrained applications may entail computational costs. In this paper, we introduce six modified versions of YOLOv8 tailored for different object sizes: small, medium, large, small–medium, medium–large, and small–large. These proposed models are evaluated based on computational cost, energy usage, and mAP-50, demonstrating reduced computational overhead compared to the original while maintaining accuracy.},
  archive      = {J_JRTIP},
  author       = {Rasheed, Areeg Fahad and Zarkoosh, M.},
  doi          = {10.1007/s11554-024-01582-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Optimized YOLOv8 for multi-scale object detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv7-rep: A re-parameterization method for surface defect detection in workpieces. <em>JRTIP</em>, <em>22</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11554-024-01583-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal of workpiece surface defect detection is to improve accuracy and real-time performance. Generally, the practice of using a network model with stronger representation ability and performing lightweight design at the same time cannot solve the problem of better performance balance between model training and inference phases. To address this, this paper optimizes and proposes a structural re-parameterization defect detection method: YOLOv7-Rep. First, we proposed a re-parameterization convolutional module: RepC2, which is a new block with cross-stage partial (CSP) connection and efficient gradient flow branch structure. Second, this article integrates the RepC2 module based on YOLOv7 and incorporates the coordinate attention (CA) mechanism in the backbone. Finally, we introduce the WIoUv1 bounding-box regression loss function. Experimental analysis demonstrates that YOLOv7-Rep outperforms object detectors with the same parameter count in terms of both detection accuracy and speed. It achieves detection accuracies of 78.3%, 87.3%, and 83.5% on three industrial component datasets (NEU-DET, TCAP-DET, and GC10-DET), respectively. Compared to YOLOv7, it significantly improves detection frame rates by 4.9% (an increase of 11.3 FPS), while achieving a better performance balance between training and inference phases in surface defect detection tasks.},
  archive      = {J_JRTIP},
  author       = {Xu, Zhigang and Fang, Pengwei and Yang, Xinyu and Wei, Pengjuan},
  doi          = {10.1007/s11554-024-01583-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLOv7-rep: A re-parameterization method for surface defect detection in workpieces},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deploying the model of improved heuristic-assisted adaptive SegUnet++ and multi-scale deep learning network for liver tumor segmentation and classification. <em>JRTIP</em>, <em>22</em>(1), 1-20. (<a href='https://doi.org/10.1007/s11554-024-01584-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Across worldwide, the major cause of death related to cancer disorder is liver tumor. Nowadays in medical industry, the liver cancer diagnosis model is a bottleneck since this disease incurs in the inner part of the body. To identify the disease, imaging techniques like Computed Tomography (CT) images are required. Hence, this paper develops an intelligent liver cancer segmentation and classification model employing diverse methodologies. At the starting stage, the raw images are fetched from the advanced benchmark data sources. The collected images undergo for segmentation. When compared to basic Unet model, it triggers the gradient vanishing issue that degrades the image resolution. Also, the parameter issue is the major hindrance to achieve the better classification process. Therefore, the Adaptive SegUnet++ (ASUnet++) is newly developed, where the essential parameters are optimally chosen by employing the Enhanced Lemurs Optimizer (ELO). The traditional model is vulnerable against training speed, gradient explosion and overfitting problem, which paves the way for misclassification and ends up with inaccurate results. Hence, this can be resolved using the residual connection in the deep learning classifier and multiscale concept. In the end, the segmented images are transferred to the Multiscale EfficientnetB7 with Residual Long Short-Term Memory (MEB7-RLSTM), which is used to provide the classified outcome as either the tumor disorder is incurred or not. The numerical findings of the model are represented in terms of 96.09 for accuracy, 93.48 for precision and 94.75 for f1-score, correspondingly. Hence, the final outcome exploits that it achieves the high true values that aid the practitioner to timely detect the disease and cure the patients efficiently.},
  archive      = {J_JRTIP},
  author       = {Lakshmi, P. Sampurna and Nagadevi, D. and Suman, K. and Deepthi, Ragodaya and Chikyal, Neetu},
  doi          = {10.1007/s11554-024-01584-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deploying the model of improved heuristic-assisted adaptive SegUnet++ and multi-scale deep learning network for liver tumor segmentation and classification},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BLP-YOLOv10: Efficient safety helmet detection for low-light mining. <em>JRTIP</em>, <em>22</em>(1), 1-11. (<a href='https://doi.org/10.1007/s11554-024-01587-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current safety helmet detection models face challenges in terms of computational complexity and hardware requirements, particularly in resource-constrained environments like underground mines. To address these issues, we propose the BLP-YOLOv10 model, which optimizes feature extraction and image processing by adjusting backbone channel parameters, incorporating sparse attention mechanisms, and integrating low-frequency enhancement filters. The experimental results demonstrate that BLP-YOLOv10 reduces the parameter count by 59.32% while achieving a mean average precision (mAP) of 98.1%, significantly improving detection speed and real-time performance. This makes the model highly robust and reliable, even under challenging lighting conditions.},
  archive      = {J_JRTIP},
  author       = {Du, Qing and Zhang, Shihao and Yang, Shijiao},
  doi          = {10.1007/s11554-024-01587-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {BLP-YOLOv10: Efficient safety helmet detection for low-light mining},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MobileMEF: Fast and efficient method for real-time mobile multi-exposure fusion. <em>JRTIP</em>, <em>22</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11554-024-01588-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in camera design and imaging technology have enabled the capture of high-quality images using smartphones. However, due to the limited dynamic range of digital cameras, the quality of photographs captured in environments with highly imbalanced lighting often results in poor-quality images. To address this issue, most devices capture multi-exposure frames and then use some multi-exposure fusion method to merge those frames into a final fused image. Nevertheless, most traditional and current deep learning approaches are unsuitable for real-time applications on mobile devices due to their heavy computational and memory requirements. We propose MobileMEF, a new method for multi-exposure fusion based on an encoder–decoder deep learning architecture with efficient building blocks tailored for mobile devices. This efficient design makes MobileMEF capable of processing 4K resolution images in less than 2 s on mid-range smartphones. MobileMEF outperforms state-of-the-art techniques regarding full-reference quality measures and computational efficiency (runtime and memory usage), making it ideal for real-time applications on hardware-constrained devices. Our code is available at: https://github.com/LucasKirsten/MobileMEF .},
  archive      = {J_JRTIP},
  author       = {Kirsten, Lucas Nedel and Fu, Zhicheng and Madhusudhana, Nikhil Ambha},
  doi          = {10.1007/s11554-024-01588-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {MobileMEF: Fast and efficient method for real-time mobile multi-exposure fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Insulator defect detection algorithm based on adaptive feature fusion and lightweight YOLOv5s. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-024-01589-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In power line inspections, detecting insulator defects is critical due to the potential breakdown and damage resulting from long-term exposure to the natural environment. However, challenges persist in practical detection processes, such as the limited availability of insulator defect images, difficulty detecting minor defects, and high memory consumption of deep learning models. We propose an insulator defect detection algorithm based on adaptive feature fusion and the lightweight YOLOv5s model to address these issues. First, we enhance the existing dataset by employing techniques like flipping, random pixel manipulation, noise addition, and contrast/brightness adjustments to increase the diversity of training samples. Next, we integrate the adaptive feature fusion (ASFF) module to enable the network to learn relationships between different feature maps, enhancing semantic information and improving the network's ability to detect minor defects. Finally, we replace the backbone of YOLOv5s with the lightweight convolutional neural network EfficientNet, making the network more efficient and enabling it to focus more on target features. Experimental results demonstrate significant improvements in insulator defect detection. The model achieves an accuracy rate of 96.4%, a recall rate of 96.1%, and an mAP of 97.6%, effectively enhancing network performance.},
  archive      = {J_JRTIP},
  author       = {He, Zhendong and Wang, Yiming and Zheng, Anping and Liu, Jie and Lou, Taishan and Zhang, Jie and Jiang, Penghao and Xu, Jiong},
  doi          = {10.1007/s11554-024-01589-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Insulator defect detection algorithm based on adaptive feature fusion and lightweight YOLOv5s},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A motion conditioned diffusion model for real-time hand trajectory semantic prediction. <em>JRTIP</em>, <em>22</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11554-024-01591-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based real-time gesture trajectory semantic prediction is challenging due to inherent ambiguity on semantic relatedness, which often result in high uncertainty in the dynamic and continuous semantic recognition scenes. In this paper, a motion-conditioned diffusion model for skeleton-based hand trajectory semantic prediction (DiffHand) is proposed. First, to improve the computational efficiency, we input the coordinates of the skeletal points representing the hand pose into the model. Then, we add random noise to the real future sequences to form a random noise sequence that conforms to a normal distribution. Next, we encode and feature fuse the past sequences. Finally, we use conditional features and random noise sequences to guide the model in generating continuous and reliable predicted motion trajectories. Experimental results show that our method has a high prediction accuracy of 76.3% and a recognition speed of 33 fps. Our method strikes a good balance between prediction accuracy and recognition speed.},
  archive      = {J_JRTIP},
  author       = {Jian, Chengfeng and Chen, Xiaoyan and Dai, Zhuoran},
  doi          = {10.1007/s11554-024-01591-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A motion conditioned diffusion model for real-time hand trajectory semantic prediction},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware-efficient FrWF-based architecture for joint image dehazing and denoising framework for visual sensors. <em>JRTIP</em>, <em>22</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11554-024-01568-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In addition to the haze, the captured images may also contain some amount of noise. The image dehazing approaches may be invalid when the input hazy image contains significant noises. To mitigate the effects of both haze and noise, a joint framework for image dehazing and denoising is vital. An effective framework for joint dehazing and denoising for single-image in real-time is proposed here. The VLSI design for the proposed framework is also presented. In the low-frequency (LF) subband, a hardware-friendly dehazing algorithm that makes use of the saturation-based transmission map (TM) estimation technique is used. In the high-frequency (HF) subbands, wavelet denoising combined with hard thresholding rule is used to improve the denoising capabilities. This research displays a competitive performance in the image quality of the dehazed visuals and computational effectiveness in the presence of Gaussian noise when compared to previous sophisticated dehazing algorithms. The hardware complexity of the suggested framework is reduced by using discrete wavelet transform (DWT) structures based on fractional wavelet filter (FrWF) and canonical-signed-digit (CSD) method. To the best of our knowledge, this is the first attempt to design and implement VLSI architecture for simultaneous dehazing and denoising in the wavelet domain. The proposed architecture is defined using Verilog hardware description language (HDL) and synthesized using the Cadence genus compiler. When employing the CSD technique, the proposed framework reduces area and power by 5.09% and 1.75%, respectively. The maximum operating frequency of the proposed architecture is 96.25 MHz.},
  archive      = {J_JRTIP},
  author       = {George, Anuja and Jayakumar, E. P.},
  doi          = {10.1007/s11554-024-01568-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hardware-efficient FrWF-based architecture for joint image dehazing and denoising framework for visual sensors},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-tuned depth-augmented U-net for enhanced semantic segmentation in indoor autonomous vision systems. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-024-01578-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent technological advancements have significantly improved indoor autonomous vision systems (IAVSs), underscoring the critical need to enhance their capability to interpret real-world environments in a manner similar to human perception. In response to this challenge, this paper introduces DEADFL-UNet, a groundbreaking framework that enhances the existing EADFL-UNet architecture. EADFL-UNet utilized the EfficientNetB3 model, supplemented by a new Super Attention Block and CBW-FL Loss Function, to tackle the significant data imbalance found in the NYUv2 dataset. Our enhancement focuses on using the MobileNetV2 model in conjunction with several fine-tuning techniques to maximize Depth characteristics in tandem with RGB ones inside the prior architecture. By applying the proposed techniques, we achieved an improvement of approximately 6% in mIoU (Mean Intersection over Union) compared to the original EADFL-UNet model, which was previously published. Furthermore, the difference between the fine-tuned and non-fine-tuned versions is 1.91% in mIoU, demonstrating the significant effectiveness of the fine-tuning technique. To confirm the real-time FPS (Frame Per Second) performance of each model, this technique has undergone extensive testing and assessment using standard metrics, not only on pre-existing datasets but also in a ROS2 (Robot Operating System) simulation environment. These proven techniques have potential for various applications in autonomous systems, such as robotic vision, GPS (Global Positioning System) position tracking, autonomous vehicles, and security, improving accuracy and efficiency.},
  archive      = {J_JRTIP},
  author       = {Tran, Hoang N. and Le, Thu A. N. and Nguyen, Nghi V. and Nguyen, Nguyen T. and Nguyen, Anh D.},
  doi          = {10.1007/s11554-024-01578-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fine-tuned depth-augmented U-net for enhanced semantic segmentation in indoor autonomous vision systems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards real-time accurate dense pedestrian detection via large-kernel perception module and multi-level feature fusion. <em>JRTIP</em>, <em>22</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11554-024-01594-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of computer vision, dense pedestrian detection remains a challenging task. Existing one-stage detectors in dense scenes often suffer from difficulties in feature extraction and inaccuracies in localizing small pedestrian targets due to severe occlusions and significant scale variations. To resolve these issues, we propose a new dense pedestrian detection algorithm named LEI-YOLO. First, we embed the proposed Large Kernel Perception (LKP) module into the backbone network to capture global information of targets in occluded scenes, extracting more comprehensive feature representations. Second, an enhanced Path Aggregation Network (E-PANet) is designed in the neck network to perform multi-level feature fusion by capturing more shallow target feature information and fully integrating it with deep feature information, effectively reducing small pedestrian misses. Finally, we propose a dynamically focused Powerful Intersection over Union (PIoU) loss function combined with auxiliary bounding boxes (Inner-PIoUv2) to improve the original loss function, leading to faster convergence and more accurate regression results. Experimental results show that on the CrowdHuman dataset, the proposed method improves the Average Precision (AP) by 4.2 $$\%$$ compared to the baseline model and reduces the log-average Miss Rate $$\left( MR^{-2}\right)$$ by 4.8 $$\%$$ . Moreover, our method also achieves significant results on the WiderPerson dataset, further validating the model’s effectiveness and generalization capability, achieving an effective balance between accuracy and real-time performance in dense pedestrian detection scenarios.},
  archive      = {J_JRTIP},
  author       = {Li, Huajie and Zhang, Sulan and Hu, Lihua and Zhou, Huiyuan},
  doi          = {10.1007/s11554-024-01594-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Towards real-time accurate dense pedestrian detection via large-kernel perception module and multi-level feature fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight deep learning model for real-time rectangle NdFeB surface defect detection with high accuracy on a global scale. <em>JRTIP</em>, <em>22</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11554-024-01592-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem that it is difficult to detect dynamic tiny square neodymium-iron-boron (NdFeB) surface defects in the case of limited computing resources, this paper proposes a square NdFeB magnet surface defect detection method based on the YOLO (YOLOv8-FCW) lightweight network. Initially, the lightweight global adaptive feature enhancement module (DFNet) network is used as the backbone feature extraction net-work. By customizing the depth of the feature matrix and reducing unnecessary branch structures, the model complexity is reduced while enhancing the network’s ability to extract multi-scale feature information. Subsequently, the deformable convolution module (DCNv3) is utilized to acquire twice downsampling feature maps without information loss, aiming to expand the receptive field for small-sized defects. Finally, to further improve detection accuracy, the Wise-IOU (WIOU) v3 bounding box loss function is introduced to focus on the samples that are difficult to identify and reduce the gradient penalty for low-quality samples. The experimental results show that the YOLOv8-FCW algorithm achieves a mean Average Precision (mAP@0.5) of 78.6% on the rectangle NdFeB magnet dataset, with a model parameter quantity and computational cost reduction of 33.2% and 24.7%, respectively compared with the baseline, and requires less computational resources for higher detection accuracy compared to other mainstream object detection algorithms. Finally, the model was deployed to industrial Automated Optical Inspection (AOI) devices using TensorRT. This deployment reduced the inference time for a single image to 2.7 ms and increased speed by 6.6 times, enabling dynamic micro-detection of surface defects in square NdFeB.},
  archive      = {J_JRTIP},
  author       = {Huang, Lin and Yuan, Heping and Chen, Shuixuan and Zhou, Bo and Guo, Yihuang},
  doi          = {10.1007/s11554-024-01592-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight deep learning model for real-time rectangle NdFeB surface defect detection with high accuracy on a global scale},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHL-YOLO: A real-time lightweight yarn inspection method. <em>JRTIP</em>, <em>22</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11554-024-01595-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the real-time and high-precision detection requirements in actual yarn production, this paper introduces a novel lightweight yarn detection method named PHL-YOLO, an optimized version of the YOLOv8 algorithm. Initially, we developed the PEC2F module by fusing PConv (partial convolution) and EMA (efficient multi-scale attention) components to enhance the model's feature extraction capabilities while concurrently reducing computational demands and the number of parameters. Subsequently, we deployed the HWD (Haar wavelet downsampling) module for downsampling, which effectively diminishes image resolution without sacrificing the critical information necessary for precise detection. Additionally, to address the excessive parameters and computational complexity in the detection head of YOLOv8, we introduced the LSCDH (lightweight shared convolutional detection head) module, which significantly simplifies the structure of the detection head. Finally, we applied a structured pruning technique, GSPMD (group-level structured pruning method with DepGraph), to further refine the model. The experimental outcomes indicate that our enhanced model PHL-YOLO has achieved a 1.0% increase in precision, a 2.5% improvement in recall, and a 2.3% increase in mAP (mean average precision), along with a 26 FPS performance boost over the original model. Furthermore, our model’s computational load and parameter count are only 29.6% and 10.2% of those in the original YOLOv8 model, respectively. PHL-YOLO not only effectively reduces model complexity, but also upholds high performance levels, providing a valuable reference for rapid and accurate yarn detection in real-world production scenarios.},
  archive      = {J_JRTIP},
  author       = {Dai, Jiachao and Ren, Jia and Li, Shangjie},
  doi          = {10.1007/s11554-024-01595-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {PHL-YOLO: A real-time lightweight yarn inspection method},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware implementation of iterative method for enhanced affine motion estimation in versatile video coding. <em>JRTIP</em>, <em>22</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11554-024-01596-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Versatile Video Coding (VVC), the latest advancement in video coding standards, significantly outperforms its predecessor, High Efficiency Video Coding (HEVC), in terms of coding efficiency. Traditional motion representation methods, which rely solely on translational motion, often fall short in accurately depicting object positions in high-definition (HD) video content. The computation of motion with high software complexity can be very time-consuming; therefore, it is essential to utilize a hardware parallel pipeline for efficient processing. This paper introduces a novel, high-throughput iterative method optimized for hardware implementation, which significantly enhances affine motion prediction efficiency within VVC. The hardware architecture employs highly parallel pipeline operations to design matching criteria for computing the optimal combination of affine motion vectors, using 64 $$\times$$ 64 processing units to support 64 $$\times$$ 64 to 128 $$\times$$ 128 coding units. Additionally, a high-efficiency affine iterative array was designed to adjust the motion vectors, along with an edge detection operator to streamline the computation of image gradients. Experimental results demonstrate that the hardware-based approach efficiently processes 4K video at a minimum of 60 frames per second, achieving up to three times the benchmark frame rate. This performance is achieved by leveraging hardware parallelism to meet stringent real-time video encoding standards without compromising coding efficiency.},
  archive      = {J_JRTIP},
  author       = {Hong, Jingping and Dong, Zhihong and Pang, Mengxin and Kang, Zetao and Cao, Peng},
  doi          = {10.1007/s11554-024-01596-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hardware implementation of iterative method for enhanced affine motion estimation in versatile video coding},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Denet: An effective and lightweight real-time semantic segmentation network for coal flow monitoring. <em>JRTIP</em>, <em>22</em>(1), 1-16. (<a href='https://doi.org/10.1007/s11554-024-01602-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic extraction of coal flow region of coal mine belt conveyor plays an important role in coal flow monitoring, and real-time control of belt speed through real-time accurate monitoring of coal flow, which realizes the purpose of energy saving and consumption reduction of belt conveyor. In this paper, a real-time semantic segmentation network with detail enhancement for pixel-level coal flow monitoring, called DENet, is proposed. First, to ensure the strong real-time performance of the network, a two-branch coding structure is used to extract the semantic information and spatial detail information. Second, to improve the feature representation of spatial detail information, we design the Parameter-free Attention-Guided Enhancement Module (PF-AGEM) and the detail enhancement module (DEM), which fully integrate the semantic information features in the semantic branch into the detail branch and further enhance the detail features. Third, we design the multi-scale channel attention (MSCA) module in the semantic branch to extract the semantic information features of small targets earlier in the high-resolution feature maps, which solves the problem that the semantic information features of small targets are easily lost in the low-resolution feature maps. Finally, we propose a selective feature fusion module (FFM) to better realize the fusion of semantic information and spatial detail information. Experimental results show that the proposed DENet achieves a mean intersection over union (mIoU) of 96.23% at 87.1 frames per second (FPS) on the Coal Flow Segmentation (CFS) dataset and 74.9% mIoU at 207 FPS on the Camvid dataset, which is competitive with the state-of-the-art real-time semantic segmentation models.},
  archive      = {J_JRTIP},
  author       = {Shao, Xiaoqiang and Lyu, Zhiyue and Li, Hao and Liu, Mingqian and Han, Zehui},
  doi          = {10.1007/s11554-024-01602-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Denet: An effective and lightweight real-time semantic segmentation network for coal flow monitoring},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oil leak detection in substation equipment based on PFDAL-DETR network. <em>JRTIP</em>, <em>22</em>(1), 1-16. (<a href='https://doi.org/10.1007/s11554-024-01593-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep-learning detection methods are widely used in power systems, there is still potential for improvement in the efficiency and automation of oil leak detection in substation equipment. This study proposes an online oil leak detection algorithm for substation equipment based on the PFDAL-DETR model. The optimization of feature sharing among convolution kernels through the perceptual field dual attention mechanism (PFDAM) enhances feature extraction efficiency in deep-learning models. A thin-head feature extraction network incorporating a lightweight model (LM) is proposed to preserve the gradual transmission of spatial feature information to the channels. A loss function based on shape intersection over union (Shape-IoU) is used to address the issue of class imbalance and improve model performance. Experimental results demonstrate that the proposed model achieves a mean average precision (MAP@0.5) of 91.20%. This surpasses current state-of-the-art detection models such as YOLO and DETR, providing meaningful research implications for the detection of oil leaks in substation equipment.},
  archive      = {J_JRTIP},
  author       = {Ji, Chao and Liu, Junpeng and Zhang, Fan and Jia, Xinghai and Song, Zhiwei and Liang, Chengliang and Huang, Xinbo},
  doi          = {10.1007/s11554-024-01593-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Oil leak detection in substation equipment based on PFDAL-DETR network},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EAPoseNet: Efficient animal pose network in low computing power scenarios. <em>JRTIP</em>, <em>22</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11554-024-01598-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient estimation of animal poses is essential in various fields, including animal behavior research, pharmaceutical studies, and bionic robotics. This capability provides immediate feedback and enables rapid decision-making in practical applications. However, pose estimation typically relies on numerous learnable parameters and complex model architectures to achieve high accuracy, making real-time detection challenging, especially in scenarios with low computational power. This paper proposes an efficient animal pose estimation network (EAPoseNet) for resource-constrained environments that achieve real-time detection without compromising accuracy. EAPoseNet designs a more advanced feature extraction network (EAPBackbone) and a key point localization network (EAPHead) for animal pose estimation tasks. We conducted multiple comparative experiments on the AP-10K, Desert Locust, Vinegar Fly, Zebra, Mouse, and Horse10 datasets to evaluate EAPoseNet. Additionally, we performed ablation studies on the Mouse dataset. The experimental results demonstrate that EAPoseNet delivers competitive performance with a minimal number of parameters. On the AP-10K dataset, our method’s average precision (AP) is 1.3% higher than that of the RTMPose-M model of the same scale, while EAPoseNet has 1.3 million fewer parameters than RTMPose-M. On the Mouse dataset, the best implementation of EAPoseNet enhances the model’s AP by 0.1% to 0.5%.},
  archive      = {J_JRTIP},
  author       = {Chen, Yuming and Guo, Chaopeng and Jiao, Tianzhe and Zhang, Zhe and Song, Jie},
  doi          = {10.1007/s11554-024-01598-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {EAPoseNet: Efficient animal pose network in low computing power scenarios},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCEI-RTDETR: An improved RT-DETR-based detection algorithm for data center equipment indicator lights. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-024-01599-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issue of low detection accuracy caused by small target sizes and complex image backgrounds in the detection of indicator lights in data center equipment rooms, a lightweight small-target detection algorithm based on the Transformer architecture, named DCEI-RTDETR, is proposed. First, EfficientFormerV2 is utilized as the feature extraction network. By reducing the number of downsampling operations, the size of the feature maps is increased, enabling the network to focus more effectively on small targets. Subsequently, a high-level screening feature aggregation network is designed, which employs the HiLo attention mechanism at the highest feature level as an intra-feature interaction module. Simultaneously, the GSConv and VoVGCSCSP modules are used for cross-scale feature fusion, dynamically optimizing the feature map’s representation capability. Additionally, a one-to-many label assignment method is introduced, utilizing a grouped decoder to optimize object query processing, thereby alleviating the issues of occlusion and the loss of small target feature information. Finally, the GIOU loss function is replaced with Inner-EIOU incorporating a scaling factor to control the auxiliary bounding box, which improves the accuracy of small target detection. Experimental results on a proprietary data center equipment status detection dataset show that, compared to the original RT-DETR algorithm, the mAP50 is increased by 4.2%, the mAP50:95 by 2.1%, and the FPS is maintained at 90. Generalization experiments on the public VisDrone2021 dataset also demonstrate the effectiveness and generality of the proposed algorithm, with the mAP50 improved by 4.4%.},
  archive      = {J_JRTIP},
  author       = {Xu, Liangjie and Zeng, Wenxian},
  doi          = {10.1007/s11554-024-01599-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DCEI-RTDETR: An improved RT-DETR-based detection algorithm for data center equipment indicator lights},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time detection of small underwater organisms with a novel lightweight SFESI-YOLOv8n model. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-024-01610-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of detecting small targets in complex underwater environments, an efficient and lightweight model, SFESI-YOLOv8n, is proposed. The model improves small target recognition by incorporating a dedicated detection layer and reduces parameter count by removing the large target detection layer. Furthermore, the introduction of the C2f-F module eliminates redundant information from consecutive convolution operations in the bottleneck, further simplifying the model. The integration of a lightweight mixed local context attention (MLCA) mechanism within the small target fusion layer increases sensitivity to small targets. The dynamic upsampler (DySample) employs point sampling to preserve enhanced edge and detail information in feature maps, resulting in clearer feature representations. In addition, the novel In-NWD loss function, utilizing Wasserstein distance and auxiliary bounding boxes, improves small target detection performance. On the UPRC2020 public dataset, SFESI-YOLOv8n achieved an mAP@0.5 of 83.7%, which is a 1.1% improvement over the baseline model. The parameter count and size of the model were reduced by 49.2% and 45.7%, respectively. The frame rate reached 227 FPS, indicating a 9 FPS increase compared to the baseline. On the NVIDIA Jetson TX2 edge device, inference latency decreased from 65 ms to 24 ms with TensorRT acceleration, thereby meeting real-time detection requirements. The SFESI-YOLOv8n model provides a viable and efficient solution for the autonomous detection of small underwater targets, demonstrating significant practical value.},
  archive      = {J_JRTIP},
  author       = {Fei, Yuhuan and Liu, Fenghao and Su, Meng and Wang, Gengchen and Sun, Xufei and Zang, Ran},
  doi          = {10.1007/s11554-024-01610-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time detection of small underwater organisms with a novel lightweight SFESI-YOLOv8n model},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEN-YOLO: A lightweight remote sensing small aircraft object detection model for satellite on-orbit detection. <em>JRTIP</em>, <em>22</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11554-024-01601-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of conventional detection algorithms in small aircraft target detection is often unsatisfactory due to the intricate backgrounds of remote sensing images and the diminutive size of aircraft targets. Furthermore, prevalent deep learning algorithms typically prove overly complex for integration into resource-constrained satellite platforms. In response to these challenges, an enhanced algorithm named LEN-YOLO (Lite backbone - Enhanced Neck - YOLO) has been devised to enhance detection accuracy while preserving model simplicity for the detection of small aircraft in satellite on-orbit scenarios. First, the EIoU Loss is adopted for target localization, enabling the network to effectively focus on small aircraft targets. Second, a Lite backbone is designed by discarding high semantic information, using low-semantic feature maps to detect small targets. Finally, a Bidirectional Weighted FPN based on SimAM and GSConv (BSG-FPN) is proposed to fuse feature maps of different scales to increase detailed information. Experimental results on RSOD and DIOR datasets demonstrate compared to the baseline YOLOv5, LEN-YOLO achieves an increase of 5.1% and 4.2% in $$\text {AP}_s$$ respectively. Notably, parameters are reduced by 78.3% and floating-point operations by 33.2%.},
  archive      = {J_JRTIP},
  author       = {Wu, Jian and Zhao, Fanyu and Jin, Zhonghe},
  doi          = {10.1007/s11554-024-01601-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {LEN-YOLO: A lightweight remote sensing small aircraft object detection model for satellite on-orbit detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time detection of coal mine safety helmet based on improved YOLOv8. <em>JRTIP</em>, <em>22</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11554-024-01604-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing coal mine safety helmet detection method has problems such as low detection accuracy, susceptibility to environmental impact, poor real-time performance, and a large number of parameters. So, this paper proposes a Miner Helmet detection algorithm based on YOLO, abbreviated as MH-YOLO. First, the convolutional block attention mechanism (CBAM) is applied to improve the CSPDarkNet53 to 2-Stage FPN (C2f) module of the backbone network and enhance feature-extraction capability. Second, the MaxPooling (MP) module is used to replace the partial subsampling convolution of YOLOv8 to reduce the impact of unbalanced sample categories and improve the recall rate. In addition, a small target detection layer is added to further improve the small target characteristics by fusing shallow network features with deep network features. Finally, the ZoomCat and Scalseq Module (ZAS) feature-extraction module is used to improve the detection accuracy of small and overlapping targets. Training and testing were conducted on the public dataset CUMT-Helmet from China University of Mining and Technology and DsLMF + helmet from Xi’an University of Science and Technology. The proposed MH-YOLO achieves mAP50 values of 92.4% and 97.8%, respectively, surpassing the comparative networks. The detection time is 10.1 ms, enabling accurate and real-time detection of whether coal miners are wearing safety helmets. Source code is released in https://github.com/xgli411/MH-YOLO .},
  archive      = {J_JRTIP},
  author       = {Li, Jie and Xie, Shuhua and Zhou, Xinyi and Zhang, Lei and Li, Xianguo},
  doi          = {10.1007/s11554-024-01604-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time detection of coal mine safety helmet based on improved YOLOv8},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time fire detection algorithm on low-power endpoint device. <em>JRTIP</em>, <em>22</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11554-024-01605-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely detection of fires can significantly reduce the damage caused by them. Most contemporary fire detection algorithms are designed for high-performance computing devices, leading to slow execution on endpoint devices. To address this issue, we explore an enhanced method based on YOLOv8 for rapid and efficient fire detection on endpoint chips equipped with Neural Processing Units (NPUs). Our algorithm is designed with consideration for factors not accounted for by indicator Floating Point Operations (FLOPs), as well as the unique characteristics of endpoint chips. We explore how different activation functions impact model execution speed and utilize quantization-friendly activation functions. We introduce a reparameterization module to address FLOPs limitations and optimize execution speed by aligning the input image resolution with sensor-captured images. Additionally, we use transfer learning techniques to improve model accuracy. Experimental results on the D-fire dataset indicate that our designed models offer advantages in terms of fast execution speed and competitive accuracy. Tests conducted on the endpoint chip reveal that the single-frame detection latency of our proposed two sizing models is 46 and 21 ms, respectively, which is significantly lower than that of YOLOv8, thereby placing minimal strain on the computing system. Furthermore, the accuracy is only slightly reduced compared to YOLOv8.},
  archive      = {J_JRTIP},
  author       = {Peng, Ruoyu and Cui, Chaoyuan and Wu, Yun},
  doi          = {10.1007/s11554-024-01605-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time fire detection algorithm on low-power endpoint device},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high-precision and real-time lightweight detection model for small defects in cold-rolled steel. <em>JRTIP</em>, <em>22</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11554-024-01606-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cold-rolled steel strip processing, small defects like linear scratches and pits are challenging to detect with existing methods, adversely affecting product quality. To address this issue, we propose SC-YOLOv8, a lightweight and real-time surface defect detection model based on You Only Look Once version 8 nano (YOLOv8n), optimized for identifying small defects on steel strip surfaces. We introduce an adaptive feature extraction module using Dynamic Snake Convolution (DSConv) to enhance detection accuracy for small and elongated targets while maintaining a lightweight model structure. Additionally, we incorporate the Convolutional Block Attention Module (CBAM) to improve feature fusion and reduce information loss, refining feature maps and enhancing the representation of small defects without significantly increasing computational complexity. We utilize the Cold-Rolled Strip Defects Dataset (CR7-DET) which includes seven defect categories and comprises 4140 images with 11,020 annotated object boxes. The proposed SC-YOLOv8 model achieved a mean Average Precision (mAP) of 87.3%, outperforming most current state-of-the-art detection algorithms on CR7-DET. The model has a parameter size of 3.53M and GFLOPs of 8.4. In experimental evaluations, SC-YOLOv8 achieved 398 Frames Per Second (FPS) on the server and 74.7 FPS in laboratory tests simulating an industrial production line, with a total processing delay of 28 ms per frame, fully meeting real-time detection requirements. The lightweight design and high efficiency of this model position it well for adapting to future increases in production speed, aligning with the evolving demands of the steel manufacturing industry.},
  archive      = {J_JRTIP},
  author       = {Chen, Shuzong and Jiang, Shengquan and Wang, Xiaoyu and Ye, Ke and Sun, Jie and Hua, Changchun},
  doi          = {10.1007/s11554-024-01606-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A high-precision and real-time lightweight detection model for small defects in cold-rolled steel},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time pedestrian recognition model on edge device using infrared vision system. <em>JRTIP</em>, <em>22</em>(1), 1-11. (<a href='https://doi.org/10.1007/s11554-024-01608-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the lightweight and real-time detection requirements of infrared pedestrians on edge devices, this paper proposes an improved infrared pedestrian recognition model based on You Only Look Once v5 nano. The model effectively reduces the number of parameters by introducing the Ghost Convolution and SlimNeck modules in the lightweight design. To enhance real-time performance, the model reduces the number of parameters and improves operation speed through the Performance-aware Approximation of Global Channel Pruning pruning strategy. After experimental analysis, a balance between operation speed and accuracy is achieved. The results show that compared to the original model, the proposed model decreases the number of parameters by 1.52 M, increases running speed by 21%, and decreases mAP@0.50 by only 0.4%. The running speed on Jetson Nano reaches 0.12 s per image. The proposed model effectively ensures high detection accuracy under lightweight and real-time requirements, providing technical support for the deployment of edge devices in scenarios such as autonomous driving.},
  archive      = {J_JRTIP},
  author       = {Liu, Li and Huang, Kaiye and Li, Yujian and Zhang, Chuxia and Zhang, Shuo and Hu, Zhengyu},
  doi          = {10.1007/s11554-024-01608-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time pedestrian recognition model on edge device using infrared vision system},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Steel surface defect detection based on the lightweight improved RT-DETR algorithm. <em>JRTIP</em>, <em>22</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11554-024-01611-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing object detection algorithms for steel surface defects detection have several drawbacks such as large model size, low efficiency, more parameters, and slow speed which limit their application in production. To solve these problems, in this paper a lightweight improved object detection algorithm, named LRT-DETR, is proposed based on the RT-DETR algorithm. Firstly, the BasicBlock modules in RT-DETR are replaced by the lightweight MobileNetV3 module. As a feature extraction backbone, it can fully capture the long-range feature interactions of steel defects and the local feature interactions. Secondly, a more efficient depthwise separable convolution (DWConv) and the VoVGSCSP structure are introduced into RT-DETR to refine the feature fusion network, which thereby enhances the feature extraction and fusion, as well as reduces the computational complexity of the algorithm and achieves a lightweight effect. Finally, a novel bounding box similarity measure MPDIoU is used to replace the loss function in the original network providing a more accurate measure of the similarity between bounding boxes to improve the accuracy of the model. Experimental results show that an average precision (AP) of the proposed LRT-DETR algorithm is 74.8%, 1.5% higher than the original algorithm, and the number of parameters and computational cost are reduced by 65.90% and 75.96%, respectively. Further experimental results demonstrate that the LRT-DETR algorithm is better than other compared algorithms in terms of detection speed and accuracy. Our algorithm can be used in the real-time and efficient detection of steel surface defects.},
  archive      = {J_JRTIP},
  author       = {Mao, Haojie and Gong, Yongwang},
  doi          = {10.1007/s11554-024-01611-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Steel surface defect detection based on the lightweight improved RT-DETR algorithm},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on infrared small target pedestrian and vehicle detection algorithm based on multi-scale feature fusion. <em>JRTIP</em>, <em>22</em>(1), 1-16. (<a href='https://doi.org/10.1007/s11554-024-01607-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared imaging technology relies on detecting the electromagnetic waves emitted by an object's spontaneous thermal radiation for imaging. It can overcome the adverse effects of complex lighting conditions on the detection of pedestrians and vehicles on the road. To address the issues of low accuracy and missed detection in visual detection under complex traffic conditions, such as during rain, snow, or at night, a pedestrian and vehicle detection model using infrared imaging has been proposed. This model improves the neck network and incorporates an attention mechanism. First, by adding a multi-scale feature fusion small-object detection layer to the model's neck, enhancing the capture of detailed information about small infrared objects and reducing missed detections. Second, a novel dual-layer routing attention mechanism is designed, allowing the model to focus on the most relevant feature areas and improving the detection accuracy of small infrared objects. Next, the CARAFE upsampling method is used for adaptive upsampling and context information fusion, which enhances the model's ability to reorganize features and capture details. Finally, a lightweight CSPPC module is constructed using partial convolutions to replace the C2f module in the neck network, which improves the model's frame rate. Experimental results show that, compared to the baseline model, BCC-YOLOv8n improves precision, recall, mAP@0.5, and mAP@0.5:0.95 by 1.4%, 4.8%, 5.3%, and 4.5%, respectively, while reducing the number of parameters by approximately 7%. Additionally, a frame rate of 70.8 FPS was achieved, satisfying the requirements for real-time detection.},
  archive      = {J_JRTIP},
  author       = {Xiang, Xinjian and Zhang, Guolong and Huang, Li and Zheng, Yongping and Xie, Zongyi and Sun, Siqi and Yuan, Tianshun and Chen, Xizhao},
  doi          = {10.1007/s11554-024-01607-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Research on infrared small target pedestrian and vehicle detection algorithm based on multi-scale feature fusion},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-GCV: A lightweight algorithm for ship object detection in complex inland waterway environments. <em>JRTIP</em>, <em>22</em>(1), 1-16. (<a href='https://doi.org/10.1007/s11554-024-01597-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight ship detection offers the dual benefits of rapid detection and low computational cost, making it particularly advantageous for inland waterway safety monitoring. This study introduces YOLO-GCV, a lightweight ship detection algorithm based on YOLOv7-tiny. The proposed algorithm strikes an effective balance between detection accuracy and speed. First, the ELAN-Ghost lightweight module is integrated into the backbone network, while VoVGSCSP, another lightweight module, is introduced into the neck to further streamline the model structure. Coordinate convolution is utilized to enhance the model’s ability to capture the spatial features of ship targets. Furthermore, the WIoU loss function is incorporated to improve convergence during training and significantly bolster the model’s generalization capability. Experimental results indicate a 19.2% reduction in model parameters and an 18.9% decrease in GFLOPs, with mAP0.5 and mAP0.5:0.95 increasing by 0.8% and 0.5%, respectively, over the baseline model. The model achieves a processing rate of approximately 42 images per second, meeting real-time detection requirements. This lightweight ship detection algorithm effectively addresses real-time detection needs in complex inland waterway environments and offers notable advancements in inland navigation safety.},
  archive      = {J_JRTIP},
  author       = {Jin, Yang and Wang, Ping and Liu, Shuwang and Xue, Kai and Li, Qiuhong and Wang, Hao},
  doi          = {10.1007/s11554-024-01597-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {YOLO-GCV: A lightweight algorithm for ship object detection in complex inland waterway environments},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDHS-RLDNet: A real-time lightweight detection network for small-dense photovoltaic hot-spots. <em>JRTIP</em>, <em>22</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11554-024-01600-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure the safe and stable operation of photovoltaic power plants, it is crucial to conduct regular fault inspections on solar arrays. In a complex inspection environment, it is difficult to ensure the accuracy of detecting small and densely distributed photovoltaic hot-spot faults using traditional algorithms, and real-time detection is also challenging. To solve the above issues, a real-time lightweight detection network for small and dense photovoltaic hot-spots is proposed. Firstly, a multi-scale target extraction module is designed to enhance the feature extraction capability of the backbone network, which can effectively detect faults at different scales. Additionally, a small target prediction head is added to improve the detection performance of small hot-spot faults. Secondly, a dense object detection module is designed to enhance the positional information of hot-spot faults and effectively suppress background interference caused by complex backgrounds. Furthermore, to achieve network lightweighting, the method of knowledge distillation is adopted. By transferring the parameters of the teacher network to the student network, it simplifies the network parameters, improves model inference speed, and ensures real-time detection performance of the network. Finally, to verify the superiority of the proposed network, seven classical algorithms are selected for comparison experiments. The experimental results demonstrate that SDHS-RLDNet can accurately detect multi-scale hot-spot faults under various conditions with an accuracy rate reaching 86.6%.},
  archive      = {J_JRTIP},
  author       = {Hao, Shuai and Li, Tong and Ma, Xu and Li, Tianqi and Chang, Chi and Qi, Tianrui},
  doi          = {10.1007/s11554-024-01600-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {SDHS-RLDNet: A real-time lightweight detection network for small-dense photovoltaic hot-spots},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel 3D-PCHCS design and application on ophthalmic medical image copyright protection with FPGA implementation. <em>JRTIP</em>, <em>22</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11554-024-01609-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital age, medical images have become an indispensable tool for the diagnosis of certain diseases and health prevention. In the face of increasing computing power and diverse means of attack, as well as the need for real-time processing of medical images, traditional encryption and watermarking techniques may no longer be sufficient to effectively protect the integrity and copyright of these sensitive images. Based on this practical requirement, this study proposes a novel digital watermarking algorithm for copyright authentication of ophthalmic medical images using a chaotic system. This algorithm integrates a novel three-dimensional pupal equilibrium curved hyperchaotic system (PCHCS) to enhance the encryption effect of the AES algorithm. Clever embedding of watermarks in the edge regions of the ROI using Least Significant Bit (LSB) technology ensures minimal impact on diagnostic quality. RONI uses DCT transform for processing to enhance copyright authentication. The chaotic system and watermarking algorithm have been implemented on FPGA to further demonstrate their feasibility. The PSNR of the proposed system can reach more than 55 dB and the SSIM value is close to 1. The values of NC and BER also perform well under various attacks. Experimental results show that the designed watermarking algorithm has good invisibility and robustness.},
  archive      = {J_JRTIP},
  author       = {Li, Shi-yi and Wu, Guang-yi and Sun, Jing-yu and Yan, Peng-fei and Zhang, Hao},
  doi          = {10.1007/s11554-024-01609-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Novel 3D-PCHCS design and application on ophthalmic medical image copyright protection with FPGA implementation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high throughput JPEG2000 lossless encoder architecture. <em>JRTIP</em>, <em>22</em>(1), 1-9. (<a href='https://doi.org/10.1007/s11554-024-01590-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the next-generation image compression standard succeeding JPEG, JPEG 2000 achieves superior compression performance by utilizing DWT and EBCOT. However, due to the high complexity of JPEG 2000, processing images requires significant computation time and resources. To address this challenge, this paper proposes a straightforward BPC encoding structure that allows EBCOT encoding of wavelet coefficients with only one scan per coefficient. Building upon this, a pipelined structure for lossless encoding is introduced, which can encode images with jpeg2000 lossless encoding at a rate of one pixel every two clock cycles. The entire encoding architecture is tested on an FPGA platform, demonstrating that a single encoder IP core achieves a maximum throughput of 165 million pixels per second at a working frequency of 330 MHz, equivalent to encoding 4k( $$3840\times 2160$$ ) gray images at 20 frames per second (fps).},
  archive      = {J_JRTIP},
  author       = {Zhao, Xi and Xue, Yongjiang and Song, Qingzeng},
  doi          = {10.1007/s11554-024-01590-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-9},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A high throughput JPEG2000 lossless encoder architecture},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High throughput VLSI design for real-time JPEG 2000 embedded block coding. <em>JRTIP</em>, <em>22</em>(1), 1-10. (<a href='https://doi.org/10.1007/s11554-024-01612-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Embedded Block Coding with Optimal Truncation (EBCOT) Tier-1 process is a critical component of the Joint Photographic Experts Group 2000 (JPEG2000) framework, significantly influencing throughput in hardware encoders. This paper presents an optimized hardware architecture for a high-performance EBCOT Tier-1 encoder, based on parallel code-block processing. The design features simultaneous bit-plane coding via three parallel channels, along with concurrent arithmetic coding enabled by six context-generating units. To overcome the throughput bottleneck of conventional designs, we introduce a novel multiplexing strategy for the Multiple Quantization coder. Our encoder achieves a throughput of 2782 Mb/s, a 7.3 times improvement over existing implementations, making it well-suited for high-speed JPEG2000 applications. The proposed architecture, when applied to the JPEG2000 encoding system, significantly enhances the circuit performance of the overall encoder on both Field Programmable Gate Array and Application Specific Integrated Circuit platforms. This provides a novel approach to architecture optimization. When processing 512 $$\times $$ 512 size grayscale map, the processing speed can reach more than 160 Frames Per Second, which can well meet the real-time requirements of edge device applications.},
  archive      = {J_JRTIP},
  author       = {Jin, Xin and Jing, Peng and Wang, Lin and Dai, Yuzhou and He, Siyuan and Ma, Zhao and Zhang, Wei},
  doi          = {10.1007/s11554-024-01612-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {J. Real-Time Image Process.},
  title        = {High throughput VLSI design for real-time JPEG 2000 embedded block coding},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RTDETR-refa: A real-time detection method for multi-breed classification of cattle. <em>JRTIP</em>, <em>22</em>(1), 1-16. (<a href='https://doi.org/10.1007/s11554-024-01613-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the farming industry, to cope with problems such as complex pasture environments and dense targets, which lead to increased difficulty in recognising and thus quickly classifying and automatically identifying cattle breeds to improve accuracy. In this paper, an RTDETR-Refa (RepConv Efficient Faster Attention) algorithm based on ResNet18 backbone network is proposed for cattle breed classification and identification. First, new improvements are made to the ResNet18 backbone network: the Faster-Block module is introduced to improve the feature extraction network and increase the computational speed without sacrificing the accuracy; the 1 $$\times$$ 1 convolution in the Faster-Block module is replaced by a 3 $$\times$$ 3 convolution using the RepConv reparameterised with the RepVGG block, which makes the algorithm more lightweight and improves the inference speed. Second, in order to enhance the feature transformation and classification, the Efficient Multiscale Attention (EMA) module is added after the Faster-Block module at different stages. Finally, the above improved Faster-Block module is used to replace the 4-layer BasicBlock after the 3 convolutional layers in the backbone network of ResNet18. The training test results of the RTDETR-Refa algorithm are compared with other classical models to validate the superiority of the RTDETR-Refa algorithm. The average accuracy of the RTDETR-Refa algorithm on the bovine The average accuracy of RTDETR-Refa algorithm on the classification training set is 91.6%, which is 0.8% higher than the original model and 0.9–5.2% higher than other classical models. The experimental results show that the RTDETR-Refa model proposed in this paper is able to identify and classify different breeds of cattle while guaranteeing similar detection speed, which proves the feasibility of convolutional neural networks in breed identification and classification.},
  archive      = {J_JRTIP},
  author       = {Li, Bingxuan and Fang, Jiandong and Zhao, Yvdong},
  doi          = {10.1007/s11554-024-01613-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {RTDETR-refa: A real-time detection method for multi-breed classification of cattle},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D2net: A dual-branch lightweight network for conveyor belt rotation detection in pipe belt conveyors. <em>JRTIP</em>, <em>22</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11554-024-01614-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the swift advancement of artificial intelligence technology, semantic segmentation has emerged as a critical method for identifying rotational flaws in conveyor belts of pipe belt conveyors. Nonetheless, current segmentation algorithm models typically exhibit issues of excessive model parameters and sluggish inference speed, making it challenging to simultaneously satisfy the accuracy demands of conveyor belt edge segmentation and real-time requirements, thereby complicating their adaptation to the real-time monitoring needs of actual production lines. This research offers an efficient backbone network, D2Net, for real-time semantic segmentation of conveyor belt edges to address this issue. The network comprises a dual-branch Deep Dual-Resolution Network that facilitates information exchange across the branches via several bilateral fusion processes. A Multi-Scale Attention Aggregation Module (MSAAM) is developed to effectively broaden the network’s sensory field and improve segmentation performance via multi-scale contextual fusion of low-resolution feature maps. This research constructs a dataset of pipe belt conveyor operations at a steel factory across three locations to validate the model’s performance, and the suggested network is trained and tested using this dataset. The experimental findings indicate that the network attains an average intersection over union (mIoU) of 75.34 $$\%$$ on a dataset with a resolution of 512 $$\times $$ 512, while the inference speed reaches 169.5 frames per second (fps), and the model comprises merely 22.9 MB of parameters. Compared to existing methods, D2Net successfully achieves a balance between accuracy and speed, making it well suited for real-time applications in practical industrial environments. This research demonstrates that the D2Net achieves an effective equilibrium between speed and accuracy in the belt edge segmentation of pipe belt conveyors. It exhibits optimal equilibrium and delivers robust technical assistance for real-time semantic segmentation in edge computing environments. To further advance research in this field, we commit to making the dataset used in this study publicly available upon acceptance of the paper. By sharing this data resource, we aim to support subsequent studies and foster continuous progress in the development of rotational detection technology for pipe belt conveyors.},
  archive      = {J_JRTIP},
  author       = {Wang, Xingyu and Hao, Nini and Yun, Yu and Zhang, Mengchao and Zhang, Yuan and Zhong, Zeqing},
  doi          = {10.1007/s11554-024-01614-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {J. Real-Time Image Process.},
  title        = {D2net: A dual-branch lightweight network for conveyor belt rotation detection in pipe belt conveyors},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PL-UNet: A real-time power line segmentation model for aerial images based on adaptive fusion and cross-stage multi-scale analysis. <em>JRTIP</em>, <em>22</em>(1), 1-12. (<a href='https://doi.org/10.1007/s11554-024-01615-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The examination of power transmission lines using UAVs (Unmanned Aerial Vehicles) is crucial for ensuring grid security. However, it is challenging for existing deep learning models to achieve a balance between accuracy and efficiency in recognizing power lines, especially when they are affected by intricate environmental backdrops and the thin structure of power lines. To address this issue, this paper proposes an improved model based on U-Net (PL-UNet), which aims to improve the ability of UAVs to recognize power lines in complex environments in real time. To reduce the model parameters, the lightweight EfficientNetV2-S is chosen as the encoder. To address the issues of information redundancy and insufficient local structure capture caused by the skip connections, we propose a multi-scale attention gate (MSAG) in the decoding part to improve the accuracy of key region feature extraction with less computational cost. Meanwhile, the dynamic weighted fusion (DWF) module is designed to effectively fuse the features through adaptive weighting to improve the flexibility of feature expression. After completing feature fusion, we further introduce a lightweight cross-stage partial pyramid block (CPPB) module, which performs multi-scale enhancement and channel optimization of the fused features through integrating multi-scale convolutional operations and separating and fusing feature channels. Finally, the hybrid loss function of weighted cross-entropy and dice is used to solve the category imbalance problem. Comparison experiments and ablation analysis are performed on top of the power line public dataset. The proposed PL-UNet has achieved 79.98% mIoU, with a parameter count of 21.27M and a detection speed of 56.86 fps. This shows that the network has a good real-time segmentation performance.},
  archive      = {J_JRTIP},
  author       = {Zhao, Qian and Fang, Haosheng and Pang, Yuye and Zhu, Gehan and Qian, Zhengzhe},
  doi          = {10.1007/s11554-024-01615-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-12},
  shortjournal = {J. Real-Time Image Process.},
  title        = {PL-UNet: A real-time power line segmentation model for aerial images based on adaptive fusion and cross-stage multi-scale analysis},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient FPGA implementation of chaos-based real-time video watermarking system in spatial and DWT domain using QIM technique. <em>JRTIP</em>, <em>22</em>(1), 1-11. (<a href='https://doi.org/10.1007/s11554-025-01620-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an efficient, lightweight, invisible, blind, real-time video watermarking system. Symmetric chaotic key encryption enhances the system’s security, ensuring robustness by randomly selecting pixels or coefficients for watermark embedding. The first-level discrete wavelet transform (DWT) is applied to selected data, embedding the watermark into the low-frequency band (LL sub-band). The approach involves random selection of data for quantization using the quantization index modulation (QIM) technique. The proposed scheme is implemented on a low-cost FPGA board (Zybo Z7-20), using a software/hardware (SW/HW) co-design approach. Experimental results demonstrate high fidelity with a peak signal-to-noise ratio (PSNR) exceeding 35 dB and normalized correlation (NC) around 0.99. The architecture achieves a balanced compromise between low FPGA area with high operational speed up to 127 MHz and minimal power consumption not exceeding 51 mW. Performance evaluation confirms the system’s robustness against various attacks, including filtering, additional noise, geometrical modifications, and contrast adjustments. This makes it highly suitable for real-time embedded video applications where data integrity is paramount.},
  archive      = {J_JRTIP},
  author       = {Aissaoui, Nour Eddine and Azzaz, Mohamed Salah and Kaibou, Redouane and Tanougast, Camel},
  doi          = {10.1007/s11554-025-01620-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient FPGA implementation of chaos-based real-time video watermarking system in spatial and DWT domain using QIM technique},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
