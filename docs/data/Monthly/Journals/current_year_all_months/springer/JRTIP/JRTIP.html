<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRTIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrtip">JRTIP - 3</h2>
<ul>
<li><details>
<summary>
(2026). Deep learning image compression with multi-channel tANS coding and hardware deployment. <em>JRTIP</em>, <em>23</em>(1), 1--14. (<a href='https://doi.org/10.1007/s11554-025-01795-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based image compression outperforms traditional methods in coding efficiency, but its computational complexity hinders real-time deployment on embedded devices. This paper proposes a heterogeneous computing system combining GPU-accelerated inference and CPU-accelerated entropy coding via lookup tables, breaking performance bottlenecks through algorithm-hardware co-design. After GPU acceleration, entropy coding becomes the dominant bottleneck (73% of runtime). To address this, we introduce three key innovations: replacing rANS with tANS encoding, converting dynamic computations into static table lookups, reducing encoding latency; a cache-friendly tANS coding scheme for the 192-channel network outputs, minimizing access latency; an out-of-range symbol encoding method, ensuring lossless and efficient compression. Experiments demonstrate that under high compression ratios, compared with traditional rANS, tANS reduces latency by 77%, with a compression ratio loss of 12.6% while still ensuring image compression quality higher than JPEG2000.},
  archive      = {J_JRTIP},
  author       = {Zhu, Yaohua and Zhang, Yong and Liu, Ya and Jiang, Jingyu and Zhu, Yanghang and Huang, Mingsheng},
  doi          = {10.1007/s11554-025-01795-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1--14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deep learning image compression with multi-channel tANS coding and hardware deployment},
  volume       = {23},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual supervision guided learning for underwater object detection. <em>JRTIP</em>, <em>23</em>(1), 1--14. (<a href='https://doi.org/10.1007/s11554-025-01792-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection (UOD) plays a crucial role in marine resource exploration and environmental conservation. To address the performance degradation caused by underwater image deterioration, several methods have been proposed to enhance image quality via underwater image enhancement (UIE) techniques to assist detection tasks. However, existing approaches often rely solely on detection loss for joint learning of UIE and UOD, limiting their effectiveness in improving detection accuracy. To overcome this limitation, we propose a dual supervision guided network (DSG-Net) for underwater object detection, which optimizes an end-to-end underwater detection model through the joint constraints of both detection loss and enhancement loss. During the training phase, we design a joint constraint loss to guide the UIE module in globally enhancing degraded images while preserving texture features critical for the detector. In the inference phase, DSG-Net uses the lightweight UIE module (LUIEM) to generate enhanced images. Through targeted processing of global and local features, it addresses challenges such as color cast and blurriness in underwater degraded images. Subsequently, a dual-branch backbone network with a multi-scale attention fusion module (MSAFM) is utilized to extract features from the original image and the enhanced image and perform adaptive fusion. This design effectively retains the discriminative features while suppressing the noise introduced by the enhancement algorithm. Considering the real-time requirements of real-world UOD applications, the design of DSG-Net has carried out lightweight design for the model size and modules. Compared with other underwater target detection models, such as ERL-Net, the number of parameters and computational load have been reduced by more than 90%.},
  archive      = {J_JRTIP},
  author       = {Yuan, Guoliang and Zhao, Liming and Li, Junchi and Chen, Hongming and Tan, Chaomin and Fu, Xianping and Wang, Yafei},
  doi          = {10.1007/s11554-025-01792-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1--14},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dual supervision guided learning for underwater object detection},
  volume       = {23},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A lightweight transformer-based framework for real-time foreign object detection in complex railway environments. <em>JRTIP</em>, <em>23</em>(1), 1--15. (<a href='https://doi.org/10.1007/s11554-025-01800-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The safe operation of railway systems critically depends on accurate and real-time detection of foreign objects intruding into track areas. To address the limitations of traditional manual monitoring methods, which often suffer from delayed response and misjudgment in complex environments, this study proposes CDH-DETR, a lightweight Transformer-based real-time detection model for railway foreign objects. Building upon the RT-DETR model, our model introduces three key structural innovations to achieve an optimal balance between accuracy, speed, and model complexity. The architecture incorporates a ContextGuided Block module to reconstruct the backbone network, substantially reducing parameter count and computational complexity while maintaining feature representation capability. Furthermore, the model employs the Dynamic upsampling (DySample) to replace traditional operations, enhancing detail perception and restoring critical features in complex scenes without increasing computational burden. Additionally, a dedicated High-level Screening-feature Fusion Pyramid Networks (HSFPN) strengthens multi-scale feature fusion and improves detection robustness for targets with significant scale variations. Experimental results on a constructed railway intrusion dataset demonstrate that CDH-DETR achieves 76.7% $$\textrm{mAP}$$ and 96.8 FPS on the HT-SD3403 embedded platform with only 14.86 MB parameters. Compared to the original RT-DETR, our model improves $$\textrm{mAP}$$ by 2.0% while reducing parameters by 27% and increasing FPS by 21.1. Ablation studies and comparative analyses confirm that each module effectively enhances model performance, and the overall architecture outperforms mainstream YOLO detection algorithms. The model maintains stable detection performance in practical video validation, demonstrating good practicality and robustness suitable for real-world railway safety monitoring applications.},
  archive      = {J_JRTIP},
  author       = {Dong, Zhe and Yang, Qing and Chen, HaoLin and Zhou, Heng and Gao, Dexin},
  doi          = {10.1007/s11554-025-01800-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1--15},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A lightweight transformer-based framework for real-time foreign object detection in complex railway environments},
  volume       = {23},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
