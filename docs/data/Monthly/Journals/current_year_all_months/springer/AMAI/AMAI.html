<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AMAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="amai">AMAI - 25</h2>
<ul>
<li><details>
<summary>
(2025). Automatic error function learning with interpretable compositional networks. <em>AMAI</em>, <em>93</em>(3), 441-469. (<a href='https://doi.org/10.1007/s10472-022-09829-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Constraint Programming, constraints are usually represented as predicates allowing or forbidding combinations of values. However, some algorithms can exploit a finer representation: error functions. By associating a function to each constraint type to evaluate the quality of an assignment, it extends the expressiveness of regular Constraint Satisfaction Problem/Constrained Optimization Problem formalisms. Their usage comes with a price though: it makes problem modeling significantly harder, since users must provide a set of error functions that are not always easy to define. Here, we propose a method to automatically learn an error function corresponding to a constraint, given its predicate version only. This is, to the best of our knowledge, the first attempt to automatically learn error functions for hard constraints. In this paper, we also give for the first time a formal definition of combinatorial problems with hard constraints represented by error functions. Our method aims to learn error functions in a supervised fashion, trying to reproduce either the Hamming or the Manhattan distance, by using a graph model we named Interpretable Compositional Networks. This model allows us to get interpretable results. We run experiments on 7 different constraints to show its versatility. Experiments show that our system can learn functions that scale to high dimensions, and can learn fairly good functions over incomplete spaces. We also show that learned error functions can be used efficiently to represent constraints in different classic problems.},
  archive      = {J_AMAI},
  author       = {Richoux, Florian and Baffier, Jean-François},
  doi          = {10.1007/s10472-022-09829-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {441-469},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Automatic error function learning with interpretable compositional networks},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-driven approach to neural architecture search initialization. <em>AMAI</em>, <em>93</em>(3), 413-440. (<a href='https://doi.org/10.1007/s10472-022-09823-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic design in neural architecture search (NAS) has received a lot of attention, aiming to improve performance and reduce computational cost. Despite the great advances made, few authors have proposed to tailor initialization techniques for NAS. However, the literature shows that a good initial set of solutions facilitates finding the optima. Therefore, in this study, we propose a data-driven technique to initialize a population-based NAS algorithm. First, we perform a calibrated clustering analysis of the search space, and second, we extract the centroids and use them to initialize a NAS algorithm. We benchmark our proposed approach against random and Latin hypercube sampling initialization using three population-based algorithms, namely a genetic algorithm, an evolutionary algorithm, and aging evolution, on CIFAR-10. More specifically, we use NAS-Bench-101 to leverage the availability of NAS benchmarks. The results show that compared to random and Latin hypercube sampling, the proposed initialization technique enables achieving significant long-term improvements for two of the search baselines, and sometimes in various search scenarios (various training budget). Besides, we also investigate how an initial population gathered on the tabular benchmark can be used for improving search on another dataset, the So2Sat LCZ-42. Our results show similar improvements on the target dataset, despite a limited training budget. Moreover, we analyse the distributions of solutions obtained and find that that the population provided by the data-driven initialization technique enables retrieving local optima (maxima) of high fitness and similar configurations.},
  archive      = {J_AMAI},
  author       = {Traoré, Kalifou René and Camero, Andrés and Zhu, Xiao Xiang},
  doi          = {10.1007/s10472-022-09823-0},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {413-440},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {A data-driven approach to neural architecture search initialization},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chance constrained conic-segmentation support vector machine with uncertain data. <em>AMAI</em>, <em>93</em>(3), 389-411. (<a href='https://doi.org/10.1007/s10472-022-09822-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machines (SVM) is one of the well known supervised machine learning model. The standard SVM models are dealing with the situation where the exact values of the data points are known. This paper studies the SVM model when the data set contains uncertain or mislabelled data points. To ensure the small probability of misclassification for the uncertain data, a chance constrained conic-segmentation SVM model is proposed for multiclass classification. Based on the data set, a mixed integer programming formulation for the chance constrained conic-segmentation SVM is derived. Kernelization of chance constrained conic-segmentation SVM model is also exploited for nonlinear classification. The geometric interpretation is presented to show how the chance constrained conic-segmentation SVM works on uncertain data. Finally, experimental results are presented to demonstrate the effectiveness of the chance constrained conic-segmentation SVM for both artificial and real-world data.},
  archive      = {J_AMAI},
  author       = {Peng, Shen and Canessa, Gianpiero and Allen-Zhao, Zhihua},
  doi          = {10.1007/s10472-022-09822-1},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {389-411},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Chance constrained conic-segmentation support vector machine with uncertain data},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online learning of variable ordering heuristics for constraint optimisation problems. <em>AMAI</em>, <em>93</em>(3), 359-388. (<a href='https://doi.org/10.1007/s10472-022-09816-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solvers for constraint optimisation problems exploit variable and value ordering heuristics. Numerous expert-designed heuristics exist, while recent research learns novel, customised heuristics from past problem instances. This article addresses unseen problems for which no historical data is available. We propose one-shot learning of customised, problem instance-specific heuristics. To do so, we introduce the concept of deep heuristics, a data-driven approach to learn extended versions of a given variable ordering heuristic online. First, for a problem instance, an initial online probing phase collects data, from which a deep heuristic function is learned. The learned heuristics can look ahead arbitrarily-many levels in the search tree instead of a ‘shallow’ localised lookahead of classical heuristics. A restart-based search strategy allows for multiple learned models to be acquired and exploited in the solver’s optimisation. We demonstrate deep variable ordering heuristics based on the smallest, anti first-fail, and maximum regret heuristics. Results on instances from the MiniZinc benchmark suite show that deep heuristics solve 20% more problem instances while improving on overall runtime for the Open Stacks and Evilshop benchmark problems.},
  archive      = {J_AMAI},
  author       = {Doolaard, Floris and Yorke-Smith, Neil},
  doi          = {10.1007/s10472-022-09816-z},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {359-388},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Online learning of variable ordering heuristics for constraint optimisation problems},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from obstructions: An effective deep learning approach for minimum vertex cover. <em>AMAI</em>, <em>93</em>(3), 347-358. (<a href='https://doi.org/10.1007/s10472-022-09813-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational intractability has for decades motivated the development of a plethora of methodologies that mainly aim at a quality-time trade-off. The use of Machine Learning has finally emerged as one of the possible tools to obtain approximate solutions to $$\mathcal {N}\mathcal {P}$$ -hard optimization problems. Recently, Dai et al. introduced a method for computing such approximate solutions for instances of the Vertex Cover problem. In this paper we consider the effectiveness of selecting a proper training strategy by considering special problem instances called obstructions that we believe carry some intrinsic properties of the problem. Capitalizing on the recent work of Dai et al. on Vertex Cover, and using the same case study as well as 19 other problem instances, we show the utility of using obstructions for training neural networks. Experiments show that training with obstructions results in a surprisingly huge reduction in number of iterations needed for convergence, thus gaining a substantial reduction in the time needed for training the model.},
  archive      = {J_AMAI},
  author       = {Abu-Khzam, Faisal N. and Abd El-Wahab, Mohamed M. and Haidous, Moussa and Yosri, Noureldin},
  doi          = {10.1007/s10472-022-09813-2},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {347-358},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Learning from obstructions: An effective deep learning approach for minimum vertex cover},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data science meets optimization II. <em>AMAI</em>, <em>93</em>(3), 343-345. (<a href='https://doi.org/10.1007/s10472-025-09980-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Zhang, Yingqian and Guns, Tias and Lombardi, Michele and De Causmaecker, Patrick},
  doi          = {10.1007/s10472-025-09980-y},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {6},
  number       = {3},
  pages        = {343-345},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Data science meets optimization II},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logic program proportions. <em>AMAI</em>, <em>93</em>(2), 321-342. (<a href='https://doi.org/10.1007/s10472-023-09904-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to present a fresh idea on how symbolic learning might be realized via analogical reasoning. For this, we introduce directed analogical proportions between logic programs of the form “P transforms into Q as R transforms into S” as a mechanism for deriving similar programs by analogy-making. The idea is to instantiate a fragment of a recently introduced abstract algebraic framework of analogical proportions in the domain of logic programming. Technically, we define proportions in terms of modularity where we derive abstract forms of concrete programs from a “known” source domain which can then be instantiated in an “unknown” target domain to obtain analogous programs. To this end, we introduce algebraic operations for syntactic logic program composition and concatenation. Interestingly, our work suggests a close relationship between modularity, generalization, and analogy which we believe should be explored further in the future. In a broader sense, this paper is a further step towards a mathematical theory of logic-based analogical reasoning and learning with potential applications to open AI-problems like commonsense reasoning and computational learning and creativity.},
  archive      = {J_AMAI},
  author       = {Antić, Christian},
  doi          = {10.1007/s10472-023-09904-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {321-342},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Logic program proportions},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study of universal morphological analysis using morpheme-based, holistic, and neural approaches under various data size conditions. <em>AMAI</em>, <em>93</em>(2), 299-319. (<a href='https://doi.org/10.1007/s10472-024-09944-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform a study on the universal morphological analysis task: given a word form, generate the lemma (lemmatisation) and its corresponding morphosyntactic descriptions (MSD analysis). Experiments are carried out on the SIGMORPHON 2018 Shared Task: Morphological Reinflection Task dataset which consists of more than 100 different languages with various morphological richness under three different data size conditions: low, medium and high. We consider three main approaches: morpheme-based (eager learning), holistic (lazy learning), and neural (eager learning). Performance is evaluated on the two subtasks of lemmatisation and MSD analysis. For the lemmatisation subtask, under all three data sizes, experimental results show that the holistic approach predicted more accurate lemmata, while the morpheme-based approach produced lemmata closer to the answers when it produces the wrong answers. For the MSD analysis subtask, under all three data sizes, the holistic approach achieves higher recall, while the morpheme-based approach is more precise. However, the trade-off between precision and recall of the two systems leads to a very similar overall F1 score. On the whole, neural approaches suffer under low resource conditions, but they achieve the best performance in comparison to the other approaches when the size of the training data increases.},
  archive      = {J_AMAI},
  author       = {Fam, Rashel and Lepage, Yves},
  doi          = {10.1007/s10472-024-09944-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {299-319},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {A study of universal morphological analysis using morpheme-based, holistic, and neural approaches under various data size conditions},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving morphological analogies: From retrieval to generation. <em>AMAI</em>, <em>93</em>(2), 263-298. (<a href='https://doi.org/10.1007/s10472-024-09945-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analogical inference is a remarkable capability of human reasoning, and has been used to solve hard reasoning tasks. Analogy based reasoning (AR) has gained increasing interest from the artificial intelligence community and has shown its potential in multiple machine learning tasks such as classification, decision making and recommendation with competitive results. We propose a deep learning (DL) framework to address and tackle two key tasks in AR: analogy detection and solving. The framework is thoroughly tested on the Siganalogies dataset of morphological analogical proportions (APs) between words, and shown to outperform symbolic approaches in many languages. Previous work have explored the behavior of the Analogy Neural Network for classification (ANNc) on analogy detection and of the Analogy Neural Network for retrieval (ANNr) on analogy solving by retrieval, as well as the potential of an autoencoder (AE) for analogy solving by generating the solution word. In this article we summarize these findings and we extend them by combining ANNr and the AE embedding model, and checking the performance of ANNc as an retrieval method. The combination of ANNr and AE outperforms the other approaches in almost all cases, and ANNc as a retrieval method achieves competitive or better performance than 3CosMul. We conclude with general guidelines on using our framework to tackle APs with DL.},
  archive      = {J_AMAI},
  author       = {Marquer, Esteban and Couceiro, Miguel},
  doi          = {10.1007/s10472-024-09945-7},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {263-298},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Solving morphological analogies: From retrieval to generation},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from masked analogies between sentences at multiple levels of formality. <em>AMAI</em>, <em>93</em>(2), 237-261. (<a href='https://doi.org/10.1007/s10472-023-09918-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the inference of sentence analogies not restricted to the formal level. We introduce MaskPrompt, a prompt-based method that addresses the analogy task as masked analogy completion. This enables us to fine-tune, in a lightweight manner, pre-trained language models on the task of reconstructing masked spans in analogy prompts. We apply constraints which are approximations of the parallelogram view of analogy to construct a corpus of sentence analogies from textual entailment sentence pairs. In the constructed corpus, sentence analogies are characterized by their level of being formal, ranging from strict to loose. We apply MaskPrompt on this corpus and compare MaskPrompt with the basic fine-tuning paradigm. Our experiments show that MaskPrompt outperforms basic fine-tuning in solving analogies in terms of overall performance, with gains of over 2% in accuracy. Furthermore, we study the contribution of loose analogies, i.e., analogies relaxed on the formal aspect. When fine-tuning with a small number of them (several hundreds), the accuracy on strict analogies jumps from 82% to 99%. This demonstrates that loose analogies effectively capture implicit but coherent analogical regularities. We also use MaskPrompt with different schemes on masked content to optimize analogy solutions. The best masking scheme during fine-tuning is to mask any term: it exhibits the highest robustness in accuracy on all tested equivalent forms of analogies.},
  archive      = {J_AMAI},
  author       = {Wang, Liyan and Lepage, Yves},
  doi          = {10.1007/s10472-023-09918-2},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {237-261},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Learning from masked analogies between sentences at multiple levels of formality},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preface to the special issue on analogies: From mathematical foundations to applications and interactions with ML and AI. <em>AMAI</em>, <em>93</em>(2), 233-235. (<a href='https://doi.org/10.1007/s10472-024-09961-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Couceiro, Miguel and Marquer, Esteban and Monnin, Pierre and Murena, Pierre-Alexandre},
  doi          = {10.1007/s10472-024-09961-7},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {4},
  number       = {2},
  pages        = {233-235},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Preface to the special issue on analogies: From mathematical foundations to applications and interactions with ML and AI},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single MCMC chain parallelisation on decision trees. <em>AMAI</em>, <em>93</em>(1), 219-232. (<a href='https://doi.org/10.1007/s10472-023-09876-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees (DT) are highly famous in machine learning and usually acquire state-of-the-art performance. Despite that, well-known variants like CART, ID3, random forest, and boosted trees miss a probabilistic version that encodes prior assumptions about tree structures and shares statistical strength between node parameters. Existing work on Bayesian DT depends on Markov Chain Monte Carlo (MCMC), which can be computationally slow, especially on high dimensional data and expensive proposals. In this study, we propose a method to parallelise a single MCMC DT chain on an average laptop or personal computer that enables us to reduce its run-time through multi-core processing while the results are statistically identical to conventional sequential implementation. We also calculate the theoretical and practical reduction in run time, which can be obtained utilising our method on multi-processor architectures. Experiments showed that we could achieve 18 times faster running time provided that the serial and the parallel implementation are statistically identical.},
  archive      = {J_AMAI},
  author       = {Drousiotis, Efthyvoulos and Spirakis, Paul},
  doi          = {10.1007/s10472-023-09876-9},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {219-232},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Single MCMC chain parallelisation on decision trees},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two parameter-tuned multi-objective evolutionary-based algorithms for zoning management in marine spatial planning. <em>AMAI</em>, <em>93</em>(1), 187-218. (<a href='https://doi.org/10.1007/s10472-023-09853-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strategic spatial planning is becoming more popular around the world as a decision-making way to build a unified vision for directing the medium- to long-term development of land/marine areas. Recently, the study of marine areas in terms of spatial planning such as Marine Spatial Planning (MSP) has received much attention. One of the challenging issues in MSP is to make a balance between determining the ideal zone for a new activity while also considering the locations of existing activities. This spatial zoning problem for multi-uses with multiple objectives could be formulated as optimization models. This paper presents and compares the results of two multi-objective evolutionary-based algorithms (MOEAs), Synchronous Hypervolume-based non-dominated sorting genetic algorithm-II (SH-NSGA-II) which is an extension of NSGA-II and a memetic algorithm (MA) in which SH-NSGA-II is enhanced with a local search. These proposed algorithms are used to solve the multi-objective spatial zoning optimization problem, which seeks to maximize the zone interest value assigned to the new activity while simultaneously maximizing its spatial compactness. We introduce several innovations in these proposed algorithms to address the problem constraints and to improve the robustness of the traditional NSGA-II and MA approaches. Unlike traditional ones, a different stop condition, multiple crossover, mutation, and repairing operators, and also a local search operator are developed. A comparative study is presented between the results obtained using both algorithms. To guarantee robust results for both algorithms, their parameters are calibrated and tuned using the Multi-Response Surface Methodology (MRSM) method. The effective and non-effective components, as well as the validity of the regression models, are determined using analysis of variance (ANOVA). Although SH-NSGA-II has revealed a good efficiency, its performance is still improved using a local search scheme within SH-NSGA-II, which is specially tailored to the problem characteristics. The two methods are designed for raster data.},
  archive      = {J_AMAI},
  author       = {Basirati, Mohadese and Billot, Romain and Meyer, Patrick},
  doi          = {10.1007/s10472-023-09853-2},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {187-218},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Two parameter-tuned multi-objective evolutionary-based algorithms for zoning management in marine spatial planning},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clique detection with a given reliability. <em>AMAI</em>, <em>93</em>(1), 173-186. (<a href='https://doi.org/10.1007/s10472-024-09928-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new notion of a clique reliability. The clique reliability is understood as the ratio of the number of statistically significant links in a clique to the number of edges of the clique. This notion relies on a recently proposed original technique for separating inferences about pairwise connections between vertices of a network into significant and admissible ones. In this paper, we propose an extension of this technique to the problem of clique detection. We propose a method of step-by-step construction of a clique with a given reliability. The results of constructing cliques with a given reliability using data on the returns of stocks included in the Dow Jones index are presented.},
  archive      = {J_AMAI},
  author       = {Semenov, Dmitry and Koldanov, Alexander and Koldanov, Petr and Pardalos, Panos},
  doi          = {10.1007/s10472-024-09928-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {173-186},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Clique detection with a given reliability},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing doubly stochastic matrices for average consensus through swarm and evolutionary algorithms. <em>AMAI</em>, <em>93</em>(1), 151-171. (<a href='https://doi.org/10.1007/s10472-023-09912-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doubly-stochastic matrices play a vital role in modern applications of complex networks such as tracking and decentralized state estimation, coordination and control of autonomous agents. A central theme in all of the above is consensus, that is, nodes reaching agreement about the value of an underlying variable (e.g. the state of the environment). Despite the fact that complex networks have been studied thoroughly, the communication graphs are usually described by symmetric matrices due to their advantageous theoretical properties. We do not yet have methods for optimizing generic doubly-stochastic matrices. In this paper, we propose a novel formulation and framework, EvoDSM, for achieving fast linear distributed averaging by: (a) optimizing the weights of a fixed graph topology, and (b) optimizing for the topology itself. We are concerned with graphs that can be described by positive doubly-stochastic matrices. Our method relies on swarm and evolutionary optimization algorithms and our experimental results and analysis showcase that our method (1) achieves comparable performance with traditional methods for symmetric graphs, (2) is applicable to non-symmetric network structures and edge weights, and (3) is scalable and can operate effectively with moderately large graphs without engineering overhead.},
  archive      = {J_AMAI},
  author       = {Syriopoulos, Panos K. and Chatzilygeroudis, Konstantinos I. and Kalampalikis, Nektarios G. and Vrahatis, Michael N.},
  doi          = {10.1007/s10472-023-09912-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {151-171},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Optimizing doubly stochastic matrices for average consensus through swarm and evolutionary algorithms},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel method for solving universum twin bounded support vector machine in the primal space. <em>AMAI</em>, <em>93</em>(1), 131-150. (<a href='https://doi.org/10.1007/s10472-023-09896-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised learning, the Universum, a third class that is not a part of either class in the classification task, has proven to be useful. In this study we propose (N $$ \mathfrak {U} $$ TBSVM), a Newton-based approach for solving in the primal space the optimization problems related to Twin Bounded Support Vector Machines with Universum data ( $$ \mathfrak {U} $$ TBSVM). In the N $$ \mathfrak {U} $$ TBSVM, the constrained programming problems of $$ \mathfrak {U} $$ TBSVM are converted into unconstrained optimization problems, and a generalization of Newton’s method for solving the unconstrained problems is introduced. Numerical experiments on synthetic, UCI, and NDC data sets show the ability and effectiveness of the proposed N $$ \mathfrak {U} $$ TBSVM. We apply the suggested method for gender detection from face images, and compare it with other methods.},
  archive      = {J_AMAI},
  author       = {Moosaei, Hossein and Khosravi, Saeed and Bazikar, Fatemeh and Hladík, Milan and Rosario Guarracino, Mario},
  doi          = {10.1007/s10472-023-09896-5},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {131-150},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {A novel method for solving universum twin bounded support vector machine in the primal space},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Realtime gray-box algorithm configuration using cost-sensitive classification. <em>AMAI</em>, <em>93</em>(1), 109-130. (<a href='https://doi.org/10.1007/s10472-023-09890-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A solver’s runtime and the quality of the solutions it generates are strongly influenced by its parameter settings. Finding good parameter configurations is a formidable challenge, even for fixed problem instance distributions. However, when the instance distribution can change over time, a once effective configuration may no longer provide adequate performance. Realtime algorithm configuration (RAC) offers assistance in finding high-quality configurations for such distributions by automatically adjusting the configurations it recommends based on instances seen so far. Existing RAC methods treat the solver as a black box, meaning the solver is given a configuration as input, and it outputs either a solution or runtime as an objective function for the configurator. However, analyzing intermediate output from the solver can enable configurators to avoid wasting time on poorly performing configurations. We propose a gray-box approach that utilizes intermediate output during evaluation and implement it within the RAC method Contextual Preselection with Plackett-Luce (CPPL blue). We apply cost-sensitive machine learning with pairwise comparisons to determine whether ongoing evaluations can be terminated to free resources. We compare our approach to a black-box equivalent on several experimental settings and show that our approach reduces the total solving time in several scenarios and improves solution quality in an additional scenario.},
  archive      = {J_AMAI},
  author       = {Weiss, Dimitri and Tierney, Kevin},
  doi          = {10.1007/s10472-023-09890-x},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {109-130},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Realtime gray-box algorithm configuration using cost-sensitive classification},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel SVM-based classification approaches for evaluating pancreatic carcinoma. <em>AMAI</em>, <em>93</em>(1), 93-108. (<a href='https://doi.org/10.1007/s10472-023-09888-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop two SVM-based classifiers named stable nested one-class support vector machines (SN-1SVMs) and decoupled margin-moment based SVMs (DMMB-SVMs), to predict the specific type of pancreatic carcinoma using quantitative histopathological signatures of images. For each patient, the diagnosis can produce hundreds of images, which can be used to classify the pancreatic tissues into three classes: chronic pancreatitis, intraductal papillary mucinous neoplasms, and pancreatic carcinoma. The proposed two approaches tackle the classification problems from two different perspectives: the SN-1SVM treats each image as a classification point in a nested fashion to predict malignancy of the tissues, while the DMMB-SVM treats each patient as a classification point by assembling information across images. One attractive feature of the DMMB-SVM is that, in addition to utilizing the mean information, it also takes into account the covariance of features extracted from images for each patient. We conduct numerical experiments to evaluate and compare performance of the two methods. It is observed that the SN-1SVM can take advantage of the data structure more effectively, while the DMMB-SVM demonstrates better computational efficiency and classification accuracy. To further improve interpretability of the final classifier, we also consider the $$\ell _1$$ -norm in the DMMB-SVM to handle feature selection.},
  archive      = {J_AMAI},
  author       = {Washburn, Ammon and Fan, Neng and Zhang, Hao Helen},
  doi          = {10.1007/s10472-023-09888-5},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {93-108},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Novel SVM-based classification approaches for evaluating pancreatic carcinoma},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian optimization over the probability simplex. <em>AMAI</em>, <em>93</em>(1), 77-91. (<a href='https://doi.org/10.1007/s10472-023-09883-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian Process based Bayesian Optimization is largely adopted for solving problems where the inputs are in Euclidean spaces. In this paper we associate the inputs to discrete probability distributions which are elements of the probability simplex. To search in the new design space, we need a distance between distributions. The optimal transport distance (aka Wasserstein distance) is chosen due to its mathematical structure and the computational strategies enabled by it. Both the GP and the acquisition function is generalized to an acquisition functional over the probability simplex. To optimize this functional two methods are proposed, one based on auto differentiation and the other based on proximal-point algorithm and the gradient flow. Finally, we report a preliminary set of computational results on a class of problems whose dimension ranges from 5 to 100. These results show that embedding the Bayesian optimization process in the probability simplex enables an effective algorithm whose performance over standard Bayesian optimization improves with the increase of problem dimensionality.},
  archive      = {J_AMAI},
  author       = {Candelieri, Antonio and Ponti, Andrea and Archetti, Francesco},
  doi          = {10.1007/s10472-023-09883-w},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {77-91},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Bayesian optimization over the probability simplex},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KNN classification: A review. <em>AMAI</em>, <em>93</em>(1), 43-75. (<a href='https://doi.org/10.1007/s10472-023-09882-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-nearest neighbors (k/NN) algorithm is a simple yet powerful non-parametric classifier that is robust to noisy data and easy to implement. However, with the growing literature on k/NN methods, it is increasingly challenging for new researchers and practitioners to navigate the field. This review paper aims to provide a comprehensive overview of the latest developments in the k/NN algorithm, including its strengths and weaknesses, applications, benchmarks, and available software with corresponding publications and citation analysis. The review also discusses the potential of k/NN in various data science tasks, such as anomaly detection, dimensionality reduction and missing value imputation. By offering an in-depth analysis of k/NN, this paper serves as a valuable resource for researchers and practitioners to make informed decisions and identify the best k/NN implementation for a given application.},
  archive      = {J_AMAI},
  author       = {Syriopoulos, Panos K. and Kalampalikis, Nektarios G. and Kotsiantis, Sotiris B. and Vrahatis, Michael N.},
  doi          = {10.1007/s10472-023-09882-x},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {43-75},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {KNN classification: A review},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved multi-task least squares twin support vector machine. <em>AMAI</em>, <em>93</em>(1), 21-41. (<a href='https://doi.org/10.1007/s10472-023-09877-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-task learning (MTL) has become a popular field in machine learning and has a key role in various domains. Sharing knowledge across tasks in MTL can improve the performance of learning algorithms and enhance their generalization capability. A new approach called the multi-task least squares twin support vector machine (MTLS-TSVM) was recently proposed as a least squares variant of the direct multi-task twin support vector machine (DMTSVM). Unlike DMTSVM, which solves two quadratic programming problems, MTLS-TSVM solves two linear systems of equations, resulting in a reduced computational time. In this paper, we propose an enhanced version of MTLS-TSVM called the improved multi-task least squares twin support vector machine (IMTLS-TSVM). IMTLS-TSVM offers a significant advantage over MTLS-TSVM by operating based on the empirical risk minimization principle, which allows for better generalization performance. The model achieves this by including regularization terms in its objective function, which helps control the model’s complexity and prevent overfitting. We demonstrate the effectiveness of IMTLS-TSVM by comparing it to several single-task and multi-task learning algorithms on various real-world data sets. Our results highlight the superior performance of IMTLS-TSVM in addressing multi-task learning problems.},
  archive      = {J_AMAI},
  author       = {Moosaei, Hossein and Bazikar, Fatemeh and Pardalos, Panos M.},
  doi          = {10.1007/s10472-023-09877-8},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {21-41},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {An improved multi-task least squares twin support vector machine},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Revised selected papers from the LION 16 conference. <em>AMAI</em>, <em>93</em>(1), 19-20. (<a href='https://doi.org/10.1007/s10472-024-09958-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Kotsireas, Ilias S. and Pardalos, Panos M.},
  doi          = {10.1007/s10472-024-09958-2},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {19-20},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Guest editorial: Revised selected papers from the LION 16 conference},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep data density estimation through donsker-varadhan representation. <em>AMAI</em>, <em>93</em>(1), 7-17. (<a href='https://doi.org/10.1007/s10472-024-09943-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the data density is one of the challenging problem topics in the deep learning society. In this paper, we present a simple yet effective methodology for estimating the data density using the Donsker-Varadhan variational lower bound on the KL divergence and the modeling based on the deep neural network. We demonstrate that the optimal critic function associated with the Donsker-Varadhan representation on the KL divergence between the data and the uniform distribution can estimate the data density. Also, we present the deep neural network-based modeling and its stochastic learning procedure. The experimental results and possible applications of the proposed method demonstrate that it is competitive with the previous methods for data density estimation and has a lot of possibilities for various applications.},
  archive      = {J_AMAI},
  author       = {Park, Seonho and Pardalos, Panos M.},
  doi          = {10.1007/s10472-024-09943-9},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {7-17},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {Deep data density estimation through donsker-varadhan representation},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The future starts now. <em>AMAI</em>, <em>93</em>(1), 5-6. (<a href='https://doi.org/10.1007/s10472-025-09970-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Dix, Jürgen and Fisher, Michael},
  doi          = {10.1007/s10472-025-09970-0},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {5-6},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {The future starts now},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 35 years of math and AI. <em>AMAI</em>, <em>93</em>(1), 1-3. (<a href='https://doi.org/10.1007/s10472-025-09969-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AMAI},
  author       = {Golumbic, Martin Charles},
  doi          = {10.1007/s10472-025-09969-7},
  journal      = {Annals of Mathematics and Artificial Intelligence},
  month        = {2},
  number       = {1},
  pages        = {1-3},
  shortjournal = {Ann. Math. Artif. Intell.},
  title        = {35 years of math and AI},
  volume       = {93},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
