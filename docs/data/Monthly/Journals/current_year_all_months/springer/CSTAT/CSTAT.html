<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSTAT</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cstat">CSTAT - 152</h2>
<ul>
<li><details>
<summary>
(2025). A bootstrap-based bandwidth selection rule for kernel quantile estimators. <em>CSTAT</em>, <em>40</em>(7), 4037-4058. (<a href='https://doi.org/10.1007/s00180-024-01582-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantile has been widely used to quantify the uncertainty in many fields. In this paper, we study the estimation of quantiles via kernels, especially for extreme quantiles, and propose a bootstrap-based bandwidth selection (BBS) method for it. This method employs bootstrap sampling of data and least-squares regression to estimate the unknown bandwidth parameter in the kernel, which plays a crucial role in kernel smoothing. From a theoretical perspective, we establish a data-driven and bootstrap-based kernel quantile estimator and provide its asymptotic bias and variance, based on which the proposed method is shown to lead to the asymptotically optimal bandwidth selection in terms of minimizing the mean squared error. Numerical experiments demonstrate that the BBS method works well in both bandwidth selection and extreme quantile estimation.},
  archive      = {J_CSTAT},
  author       = {Liu, Xiaoyu and Song, Yan and Cheng, Hong-Fa and Zhang, Kun},
  doi          = {10.1007/s00180-024-01582-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {4037-4058},
  shortjournal = {Comput. Stat.},
  title        = {A bootstrap-based bandwidth selection rule for kernel quantile estimators},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic data generation method providing enhanced covariance matrix estimation. <em>CSTAT</em>, <em>40</em>(7), 4007-4035. (<a href='https://doi.org/10.1007/s00180-025-01643-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic data generation is an important tool to ensure data confidentiality. Various synthetic data generators have been developed in the literature. The methods in the literature are mostly for general purposes. They aim to generate data whose distributions are the same as the original data set, and the synthesized data are used for every purpose depending on who uses them. However, it could not be good for all purposes. In this paper, we study the synthetic data generation tailored for a specific purpose. We are particularly interested incovariance matrix estimation, which is a key part of many multivariate statistical analyses. To do it, we first see the connection between the sequential regression model and the modified Cholesky decomposition. We then devise a new synthetic data generator, named SynCov, that controls the error variances of the sequential regression model. We show that the sample covariance matrix of the synthetic data generated by SynCov is equivalent to a shrinkage covariance matrix estimator, which reduces estimation error in Frobenius norm. Our comprehensive numerical study shows that SynCov performs better than other synthetic data generation methods in covariance matrix estimation. Finally, we apply our SynCov to two real data examples, (i) the estimation of the covariance matrix of the (selected) variables of the Los Angeles City Employee Payroll data and (ii) the classification of the Taiwanese Bankruptcy Data.},
  archive      = {J_CSTAT},
  author       = {Kim, Seungkyu and Lim, Johan and Yu, Donghyeon},
  doi          = {10.1007/s00180-025-01643-0},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {4007-4035},
  shortjournal = {Comput. Stat.},
  title        = {Synthetic data generation method providing enhanced covariance matrix estimation},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially functional linear expectile regression model with missing observations. <em>CSTAT</em>, <em>40</em>(7), 3981-4005. (<a href='https://doi.org/10.1007/s00180-025-01652-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate estimation for the partially functional linear expectile regression model where observations are missing at random (MAR). First, we construct expectile regression (ER) estimators for both the slope functions and scalar parameters. Second, to obtain confidence intervals for the scalar parameters, we propose both the multiplier bootstrap method and the empirical likelihood (EL) method. Meanwhile, the maximum empirical likelihood (MEL) estimators for the scalar parameters are derived using the empirical log-likelihood ratio function. Furthermore, under mild conditions, we establish several asymptotic properties, including the convergence rates of the ER estimators for the scalar parameters and the slope function, the asymptotic normality of the ER estimators and the MEL estimators for the scalar parameters, and the convergence of the empirical log-likelihood ratio function to the standard chi-squared distribution. Finally, simulation studies and a real data analysis are conducted to evaluate the performance of the proposed methods.},
  archive      = {J_CSTAT},
  author       = {Wu, Chengxin and Ling, Nengxiang},
  doi          = {10.1007/s00180-025-01652-z},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3981-4005},
  shortjournal = {Comput. Stat.},
  title        = {Partially functional linear expectile regression model with missing observations},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning causal graphs using variable grouping according to ancestral relationship. <em>CSTAT</em>, <em>40</em>(7), 3947-3979. (<a href='https://doi.org/10.1007/s00180-025-01633-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the sample size is small relative to the number of variables, the accuracy of the conventional causal learning algorithm decreases. Some causal discovery methods are not feasible when the sample size is smaller than the number of variables. To circumvent these problems, some researchers proposed causal discovery algorithms using divide-and-conquer approaches (e.g., Cai et al. in Sada: a general framework to support robust causation discovery. In: International Conference on machine learning, PMLR, pp 208–216, 2013; Zhang et al. in IEEE Trans Cybern 52:3232–3243, 2020). For learning an entire causal graph, divide-and-conquer approaches first split variables into several subsets according to the conditional independence relationships among the variables, then apply a conventional causal discovery algorithm to each subset and merge the estimated results. Since the divide-and-conquer approach reduces the number of variables to which a causal discovery algorithm is applied, it is expected to improve the estimation accuracy, especially when the sample size is small relative to the number of variables and the model is sparse. However, existing methods are computationally expensive or do not provide sufficient accuracy when the sample size is small. This paper proposes a new algorithm for grouping variables according to the causal ancestral relationships, assuming that the causal model is LiNGAM (Shimizu et al. J Mach Learn Res 7:2003–2030, 2006). We call the proposed algorithm the causal ancestral-relationship-based grouping (CAG). The time complexity of the ancestor finding in the CAG is shown to be cubic in the number of variables. Extensive computer experiments confirm that the proposed method outperforms the original DirectLiNGAM (Shimizu et al. in J Mach Learn Res-JMLR 12:1225–1248, 2011) and other divide-and-conquer approaches not only in estimation accuracy but also in computation time when the sample size is small relative to the number of variables and the causal model is sparse or moderately dense. We also apply the proposed method to two real datasets to confirm its usefulness.},
  archive      = {J_CSTAT},
  author       = {Cai, Ming and Hara, Hisayuki},
  doi          = {10.1007/s00180-025-01633-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3947-3979},
  shortjournal = {Comput. Stat.},
  title        = {Learning causal graphs using variable grouping according to ancestral relationship},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical relations among principal component and factor analysis procedures elucidated from a comprehensive model. <em>CSTAT</em>, <em>40</em>(7), 3911-3946. (<a href='https://doi.org/10.1007/s00180-025-01611-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this review article, the term “hierarchy” is related to constrained-ness, but not to superiority. Procedures A and B forming a hierarchy means that A is a constrained variant of B or vice versa. A goal of this article is to present a hierarchy of principal component analysis (PCA) and factor analysis (FA) procedures, which follows from a comprehensive FA (CompFA) model. This model can be regarded as a hybrid of PCA and prevalent FA models. First, we show how a non-random version of the CompFA model leads to the following hierarchy: PCA is a constrained variant of completely decomposed FA, which itself is a constrained variant of matrix decomposition FA. Then, we prove that a random version of the CompFA model leads to minimum rank FA (MRFA) and constraining MRFA leads to random PCA (RPCA), so as to present the following hierarchy: Probabilistic PCA is a constrained variant of prevalent FA, and the latter is a constrained variant of RPCA, which is itself a constrained variant of MRFA. Finally, this hierarchy and the above hierarchy following from the non-random version are unified into one. We further utilize the unified hierarchy to present a strategy for selecting a procedure suitable to a data set.},
  archive      = {J_CSTAT},
  author       = {Adachi, Kohei},
  doi          = {10.1007/s00180-025-01611-8},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3911-3946},
  shortjournal = {Comput. Stat.},
  title        = {Hierarchical relations among principal component and factor analysis procedures elucidated from a comprehensive model},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-expanded ECME algorithms for logistic and penalized logistic regression. <em>CSTAT</em>, <em>40</em>(7), 3883-3909. (<a href='https://doi.org/10.1007/s00180-025-01619-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter estimation in logistic regression is a well-studied problem with the Newton–Raphson method being one of the most prominent optimization techniques used in practice. A number of monotone optimization methods including minorization-maximization (MM) algorithms, expectation-maximization (EM) algorithms and related variational Bayes approaches offer useful alternatives guaranteed to increase the logistic regression likelihood at every iteration. In this article, we propose and evaluate an optimization procedure that is based on a straightforward modification of an EM algorithm for logistic regression. Our method can substantially improve the computational efficiency of the EM algorithm while preserving the monotonicity of EM and the simplicity of the EM parameter updates. By introducing an additional latent parameter and selecting this parameter to maximize the penalized observed-data log-likelihood at every iteration, our iterative algorithm can be interpreted as a parameter-expanded expectation-conditional maximization either (ECME) algorithm, and we demonstrate how to use the parameter-expanded ECME with an arbitrary choice of weights and penalty function. In addition, we describe a generalized version of our parameter-expanded ECME algorithm that can be tailored to the challenges encountered in specific high-dimensional problems, and we study several interesting connections between this generalized algorithm and other well-known methods. Performance comparisons between our method, the EM algorithm, Newton–Raphson, and several other optimization methods are presented using an extensive series of simulation studies based upon both real and synthetic datasets.},
  archive      = {J_CSTAT},
  author       = {Henderson, Nicholas C. and Ouyang, Zhongzhe},
  doi          = {10.1007/s00180-025-01619-0},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3883-3909},
  shortjournal = {Comput. Stat.},
  title        = {Parameter-expanded ECME algorithms for logistic and penalized logistic regression},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian nonparametric hypothesis testing methods on multiple comparisons. <em>CSTAT</em>, <em>40</em>(7), 3867-3882. (<a href='https://doi.org/10.1007/s00180-025-01615-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce Bayesian testing procedures based on the Bayes factor to compare the means across multiple populations in classical nonparametric contexts. The proposed Bayesian methods are designed to maximize the probability of rejecting the null hypothesis when the Bayes factor exceeds a specified evidence threshold. It is shown that these procedures have straightforward closed-form expressions based on classical nonparametric test statistics and their corresponding critical values, allowing for easy computation. We also demonstrate that they effectively control Type I error and enable researchers to make consistent decisions aligned with both frequentist and Bayesian approaches, provided that the evidence threshold for the Bayesian methods is set according to the significance level of the frequentist tests. Importantly, the proposed approaches allow for the quantification of evidence from empirical data in favor of the null hypothesis, an advantage that frequentist methods lack, as they cannot quantify support for the null when the null hypothesis is not rejected. We also present simulation studies and real-world applications to illustrate the performance of the proposed testing procedures.},
  archive      = {J_CSTAT},
  author       = {Hai, Qiuchen and Ma, Zhuanzhuan},
  doi          = {10.1007/s00180-025-01615-4},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3867-3882},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian nonparametric hypothesis testing methods on multiple comparisons},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Derandomized truncated D-vine copula knockoffs with e-values to control the false discovery rate. <em>CSTAT</em>, <em>40</em>(7), 3843-3866. (<a href='https://doi.org/10.1007/s00180-024-01587-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Model-X knockoffs is a practical methodology for variable selection, which stands out from other selection strategies since it allows for the control of the false discovery rate, relying on finite-sample guarantees. In this article, we propose a Truncated D-vine Copula Knockoffs (TDCK) algorithm for sampling approximate knockoffs from complex multivariate distributions. Our algorithm enhances and improves features of previous attempts to sample knockoffs under the multivariate setting, with the three main contributions being: (1) the truncation of the D-vine copula, which reduces the dependence between the original variables and their corresponding knockoffs, thus improving the statistical power; (2) the employment of a straightforward non-parametric formulation for marginal transformations, eliminating the need for a specific parametric family or a kernel density estimator; (3) the use of the “rvinecopulib” R package offers better flexibility than the existing fitting vine copula knockoff methods. To eliminate the randomness from the different sets of selected variables in distinct realizations, we wrap the TDCK method with an existing derandomizing procedure for knockoffs, leading to a Derandomized Truncated D-vine Copula Knockoffs with e-values (DTDCKe) procedure. We demonstrate the robustness of the DTDCKe procedure under various scenarios with extensive simulation studies. We further illustrate its efficacy using a gene expression dataset, showing it achieves a more reliable gene selection than other competing methods when the findings are compared with those of a meta-analysis. The results indicate that our Truncated D-vine copula approach is robust and has superior power, representing an appealing approach for variable selection in different multivariate applications, particularly in gene expression analysis.},
  archive      = {J_CSTAT},
  author       = {Vásquez, Alejandro Román and Márquez Urbina, José Ulises and González Farías, Graciela and Escarela, Gabriel},
  doi          = {10.1007/s00180-024-01587-x},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3843-3866},
  shortjournal = {Comput. Stat.},
  title        = {Derandomized truncated D-vine copula knockoffs with e-values to control the false discovery rate},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated fitting of joint models of survival and longitudinal data with cumulative variations. <em>CSTAT</em>, <em>40</em>(7), 3819-3842. (<a href='https://doi.org/10.1007/s00180-025-01639-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been well recognized that not only biomarkers but also their variability are important for predicting biomarker-related diseases. Understanding and adequately modeling the variability of biomarkers is crucial for detecting and predicting health risks, leading to improved health outcomes and patient care. However, biomarker variability modeling comes with a high computational cost, as statistical models incorporating biomarkers’ variability rely on double integrals with two nested integrations, which must be repeatedly calculated during modeling. To reduce the computational burden, we propose a novel approach aligned with arc length in mathematics to approximate and model biomarker fluctuations. Furthermore, we propose an algorithm that aligns with fast arc length evaluations for the joint modeling of survival and longitudinal data. We synthesize multiple efficient computing methods into a unified framework to accelerate the entire computational process. The core component of the acceleration is the computational efficiency of the double integrals, even when the iterated integral representation of the double integral is not possible. Finally, we illustrate the usage and benefit of our algorithm in joint models in numerical examples and the primary biliary cholangitis clinical study.},
  archive      = {J_CSTAT},
  author       = {Gao, Yan and Sparapani, Rodney A. and Tarima, Sergey},
  doi          = {10.1007/s00180-025-01639-w},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3819-3842},
  shortjournal = {Comput. Stat.},
  title        = {Accelerated fitting of joint models of survival and longitudinal data with cumulative variations},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of stress–strength reliability for the generalized inverted exponential distribution based on improved adaptive type-II progressive censoring. <em>CSTAT</em>, <em>40</em>(7), 3781-3817. (<a href='https://doi.org/10.1007/s00180-025-01612-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to estimate the reliability of a stress–strength system using the generalized inverted exponential distribution (GIED). We achieve this by employing an improved adaptive Type-II progressive censoring scheme and utilizing various estimation techniques. The techniques used include maximum likelihood estimation through the EM algorithm and Bayesian inference. We use Markov chain Monte Carlo (MCMC) methods and TK approximation in the Bayesian framework. We compute various intervals, such as asymptotic confidence, arcsin transformed, Bayesian credible, and higher posterior density confidence intervals. To guide the estimation process, we use a generalized entropy loss function. Additionally, we conduct a comprehensive simulation analysis to validate the method’s performance and rigorously assess its applicability through real-life data analysis.},
  archive      = {J_CSTAT},
  author       = {Swaroop, Chatany and Dutta, Subhankar and Saini, Shubham and Tiwari, Neeraj},
  doi          = {10.1007/s00180-025-01612-7},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3781-3817},
  shortjournal = {Comput. Stat.},
  title        = {Estimation of stress–strength reliability for the generalized inverted exponential distribution based on improved adaptive type-II progressive censoring},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive importance sampling for locally stable point processes. <em>CSTAT</em>, <em>40</em>(7), 3745-3779. (<a href='https://doi.org/10.1007/s00180-025-01609-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding the expected value of a statistic of a locally stable point process in a bounded region is addressed. We propose an adaptive importance sampling for solving the problem. In our proposal, we restrict the importance point process to the family of homogeneous Poisson point processes, which enables us to generate quickly independent samples of the importance point process. The optimal intensity of the importance point process is found by applying the cross-entropy minimization method. In the proposed scheme, the expected value of the statistic and the optimal intensity are iteratively estimated in an adaptive manner. We show that the proposed estimator converges to the target value almost surely, and prove the asymptotic normality of it. We explain how to apply the proposed scheme to the estimation of the intensity of a stationary pairwise interaction point process. The performance of the proposed scheme is compared numerically with Markov chain Monte Carlo simulation and perfect sampling.},
  archive      = {J_CSTAT},
  author       = {Kang, Hee-Geon and Kim, Sunggon},
  doi          = {10.1007/s00180-025-01609-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3745-3779},
  shortjournal = {Comput. Stat.},
  title        = {An adaptive importance sampling for locally stable point processes},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On practical implementation of the fully robust one-sided cross-validation method in the nonparametric regression and density estimation contexts. <em>CSTAT</em>, <em>40</em>(7), 3715-3743. (<a href='https://doi.org/10.1007/s00180-025-01602-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fully robust one-sided cross-validation (OSCV) method has versions in the nonparametric regression and density estimation settings. It selects the consistent bandwidths for estimating the continuous regression and density functions that might have finitely many discontinuities in their first derivatives. The theoretical results underlying the method were thoroughly elaborated in the preceding publications, while its practical implementations needed improvement. In particular, until this publication, no appropriate implementation of the method existed in the density estimation context. In the regression setting, the previously proposed implementation has a serious disadvantage of occasionally producing the irregular OSCV functions that complicates the bandwidth selection procedure. In this article, we make a substantial progress towards resolving the aforementioned issues by proposing a suitable implementation of fully robust OSCV for density estimation and providing specific recommendations for the further improvement of the method in the regression setting.},
  archive      = {J_CSTAT},
  author       = {Savchuk, Olga},
  doi          = {10.1007/s00180-025-01602-9},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3715-3743},
  shortjournal = {Comput. Stat.},
  title        = {On practical implementation of the fully robust one-sided cross-validation method in the nonparametric regression and density estimation contexts},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the economic-statistical performance of variable acceptance sampling plans based on loss function. <em>CSTAT</em>, <em>40</em>(7), 3665-3713. (<a href='https://doi.org/10.1007/s00180-024-01581-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acceptance sampling plans (ASPs) for attributes are sometimes misapplied to normal quality characteristics. When inspection costs and quality levels are high, using variable ASPs (VASPs) can be preferable. Among developed approaches to design ASPs, few studies have incorporated losses into the cost objective function. Their limited attention, such as focusing on limited random scenarios, considering only the activation of one specification limit, failing to compare VASPs with military standards still in use, and relying on time-consuming solution procedures, motivated us to utilize the advantages of loss-based economic-statistical design, evaluate four VASPs and two military standards, and presenting detailed results. Additionally, we develop the first Particle swarm optimization (PSO)-based solution procedure for designing VASPs. Numerical and real case studies, which consider the activation of lower and upper specification limits, demonstrate the superior performance of (1) the repetitive group sampling plan, (2) MIL-STD-414 over MIL-STD-105E, and (3) PSO compared to other approaches.},
  archive      = {J_CSTAT},
  author       = {Jafarian-Namin, Samrad and Fattahi, Parviz and Salmasnia, Ali},
  doi          = {10.1007/s00180-024-01581-3},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3665-3713},
  shortjournal = {Comput. Stat.},
  title        = {Assessing the economic-statistical performance of variable acceptance sampling plans based on loss function},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient-based smoothing parameter estimation for neural P-splines. <em>CSTAT</em>, <em>40</em>(7), 3645-3663. (<a href='https://doi.org/10.1007/s00180-024-01593-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the popularity of deep learning models there have recently been many attempts to translate generalized additive models to neural nets. Generalized additive models are usually regularized by a penalty in the loss function and the magnitude of penalization is controlled by one or more smoothing parameters. In the statistical literature these smoothing parameters are estimated by criteria such as generalized cross-validation or restricted maximum likelihood. While the estimation of the primary regression coefficients is well calibrated and investigated for neural net based additive models, the estimation of smoothing parameters is often either based on testing data (and grid search), implicitly estimated or completely neglected. In this paper, we address the issue of explicit smoothing parameter estimation in neural net-based additive models fitted via gradient-based methods, such as the well-known Adam algorithm. We therefore investigate the data-driven smoothing parameter selection via gradient-based optimization of generalized cross-validation and restricted maximum likelihood. Thus we do not need to calculate Hessian information of the smoothing parameters. As an additive model structure, we use a translation of P-splines to neural nets, so-called neural P-splines. The fitting process of neural P-splines as well as the gradient-based smoothing parameter selection are investigated in a simulation study and an application.},
  archive      = {J_CSTAT},
  author       = {Dammann, Lea M. and Freitag, Marei and Thielmann, Anton and Säfken, Benjamin},
  doi          = {10.1007/s00180-024-01593-z},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3645-3663},
  shortjournal = {Comput. Stat.},
  title        = {Gradient-based smoothing parameter estimation for neural P-splines},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel-diffeomorphism bayesian bootstrap filter to reduce speckle noise on SAR images. <em>CSTAT</em>, <em>40</em>(7), 3613-3643. (<a href='https://doi.org/10.1007/s00180-025-01650-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite imagery is frequently subject to degradation by noise during both image acquisition and transmission processes. The primary goal of noise reduction techniques is to remove Speckle noise while retaining critical features of the images. In remote sensing applications, Synthetic Aperture Radar (SAR) imagery plays a vital role. Speckle, a granular disturbance typically modelled as multiplicative noise, impacts SAR images as well as all coherent images, resulting in a reduction in image quality. Over the past three decades, numerous techniques have been proposed to mitigate Speckle noise in SAR imagery. This study proposes the Kernel-Diffeomorphism Bayesian Bootstrap Filter (KDBBF) as a novel method for satellite image restoration. The method relies on the multivariate Kernel Diffeomorphism estimator and the Bayesian Bootstrap Filter (BBF). Comparative analyses of the results produced by the new method with those of other image restoration techniques reveal superior performance in Speckle noise reduction in SAR imagery, both quantitatively and qualitatively.},
  archive      = {J_CSTAT},
  author       = {Zribi, Mourad and Sadok, Ibrahim and Marhaba, Bassel},
  doi          = {10.1007/s00180-025-01650-1},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3613-3643},
  shortjournal = {Comput. Stat.},
  title        = {Kernel-diffeomorphism bayesian bootstrap filter to reduce speckle noise on SAR images},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable selection method based on BIC with consistency for non-zero partial correlations under a large-dimensional setting. <em>CSTAT</em>, <em>40</em>(7), 3585-3611. (<a href='https://doi.org/10.1007/s00180-025-01628-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of selecting non-zero partial correlations under the assumption of normality. It is cumbersome to compute variable selection criteria for all subsets of variable pairs when the number of variables is large, even if it is smaller than the sample size. To tackle this problem, we propose a fast and consistent variable selection method based on Bayesian information criterion (BIC). The consistency of the method is provided in a high-dimensional asymptotic framework such that the sample size and the number of variables both tend toward infinity under a certain rule. Through numerical simulations, it is shown that the proposed method has a high probability of selecting the true subset of pairs of non-zero partial correlation.},
  archive      = {J_CSTAT},
  author       = {Yamada, Takayuki and Sakurai, Tetsuro and Fujikoshi, Yasunori},
  doi          = {10.1007/s00180-025-01628-z},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3585-3611},
  shortjournal = {Comput. Stat.},
  title        = {Variable selection method based on BIC with consistency for non-zero partial correlations under a large-dimensional setting},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning based regulatory risk index for cryptocurrencies. <em>CSTAT</em>, <em>40</em>(7), 3563-3583. (<a href='https://doi.org/10.1007/s00180-025-01629-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrency markets are highly sensitive to regulatory changes, often experiencing sharp price fluctuations in response to new policies and government interventions. Despite this, existing market indices fail to adequately capture the risks associated with regulatory uncertainty. In this paper, we introduce the Cryptocurrency Regulatory Risk Index (CRRIX), a machine learning-based index designed to quantify the impact of regulatory developments on cryptocurrency markets. Our methodology employs Latent Dirichlet Allocation (LDA) to classify policy-related news articles from major cryptocurrency news platforms, providing an objective measure of regulatory risk. We find that the CRRIX exhibits strong synchronicity with VCRIX, a cryptocurrency volatility index, suggesting that regulatory uncertainty plays a significant role in driving market fluctuations. Our results indicate that regulatory risk is a leading factor in market volatility, with major policy shifts triggering significant market movements. The proposed regulatory risk index provides a novel approach to quantifying policy uncertainty in the cryptocurrency sector, offering valuable insights for market participants navigating this rapidly changing environment.},
  archive      = {J_CSTAT},
  author       = {Ni, Xinwen and Xie, Taojun and Härdle, Wolfgang Karl and Zuo, Xiaorui},
  doi          = {10.1007/s00180-025-01629-y},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3563-3583},
  shortjournal = {Comput. Stat.},
  title        = {A machine learning based regulatory risk index for cryptocurrencies},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinate gradient descent algorithm in adaptive LASSO for pure ARCH and pure GARCH models. <em>CSTAT</em>, <em>40</em>(7), 3527-3561. (<a href='https://doi.org/10.1007/s00180-025-01642-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a coordinate gradient descent (CGD) algorithm, based on the work of Tseng and Yun (Math Program 117:387–423; 2009a; J Optim Theory Appl 140(3):513–535, 2009b), to optimize the constrained negative quasi maximum likelihood with adaptive LASSO penalization for pure autoregressive conditional heteroscedasticity (ARCH) model and its generalized form (GARCH). The strategy for choosing the appropriate values of the shrinkage parameter through information criteria (IC) is also discussed. We evaluate the numerical efficiency of the proposed algorithm through simulated data. Results of simulation studies show that for moderate sample sizes, the adaptive LASSO with the Bayesian variant of IC correctly estimates the ARCH structure at a high rate, even when model orders are over-specified. On the other hand, the adaptive LASSO has a low rate of correctly estimating true GARCH structure, especially when the model orders are over-specified regardless of the choice of IC. In our case study using daily ASX Ordinary log returns, the adaptive LASSO yields sparser ARCH and GARCH models while maintaining adequate fit for the volatility.},
  archive      = {J_CSTAT},
  author       = {Nasir, Muhammad Jaffri Mohd and Khan, Ramzan Nazim and Nair, Gopalan and Nur, Darfiana},
  doi          = {10.1007/s00180-025-01642-1},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3527-3561},
  shortjournal = {Comput. Stat.},
  title        = {Coordinate gradient descent algorithm in adaptive LASSO for pure ARCH and pure GARCH models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A newton-based variant of exclusive lasso for improved sparse solutions. <em>CSTAT</em>, <em>40</em>(7), 3505-3525. (<a href='https://doi.org/10.1007/s00180-025-01630-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exclusive Lasso offers significant advantages in scenarios that require sparse solutions within groups, such as multi-omics or gene expression analysis. These applications involve inherent grouping structures where selecting only a subset of variables from each group is crucial due to high correlations among variables within groups. However, a key challenge in optimizing Exclusive Lasso stems from the non-differentiability of the $$L_{1}$$ -norm within each group. To tackle this issue, we propose a method to transform this norm into a differentiable form using quadratic and sigmoid function approximations. This transformation facilitates the use of a straightforward Newton-based approach to solve the intricate optimization problem. Importantly, our proposed variant of Exclusive Lasso relaxes the strict requirement of selecting at least one variable per group, in contrast to the conventional Exclusive Lasso, and hence enables sparser solutions. Extensive simulation studies underscore the superior performance of our approach compared to both traditional Lasso methods and conventional Exclusive Lasso formulations.},
  archive      = {J_CSTAT},
  author       = {Ravi, Dayasri and Groll, Andreas},
  doi          = {10.1007/s00180-025-01630-5},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3505-3525},
  shortjournal = {Comput. Stat.},
  title        = {A newton-based variant of exclusive lasso for improved sparse solutions},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing non-inferiority for three-arm trials under the PH model. <em>CSTAT</em>, <em>40</em>(7), 3477-3503. (<a href='https://doi.org/10.1007/s00180-025-01624-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of non-inferiority (NI) trials is to show that a new treatment is not worse than a reference treatment by more than a pre-specified margin. For ethical reasons, NI trials usually do not include a placebo arm such that neither the assay sensitivity nor the constancy can be validated. On the other hand, three-arm NI trials consisting of the new treatment, reference treatment, and placebo, can simultaneously test the superiority of the new treatment over placebo and the NI of the new treatment compared with the reference treatment. In this article, we consider assessing NI of a new treatment in three-arm trials with time to event outcomes subject to right censoring. Under the proportional hazards model, we develop a testing procedure for assessing NI based on the infimum of ratio of survival difference between the new treatment and the placebo to that between the reference treatment and the placebo within a specific time period. The proposed test statistics involves the estimates of treatment parameters and survival function evaluated at a specific time point and their corresponding standard error estimates. Simulation study indicates that the proposed test controls the type I error well and has decent power to detect the NI under moderate to large sample settings.},
  archive      = {J_CSTAT},
  author       = {Shen, Pao-sheng},
  doi          = {10.1007/s00180-025-01624-3},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3477-3503},
  shortjournal = {Comput. Stat.},
  title        = {Testing non-inferiority for three-arm trials under the PH model},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian modeling and forecasting of seasonal autoregressive models with scale-mixtures of normal errors. <em>CSTAT</em>, <em>40</em>(7), 3453-3475. (<a href='https://doi.org/10.1007/s00180-025-01617-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing Bayesian analysis methods of time series with seasonal pattern are based on the normality assumption; however, most of the real time series violate this assumption. With assuming the scale-mixtures of normal (SMN) distribution for the model errors, we introduce the Bayesian estimation and prediction of seasonal autoregressive (SAR) models, using the Gibbs sampler and Metropolis-Hastings algorithms. The SMN distribution is a general class that includes different symmetric heavy-tailed distributions as special cases, such as the Student’s t, slash and contaminated normal distributions. With employing different priors for the SAR parameters, we derive the full conditional posterior distributions of the SAR coefficients and scale parameter to be the multivariate normal and inverse gamma, respectively, and the conditional predictive distribution of the future observations to be the multivariate normal. For the other parameters related to the SMN distribution, we derive their conditional posteriors to be in a closed form but some of them are not standard distributions. Using the derived closed-form conditional posterior and predictive distributions, we propose the Gibbs sampler with the Metropolis-Hastings algorithm to approximate empirically the marginal posterior and predictive distributions. We introduce an extensive simulation study and a real application in order to evaluate the accuracy of the proposed MCMC algorithm.},
  archive      = {J_CSTAT},
  author       = {Amin, Ayman A.},
  doi          = {10.1007/s00180-025-01617-2},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3453-3475},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian modeling and forecasting of seasonal autoregressive models with scale-mixtures of normal errors},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate bayesian inference in a model for self-generated gradient collective cell movement. <em>CSTAT</em>, <em>40</em>(7), 3399-3452. (<a href='https://doi.org/10.1007/s00180-025-01606-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we explore parameter inference in a novel hybrid discrete-continuum model describing the movement of a population of cells in response to a self-generated chemotactic gradient. The model employs a drift-diffusion stochastic process, rendering likelihood-based inference methods impractical. Consequently, we consider approximate Bayesian computation (ABC) methods, which have gained popularity for models with intractable or computationally expensive likelihoods. ABC involves simulating from the generative model, using parameters from generated observations that are “close enough” to the true data to approximate the posterior distribution. Given the plethora of existing ABC methods, selecting the most suitable one for a specific problem can be challenging. To address this, we employ a simple drift-diffusion stochastic differential equation (SDE) as a benchmark problem. This allows us to assess the accuracy of popular ABC algorithms under known configurations. We also evaluate the bias between ABC-posteriors and the exact posterior for the basic SDE model, where the posterior distribution is tractable. The top-performing ABC algorithms are subsequently applied to the proposed cell movement model to infer its key parameters. This study not only contributes to understanding cell movement but also sheds light on the comparative efficiency of different ABC algorithms in a well-defined context.},
  archive      = {J_CSTAT},
  author       = {Devlin, Jon and Borowska, Agnieszka and Husmeier, Dirk and Mackenzie, John},
  doi          = {10.1007/s00180-025-01606-5},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3399-3452},
  shortjournal = {Comput. Stat.},
  title        = {Approximate bayesian inference in a model for self-generated gradient collective cell movement},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Informative right censoring in nonparametric survival models. <em>CSTAT</em>, <em>40</em>(7), 3385-3397. (<a href='https://doi.org/10.1007/s00180-025-01610-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis models allow us to analyze and predict the time until a certain event occurs. Existing nonparametric models assume that the censoring of observations is random and unrelated to the study conditions. The estimators of the survival and hazard functions assume a constant survival probability between modes, have poor interpretability for datasets with multimodal time distributions, and lead to poor-quality data descriptions. In this paper, we investigate the quality of nonparametric models on four medical datasets with informative censoring and multimodal time distribution and propose a modification to improve the description quality. Proved properties of IBS and AUPRC metrics show that the best quality is achieved at survival function with unimodal time distribution. We propose modifying the nonparametric model based on virtual events from a truncated normal distribution that allows for the suppression of informative censoring. We compared the quality of the nonparametric models on multiple random subsets of datasets of different sizes using the AUPRC and IBS metrics. According to the comparison of the quality using Welch’s test, the proposed model with virtual events significantly outperformed the existing Kaplan–Meier model for all datasets (p-value $$<10^{-6}$$ ). The quality increase of IBS is from 6.91 to 21.92%, and the quality increase of AUPRC is from 12.92 to 18.4%. The nonparametric models with virtual events provide a better interpretation, allow a better description of the observed data, and are stable in terms of the informativeness of censoring. The proposed method is embedded in an open-source survivors Python library.},
  archive      = {J_CSTAT},
  author       = {Vasilev, Iulii and Petrovskiy, Mikhail and Mashechkin, Igor},
  doi          = {10.1007/s00180-025-01610-9},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3385-3397},
  shortjournal = {Comput. Stat.},
  title        = {Informative right censoring in nonparametric survival models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KNN estimators for time series prediction: A functional partial linear single index model with missing responses and error-prone covariates. <em>CSTAT</em>, <em>40</em>(7), 3359-3384. (<a href='https://doi.org/10.1007/s00180-024-01573-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates a functional partial linear single index model for strong $$\alpha$$ -mixing functional time series data when the responses are missing not at random and the real covariates are observed with measurement errors. We first extend three insertion methods developed for regression models with missing responses in finite dimensions, namely imputation, semiparametric regression surrogate and inverse marginal probability weighted approaches, to functional scenarios when the responses are scalar. Then the attenuation correction method is employed to eliminate the impact of measurement error on model estimation of unknown parameter in linear component, after we completing the missingness of responses by using above insertion methods. Meanwhile, we combine the kNN approach with insertion and attenuation correction approaches to capture the local structure of functional time series data and provide the estimations of unknown operators in the estimation process. The asymptotic properties of unknown parameters in the model are established under some mild assumptions. Furthermore, we make a comparison of three insertion methods, the oracle method and the ignoring method in simulation study and electricity consumption data analysis. All results indicate that our methodology has good performance.},
  archive      = {J_CSTAT},
  author       = {Meng, Shuyu and Huang, Zhensheng and Ling, Nengxiang},
  doi          = {10.1007/s00180-024-01573-3},
  journal      = {Computational Statistics},
  month        = {9},
  number       = {7},
  pages        = {3359-3384},
  shortjournal = {Comput. Stat.},
  title        = {KNN estimators for time series prediction: A functional partial linear single index model with missing responses and error-prone covariates},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of excess deaths from COVID-19 in el salvador through time series. <em>CSTAT</em>, <em>40</em>(6), 3321-3357. (<a href='https://doi.org/10.1007/s00180-025-01640-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The databases of deaths in El Salvador are analyzed for the years from 2015 to 2020. From these databases, the monthly time series of the number of deaths per month is constructed for the aforementioned years, treating the following five cases: Death from kidney disease, death from some type of failure (heart attack, respiratory failure, cardiorespiratory failure), death from cancer, death from causes other than firearms or traffic accidents (causes that are considered to have suffered intervention in 2020 due to the mandatory quarantine that was imposed), finally a model that includes all causes of death is considered. Time series models are adjusted, in each case, to predict the months of the year 2020. These forecasts are compared with real cases and the underreporting of deaths from COVID-19 is measured according to official data. In each case, two models are adjusted: Box–Jenkins Method (Seasonal Autoregressive Integrated Moving Average, SARIMA) and Holt-Winters Additive Method (it is optimized with a developed heuristic). This work shows that there are many people who really died from COVID-19, but the official record lists them in other cases. In such a way that there is high statistical evidence of under-registration from official data provided by the government on deaths from COVID-19, in the period covered by this study.},
  archive      = {J_CSTAT},
  author       = {Campos, W. O.},
  doi          = {10.1007/s00180-025-01640-3},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3321-3357},
  shortjournal = {Comput. Stat.},
  title        = {Analysis of excess deaths from COVID-19 in el salvador through time series},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental singular value decomposition for some numerical aspects of multiblock redundancy analysis. <em>CSTAT</em>, <em>40</em>(6), 3291-3319. (<a href='https://doi.org/10.1007/s00180-023-01418-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneously processing several large blocks of streaming data is a computationally expensive problem. Based on the incremental singular value decomposition algorithm, we propose a new procedure for calculating the factorization of the multiblock redundancy matrix $${{\textbf {M}}}$$ , which makes the multiblock method more fast and efficient when analyzing large streaming data and high-dimensional dense matrices. The procedure transforms a big data problem into a small one by processing small high-dimensional matrices where variables are in rows. Numerical experiments illustrate the accuracy and performance of the incremental solution for analyzing streaming multiblock redundancy data. The experiments demonstrate that the incremental algorithm may decompose a large matrix with a 75% reduction in execution time. It is more efficient to first partition the matrix $${{\textbf {M}}}$$ and then decompose it with the incremental algorithm than to decompose the entire matrix $${{\textbf {M}}}$$ using the standard singular value decomposition algorithm.},
  archive      = {J_CSTAT},
  author       = {Martinez-Ruiz, Alba and Lauro, Natale Carlo},
  doi          = {10.1007/s00180-023-01418-5},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3291-3319},
  shortjournal = {Comput. Stat.},
  title        = {Incremental singular value decomposition for some numerical aspects of multiblock redundancy analysis},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust singular spectrum analysis: Comparison between classical and robust approaches for model fit and forecasting. <em>CSTAT</em>, <em>40</em>(6), 3257-3289. (<a href='https://doi.org/10.1007/s00180-022-01322-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Singular spectrum analysis is a powerful and widely used non-parametric method to analyse and forecast time series. Although singular spectrum analysis has proven to outperform traditional parametric methods for model fit and model forecasting, one of the steps of this algorithm is the singular value decomposition of the trajectory matrix, which is very sensitive to the presence of outliers because it uses the $$L_{2}$$ norm optimization. Therefore the presence of outlying observations have a significant impact on the singular spectrum analysis reconstruction and forecasts. The main aim of this paper is to introduce four robust alternatives to the singular spectrum analysis, where the singular value decomposition is replaced by the: (i) robust regularized singular value decomposition; (ii) robust principal component analysis algorithm, which combines projection pursuit ideas with robust scatter matrix estimation; (iii) robust principal component analysis based on the grid algorithm and projection pursuit; and (iv) robust principal component analysis based on a robust covariance matrix. The four proposed robust singular spectrum analysis alternatives are compared with the classical singular spectrum analysis and other available robust singular spectrum analysis algorithms, in terms of model fit and model forecasting via Monte Carlo simulations based on synthetic and real data, considering several contamination scenarios.},
  archive      = {J_CSTAT},
  author       = {Kazemi, Mohammad and Rodrigues, Paulo Canas},
  doi          = {10.1007/s00180-022-01322-4},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3257-3289},
  shortjournal = {Comput. Stat.},
  title        = {Robust singular spectrum analysis: Comparison between classical and robust approaches for model fit and forecasting},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Volatility forecasting using deep recurrent neural networks as GARCH models. <em>CSTAT</em>, <em>40</em>(6), 3229-3255. (<a href='https://doi.org/10.1007/s00180-023-01349-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating and predicting volatility in time series is of great importance in different areas where it is required to quantify risk based on variability and uncertainty. This work proposes a new methodology to predict Time Series volatility by combining Generalized AutoRegressive Conditional Heteroscedasticity (GARCH) methods with Deep Neural Networks. Additionally, the proposal incorporates a mechanism to determine the optimal size of the sliding window used to estimate volatility. In this work, the recurrent neural networks Gated Recurrent Units, Long/Short-Term Memory (LSTM), and Bidirectional Long/Short-Term Memory (BiLSTM) are evaluated with the methods of the family Garch (fGARCH). We conducted Monte Carlo simulation studies with heteroscedastic time series to validate our proposed methodology. Moreover, we have applied the proposed method to real financial data from the stock market, such as the Selective Stock Price Index Chile index, Standard & Poor’s 500 Index (S &P500), and the prices of the Stock Exchange from Australia (ASX200). The proposed methodology performs well in predicting the stock options returns volatility one week ahead.},
  archive      = {J_CSTAT},
  author       = {Di-Giorgi, Gustavo and Salas, Rodrigo and Avaria, Rodrigo and Ubal, Cristian and Rosas, Harvey and Torres, Romina},
  doi          = {10.1007/s00180-023-01349-1},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3229-3255},
  shortjournal = {Comput. Stat.},
  title        = {Volatility forecasting using deep recurrent neural networks as GARCH models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust order selection of mixtures of regression models with random effects. <em>CSTAT</em>, <em>40</em>(6), 3205-3228. (<a href='https://doi.org/10.1007/s00180-021-01177-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite mixtures of regression models with random effects are a very flexible statistical tool to model data, as these models allow to model the heterogeneity of the population and to account for multiple correlated observations from the same individual at the same time. The selection of the number of components for these models has been a long-standing challenging problem in statistics. However, the majority of the existent methods for the estimation of the number of components are not robust and, therefore, are quite sensitive to outliers. In this article we study a robust estimation of the number of components for mixtures of regression models with random effects, investigating the performance of trimmed information and classification criteria comparatively to the performance of the traditional information and classification criteria. The simulation study and a real-world application showcase the superiority of the trimmed information and classification criteria in the presence of contaminated data.},
  archive      = {J_CSTAT},
  author       = {Novais, Luísa and Faria, Susana},
  doi          = {10.1007/s00180-021-01177-1},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3205-3228},
  shortjournal = {Comput. Stat.},
  title        = {Robust order selection of mixtures of regression models with random effects},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moments and probability density of threshold crossing times for populations in random environments under sustainable harvesting policies. <em>CSTAT</em>, <em>40</em>(6), 3191-3203. (<a href='https://doi.org/10.1007/s00180-022-01237-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic differential equations are used to model the dynamics of harvested populations in random environments. The main goal of this work is to compute, for a particular fish population under constant effort harvesting, the mean and standard deviation of first passage times by several lower and upper thresholds values. We apply logistic or logistic-like with Allee effects average growth dynamics. In addition, we present a method to obtain the probability density function of the first passage time by a threshold through the numerical inversion of its Laplace transform.},
  archive      = {J_CSTAT},
  author       = {Brites, Nuno M. and Braumann, Carlos A.},
  doi          = {10.1007/s00180-022-01237-0},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3191-3203},
  shortjournal = {Comput. Stat.},
  title        = {Moments and probability density of threshold crossing times for populations in random environments under sustainable harvesting policies},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple partition markov model for b.1.1.7, b.1.351, b.1.617.2, and p.1 variants of SARS-CoV 2 virus. <em>CSTAT</em>, <em>40</em>(6), 3153-3189. (<a href='https://doi.org/10.1007/s00180-022-01291-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With tools originating from Markov processes, we investigate the similarities and differences between genomic sequences in FASTA format coming from four variants of the SARS-CoV 2 virus, B.1.1.7 (UK), B.1.351 (South Africa), B.1.617.2 (India), and P.1 (Brazil). We treat the virus’ sequences as samples of finite memory Markov processes acting in $$A=\{a,c,g,t\}.$$ We model each sequence, revealing some heterogeneity between sequences belonging to the same variant. We identified the five most representative sequences for each variant using a robust notion of classification, see Fernández et al. (Math Methods Appl Sci 43(13):7537–7549. https://doi.org/10.1002/mma.5705 ). Using a notion derived from a metric between processes, see García et al. (Appl Stoch Models Bus Ind 34(6):868–878. https://doi.org/10.1002/asmb.2346 ), we identify four groups, each group representing a variant. It is also detected, by this metric, global proximity between the variants B.1.351 and B.1.1.7. With the selected sequences, we assemble a multiple partition model, see Cordeiro et al. (Math Methods Appl Sci 43(13):7677–7691. https://doi.org/10.1002/mma.6079 ), revealing in which states of the state space the variants differ, concerning the mechanisms for choosing the next element in A. Through this model, we identify that the variants differ in their transition probabilities in eleven states out of a total of 256 states. For these eleven states, we reveal how the transition probabilities change from variant (group of variants) to variant (group of variants). In other words, we indicate precisely the stochastic reasons for the discrepancies.},
  archive      = {J_CSTAT},
  author       = {García, Jesús Enrique and González-López, Verónica Andrea and Tasca, Gustavo Henrique},
  doi          = {10.1007/s00180-022-01291-8},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3153-3189},
  shortjournal = {Comput. Stat.},
  title        = {Multiple partition markov model for b.1.1.7, b.1.351, b.1.617.2, and p.1 variants of SARS-CoV 2 virus},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Results and student perspectives on a web-scraping assignment from utah state university’s data technologies course to evaluate the african activity in the statistical computing community. <em>CSTAT</em>, <em>40</em>(6), 3127-3151. (<a href='https://doi.org/10.1007/s00180-022-01222-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2019, members of the Executive Committee of the International Association for Statistical Computing (IASC) were contacted by members of the IASC from Africa asking whether it would be feasible to establish a new regional IASC section in Africa. The establishment of a new regional section requires several steps that are outlined in the IASC Statutes at https://iasc-isi.org/statutes/ . The approval likely depends on whether the proposed new regional section has the potential to conduct typical section activities, such as organizing regional conferences, workshops, and short courses. To establish whether it is feasible to add a regional section in Africa, the IASC must know whether there is currently enough high-level activity within African countries with respect to computational statistics. To answer this question, we looked at author affiliations of articles published in the Springer journal Computational Statistics (COST) and the Elsevier journal Computational Statistics & Data Analysis (CSDA) from 2015 to 2020 and used these data as a proxy to compare author productivity for authors with an affiliation in Africa in 2019 and 2020, compared to authors with an affiliation in Latin America in 2015 and 2016. This article looks at quantitative results to the questions above, provides insight on how students from Utah State University’s STAT 5080/6080 “Data Technologies” course from the Fall 2019 semester used web scraping techniques in a homework assignment to gather author affiliations from COST and CSDA to answer these questions, and includes the evaluation of student feedback obtained after the end of the course.},
  archive      = {J_CSTAT},
  author       = {Fleming, Adelyn and Coltrin, Joanna D. and Medri, Jhonatan and Hilyard, Cody and Tellez, Rigoberto and Symanzik, Jürgen},
  doi          = {10.1007/s00180-022-01222-7},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3127-3151},
  shortjournal = {Comput. Stat.},
  title        = {Results and student perspectives on a web-scraping assignment from utah state university’s data technologies course to evaluate the african activity in the statistical computing community},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interactive graphics for visually diagnosing forest classifiers in r. <em>CSTAT</em>, <em>40</em>(6), 3105-3125. (<a href='https://doi.org/10.1007/s00180-023-01323-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes structuring data and constructing plots to explore forest classification models interactively. A forest classifier is an example of an ensemble since it is produced by bagging multiple trees. The process of bagging and combining results from multiple trees produces numerous diagnostics which, with interactive graphics, can provide a lot of insight into class structure in high dimensions. Various aspects of models are explored in this article, to assess model complexity, individual model contributions, variable importance and dimension reduction, and uncertainty in prediction associated with individual observations. The ideas are applied to the random forest algorithm and projection pursuit forest but could be more broadly applied to other bagged ensembles helping in the interpretability deficit of these methods. Interactive graphics are built in R using the ggplot2, plotly, and shiny packages.},
  archive      = {J_CSTAT},
  author       = {da Silva, Natalia and Cook, Dianne and Lee, Eun-Kyung},
  doi          = {10.1007/s00180-023-01323-x},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3105-3125},
  shortjournal = {Comput. Stat.},
  title        = {Interactive graphics for visually diagnosing forest classifiers in r},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel matrix hit and run for sampling polytopes and its GPU implementation. <em>CSTAT</em>, <em>40</em>(6), 3067-3104. (<a href='https://doi.org/10.1007/s00180-023-01411-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and analyze a new Markov Chain Monte Carlo algorithm that generates a uniform sample over full and non-full-dimensional polytopes. This algorithm, termed “Matrix Hit and Run” (MHAR), is a modification of the Hit and Run framework. For a polytope in $$\mathbb {R}^n$$ defined by m linear constraints, the regime $$n^{1+\frac{1}{3}} \ll m$$ has a lower asymptotic cost per sample in terms of soft-O notation ( $$\mathcal {O}^*$$ ) than do existing sampling algorithms after a warm start. MHAR is designed to take advantage of matrix multiplication routines that require less computational and memory resources. Our tests show this implementation to be substantially faster than the hitandrun R package, especially for higher dimensions. Finally, we provide a python library based on PyTorch and a Colab notebook with the implementation ready for deployment in architectures with GPU or just CPU.},
  archive      = {J_CSTAT},
  author       = {Corte, Mario Vazquez and Montiel, Luis V.},
  doi          = {10.1007/s00180-023-01411-y},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3067-3104},
  shortjournal = {Comput. Stat.},
  title        = {Novel matrix hit and run for sampling polytopes and its GPU implementation},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical evaluation of initial transient deletion rules for the steady-state mean estimation problem. <em>CSTAT</em>, <em>40</em>(6), 3041-3065. (<a href='https://doi.org/10.1007/s00180-022-01243-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose three new initial transient deletion rules (denoted H1, H2 and H3) to reduce the bias of the natural point estimator when estimating the steady-state mean of a performance variable from the output of a single (long) run of a simulation. Although the rules are designed for the estimation of a steady-state mean, our experimental results show that these rules may perform well for the estimation of variances and quantiles of a steady-state distribution. One of the proposed rules can be applied under the only assumption that the output of interest $$\{ Y(s): s\ge 0 \}$$ has a stationary distribution whereas the other two rules require that $$Y(s)= f(X(s))$$ for an $$\mathfrak {R}^d$$ -valued Markov chain $$\{ X(s): s \ge 0 \}$$ . Our proposed rules are based on the use of sample quantiles and multivariate batch means to test the null hypothesis that a current observation Y(s) comes from a stationary distribution for $$\{ X(s): s \ge 0 \}$$ . We present experimental results to compare the performance of the new rules against three variants of the Marginal Standard Error Rule and the Glynn-Iglehart deletion rule. When the run length was sufficiently large to provide a reliable confidence interval for the estimated parameter, one of the proposed rules (H3) provided the best reductions in Mean Square Error, so that the identification of an underlying Markov chain X for which $$Y(s)= f(X(s))$$ can be useful to determine an appropriate deletion point to reduce the initial transient, and one of our proposed rules (H2) can be useful to detect that a run length is too small to provide a reliable confidence interval.},
  archive      = {J_CSTAT},
  author       = {Muñoz, David F.},
  doi          = {10.1007/s00180-022-01243-2},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3041-3065},
  shortjournal = {Comput. Stat.},
  title        = {Empirical evaluation of initial transient deletion rules for the steady-state mean estimation problem},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian quantile regression models for heavy tailed bounded variables using the no-U-turn sampler. <em>CSTAT</em>, <em>40</em>(6), 3007-3040. (<a href='https://doi.org/10.1007/s00180-022-01297-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When we are interested in knowing how covariates impact different levels of the response variable, quantile regression models can be very useful, with their practical use being benefited from the increasing of computational power. The use of bounded response variables is also very common when there are data containing percentages, rates, or proportions. In this work, with the generalized Gompertz distribution as the baseline distribution, we derive two new two-parameter distributions with bounded support, and new quantile parametric mixed regression models are proposed based on these distributions, which consider bounded response variables with heavy tails. Estimation of the parameters using the Bayesian approach is considered for both models, relying on the No-U-Turn sampler algorithm. The inferential methods can be implemented and then easily used for data analysis. Simulation studies with different quantiles ( $$q=0.1$$ , $$q=0.5$$ and $$q=0.9$$ ) and sample sizes ( $$n=100$$ , $$n=200$$ , $$n=500$$ , $$n=2000$$ , $$n=5000$$ ) were conducted for 100 replicas of simulated data for each combination of settings, in the (0, 1) and [0, 1), showing the good performance of the recovery of parameters for the proposed inferential methods and models, which were compared to Beta Rectangular and Kumaraswamy regression models. Furthermore, a dataset on extreme poverty is analyzed using the proposed regression models with fixed and mixed effects. The quantile parametric models proposed in this work are an alternative and complementary modeling tool for the analysis of bounded data.},
  archive      = {J_CSTAT},
  author       = {de Oliveira, Eduardo S. B. and de Castro, Mário and Bayes, Cristian L. and Bazán, Jorge L.},
  doi          = {10.1007/s00180-022-01297-2},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {3007-3040},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian quantile regression models for heavy tailed bounded variables using the no-U-turn sampler},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simulation model to analyze the behavior of a faculty retirement plan: A case study in mexico. <em>CSTAT</em>, <em>40</em>(6), 2981-3006. (<a href='https://doi.org/10.1007/s00180-024-01456-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal in this study was to determine confidence intervals for average age, average seniority, and average money-savings, for faculty members in a university retirement system using a simulation model. The simulation—built-in Arena—considers age, seniority, and the probability of continuing in the institution as the main input random variables in the model. An annual interest rate of 7% and an average annual salary increase of 3% were considered. The scenario simulated consisted of the teacher and the university making contributions, the faculty 5% of his salary, and the university 5% of the teacher’s salary. Since the base salaries with which teachers join to university are variable, we considered a monthly salary of MXN 23 181.2, corresponding to full-time teachers with middle salaries. The results obtained by a simulation of 30 replicates showed that the confidence intervals for the average age at retirement were (55.0, 55.2) years, for the average seniority (22.1, 22.3) years, and for the average savings amount (329 795.2, 341 287.0) MXN. Moreover, the risk that a retiree of 62 years of age and more of 25 years of work, is alive after his savings runs out is approximately 98% and this happens at 64 years of age.},
  archive      = {J_CSTAT},
  author       = {Montufar-Benítez, Marco Antonio and Mora-Vargas, Jaime and Soto-Campos, Carlos Arturo and Pérez-Lechuga, Gilberto and Castro-Esparza, José Raúl},
  doi          = {10.1007/s00180-024-01456-7},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {2981-3006},
  shortjournal = {Comput. Stat.},
  title        = {A simulation model to analyze the behavior of a faculty retirement plan: A case study in mexico},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Policy evaluation using model over-fitting: The nordic case. <em>CSTAT</em>, <em>40</em>(6), 2955-2980. (<a href='https://doi.org/10.1007/s00180-023-01348-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interest of this article is to better understand the effects of different public policy alternatives to handle the COVID-19 pandemic. In this work we use the susceptible, infected, recovered (SIR) model to find which of these policies have an actual impact on the dynamic of the spread. Starting with raw data on the number of deceased people in a country, we over-fit our SIR model to find the times $$t_i$$ at which the main parameters, the number of daily contacts and the probability of contagion, require adjustments. For each $$t_i$$ , we go to historic records to find policies and social events that could explain these changes. This approach helps to evaluate events through the eyes of the popular epidemiological SIR model, and to find insights that are hard to recognize in a standard econometric model.},
  archive      = {J_CSTAT},
  author       = {Tapia, Armando and González, Silvestre L. and Vergara, Jose R. and Villafuerte, Mariano and Montiel, Luis V.},
  doi          = {10.1007/s00180-023-01348-2},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {2955-2980},
  shortjournal = {Comput. Stat.},
  title        = {Policy evaluation using model over-fitting: The nordic case},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate nonnegative trigonometric sums distributions for high-dimensional multivariate circular data. <em>CSTAT</em>, <em>40</em>(6), 2931-2954. (<a href='https://doi.org/10.1007/s00180-024-01583-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fernández-Durán and Gregorio-Domínguez (2014) defined a family of probability distributions for a vector of circular random variables by considering multiple nonnegative trigonometric sums. These distributions are highly flexible and can present numerous modes and skewness. Several operations on these multivariate distributions were translated into operations on the vector of parameters; for instance, marginalization involves calculating the eigenvectors and eigenvalues of a matrix, and independence among subsets of the vector of circular variables translates to a Kronecker product of the corresponding subsets of the vector of parameters. Furthermore, it was demonstrated that the family of multivariate circular distributions based on nonnegative trigonometric sums is closed under marginalization and conditioning, that is, the marginal and conditional densities of any order are also members of the family. The derivation of marginal and conditional densities from the joint multivariate density is important when applying this model in practice to real datasets. A goodness-of-fit test based on the characteristic function and an alternative parameter estimation algorithm for high-dimensional circular data was presented and applied to a real dataset on the daily times of occurrence of maxima and minima of prices in financial markets.},
  archive      = {J_CSTAT},
  author       = {Fernández-Durán, Juan José and Gregorio-Domínguez, María Mercedes},
  doi          = {10.1007/s00180-024-01583-1},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {2931-2954},
  shortjournal = {Comput. Stat.},
  title        = {Multivariate nonnegative trigonometric sums distributions for high-dimensional multivariate circular data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An extended approach for the generalized powered uniform distribution. <em>CSTAT</em>, <em>40</em>(6), 2907-2930. (<a href='https://doi.org/10.1007/s00180-022-01296-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new uniform distribution model, generalized powered uniform distribution (GPUD), which is based on incorporating the parameter k into the probability density function (pdf) associated with the power of random variable values and includes a powered mean operator, is introduced in this paper. From this new model, the shape properties of the pdf as well as the higher-order moments, the moment generating function, the model that simulates the GPUD and other important statistics can be derived. This approach allows the generalization of the distribution presented by Jayakumar and Sankaran (2016) through the new $${ GPUD }_{ (J-S)}$$ distribution. Two sets of real data related to COVID-19 and bladder cancer were tested to demonstrate the proposed model’s potential. The maximum likelihood method was used to calculate the parameter estimators by applying the maxLik package in R. The results showed that this new model is more flexible and useful than other comparable models.},
  archive      = {J_CSTAT},
  author       = {Rondero-Guerrero, Carlos and González-Hernández, Isidro and Soto-Campos, Carlos},
  doi          = {10.1007/s00180-022-01296-3},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {2907-2930},
  shortjournal = {Comput. Stat.},
  title        = {An extended approach for the generalized powered uniform distribution},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact inference for progressively type-I censored step-stress accelerated life test under interval monitoring. <em>CSTAT</em>, <em>40</em>(6), 2877-2905. (<a href='https://doi.org/10.1007/s00180-022-01314-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to continuously advancing technology and manufacturing processes, the products and devices are becoming highly reliable but performing the life tests of these products at normal operating conditions has become extremely difficult, if not impossible, due to their long lifespans. This problem is solved by accelerated life tests where the test units are subjected to higher stress levels than the normal usage level so that information on the lifetime parameters can be obtained more quickly. The lifetime at the design condition is then estimated through extrapolation using a regression model. Although continuous inspection of the exact failure times is an ideal mode, the exact failure times of test units may not be available in practice due to technical limitations and/or budgetary constraints, but only the failure counts are collected at certain time points during the test (i.e., interval inspection). In this work, we consider the progressively Type-I censored step-stress accelerated life test under the assumption that the lifetime of each test unit is exponentially distributed. Under this setup, we obtain the maximum likelihood estimator of the mean time to failure at each stress level and derive its exact sampling distribution under the condition that its existence is ensured. Using the exact distribution of the MLE as well as its asymptotic distribution and the parametric bootstrap method, we then discuss the construction of confidence intervals for the mean parameters and their performance is assessed through Monte Carlo simulations. Finally, an example is presented to illustrate all the methods of inference discussed here.},
  archive      = {J_CSTAT},
  author       = {Han, David and Bai, Tianyu},
  doi          = {10.1007/s00180-022-01314-4},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {2877-2905},
  shortjournal = {Comput. Stat.},
  title        = {Exact inference for progressively type-I censored step-stress accelerated life test under interval monitoring},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distribution-free control chart for joint monitoring of location and scale in finite horizon productions. <em>CSTAT</em>, <em>40</em>(6), 2857-2875. (<a href='https://doi.org/10.1007/s00180-023-01361-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a distribution-free Shewhart-type control chart for jointly monitoring location and scale in a finite horizon production, commonly presented in manufacturing environments characterized by high flexibility, production variety, and a limited number of scheduled inspections. The Lepage-type statistic used in this research combines the Wilcoxon rank sums and Mood statistics with control limits obtained through Monte Carlo simulation to achieve a guaranteed in-control performance considering the practitioner-to-practitioner variation across their reference samples. We performed an extensive Monte-Carlo simulation to evaluate the performance of the proposed chart over several scenarios. The in-control results show the benefits of using the guaranteed limits compared with the unconditional version of the proposed control chart. In most scenarios, the out-of-control experimentation presents a better performance of the proposed control chart than the chart using the classical Lepage statistic with the Ansary–Bradley statistic instead of the Mood statistic. We present a real example to show practitioners the implementation of the proposed control chart.},
  archive      = {J_CSTAT},
  author       = {Diaz Pulido, Arturo Javier and Cordero Franco, Alvaro Eduardo and Tercero Gómez, Víctor Gustavo},
  doi          = {10.1007/s00180-023-01361-5},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {2857-2875},
  shortjournal = {Comput. Stat.},
  title        = {A distribution-free control chart for joint monitoring of location and scale in finite horizon productions},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial on the special issue on the v latin american conference on statistical computing. <em>CSTAT</em>, <em>40</em>(6), 2849-2856. (<a href='https://doi.org/10.1007/s00180-025-01644-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CSTAT},
  author       = {Muñoz, David Fernando and González-López, Verónica Andrea and Symanzik, Jürgen},
  doi          = {10.1007/s00180-025-01644-z},
  journal      = {Computational Statistics},
  month        = {7},
  number       = {6},
  pages        = {2849-2856},
  shortjournal = {Comput. Stat.},
  title        = {Editorial on the special issue on the v latin american conference on statistical computing},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical visualisation of tidy and geospatial data in r via kernel smoothing methods in the eks package. <em>CSTAT</em>, <em>40</em>(5), 2825-2847. (<a href='https://doi.org/10.1007/s00180-024-01543-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel smoothers are essential tools for data analysis due to their ability to convey complex statistical information with concise graphical visualisations. Their inclusion in the base distribution and in the many user-contributed add-on packages of the R statistical analysis environment caters well to many practitioners. Though there remain some important gaps for specialised data, most notably for tidy and geospatial data. The proposed eks package fills in these gaps. In addition to kernel density estimation, this package also caters for more complex data analysis situations, such as density derivative estimation, density-based classification (supervised learning) and mean shift clustering (unsupervised learning). We illustrate with experimental data how to obtain and to interpret the statistical visualisations for these kernel smoothing methods.},
  archive      = {J_CSTAT},
  author       = {Duong, Tarn},
  doi          = {10.1007/s00180-024-01543-9},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2825-2847},
  shortjournal = {Comput. Stat.},
  title        = {Statistical visualisation of tidy and geospatial data in r via kernel smoothing methods in the eks package},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BARMPy: Bayesian additive regression models python package. <em>CSTAT</em>, <em>40</em>(5), 2807-2824. (<a href='https://doi.org/10.1007/s00180-024-01535-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We make Bayesian additive regression networks (BARN) available as a Python package, barmpy, with documentation at https://dvbuntu.github.io/barmpy/ for general machine learning practitioners. Our object-oriented design is compatible with SciKit-Learn, allowing usage of their tools like cross-validation. To ease learning to use barmpy, we produce a companion tutorial that expands on reference information in the documentation. Any interested user can pip install barmpy from the official PyPi repository. barmpy also serves as a baseline Python library for generic Bayesian additive regression models.},
  archive      = {J_CSTAT},
  author       = {Van Boxel, Danielle},
  doi          = {10.1007/s00180-024-01535-9},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2807-2824},
  shortjournal = {Comput. Stat.},
  title        = {BARMPy: Bayesian additive regression models python package},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble kalman filter with precision localization. <em>CSTAT</em>, <em>40</em>(5), 2781-2805. (<a href='https://doi.org/10.1007/s00180-024-01588-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel localization strategy for the ensemble Kalman filter procedure based on sparse precision matrices, which we denote precision localization. We assume the latent state vector to come from a Gaussian Markov random field and use the iterative proportional scaling algorithm to estimate the associated sparse precision matrix. Precision localization is compared against a standard covariance localization technique in two simulation studies; one with a Gauss-linear model and one based on the Lorenz 96 model. We evaluate the results by their prediction accuracy and to what degree the generated ensembles give a realistic representation of the exact filtering distributions. In the Gauss-linear example we also compare our results with the Kalman filter solution. Here we see that both precision and covariance localization produce reasonably good results in terms of prediction accuracy, but that precision localization provides the best representation of the correlation structure in the filtering distribution. For the Lorenz model we cannot compare with the Kalman filter solution, but precision localization seems to provide the most accurate predictions and the most realistic uncertainty representation.},
  archive      = {J_CSTAT},
  author       = {Gryvill, Håkon and Tjelmeland, Håkon},
  doi          = {10.1007/s00180-024-01588-w},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2781-2805},
  shortjournal = {Comput. Stat.},
  title        = {Ensemble kalman filter with precision localization},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential monte carlo for cut-bayesian posterior computation. <em>CSTAT</em>, <em>40</em>(5), 2749-2779. (<a href='https://doi.org/10.1007/s00180-024-01576-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a sequential Monte Carlo (SMC) method to efficiently and accurately compute cut-Bayesian posterior quantities of interest, variations of standard Bayesian approaches constructed primarily to account for model misspecification. We prove finite sample concentration bounds for estimators derived from the proposed method and apply these results to a realistic setting where a computer model is misspecified. Two theoretically justified variations are presented for making the sequential Monte Carlo estimator more computationally efficient, based on linear tempering and finding suitable permutations of initial parameter draws. We then illustrate the SMC method for inference in a modular chemical reactor example that includes submodels for reaction kinetics, turbulence, mass transfer, and diffusion. The samples obtained are commensurate with a direct-sampling approach that consists of running multiple Markov chains, with computational efficiency gains using the SMC method. Overall, the SMC method presented yields a novel, rigorous approach to computing with cut-Bayesian posterior distributions.},
  archive      = {J_CSTAT},
  author       = {Mathews, Joseph and Gopalan, Giri and Gattiker, James and Smith, Sean and Francom, Devin},
  doi          = {10.1007/s00180-024-01576-0},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2749-2779},
  shortjournal = {Comput. Stat.},
  title        = {Sequential monte carlo for cut-bayesian posterior computation},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient estimation of a disease prevalence using auxiliary ranks information. <em>CSTAT</em>, <em>40</em>(5), 2729-2748. (<a href='https://doi.org/10.1007/s00180-024-01580-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a common challenge in medical field to obtain the prevalence of a specific disease within a given population. To tackle this problem, researchers usually draw a random sample from the target population to obtain an accurate estimate of the proportion of diseased people. However, some limitations may occur in practice due to constraints, such as complexity or cost. In these situations, some alternative sampling techniques are needed to achieve precision with smaller sample sizes. One such approach is Neoteric Ranked Set Sampling (NRSS), which is a variation of Ranked Set Sampling (RSS) design. NRSS scheme involves selecting sample units using a rank-based method that incorporates auxiliary information to obtain a more informative sample. In this article, we focus on the problem of estimating the population proportion using NRSS. We develop an estimator for the population proportion using the NRSS design and establish some of its properties. We employ Monte Carlo simulations to compare the proposed estimator with competitors in Simple Random Sampling (SRS) and RSS designs. Our results demonstrate that statistical inference based on the introduced estimator can be significantly more efficient than its competitors in RSS and SRS designs. Finally, to demonstrate the effectiveness of the proposed procedure in estimating breast cancer prevalence within the target population, we apply it to analyze Wisconsin Breast Cancer data.},
  archive      = {J_CSTAT},
  author       = {Zamanzade, Ehsan and Saboori, Hadi and Samawi, Hani M.},
  doi          = {10.1007/s00180-024-01580-4},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2729-2748},
  shortjournal = {Comput. Stat.},
  title        = {Efficient estimation of a disease prevalence using auxiliary ranks information},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional regression models with functional response: A new approach and a comparative study. <em>CSTAT</em>, <em>40</em>(5), 2701-2727. (<a href='https://doi.org/10.1007/s00180-024-01572-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new nonlinear approach for additive functional regression with functional response based on kernel methods along with some slight reformulation and implementation of the linear regression and the spectral additive model. The latter methods have in common that the covariates and the response are represented in a basis and so, can only be applied when the response and the covariates belong to a Hilbert space, while the proposed method only uses the distances among data and thus can be applied to those situations where any of the covariates or the response is not Hilbert, typically normed or even metric spaces with a real vector structure. A comparison of these methods with other procedures readily available in R is perfomed in a simulaton study and in real datasets showing the results the advantages of the nonlinear proposals and the small loss of efficiency when the simulation scenario is truly linear. The comparison is done in the Hilbert case as it is the only scenario where all the procedures can be compared. Finally, the supplementary material provides a visualization tool for checking the linearity of the relationship between a single covariate and the response, another real data example and a link to a GitHub repository where the code and data is available.},
  archive      = {J_CSTAT},
  author       = {Febrero–Bande, Manuel and Oviedo-de la Fuente, Manuel and Darbalaei, Mohammad and Amini, Morteza},
  doi          = {10.1007/s00180-024-01572-4},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2701-2727},
  shortjournal = {Comput. Stat.},
  title        = {Functional regression models with functional response: A new approach and a comparative study},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Order restricted inference for chen life time populations under progressive type-II censoring scheme with partially observed competing risks. <em>CSTAT</em>, <em>40</em>(5), 2657-2699. (<a href='https://doi.org/10.1007/s00180-024-01577-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the estimation of a competing risks model when failure reasons are partially observed. Specifically, we explore the scenario in which the Chen distribution governs the latent lifetime data of competing risk units under a progressive type-II censoring scheme. Estimations are conducted using both classical and Bayesian approaches, taking into account non-order cases and order restrictions on scale parameters under both unknown and known shape parameters.The uniqueness and existence of maximum likelihood estimates for the undetermined parameters are discussed. Subsequently, asymptotic confidence intervals and modified confidence intervals are calculated from the observed Fisher matrix. Additionally, we perform Bayesian inference for the model parameters utilizing a balanced loss function, subsequently deriving the credible interval. Furthermore, we examine the conditions under which supplementary order information is available for the competing risk parameters, and under these circumstances, we derive both classical and Bayesian estimates. Monte Carlo simulations are used to evaluate the effectiveness of the proposed estimation methods. Furthermore, we assess two real datasets to demonstrate the practical utility of these methods. Finally, we investigate the optimal censoring scheme by conducting tests based on various criteria.},
  archive      = {J_CSTAT},
  author       = {Zhang, Jiaxin and Gui, Wenhao},
  doi          = {10.1007/s00180-024-01577-z},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2657-2699},
  shortjournal = {Comput. Stat.},
  title        = {Order restricted inference for chen life time populations under progressive type-II censoring scheme with partially observed competing risks},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian inferences and prediction of exponentiated exponential distribution based on multiple interval censored data. <em>CSTAT</em>, <em>40</em>(5), 2635-2655. (<a href='https://doi.org/10.1007/s00180-024-01565-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article carefully defines a multiple interval censoring plan, and its scope of application in the Bayesian setup is demonstrated. The Bayes estimators of shape and scale parameters of the exponentiated exponential distribution are obtained under symmetric and asymmetric loss functions. Additionally, the credible intervals for both parameters are obtained. The performances of Bayes estimators and credible intervals are investigated through the appropriate Monte Carlo method. Furthermore, the authors also considered the prediction of future samples as well as the prediction interval. Lastly, a real-world example is presented in order to illustrate the effectiveness of the proposed methods.},
  archive      = {J_CSTAT},
  author       = {Agnihotri, Shubham and Singh, Sanjay Kumar and Singh, Umesh},
  doi          = {10.1007/s00180-024-01565-3},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2635-2655},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian inferences and prediction of exponentiated exponential distribution based on multiple interval censored data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple-response logistic regression modeling with application to an analysis of cirrhosis liver disease data. <em>CSTAT</em>, <em>40</em>(5), 2611-2634. (<a href='https://doi.org/10.1007/s00180-024-01575-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical data analysis, individual measurements usually include two or more responses, and some statistical correlations often exist between the responses. Especially in medical data analysis, observations are often binary responses. A class of multi-response logistic regression model based on a joint modeling approach is investigated in this paper, and an application to a group data of primary biliary cirrhosis diseases is considered. Firstly, we propose a new class of multi-response logistic distribution and investigate its statistical properties. Secondly, a multi-response logistic regression model is constructed using a latent variable model and multi-variate logistic error distribution. Furthermore, the parameter estimation method of the model is provided by applying the monte carlo expectation maximization (MCEM) algorithm and the multiple imputation method. Finally, numerical simulations and comparative predictions on a test set are performed to validate the finite sample performance of the proposed model, and the model is applied to a cirrhosis disease dataset for analysis.},
  archive      = {J_CSTAT},
  author       = {Jing-Nan, Yang and Yu-Zhu, Tian and Yue, Wang and Chun-Ho, Wu},
  doi          = {10.1007/s00180-024-01575-1},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2611-2634},
  shortjournal = {Comput. Stat.},
  title        = {Multiple-response logistic regression modeling with application to an analysis of cirrhosis liver disease data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Copula hidden markov model with unknown number of states. <em>CSTAT</em>, <em>40</em>(5), 2583-2610. (<a href='https://doi.org/10.1007/s00180-024-01571-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we investigate the construction of hidden Markov models (HMMs) with copulas serving as the emission distributions. Additionally, we relax the traditional assumption that the number of hidden states must be predetermined before model fitting. Instead, in our approach, the number of states is estimated simultaneously with other model parameters of the copula-HMM when datasets are applied. This is achieved by incorporating the hierarchical Dirichlet process as a prior during the Bayesian inference procedure. We provide a comprehensive algorithm for this methodology, including a detailed implementable example using the t-copula, which is a novel contribution not previously available in the copula literature. The proposed estimator was validated through simulation studies, which demonstrated its superiority over traditional BIC-based approaches for model selection in HMMs. Furthermore, we applied this methodology to real data to examine the dependence structure among stock markets.},
  archive      = {J_CSTAT},
  author       = {Liu, Yujian and Xie, Dejun and Yu, Siyi},
  doi          = {10.1007/s00180-024-01571-5},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2583-2610},
  shortjournal = {Comput. Stat.},
  title        = {Copula hidden markov model with unknown number of states},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The power of visualizing distributional differences: Formal graphical n-sample tests. <em>CSTAT</em>, <em>40</em>(5), 2553-2582. (<a href='https://doi.org/10.1007/s00180-024-01569-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical tests are available for the two-sample test of correspondence of distribution functions. From these, the Kolmogorov–Smirnov test provides also the graphical interpretation of the test results, in different forms. Here, we propose modifications of the Kolmogorov–Smirnov test with higher power. The proposed tests are based on the so-called global envelope test which allows for graphical interpretation, similarly as the Kolmogorov–Smirnov test. The tests are based on rank statistics and are suitable also for the comparison of n samples, with $$n \ge 2$$ . We compare the alternatives for the two-sample case through an extensive simulation study and discuss their interpretation. Finally, we apply the tests to real data. Specifically, we compare the height distributions between boys and girls at different ages, the sepal length distributions of different flower species, and distributions of standardized residuals from a time series model for different exchange courses using the proposed methodologies.},
  archive      = {J_CSTAT},
  author       = {Konstantinou, Konstantinos and Mrkvička, Tomáš and Myllymäki, Mari},
  doi          = {10.1007/s00180-024-01569-z},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2553-2582},
  shortjournal = {Comput. Stat.},
  title        = {The power of visualizing distributional differences: Formal graphical n-sample tests},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional sufficient variable selection with prior information. <em>CSTAT</em>, <em>40</em>(5), 2519-2551. (<a href='https://doi.org/10.1007/s00180-024-01563-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimension reduction and variable selection play crucial roles in high-dimensional data analysis. Numerous existing methods have been demonstrated to attain either or both of these goals. The Minimum Average Variance Estimation (MAVE) method and its variants are effective approaches to estimate directions on the Central Mean Subspace (CMS). The Sparse Minimum Average Variance Estimation (SMAVE) combines the concepts of sufficient dimension reduction and variable selection and has been demonstrated to exhaustively estimate CMS while simultaneously selecting informative variables using LASSO without assuming any specific model or distribution on the predictor variables. In many applications, however, researchers typically possess prior knowledge for a set of predictors that is associated with response. In the presence of a known set of variables, the conditional contribution of additional predictors provides a natural evaluation of the relative importance. Based on this concept, we propose the Conditional Sparse Minimum Average Variance Estimation (CSMAVE) method. By utilizing prior information and creating a meaningful conditioning set for SMAVE, we intend to select variables that will result in a more parsimonious model and a more accurate interpretation than SMAVE. We evaluate our strategy by analyzing simulation examples and comparing them to the SMAVE method. And a real-world dataset validates the applicability and efficiency of our method.},
  archive      = {J_CSTAT},
  author       = {Wang, Pei and Lu, Jing and Weng, Jiaying and Mitra, Shouryya},
  doi          = {10.1007/s00180-024-01563-5},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2519-2551},
  shortjournal = {Comput. Stat.},
  title        = {Conditional sufficient variable selection with prior information},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the effectiveness of partially deterministic bayesian neural networks. <em>CSTAT</em>, <em>40</em>(5), 2491-2518. (<a href='https://doi.org/10.1007/s00180-024-01561-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian neural networks (BNNs) with computationally expensive Hamiltonian Monte Carlo sampling methods are often considered to provide better predictive performance than the maximum a posterior (MAP) solution. Here, as an alternative to sampling all parameters of a BNN (full-random), we experimentally evaluate partially deterministic BNNs that fix some part of the neural network parameters to their MAP solution. In particular, we consider various strategies for fixing half, or all parameters of a layer to the MAP-solution. Over a wide variety of regression and classification tasks, we find that partially deterministic BNNs often significantly improve predictive performance over the MAP-solution, with up to around 24% reduction in negative log-likelihood. Notably, we also find that partially deterministic BNNs that fix half of the parameters in each layer can also reduce under-fitting of full-random BNNs, resulting in up to 7% reduction in negative log-likelihood.},
  archive      = {J_CSTAT},
  author       = {Andrade, Daniel and Sato, Koki},
  doi          = {10.1007/s00180-024-01561-7},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2491-2518},
  shortjournal = {Comput. Stat.},
  title        = {On the effectiveness of partially deterministic bayesian neural networks},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Bayesian estimation of the number of species from Poisson–Lindley stochastic abundance model using non-informative priors. <em>CSTAT</em>, <em>40</em>(5), 2471-2489. (<a href='https://doi.org/10.1007/s00180-024-01550-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is a correction for issues encountered in Pathak et al. (Comput Stat 1–26, 2024). The main issue was the wrong formulas used for the elements of the information matrix of Poisson Lindley stochastic abundance model which affected both the frequentist estimation and Bayesian estimation. The illustrative example employed in Pathak et al. (Comput Stat 1–26, 2024) was revised accordingly and new results were reported.},
  archive      = {J_CSTAT},
  author       = {Dyab, Alaaeldin Mohamed and Muhammed, Hiba Zeyada},
  doi          = {10.1007/s00180-024-01550-w},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2471-2489},
  shortjournal = {Comput. Stat.},
  title        = {Correction to: Bayesian estimation of the number of species from Poisson–Lindley stochastic abundance model using non-informative priors},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel finite mixture model based on the generalized scale mixtures of asymmetric generalized normal distributions: Properties, estimation methodology and applications. <em>CSTAT</em>, <em>40</em>(5), 2425-2470. (<a href='https://doi.org/10.1007/s00180-024-01534-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a family of distributions known as generalized scale mixtures of asymmetric generalized normal distributions (GSMAGN), characterized by remarkable flexibility in shape. We propose a novel finite mixture model based on this distribution family, offering an effective tool for modeling intricate data featuring skewness, heavy tails, and multi-modality. To facilitate parameter estimation for this model, we devise an ECM-PLA ensemble algorithm that combines the Profile Likelihood Approach (PLA) with the classical Expectation Conditional Maximization (ECM) algorithm. By incorporating analytical expressions in the E-step and manageable computations in the M-step, this approach significantly enhances computational speed and overall efficiency. Furthermore, we persent the closed-form expressions for the observed information matrix, which serves as an approximation for the asymptotic covariance matrix of the maximum likelihood estimates. Additionally, we expound upon the corresponding consistency characteristics inherent to this particular mixture model. The applicability of the proposed model is elucidated through several simulation studies and practical datasets.},
  archive      = {J_CSTAT},
  author       = {Guan, Ruijie and Jiao, Junjun and Cheng, Weihu and Hu, Guozhi},
  doi          = {10.1007/s00180-024-01534-w},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2425-2470},
  shortjournal = {Comput. Stat.},
  title        = {A novel finite mixture model based on the generalized scale mixtures of asymmetric generalized normal distributions: Properties, estimation methodology and applications},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new class of information criteria for improved prediction in the presence of training/validation data heterogeneity. <em>CSTAT</em>, <em>40</em>(5), 2389-2423. (<a href='https://doi.org/10.1007/s00180-024-01559-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information criteria provide a cogent approach for identifying models that provide an optimal balance between the competing objectives of goodness-of-fit and parsimony. Models that better conform to a dataset are often more complex, yet such models are plagued by greater variability in estimation and prediction. Conversely, overly simplistic models reduce variability at the cost of increases in bias. Asymptotically efficient criteria are those that, for large samples, select the fitted candidate model whose predictors minimize the mean squared prediction error, optimizing between prediction bias and variability. In the context of prediction, asymptotically efficient criteria are thus a preferred tool for model selection, with the Akaike information criterion (AIC) being among the most widely used. However, asymptotic efficiency relies upon the assumption of a panel of validation data generated independently from, but identically to, the set of training data. We argue that assuming identically distributed training and validation data is misaligned with the premise of prediction and often violated in practice. This is most apparent in a regression context, where assuming training/validation data homogeneity requires identical panels of regressors. We therefore develop a new class of predictive information criteria (PIC) that do not assume training/validation data homogeneity and are shown to generalize AIC to the more practically relevant setting of training/validation data heterogeneity. The analytic properties and predictive performance of these new criteria are explored within the traditional regression framework. We consider both simulated and real-data settings. Software for implementing these methods is provided in the R package, picR, available through CRAN.},
  archive      = {J_CSTAT},
  author       = {Flores, Javier E. and Cavanaugh, Joseph E. and Neath, Andrew A.},
  doi          = {10.1007/s00180-024-01559-1},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2389-2423},
  shortjournal = {Comput. Stat.},
  title        = {A new class of information criteria for improved prediction in the presence of training/validation data heterogeneity},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast divide-and-conquer strategy for single-index model with massive data. <em>CSTAT</em>, <em>40</em>(5), 2367-2387. (<a href='https://doi.org/10.1007/s00180-024-01562-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of modern technology, massive data has received widespread attention. Constrained by computer performance, traditional statistical analysis methods are difficult to obtain quantitative analysis results of massive data. Currently, the most popular analytical method for massive data is the divide-and-conquer(DC) strategy, but relevant studies rarely mention the fitting of semiparametric models under massive data. In this paper, we combine the ideas of DC and refined outer product gradient (rOPG) to propose DC-lsrOPG and DC-qrOPG methods for analyzing massive data for single-index models, based on least squares regression and quantile regression, respectively. The newly developed method significantly reduces the amount of main memory required and running time. The asymptotic normality of the proposed method has been established under some mild conditions. The resulting estimators are theoretically as efficient as the traditional rOPG estimators on the entire data. Some simulation studies and a real data analysis are conducted to illustrate the finite sample performance of the proposed methods.},
  archive      = {J_CSTAT},
  author       = {Li, Na and Yang, Jing},
  doi          = {10.1007/s00180-024-01562-6},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2367-2387},
  shortjournal = {Comput. Stat.},
  title        = {A fast divide-and-conquer strategy for single-index model with massive data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group sparse sufficient dimension reduction: A model-free group variable selection method. <em>CSTAT</em>, <em>40</em>(5), 2323-2366. (<a href='https://doi.org/10.1007/s00180-024-01547-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many scientific applications, the covariates fall naturally into different groups, for example, the genes can be grouped by biological pathways in biological studies. In this study, we propose a new model-free group variable selection method by introducing a novel penalty, called adaptive group composite penalty. The proposed method can simultaneously achieve both sufficient dimension reduction and group variable selection in the case of diverging number of covariates. It can also simultaneously select important individual and group variables in a model-free fashion. An iterative two-stage algorithm is built to carry out the proposed method by reformulating the penalized objective functions. We provide the penalized sufficient dimension reduction estimators that estimate the targeted central subspace, and study their asymptotic properties. Simulation studies show that the proposed method gains significant efficiency in dimension reduction and variable selection, and it outperforms the other classical sparse sufficient dimension reduction methods in removing unimportant covariates, especially the unimportant groups. We illustrate the proposed method using a data set of RNA splicing signals.},
  archive      = {J_CSTAT},
  author       = {Cai, Kaida and Lu, Xuewen and Shen, Hua},
  doi          = {10.1007/s00180-024-01547-5},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2323-2366},
  shortjournal = {Comput. Stat.},
  title        = {Group sparse sufficient dimension reduction: A model-free group variable selection method},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting the cost of drought events in france by super learning from a short time series of many slightly dependent data. <em>CSTAT</em>, <em>40</em>(5), 2277-2321. (<a href='https://doi.org/10.1007/s00180-024-01549-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drought events are the second most expensive type of natural disaster within the French legal framework known as the natural disasters compensation scheme. In recent years, drought events have been remarkable in their geographical location and scale and in their intensity. We develop and apply a new methodology to forecast the cost of a drought event in France. The methodology hinges on Super Learning (van der Laan et al. in Stat Appl Genet Mol Biol 6:23, 2007; Benkeser et al. Stat Med 37:249-260, 2018), a general aggregation strategy to learn a feature of the law of the data identified through an ad hoc risk function by relying on a library of algorithms. The algorithms either compete (discrete Super Learning) or collaborate (continuous Super Learning), with a cross-validation scheme determining the best performing algorithm or combination of algorithms. The theoretical analysis reveals that our Super Learner can learn from a short time series where each time-t-specific data-structure consists of many slightly dependent data indexed by a. We use a dependency graph to model the amount of conditional independence within each t-specific data-structure and a concentration inequality by Janson (Random Struct Algorithms 24:234-248, 2004) and leverage a large ratio of the number of distinct a-s to the degree of the dependency graph in the face of a small number of t-specific data-structures.},
  archive      = {J_CSTAT},
  author       = {Ecoto, Geoffrey and Bibaut, Aurélien F. and Chambaz, Antoine},
  doi          = {10.1007/s00180-024-01549-3},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2277-2321},
  shortjournal = {Comput. Stat.},
  title        = {Forecasting the cost of drought events in france by super learning from a short time series of many slightly dependent data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayes estimation of ratio of scale-like parameters for inverse gaussian distributions and applications to classification. <em>CSTAT</em>, <em>40</em>(5), 2249-2276. (<a href='https://doi.org/10.1007/s00180-024-01554-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider two inverse Gaussian populations with a common mean but different scale-like parameters, where all parameters are unknown. We construct noninformative priors for the ratio of the scale-like parameters to derive matching priors of different orders. Reference priors are proposed for different groups of parameters. The Bayes estimators of the common mean and ratio of the scale-like parameters are also derived. We propose confidence intervals of the conditional error rate in classifying an observation into inverse Gaussian distributions. A generalized variable-based confidence interval and the highest posterior density credible intervals for the error rate are computed. We estimate parameters of the mixture of these inverse Gaussian distributions and obtain estimates of the expected probability of correct classification. An intensive simulation study has been carried out to compare the estimators and expected probability of correct classification. Real data-based examples are given to show the practicality and effectiveness of the estimators.},
  archive      = {J_CSTAT},
  author       = {Chakraborty, Ankur and Jana, Nabakumar},
  doi          = {10.1007/s00180-024-01554-6},
  journal      = {Computational Statistics},
  month        = {6},
  number       = {5},
  pages        = {2249-2276},
  shortjournal = {Comput. Stat.},
  title        = {Bayes estimation of ratio of scale-like parameters for inverse gaussian distributions and applications to classification},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data science approach to simulating the FIFA world cup qatar 2022 at a website in tribute to maradona. <em>CSTAT</em>, <em>40</em>(4), 2223-2247. (<a href='https://doi.org/10.1007/s00180-024-01557-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article documents the authors’ experience developing an Argentinean website in tribute to Diego Maradona (301060.exactas.uba.ar) that leverages the popularity of football in South America (and the world) to illustrate the application of data science models in sports analytics. In particular, we demonstrate their use in computing probabilities associated with various events (winning matches, advancing rounds, and becoming champions) of the FIFA World Cup Qatar 2022. Building on Dixon and Cole’s 1997 seminal model, we develop a competing Poisson model that incorporates for each participating team its attack and defense strengths as well as home-advantage effects. The calibration of the model considers match importance levels and emphasizes the recency of a team’s performance. Evaluations of the model’s results on various prediction accuracy and error metrics indicate that its performance equals or betters the traditional Poisson model and is similar to established betting sites. Our website featuring the model received over 30,000 visits from 11,000 users across 10 countries during the 2022 World Cup and garnered significant media coverage in Argentina. This successful endeavor underlines the potential of mathematics for predicting football match outcomes but also showcases its potential for countless practical applications and its ability to capture the attention and interest of a wide audience.},
  archive      = {J_CSTAT},
  author       = {Álvarez, Alejandro and Cataldo, Alejandro and Durán, Guillermo and Durán, Manuel and Galaz, Pablo and Monardo, Iván and Sauré, Denis},
  doi          = {10.1007/s00180-024-01557-3},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2223-2247},
  shortjournal = {Comput. Stat.},
  title        = {Data science approach to simulating the FIFA world cup qatar 2022 at a website in tribute to maradona},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can the hot hand phenomenon be modelled? a bayesian hidden markov approach. <em>CSTAT</em>, <em>40</em>(4), 2195-2222. (<a href='https://doi.org/10.1007/s00180-024-01560-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports data analytics has been gaining importance over recent years as an essential topic in applied statistics. Specifically, basketball has emerged as one of the iconic sports where the use and immediate collection of data have become widespread. Within this domain, the hot hand phenomenon has sparked a significant scientific controversy, with sceptics claiming its non-existence while other authors provide evidence for it. We propose a Bayesian longitudinal hidden Markov model that examines the hot hand phenomenon in consecutive shots of a basketball team, each of which can be either missed or made. We assume two states (cold or hot) in the hidden Markov chains associated with each math and model the probability of success for each shot with regard the hidden state, the random effects related the match, and the covariates. This model is applied to real data sets of three teams from the USA National Basketball Association: the Miami Heat team and the Toronto Raptors team in the 2005–2006 season, and the Chicago Bulls in the 2022–2023 season. We show that this model is a powerful tool for assessing the overall performance of a team during a game and, in particular, for quantifying the magnitude of team streaks in probabilistic terms.},
  archive      = {J_CSTAT},
  author       = {Calvo, Gabriel and Armero, Carmen and Spezia, Luigi},
  doi          = {10.1007/s00180-024-01560-8},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2195-2222},
  shortjournal = {Comput. Stat.},
  title        = {Can the hot hand phenomenon be modelled? a bayesian hidden markov approach},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ranking handball teams from statistical strength estimation. <em>CSTAT</em>, <em>40</em>(4), 2183-2194. (<a href='https://doi.org/10.1007/s00180-024-01522-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a methodology to estimate the strength of handball teams. We propose the use of the Conway-Maxwell-Poisson distribution to model the number of goals scored by a team as a flexible discrete distribution which can handle situations of non equi-dispersion. From its parameters, we derive a mathematical formula to determine the strength of a team. We propose a ranking based on the estimated strengths to compare teams across different championships. Applied to female handball club data from European competitions over the 2022/2023 season, we show that our new proposed ranking can have an echo in real sports events and is linked to recent results from European competitions.},
  archive      = {J_CSTAT},
  author       = {Felice, Florian},
  doi          = {10.1007/s00180-024-01522-0},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2183-2194},
  shortjournal = {Comput. Stat.},
  title        = {Ranking handball teams from statistical strength estimation},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lasso multinomial performance indicators for in-play basketball data. <em>CSTAT</em>, <em>40</em>(4), 2157-2181. (<a href='https://doi.org/10.1007/s00180-025-01604-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A typical approach to quantify the contribution of each player in basketball uses the plus–minus method. The ratings obtained by such a method are estimated using simple regression models and their regularized variants, with response variable being either the points scored or the point differences. To capture more precisely the effect of each player, detailed possession-based play-by-play data may be used. This is the direction we take in this article, in which we investigate the performance of regularized adjusted plus–minus (RAPM) indicators estimated by different regularized models having as a response the number of points scored in each possession. Therefore, we use possession play-by-play data from all NBA games for the season 2021–2022 (322,852 possessions). We initially present simple regression model-based indices starting from the implementation of ridge regression which is the standard technique in the relevant literature. We proceed with the lasso approach which has specific advantages and better performance than ridge regression when compared with selected objective validation criteria. Then, we implement regularized binary and multinomial logistic regression models to obtain more accurate performance indicators since the response is a discrete variable taking values mainly from zero to three. Our final proposal is an improved RAPM measure which is based on the expected points of a multinomial logistic regression model where each player’s contribution is weighted by his participation in the team’s possessions. The proposed indicator, called weighted expected points (wEPTS), outperforms all other RAPM measures we investigate in this study.},
  archive      = {J_CSTAT},
  author       = {Damoulaki, Argyro and Ntzoufras, Ioannis and Pelechrinis, Konstantinos},
  doi          = {10.1007/s00180-025-01604-7},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2157-2181},
  shortjournal = {Comput. Stat.},
  title        = {Lasso multinomial performance indicators for in-play basketball data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the CTA-PLS test for hierarchical models: An application to the football player’s performance. <em>CSTAT</em>, <em>40</em>(4), 2135-2155. (<a href='https://doi.org/10.1007/s00180-024-01566-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to develop and explore an inferential procedure for implementing the Confirmatory tetrad analysis (CTA-PLS) multiple test in a hierarchical Partial least squares structural equation model (PLS-SEM) framework for addressing theoretical constructs (reflective or formative); the approach is applied to evaluate the performance of football players. In the paper, the procedure for the second order (potentially extendable to higher orders) is proposed, then the results of a simulation to estimate the actual significance level and power are presented; finally, the multiple test aims to specify an original second-order hierarchical model, proposing an alternative for measuring goalkeepers performance. In the simulation, the Bonferroni and the Benjamini-Hochberg corrections are considered across diverse scenarios, and the results are that the Bonferroni correction tends to be overly conservative, exhibiting diminished power, particularly evident in smaller sample sizes. The sport application illustrates the efficacy of this approach in measuring goalkeeper performance within the world of football analytics, revealing some interesting and useful implications for football stakeholders.},
  archive      = {J_CSTAT},
  author       = {Cefis, Mattia and Carpita, Maurizio},
  doi          = {10.1007/s00180-024-01566-2},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2135-2155},
  shortjournal = {Comput. Stat.},
  title        = {On the CTA-PLS test for hierarchical models: An application to the football player’s performance},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survival modeling of goal arrival times in english premier league. <em>CSTAT</em>, <em>40</em>(4), 2109-2133. (<a href='https://doi.org/10.1007/s00180-024-01589-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction and modeling of association football (soccer) outcomes has gained increasing interest in the scientific community in recent years, both due to betting concerns and the need for a deeper understanding of the factors influencing soccer events. We introduce and examine the validity of a Bayesian model, which belongs to the class of accelerated failure time (survival) models and is characterized by its straightforward structure. We implement MCMC methodology to estimate the posterior summaries of the model parameters and suggest a novel algorithm that can be used to transform simulated goal arrival times into predicted goals. The proposed model achieves exceptional in-sample and out-of-sample performance by replicating the entire league in a remarkably precise manner and by making accurate predictions on the second half of the league using the first half as a training dataset. The structure of the proposed model is extendable, allowing for the inclusion of in-play covariates that can be used to further map the complex dynamics of soccer matches.},
  archive      = {J_CSTAT},
  author       = {Leriou, Ilias and Ntzoufras, Ioannis},
  doi          = {10.1007/s00180-024-01589-9},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2109-2133},
  shortjournal = {Comput. Stat.},
  title        = {Survival modeling of goal arrival times in english premier league},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The game beyond the field: On football players’ performance through social media, sentiment and topic analysis. <em>CSTAT</em>, <em>40</em>(4), 2085-2108. (<a href='https://doi.org/10.1007/s00180-024-01584-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the complex relationship between social media sentiment and football players’ performance in the English Premier League (EPL). We adapt the TOpic modeling Based Index Assessment through Sentiment (TOBIAS) framework, originally developed for educational settings, to the domain of sports analytics. This novel application faces difficulties in handling the volume and variability of social media data, as well as in accurately linking pre-match sentiments to post-match performance metrics. Our methodology integrates advanced Natural Language Processing (NLP) techniques, including sentiment analysis and topic modeling, with Partial Least Squares Path Modeling (PLS-PM). We analyze a dataset of 167,841 tweets related to 512 English Premier League (EPL) players, collected from May 2022 to May 2023. The study is conducted in two phases: pre-match analysis to assess public expectations, and post-match analysis to evaluate reactions to player performances. Experimental analysis reveals significant correlations between pre-match sentiments and subsequent player performance, with negative sentiments showing a stronger predictive power than positive ones. Post-match, we observe a shift in the relationship between sentiments and performance metrics, indicating the public’s responsiveness to match outcomes. Our findings contribute to the broader understanding of social media’s role in sports performance and offer insights for potential applications in regulating online behaviors in sports contexts.},
  archive      = {J_CSTAT},
  author       = {Ortu, Marco and Mola, Francesco},
  doi          = {10.1007/s00180-024-01584-0},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2085-2108},
  shortjournal = {Comput. Stat.},
  title        = {The game beyond the field: On football players’ performance through social media, sentiment and topic analysis},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the time of corner kicks in soccer: An analysis of event history data. <em>CSTAT</em>, <em>40</em>(4), 2067-2083. (<a href='https://doi.org/10.1007/s00180-024-01567-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To understand the patterns of times to corner kicks in soccer and how they are associated with a few important factors, we analyze the corner kick records from the 2019 regular season of the Chinese Super League. This paper is particularly concerned with the elapsed time to a corner kick from a natural starting point. We overcome 2 challenges arising from such time-to-event analyses, which have not been discussed in the sports analytics literature. The first is that observations of times to corner kicks are subject to right-censoring. A given soccer starting point rarely ends with a corner kick but the occurrence of a different terminal event. The second issue is the mixture feature of short and typical gap times to the next corner kick from a particular one. There is often a subsequent corner kick quickly following a corner kick. The conventional event time models are thus inappropriate for formulating distributions of corner kick times. Our analysis reveals how the timing of corner kicks is associated with the factors of first versus second half of the game, home versus away team, score differential, betting odds prior to the game, and red card differential. We present applications of the developed statistical model for prediction to support tactics and sports betting.},
  archive      = {J_CSTAT},
  author       = {Peng, K. Ken and Hu, X. Joan and Swartz, Tim B.},
  doi          = {10.1007/s00180-024-01567-1},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2067-2083},
  shortjournal = {Comput. Stat.},
  title        = {On the time of corner kicks in soccer: An analysis of event history data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eras of dominance: Identifying strong and weak periods in professional tennis. <em>CSTAT</em>, <em>40</em>(4), 2049-2066. (<a href='https://doi.org/10.1007/s00180-024-01578-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sports journalism and among fans, there is an ongoing debate on identifying eras where the level of competition is extremely high. In tennis, a common question concerning the advent of the so-called ‘Big Three’—listed alphabetically, Novak Djokovic, Roger Federer, and Rafael Nadal—is: Did these players lead to an unprecedented high level of competition? We contribute to this debate by identifying, from a statistical point of view, strong players, periods, and eras in men’s tennis, where a strong era is defined as a time frame in which a subset of (strong) players consistently dominate all the others. Hence, this work extends the idea of the Greatest Player of All Time (GOAT), largely investigated in the literature, to a dynamic subset of players. Through cointegration analysis of over 30 years of professional tennis data, we identify five strong eras. Interestingly, the player with the largest participation during these strong eras is Roger Federer and the most recent strong era concluded in July 2019. Moreover, we examine the relationship between the match duration and strong players/periods/eras, finding that the occurrence of a match between strong and not-strong players decreases the match duration, on average. Furthermore, when strong players meet, the match duration generally increases.},
  archive      = {J_CSTAT},
  author       = {Breznik, Kristijan and Candila, Vincenzo and Milekhina, Antonina and Restaino, Marialuisa},
  doi          = {10.1007/s00180-024-01578-y},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2049-2066},
  shortjournal = {Comput. Stat.},
  title        = {Eras of dominance: Identifying strong and weak periods in professional tennis},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A model-based approach to shot charts estimation in basketball. <em>CSTAT</em>, <em>40</em>(4), 2031-2048. (<a href='https://doi.org/10.1007/s00180-025-01599-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shot charts in basketball analytics provide an indispensable tool for evaluating players’ shooting performance by visually representing the distribution of field goal attempts across different court locations. However, conventional methods often overlook the bounded nature of the basketball court, leading to inaccurate representations, particularly along the boundaries and corners. In this paper, we propose a novel model-based approach to shot charts estimation and visualization that explicitly considers the physical boundaries of the basketball court. By employing Gaussian mixtures for bounded data, our methodology allows to obtain more accurate estimation of shot density distributions for both made and missed shots. Bayes’ rule is then applied to derive estimates for the probability of successful shooting from any given locations, and to identify the regions with the highest expected scores. Additionally, calibration plots are introduced to compare the estimated scoring probabilities with the observed proportions of made shots across different offensive areas, complemented by the normalized calibration error to summarize the overall goodness-of-fit of the model-based estimates. To illustrate the efficacy of our proposal, we apply it to data from the 2022/2023 NBA regular season, showing its usefulness through detailed analyses of shot patterns and calibration performance for two prominent players.},
  archive      = {J_CSTAT},
  author       = {Scrucca, Luca and Karlis, Dimitris},
  doi          = {10.1007/s00180-025-01599-1},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2031-2048},
  shortjournal = {Comput. Stat.},
  title        = {A model-based approach to shot charts estimation in basketball},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Should sports professionals consider their adversary’s strategy? a case study of match play in golf. <em>CSTAT</em>, <em>40</em>(4), 2005-2029. (<a href='https://doi.org/10.1007/s00180-024-01555-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores strategic considerations in professional golf’s Match Play format. Leveraging Professional Golfers’ Association Tour data, we investigate the impact of factoring in an adversary’s strategy. Our findings suggest that while slight strategy adjustments can be advantageous in specific scenarios, the overall benefit of considering an opponent’s strategy remains modest. This confirms the common wisdom in golf, reinforcing the recommendation to adhere to optimal stroke-play strategies due to challenges in obtaining precise opponent statistics. The methodology employed here is generic and could offer valuable insights into whether opponents’ performances should also be considered in other two-player or team sports, such as tennis, darts, soccer, volleyball, etc. We hope that this research will pave the way for new avenues of study in these areas.},
  archive      = {J_CSTAT},
  author       = {Wajge, Nishad and Stauffer, Gautier},
  doi          = {10.1007/s00180-024-01555-5},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {2005-2029},
  shortjournal = {Comput. Stat.},
  title        = {Should sports professionals consider their adversary’s strategy? a case study of match play in golf},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of the frailty in the evaluation of injury risk factors for national basketball association players. <em>CSTAT</em>, <em>40</em>(4), 1985-2003. (<a href='https://doi.org/10.1007/s00180-024-01556-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Injuries often occur in sports and, due to medical and economic reasons, it is important to understand the factors that mainly affect the risk of experiencing them. This work aims to explore this field in the context of the National Basketball Association (NBA) league. Thus, the main purpose is to identify the main individual players’ characteristics that are associated to a higher risk of suffering an injury in a shorter time, taking into account ten seasons, from the beginning of 2010–2011 season until the end of 2019–2020 season. All the needed information has been retrieved from different big datasets regarding NBA players. The work stands in the survival data analysis framework and, for the purpose, a Cox regression model with frailty has been used. Results suggest that the player’s position and the Body Mass Index have a significant effect on the injury’s risk. From a methodological point of view, this manuscript provides an insight into the role of the frailty in the model, studying its relationship with the residuals of a mispecified Cox model.},
  archive      = {J_CSTAT},
  author       = {Macis, Ambra},
  doi          = {10.1007/s00180-024-01556-4},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1985-2003},
  shortjournal = {Comput. Stat.},
  title        = {The role of the frailty in the evaluation of injury risk factors for national basketball association players},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting the opening goal in second-half of a football match: Bayesian and frequentist perspectives. <em>CSTAT</em>, <em>40</em>(4), 1959-1984. (<a href='https://doi.org/10.1007/s00180-024-01558-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the timing of the first goal in the second half of a football match can offer valuable insights and strategic implications, as it can inform tactical adjustments at halftime based on first-half performance. This particular issue has not received much attention in sports analytics despite its significance. This study utilises survival analysis techniques to model the time-to-event of the opening goal after halftime using data from the Indian Super League (ISL), focusing on examining how a team’s first-half performance statistics impact their scoring timing in the second half. The extended Cox model revealed that the number of completed passes in the first half is a statistically significant factor that affects the timing of the initial second-half goal. In an effort to enhance the comprehensiveness of the analysis, the study then transits to the Bayesian proportional hazards model and the Bayesian Accelerated Failure Time (AFT) models. It was found that the number of corners and goal saves by the teams were significant indicators of the timing of the opening second-half goal. The unique aspect of this study lies in its innovative application of Bayesian survival modelling techniques to football-related data. A comparison of the models indicates that the Bayesian viewpoint exhibits superiority in this evaluation. Through the quantification of crucial in-game metrics on scoring trends post-halftime, this framework presents a valuable tool for sports analysts and coaches to assess strategic choices during crucial intervals of a match. Furthermore, the methodologies investigated have the potential for extension to other top-tier leagues and sports, paving the way for improved data-informed decision-making and intra-match analysis in professional sports.},
  archive      = {J_CSTAT},
  author       = {Dutta, Anirban and Saikia, Hemanta and Gogoi, Jonali and Bhattacharjee, Dibyojyoti},
  doi          = {10.1007/s00180-024-01558-2},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1959-1984},
  shortjournal = {Comput. Stat.},
  title        = {Forecasting the opening goal in second-half of a football match: Bayesian and frequentist perspectives},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification of genes shared between sedentary behaviour and inflammation: A bivariate genetic correlation analysis. <em>CSTAT</em>, <em>40</em>(4), 1933-1958. (<a href='https://doi.org/10.1007/s00180-024-01551-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sedentary behaviour is associated with increased risk of several chronic conditions and all-cause mortality. Leisure screen time (LST) represents a substantial component of sedentary behaviour and is linked with higher levels of inflammation and increased body mass index (BMI), which in turn predispose to different disorders and negative health outcomes, both in the general population as well as in people with high levels of physical activity. Sedentary behaviour and inflammation are complex traits characterized by the interaction between genetic and environmental factors. While some of the genetic factors underlying LST have been suggested to be shared with body composition, to our knowledge the pleiotropy between LST and inflammation has not been investigated. In this study, we used global and local genetic correlation to identify genetic determinants shared between LST and two inflammatory markers, C-reactive protein (CRP) and interleukin 6 (IL6), using large genomic datasets. We compare results obtained with the definition of genomic loci based on the genome partitioning algorithm proposed in LAVA and a modified version in which loci are defined based on gene coordinates. We found a significant global genetic correlation between LST and CRP ( $$r_g$$ = 0.37, p= 1.6E-34) or IL6 ( $$r_g$$ = 0.41, p = 1.1E-08) and identified several genes shared between LST and inflammatory markers, dissecting loci for which the association was mediated by, or independent from, BMI. Our results provide novel knowledge on shared genetic determinants between sedentary behaviour and inflammation and suggest that different definitions of genomic loci can allow to obtain complementary information when using local genetic correlation analysis.},
  archive      = {J_CSTAT},
  author       = {Zammarchi, Gianpaolo and Pisanu, Claudia},
  doi          = {10.1007/s00180-024-01551-9},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1933-1958},
  shortjournal = {Comput. Stat.},
  title        = {Identification of genes shared between sedentary behaviour and inflammation: A bivariate genetic correlation analysis},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The home advantage and COVID-19: The crowd support effect on the english football premier league and the championship. <em>CSTAT</em>, <em>40</em>(4), 1919-1932. (<a href='https://doi.org/10.1007/s00180-025-01600-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that there is an home advantage in football (American English: soccer) where the home team wins in about 45% of the games compared to the 27% of the away team. This has mainly been attributed to the support of the home audience and is commonly denoted the crowd support effect. The COVID-19 pandemic forced many football leagues to play the games without spectators thus making it possible to analyse the effect of crowd support in football. We analyse more than 18,000 games in the two top English football leagues during the period 2001–2020 for the Premier league and the Championship with an ordinal logistic model with explanatory variables (e.g., previous team performance, a time trend, league dummy) including a pandemic dummy. We discovered that the absence of spectators has no impact on the outcome probability in the Premier League. However, it significantly reduces the probability of the home team wins in the Championship.},
  archive      = {J_CSTAT},
  author       = {Lyhagen, Johan},
  doi          = {10.1007/s00180-025-01600-x},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1919-1932},
  shortjournal = {Comput. Stat.},
  title        = {The home advantage and COVID-19: The crowd support effect on the english football premier league and the championship},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alternative ranking measures to predict international football results. <em>CSTAT</em>, <em>40</em>(4), 1899-1917. (<a href='https://doi.org/10.1007/s00180-024-01585-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities. In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 FIFA World Cup and the 2023 CAF Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams. More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football. The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs.},
  archive      = {J_CSTAT},
  author       = {Macrì Demartino, Roberto and Egidi, Leonardo and Torelli, Nicola},
  doi          = {10.1007/s00180-024-01585-z},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1899-1917},
  shortjournal = {Comput. Stat.},
  title        = {Alternative ranking measures to predict international football results},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A copula-based bayesian framework for doping detection. <em>CSTAT</em>, <em>40</em>(4), 1873-1898. (<a href='https://doi.org/10.1007/s00180-024-01579-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doping control is an essential component of anti-doping organizations for protecting sport competitions. Since 2009, this mission has been complemented worldwide by the Athlete Biological Passport (ABP), used to monitor athletes’ individual profiles over time. The practical implementation of the ABP is based on a Bayesian framework, called ADAPTIVE, intended to identify individual reference ranges outside of which an observation may indicate doping abuse. Currently, this method follows a univariate approach, relying on simultaneous analysis of different markers. This work extends the ADAPTIVE method to a multivariate testing framework, making use of copula models to couple the marginal distribution of biomarkers with their dependence structure. After introducing the proposed copula-based hierarchical model, we discuss our approach to inference, grounded in a Bayesian spirit, and present an extension to multidimensional predictive reference regions. Focusing on the hematological module of the ABP, we evaluate the proposed framework in both data-driven simulations and real data.},
  archive      = {J_CSTAT},
  author       = {Deliu, Nina and Liseo, Brunero},
  doi          = {10.1007/s00180-024-01579-x},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1873-1898},
  shortjournal = {Comput. Stat.},
  title        = {A copula-based bayesian framework for doping detection},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kendall correlations and radar charts to include goals for and goals against in soccer rankings. <em>CSTAT</em>, <em>40</em>(4), 1849-1872. (<a href='https://doi.org/10.1007/s00180-024-01542-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the challenging themes of the way sporting teams and athletes are ranked in sports competitions. Starting from the paradigmatic case of soccer, we advance a new method for ranking teams in the official national championships through computational statistics methods based on Kendall correlations and radar charts. In detail, we consider the goals for and against the teams in the individual matches as a further source of score assignment beyond the usual win-tie-lose trichotomy. Our approach overcomes some biases in the scoring rules that are currently employed. The methodological proposal is tested over the relevant case of the Italian “Serie A” championships played during 1930–2023.},
  archive      = {J_CSTAT},
  author       = {Cerqueti, Roy and Mattera, Raffaele and Ficcadenti, Valerio},
  doi          = {10.1007/s00180-024-01542-w},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1849-1872},
  shortjournal = {Comput. Stat.},
  title        = {Kendall correlations and radar charts to include goals for and goals against in soccer rankings},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysing kinematic data from recreational runners using functional data analysis. <em>CSTAT</em>, <em>40</em>(4), 1825-1847. (<a href='https://doi.org/10.1007/s00180-024-01591-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multivariate functional mixed effects model for kinematic data from a large number of recreational runners. The runners’ sagittal plane hip and knee angles are modelled jointly as a bivariate function with random effects functions accounting for the dependence among bilateral measurements. The model is fitted by applying multivariate functional principal component analysis (mv-FPCA) and modelling the mv-FPCA scores using scalar linear mixed effects models. Simulation and bootstrap approaches are introduced to construct simultaneous confidence bands for the fixed effects functions, and covariance functions are reconstructed to summarise the variability structure in the data and thoroughly investigate the suitability of the proposed model. In our scientific application, we observe a statistically significant effect of running speed on both joints. We observe strong within-subject correlations, reflecting the highly idiosyncratic nature of running technique. Our approach is applicable to modelling multiple streams of smooth biomechanical data collected in complex experimental designs.},
  archive      = {J_CSTAT},
  author       = {Gunning, Edward and Golovkine, Steven and Simpkin, Andrew J. and Burke, Aoife and Dillon, Sarah and Gore, Shane and Moran, Kieran and O’Connor, Siobhan and White, Enda and Bargary, Norma},
  doi          = {10.1007/s00180-024-01591-1},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1825-1847},
  shortjournal = {Comput. Stat.},
  title        = {Analysing kinematic data from recreational runners using functional data analysis},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical models for classification by handedness of olympic trap shooters in digital training services and remote coaching. <em>CSTAT</em>, <em>40</em>(4), 1801-1823. (<a href='https://doi.org/10.1007/s00180-024-01552-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of classification by handedness of Olympic Trap shooters applying statistical methods to newly available data gathered from the field. We assess the performance of binary classification models based on KNN and Binary Regression, with both symmetric and asymmetric link functions, in a context characterized by unbalanced data. Our results show promising classification performance, suitable for first non-critical applications in data driven training services and remote coaching, encouraging further future research.},
  archive      = {J_CSTAT},
  author       = {Zanardelli, Riccardo and Carpita, Maurizio and Manisera, Marica},
  doi          = {10.1007/s00180-024-01552-8},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1801-1823},
  shortjournal = {Comput. Stat.},
  title        = {Statistical models for classification by handedness of olympic trap shooters in digital training services and remote coaching},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate approaches to investigate the home and away behavior of football teams playing football matches. <em>CSTAT</em>, <em>40</em>(4), 1779-1799. (<a href='https://doi.org/10.1007/s00180-024-01553-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to other European competitions, participation in the Uefa Champions League is a real “bargain” for football clubs due to the hefty bonuses awarded based on performance during the group qualification phase. To perform successfully in football depends on several multidimensional factors, and analyzing the main ones remains challenging. In the performance study, little attention has been paid to teams’ behavior when playing at home and away. Our study combines statistical techniques to develop a procedure to examine teams’ performance. Several considerations make the 2022–2023 Serie A league season particularly interesting to analyze with our approach. Except for Napoli, all the teams showed different home-and-away behaviors concerning the results obtained at the season’s end. Ball possession and corners have positively influenced scored points in both home and away games with a different impact. The precision indicator was not an essential variable. The procedure highlighted the negative roles played by offside, as well as yellow and red cards.},
  archive      = {J_CSTAT},
  author       = {D’Ambra, Antonello and Amenta, Pietro and Lucadamo, Antonio},
  doi          = {10.1007/s00180-024-01553-7},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1779-1799},
  shortjournal = {Comput. Stat.},
  title        = {Multivariate approaches to investigate the home and away behavior of football teams playing football matches},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized reduced K–Means. <em>CSTAT</em>, <em>40</em>(4), 1753-1778. (<a href='https://doi.org/10.1007/s00180-024-01592-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of sports analytics, the evaluation of players’ performance has traditionally been a complex endeavor, given the multidimensional nature of the data involved. This paper introduces a novel approach for multivariate analyses of complex data sets, with a focus on professional basketball data. The proposed model simultaneously performs unsupervised classification of units into K clusters and their optimal low-dimensional reconstruction. This is done considering variables’ dimensionality representation into Q components for each group of clusters that can be identified by the same latent dimensions. Consequently, we refer to the new model as Generalized Reduced K-Means (GRKM), which includes RKM as a special case when a unique lower rank reconstruction of the variables is needed. Before the application on real data, the effectiveness of the proposal is shown by means of an extended simulation study. By applying this innovative method to a comprehensive set of National Basketball Association (NBA) statistics, we demonstrate its efficacy in distinguishing player profiles across offensive and defensive spectrums, simultaneously grouping them into coherent clusters.},
  archive      = {J_CSTAT},
  author       = {Bottazzi Schenone, Mariaelena and Rocci, Roberto and Vichi, Maurizio},
  doi          = {10.1007/s00180-024-01592-0},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1753-1778},
  shortjournal = {Comput. Stat.},
  title        = {Generalized reduced K–Means},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scoring probability maps in the basketball court with indicator kriging estimation. <em>CSTAT</em>, <em>40</em>(4), 1731-1751. (<a href='https://doi.org/10.1007/s00180-024-01564-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring players’ and teams’ shooting performance in the basketball court can give important information aimed to the definition of both game strategies and personalized training programs. From a methodological point of view, the estimation of the scoring probability can be faced by resorting to different tools in the field of statistical or algorithmic modelling. As a matter of fact, the most natural theoretical framework for this problem is that of spatial statistics, with the particularity that the analysis is based on the binary measurement variable informing about whether a shot is made or missed. In this paper we propose the use of spatial statistics tools suited to this specific context, namely lorelograms to investigate the spatial correlation and Indicator Kriging to draw scoring probability maps. A structured case study is presented, dealing with all the teams of the Italian Basketball First League, based on a non-public dataset containing substantive additional information, that allows interesting insights about assisted and uncontested shots.},
  archive      = {J_CSTAT},
  author       = {Carlesso, Mirko Luigi and Cappozzo, Andrea and Manisera, Marica and Zuccolotto, Paola},
  doi          = {10.1007/s00180-024-01564-4},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1731-1751},
  shortjournal = {Comput. Stat.},
  title        = {Scoring probability maps in the basketball court with indicator kriging estimation},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new metric for pitch control based on an intuitive motion model. <em>CSTAT</em>, <em>40</em>(4), 1713-1730. (<a href='https://doi.org/10.1007/s00180-024-01512-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the availability of tracking data, the determination of pitch control (field ownership) is an increasingly important topic in sports analytics. This paper reviews various approaches for the determination of pitch control and introduces a new field ownership metric that takes into account associated sporting dynamics. The methods that are proposed utilize the movement of the ball and players. Specifically, physical characteristics such as current velocity, acceleration and maximum velocity are considered. The determination of pitch control is based on the time that it takes the ball and the players to reach a given location. The main result of our investigation concerns the validation of the resultant pitch control diagram. Based on a sample of 5887 passes, the team identified as having pitch control was the observed recipient of the pass with 91% accuracy. The approach is generally applicable to invasion sports and is illustrated in the context of soccer. Various parameters are introduced that allow a user to modify the methods to alternative sports and to introduce player-specific maximum velocities and player-specific accelerations.},
  archive      = {J_CSTAT},
  author       = {Wu, Lucas and Swartz, Tim B.},
  doi          = {10.1007/s00180-024-01512-2},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1713-1730},
  shortjournal = {Comput. Stat.},
  title        = {A new metric for pitch control based on an intuitive motion model},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network and attribute-based clustering of tennis players and tournaments. <em>CSTAT</em>, <em>40</em>(4), 1689-1712. (<a href='https://doi.org/10.1007/s00180-024-01493-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at targeting some relevant issues for clustering tennis players and tournaments: (i) it considers players, tournaments and the relation between them; (ii) the relation is taken into account in the fuzzy clustering model based on the Partitioning Around Medoids (PAM) algorithm through spatial constraints; (iii) the attributes of the players and of the tournaments are of different nature, qualitative and quantitative. The proposal is novel for the methodology used, a spatial Fuzzy clustering model for players and for tournaments (based on related attributes), where the spatial penalty term in each clustering model depends on the relation between players and tournaments described in the adjacency matrix. The proposed model is compared with a bipartite players-tournament complex network model (the Degree-Corrected Stochastic Blockmodel) that considers only the relation between players and tournaments, described in the adjacency matrix, to obtain communities on each side of the bipartite network. An application on data taken from the ATP official website with regards to the draws of the tournaments, and from the sport statistics website Wheelo ratings for the performance data of players and tournaments, shows the performances of the proposed clustering model.},
  archive      = {J_CSTAT},
  author       = {D’Urso, Pierpaolo and De Giovanni, Livia and Federico, Lorenzo and Vitale, Vincenzina},
  doi          = {10.1007/s00180-024-01493-2},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1689-1712},
  shortjournal = {Comput. Stat.},
  title        = {Network and attribute-based clustering of tennis players and tournaments},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial: Special issue on sports data science. <em>CSTAT</em>, <em>40</em>(4), 1683-1688. (<a href='https://doi.org/10.1007/s00180-025-01622-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CSTAT},
  author       = {D’Urso, Pierpaolo and Gallo, Michele and Zuccolotto, Paola},
  doi          = {10.1007/s00180-025-01622-5},
  journal      = {Computational Statistics},
  month        = {4},
  number       = {4},
  pages        = {1683-1688},
  shortjournal = {Comput. Stat.},
  title        = {Editorial: Special issue on sports data science},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian adaptive lasso quantile regression with non-ignorable missing responses. <em>CSTAT</em>, <em>40</em>(3), 1643-1682. (<a href='https://doi.org/10.1007/s00180-024-01546-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a fully Bayesian adaptive lasso quantile regression model to analyze data with non-ignorable missing responses, which frequently occur in various fields of study. Specifically, we employ a logistic regression model to deal with missing data of non-ignorable mechanism. By using the asymmetric Laplace working likelihood for the data and specifying Laplace priors for the regression coefficients, our proposed method extends the Bayesian lasso framework by imposing specific penalization parameters on each regression coefficient, enhancing our estimation and variable selection capability. Furthermore, we embrace the normal-exponential mixture representation of the asymmetric Laplace distribution and the Student-t approximation of the logistic regression model to develop a simple and efficient Gibbs sampling algorithm for generating posterior samples and making statistical inferences. The finite-sample performance of the proposed algorithm is investigated through various simulation studies and a real-data example.},
  archive      = {J_CSTAT},
  author       = {Chen, Ranran and Dao, Mai and Ye, Keying and Wang, Min},
  doi          = {10.1007/s00180-024-01546-6},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1643-1682},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian adaptive lasso quantile regression with non-ignorable missing responses},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using the krylov subspace formulation to improve regularisation and interpretation in partial least squares regression. <em>CSTAT</em>, <em>40</em>(3), 1621-1642. (<a href='https://doi.org/10.1007/s00180-024-01545-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial least squares regression (PLS-R) has been an important regression method in the life sciences and many other fields for decades. However, PLS-R is typically solved using an opaque algorithmic approach, rather than through an optimisation formulation and procedure. There is a clear optimisation formulation of the PLS-R problem based on a Krylov subspace formulation, but it is only rarely considered. The popularity of PLS-R is attributed to the ability to interpret the data through the model components, but the model components are not available when solving the PLS-R problem using the Krylov subspace formulation. We therefore highlight a simple reformulation of the PLS-R problem using the Krylov subspace formulation as a promising modelling framework for PLS-R, and illustrate one of the main benefits of this reformulation—that it allows arbitrary penalties of the regression coefficients in the PLS-R model. Further, we propose an approach to estimate the PLS-R model components for the solution found through the Krylov subspace formulation, that are those we would have obtained had we been able to use the common algorithms for estimating the PLS-R model. We illustrate the utility of the proposed method on simulated and real data.},
  archive      = {J_CSTAT},
  author       = {Löfstedt, Tommy},
  doi          = {10.1007/s00180-024-01545-7},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1621-1642},
  shortjournal = {Comput. Stat.},
  title        = {Using the krylov subspace formulation to improve regularisation and interpretation in partial least squares regression},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust matrix factor analysis method with adaptive parameter adjustment using cauchy weighting. <em>CSTAT</em>, <em>40</em>(3), 1597-1620. (<a href='https://doi.org/10.1007/s00180-024-01548-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, high-dimensional matrix factor models have been widely applied in various fields. However, there are few methods that effectively handle heavy-tailed data. To address this problem, we introduced a smooth Cauchy loss function and established an optimization objective through norm minimization, deriving a Cauchy version of the weighted iterative estimation method. Unlike the Huber loss weighted estimation method, the weight calculation in this method is a smooth function rather than a piecewise function. It also considers the need to update parameters in the Cauchy loss function with each iteration during estimation. Ultimately, we propose a weighted estimation method with adaptive parameter adjustment. Subsequently, this paper analyzes the theoretical properties of the method, proving that it has a fast convergence rate. Through data simulation, our method demonstrates significant advantages. Thus, it can serve as a better alternative to other existing estimation methods. Finally, we analyzed a dataset of regional population movements between cities, demonstrating that our proposed method offers estimations with excellent interpretability compared to other methods.},
  archive      = {J_CSTAT},
  author       = {Li, Junchen},
  doi          = {10.1007/s00180-024-01548-4},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1597-1620},
  shortjournal = {Comput. Stat.},
  title        = {Robust matrix factor analysis method with adaptive parameter adjustment using cauchy weighting},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A precise and efficient exceedance-set algorithm for detecting environmental extremes. <em>CSTAT</em>, <em>40</em>(3), 1583-1595. (<a href='https://doi.org/10.1007/s00180-024-01540-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference for predicted exceedance sets is important for various environmental issues such as detecting environmental anomalies and emergencies with high confidence. A critical part is to construct inner and outer predicted exceedance sets using an algorithm that samples from the predictive distribution. The simple currently used sampling procedure can lead to misleading conclusions for some locations due to relatively large standard errors when proportions are estimated from independent observations. Instead we propose an algorithm that calculates probabilities numerically using the Genz–Bretz algorithm, which is based on quasi-random numbers leading to more accurate inner and outer sets, as illustrated on rainfall data in the state of Paraná, Brazil.},
  archive      = {J_CSTAT},
  author       = {Suesse, Thomas and Brenning, Alexander},
  doi          = {10.1007/s00180-024-01540-y},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1583-1595},
  shortjournal = {Comput. Stat.},
  title        = {A precise and efficient exceedance-set algorithm for detecting environmental extremes},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Change point estimation for gaussian time series data with copula-based markov chain models. <em>CSTAT</em>, <em>40</em>(3), 1541-1581. (<a href='https://doi.org/10.1007/s00180-024-01541-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method for change-point estimation, focusing on detecting structural shifts within time series data. Traditional maximum likelihood estimation (MLE) methods assume either independence or linear dependence via auto-regressive models. To address this limitation, the paper introduces copula-based Markov chain models, offering more flexible dependence modeling. These models treat a Gaussian time series as a Markov chain and utilize copula functions to handle serial dependence. The profile MLE procedure is then employed to estimate the change-point and other model parameters, with the Newton–Raphson algorithm facilitating numerical calculations for the estimators. The proposed approach is evaluated through simulations and real stock return data, considering two distinct periods: the 2008 financial crisis and the COVID-19 pandemic in 2020.},
  archive      = {J_CSTAT},
  author       = {Sun, Li-Hsien and Wang, Yu-Kai and Liu, Lien-Hsi and Emura, Takeshi and Chiu, Chi-Yang},
  doi          = {10.1007/s00180-024-01541-x},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1541-1581},
  shortjournal = {Comput. Stat.},
  title        = {Change point estimation for gaussian time series data with copula-based markov chain models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). INet for network integration. <em>CSTAT</em>, <em>40</em>(3), 1517-1539. (<a href='https://doi.org/10.1007/s00180-024-01536-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When collecting several data sets and heterogeneous data types on a given phenomenon of interest, the individual analysis of each data set will provide only a particular view of such phenomenon. Instead, integrating all the data may widen and deepen the results, offering a better view of the entire system. In the context of network integration, we propose the INet algorithm. INet assumes a similar network structure, representing latent variables in different network layers of the same system. Therefore, by combining individual edge weights and topological network structures, INet first constructs a Consensus Network that represents the shared information underneath the different layers to provide a global view of the entities that play a fundamental role in the phenomenon of interest. Then, it derives a Case Specific Network for each layer containing peculiar information of the single data type not present in all the others. We demonstrated good performance with our method through simulated data and detected new insights by analyzing biological and sociological datasets.},
  archive      = {J_CSTAT},
  author       = {Policastro, Valeria and Magnani, Matteo and Angelini, Claudia and Carissimo, Annamaria},
  doi          = {10.1007/s00180-024-01536-8},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1517-1539},
  shortjournal = {Comput. Stat.},
  title        = {INet for network integration},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized linear model based on latent factors and supervised components. <em>CSTAT</em>, <em>40</em>(3), 1475-1516. (<a href='https://doi.org/10.1007/s00180-024-01544-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a context of component-based multivariate modeling we propose to model the residual dependence of the responses. Each response of a response vector is assumed to depend, through a Generalized Linear Model, on a set of explanatory variables. The vast majority of explanatory variables are partitioned into conceptually homogeneous variable groups, viewed as explanatory themes. Variables in themes are supposed many and some of them are highly correlated or even collinear. Thus, generalized linear regression demands dimension reduction and regularization with respect to each theme. Besides them, we consider a small set of “additional” covariates not conceptually linked to the themes, and demanding no regularization. Supervised Component Generalized Linear Regression proposed to both regularize and reduce the dimension of the explanatory space by searching each theme for an appropriate number of orthogonal components, which both contribute to predict the responses and capture relevant structural information in themes. In this paper, we introduce random latent variables (a.k.a. factors) so as to model the covariance matrix of the linear predictors of the responses conditional on the components. To estimate the model, we present an algorithm combining supervised component-based model estimation with factor model estimation. This methodology is tested on simulated data and then applied to an agricultural ecology dataset.},
  archive      = {J_CSTAT},
  author       = {Gibaud, Julien and Bry, Xavier and Trottier, Catherine},
  doi          = {10.1007/s00180-024-01544-8},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1475-1516},
  shortjournal = {Comput. Stat.},
  title        = {Generalized linear model based on latent factors and supervised components},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance of evaluation metrics for classification in imbalanced data. <em>CSTAT</em>, <em>40</em>(3), 1447-1473. (<a href='https://doi.org/10.1007/s00180-024-01539-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the effectiveness of various metrics for selecting the adequate model for binary classification when data is imbalanced. Through an extensive simulation study involving 12 commonly used metrics of classification, our findings indicate that the Matthews Correlation Coefficient, G-Mean, and Cohen’s kappa consistently yield favorable performance. Conversely, the area under the curve and Accuracy metrics demonstrate poor performance across all studied scenarios, while other seven metrics exhibit varying degrees of effectiveness in specific scenarios. Furthermore, we discuss a practical application in the financial area, which confirms the robust performance of these metrics in facilitating model selection among alternative link functions.},
  archive      = {J_CSTAT},
  author       = {de la Cruz Huayanay, Alex and Bazán, Jorge L. and Russo, Cibele M.},
  doi          = {10.1007/s00180-024-01539-5},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1447-1473},
  shortjournal = {Comput. Stat.},
  title        = {Performance of evaluation metrics for classification in imbalanced data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A theory of contrasts for modified Freeman–Tukey statistics and its applications to tukey’s post-hoc tests for contingency tables. <em>CSTAT</em>, <em>40</em>(3), 1423-1446. (<a href='https://doi.org/10.1007/s00180-024-01537-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a theory of contrasts designed for modified Freeman–Tukey (FT) statistics which are derived through square-root transformations of observed frequencies (proportions) in contingency tables. Some modifications of the original FT statistic are necessary to allow for ANOVA-like exact decompositions of the global goodness of fit (GOF) measures. The square-root transformations have an important effect of stabilizing (equalizing) variances. The theory is then used to derive Tukey’s post-hoc pairwise comparison tests for contingency tables. Tukey’s tests are more restrictive, but are more powerful, than Scheffè’s post-hoc tests developed earlier for the analysis of contingency tables. Throughout this paper, numerical examples are given to illustrate the theory. Modified FT statistics, like other similar statistics for contingency tables, are based on a large-sample rationale. Small Monte-Carlo studies are conducted to investigate asymptotic (and non-asymptotic) behaviors of the proposed statistics.},
  archive      = {J_CSTAT},
  author       = {Takane, Yoshio and Beh, Eric J. and Lombardo, Rosaria},
  doi          = {10.1007/s00180-024-01537-7},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1423-1446},
  shortjournal = {Comput. Stat.},
  title        = {A theory of contrasts for modified Freeman–Tukey statistics and its applications to tukey’s post-hoc tests for contingency tables},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel nonconvex, smooth-at-origin penalty for statistical learning. <em>CSTAT</em>, <em>40</em>(3), 1397-1422. (<a href='https://doi.org/10.1007/s00180-024-01525-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonconvex penalties are utilized for regularization in high-dimensional statistical learning algorithms primarily because they yield unbiased or nearly unbiased estimators for the parameters in the model. Nonconvex penalties existing in the literature such as SCAD, MCP, Laplace and arctan have a singularity at origin which makes them useful also for variable selection. However, in several high-dimensional frameworks such as deep learning, variable selection is less of a concern. In this paper, we present a nonconvex penalty which is smooth at origin. The paper includes asymptotic results for ordinary least squares estimators regularized with the new penalty function, showing asymptotic bias that vanishes exponentially fast. We also conducted simulations to better understand the finite sample properties and conducted an empirical study employing deep neural network architecture on three datasets and convolutional neural network on four datasets. The empirical study based on artificial neural networks showed better performance for the new regularization approach in five out of the seven datasets.},
  archive      = {J_CSTAT},
  author       = {John, Majnu and Vettam, Sujit and Wu, Yihren},
  doi          = {10.1007/s00180-024-01525-x},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1397-1422},
  shortjournal = {Comput. Stat.},
  title        = {A novel nonconvex, smooth-at-origin penalty for statistical learning},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantinar: A blockchain peer-to-peer ecosystem for modern data analytics. <em>CSTAT</em>, <em>40</em>(3), 1361-1396. (<a href='https://doi.org/10.1007/s00180-024-01529-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power of data and correct statistical analysis has never been more prevalent. Academics and practitioners require nowadays an accurate application of quantitative methods. Yet many branches are subject to a crisis of integrity, which is shown in an improper use of statistical models, p-hacking, HARKing, or failure to replicate results. We propose the use of a Peer-to-Peer (P2P) ecosystem based on a blockchain network, Quantinar , to support quantitative analytics knowledge paired with code in the form of Quantlets or software snippets. The integration of blockchain technology allows Quantinar to ensure fully transparent and reproducible scientific research.},
  archive      = {J_CSTAT},
  author       = {Bag, Raul and Spilak, Bruno and Winkel, Julian and Härdle, Wolfgang Karl},
  doi          = {10.1007/s00180-024-01529-7},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1361-1396},
  shortjournal = {Comput. Stat.},
  title        = {Quantinar: A blockchain peer-to-peer ecosystem for modern data analytics},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust confidence intervals for meta-regression with interaction effects. <em>CSTAT</em>, <em>40</em>(3), 1337-1360. (<a href='https://doi.org/10.1007/s00180-024-01530-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis is an important statistical technique for synthesizing the results of multiple studies regarding the same or closely related research question. So-called meta-regression extends meta-analysis models by accounting for study-level covariates. Mixed-effects meta-regression models provide a powerful tool for evidence synthesis, by appropriately accounting for between-study heterogeneity. In fact, modelling the study effect in terms of random effects and moderators not only allows to examine the impact of the moderators, but often leads to more accurate estimates of the involved parameters. Nevertheless, due to the often small number of studies on a specific research topic, interactions are often neglected in meta-regression. In this work we consider the research questions (i) how moderator interactions influence inference in mixed-effects meta-regression models and (ii) whether some inference methods are more reliable than others. Here we review robust methods for confidence intervals in meta-regression models including interaction effects. These methods are based on the application of robust sandwich estimators of Hartung-Knapp-Sidik-Jonkman (HKSJ) or heteroscedasticity-consistent (HC)-type for estimating the variance-covariance matrix of the vector of model coefficients. Furthermore, we compare different versions of these robust estimators in an extensive simulation study. We thereby investigate coverage and width of seven different confidence intervals under varying conditions. Our simulation study shows that the coverage rates as well as the interval widths of the parameter estimates are only slightly affected by adjustment of the parameters. It also turned out that using the Satterthwaite approximation for the degrees of freedom seems to be advantageous for accurate coverage rates. In addition, different to previous analyses for simpler models, the $$\textbf{HKSJ}$$ -estimator shows a worse performance in this more complex setting compared to some of the $$\textbf{HC}$$ -estimators.},
  archive      = {J_CSTAT},
  author       = {Thurow, Maria and Welz, Thilo and Knop, Eric and Friede, Tim and Pauly, Markus},
  doi          = {10.1007/s00180-024-01530-0},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1337-1360},
  shortjournal = {Comput. Stat.},
  title        = {Robust confidence intervals for meta-regression with interaction effects},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ordinal causal discovery based on markov blankets. <em>CSTAT</em>, <em>40</em>(3), 1311-1335. (<a href='https://doi.org/10.1007/s00180-024-01513-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on learning causal network structures from ordinal categorical data. By combining constraint-based with score-and-search methodologies in structural learning, we propose a hybrid method called Markov Blanket Based Ordinal Causal Discovery (MBOCD) algorithm, which can capture the ordinal relationship of values in ordinal categorical variables. Theoretically, it is proved that for ordinal causal networks, two adjacent DAGs belonging to the same Markov equivalence class are identifiable, which results in the generation of a causal graph. Simulation experiments demonstrate that the proposed algorithm outperforms existing methods in terms of computational efficiency and accuracy. The code of this work is open at: https://github.com/leoydu/MBOCDcode.git .},
  archive      = {J_CSTAT},
  author       = {Du, Yu and Sun, Yi and Tan, Luyao},
  doi          = {10.1007/s00180-024-01513-1},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1311-1335},
  shortjournal = {Comput. Stat.},
  title        = {Ordinal causal discovery based on markov blankets},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric regression analysis of panel binary data with an informative observation process. <em>CSTAT</em>, <em>40</em>(3), 1285-1309. (<a href='https://doi.org/10.1007/s00180-024-01528-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panel binary data arise in an event history study when study subjects are observed only at discrete time points instead of continuously and the only available information on the occurrence of the recurrent event of interest is whether the event has occurred over two consecutive observation times or each observation window. Although some methods have been proposed for regression analysis of such data, all of them assume independent observation times or processes, which may not be true sometimes. To address this, we propose a joint modeling procedure that allows for informative observation processes. For the implementation of the proposed method, a computationally efficient EM algorithm is developed and the resulting estimators are consistent and asymptotically normal. The simulation study conducted to assess its performance indicates that it works well in practical situations, and the proposed approach is applied to the motivating data set from the Health and Retirement Study.},
  archive      = {J_CSTAT},
  author       = {Ge, Lei and Li, Yang and Sun, Jianguo},
  doi          = {10.1007/s00180-024-01528-8},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1285-1309},
  shortjournal = {Comput. Stat.},
  title        = {Semiparametric regression analysis of panel binary data with an informative observation process},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A Metropolis–Hastings Robbins–Monro algorithm via variational inference for estimating the multidimensional graded response model: A calculationally efficient estimation scheme to deal with complex test structures. <em>CSTAT</em>, <em>40</em>(3), 1253-1284. (<a href='https://doi.org/10.1007/s00180-024-01533-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the Metropolis–Hastings variational inference Robbins–Monro (MHVIRM) algorithm, a modification of the Metropolis–Hastings Robbins–Monro (MHRM) method, designed for estimating parameters in complex multidimensional graded response models (MGRM). By integrating a black-box variational inference (BBVI) approach, MHVIRM enhances computational efficiency and estimation accuracy, particularly for models with high-dimensional data and complex test structures. The algorithms effectiveness is demonstrated through simulations, showing improved precision over traditional MHRM, especially in scenarios with complex structures and small sample sizes. Moreover, MHVIRM is robust to initial values. The applicability is further illustrated with a real dataset analysis.},
  archive      = {J_CSTAT},
  author       = {Wang, Xue and Lu, Jing and Zhang, Jiwei},
  doi          = {10.1007/s00180-024-01533-x},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1253-1284},
  shortjournal = {Comput. Stat.},
  title        = {A Metropolis–Hastings Robbins–Monro algorithm via variational inference for estimating the multidimensional graded response model: A calculationally efficient estimation scheme to deal with complex test structures},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Profile transformations for reciprocal averaging and singular value decomposition. <em>CSTAT</em>, <em>40</em>(3), 1217-1251. (<a href='https://doi.org/10.1007/s00180-024-01517-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power transformations of count data, including cell frequencies of a contingency table, have been well understood for nearly 100 years, with much of the attention focused on the square root transformation. Over the past 15 years, this topic has been the focus of some new insights into areas of correspondence analysis where two forms of power transformation have been discussed. One type considers the impact of raising the joint proportions of the cell frequencies of a table to a known power while the other examines the power transformation of the relative distribution of the cell frequencies. While the foundations of the graphical features of correspondence analysis rest with the numerical algorithms like reciprocal averaging, and other analogous techniques, discussions of the role of power transformations in reciprocal averaging have not been described. Therefore, this paper examines this link where a power transformation is applied to the cell frequencies of a two-way contingency table. In doing so, we show that reciprocal averaging can be performed under such a transformation to obtain row and column scores that provide the maximum association between the variables and the greatest discrimination between the categories. Finally, we discuss the connection between performing reciprocal averaging and singular value decomposition under this type of power transformation. The R function, powerRA.exe is included in the Appendix and performs reciprocal averaging of a power transformation of the cell frequencies of a two-way contingency table.},
  archive      = {J_CSTAT},
  author       = {Wang, Ting-Wu and Beh, Eric J. and Lombardo, Rosaria and Renner, Ian W.},
  doi          = {10.1007/s00180-024-01517-x},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1217-1251},
  shortjournal = {Comput. Stat.},
  title        = {Profile transformations for reciprocal averaging and singular value decomposition},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Positive time series regression models: Theoretical and computational aspects. <em>CSTAT</em>, <em>40</em>(3), 1185-1215. (<a href='https://doi.org/10.1007/s00180-024-01531-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses dynamic ARMA-type regression models for positive time series, which can handle bounded non-Gaussian time series without requiring data transformations. Our proposed model includes a conditional mean modeled by a dynamic structure containing autoregressive and moving average terms, time-varying covariates, unknown parameters, and link functions. Additionally, we present the PTSR package and discuss partial maximum likelihood estimation, asymptotic theory, hypothesis testing inference, diagnostic analysis, and forecasting for a variety of regression-based dynamic models for positive time series. A Monte Carlo simulation and a real data application are provided.},
  archive      = {J_CSTAT},
  author       = {Prass, Taiane Schaedler and Pumi, Guilherme and Taufemback, Cleiton Guollo and Carlos, Jonas Hendler},
  doi          = {10.1007/s00180-024-01531-z},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1185-1215},
  shortjournal = {Comput. Stat.},
  title        = {Positive time series regression models: Theoretical and computational aspects},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The root-gaussian cox process for spatial-temporal disease mapping with aggregated data. <em>CSTAT</em>, <em>40</em>(3), 1171-1184. (<a href='https://doi.org/10.1007/s00180-024-01532-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of aggregated data influenced by time, space, and extra changes in geographic region borders was the main emphasis of the current paper. This may occur if the regions used to count the reported incidences of a health outcome over time change periodically. In order to handle the spatial-temporal scenario, we enhance the spatial root-Gaussian Cox Process (RGCP), which makes use of the square-root link function rather than the more typical log-link function. The algorithm’s ability to estimate a risk surface has been proven by a simulation study, and it has also been validated by real datasets.},
  archive      = {J_CSTAT},
  author       = {Asfaw, Zeytu Gashaw and Brown, Patrick E. and Stafford, Jamie},
  doi          = {10.1007/s00180-024-01532-y},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1171-1184},
  shortjournal = {Comput. Stat.},
  title        = {The root-gaussian cox process for spatial-temporal disease mapping with aggregated data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A class of transformed joint quantile time series models with applications to health studies. <em>CSTAT</em>, <em>40</em>(3), 1147-1170. (<a href='https://doi.org/10.1007/s00180-024-01484-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensions of quantile regression modeling for time series analysis are extensively employed in medical and health studies. This study introduces a specific class of transformed quantile-dispersion regression models for non-stationary time series. These models possess the flexibility to incorporate the time-varying structure into the model specification, enabling precise predictions for future decisions. Our proposed modeling methodology applies to dynamic processes characterized by high variation and possible periodicity, relying on a non-linear framework. Additionally, unlike the transformed time series model, our approach directly interprets the regression parameters concerning the initial response. For computational purposes, we present an iteratively reweighted least squares algorithm. To assess the performance of our model, we conduct simulation experiments. To illustrate the modeling strategy, we analyze time-series measurements of influenza infection and daily COVID-19 deaths.},
  archive      = {J_CSTAT},
  author       = {Tourani-Farani, Fahimeh and Aghabazaz, Zeynab and Kazemi, Iraj},
  doi          = {10.1007/s00180-024-01484-3},
  journal      = {Computational Statistics},
  month        = {3},
  number       = {3},
  pages        = {1147-1170},
  shortjournal = {Comput. Stat.},
  title        = {A class of transformed joint quantile time series models with applications to health studies},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FPDclustering: A comprehensive r package for probabilistic distance clustering based methods. <em>CSTAT</em>, <em>40</em>(2), 1123-1146. (<a href='https://doi.org/10.1007/s00180-024-01490-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data clustering has a long history and refers to a vast range of models and methods that exploit the ever-more-performing numerical optimization algorithms and are designed to find homogeneous groups of observations in data. In this framework, the probability distance clustering (PDC) family methods offer a numerically effective alternative to model-based clustering methods and a more flexible opportunity in the framework of geometric data clustering. Given n J-dimensional data vectors arranged in a data matrix and the number K of clusters, PDC maximizes the joint density function that is defined as the sum of the products between the distance and the probability, both of which are measured for each data vector from each center. This article shows the capabilities of the PDC family, illustrating the R package FPDclustering.},
  archive      = {J_CSTAT},
  author       = {Tortora, Cristina and Palumbo, Francesco},
  doi          = {10.1007/s00180-024-01490-5},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {1123-1146},
  shortjournal = {Comput. Stat.},
  title        = {FPDclustering: A comprehensive r package for probabilistic distance clustering based methods},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trend of high dimensional time series estimation using low-rank matrix factorization: Heuristics and numerical experiments via the TrendTM package. <em>CSTAT</em>, <em>40</em>(2), 1097-1122. (<a href='https://doi.org/10.1007/s00180-024-01519-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the practical issue of a recent theoretical method proposed for trend estimation in high dimensional time series. This method falls within the scope of the low-rank matrix factorization methods in which the temporal structure is taken into account. It consists of minimizing a penalized criterion, theoretically efficient but which depends on two constants to be chosen in practice. We propose a two-step strategy to solve this question based on two different known heuristics. The performance and a comparison of the strategies are studied through an important simulation study in various scenarios. In order to make the estimation method with the best strategy available to the community, we implemented the method in an R package TrendTM which is presented and used here. Finally, we give a geometric interpretation of the results by linking it to PCA and use the results to solve a high-dimensional curve clustering problem. The package is available on CRAN.},
  archive      = {J_CSTAT},
  author       = {Lebarbier, Emilie and Marie, Nicolas and Rosier, Amélie},
  doi          = {10.1007/s00180-024-01519-9},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {1097-1122},
  shortjournal = {Comput. Stat.},
  title        = {Trend of high dimensional time series estimation using low-rank matrix factorization: Heuristics and numerical experiments via the TrendTM package},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring local explanations of nonlinear models using animated linear projections. <em>CSTAT</em>, <em>40</em>(2), 1071-1095. (<a href='https://doi.org/10.1007/s00180-023-01453-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increased predictive power of machine learning models comes at the cost of increased complexity and loss of interpretability, particularly in comparison to parametric statistical models. This trade-off has led to the emergence of eXplainable AI (XAI) which provides methods, such as local explanations (LEs) and local variable attributions (LVAs), to shed light on how a model use predictors to arrive at a prediction. These provide a point estimate of the linear variable importance in the vicinity of a single observation. However, LVAs tend not to effectively handle association between predictors. To understand how the interaction between predictors affects the variable importance estimate, we can convert LVAs into linear projections and use the radial tour. This is also useful for learning how a model has made a mistake, or the effect of outliers, or the clustering of observations. The approach is illustrated with examples from categorical (penguin species, chocolate types) and quantitative (soccer/football salaries, house prices) response models. The methods are implemented in the R package cheem, available on CRAN.},
  archive      = {J_CSTAT},
  author       = {Spyrison, Nicholas and Cook, Dianne and Biecek, Przemyslaw},
  doi          = {10.1007/s00180-023-01453-2},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {1071-1095},
  shortjournal = {Comput. Stat.},
  title        = {Exploring local explanations of nonlinear models using animated linear projections},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Site-specific nitrogen recommendation: Fast, accurate, and feasible bayesian kriging. <em>CSTAT</em>, <em>40</em>(2), 1053-1069. (<a href='https://doi.org/10.1007/s00180-024-01527-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Kriging (BK) provides a way to estimate regression models where the parameters are smoothed across space. Such estimates could help guide site-specific fertilizer recommendations. One advantage of BK is that it can readily fill in the missing values that are common in yield monitor data. The problem is that previous methods are too computationally intensive to be commercially feasible when estimating a nonlinear production function. This paper sought to increase computational speed by imposing restrictions on the spatial covariance matrix. Previous research used an exponential function for the spatial covariance matrix. The two alternatives considered are the conditional autoregressive and simultaneous autoregressive models. In addition, a new analytical solution is provided for finding the optimal value of nitrogen with a stochastic linear plateau model. A comparison among models in the accuracy and computational burden shows that the restrictions significantly reduced the computational burden, although they did sacrifice some accuracy in the dataset considered.},
  archive      = {J_CSTAT},
  author       = {Poursina, Davood and Brorsen, B. Wade},
  doi          = {10.1007/s00180-024-01527-9},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {1053-1069},
  shortjournal = {Comput. Stat.},
  title        = {Site-specific nitrogen recommendation: Fast, accurate, and feasible bayesian kriging},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian diagnostics in a partially linear model with first-order autoregressive skew-normal errors. <em>CSTAT</em>, <em>40</em>(2), 1021-1051. (<a href='https://doi.org/10.1007/s00180-024-01504-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a Bayesian local influence method to detect influential observations in a partially linear model with first-order autoregressive skew-normal errors. This method appears suitable for small or moderate-sized data sets ( $$n=200{\sim }400$$ ) and overcomes some theoretical limitations, bridging the diagnostic gap for small or moderate-sized data in classical methods. The MCMC algorithm is employed for parameter estimation, and Bayesian local influence analysis is made using three perturbation schemes (priors, variances, and data) and three measurement scales (Bayes factor, $$\phi $$ -divergence, and posterior mean). Simulation studies are conducted to validate the reliability of the diagnostics. Finally, a practical application uses data on the 1976 Los Angeles ozone concentration to further demonstrate the effectiveness of the diagnostics.},
  archive      = {J_CSTAT},
  author       = {Liu, Yonghui and Lu, Jiawei and Paula, Gilberto A. and Liu, Shuangzhe},
  doi          = {10.1007/s00180-024-01504-2},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {1021-1051},
  shortjournal = {Comput. Stat.},
  title        = {Bayesian diagnostics in a partially linear model with first-order autoregressive skew-normal errors},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical likelihood change point detection in quantile regression models. <em>CSTAT</em>, <em>40</em>(2), 999-1020. (<a href='https://doi.org/10.1007/s00180-024-01526-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression is an extension of linear regression which estimates a conditional quantile of interest. In this paper, we propose an empirical likelihood-based non-parametric procedure to detect structural changes in the quantile regression models. Further, we have modified the proposed smoothed empirical likelihood-based method using adjusted smoothed empirical likelihood and transformed smoothed empirical likelihood techniques. We have shown that under the null hypothesis, the limiting distribution of the smoothed empirical likelihood ratio test statistic is identical to that of the classical parametric likelihood. Simulations are conducted to investigate the finite sample properties of the proposed methods. Finally, to demonstrate the effectiveness of the proposed method, it is applied to urinary Glycosaminoglycans (GAGs) data to detect structural changes.},
  archive      = {J_CSTAT},
  author       = {Ratnasingam, Suthakaran and Gamage, Ramadha D. Piyadi},
  doi          = {10.1007/s00180-024-01526-w},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {999-1020},
  shortjournal = {Comput. Stat.},
  title        = {Empirical likelihood change point detection in quantile regression models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust variable selection for additive coefficient models. <em>CSTAT</em>, <em>40</em>(2), 977-997. (<a href='https://doi.org/10.1007/s00180-024-01524-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive coefficient models generalize linear regression models by assuming that the relationship between the response and some covariates is linear, while their regression coefficients are additive functions. Because of its advantages in dealing with the “curse of dimensionality”, additive coefficient models gain a lot of attention. The commonly used estimation methods for additive coefficient models are not robust against high leverage points. To circumvent this difficulty, we develop a robust variable selection procedure based on the exponential squared loss function and group penalty for the additive coefficient models, which can tackle outliers in the response and covariates simultaneously. Under some regularity conditions, we show that the oracle estimator is a local solution of the proposed method. Furthermore, we apply the local linear approximation and minorization-maximization algorithm for the implementation of the proposed estimator. Meanwhile, we propose a data-driven procedure to select the tuning parameters. Simulation studies and an application to a plasma beta-carotene level data set illustrate that the proposed method can offer more reliable results than other existing methods in contamination schemes.},
  archive      = {J_CSTAT},
  author       = {Zou, Hang and Huang, Xiaowen and Jiang, Yunlu},
  doi          = {10.1007/s00180-024-01524-y},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {977-997},
  shortjournal = {Comput. Stat.},
  title        = {Robust variable selection for additive coefficient models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable selection and structure identification for additive models with longitudinal data. <em>CSTAT</em>, <em>40</em>(2), 951-975. (<a href='https://doi.org/10.1007/s00180-024-01521-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a polynomial structure identification (PSI) method for variable selection and model structure identification of additive models with longitudinal data. First, the backfitting algorithm and zero-order local polynomial smoothing method are used to select important variables in the additive model, and the importance of variables is determined through the inverse of the bandwidth parameter in the nonparametric partial kernel function. Second, the backfitting algorithm and Q-order local polynomial smoothing method are utilized to identify the specific structure of each selected predictor. To incorporate correlations within longitudinal data, a two-stage estimation method is proposed for estimating the regression parameters of the identified important variables: (i) Parameter estimators of the important variables are firstly obtained under an independence working model assumption; (ii) Generalized estimating equations with a working correlation matrix based on B-splines are constructed to obtain the final estimators of the parameters, which improve the efficiency of parameter estimation. Finally, simulation studies are carried out to evaluate the performance of the proposed method, followed by the presentation of two real-world examples for illustration.},
  archive      = {J_CSTAT},
  author       = {Wang, Ting and Fu, Liya and Song, Yanan},
  doi          = {10.1007/s00180-024-01521-1},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {951-975},
  shortjournal = {Comput. Stat.},
  title        = {Variable selection and structure identification for additive models with longitudinal data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple imputation with competing risk outcomes. <em>CSTAT</em>, <em>40</em>(2), 929-949. (<a href='https://doi.org/10.1007/s00180-024-01518-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In time-to-event analyses, a competing risk is an event whose occurrence precludes the occurrence of the event of interest. Settings with competing risks occur frequently in clinical research. Missing data, which is a common problem in research, occurs when the value of a variable is recorded for some, but not all, records in the dataset. Multiple Imputation (MI) is a popular method to address the presence of missing data. MI uses an imputation model to generate M (M > 1) values for each variable that is missing, resulting in the creation of M complete datasets. A popular algorithm for imputing missing data is multivariate imputation using chained equations (MICE). We used a complex simulation design with covariates and missing data patterns reflective of patients hospitalized with acute myocardial infarction (AMI) to compare three strategies for imputing missing predictor variables when the analysis model is a cause-specific hazard when there were three different event types. We compared two MICE-based strategies that differed according to which cause-specific cumulative hazard functions were included in the imputation models (the three cause-specific cumulative hazard functions vs. only the cause-specific cumulative hazard function for the primary outcome) with the use of the substantive model compatible fully conditional specification (SMCFCS) algorithm. While no strategy had consistently superior performance compared to the other strategies, SMCFCS may be the preferred strategy. We illustrated the application of the strategies using a case study of patients hospitalized with AMI.},
  archive      = {J_CSTAT},
  author       = {Austin, Peter C.},
  doi          = {10.1007/s00180-024-01518-w},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {929-949},
  shortjournal = {Comput. Stat.},
  title        = {Multiple imputation with competing risk outcomes},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypothesis testing in cox models when continuous covariates are dichotomized: Bias analysis and bootstrap-based test. <em>CSTAT</em>, <em>40</em>(2), 907-927. (<a href='https://doi.org/10.1007/s00180-024-01520-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypothesis testing for the regression coefficient associated with a dichotomized continuous covariate in a Cox proportional hazards model has been considered in clinical research. Although most existing testing methods do not allow covariates, except for a dichotomized continuous covariate, they have generally been applied. Through an analytic bias analysis and a numerical study, we show that the current practice is not free from an inflated type I error and a loss of power. To overcome this limitation, we develop a bootstrap-based test that allows additional covariates and dichotomizes two-dimensional covariates into a binary variable. In addition, we develop an efficient algorithm to speed up the calculation of the proposed test statistic. Our numerical study demonstrates that the proposed bootstrap-based test maintains the type I error well at the nominal level and exhibits higher power than other methods, as well as that the proposed efficient algorithm reduces computational costs.},
  archive      = {J_CSTAT},
  author       = {Sim, Hyunman and Lee, Sungjeong and Kim, Bo-Hyung and Shin, Eun and Lee, Woojoo},
  doi          = {10.1007/s00180-024-01520-2},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {907-927},
  shortjournal = {Comput. Stat.},
  title        = {Hypothesis testing in cox models when continuous covariates are dichotomized: Bias analysis and bootstrap-based test},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Some aspects of nonlinear dimensionality reduction. <em>CSTAT</em>, <em>40</em>(2), 883-906. (<a href='https://doi.org/10.1007/s00180-024-01514-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we discuss nonlinear dimensionality reduction within the framework of principal curves. We formulate dimensionality reduction as problems of estimating principal subspaces for both noiseless and noisy cases, and propose the corresponding iterative algorithms that modify existing principal curve algorithms. An R squared criterion is introduced to estimate the dimension of the principal subspace. In addition, we present new regression and density estimation strategies based on our dimensionality reduction algorithms. Theoretical analyses and numerical experiments show the effectiveness of the proposed methods.},
  archive      = {J_CSTAT},
  author       = {Wang, Liwen and Wang, Yongda and Xiong, Shifeng and Yang, Jiankui},
  doi          = {10.1007/s00180-024-01514-0},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {883-906},
  shortjournal = {Comput. Stat.},
  title        = {Some aspects of nonlinear dimensionality reduction},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A time-varying coefficient rate model with intermittently observed covariates for recurrent event data. <em>CSTAT</em>, <em>40</em>(2), 863-882. (<a href='https://doi.org/10.1007/s00180-024-01515-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis of recurrent event data, some covariates have time-varying effect such as efficacy of certain treatments, while others are internally time-dependent, like blood pressure. Considering the variability of the covariate effects and covariate observation over time, a time-varying coefficient rate model with intermittently observed covariates was proposed. Generally, time-dependent covariates cannot be continuously observed. They are only recorded intermittently. The unobserved time-dependent covariates need to be imputed. Estimators were obtained using kernel likelihood for the time-varying coefficient and kernel smoothing for the time-dependent covariate. The proposed estimator was proved to be asymptotically unbiased and normally distributed. Some simulations were conducted to evaluate the estimation method, and the proposed method was applied to analyze a real data.},
  archive      = {J_CSTAT},
  author       = {Kang, Fangyuan and Zhao, Jianxi},
  doi          = {10.1007/s00180-024-01515-z},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {863-882},
  shortjournal = {Comput. Stat.},
  title        = {A time-varying coefficient rate model with intermittently observed covariates for recurrent event data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monitoring mean of INAR(1) process with discrete mixture exponential innovations. <em>CSTAT</em>, <em>40</em>(2), 821-862. (<a href='https://doi.org/10.1007/s00180-024-01511-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a discrete counterpart of the mixture exponential distribution, namely discrete mixture exponential distribution, by utilizing the survival discretization method. The moment generating function and associated moment measures are discussed. The distribution’s hazard rate function can assume increasing or decreasing forms, making it adaptable for diverse fields requiring count data modeling. The paper delves into two parameter estimation methods and evaluates their performance through a Monte Carlo simulation study. The applicability of this distribution extends to time series analysis, particularly within the framework of the first-order integer-valued autoregressive process. Consequently, an INAR(1) process with discrete mixture exponential innovations is proposed, outlining its fundamental properties, and the performance of conditional maximum likelihood and conditional least squares estimation methods is evaluated through a simulation study. Real data analysis showcases the proposed model’s superior performance compared to alternative models. Additionally, the paper explores quality control applications, addressing serial dependence challenges in count data encountered in production and market management. As a result, the INAR(1)DME process is employed to explore control charts for monitoring autocorrelated count data. The performance of two distinct control charts, the cumulative sum chart and the exponentially weighted moving average chart, are evaluated for their effectiveness in detecting shifts in the process mean across various designs. A bivariate Markov chain approach is used to estimate the average run length and their deviations for these charts, providing valuable insights for practical implementation. The nature of design parameters to improve the robustness of process monitoring under the considered charts is examined through a simulation study. The practical superiority of the proposed charts is demonstrated through effective modeling with real data, surpassing competing models.},
  archive      = {J_CSTAT},
  author       = {Irshad, M. R. and Ahammed, Muhammed and Maya, R.},
  doi          = {10.1007/s00180-024-01511-3},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {821-862},
  shortjournal = {Comput. Stat.},
  title        = {Monitoring mean of INAR(1) process with discrete mixture exponential innovations},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging single-case results to bayesian hierarchical modelling. <em>CSTAT</em>, <em>40</em>(2), 795-819. (<a href='https://doi.org/10.1007/s00180-024-01516-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In scientific research, we often aim to learn one or more parameters of instances(objects) from a population—such as the batting averages of a group of baseball players and characteristics of white dwarfs from the Galactic Halo-and the distribution of fitted parameters across the population. Bayesian hierarchical models are well suited to this kind of situation. Despite there are many general-purpose and specialized Bayesian inference packages, many of them are designed for the single-case analysis, i.e., fitting a single unit of data at a time, rather than simultaneously fitting the hierarchical model for multiple datasets. This is especially true when the likelihood function is complicated and has no analytical form. In this paper, we fill this gap by proposing general algorithms to efficiently compute the exact hierarchical models by utilizing available packages that can perform Bayesian inference for single-case analysis. Our algorithms are efficient and easy-to-implement, thus significantly saving time and effort. We illustrate the application of our methods on three datasets, to verify the effectiveness, efficiency and benefits of our methods.},
  archive      = {J_CSTAT},
  author       = {Si, Shijing and Gu, Jia-wen and Tian, Maozai},
  doi          = {10.1007/s00180-024-01516-y},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {795-819},
  shortjournal = {Comput. Stat.},
  title        = {Leveraging single-case results to bayesian hierarchical modelling},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High dimensional T-type estimator for robust covariance matrix estimation with applications to elliptical factor models. <em>CSTAT</em>, <em>40</em>(2), 767-794. (<a href='https://doi.org/10.1007/s00180-024-01505-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a regularized t-type estimator is proposed for high-dimensional scatter matrix estimation, where the number of dimensions p is comparable to, or even larger than the sample size n, and its thresholding form is employed to deal with sparse settings. Then, regularized t-type estimator is extended to high-dimensional elliptic factor models with outliers for robust identification of common factor numbers. Finally, we illustrate that the proposed regularized t-type estimator significantly outperforms the competitors through extensive simulations, even in cases with high-dimensional data. Meanwhile, the t-type estimator can significantly improve the efficiency of Tyler’s M-estimator in Goes et al. (Ann Stat 48(1):86–110, 2020) when the samples follow a possibly heavy-tailed elliptical distribution with a non-central or unknown location parameter.},
  archive      = {J_CSTAT},
  author       = {Wang, Guanpeng and Cui, Hengjian},
  doi          = {10.1007/s00180-024-01505-1},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {767-794},
  shortjournal = {Comput. Stat.},
  title        = {High dimensional T-type estimator for robust covariance matrix estimation with applications to elliptical factor models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double truncation method for controlling local false discovery rate in case of spiky null. <em>CSTAT</em>, <em>40</em>(2), 745-766. (<a href='https://doi.org/10.1007/s00180-024-01510-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many multiple test procedures, which control the false discovery rate, have been developed to identify some cases (e.g. genes) showing statistically significant difference between two different groups. However, a common issue encountered in some practical data sets is the presence of highly spiky null distributions. Existing methods struggle to control type I error in such cases due to the “inflated false positives," but this problem has not been addressed in previous literature. Our team recently encountered this issue while analyzing SET4 gene deletion data and proposed modeling the null distribution using a scale mixture normal distribution. However, the use of this approach is limited due to strong assumptions on the spiky peak. In this paper, we present a novel multiple test procedure that can be applied to any type of spiky peak data, including situations with no spiky peak or with one or two spiky peaks. Our approach involves truncating the central statistics around 0, which primarily contribute to the null spike, as well as the two tails that may be contaminated by alternative distributions. We refer to this method as the “double truncation method." After applying double truncation, we estimate the null density using the doubly truncated maximum likelihood estimator. We demonstrate numerically that our proposed method effectively controls the false discovery rate at the desired level using simulated data. Furthermore, we apply our method to two real data sets, namely the SET protein data and peony data.},
  archive      = {J_CSTAT},
  author       = {Kim, Shinjune and Oh, Youngjae and Lim, Johan and Park, DoHwan and Green, Erin M. and Ramos, Mark L. and Jeong, Jaesik},
  doi          = {10.1007/s00180-024-01510-4},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {745-766},
  shortjournal = {Comput. Stat.},
  title        = {Double truncation method for controlling local false discovery rate in case of spiky null},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic properties of kernel density and hazard rate function estimators with censored widely orthant dependent data. <em>CSTAT</em>, <em>40</em>(2), 723-743. (<a href='https://doi.org/10.1007/s00180-024-01509-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel estimators of density function and hazard rate function are very important in nonparametric statistics. The paper aims to investigate the uniformly strong representations and the rates of uniformly strong consistency for kernel smoothing density and hazard rate function estimation with censored widely orthant dependent data based on the Kaplan–Meier estimator. Under some mild conditions, the rates of the remainder term and strong consistency are shown to be $$O\big (\sqrt{\log (ng(n))/\big (nb_{n}^{2}\big )}\big )~a.s.$$ and $$O\big (\sqrt{\log (ng(n))/\big (nb_{n}^{2}\big )}\big )+O\big (b_{n}^{2}\big )~a.s.$$ , respectively, where g(n) are the dominating coefficients of widely orthant dependent random variables. Some numerical simulations and a real data analysis are also presented to confirm the theoretical results based on finite sample performances.},
  archive      = {J_CSTAT},
  author       = {Wu, Yi and Wang, Wei and Yu, Wei and Wang, Xuejun},
  doi          = {10.1007/s00180-024-01509-x},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {723-743},
  shortjournal = {Comput. Stat.},
  title        = {Asymptotic properties of kernel density and hazard rate function estimators with censored widely orthant dependent data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Projection predictive variable selection for discrete response families with finite support. <em>CSTAT</em>, <em>40</em>(2), 701-721. (<a href='https://doi.org/10.1007/s00180-024-01506-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The projection predictive variable selection is a decision-theoretically justified Bayesian variable selection approach achieving an outstanding trade-off between predictive performance and sparsity. Its projection problem is not easy to solve in general because it is based on the Kullback–Leibler divergence from a restricted posterior predictive distribution of the so-called reference model to the parameter-conditional predictive distribution of a candidate model. Previous work showed how this projection problem can be solved for response families employed in generalized linear models and how an approximate latent-space approach can be used for many other response families. Here, we present an exact projection method for all response families with discrete and finite support, called the augmented-data projection. A simulation study for an ordinal response family shows that the proposed method performs better than or similarly to the previously proposed approximate latent-space projection. The cost of the slightly better performance of the augmented-data projection is a substantial increase in runtime. Thus, if the augmented-data projection’s runtime is too high, we recommend the latent projection in the early phase of the model-building workflow and the augmented-data projection for final results. The ordinal response family from our simulation study is supported by both projection methods, but we also include a real-world cancer subtyping example with a nominal response family, a case that is not supported by the latent projection.},
  archive      = {J_CSTAT},
  author       = {Weber, Frank and Glass, Änne and Vehtari, Aki},
  doi          = {10.1007/s00180-024-01506-0},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {701-721},
  shortjournal = {Comput. Stat.},
  title        = {Projection predictive variable selection for discrete response families with finite support},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expectile regression averaging method for probabilistic forecasting of electricity prices. <em>CSTAT</em>, <em>40</em>(2), 683-700. (<a href='https://doi.org/10.1007/s00180-024-01508-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new method for probabilistic forecasting of electricity prices. It is based on averaging point forecasts from different models combined with expectile regression. We show that deriving the predicted distribution in terms of expectiles, might be in some cases advantageous to the commonly used quantiles. We apply the proposed method to the day-ahead electricity prices from the German market and compare its accuracy with the Quantile Regression Averaging method and quantile- as well as expectile-based historical simulation. The obtained results indicate that using the expectile regression improves the accuracy of the probabilistic forecasts of electricity prices, but a variance stabilizing transformation should be applied prior to modelling.},
  archive      = {J_CSTAT},
  author       = {Janczura, Joanna},
  doi          = {10.1007/s00180-024-01508-y},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {683-700},
  shortjournal = {Comput. Stat.},
  title        = {Expectile regression averaging method for probabilistic forecasting of electricity prices},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric analysis of competing risks data with covariate measurement error. <em>CSTAT</em>, <em>40</em>(2), 651-682. (<a href='https://doi.org/10.1007/s00180-024-01502-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the competing risks data with covariate measurement error. A semiparametric linear transformation model for the right-censored competing risks data when the covariates are measured with error is proposed. The parameters involved in the model are estimated using a set of estimating equations. An adaptable simulation extrapolation (SIMEX) technique is employed to handle the covariate measurement error. Simulation studies are conducted, to examine the finite sample properties of the estimators. Also, we demonstrated the proposed method using a real dataset.},
  archive      = {J_CSTAT},
  author       = {Jayanagasri, Akurathi and Anjana, S.},
  doi          = {10.1007/s00180-024-01502-4},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {651-682},
  shortjournal = {Comput. Stat.},
  title        = {Semiparametric analysis of competing risks data with covariate measurement error},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in rényi entropy and divergence estimation for model assessment. <em>CSTAT</em>, <em>40</em>(2), 633-650. (<a href='https://doi.org/10.1007/s00180-024-01507-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entropy and divergence, fundamental concepts in machine learning and computer science, have gained significant traction over the past decade. Statisticians have been developing estimators for these measures, advancing computational analysis. In this paper, we present nonparametric estimators for Rényi entropy and divergence. Through a range of examples, we showcase the effectiveness of our approach, demonstrating its applicability across various contexts. Furthermore, we leverage these estimators for model assessment.},
  archive      = {J_CSTAT},
  author       = {Al-Labadi, Luai and Chu, Zhirui and Xu, Ying},
  doi          = {10.1007/s00180-024-01507-z},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {633-650},
  shortjournal = {Comput. Stat.},
  title        = {Advancements in rényi entropy and divergence estimation for model assessment},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient regression analyses with zero-augmented models based on ranking. <em>CSTAT</em>, <em>40</em>(2), 601-632. (<a href='https://doi.org/10.1007/s00180-024-01503-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several zero-augmented models exist for estimation involving outcomes with large numbers of zero. Two of such models for handling count endpoints are zero-inflated and hurdle regression models. In this article, we apply the extreme ranked set sampling (ERSS) scheme in estimation using zero-inflated and hurdle regression models. We provide theoretical derivations showing superiority of ERSS compared to simple random sampling (SRS) using these zero-augmented models. A simulation study is also conducted to compare the efficiency of ERSS to SRS and lastly, we illustrate applications with real data sets.},
  archive      = {J_CSTAT},
  author       = {Kanda, Deborah and Yin, Jingjing and Zhang, Xinyan and Samawi, Hani},
  doi          = {10.1007/s00180-024-01503-3},
  journal      = {Computational Statistics},
  month        = {2},
  number       = {2},
  pages        = {601-632},
  shortjournal = {Comput. Stat.},
  title        = {Efficient regression analyses with zero-augmented models based on ranking},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximizing adjusted covariance: New supervised dimension reduction for classification. <em>CSTAT</em>, <em>40</em>(1), 573-599. (<a href='https://doi.org/10.1007/s00180-024-01472-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a new linear dimension reduction technique called Maximizing Adjusted Covariance (MAC), which is suitable for supervised classification. The new approach is to adjust the covariance matrix between input and target variables using the within-class sum of squares, thereby promoting class separation after linear dimension reduction. MAC has a low computational cost and can complement existing linear dimensionality reduction techniques for classification. In this study, the classification performance by MAC was compared with those of the existing linear dimension reduction methods using 44 datasets. In most of the classification models used in the experiment, the MAC dimension reduction method showed better classification accuracy and F1 score than other linear dimension reduction methods.},
  archive      = {J_CSTAT},
  author       = {Park, Hyejoon and Kim, Hyunjoong and Lee, Yung-Seop},
  doi          = {10.1007/s00180-024-01472-7},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {573-599},
  shortjournal = {Comput. Stat.},
  title        = {Maximizing adjusted covariance: New supervised dimension reduction for classification},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact and approximate computation of the scatter halfspace depth. <em>CSTAT</em>, <em>40</em>(1), 547-572. (<a href='https://doi.org/10.1007/s00180-024-01500-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scatter halfspace depth (sHD) is an extension of the location halfspace (also called Tukey) depth that is applicable in the nonparametric analysis of scatter. Using sHD, it is possible to define minimax optimal robust scatter estimators for multivariate data. The problem of exact computation of sHD for data of dimension $$d \ge 2$$ has, however, not been addressed in the literature. We develop an exact algorithm for the computation of sHD in any dimension d and implement it efficiently for any dimension $$d \ge 1$$ . Since the exact computation of sHD is slow especially for higher dimensions, we also propose two fast approximate algorithms. All our programs are freely available in the R package scatterdepth.},
  archive      = {J_CSTAT},
  author       = {Liu, Xiaohui and Liu, Yuzi and Laketa, Petra and Nagy, Stanislav and Chen, Yuting},
  doi          = {10.1007/s00180-024-01500-6},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {547-572},
  shortjournal = {Comput. Stat.},
  title        = {Exact and approximate computation of the scatter halfspace depth},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian approach for clustering and exact finite-sample model selection in longitudinal data mixtures. <em>CSTAT</em>, <em>40</em>(1), 509-545. (<a href='https://doi.org/10.1007/s00180-024-01501-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider mixtures of longitudinal trajectories, where one trajectory contains measurements over time of the variable of interest for one individual and each individual belongs to one cluster. The number of clusters as well as individual cluster memberships are unknown and must be inferred. We propose an original Bayesian clustering framework that allows us to obtain an exact finite-sample model selection criterion for selecting the number of clusters. Our finite-sample approach is more flexible and parsimonious than asymptotic alternatives such as Bayesian information criterion or integrated classification likelihood criterion in the choice of the number of clusters. Moreover, our approach has other desirable qualities: (i) it keeps the computational effort of the clustering algorithm under control and (ii) it generalizes to several families of regression mixture models, from linear to purely non-parametric. We test our method on simulated datasets as well as on a real world dataset from the Alzheimer’s disease neuroimaging initative database.},
  archive      = {J_CSTAT},
  author       = {Corneli, M. and Erosheva, E. and Qian, X. and Lorenzi, M.},
  doi          = {10.1007/s00180-024-01501-5},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {509-545},
  shortjournal = {Comput. Stat.},
  title        = {A bayesian approach for clustering and exact finite-sample model selection in longitudinal data mixtures},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture models for simultaneous classification and reduction of three-way data. <em>CSTAT</em>, <em>40</em>(1), 469-507. (<a href='https://doi.org/10.1007/s00180-024-01478-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite mixture of Gaussians are often used to classify two- (units and variables) or three- (units, variables and occasions) way data. However, two issues arise: model complexity and capturing the true cluster structure. Indeed, a large number of variables and/or occasions implies a large number of model parameters; while the existence of noise variables (and/or occasions) could mask the true cluster structure. The approach adopted in the present paper is to reduce the number of model parameters by identifying a sub-space containing the information needed to classify the observations. This should also help in identifying noise variables and/or occasions. The maximum likelihood model estimation is carried out through an EM-like algorithm. The effectiveness of the proposal is assessed through a simulation study and an application to real data.},
  archive      = {J_CSTAT},
  author       = {Rocci, Roberto and Vichi, Maurizio and Ranalli, Monia},
  doi          = {10.1007/s00180-024-01478-1},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {469-507},
  shortjournal = {Comput. Stat.},
  title        = {Mixture models for simultaneous classification and reduction of three-way data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust bayesian cumulative probit linear mixed models for longitudinal ordinal data. <em>CSTAT</em>, <em>40</em>(1), 441-468. (<a href='https://doi.org/10.1007/s00180-024-01499-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal studies have been conducted in various fields, including medicine, economics and the social sciences. In this paper, we focus on longitudinal ordinal data. Since the longitudinal data are collected over time, repeated outcomes within each subject may be serially correlated. To address both the within-subjects serial correlation and the specific variance between subjects, we propose a Bayesian cumulative probit random effects model for the analysis of longitudinal ordinal data. The hypersphere decomposition approach is employed to overcome the positive definiteness constraint and high-dimensionality of the correlation matrix. Additionally, we present a hybrid Gibbs/Metropolis-Hastings algorithm to efficiently generate cutoff points from truncated normal distributions, thereby expediting the convergence of the Markov Chain Monte Carlo (MCMC) algorithm. The performance and robustness of our proposed methodology under misspecified correlation matrices are demonstrated through simulation studies under complete data, missing completely at random (MCAR), and missing at random (MAR). We apply the proposed approach to analyze two sets of actual ordinal data: the arthritis dataset and the lung cancer dataset. To facilitate the implementation of our method, we have developed BayesRGMM, an open-source R package available on CRAN, accompanied by comprehensive documentation and source code accessible at https://github.com/kuojunglee/BayesRGMM/ .},
  archive      = {J_CSTAT},
  author       = {Lee, Kuo-Jung and Chen, Ray-Bing and Lee, Keunbaik},
  doi          = {10.1007/s00180-024-01499-w},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {441-468},
  shortjournal = {Comput. Stat.},
  title        = {Robust bayesian cumulative probit linear mixed models for longitudinal ordinal data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). R-estimation in linear models: Algorithms, complexity, challenges. <em>CSTAT</em>, <em>40</em>(1), 405-439. (<a href='https://doi.org/10.1007/s00180-024-01495-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of this paper is to discuss selected computational aspects of robust estimation in the linear model with the emphasis on R-estimators. We focus on numerical algorithms and computational efficiency rather than on statistical properties. In addition, we formulate some algorithmic properties that a “good” method for R-estimators is expected to satisfy and show how to satisfy them using the currently available algorithms. We illustrate both good and bad properties of the existing algorithms. We propose two-stage methods to minimize the effect of the bad properties. Finally we justify a challenge for new approaches based on interior-point methods in optimization.},
  archive      = {J_CSTAT},
  author       = {Antoch, Jaromír and Černý, Michal and Miura, Ryozo},
  doi          = {10.1007/s00180-024-01495-0},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {405-439},
  shortjournal = {Comput. Stat.},
  title        = {R-estimation in linear models: Algorithms, complexity, challenges},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage regression spline modeling based on local polynomial kernel regression. <em>CSTAT</em>, <em>40</em>(1), 383-403. (<a href='https://doi.org/10.1007/s00180-024-01498-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new nonparametric estimator of the regression based on local quasi-interpolation spline method. This model combines a B-spline basis with a simple local polynomial regression, via blossoming approach, to produce a reduced rank spline like smoother. Different coefficients functionals are allowed to have different smoothing parameters (bandwidths) if the function has different smoothness. In addition, the number and location of the knots of this estimator are not fixed. In practice, we may employ a modest number of basis functions and then determine the smoothing parameter as the minimizer of the criterion. In simulations, the approach achieves very competitive performance with P-spline and smoothing spline methods. Simulated data and a real data example are used to illustrate the effectiveness of the method proposed in this paper.},
  archive      = {J_CSTAT},
  author       = {Mraoui, Hamid and El-Alaoui, Ahmed and Bechrouri, Souad and Mohaoui, Nezha and Monir, Abdelilah},
  doi          = {10.1007/s00180-024-01498-x},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {383-403},
  shortjournal = {Comput. Stat.},
  title        = {Two-stage regression spline modeling based on local polynomial kernel regression},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in reliability estimation for the exponentiated pareto distribution: A comparison of classical and bayesian methods with lower record values. <em>CSTAT</em>, <em>40</em>(1), 353-382. (<a href='https://doi.org/10.1007/s00180-024-01497-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the reliability of multicomponent systems is crucial in various engineering and reliability analysis applications. This paper investigates the multicomponent stress strength reliability estimation using lower record values, specifically for the exponentiated Pareto distribution. We compare classical estimation techniques, such as maximum likelihood estimation, with Bayesian estimation methods. Under Bayesian estimation, we employ Markov Chain Monte Carlo techniques and Tierney–Kadane’s approximation to obtain the posterior distribution of the reliability parameter. To evaluate the performance of the proposed estimation approaches, we conduct a comprehensive simulation study, considering various system configurations and sample sizes. Additionally, we analyze real data to illustrate the practical applicability of our methods. The proposed methodologies provide valuable insights for engineers and reliability analysts in accurately assessing the reliability of multicomponent systems using lower record values.},
  archive      = {J_CSTAT},
  author       = {Saini, Shubham},
  doi          = {10.1007/s00180-024-01497-y},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {353-382},
  shortjournal = {Comput. Stat.},
  title        = {Advancements in reliability estimation for the exponentiated pareto distribution: A comparison of classical and bayesian methods with lower record values},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDAS: A newton-type method for $$L_0$$ regularized accelerated failure time model. <em>CSTAT</em>, <em>40</em>(1), 331-352. (<a href='https://doi.org/10.1007/s00180-024-01496-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regularization methods are commonly utilized in survival analysis to address variable selection and estimation problems. Although most of the penalties can be regarded as variations of $$L_0$$ regularization to handle computational challenges, they may not always be efficient or effective in sparse recovery scenarios with massive amounts of data. To address this concern, this paper proposes a method for $$L_0$$ regularized estimation in the high-dimensional accelerated failure time (AFT) model, called the Primal Dual Active Set (PDAS) algorithm. Our approach introduces a tuning parameter to select active sets based on primal and dual information and performs root finding using the Karush-Kuhn-Tucker (KKT) conditions. To generate a sequence of solutions iteratively, this work also presents a sequential Primal Dual Active Set (SPDAS) algorithm that incorporates the PDAS algorithm in each iteration. Our approach can be classified as a Newton-type method to address the $$L_0$$ regularization problem directly. Extensive analysis, including simulations and real data studies, demonstrates that our approach provides competitive performance in terms of computational efficiency and predictive accuracy compared with existing methods for sparse recovery.},
  archive      = {J_CSTAT},
  author       = {Su, Ning and Liu, Yanyan and Kang, Lican},
  doi          = {10.1007/s00180-024-01496-z},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {331-352},
  shortjournal = {Comput. Stat.},
  title        = {PDAS: A newton-type method for $$L_0$$ regularized accelerated failure time model},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized function-on-function linear quantile regression. <em>CSTAT</em>, <em>40</em>(1), 301-329. (<a href='https://doi.org/10.1007/s00180-024-01494-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel function-on-function linear quantile regression model to characterize the entire conditional distribution of a functional response for a given functional predictor. Tensor cubic B-splines expansion is used to represent the regression parameter functions, where a derivative-free optimization algorithm is used to obtain the estimates. Quadratic roughness penalties are applied to the coefficients to control the smoothness of the estimates. The optimal degree of smoothness depends on the quantile of interest. An automatic grid-search algorithm based on the Bayesian information criterion is used to estimate the optimum values of the smoothing parameters. Via a series of Monte-Carlo experiments and an empirical data analysis using Mary River flow data, we evaluate the estimation and predictive performance of the proposed method, and the results are compared favorably with several existing methods.},
  archive      = {J_CSTAT},
  author       = {Beyaztas, Ufuk and Shang, Han Lin and Saricam, Semanur},
  doi          = {10.1007/s00180-024-01494-1},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {301-329},
  shortjournal = {Comput. Stat.},
  title        = {Penalized function-on-function linear quantile regression},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Closed-form expressions of the run-length distribution of the nonparametric double sampling precedence monitoring scheme. <em>CSTAT</em>, <em>40</em>(1), 273-299. (<a href='https://doi.org/10.1007/s00180-024-01488-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A significant challenge in statistical process monitoring (SPM) is to find exact and closed-form expressions (CFEs) (i.e. formed with constants, variables and a finite set of essential functions connected by arithmetic operations and function composition) for the run-length properties such as the average run-length ( $$ARL$$ ), the standard deviation of the run-length ( $$SDRL$$ ), and the percentiles of the run-length ( $$PRL$$ ) of nonparametric monitoring schemes. Most of the properties of these schemes are usually evaluated using simulation techniques. Although simulation techniques are helpful when the expression for the run-length is complicated, their shortfall is that they require a high number of replications to reach reasonably accurate answers. Consequently, they take too much computational time compared to other methods, such as the Markov chain method or integration techniques, and even with many replications, the results are always affected by simulation error and may result in an inaccurate estimation. In this paper, closed-form expressions of the run-length properties for the nonparametric double sampling precedence monitoring scheme are derived and used to evaluate its ability to detect shifts in the location parameter. The computational times of the run-length properties for the CFE and the simulation approach are compared under different scenarios. It is found that the proposed approach requires less computational time compared to the simulation approach. Moreover, once derived, CFEs have the added advantage of ease of implementation, cutting off on complex convergence techniques. CFE's can also easily be built into mathematical software for ease of computation and may be recalled for further work.},
  archive      = {J_CSTAT},
  author       = {Magagula, Zwelakhe and Malela-Majika, Jean-Claude and Human, Schalk William and Castagliola, Philippe and Chatterjee, Kashinath and Koukouvinos, Christos},
  doi          = {10.1007/s00180-024-01488-z},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {273-299},
  shortjournal = {Comput. Stat.},
  title        = {Closed-form expressions of the run-length distribution of the nonparametric double sampling precedence monitoring scheme},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple algorithm for computing the probabilities of count models based on pure birth processes. <em>CSTAT</em>, <em>40</em>(1), 249-272. (<a href='https://doi.org/10.1007/s00180-024-01491-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, non-monotonic rate sequences of pure birth processes have been the focus of much attention in the analysis of count data due to their ability to provide a combination of over-, under-, and equidispersed distributions without the need to reuse covariates (traditional methods). They also permit the modeling of excess counts, a frequent issue arising when using count models based on monotonic rate sequences such as the Poisson, gamma, Weibull, Conway-Maxwell-Poisson (CMP), Faddy (1997), etc. Matrix-exponential approaches have always been used for computing the probabilities for count models based on pure birth processes, although none have been proposed for them as a specific algorithm. It is intractable to calculate these pure birth probabilities numerically in an analytic form because severe numerical cancellations may occur. However, we circumvent this difficulty by exploiting a Taylor series expansion, and then a new analytic form is derived. We developed a simple algorithm for efficiently implementing the new formula and conducted numerical experiments to study the efficiency and accuracy of the developed algorithm. The results indicate that this new approach is faster and more accurate than the matrix-exponential methods.},
  archive      = {J_CSTAT},
  author       = {Hunkrajok, Mongkol and Skulpakdee, Wanrudee},
  doi          = {10.1007/s00180-024-01491-4},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {249-272},
  shortjournal = {Comput. Stat.},
  title        = {A simple algorithm for computing the probabilities of count models based on pure birth processes},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modal regression models based on B-splines. <em>CSTAT</em>, <em>40</em>(1), 225-248. (<a href='https://doi.org/10.1007/s00180-024-01487-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nonparametric model based on B-splines is given for modal regression. The existing nonparametric local polynomial modal regression performs well in goodness of fit but with high computational complexity. Given the nice properties of B-splines, modal regression based on B-splines contains the same performance for estimation compared to that of local polynomial modal regression but requires much less computational burden. We also establish asymptotic properties for the proposed estimator under noise density assumptions. As the commonly used cross-validation hyperparameter selection criteria are not suitable for modal regression, we construct a new cross-validation hyperparameter selection criterion. Furthermore, simulations and applications show that this criterion behaves well for modal regression.},
  archive      = {J_CSTAT},
  author       = {Yang, Lianqiang and Yuan, Wanli and Wang, Shijie},
  doi          = {10.1007/s00180-024-01487-0},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {225-248},
  shortjournal = {Comput. Stat.},
  title        = {Modal regression models based on B-splines},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning embedded EM algorithms for semiparametric mixture regression models. <em>CSTAT</em>, <em>40</em>(1), 205-224. (<a href='https://doi.org/10.1007/s00180-024-01482-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose two machine learning embedded algorithms for a class of semiparametric mixture models, where the mixing proportions and mean functions are unknown but smooth functions of covariates. Embedding machine learning techniques into a modified EM algorithm, the hybrid estimation technique applies the neural network to estimate the nonparametric parts of the model while keeping the structure of the mixture regression model. Compared to the kernel-based techniques, the new method greatly improves the estimation of the nonparametric functions, when the dimension of the covariates is moderately high. Simulation and real data analysis show the superiority of the new method.},
  archive      = {J_CSTAT},
  author       = {Xue, Jiacheng and Yao, Weixin and Xiang, Sijia},
  doi          = {10.1007/s00180-024-01482-5},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {205-224},
  shortjournal = {Comput. Stat.},
  title        = {Machine learning embedded EM algorithms for semiparametric mixture regression models},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An attribute-based Node2Vec model for dynamic community detection on co-authorship network. <em>CSTAT</em>, <em>40</em>(1), 177-204. (<a href='https://doi.org/10.1007/s00180-024-01486-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networks offer a wide range of applications in various domains of life and scientific research. Community detection, which aims at understanding the structure and function of complex networks, is a basic and essential task in network analysis. In this study, we propose an approach for community detection in a dynamic network based on network embedding, incorporating both network topology and node attributes. Furthermore, we analyze the evolution of statistician collaborative patterns and statistical research topics based on dynamic co-authorship networks through publications that are collected from 43 statistical journals from 2001 to 2021. Specifically, we explore the dynamic community detection results based on the newly proposed approach and conduct statistical analysis from the following perspectives. First, the evolution information of the community center is mined. Second, we explore the collaboration mode of community institutions. Finally, we track the evolution of community research content. This study provides a novel method for exploring network representation with node attributes and the analysis of dynamic community detection, as well as offers multiple perspectives for community detection analysis.},
  archive      = {J_CSTAT},
  author       = {Zhou, Tong and Pan, Rui and Zhang, Junfei and Wang, Hansheng},
  doi          = {10.1007/s00180-024-01486-1},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {177-204},
  shortjournal = {Comput. Stat.},
  title        = {An attribute-based Node2Vec model for dynamic community detection on co-authorship network},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A difference-based method for testing no effect in nonparametric regression. <em>CSTAT</em>, <em>40</em>(1), 153-176. (<a href='https://doi.org/10.1007/s00180-024-01479-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a novel difference-based method for testing the hypothesis of no relationship between the dependent and independent variables. We construct three test statistics for nonparametric regression with Gaussian and non-Gaussian random errors. These test statistics have the standard normal as the asymptotic null distribution. Furthermore, we show that these tests can detect local alternatives that converge to the null hypothesis at a rate close to $$n^{-1/2}$$ previously achieved only by the residual-based tests. We also propose a permutation test as a flexible alternative. Our difference-based method does not require estimating the mean function or its first derivative, making it easy to implement and computationally efficient. Simulation results demonstrate that our new tests are more powerful than existing methods, especially when the sample size is small. The usefulness of the proposed tests is also illustrated using two real data examples.},
  archive      = {J_CSTAT},
  author       = {Li, Zhijian and Tong, Tiejun and Wang, Yuedong},
  doi          = {10.1007/s00180-024-01479-0},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {153-176},
  shortjournal = {Comput. Stat.},
  title        = {A difference-based method for testing no effect in nonparametric regression},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Likelihood inference for unified transformation cure model with interval censored data. <em>CSTAT</em>, <em>40</em>(1), 125-151. (<a href='https://doi.org/10.1007/s00180-024-01480-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we extend the unified class of Box–Cox transformation (BCT) cure rate models to accommodate interval-censored data. The probability of cure is modeled using a general covariate structure, whereas the survival distribution of the uncured is modeled through a proportional hazards structure. We develop likelihood inference based on the expectation maximization (EM) algorithm for the BCT cure model. Within the EM framework, both simultaneous maximization and profile likelihood are addressed with respect to estimating the BCT transformation parameter. Through Monte Carlo simulations, we demonstrate the performance of the proposed estimation method through calculated bias, root mean square error, and coverage probability of the asymptotic confidence interval. Also considered is the efficacy of the proposed EM algorithm as compared to direct maximization of the observed log-likelihood function. Finally, data from a smoking cessation study is analyzed for illustrative purpose.},
  archive      = {J_CSTAT},
  author       = {Treszoks, Jodi and Pal, Suvra},
  doi          = {10.1007/s00180-024-01480-7},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {125-151},
  shortjournal = {Comput. Stat.},
  title        = {Likelihood inference for unified transformation cure model with interval censored data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous confidence bands for multiple comparisons of several percentile lines. <em>CSTAT</em>, <em>40</em>(1), 111-123. (<a href='https://doi.org/10.1007/s00180-024-01481-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, it is often necessary to compare several percentile lines. To that end, a set of simultaneous confidence bands has been constructed. The contributions of this research are as follows: (1) the proposed bands are constructed and used to multiple comparisons of several percentile lines for the first time; (2) they allow to draw various comparisons: pairwise, successive and many-to-one; and (3) the comparisons can be drawn on any intervals of interest, and provide more information on both the magnitude and the direction of difference. In addition, practical applications are presented.},
  archive      = {J_CSTAT},
  author       = {Zhou, Sanyu and Zhang, Yu},
  doi          = {10.1007/s00180-024-01481-6},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {111-123},
  shortjournal = {Comput. Stat.},
  title        = {Simultaneous confidence bands for multiple comparisons of several percentile lines},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust estimation of functional factor models with functional pairwise spatial signs. <em>CSTAT</em>, <em>40</em>(1), 87-110. (<a href='https://doi.org/10.1007/s00180-024-01477-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor model analysis has emerged as a powerful tool to capture the latent dynamic structure of functional data from a dimension-reduction viewpoint. Conventional methods for estimating the factor model are sensitive to heavy tails and outliers. To address this issue and achieve robustness, we provide an eigenvalue-ratio based method to estimate the number of factors by replacing the covariance operator with the functional pairwise spatial sign operator. Moreover, we propose a two-step robust approach to recover the factor space. The convergence rates of the robust estimators for factor loadings, factor scores, and common components are derived under some mild conditions. Numerical studies and a real data analysis confirm the proposed procedures remain reliable even when the factors and idiosyncratic errors have heavy-tailed distributions.},
  archive      = {J_CSTAT},
  author       = {Yang, Shuquan and Ling, Nengxiang},
  doi          = {10.1007/s00180-024-01477-2},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {87-110},
  shortjournal = {Comput. Stat.},
  title        = {Robust estimation of functional factor models with functional pairwise spatial signs},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A subspace aggregating algorithm for accurate classification. <em>CSTAT</em>, <em>40</em>(1), 65-86. (<a href='https://doi.org/10.1007/s00180-024-01476-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a technique for learning via aggregation in supervised classification. The new method improves classification performance, regardless of which classifier is at its core. This approach exploits the information hidden in subspaces by combinations of aggregating variables and is applicable to high-dimensional data sets. We provide algorithms that randomly divide the variables into smaller subsets and permute them before applying a classification method to each subset. We combine the resulting classes to predict the class membership. Theoretical and simulation analyses consistently demonstrate the high accuracy of our classification methods. In comparison to aggregating observations through sampling, our approach proves to be significantly more effective. Through extensive simulations, we evaluate the accuracy of various classification methods. To further illustrate the effectiveness of our techniques, we apply them to five real-world data sets.},
  archive      = {J_CSTAT},
  author       = {Amiri, Saeid and Modarres, Reza},
  doi          = {10.1007/s00180-024-01476-3},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {65-86},
  shortjournal = {Comput. Stat.},
  title        = {A subspace aggregating algorithm for accurate classification},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imbalanced data sampling design based on grid boundary domain for big data. <em>CSTAT</em>, <em>40</em>(1), 27-64. (<a href='https://doi.org/10.1007/s00180-024-01471-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data distribution is often associated with a priori-known probability, and the occurrence probability of interest events is small, so a large amount of imbalanced data appears in sociology, economics, engineering, and various other fields. The existing over- and under-sampling methods are widely used in imbalanced data classification problems, but over-sampling leads to overfitting, and under-sampling ignores the effective information. We propose a new sampling design algorithm called the neighbor grid of boundary mixed-sampling (NGBM), which focuses on the boundary information. This paper obtains the classification boundary information through grid boundary domain identification, thereby determining the importance of the samples. Based on this premise, the synthetic minority oversampling technique is applied to the boundary grid, and random under-sampling is applied to the other grids. With the help of this mixed sampling strategy, more important classification boundary information, especially for positive sample information identification is extracted. Numerical simulations and real data analysis are used to discuss the parameter-setting strategy of the NGBM and illustrate the advantages of the proposed NGBM in the imbalanced data, as well as practical applications.},
  archive      = {J_CSTAT},
  author       = {He, Hanji and He, Jianfeng and Zhang, Liwei},
  doi          = {10.1007/s00180-024-01471-8},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {27-64},
  shortjournal = {Comput. Stat.},
  title        = {Imbalanced data sampling design based on grid boundary domain for big data},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Degree selection methods for curve estimation via bernstein polynomials. <em>CSTAT</em>, <em>40</em>(1), 1-26. (<a href='https://doi.org/10.1007/s00180-024-01473-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bernstein Polynomial (BP) bases can uniformly approximate any continuous function based on observed noisy samples. However, a persistent challenge is the data-driven selection of a suitable degree for the BPs. In the absence of noise, asymptotic theory suggests that a larger degree leads to better approximation. However, in the presence of noise, which reduces bias, a larger degree also results in larger variances due to high-dimensional parameter estimation. Thus, a balance in the classic bias-variance trade-off is essential. The main objective of this work is to determine the minimum possible degree of the approximating BPs using probabilistic methods that are robust to various shapes of an unknown continuous function. Beyond offering theoretical guidance, the paper includes numerical illustrations to address the issue of determining a suitable degree for BPs in approximating arbitrary continuous functions.},
  archive      = {J_CSTAT},
  author       = {de Mello e Silva, Juliana Freitas and Ghosh, Sujit Kumar and Mayrink, Vinícius Diniz},
  doi          = {10.1007/s00180-024-01473-6},
  journal      = {Computational Statistics},
  month        = {1},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Comput. Stat.},
  title        = {Degree selection methods for curve estimation via bernstein polynomials},
  volume       = {40},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
