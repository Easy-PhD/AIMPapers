<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJCV</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijcv">IJCV - 361</h2>
<ul>
<li><details>
<summary>
(2025). Correction: BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos. <em>IJCV</em>, <em>133</em>(10), 7513. (<a href='https://doi.org/10.1007/s11263-025-02532-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Duporge, Isla and Kholiavchenko, Maksim and Harel, Roi and Wolf, Scott and Rubenstein, Daniel I and Crofoot, Margaret C and Berger-Wolf, Tanya and Lee, Stephen J and Barreau, Julie and Kline, Jenna and Ramirez, Michelle and Stewart, Charles V},
  doi          = {10.1007/s11263-025-02532-1},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7513},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: A generalized contour vibration model for building extraction. <em>IJCV</em>, <em>133</em>(10), 7512. (<a href='https://doi.org/10.1007/s11263-025-02512-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Xu, Chunyan and Yao, Shuaizhen and Xu, Ziqiang and Cui, Zhen and Yang, Jian},
  doi          = {10.1007/s11263-025-02512-5},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7512},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: A generalized contour vibration model for building extraction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Local concept embeddings for analysis of concept distributions in vision DNN feature spaces. <em>IJCV</em>, <em>133</em>(10), 7511. (<a href='https://doi.org/10.1007/s11263-025-02501-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Mikriukov, Georgii and Schwalbe, Gesina and Bade, Korinna},
  doi          = {10.1007/s11263-025-02501-8},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7511},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Local concept embeddings for analysis of concept distributions in vision DNN feature spaces},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RINDNet++: Edge detection for discontinuity in reflectance, illumination, normal, and depth. <em>IJCV</em>, <em>133</em>(10), 7486-7510. (<a href='https://doi.org/10.1007/s11263-025-02541-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental building block in computer vision, edges, categorized by the discontinuity in surface-Reflectance, Illumination, surface-Normal or Depth, are critical for scene understanding. Despite significant progress in detecting generic or individual edge types, a holistic approach to simultaneously identifying all four edge types presents a unique challenge. In this paper, we propose RINDNet++, a novel neural network solution to address this challenge by detecting all four edge types. RINDNet++ is designed with a three-stage process that effectively leverages the distinct attributes of each edge type and their relationship. In Stage I, a common backbone extracts hierarchical features for all edges. Stage II then tailors these features for each edge type using specialized decoders. Stage III predicts the initial detection results with independent decision heads based on the enhanced features from the preceding stages. Additionally, RINDNet++ incorporates an attention module that refines edge detection by highlighting inter-type relationships, leading to enhanced edge maps. To enhance rigorous training and evaluation, we introduce the first benchmark, BSDS-RIND, incorporating annotations for all four edge types and supporting both single-scale and multi-scale testing. With the integration of state-of-the-art edge detection methods, BSDS-RIND establishes a robust framework for performance evaluation. Extensive experiments show that our proposed RINDNet++ yields promising results in comparison with the state-of-the-art approaches. Moreover, RINDNet++ excels in detecting generic edges and enhances performance in downstream applications such as shadow detection and depth estimation.},
  archive      = {J_IJCV},
  author       = {Pu, Mengyang and Huang, Yaping and Guan, Qingji and Liu, Zhihao and Ling, Haibin},
  doi          = {10.1007/s11263-025-02541-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7486-7510},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RINDNet++: Edge detection for discontinuity in reflectance, illumination, normal, and depth},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segment anything in context with vision foundation models. <em>IJCV</em>, <em>133</em>(10), 7460-7485. (<a href='https://doi.org/10.1007/s11263-025-02517-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of large-scale pre-training, vision foundation models have emerged as powerful tools for open-world image understanding, showcasing remarkable capabilities across a range of visual tasks. However, unlike large language models that excel at directly tackling various language tasks, vision foundation models often require the integration of task-specific architectural modifications and extensive fine-tuning to achieve satisfactory performance in specific domains. This limitation not only increases the complexity of deployment but also restricts their broader applicability in dynamic, real-world scenarios. In this work, we present Matcher, a novel perception paradigm that utilizes off-the-shelf vision foundation models to address various segmentation tasks. Matcher can segment anything by using in-context examples without training. Additionally, we design three effective components within the Matcher framework to collaborate with these foundation models and unleash their full potential in diverse tasks. Specifically, we develop a bidirectional matching strategy to ensure precise visual matching. Then, we design a robust prompt sampler that generates mask proposals with diverse semantic granularity. To further enhance accuracy, we propose an innovative instance-level matching strategy that effectively filters out false-positive mask fragments. In addition, we deploy another vision foundation model to retrieve in-context examples for better prompt engineering for Matcher. Our comprehensive experiments demonstrate that Matcher, without any additional training, has impressive generalization performance across a wide range of visual tasks, underscoring its substantial potential in advancing toward general perception.},
  archive      = {J_IJCV},
  author       = {Liu, Yang and Zhu, Muzhi and Chen, Hao and Wang, Xinlong and Feng, Bo and Wang, Hao and Li, Shiyu and Vemulapalli, Raviteja and Shen, Chunhua},
  doi          = {10.1007/s11263-025-02517-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7460-7485},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Segment anything in context with vision foundation models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FovEx: Human-inspired explanations for vision transformers and convolutional neural networks. <em>IJCV</em>, <em>133</em>(10), 7437-7459. (<a href='https://doi.org/10.1007/s11263-025-02543-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainability in artificial intelligence (XAI) remains a crucial aspect for fostering trust and understanding in machine learning models. Current visual explanation techniques, such as gradient-based or class-activation-based methods, often exhibit a strong dependence on specific model architectures. Conversely, perturbation-based methods, despite being model-agnostic, are computationally expensive as they require evaluating models on a large number of forward passes. We introduce Foveation-based Explanations (FovEx), a novel XAI method inspired by human vision, which combines biologically inspired foveation-based transformations with gradient-driven overt attention to iteratively select locations of interest. These locations are selected to maximize the performance of the model to be explained with respect to the downstream task and then combined to generate an attribution map. We provide a thorough evaluation with qualitative and quantitative assessments on established benchmarks. Our method achieves state-of-the-art performance on both transformers (on 4 out of 5 metrics) and convolutional models (on 3 out of 5 metrics), demonstrating its versatility among various architectures. Furthermore, we show the alignment between the explanation map produced by FovEx and human gaze patterns (+14% in NSS compared to RISE, +203% in NSS compared to GradCAM). This comparison enhances our confidence in FovEx’s ability to close the interpretation gap between humans and machines.},
  archive      = {J_IJCV},
  author       = {Panda, Mahadev Prasad and Tiezzi, Matteo and Vilas, Martina and Roig, Gemma and Eskofier, Bjoern M. and Zanca, Dario},
  doi          = {10.1007/s11263-025-02543-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7437-7459},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FovEx: Human-inspired explanations for vision transformers and convolutional neural networks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On efficient variants of segment anything model: A survey. <em>IJCV</em>, <em>133</em>(10), 7406-7436. (<a href='https://doi.org/10.1007/s11263-025-02539-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Segment Anything Model (SAM) is a foundational model for image segmentation tasks, known for its strong generalization across diverse applications. However, its impressive performance comes with significant computational and resource demands, making it challenging to deploy in resource-limited environments such as edge devices. To address this, a variety of SAM variants have been proposed to enhance efficiency while keeping accuracy. This survey provides the first comprehensive review of these efficient SAM variants. We begin by exploring the motivations driving this research. We then present core techniques used in SAM and model acceleration. This is followed by a detailed exploration of SAM acceleration strategies, categorized by approach, and a discussion of several future research directions. Finally, we offer a unified and extensive evaluation of these methods across various hardware, assessing their efficiency and accuracy on representative benchmarks, and providing a clear comparison of their overall performance. To complement this survey, we summarize the papers and codes related to efficient SAM variants at https://github.com/Image-and-Video-Computing-Group/On-Efficient-Variants-of-Segment-Anything-Model .},
  archive      = {J_IJCV},
  author       = {Sun, Xiaorui and Liu, Jun and Shen, Hengtao and Zhu, Xiaofeng and Hu, Ping},
  doi          = {10.1007/s11263-025-02539-8},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7406-7436},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {On efficient variants of segment anything model: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of data augmentation in visual reinforcement learning. <em>IJCV</em>, <em>133</em>(10), 7368-7405. (<a href='https://doi.org/10.1007/s11263-025-02472-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual reinforcement learning (RL), which makes decisions directly from high-dimensional visual inputs, has demonstrated significant potential in various domains. However, deploying visual RL techniques in the real world remains challenging due to their low sample efficiency and large generalization gaps. To tackle these obstacles, data augmentation (DA) has become a widely used technique in visual RL for acquiring sample-efficient and generalizable policies by diversifying the training data. This survey aims to provide a timely and essential review of DA techniques in visual RL in recognition of the thriving development in this field. In particular, we propose a unified framework for analyzing visual RL and understanding the role of DA in it. We then present a principled taxonomy of the existing augmentation techniques used in visual RL and conduct an in-depth discussion on how to better leverage augmented data in various scenarios. Moreover, we report the empirical evaluation of DA-based techniques in visual RL and conclude by highlighting the directions for future research. As the first comprehensive survey of DA in visual RL, this work is expected to offer valuable guidance to this emerging field.},
  archive      = {J_IJCV},
  author       = {Ma, Guozheng and Wang, Zhen and Yuan, Zhecheng and Wang, Xueqian and Yuan, Bo and Tao, Dacheng},
  doi          = {10.1007/s11263-025-02472-w},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7368-7405},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A comprehensive survey of data augmentation in visual reinforcement learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FineBio: A fine-grained video dataset of biological experiments with hierarchical annotation. <em>IJCV</em>, <em>133</em>(10), 7352-7367. (<a href='https://doi.org/10.1007/s11263-025-02523-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the development of science, accurate and reproducible documentation of the experimental process is crucial. Automatic recognition of the actions in experiments from videos would help experimenters by complementing the recording of experiments. Towards this goal, we propose FineBio, a new fine-grained video dataset of people performing biological experiments. The dataset consists of multi-view videos of 32 participants performing mock biological experiments with a total duration of 14.5 hours. One experiment forms a hierarchical structure, where a protocol consists of several steps, each further decomposed into a set of atomic operations. The uniqueness of biological experiments is that while they require strict adherence to steps described in each protocol, there is freedom in the order of atomic operations. We provide hierarchical annotation on protocols, steps, atomic operations, object locations, and their manipulation states, providing new challenges for structured activity understanding and hand-object interaction recognition. To find out challenges on activity understanding in biological experiments, we introduce baseline models and results on four different tasks, including (i) step segmentation, (ii) atomic operation detection (iii) object detection, and (iv) manipulated/affected object detection. Dataset and code are available from https://github.com/aistairc/FineBio .},
  archive      = {J_IJCV},
  author       = {Yagi, Takuma and Ohashi, Misaki and Huang, Yifei and Furuta, Ryosuke and Adachi, Shungo and Mitsuyama, Toutai and Sato, Yoichi},
  doi          = {10.1007/s11263-025-02523-2},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7352-7367},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FineBio: A fine-grained video dataset of biological experiments with hierarchical annotation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-rate monocular depth estimation via cross frame-rate collaboration of frames and events. <em>IJCV</em>, <em>133</em>(10), 7332-7351. (<a href='https://doi.org/10.1007/s11263-025-02488-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining the complementary benefits of frames and events has been widely used for monocular depth estimation in challenging scenarios. However, most existing methods utilize a synchronous fusion of two modalities, ignoring the advantages of high temporal resolution from event cameras, which results in low-rate depth maps constrained by the frame sampling rate. To this end, this paper proposes a novel cross frame-rate frame-event joint learning network, namely CFRNet, collaborating two heterogeneous streams for high-rate and fine-grained monocular depth estimation. Technically, a cross frame-rate multimodal fusion (CFMF) module is first designed for the joint representation of frames and events. By employing implicit spatial alignment and dynamic attention-based fusion, it addresses the misalignment between frames and events at different moments, robustly combining the strengths of both modalities in diverse challenging scenarios. Following the CFMF, a temporal consistent modeling (TCM) module adopting the recurrent structure is created to keep the temporal consistency of joint representations from CFMF. Experimental results demonstrate that the depth estimation accuracy of our approach outperforms existing five state-of-the-art methods and our three baselines involving single modality on two public datasets (i.e., DSEC and MVSEC) while achieving a high frame rate up to 100 Hz. Codes can be available at https://github.com/liuxu0303/CFRNet .},
  archive      = {J_IJCV},
  author       = {Liu, Xu and Fan, Xiaopeng and Li, Jianing and Li, Dianze and Zhang, Wei and Ma, Zhengyu and Tian, Yonghong},
  doi          = {10.1007/s11263-025-02488-2},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7332-7351},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {High-rate monocular depth estimation via cross frame-rate collaboration of frames and events},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning latent part-whole hierarchies for point clouds. <em>IJCV</em>, <em>133</em>(10), 7312-7331. (<a href='https://doi.org/10.1007/s11263-025-02533-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strong evidence suggests that humans perceive the 3D world by parsing visual scenes and objects into part-whole hierarchies. Although deep neural networks have the capability of learning powerful multi-level representations, they can not explicitly model part-whole hierarchies, which limits their expressiveness and interpretability in processing 3D vision data such as point clouds. To this end, we propose an encoder-decoder style latent variable model that explicitly learns the part-whole hierarchies for the multi-level point cloud segmentation. Specifically, the encoder takes a point cloud as input and predicts the per-point latent subpart distribution at the middle level. The decoder takes the latent variable and the feature from the encoder as an input and predicts the per-point part distribution at the top level. During training, only annotated part labels at the top level are provided, thus making the whole framework weakly supervised. We explore two kinds of approximated inference algorithms, i.e., most-probable-latent and Monte Carlo methods, and three stochastic gradient estimations for learning discrete latent variables, i.e., straight-through, REINFORCE, and pathwise estimators. Experimental results on the PartNet dataset show that the proposed method achieves state-of-the-art performance in not only top-level part segmentation but also middle-level latent subpart segmentation.},
  archive      = {J_IJCV},
  author       = {Gao, Xiang and Hu, Wei and Liao, Renjie},
  doi          = {10.1007/s11263-025-02533-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7312-7331},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning latent part-whole hierarchies for point clouds},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards balanced representation learning with semantic anchor regularization. <em>IJCV</em>, <em>133</em>(10), 7293-7311. (<a href='https://doi.org/10.1007/s11263-025-02519-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation learning refers to the process of learning meaningful and informative features from raw data, of which one good criterion is to attain intra-class compactness and inter-class separability in the semantic space. However,real-world data are always imbalanced and noisy. Existing methods such as prototype-based learning and contrastive learning are deeply bounded to the feature learning process and susceptible to imbalanced data distribution. In this paper, we disentangle the representation regularization from the feature learning process and propose a semantic anchor regularization (SAR) that is generated from predefined anchors. These anchors serve as an independent third-party measurement and are made semantic-aware by sharing the task head with feature learning. By controlling the separability between semantic anchors and pulling the learned representation to these semantic anchors, the intra-class compactness and inter-class separability can be intuitively achieved. In essence, SAR performs in the manner of visual-language alignment but is more flexible. Extensive results on classification, segmentation, long-tailed learning, and semi-supervised learning demonstrate the SAR’s effectiveness for different downstream tasks.},
  archive      = {J_IJCV},
  author       = {Wang, Chengjie and Nie, Qiang and Chen, Ying and Li, Jialin and Liu, Yong and Jiang, Xi and Ge, Yanqi and Wu, Yunsheng and Zheng, Feng and Ma, Lizhuang},
  doi          = {10.1007/s11263-025-02519-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7293-7311},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards balanced representation learning with semantic anchor regularization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Six-point method for multi-camera systems with reduced solution space. <em>IJCV</em>, <em>133</em>(10), 7270-7292. (<a href='https://doi.org/10.1007/s11263-025-02531-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relative pose estimation using point correspondences (PC) is a widely used technique. A minimal configuration of six PCs is required for two views of generalized cameras. In this paper, we present several minimal solvers that use six PCs to compute the 6DOF relative pose of multi-camera systems, including a minimal solver for the generalized camera and two minimal solvers for the practical configuration of two-camera rigs. The equation construction is based on the decoupling of rotation and translation. Rotation is represented by Cayley or quaternion parametrization, and translation can be eliminated by using the hidden variable technique. Ray bundle constraints are found and proven when a subset of PCs relate the same cameras across two views. This is the key to reducing the number of solutions and generating numerically stable solvers. Moreover, all configurations of six-point problems for multi-camera systems are enumerated by the Pólya enumeration theorem. Extensive experiments demonstrate the superior accuracy and efficiency of our solvers compared to state-of-the-art six-point methods. The code is available at https://github.com/jizhaox/relpose-6pt .},
  archive      = {J_IJCV},
  author       = {Guan, Banglei and Zhao, Ji and Mitra, Saibal and Kneip, Laurent},
  doi          = {10.1007/s11263-025-02531-2},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7270-7292},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Six-point method for multi-camera systems with reduced solution space},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active stereo in the wild through virtual pattern projection. <em>IJCV</em>, <em>133</em>(10), 7242-7269. (<a href='https://doi.org/10.1007/s11263-025-02511-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel general-purpose guided stereo paradigm that mimics the active stereo principle by replacing the unreliable physical pattern projector with a depth sensor. It works by projecting virtual patterns consistent with the scene geometry onto the left and right images acquired by a conventional stereo camera, using the sparse hints obtained from a depth sensor, to facilitate the visual correspondence. Purposely, any depth sensing device can be seamlessly plugged into our framework, enabling the deployment of a virtual active stereo setup in any possible environment and overcoming the severe limitations of physical pattern projection, such as the limited working range and environmental conditions. Exhaustive experiments on indoor and outdoor datasets featuring both long and close range, including those providing raw, unfiltered depth hints from off-the-shelf depth sensors, highlight the effectiveness of our approach in notably boosting the robustness and accuracy of algorithms and deep stereo without any code modification and even without re-training. Additionally, we assess the performance of our strategy on active stereo evaluation datasets with conventional pattern projection. Indeed, in all these scenarios, our virtual pattern projection paradigm achieves state-of-the-art performance. The source code is available at: https://github.com/bartn8/vppstereo .},
  archive      = {J_IJCV},
  author       = {Bartolomei, Luca and Poggi, Matteo and Tosi, Fabio and Conti, Andrea and Mattoccia, Stefano},
  doi          = {10.1007/s11263-025-02511-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7242-7269},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Active stereo in the wild through virtual pattern projection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CEDFlow++: Latent contour enhancement for dark optical flow estimation. <em>IJCV</em>, <em>133</em>(10), 7222-7241. (<a href='https://doi.org/10.1007/s11263-025-02528-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CEDFlow introduces a latent contour enhancement method into dark optical flow estimation and achieves advanced performance. Nevertheless, it largely focuses on addressing the motion boundary in a local manner. Unfortunately, it falls short in performance when addressing significant variations or large-scale degraded scenes. This paper introduces CEDFlow++, which features three innovative modules to address the key challenges of CEDFlow. Firstly, we introduce a decomposition-based feature encoder (DBFE), which captures both fine-grained and large-scale features through its local encoder and a uniquely designed sparse attention-based global encoder that suppresses noise and interference that only exist in the dark. Secondly, for reliable motion analysis, we propose a customized dual cost-volume reasoning (DCVR), which integrates important high-contrast feature correlations of the global cost volume into the local cost volume, effectively capturing salient yet holistic motion information while mitigating motion ambiguity caused by darkness. Importantly, we present a contour-guided attention (CGA) which enables context-adaptive extraction of contour features by modifying the sign properties of the Sobel kernel parameters in latent space, specifically targeting large-scale contours that are suitable for motion boundaries. Experimental results on the FCDN and VBOF datasets show that CEDFlow++ outperforms state-of-the-art methods in terms of the EPE index and produces more accurate and robust optical flow.},
  archive      = {J_IJCV},
  author       = {Zuo, Fengyuan and Jin, Haiyan and Xiao, Zhaolin and Su, Haonan and Zhang, Meng},
  doi          = {10.1007/s11263-025-02528-x},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7222-7241},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CEDFlow++: Latent contour enhancement for dark optical flow estimation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mamba capsule routing towards part-whole relational camouflaged object detection. <em>IJCV</em>, <em>133</em>(10), 7201-7221. (<a href='https://doi.org/10.1007/s11263-025-02530-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The part-whole relational property endowed by Capsule Networks (CapsNets) has been known successful for camouflaged object detection due to its segmentation integrity. However, the previous Expectation Maximization (EM) capsule routing algorithm with heavy computation and large parameters obstructs this trend. The primary attribution behind lies in the pixel-level capsule routing. Alternatively, in this paper, we propose a novel mamba capsule routing at the type level. Specifically, we first extract the implicit latent state in mamba as capsule vectors, which abstract type-level capsules from pixel-level versions. These type-level mamba capsules are fed into the EM routing algorithm to get the high-layer mamba capsules, which greatly reduce the computation and parameters caused by the pixel-level capsule routing for part-whole relationships exploration. On top of that, to retrieve the pixel-level capsule features for further camouflaged prediction, we achieve this on the basis of the low-layer pixel-level capsules with the guidance of the correlations from adjacent-layer type-level mamba capsules. Extensive experiments on three widely used COD benchmark datasets demonstrate that our method significantly outperforms state-of-the-arts. Code has been available on https://github.com/Liangbo-Cheng/mamba_capsule .},
  archive      = {J_IJCV},
  author       = {Zhang, Dingwen and Cheng, Liangbo and Liu, Yi and Wang, Xinggang and Han, Junwei},
  doi          = {10.1007/s11263-025-02530-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7201-7221},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mamba capsule routing towards part-whole relational camouflaged object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked image modeling: A survey. <em>IJCV</em>, <em>133</em>(10), 7154-7200. (<a href='https://doi.org/10.1007/s11263-025-02524-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we survey recent studies on masked image modeling (MIM), an approach that emerged as a powerful self-supervised learning technique in computer vision. The MIM task involves masking some information, e.g. pixels, patches, or even latent representations, and training a model, usually an autoencoder, to predicting the missing information by using the context available in the visible part of the input. We identify and formalize two categories of approaches on how to implement MIM as a pretext task, one based on reconstruction and one based on contrastive learning. Then, we construct a taxonomy and review the most prominent papers in recent years. We complement the manually constructed taxonomy with a dendrogram obtained by applying a hierarchical clustering algorithm. We further identify relevant clusters via manually inspecting the resulting dendrogram. Our review also includes datasets that are commonly used in MIM research. We aggregate the performance results of various masked image modeling methods on the most popular datasets, to facilitate the comparison of competing methods. Finally, we identify research gaps and propose several interesting directions of future work. We supplement our survey with the following public repository containing organized references: https://github.com/vladhondru25/MIM-Survey .},
  archive      = {J_IJCV},
  author       = {Hondru, Vlad and Croitoru, Florinel Alin and Minaee, Shervin and Ionescu, Radu Tudor and Sebe, Nicu},
  doi          = {10.1007/s11263-025-02524-1},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7154-7200},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Masked image modeling: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Insect-foundation: A foundation model and large multimodal dataset for vision-language insect understanding. <em>IJCV</em>, <em>133</em>(10), 7128-7153. (<a href='https://doi.org/10.1007/s11263-025-02521-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual insects since they are often trained on the general knowledge of vision-language data. Meanwhile, understanding insects is a fundamental problem in precision agriculture, helping to promote sustainable development in agriculture. Therefore, this paper proposes a novel multimodal conversational model, Insect-LLaVA, to promote visual understanding in insect-domain knowledge. In particular, we first introduce a new large-scale Multimodal Insect Dataset with Visual Insect Instruction Data that enables the capability of learning the multimodal foundation models. Our proposed dataset enables conversational models to comprehend the visual and semantic features of the insects. Second, we propose a new Insect-LLaVA model, a new general Large Language and Vision Assistant in Visual Insect Understanding. Then, to enhance the capability of learning insect features, we develop an Insect Foundation Model by introducing a new micro-feature self-supervised learning with a Patch-wise Relevant Attention mechanism to capture the subtle differences among insect images. We also present Description Consistency loss to improve micro-feature learning via text descriptions. The experimental results evaluated on our new Visual Insect Question Answering benchmarks illustrate the effective performance of our proposed approach in visual insect understanding and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks Project Page: https://uarkcviu.github.io/projects/insectfoundation .},
  archive      = {J_IJCV},
  author       = {Truong, Thanh-Dat and Nguyen, Hoang-Quan and Nguyen, Xuan-Bac and Dowling, Ashley and Li, Xin and Luu, Khoa},
  doi          = {10.1007/s11263-025-02521-4},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7128-7153},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Insect-foundation: A foundation model and large multimodal dataset for vision-language insect understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A causal intervention method for domain generalization with a self-supervised auxiliary task. <em>IJCV</em>, <em>133</em>(10), 7110-7127. (<a href='https://doi.org/10.1007/s11263-025-02529-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization tackles the challenge of domain shifts by learning a model from diverse source domains that can effectively generalize to unseen target domains. This paper explores domain generalization on image data with auxiliary learning task(s), which leverages auxiliary task(s) to extract transferable features and boost the performance of the primary domain generalization task on unseen domains. Causal intervention provides an attractive strategy to tackle domain shifts and learn causal dependencies in domain generalization tasks. However, most of the existing causal intervention methods are tailored for single-task learning. Causal intervention on multiple tasks (e.g., the primary and auxiliary tasks in domain generalization) remains under-explored. In this paper, we propose CI-DGA, a novel causal intervention method for domain generalization on image data with self-supervised auxiliary task(s). In CI-DGA, we employ a hidden confounder to model the data distribution of a specific domain, wherein this distribution changing with domain shifts. Theoretically, we show that the negative effects by this confounder can be eliminated by causal intervention, and the causal relations between the input images and the labels in both the primary and auxiliary tasks are identifiable. Additionally, we develop a deep architecture to implement the causal inference models, in which we provide an approximate strategy to reduce the computational cost and avoid simultaneous sampling operations on multiple variables. Comprehensive experimental results on three widely-used benchmark datasets show that the proposed CI-DGA has superior performance against state-of-the-art baselines for domain generalization on images.},
  archive      = {J_IJCV},
  author       = {Gong, Qinkang and Pan, Yan and Lai, Hanjiang and Yin, Jian},
  doi          = {10.1007/s11263-025-02529-w},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7110-7127},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A causal intervention method for domain generalization with a self-supervised auxiliary task},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayes-CAL: Robust cross-modal alignment by bayesian approach for few-shot OoD generalization. <em>IJCV</em>, <em>133</em>(10), 7076-7109. (<a href='https://doi.org/10.1007/s11263-025-02527-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, fine-tuning large pre-trained models has yielded promising results in few-shot learning regime. However, their ability to generalize on two-dimensional Out-of-Distribution (OoD) data, characterized by correlation shift and diversity shift, remains largely unexplored. Recent studies demonstrate that even with a vast amount of data, few methods can beat the empirical risk minimization method (ERM) simultaneously on the two-dimensional OoD generalization. Consequently, OoD generalization in the few-shot regime emerges as a challenging area in model generalization research, where the OoD test performance suffers from the synergies of both data distribution shifts and overfitting few-shot samples. In this paper, utilizing informative natural language supervision, we investigate a novel Bayesian cross-modal image-text alignment learning method (Bayes-CAL) to address this issue, where only the texts representations are fine-tuned via image-text alignment in a domain-invariant manner under the proposed regularization. The Bayesian approach is essentially introduced to avoid overfitting the base classes observed in the training process and improve generalization to unseen classes. Under mild assumptions, we have theoretically demonstrated our Bayes-CAL method can achieve lower generalization errors under distribution shifts, compared to previous methods. To validate the effectiveness of the proposed Bayes-CAL, in addition to the extensive experiments on the image classification task, we also verify the OoD generalization performance on the object detection and instance segmentation tasks, addressing the gap in previous works on the challenging few-shot OoD generalization. Compared with recent CLIP-based methods, Bayes-CAL achieved state-of-the-art OoD generalization performance on these tasks with a large margin. More stable generalization performances on unseen classes of the Bayes-CAL are further demonstrated in the experiments.},
  archive      = {J_IJCV},
  author       = {Zhu, Lin and Yin, Weihan and Wu, Fan and Gu, Qinying and Wang, Xinbing and Zhou, Chenghu and Ye, Nanyang},
  doi          = {10.1007/s11263-025-02527-y},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7076-7109},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bayes-CAL: Robust cross-modal alignment by bayesian approach for few-shot OoD generalization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot paragraph-level handwriting imitation with latent diffusion models. <em>IJCV</em>, <em>133</em>(10), 7054-7075. (<a href='https://doi.org/10.1007/s11263-025-02525-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The imitation of cursive handwriting is mainly limited to generating handwritten words or lines. Multiple synthetic outputs must be stitched together to create paragraphs or whole pages, whereby consistency and layout information are lost. To close this gap, we propose a method for imitating handwriting at the paragraph level that also works for unseen writing styles. Therefore, we introduce a modified latent diffusion model that enriches the encoder-decoder mechanism with specialized loss functions that explicitly preserve the style and content. We enhance the attention mechanism of the diffusion model with adaptive 2D positional encoding and the conditioning mechanism to work with two modalities simultaneously: a style image and the target text. This significantly improves the realism of the generated handwriting. We set a new benchmark in our comprehensive evaluation, achieving 61 % mAP and 56 % top-1 accuracy in style preservation, significantly outperforming the previous best method (37 % mAP, 30 % top-1). We are making our code publicly available for reproducibility, supporting research in this area and research into potential countermeasures: https://github.com/M4rt1nM4yr/paragraph_handwriting_imitation_ldm},
  archive      = {J_IJCV},
  author       = {Mayr, Martin and Dreier, Marcel and Kordon, Florian and Seuret, Mathias and Zöllner, Jochen and Wu, Fei and Maier, Andreas and Christlein, Vincent},
  doi          = {10.1007/s11263-025-02525-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7054-7075},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Zero-shot paragraph-level handwriting imitation with latent diffusion models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DreamArtist: Controllable one-shot text-to-image generation via positive-negative adapter. <em>IJCV</em>, <em>133</em>(10), 7037-7053. (<a href='https://doi.org/10.1007/s11263-025-02526-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-arts text-to-image generation models such as Imagen Saharia et al. (2022) and Stable Diffusion Model Rombach et al. (2022) have succeed remarkable progresses in synthesizing high-quality, feature-rich images with high resolution guided by human text prompts. Since certain characteristics of image content e.g., very specific object entities or styles, are very hard to be accurately described by text, some example-based image generation approaches have been proposed, i.e. generating new concepts based on absorbing the salient features of a few input references. Despite of acknowledged successes, these methods have struggled on accurately capturing the reference examples’ characteristics while keeping diverse and high-quality image generation, particularly in the one-shot scenario (i.e. given only one reference). To tackle this problem, we propose a simple yet effective framework, namely DreamArtist, which adopts a novel positive-negative prompt-tuning learning strategy on the pre-trained diffusion model, and it has shown to well handle the trade-off between the accurate controllability and fidelity of image generation with only one reference example. Specifically, our proposed framework incorporates both positive and negative embeddings or adapters and optimizes them in a joint manner. The positive part aggressively captures the salient characteristics of the reference image to drive diversified generation and the negative part rectifies inadequacies from the positive part. We have conducted extensive experiments and evaluated the proposed method from image similarity (fidelity) and diversity, generation controllability, and style cloning. And our DreamArtist has achieved a superior generation performance over existing methods. Besides, our additional evaluation on extended tasks, including concept compositions and prompt-guided image editing, demonstrates its effectiveness for more applications. DreamArtist project page: https://www.sysu-hcp.net/projects/dreamartist/index.html},
  archive      = {J_IJCV},
  author       = {Dong, Ziyi and Wei, Pengxu and Lin, Liang},
  doi          = {10.1007/s11263-025-02526-z},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7037-7053},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DreamArtist: Controllable one-shot text-to-image generation via positive-negative adapter},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DomainStudio: Fine-tuning diffusion models for domain-driven image generation using limited data. <em>IJCV</em>, <em>133</em>(10), 7012-7036. (<a href='https://doi.org/10.1007/s11263-025-02498-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising diffusion probabilistic models (DDPMs) have been proven capable of synthesizing high-quality images with remarkable diversity when trained on large amounts of data. Unfortunately, they are still vulnerable to overfitting when fine-tuned on limited data. Existing works have explored subject-driven generation with text-to-image (T2I) models using a few samples. However, there is still a lack of effective and stable data-efficient methods to synthesize images in specific domains (e.g. styles or properties), which remains challenging due to ambiguities inherent in natural language and out-of-distribution effects. This paper introduces a few-shot fine-tuning approach named DomainStudio as a domain-driven image generation paradigm, which is designed to retain the subjects from prior knowledge provided by pre-trained models and adapt them to the domain extracted from training data, pursuing high quality and great diversity. We propose to keep the image-level relative distances between adapted samples and enhance the learning of high-frequency details from both pre-trained models and training samples. DomainStudio is compatible with both unconditional and T2I DDPMs. The proposed method achieves better results than current state-of-the-art GAN-based approaches in unconditional few-shot image generation. It also outperforms existing few-shot fine-tuning methods for modern large-scale T2I diffusion models like Textual Inversion and DreamBooth on synthesizing samples in specific domains characterized by few-shot training data.},
  archive      = {J_IJCV},
  author       = {Zhu, Jingyuan and Ma, Huimin and Chen, Jiansheng and Yuan, Jian},
  doi          = {10.1007/s11263-025-02498-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {7012-7036},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DomainStudio: Fine-tuning diffusion models for domain-driven image generation using limited data},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RealDTT: Towards a comprehensive real-world dataset for tampered text detection. <em>IJCV</em>, <em>133</em>(10), 6993-7011. (<a href='https://doi.org/10.1007/s11263-025-02515-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The swift advancement of text manipulation in AI-generated images and the rise of false document fabrication emphasize the need for effective detection methods applicable in real-world settings. While current forensics research primarily addresses tampered text in natural images, text manipulation in documents presents a more realistic struggle to handle. To address the robustness of current detection methods and datasets, we aim to develop a real-world, large-scale dataset containing manually tampered documents and diverse automatic tampering techniques. Our work distinguishes itself from existing benchmarks through three key features: Manual Tampering: encompassing the simulation of realism and cognition, where human edits are often subtle and contextually coherent. Diverse Generators: rich manipulating types for tampered images ensure the coverage of traditional and advanced tampering techniques. Multilingual and Multiscene Coverage: spanning English and Chinese text across natural scenes and documents, with varied resolutions. We have developed a comprehensive dataset, RealDTT, to evaluate the open-set generalization capabilities of text-tampered detection models. The RealDTT encompasses approximately 300,000 diverse synthetic samples originating from nine distinct generative models. To our knowledge, this represents the most extensive collection of Deepfake model types currently available. Complementing these synthetic samples are 4,012 meticulously manually tampered images. Moreover, leveraging the RealDTT dataset, we propose a robust tampered text detection model, TTDMamba, which fully harnesses the unique strengths of the Mamba architecture and integrates selective scanning, high-frequency feature aggregation, and disentangled semantic axial attention to process global information while maintaining linear complexity. Extensive experiments demonstrate that the proposed TTDMamba exhibits remarkable efficacy.},
  archive      = {J_IJCV},
  author       = {Duan, Junxian and Sun, Hao and Ji, Fan and Zhou, Kai and Wang, Zhiyong and Huang, Huaibo and Jin, Lianwen},
  doi          = {10.1007/s11263-025-02515-2},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6993-7011},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RealDTT: Towards a comprehensive real-world dataset for tampered text detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale texture fusion for reference-based image super-resolution: New dataset and solution. <em>IJCV</em>, <em>133</em>(10), 6971-6992. (<a href='https://doi.org/10.1007/s11263-025-02514-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR) methods often encounter great performance drops on severe degraded low-resolution (LR) images. Recently, reference-based super-resolution (RefSR) methods offer a promising solution by introducing high-quality reference (Ref) images as prior for reconstruction. However, existing RefSR methods often struggle to effectively explore informative textures from Ref images. Additionally, their performance is significantly restricted by the quality of the available training dataset. To address these challenges, we propose an innovative multi-scale texture fusion for reference-based super-resolution via a state-space model, which enables efficient multi-scale feature fusion and long-term dependency modeling for better texture restoration. Specifically, our method mainly consists of a series of texture matching fusion groups (TMFG), which include a multi-scale matching module (MSMM) and a state-space fusion module (SSFM). MSMM can match multi-scale features of Ref images at different stages of the image restoration process to accurately locate key multi-scale similar textures contained in Ref images. SSFM effectively fuses multi-scale textures obtained from Ref images by leveraging stronger modeling relationships based on long-term dependencies of linear complexity. Such a design encourages in-depth exploration of the multi-scale texture correspondence between LR and Ref images, thereby achieving the utilization of textures in Ref and better assisting in image restoration. Notably, we introduce a new large-scale dataset, dubbed DRefSR, designed explicitly for the RefSR task. Our DRefSR offers a wider variety of scenes, more accurately matched image pairs, and a larger volume of samples. With 47,653 image pairs, our dataset substantially exceeds existing datasets (13,761 pairs) in scale. Experiments verify our dataset’s superiority and demonstrate that our method outperforms SOTA methods both quantitatively and qualitatively. DRefSR dataset and code are available at: https://github.com/edbca/SSMTF .},
  archive      = {J_IJCV},
  author       = {Zhou, Hongyang and Zhu, Xiaobin and Qin, Jingyan and Xu, Yu and Cesar-Jr., Roberto M. and Yin, Xu-Cheng},
  doi          = {10.1007/s11263-025-02514-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6971-6992},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-scale texture fusion for reference-based image super-resolution: New dataset and solution},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reblurring-guided single image defocus deblurring: A learning framework with misaligned training pairs. <em>IJCV</em>, <em>133</em>(10), 6953-6970. (<a href='https://doi.org/10.1007/s11263-025-02522-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For single image defocus deblurring, acquiring well-aligned training pairs (or training triplets), i.e., a defocus blurry image, an all-in-focus sharp image (and a defocus blur map), is a challenging task for developing effective deblurring models. Existing image defocus deblurring methods typically rely on training data collected by specialized imaging equipment, with the assumption that these pairs or triplets are perfectly aligned. However, in practical scenarios involving the collection of real-world data, direct acquisition of training triplets is infeasible, and training pairs inevitably encounter spatial misalignment issues. In this work, we introduce a reblurring-guided learning framework for single image defocus deblurring, enabling the learning of a deblurring network even with misaligned training pairs. By reconstructing spatially variant isotropic blur kernels, our reblurring module ensures spatial consistency between the deblurred image, the reblurred image and the input blurry image, thereby addressing the misalignment issue while effectively extracting sharp textures from the all-in-focus sharp image. Moreover, spatially variant blur can be derived from the reblurring module, and serve as pseudo supervision for defocus blur map during training, interestingly transforming training pairs into training triplets. To leverage this pseudo supervision, we propose a lightweight defocus blur estimator coupled with a fusion block, which enhances deblurring performance through seamless integration with state-of-the-art deblurring networks. Additionally, we have collected a new dataset for single image defocus deblurring (SDD) with typical misalignments, which not only validates our proposed method but also serves as a benchmark for future research. The effectiveness of our method is validated by notable improvements in both quantitative metrics and visual quality across several datasets with real-world defocus blurry images, including DPDD, RealDOF, DED, and our SDD. The source code and dataset are available at https://github.com/ssscrystal/Reblurring-guided-JDRL .},
  archive      = {J_IJCV},
  author       = {Ren, Dongwei and Shu, Xinya and Li, Yu and Wu, Xiaohe and Li, Jin and Zuo, Wangmeng},
  doi          = {10.1007/s11263-025-02522-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6953-6970},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Reblurring-guided single image defocus deblurring: A learning framework with misaligned training pairs},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAT-TPT: Class-agnostic text-based test-time prompt tuning for vision-language models. <em>IJCV</em>, <em>133</em>(10), 6930-6952. (<a href='https://doi.org/10.1007/s11263-025-02508-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning has emerged as an effective method for adapting pre-trained vision-language models (VLMs) to diverse downstream tasks. However, it often struggles with generalization to unseen domains due to its dependence on labeled data. Unlike traditional approaches that rely on fixed prompts or parameters learned during training, Test-time Prompt Tuning (TPT) dynamically refines learnable prompts for individual samples at test time. Nevertheless, existing TPT methods frequently overlook alignment between visual and textual embeddings and lack mechanisms to ensure intra-modal diversity. In this work, we introduce CAT-TPT (Class-Agnostic Text-based Test-time Prompt Tuning), a novel approach that integrates attribute-guided augmentation, improved visual-textual alignment, and label-free adaptation for VLMs. By leveraging class-agnostic attributes generated by a large language model, CAT-TPT jointly optimizes both vision and language modalities, promoting enhanced intra-class diversity and seamless adaptation at test time. Extensive experiments demonstrate that CAT-TPT consistently outperforms state-of-the-art methods in zero-shot generalization, achieving an average improvement of 6.66% over existing TPT methods on out-of-distribution (OOD) data across five benchmarks, 3.17% in cross-dataset evaluations across ten fine-grained datasets, and 4.04% under fifteen diverse and challenging corruption types. Code is available at https://github.com/AIM-SKKU/CAT-TPT .},
  archive      = {J_IJCV},
  author       = {Zhang, Youjia and Liu, Huiling and Kim, Youngeun and Hong, Sungeun},
  doi          = {10.1007/s11263-025-02508-1},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6930-6952},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CAT-TPT: Class-agnostic text-based test-time prompt tuning for vision-language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guiding audio-visual question answering with collective question reasoning. <em>IJCV</em>, <em>133</em>(10), 6912-6929. (<a href='https://doi.org/10.1007/s11263-025-02510-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-Visual Question Answering (AVQA) requires the model to answer questions with complex dynamic audio-visual information. Prior works on this task mainly consider only using single question-answer pairs during training, overlooking the rich semantic associations between questions. In this work, we propose a novel Collective Question-Guided Network (CoQo), which accepts multiple question-answer pairs as input and leverages the reasoning over these questions to assist the model training process. The core module is the proposed Question Guided Transformer (QGT), which uses collective question reasoning to perform question-guided feature extraction. Since multiple question-answer pairs are not always available, especially during inference, our QGT uses a set of learnable tokens to learn the collective information from multiple questions during training. At inference time, these learnable tokens bring additional reasoning information even when only one question is used as input. We employ QGT in both spatial and temporal dimensions to extract question-related features effectively and efficiently. To better capture detailed audio-visual associations, we train the model in a finer level by distinguishing feature pairs of different questions within the same video. Extensive experiments demonstrate that our method can achieve state-of-the-art performance on three AVQA datasets while reducing training time significantly. We also observe strong performances of our method on three VQA benchmarks. Detailed ablation studies further confirm the effectiveness of our proposed collective question reasoning scheme, both quantitatively and qualitatively.},
  archive      = {J_IJCV},
  author       = {Pei, Baoqi and Huang, Yifei and Chen, Guo and Xu, Jilan and Wang, Yali and Wang, Limin and Lu, Tong and Qiao, Yu and Wu, Fei},
  doi          = {10.1007/s11263-025-02510-7},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6912-6929},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guiding audio-visual question answering with collective question reasoning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic masking with curriculum learning for robust HDR image reconstruction. <em>IJCV</em>, <em>133</em>(10), 6896-6911. (<a href='https://doi.org/10.1007/s11263-025-02504-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High Dynamic Range (HDR) image reconstruction aims to reconstruct images with a larger dynamic range from multiple Low Dynamic Range (LDR) images with different exposures. Existing methods face two challenges: visual artifacts in the restored images and insufficient model generalization capabilities. This paper addresses these issues by leveraging the inherent potential of Masked Image Modeling (MIM). We propose a Segment Anything Model (SAM)-guided masking strategy, leveraging large-model priors to direct the HDR reconstruction network via curriculum learning. This strategy gradually increases the difficulty from simple to complex tasks, guiding the model to effectively learn semantic priors that prevent the model from overfitting to the training data. Our approach starts by training the model without any masks, then progressively increasing the masking ratio of input features guided by semantic segmentation maps, which compels the model to learn semantic information during restoration. Subsequently, we make an adaption to reduce the masking ratio to minimize the input discrepancy between the training and testing stage. Besides, we manipulate the computation of the loss based on the perceptual quality of reconstructed images, where challenging areas (e.g., over-/under-exposed regions) are given more weight to improve image restoration results. Furthermore, through specialized module design, our method can be fine-tuned to any number of inputs, achieving comparable performance to models trained from scratch with only 5.5% of parameter adjustments. Extensive qualitative and quantitative experiments demonstrate that our approach surpasses state-of-the-art methods in both effectiveness and generalization. Our code is available at: https://github.com/eezkni/SMHDR},
  archive      = {J_IJCV},
  author       = {Ni, Zhangkai and Zhang, Yang and Ren, Kerui and Yang, Wenhan and Wang, Hanli and Kwong, Sam},
  doi          = {10.1007/s11263-025-02504-5},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6896-6911},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic masking with curriculum learning for robust HDR image reconstruction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning extensible series-parallel lookup tables for efficient image super-resolution. <em>IJCV</em>, <em>133</em>(10), 6873-6895. (<a href='https://doi.org/10.1007/s11263-025-02516-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lookup table (LUT) has shown its efficacy in low-level vision tasks due to the valuable characteristics of low computational cost and hardware independence. However, recent attempts to address the problem of single image super-resolution (SISR) with lookup tables are highly constrained by the small receptive field size. Besides, their frameworks of finite-layer lookup tables limit the extension and generalization capacities of the model. In this paper, we propose a framework of series-parallel lookup tables (SPLUT) to alleviate the above issues and achieve efficient image super-resolution. On the one hand, we cascade multiple lookup tables to enlarge the receptive field of each extracted feature vector. On the other hand, we propose a parallel network which includes two branches of cascaded lookup tables which process different components of the input low-resolution images. By doing so, the two branches collaborate with each other and compensate for the precision loss of discretizing input pixels when establishing a lookup table. Since SPLUT uniformly discretizes the input density space for the series-parallel index, the input distortion by sub-optimal index quantizers results in inefficient utilization of the non-linearities capability for each LUT. With LUTs expansion, the performance becomes increasingly constrained due to the cumulative quantization errors. To address this, we propose SPLUT with learnable index quantizers, named LISPLUT, which automatically adjusts index distributions by non-uniformly allocating the quantization thresholds according to the input density distribution with minimal storage and computational overhead. Moreover, we introduce a lightweight bit-level spatial compensation to inherit the computational efficiency of the independent parallel topology and compensate for the quantization drop by enhancing the guidance of the complementary bit-level components. Compared to previous lookup table-based methods, our framework has stronger representation abilities with more flexible architectures. Furthermore, we no longer need interpolation methods which introduce redundant computations so that our method can achieve less energy cost. Extensive experimental results on five popular benchmark datasets show that our method obtains superior SISR performance in a more efficient way. The code is available at https://github.com/zhjy2016/SPLUT .},
  archive      = {J_IJCV},
  author       = {Zhang, Jingyi and Wang, Ziwei and Ma, Cheng and Zhou, Jie and Lu, Jiwen},
  doi          = {10.1007/s11263-025-02516-1},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6873-6895},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning extensible series-parallel lookup tables for efficient image super-resolution},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A likelihood ratio-based approach to segmenting unknown objects. <em>IJCV</em>, <em>133</em>(10), 6860-6872. (<a href='https://doi.org/10.1007/s11263-025-02509-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the Out-of-Distribution (OoD) segmentation task is a prerequisite for perception systems operating in an open-world environment. Large foundational models are frequently used in downstream tasks, however, their potential for OoD remains mostly unexplored. We seek to leverage a large foundational model to achieve robust representation. Outlier supervision is a widely used strategy for improving OoD detection of the existing segmentation networks. However, current approaches for outlier supervision involve retraining parts of the original network, which is typically disruptive to the model’s learned feature representation. Furthermore, retraining becomes infeasible in the case of large foundational models. Our goal is to retrain for outlier segmentation without compromising the strong representation space of the foundational model. To this end, we propose an adaptive, lightweight unknown estimation module (UEM) for outlier supervision that significantly enhances the OoD segmentation performance without affecting the learned feature representation of the original network. UEM learns a distribution for outliers and a generic distribution for known classes. Using the learned distributions, we propose a likelihood-ratio-based outlier scoring function that fuses the confidence of UEM with that of the pixel-wise segmentation inlier network to detect unknown objects. We also propose an objective to optimize this score directly. Our approach achieves a new state-of-the-art across multiple datasets, outperforming the previous best method by 5.74% average precision points while having a lower false-positive rate. Importantly, strong inlier performance remains unaffected. The code and pre-trained models are available at: https://github.com/NazirNayal8/UEM-likelihood-ratio .},
  archive      = {J_IJCV},
  author       = {Nayal, Nazir and Shoeb, Youssef and Güney, Fatma},
  doi          = {10.1007/s11263-025-02509-0},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6860-6872},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A likelihood ratio-based approach to segmenting unknown objects},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of few-shot action recognition. <em>IJCV</em>, <em>133</em>(10), 6832-6859. (<a href='https://doi.org/10.1007/s11263-025-02503-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot action recognition aims to address the high cost and impracticality of manually labeling complex and variable video data in action recognition. It requires accurately classifying human actions in videos using only a few labeled examples per class. Compared to few-shot learning in image scenarios, few-shot action recognition is more challenging due to the intrinsic complexity of video data. Numerous approaches have driven significant advancements in few-shot action recognition, which underscores the need for a comprehensive survey. Unlike early surveys that focus on few-shot image or text classification, we deeply consider the unique challenges of few-shot action recognition. In this survey, we provide a comprehensive review of recent methods and introduce a novel and systematic taxonomy of existing approaches, accompanied by a detailed analysis. We categorize the methods into generative-based and meta-learning frameworks, and further elaborate on the methods within the meta-learning framework, covering aspects: video instance representation, category prototype learning, and generalized video alignment. Additionally, the survey presents the commonly used benchmarks and discusses relevant advanced topics and promising future directions. We hope this survey can serve as a valuable resource for researchers, offering essential guidance to newcomers and stimulating seasoned researchers with fresh insights.},
  archive      = {J_IJCV},
  author       = {Wanyan, Yuyang and Yang, Xiaoshan and Dong, Weiming and Xu, Changsheng},
  doi          = {10.1007/s11263-025-02503-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6832-6859},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A comprehensive review of few-shot action recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generic scene graph generation model with hierarchical prompt learning. <em>IJCV</em>, <em>133</em>(10), 6813-6831. (<a href='https://doi.org/10.1007/s11263-025-02499-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene Graph Generation (SGG) delivers structured knowledge to represent complex scenes and has proven effective in many computer vision tasks. However, traditional SGG models suffer from two limitations that hinder their applicability for higher-level visual tasks: (1) a rigid structure that results in low efficiency and limited flexibility, and (2) biased optimization that results in biased predictions that favor uninformative predicates. To resolve these two issues, we propose GSGG (Generic Scene Graph Generation), a novel, efficient, and flexible SGG model that (1) combines generalized modules to construct top-performance and high-efficiency SGG model and (2) employs a prompt learning-based relation decoder with a novel Hierarchical Prompt (HP) learning method to mitigate biased optimization. HP utilizes the composition of basic prompts constrained to progressively narrowed class groups and encourages the corresponding prompts to focus on the learning of increasingly informative predicates. Extensive evaluations on three SGG benchmarks demonstrate the excellent efficiency and performance of GSGG with HP. We also introduce a novel predicate generalization task with a new benchmark, and experiments on it demonstrate the effectiveness of HP in base-to-novel predicate generalization.},
  archive      = {J_IJCV},
  author       = {Zhu, Xuhan and Xing, Yifei and Wang, Ruiping and Wang, Yaowei and Lan, Xiangyuan},
  doi          = {10.1007/s11263-025-02499-z},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6813-6831},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generic scene graph generation model with hierarchical prompt learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TokenPacker: Efficient visual projector for multimodal LLM. <em>IJCV</em>, <em>133</em>(10), 6794-6812. (<a href='https://doi.org/10.1007/s11263-025-02491-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multimodal large language models (MLLMs), the visual projector is a crucial component that connects the visual encoder with the large language model (LLM). Most current MLLMs adopt a simple multi-layer perceptron (MLP) to preserve visual contexts via direct transformation. However, this approach tends to generate redundant visual tokens, particularly when processing high-resolution images, ultimately reducing the efficiency of MLLMs. Recent efforts to address this issue have employed resamplers or abstractors to reduce token quantity. Unfortunately, these methods often fail to capture finer details, thereby limiting the model’s visual reasoning capabilities. In this work, we introduce TokenPacker, a novel visual projector designed to generate condensed visual tokens through a coarse-to-fine scheme. Initially, we interpolate the visual features into a low-resolution point query that provides an overall visual representation. We then integrate high-resolution, multi-level regional cues using a region-to-point injection module, which enriches the point query with local context. This enhancement effectively transforms the initial query into a more detailed representation suitable for LLM reasoning. Furthermore, we propose a dynamic image slicing scheme to efficiently handle high-resolution images with TokenPacker. Extensive experiments demonstrate that TokenPacker can compress the visual tokens by 75% $$\sim $$ 89%, while maintaining or even improving performance on various benchmarks, achieving significantly higher efficiency. The source codes and models can be found at https://github.com/CircleRadon/TokenPacker .},
  archive      = {J_IJCV},
  author       = {Li, Wentong and Yuan, Yuqian and Liu, Jian and Tang, Dongqi and Wang, Song and Qin, Jie and Zhu, Jianke and Zhang, Lei},
  doi          = {10.1007/s11263-025-02491-7},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6794-6812},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {TokenPacker: Efficient visual projector for multimodal LLM},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust object detection with domain-invariant training and continual test-time adaptation. <em>IJCV</em>, <em>133</em>(10), 6768-6793. (<a href='https://doi.org/10.1007/s11263-025-02465-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world environment can be highly dynamic causing substantial domain shifts. Such real-world domain shifts can span over time with domain changes across multiple domains, manifested into the pertinent content or style changes, or both, where content may refer to underlying image layout and styles are domain-specific such as color and texture. Performance of safety-critical applications, especially robust object detection system in autonomous driving, must adapt to such test-time domain shifts. However, our empirical analysis shows existing domain adaptation and generalization methods fail to fit the domain changes with substantial style or content shifts. In this paper, we first analyze and investigate effective solutions to overcome domain overfitting for robust object detection without the above shortcomings. To simultaneously address temporal and multiple domain shifts, we propose a continual test-time generalizable domain adaptation (CoTGA) method for robust object detection: 1) the domain-invariant training (DIT) module leverages the Normalization Perturbation (NP) method to initialize a style-invariant object detection model, by perturbing the channel statistics of source domain low-level features to synthesize various latent styles. The trained deep model can perceive diverse potential domains and generalizes well even without observations of target domain data in training; 2) the test-time adaptation (TTA) module updates the DIT-trained model online during inference, through the consistency regularization between predictions of the weakly and strongly augmented unlabeled images. TTA addresses the content discrepancies problem of the DIT-initialized generalizable model; 3) the generalizable weights preservation (GWP) module keeps the learned generalizable weights to avoid domain overfitting in generalization across multiple domains. Extensive experiments demonstrate these three modules collaboratively enable a deep model to generalize well under challenging real-world domain shifts.},
  archive      = {J_IJCV},
  author       = {Fan, Qi and Segu, Mattia and Schiele, Bernt and Dai, Dengxin and Tai, Yu-Wing and Tang, Chi-Keung},
  doi          = {10.1007/s11263-025-02465-9},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6768-6793},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust object detection with domain-invariant training and continual test-time adaptation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study on training paradigms for deep supervised hashing. <em>IJCV</em>, <em>133</em>(10), 6729-6767. (<a href='https://doi.org/10.1007/s11263-025-02506-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep supervised hashing have made remarkable achievements in the large-scale image retrieval task. However, the main training paradigms (i.e., pairwise and pointwise) of existing deep supervised hashing methods, which will significantly impact the performance of deep supervised hashing methods under practical retrieval tasks, remain insufficiently explored. Motivated by the critical role of training paradigms in deep supervised hashing and the lack of comprehensive evaluations in this area, we systematically establish the evaluation protocols and conduct an extensive study through 1,833 experiments, yielding 7,332 results across 12 datasets. Our key findings include observations such as: 1) Pointwise hashing methods tend to exhibit higher retrieval accuracy in scenarios with seen-class queries but underperform significantly with unseen-class queries. 2) Pointwise hashing methods show greater robustness with seen-class queries, whereas pairwise hashing methods with soft constraints excel when queries are from unseen classes. 3) The impact of hash code dimensions is minimal on the retrieval performance of pointwise hashing methods but more pronounced for pairwise hashing, primarily due to suboptimal real-valued feature optimization. Code along with training logs for all experiments are open-source and available at https://github.com/aassxun/DSH_Analysis .},
  archive      = {J_IJCV},
  author       = {Shen, Yang and Wang, Peng and Wei, Xiu-Shen and Yao, Yazhou},
  doi          = {10.1007/s11263-025-02506-3},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6729-6767},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An empirical study on training paradigms for deep supervised hashing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implementation and validation of distributed bundle adjustment for super large scale datasets. <em>IJCV</em>, <em>133</em>(10), 6712-6728. (<a href='https://doi.org/10.1007/s11263-025-02505-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a distributed bundle adjustment (DBA) method using the Levenberg-Marquardt (LM) algorithm for super large-scale datasets. Most of the existing methods partition the global map to small ones and conduct bundle adjustment in the submaps. In order to fit the parallel framework, they use approximate solutions instead of the LM algorithm. However, those methods often give sub-optimal results. Different from them, we utilize the LM algorithm to conduct global bundle adjustment where the formation of the reduced camera system (RCS) is actually parallelized and executed in a distributed way. To store the large RCS, we compress it with a block-based sparse matrix compression format (BSMC), which fully exploits its block feature. The BSMC format also enables the distributed storage and updating of the global RCS. The proposed method is extensively evaluated and compared with the state-of-the-art pipelines using both synthetic and real datasets. Preliminary results demonstrate the efficient memory usage and vast scalability of the proposed method compared with the baselines. For the first time, we conducted parallel bundle adjustment using LM algorithm on a real datasets with 1.18 million images and a synthetic dataset with 10 million images (about 500 times that of the state-of-the-art LM-based BA) on a distributed computing system.},
  archive      = {J_IJCV},
  author       = {Zheng, Maoteng and Chen, Nengcheng and Zhu, Junfeng and Zeng, Xiaoru and Qiu, Huanbin and Jiang, Yuyao and Lu, Xingyue and Qu, Hao},
  doi          = {10.1007/s11263-025-02505-4},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6712-6728},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Implementation and validation of distributed bundle adjustment for super large scale datasets},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting lightweight hierarchical ViT and dynamic framework for efficient visual tracking. <em>IJCV</em>, <em>133</em>(10), 6689-6711. (<a href='https://doi.org/10.1007/s11263-025-02500-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based visual trackers have demonstrated significant advancements due to their powerful modeling capabilities. However, their practicality is limited on resource-constrained devices because of their slow processing speeds. To address this challenge, we present HiT, a novel family of efficient tracking models that achieve high performance while maintaining fast operation across various devices. The core innovation of HiT lies in its Bridge Module, which connects lightweight transformers to the tracking framework, enhancing feature representation quality. Additionally, we introduce a dual-image position encoding approach to effectively encode spatial information. HiT achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark, outperforming all previous efficient trackers. Building on HiT, we propose DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by selecting routes with varying computational requirements. DyHiT uses search area features extracted by the backbone network and inputs them into an efficient dynamic router to classify tracking scenarios. Based on the classification, DyHiT applies a divide-and-conquer strategy, selecting appropriate routes to achieve a superior trade-off between accuracy and speed. The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while maintaining an AUC of 62.4% on LaSOT. Furthermore, we introduce a training-free acceleration method based on the dynamic routing architecture of DyHiT. This method significantly improves the execution speed of various high-performance trackers without sacrificing accuracy. For instance, our acceleration method enables the state-of-the-art tracker SeqTrack-B256 to achieve a $$2.68\times $$ speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of 69.9% on the LaSOT. Codes, models, and results are available at https://github.com/kangben258/HiT .},
  archive      = {J_IJCV},
  author       = {Kang, Ben and Chen, Xin and Zhao, Jie and Bo, Chunjuan and Wang, Dong and Lu, Huchuan},
  doi          = {10.1007/s11263-025-02500-9},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6689-6711},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploiting lightweight hierarchical ViT and dynamic framework for efficient visual tracking},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From forest to zoo: Great ape behavior recognition with ChimpBehave. <em>IJCV</em>, <em>133</em>(10), 6668-6688. (<a href='https://doi.org/10.1007/s11263-025-02484-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the significant challenge of recognizing behaviors in non-human primates, specifically focusing on chimpanzees. Automated behavior recognition is crucial for both conservation efforts and the advancement of behavioral research. However, it is often hindered by the labor-intensive process of manual video annotation. Despite the availability of large-scale animal behavior datasets, effectively applying machine learning models across varied environmental settings remains a critical challenge due to the variability in data collection contexts and the specificity of annotations. In this paper, we introduce ChimpBehave, a novel dataset comprising over 2 h and 20 min of video (approximately 215,000 frames) of zoo-housed chimpanzees, annotated with bounding boxes and fine-grained locomotive behavior labels. Uniquely, ChimpBehave aligns its behavior classes with those in PanAf, an existing dataset collected in distinct visual environments, enabling the study of cross-dataset generalization - where models are trained on one dataset and tested on another with differing data distributions. We benchmark ChimpBehave using state-of-the-art video-based and skeleton-based action recognition models, establishing performance baselines for both within-dataset and cross-dataset evaluations. Our results highlight the strengths and limitations of different model architectures, providing insights into the application of automated behavior recognition across diverse visual settings. The dataset, models, and code can be accessed at: https://github.com/MitchFuchs/ChimpBehave},
  archive      = {J_IJCV},
  author       = {Fuchs, Michael and Genty, Emilie and Bangerter, Adrian and Zuberbühler, Klaus and Odobez, Jean-Marc and Cotofrei, Paul},
  doi          = {10.1007/s11263-025-02484-6},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6668-6688},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {From forest to zoo: Great ape behavior recognition with ChimpBehave},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision generalist model: A survey. <em>IJCV</em>, <em>133</em>(10), 6639-6667. (<a href='https://doi.org/10.1007/s11263-025-02502-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have witnessed the great success of the generalist model in natural language processing. The generalist model is a general framework trained with massive data and is able to process various downstream tasks simultaneously. Encouraged by their impressive performance, an increasing number of researchers are venturing into the realm of applying these models to computer vision tasks. However, the inputs and outputs of vision tasks are more diverse, and it is difficult to summarize them as a unified representation. In this paper, we provide a comprehensive overview of the vision generalist models, delving into their characteristics and capabilities within the field. First, we review the background, including the datasets, tasks, and benchmarks. Then, we dig into the design of frameworks that have been proposed in existing research, while also introducing the techniques employed to enhance their performance. To better help the researchers comprehend the area, we take a brief excursion into related domains, shedding light on their interconnections and potential synergies. To conclude, we provide some real-world application scenarios, undertake a thorough examination of the persistent challenges, and offer insights into possible directions for future research endeavors.},
  archive      = {J_IJCV},
  author       = {Wang, Ziyi and Rao, Yongming and Sun, Shuofeng and Liu, Xinrun and Wei, Yi and Yu, Xumin and Liu, Zuyan and Wang, Yanbo and Liu, Hongmin and Zhou, Jie and Lu, Jiwen},
  doi          = {10.1007/s11263-025-02502-7},
  journal      = {International Journal of Computer Vision},
  month        = {10},
  number       = {10},
  pages        = {6639-6667},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Vision generalist model: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Investigating self-supervised methods for label-efficient learning. <em>IJCV</em>, <em>133</em>(9), 6638. (<a href='https://doi.org/10.1007/s11263-025-02455-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Nandam, Srinivasa Rao and Atito, Sara and Feng, Zhenhua and Kittler, Josef and Awais, Muhammad},
  doi          = {10.1007/s11263-025-02455-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6638},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Investigating self-supervised methods for label-efficient learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Parameter efficient fine-tuning for multi-modal generative vision models with möbius-inspired transformation. <em>IJCV</em>, <em>133</em>(9), 6637. (<a href='https://doi.org/10.1007/s11263-025-02456-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Duan, Haoran and Shao, Shuai and Zhai, Bing and Shah, Tejal and Han, Jungong and Ranjan, Rajiv},
  doi          = {10.1007/s11263-025-02456-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6637},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Parameter efficient fine-tuning for multi-modal generative vision models with möbius-inspired transformation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on DAGM GCPR 2023. <em>IJCV</em>, <em>133</em>(9), 6636. (<a href='https://doi.org/10.1007/s11263-025-02490-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-025-02490-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6636},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on DAGM GCPR 2023},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OCCO: LVM-guided infrared and visible image fusion framework based on object-aware and contextual contrastive learning. <em>IJCV</em>, <em>133</em>(9), 6611-6635. (<a href='https://doi.org/10.1007/s11263-025-02507-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion is a crucial technique in the field of computer vision, and its goal is to generate high-quality fused images and improve the performance of downstream tasks. However, existing fusion methods struggle to balance these two factors. Achieving high quality in fused images may result in lower performance in downstream visual tasks, and vice versa. To address this drawback, a novel LVM (large vision model)-guided fusion framework with Object-aware and Contextual COntrastive learning is proposed, termed as OCCO. The pre-trained LVM is utilized to provide semantic guidance, allowing the network to focus solely on fusion tasks while emphasizing learning salient semantic features in form of contrastive learning. Additionally, a novel feature interaction fusion network is also designed to resolve information conflicts in fusion images caused by modality differences. By learning the distinction between positive samples and negative samples in the latent feature space (contextual space), the integrity of target information in fused image is improved, thereby benefiting downstream performance. Finally, compared with eight state-of-the-art methods on four datasets, the effectiveness of the proposed method is validated, and exceptional performance is also demonstrated on downstream visual task.},
  archive      = {J_IJCV},
  author       = {Li, Hui and Bian, Congcong and Zhang, Zeyang and Song, Xiaoning and Li, Xi and Wu, Xiao-Jun},
  doi          = {10.1007/s11263-025-02507-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6611-6635},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {OCCO: LVM-guided infrared and visible image fusion framework based on object-aware and contextual contrastive learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unaligned RGB guided hyperspectral image super-resolution with spatial-spectral concordance. <em>IJCV</em>, <em>133</em>(9), 6590-6610. (<a href='https://doi.org/10.1007/s11263-025-02466-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) super-resolution (SR) aims to improve the spatial resolution, yet its performance is often limited at high-resolution ratios. The recent adoption of high-resolution reference images for super-resolution is driven by the poor spatial detail found in low-resolution HSIs, presenting it as a favorable method. However, these approaches cannot effectively utilize information from the reference image, due to the inaccuracy of alignment and its inadequate interaction between alignment and fusion modules. In this paper, we introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution (SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the issues of inaccurate alignment and poor interactivity of the previous approaches. Specifically, to ensure spatial concordance, i.e., align images more accurately across resolutions and refine textures, we construct a Two-Stage Image Alignment (TSIA) with a synthetic generation pipeline in the image alignment module, where the fine-tuned optical flow model can produce a more accurate optical flow in the first stage and warp model can refine damaged textures in the second stage. To enhance the interaction between alignment and fusion modules and ensure spectral concordance during reconstruction, we propose a Feature Aggregation (FA) module and an Attention Fusion (AF) module. In the feature aggregation module, we introduce an Iterative Deformable Feature Aggregation (IDFA) block to achieve significant feature matching and texture aggregation with the fusion multi-scale results guidance, iteratively generating learnable offset. Besides, we introduce two basic spectral-wise attention blocks in the attention fusion module to model the inter-spectra interactions. Extensive experiments on three natural or remote-sensing datasets show that our method outperforms state-of-the-art approaches on both quantitative and qualitative evaluations. Our code is publicly available to the community ( https://github.com/BITYKZhang/SSC-HSR ).},
  archive      = {J_IJCV},
  author       = {Zhang, Yingkai and Lai, Zeqiang and Zhang, Tao and Fu, Ying and Zhou, Chenghu},
  doi          = {10.1007/s11263-025-02466-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6590-6610},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unaligned RGB guided hyperspectral image super-resolution with spatial-spectral concordance},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos. <em>IJCV</em>, <em>133</em>(9), 6578-6589. (<a href='https://doi.org/10.1007/s11263-025-02493-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using unmanned aerial vehicles (UAVs) to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding the collective behavior of primates. Previous studies have demonstrated the feasibility of automating primate behavior classification from video data, but these studies have been carried out in captivity or from ground-based cameras. However, to understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can be seen in relation to the natural environment in which ecological decisions are made. To tackle this challenge, this study presents a novel dataset for baboon detection, tracking, and behavior recognition from drone videos where troops are observed on-the-move in their natural environment as they move to and from their sleeping sites. Videos were captured from drones at Mpala Research Centre, a research station located in Laikipia County, in central Kenya. The baboon detection dataset was created by manually annotating all baboons in drone videos with bounding boxes. A tiling method was subsequently applied to create a pyramid of images at various scales from the original 5.3K resolution images, resulting in approximately 30K images used for baboon detection. The baboon tracking dataset is derived from the baboon detection dataset, where bounding boxes are consistently assigned the same ID throughout the video. This process resulted in half an hour of dense tracking data. The baboon behavior recognition dataset was generated by converting tracks into mini-scenes, a video subregion centered on each animal. These mini-scenes were annotated with 12 distinct behavior types and one additional category for occlusion, resulting in over 20 hours of data. Benchmark results show mean average precision (mAP) of 92.62% for the YOLOv8-X detection model, multiple object tracking precision (MOTP) of 87.22% for the DeepSORT tracking algorithm, and micro top-1 accuracy of 64.89% for the X3D behavior recognition model. Using deep learning to rapidly and accurately classify wildlife behavior from drone footage facilitates non-invasive data collection on behavior enabling the behavior of a whole group to be systematically and accurately recorded. The dataset can be accessed at https://baboonland.xyz .},
  archive      = {J_IJCV},
  author       = {Duporge, Isla and Kholiavchenko, Maksim and Harel, Roi and Wolf, Scott and Rubenstein, Daniel I and Crofoot, Margaret C and Berger-Wolf, Tanya and Lee, Stephen J and Barreau, Julie and Kline, Jenna and Ramirez, Michelle and Stewart, Charles V},
  doi          = {10.1007/s11263-025-02493-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6578-6589},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {BaboonLand dataset: Tracking primates in the wild and automating behaviour recognition from drone videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised salient object detection with oversize bounding box. <em>IJCV</em>, <em>133</em>(9), 6558-6577. (<a href='https://doi.org/10.1007/s11263-025-02482-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to laborious pixel-level annotations, scribbles, bounding boxes, and points are much more efficient in salient object detection (SOD). However, the annotation cost of these forms linearly increases with the number of salient objects in the image, which is not ideal for real-world scenarios. To address this issue, we propose a novel annotation form called oversize bounding box (OBB), i.e., a box that encompasses all salient objects without the need for tight enclosure. It has two characteristics: (1) All pixels outside the box are from the background. (2) A subset of the pixels inside the box belongs to salient objects. Therefore, the core issue is how to highlight integral and accurate object regions as pseudo-labels. Inspired by the powerful visual understanding and vision-language correlations demonstrated by large multimodal models, we devise the first multimodal model-based pseudo-label generation method for SOD. It utilizes MiniGPT-4 to generate descriptions of salient objects as text prompts for CLIP, combined with the CAM-based technique and proposed refinement algorithm based on the multi-head self-attention and superpixel to activate the object regions. Then we use OBB to further correct the activation regions based on its two characteristics. Given the potential errors of pseudo-labels within the box, we propose a center-pixel-based cross entropy loss, as activated pixels closer to the center are generally more reliable. Moreover, we establish an OBB-supervised training dataset by relabeling the DUTS dataset. Extensive experiments on six benchmarks demonstrate that our method achieves the state-of-the-art performance.},
  archive      = {J_IJCV},
  author       = {Wu, Zhihao and Xu, Yong and Yang, Jian and Zhang, David},
  doi          = {10.1007/s11263-025-02482-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6558-6577},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Weakly supervised salient object detection with oversize bounding box},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit diffusion models for continuous super-resolution. <em>IJCV</em>, <em>133</em>(9), 6535-6557. (<a href='https://doi.org/10.1007/s11263-025-02462-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) has attracted increasing attention due to its widespread applications. However, current SR methods generally suffer from over-smoothing and artifacts, and most of them work only with fixed magnifications. To address these problems, this paper introduces an Implicit Diffusion Model (IDM) for high-fidelity continuous image super-resolution. IDM integrates an implicit neural representation and a denoising diffusion model in a unified end-to-end framework, where the implicit neural representation is adopted in the decoding process to learn continuous-resolution representation. Moreover, we design a scale-adaptive conditioning mechanism that consists of a low-resolution (LR) conditioning network and a scaling factor. The LR conditioning network adopts a parallel architecture to provide multi-resolution LR conditions for the denoising model. The scaling factor further regulates the resolution and accordingly modulates the proportion of the LR information and generated features in the final output, which enables the model to accommodate the continuous-resolution requirement. Furthermore, we accelerate the inference process by adjusting the denoising equation and employing post-training quantization to compress the learned denoising network in a training-free manner. Extensive experiments on six benchmark datasets validate the effectiveness of our IDM and demonstrate its superior performance over prior arts. The source code is available at https://github.com/Ree1s/IDM .},
  archive      = {J_IJCV},
  author       = {Liu, Xuhui and Gao, Sicheng and Zeng, Bohan and Zhang, Luping and Wang, Tian and Liu, Jianzhuang and Zhang, Baochang},
  doi          = {10.1007/s11263-025-02462-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6535-6557},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Implicit diffusion models for continuous super-resolution},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal prompt alignment with fine-grained LLM knowledge for unsupervised domain adaptation. <em>IJCV</em>, <em>133</em>(9), 6513-6534. (<a href='https://doi.org/10.1007/s11263-025-02497-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge from source domain to target domain, which always struggles with severe domain shift between the data. The recent progress on visual-language models (VLMs) have provided a promising way to address UDA, leveraging the knowledge from text for more guided adaptation. However, directly deploying such models on downstream UDA tasks with conventional prompt learning can be challenging, which neglects the diversity of visual samples and can cause mis-alignment between modalities, thus lacking flexibility to adapt both modalities dynamically and limiting the cross-domain knowledge transfer. In this paper, we propose an innovative domain-invariant prompt learning method to align the prompts from different modalities. Specifically, we first introduce a hybrid-modality guided prompting module that leverages the prompted multi-modal representation to synergistically help uni-modal learning, thus mutually aligning visual and textual embeddings. We also take advantage of the the category-wise attributes derived from Large Language Model (LLM) to incorporate fine-grained semantic knowledge into prompt learning, ensuring better discrimination among different classes. Besides, to further minimize domain discrepancy, we propose to fuse the textual prototypes with the visual prototypes from each domain, thus to make the input attend to overall domain distribution, which effectively integrates self-enhanced and cross-domain features into the model prediction. With our framework, the two modalities can be mutually promoted to better enhance the adaptation of VLMs for UDA. Experiments on several different benchmarks demonstrate the superiority of our method over previous approaches.},
  archive      = {J_IJCV},
  author       = {Xing, Bowei and Ying, Xianghua and Wang, Ruibin and Guo, Ruohao},
  doi          = {10.1007/s11263-025-02497-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6513-6534},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-modal prompt alignment with fine-grained LLM knowledge for unsupervised domain adaptation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ricci curvature tensor-based volumetric segmentation. <em>IJCV</em>, <em>133</em>(9), 6491-6512. (<a href='https://doi.org/10.1007/s11263-025-02492-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing level set models employ regularization based only on gradient information, 1D curvature or 2D curvature. For 3D image segmentation, however, an appropriate curvature-based regularization should involve a well-defined 3D curvature energy. This is the first paper to introduce a regularization energy that incorporates 3D scalar curvature for 3D image segmentation, inspired by the Einstein-Hilbert functional. To derive its Euler-Lagrange equation, we employ a two-step gradient descent strategy, alternately updating the level set function and its gradient. The paper also establishes the existence and uniqueness of the viscosity solution for the proposed model. Experimental results demonstrate that our proposed model outperforms other state-of-the-art models in 3D image segmentation.},
  archive      = {J_IJCV},
  author       = {Huang, Jisui and Chen, Ke and Alpers, Andreas and Lei, Na},
  doi          = {10.1007/s11263-025-02492-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6491-6512},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Ricci curvature tensor-based volumetric segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $$E^{3}DGE$$: Self-supervised geometry-aware encoder for style-based 3D GAN inversion. <em>IJCV</em>, <em>133</em>(9), 6473-6490. (<a href='https://doi.org/10.1007/s11263-025-02496-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {StyleGAN has excelled in 2D face reconstruction and semantic editing, but the extension to 3D lacks a generic inversion framework, limiting its applications in 3D reconstruction. In this paper, we address the challenge of 3D GAN inversion, focusing on predicting a latent code from a single 2D image to faithfully recover 3D shapes and textures. The inherent ill-posed nature of the problem, coupled with the limited capacity of global latent codes, presents significant challenges. To overcome these challenges, we introduce an efficient self-training scheme that does not rely on real-world 2D-3D pairs but instead utilizes proxy samples generated from a 3D GAN. Additionally, our approach goes beyond the global latent code by enhancing the generation network with a local branch. This branch incorporates pixel-aligned features to accurately reconstruct texture details. Furthermore, we introduce a novel pipeline for 3D view-consistent editing. The efficacy of our method is validated on two representative 3D GANs, namely StyleSDF and EG3D. Through extensive experiments, we demonstrate that our approach consistently outperforms state-of-the-art inversion methods, delivering superior quality in both shape and texture reconstruction.},
  archive      = {J_IJCV},
  author       = {Lan, Yushi and Meng, Xuyi and Yang, Shuai and Loy, Chen Change and Dai, Bo},
  doi          = {10.1007/s11263-025-02496-2},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6473-6490},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {$$E^{3}DGE$$: Self-supervised geometry-aware encoder for style-based 3D GAN inversion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StimuVAR: Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models. <em>IJCV</em>, <em>133</em>(9), 6456-6472. (<a href='https://doi.org/10.1007/s11263-025-02495-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting and reasoning how a video would make a human feel is crucial for developing socially intelligent systems. Although Multimodal Large Language Models (MLLMs) have shown impressive video understanding capabilities, they tend to focus more on the semantic content of videos, often overlooking emotional stimuli. Hence, most existing MLLMs fall short in estimating viewers’ emotional reactions and providing plausible explanations. To address this issue, we propose StimuVAR, a spatiotemporal Stimuli-aware framework for Video Affective Reasoning (VAR) with MLLMs. StimuVAR incorporates a two-level stimuli-aware mechanism: frame-level awareness and token-level awareness. Frame-level awareness involves sampling video frames with events that are most likely to evoke viewers’ emotions. Token-level awareness performs tube selection in the token space to make the MLLM concentrate on emotion-triggered spatiotemporal regions. Furthermore, we create VAR instruction data to perform affective training, steering MLLMs’ reasoning strengths towards emotional focus and thereby enhancing their affective reasoning ability. To thoroughly assess the effectiveness of VAR, we provide a comprehensive evaluation protocol with extensive metrics. StimuVAR is the first MLLM-based method for viewer-centered VAR. Experiments demonstrate its superiority in understanding viewers’ emotional responses to videos and providing coherent and insightful explanations.},
  archive      = {J_IJCV},
  author       = {Guo, Yuxiang and Siddiqui, Faizan and Zhao, Yang and Chellappa, Rama and Lo, Shao-Yuan},
  doi          = {10.1007/s11263-025-02495-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6456-6472},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {StimuVAR: Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EventEgo3D++: 3D human motion capture from a head-mounted event camera. <em>IJCV</em>, <em>133</em>(9), 6432-6455. (<a href='https://doi.org/10.1007/s11263-025-02489-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular egocentric 3D human motion capture remains a significant challenge, particularly under conditions of low lighting and fast movements, which are common in head-mounted device applications. Existing methods that rely on RGB cameras often fail under these conditions. To address these limitations, we introduce EventEgo3D++, the first approach that leverages a monocular event camera with a fisheye lens for 3D human motion capture. Event cameras excel in high-speed scenarios and varying illumination due to their high temporal resolution, providing reliable cues for accurate 3D human motion capture. EventEgo3D++ leverages the LNES representation of event streams to enable precise 3D reconstructions. We have also developed a mobile head-mounted device (HMD) prototype equipped with an event camera, capturing a comprehensive dataset that includes real event observations from both controlled studio environments and in-the-wild settings, in addition to a synthetic dataset. Additionally, to provide a more holistic dataset, we include allocentric RGB streams that offer different perspectives of the HMD wearer, along with their corresponding SMPL body model. Our experiments demonstrate that EventEgo3D++ achieves superior 3D accuracy and robustness compared to existing solutions, even in challenging conditions. Moreover, our method supports real-time 3D pose updates at a rate of 140Hz. This work is an extension of the EventEgo3D approach (CVPR 2024) and further advances the state of the art in egocentric 3D human motion capture. For more details, visit the project page at https://eventego3d.mpi-inf.mpg.de .},
  archive      = {J_IJCV},
  author       = {Millerdurai, Christen and Akada, Hiroyasu and Wang, Jian and Luvizon, Diogo and Pagani, Alain and Stricker, Didier and Theobalt, Christian and Golyanik, Vladislav},
  doi          = {10.1007/s11263-025-02489-1},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6432-6455},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {EventEgo3D++: 3D human motion capture from a head-mounted event camera},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast sampling through the reuse of attention maps in diffusion models. <em>IJCV</em>, <em>133</em>(9), 6422-6431. (<a href='https://doi.org/10.1007/s11263-025-02463-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image diffusion models have demonstrated unprecedented capabilities for flexible and realistic image synthesis. Nevertheless, these models rely on a time-consuming sampling procedure, which has motivated attempts to reduce their latency. When improving efficiency, researchers often use the original diffusion model to train an additional network designed specifically for fast image generation. In contrast, our approach seeks to reduce latency directly, without any retraining, fine-tuning, or knowledge distillation. In particular, we find the repeated calculation of attention maps to be costly yet redundant, and instead suggest reusing them during sampling. Our specific reuse strategies are based on ODE theory, which implies that the later a map is reused, the smaller the distortion in the final image. We empirically compare our reuse strategies with few-step sampling procedures of comparable latency, finding that reuse generates images that are closer to those produced by the original high-latency diffusion model.},
  archive      = {J_IJCV},
  author       = {Hunter, Rosco and Dudziak, Łukasz and Abdelfattah, Mohamed S. and Mehrotra, Abhinav and Bhattacharya, Sourav and Wen, Hongkai},
  doi          = {10.1007/s11263-025-02463-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6422-6431},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fast sampling through the reuse of attention maps in diffusion models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RML++: Regroup median loss for combating label noise. <em>IJCV</em>, <em>133</em>(9), 6400-6421. (<a href='https://doi.org/10.1007/s11263-025-02494-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep neural networks (DNNs) typically necessitates large-scale, high-quality annotated datasets. However, due to the inherent challenges of precisely annotating vast numbers of training samples, label noise—characterized by potentially erroneous annotations—is common yet detrimental in practice. Currently, to combat the negative impacts of label noise, mainstream studies follow a pipeline that begins with data sampling and is followed by loss correction. Data sampling aims to partition the original training dataset into clean and noisy subsets, but it often suffers from biased sampling that can mislead models. Additionally, loss correction typically requires knowledge of the noise rate as a priori information, of which the precise estimation can be challenging. To this end, we propose a novel method, Regroup Median Loss Plus Plus (RML++), that addresses both of the previous drawbacks. Specifically, the training dataset is partitioned into clean and noisy subsets using a newly designed separation approach, which synergistically combines prediction consistency with an adaptive threshold to ensure a reliable sampling. Moreover, to ensure the noisy subsets can be robustly learned by models, we suggest to estimate the losses of noisy training samples by utilizing the same-class samples from the clean subset. Subsequently, the proposed method corrects the labels of noisy samples based on the model predictions with the regularization of RML++. Compared to state-of-the-art (SOTA) methods, RML++ achieves significant improvements on both synthetic and challenging real-world datasets. The source code is available at https://github.com/Feng-peng-Li/RML-Extension .},
  archive      = {J_IJCV},
  author       = {Li, Fengpeng and Li, Kemou and Wang, Qizhou and Han, Bo and Tian, Jinyu and Zhou, Jiantao},
  doi          = {10.1007/s11263-025-02494-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6400-6421},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RML++: Regroup median loss for combating label noise},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A physics-informed deep learning deformable medical image registration method based on neural ODEs. <em>IJCV</em>, <em>133</em>(9), 6374-6399. (<a href='https://doi.org/10.1007/s11263-025-02476-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An unsupervised machine learning method is introduced to align medical images in the context of the large deformation elasticity coupled with growth and remodeling biophysics. The technique, which stems from the principle of minimum potential energy in solid mechanics, consists of two steps: Firstly, in the predictor step, the geometric registration is achieved by minimizing a loss function composed of a dissimilarity measure and a regularizing term. Secondly, the physics of the problem, including the equilibrium equations along with growth mechanics, are enforced in a corrector step by minimizing the potential energy corresponding to a Dirichlet problem, where the predictor solution defines the boundary condition and is maintained by distance functions. The features of the new solution procedure, as well as the nature of the registration problem, are highlighted by considering several examples. In particular, registration problems containing large non-uniform deformations caused by extension, shearing, and bending of multiply-connected regions are used as benchmarks. In addition, we analyzed a benchmark biological example (registration for brain data) to showcase that the new deep learning method competes with available methods in the literature. We then applied the method to various datasets. First, we analyze the regrowth of the zebrafish embryonic fin from confocal imaging data. Next, we evaluate the quality of the solution procedure for two examples related to the brain. For one, we apply the new method for 3D image registration of longitudinal magnetic resonance images of the brain to assess cerebral atrophy, where a first-order ODE describes the volume loss mechanism. For the other, we explore cortical expansion during early fetal brain development by coupling the elastic deformation with morphogenetic growth dynamics. The method and examples show the ability of our framework to attain high-quality registration and, concurrently, solve large deformation elasticity balance equations and growth and remodeling dynamics.},
  archive      = {J_IJCV},
  author       = {Amiri-Hezaveh, Amirhossein and Tan, Shelly and Deng, Qing and Umulis, David and Cunniff, Lauren and Weickenmeier, Johannes and Buganza Tepole, Adrian},
  doi          = {10.1007/s11263-025-02476-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6374-6399},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A physics-informed deep learning deformable medical image registration method based on neural ODEs},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time neural radiance talking portrait synthesis via audio-spatial decomposition. <em>IJCV</em>, <em>133</em>(9), 6362-6373. (<a href='https://doi.org/10.1007/s11263-025-02481-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Neural Radiance Fields (NeRF) have been successful in high-fidelity 3D modeling of talking portraits. However, slow training and inference speed have obstructed their potential usage. This paper proposes an efficient NeRF-based framework, which enables faster convergence and real-time synthesizing of stable talking portraits, by utilizing the recent success of grid-based NeRF. This is accomplished by decomposing the inherently high-dimensional talking portrait representation into three low-dimensional feature grids. Specifically, a Decomposed Audio-Spatial Encoding Module models the dynamic head with a 3D spatial grid and a 2D audio grid, where audio dynamics are modeled in a spatial-dependent manner to avoid undesirable flickering. The torso is handled with another 2D grid in a lightweight Pseudo-3D Deformable Module. Extensive experiments demonstrate that our method can generate realistic and audio-lips synchronized talking portrait videos, while also being highly efficient. Our project page is available at https://me.kiui.moe/radnerf/ .},
  archive      = {J_IJCV},
  author       = {Tang, Jiaxiang and Wang, Kaisiyuan and Zhou, Hang and Chen, Xiaokang and He, Dongliang and Hu, Tianshu and Liu, Jingtuo and Liu, Ziwei and Zeng, Gang and Wang, Jingdong},
  doi          = {10.1007/s11263-025-02481-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6362-6373},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Real-time neural radiance talking portrait synthesis via audio-spatial decomposition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skim then focus: Integrating contextual and fine-grained views for repetitive action counting. <em>IJCV</em>, <em>133</em>(9), 6347-6361. (<a href='https://doi.org/10.1007/s11263-025-02471-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to action counting is accurately locating each video’s repetitive actions. Instead of estimating the probability of each frame belonging to an action directly, we propose a dual-branch network, i.e., SkimFocusNet, working in a two-step manner. The model draws inspiration from empirical observations indicating that humans initially engage in coarse skimming of entire sequences to quickly locate potential target action frames and grasp general motion patterns. This is followed by finer, frame-by-frame focusing to precisely determine whether the located frames align with the target actions. Specifically, SkimFocusNet incorporates a skim branch and a focus branch. The skim branch scans the global contextual information throughout the sequence to identify potential target action for guidance. Subsequently, the focus branch utilizes the guidance to diligently identify repetitive actions using a long-short adaptive guidance (LSAG) block. Additionally, we have observed that videos in existing datasets often feature only one type of repetitive action, which inadequately represents real-world scenarios. To more accurately describe real-life situations, we establish the Multi-RepCount dataset, which includes videos containing multiple repetitive motions. On Multi-RepCount, our SkimFoucsNet can perform specified action counting, that is, to enable counting a particular action type by referencing an exemplary video. This capability substantially exhibits our method’s robustness, particularly in accurately performing action counting despite the presence of interfering actions. Extensive experiments demonstrate that SkimFocusNet achieves state-of-the-art performances with significant improvements. We also conduct a thorough ablation study to evaluate the network components. The source code will be published upon acceptance https://github.com/isotopezzq/SkimFocusNet .},
  archive      = {J_IJCV},
  author       = {Zhao, Zhengqi and Huang, Xiaohu and Zhou, Hao and Yao, Kun and Ding, Errui and Wang, Jingdong and Wang, Xinggang and Liu, Wenyu and Bin, Feng},
  doi          = {10.1007/s11263-025-02471-x},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6347-6361},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Skim then focus: Integrating contextual and fine-grained views for repetitive action counting},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DogRecon: Canine prior-guided animatable 3D gaussian dog reconstruction from a single image. <em>IJCV</em>, <em>133</em>(9), 6332-6346. (<a href='https://doi.org/10.1007/s11263-025-02485-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle animatable 3D dog reconstruction from a single image, noting the overlooked potential of animals. Particularly, we focus on dogs, emphasizing their intrinsic characteristics that complicate 3D observation. First, the considerable variation in shapes across breeds presents a complexity for modeling. Additionally, the nature of quadrupeds leads to frequent joint occlusions compared to humans. These challenges make 3D reconstruction from 2D observations difficult, and it becomes dramatically harder when constrained to a single image. To address these challenges, our insight is to combine the acquisition of appearance from generative models, without additional data, with geometric guidance provided by a parametric representation, aiming to achieve complete geometry. To this end, we present DogRecon, our framework consists of two key components: Canine-centric novel view synthesis with canine prior for multi-view generation of dog and a reliable sampling weight strategy with Gaussian Splatting for animatable 3D dog reconstruction. Extensive experiments on the GART, DFA, and internet-sourced datasets confirm our framework has state-of-the-art performance in image-to-3D generation and comparable performance in animatable 3D reconstruction. Additionally, we demonstrate novel pose animation and text-to-3D dog reconstruction as applications. Project page: https://vision3d-lab.github.io/dogrecon/},
  archive      = {J_IJCV},
  author       = {Cho, Gyeongsu and Kang, Changwoo and Soon, Donghyeon and Joo, Kyungdon},
  doi          = {10.1007/s11263-025-02485-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6332-6346},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DogRecon: Canine prior-guided animatable 3D gaussian dog reconstruction from a single image},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thread counting in plain weave for old paintings using regression deep learning models. <em>IJCV</em>, <em>133</em>(9), 6316-6331. (<a href='https://doi.org/10.1007/s11263-025-02473-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel algorithm designed to improve thread density estimation in canvas analysis. Our approach incorporates three major contributions. First, we eliminate the need for post-segmentation processing by integrating regression techniques, enabling the deep learning (DL) model to directly compute thread density. This does not only reduce computational time but also shifts the training focus from locating crossing points to minimizing thread counting errors, thereby enhancing accuracy. We develop and rigorously evaluate various models, selecting the one with optimal performance through a hyperparameter search. Second, we refine the data generation process by dynamically adjusting filter lengths based on initial thread density estimates and incorporating equalization. We also enhance data augmentation. Third, we implement semi-supervised training to expand the dataset and fine-tune model weights. This involves incorporating new inputs into the training set when both the DL model and Fourier transform yield similar density estimates for new paintings. Our proposed algorithm demonstrates superior performance in thread density error reduction and operational efficiency compared to previous DL segmentation solutions for masterpieces from Ribera, Velázquez, or Poussin. Additionally, it has been effectively applied to identify fabric matches between canvases attributed to different authors, showcasing its practical applicability in art analysis.},
  archive      = {J_IJCV},
  author       = {Delgado, Antonio and Murillo-Fuentes, Juan José and Alba-Carcelén, Laura},
  doi          = {10.1007/s11263-025-02473-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6316-6331},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Thread counting in plain weave for old paintings using regression deep learning models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). About time: Advances, challenges, and outlooks of action understanding. <em>IJCV</em>, <em>133</em>(9), 6251-6315. (<a href='https://doi.org/10.1007/s11263-025-02478-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have witnessed impressive advances in video action understanding. Increased dataset sizes, variability, and computation availability have enabled leaps in performance and task diversification. Current systems can provide coarse- and fine-grained descriptions of video scenes, extract segments corresponding to queries, synthesize unobserved parts of videos, and predict context across multiple modalities. This survey comprehensively reviews advances in uni- and multi-modal action understanding across a range of tasks. We focus on prevalent challenges, overview widely adopted datasets, and survey seminal works with an emphasis on recent advances. We broadly distinguish between three temporal scopes: (1) recognition tasks of actions observed in full, (2) prediction tasks for ongoing partially observed actions, and (3) forecasting tasks for subsequent unobserved action(s). This division allows us to identify specific action modeling and video representation challenges. Finally, we outline future directions to address current shortcomings.},
  archive      = {J_IJCV},
  author       = {Stergiou, Alexandros and Poppe, Ronald},
  doi          = {10.1007/s11263-025-02478-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6251-6315},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {About time: Advances, challenges, and outlooks of action understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for text recognition: A critical survey. <em>IJCV</em>, <em>133</em>(9), 6221-6250. (<a href='https://doi.org/10.1007/s11263-025-02487-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text Recognition (TR) refers to the research area that focuses on retrieving textual information from images, a topic that has seen significant advancements in the last decade due to the use of Deep Neural Networks (DNN). However, these solutions often necessitate vast amounts of manually labeled or synthetic data. Addressing this challenge, Self-Supervised Learning (SSL) has gained attention by utilizing large datasets of unlabeled data to train DNN, thereby generating meaningful and robust representations. Although SSL was initially overlooked in TR because of its unique characteristics, recent years have witnessed a surge in the development of SSL methods specifically for this field. This rapid development, however, has led to many methods being explored independently, without taking previous efforts in methodology or comparison into account, thereby hindering progress in the field of research. This paper, therefore, seeks to consolidate the use of SSL in the field of TR, offering a critical and comprehensive overview of the current state of the art. We will review and analyze the existing methods, compare their results, and highlight inconsistencies in the current literature. This thorough analysis aims to provide general insights into the field, propose standardizations, identify new research directions, and foster its proper development.},
  archive      = {J_IJCV},
  author       = {Penarrubia, Carlos and Valero-Mas, Jose J. and Calvo-Zaragoza, Jorge},
  doi          = {10.1007/s11263-025-02487-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6221-6250},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Self-supervised learning for text recognition: A critical survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Img2Tab: Automatic class relevant concept discovery from StyleGAN features for explainable image classification. <em>IJCV</em>, <em>133</em>(9), 6201-6220. (<a href='https://doi.org/10.1007/s11263-025-02474-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional tabular classifiers provide explainable decision-making with interpretable features (concepts). However, using their explainability in vision tasks has been limited due to the pixel representation of images. In this paper, we design Img2Tabs that classify images by concepts to harness the explainability of tabular classifiers. Img2Tabs encode image pixels into tabular features by StyleGAN inversion. Since not all of the resulting features are class-relevant or interpretable due to their generative nature, the Img2Tab classifier should automatically discover class-relevant concepts from the StyleGAN features. Thus, we propose a novel algorithm using the Wasserstein-1 metric to quantify class-relevancy and interpretability simultaneously. By this method of concept visualization, we quantitatively investigate whether important features extracted by tabular classifiers are class-relevant concepts. Consequently, we determine the most effective classifier for Img2Tabs in terms of discovering class-relevant concepts automatically from StyleGAN features. In evaluations, we demonstrate concept-based explanations through importance and visualization. Img2Tab achieves top-1 accuracy on par with CNN classifiers and deep feature learning baselines. Additionally, we show that users can interactively debug Img2Tab classifier to prevent erroneous decision-making from data bias without sacrificing accuracy. The source and demo code for Img2Tab are available at https://github.com/songsnim/Img2Tab_pytorch},
  archive      = {J_IJCV},
  author       = {Song, Youngjae and Shyn, Sung Kuk and Kim, Kwang-su},
  doi          = {10.1007/s11263-025-02474-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6201-6220},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Img2Tab: Automatic class relevant concept discovery from StyleGAN features for explainable image classification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal transport with arbitrary prior for dynamic resolution network. <em>IJCV</em>, <em>133</em>(9), 6187-6200. (<a href='https://doi.org/10.1007/s11263-025-02483-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic resolution network is proved to be crucial in reducing computational redundancy by automatically assigning satisfactory resolution for each input image. However, it is observed that resolution choices are often collapsed, where prior works tend to assign images to the resolution routes whose computational cost is close to the required FLOPs. In this paper, we propose a novel optimal transport dynamic resolution network (OTD-Net) by establishing an intrinsic connection between resolution assignment and optimal transport problem. In this framework, each sample owns a resolution assignment choice viewed as supplier, and each resolution requires unallocated images considered as demander. With two assignment priors, OTD-Net benefits from the non-collapse division under theoretical support, and produces the desired assignment policy by balancing the computation budget and prediction accuracy. On that basis, a multi-resolution inference is proposed to ensemble low-resolution predictions. Extensive experiments including image classification, object detection and depth estimation, show our approach is both efficient and effective for both ResNet and Transformer, achieving state-of-the-art performance on various benchmarks.},
  archive      = {J_IJCV},
  author       = {Zhang, Zhizhong and Li, Shujun and Zhang, Chenyang and Ma, Lizhuang and Tan, Xin and Xie, Yuan},
  doi          = {10.1007/s11263-025-02483-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6187-6200},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Optimal transport with arbitrary prior for dynamic resolution network},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoViT: Achieving real-time vision transformers on mobile via latency-aware coarse-to-fine search. <em>IJCV</em>, <em>133</em>(9), 6170-6186. (<a href='https://doi.org/10.1007/s11263-025-02480-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their impressive performance on various tasks, vision transformers (ViTs) are heavy for mobile vision applications. Recent works have proposed combining the strengths of ViTs and convolutional neural networks (CNNs) to build lightweight networks. Still, these approaches rely on hand-designed architectures with a pre-determined number of parameters. In this work, we address the challenge of finding optimal light-weight ViTs given constraints on model size and computational cost using neural architecture search. We use a search algorithm that considers both model parameters and on-device deployment latency. This method analyzes network properties, hardware memory access pattern, and degree of parallelism to directly and accurately estimate the network latency. To prevent the need for extensive testing during the search process, we use a lookup table based on a detailed breakdown of the speed of each component and operation, which can be reused to evaluate the whole latency of each search structure. Our approach leads to improved efficiency compared to testing the speed of the whole model during the search process. Extensive experiments demonstrate that, under similar parameters and FLOPs, our searched lightweight ViTs achieve higher accuracy and lower latency than state-of-the-art models. For instance, on ImageNet-1K, AutoViT_XXS (71.3% Top-1 accuracy, 10.2ms latency) outperforms MobileViTv3_XXS (71.0% Top-1 accuracy, 12.5ms latency) with 0.3% higher accuracy and 2.3ms lower latency.},
  archive      = {J_IJCV},
  author       = {Kong, Zhenglun and Xu, Dongkuan and Li, Zhengang and Dong, Peiyan and Tang, Hao and Wang, Yanzhi and Mukherjee, Subhabrata},
  doi          = {10.1007/s11263-025-02480-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6170-6186},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AutoViT: Achieving real-time vision transformers on mobile via latency-aware coarse-to-fine search},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking open-set object detection: Issues, a new formulation, and taxonomy. <em>IJCV</em>, <em>133</em>(9), 6145-6169. (<a href='https://doi.org/10.1007/s11263-025-02479-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-set object detection (OSOD), a task involving the detection of unknown objects while accurately detecting known objects, has recently gained attention. However, we identify a fundamental issue with the problem formulation employed in current OSOD studies. Inherent to object detection is knowing “what to detect,” which contradicts the idea of identifying “unknown” objects. This sets OSOD apart from open-set recognition (OSR). This contradiction complicates a proper evaluation of methods’ performance, a fact that previous studies have overlooked. Next, we propose a novel formulation wherein detectors are required to detect both known and unknown classes within specified super-classes of object classes. This new formulation is free from the aforementioned issues and has practical applications. Finally, we design benchmark tests utilizing existing datasets and report the experimental evaluation of existing OSOD methods. The results show that existing methods fail to accurately detect unknown objects due to misclassification of known and unknown classes rather than incorrect bounding box prediction. As a byproduct, we introduce a taxonomy of OSOD, resolving confusion prevalent in the literature. We anticipate that our study will encourage the research community to reconsider OSOD and facilitate progress in the right direction.},
  archive      = {J_IJCV},
  author       = {Hosoya, Yusuke and Suganuma, Masanori and Okatani, Takayuki},
  doi          = {10.1007/s11263-025-02479-3},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6145-6169},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rethinking open-set object detection: Issues, a new formulation, and taxonomy},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight structure-aware attention for visual understanding. <em>IJCV</em>, <em>133</em>(9), 6129-6144. (<a href='https://doi.org/10.1007/s11263-025-02475-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention operator has been widely used as a basic brick in visual understanding since it provides some flexibility through its adjustable kernels. However, this operator suffers from inherent limitations: (1) the attention kernel is not discriminative enough, resulting in high redundancy, and (2) the complexity in computation and memory is quadratic in the sequence length. In this paper, we propose a novel attention operator, called Lightweight Structure-aware Attention (LiSA), which has a better representation power with log-linear complexity. Our operator transforms the attention kernels to be more discriminative by learning structural patterns. These structural patterns are encoded by exploiting a set of relative position embeddings (RPEs) as multiplicative weights, thereby improving the representation power of the attention kernels. Additionally, the RPEs are approximated to obtain log-linear complexity. Our experiments and analyses demonstrate that the proposed operator outperforms self-attention and other existing operators, achieving state-of-the-art results on ImageNet-1K and other downstream tasks such as video action recognition on Kinetics-400, object detection & instance segmentation on COCO, and semantic segmentation on ADE-20K.},
  archive      = {J_IJCV},
  author       = {Kwon, Heeseung and Castro, Francisco M. and Marin-Jimenez, Manuel J. and Guil, Nicolas and Alahari, Karteek},
  doi          = {10.1007/s11263-025-02475-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6129-6144},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lightweight structure-aware attention for visual understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointOBB-v3: Expanding performance boundaries of single point-supervised oriented object detection. <em>IJCV</em>, <em>133</em>(9), 6108-6128. (<a href='https://doi.org/10.1007/s11263-025-02486-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing demand for oriented object detection (OOD), recent studies on point-supervised OOD have attracted significant interest. In this paper, we propose PointOBB-v3, a stronger single point-supervised OOD framework. Compared to existing methods, it generates pseudo rotated boxes without additional priors and incorporates support for the end-to-end paradigm. PointOBB-v3 functions by integrating three unique image views: the original view, a resized view, and a rotated/flipped (rot/flp) view. Based on the views, a scale augmentation module and an angle acquisition module are constructed. In the first module, a Scale-Sensitive Consistency (SSC) loss and a Scale-Sensitive Feature Fusion (SSFF) module are introduced to improve the model’s ability to estimate object scale. To achieve precise angle predictions, the second module employs symmetry-based self-supervised learning. Additionally, we introduce an end-to-end version that eliminates the pseudo-label generation process by integrating a detector branch and introduces an Instance-Aware Weighting (IAW) strategy to focus on high-quality predictions. We conducted extensive experiments on the DIOR-R, DOTA-v1.0/v1.5/v2.0, FAIR1M, STAR, and RSAR datasets. Across all these datasets, our method achieves an average improvement in accuracy of 3.56% in comparison to previous state-of-the-art methods. The code will be available at https://github.com/ZpyWHU/PointOBB-v3 .},
  archive      = {J_IJCV},
  author       = {Zhang, Peiyuan and Luo, Junwei and Yang, Xue and Yu, Yi and Li, Qingyun and Zhou, Yue and Jia, Xiaosong and Lu, Xudong and Chen, Jingdong and Li, Xiang and Yan, Junchi and Li, Yansheng},
  doi          = {10.1007/s11263-025-02486-4},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6108-6128},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PointOBB-v3: Expanding performance boundaries of single point-supervised oriented object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling scattering effect for under-display camera image restoration. <em>IJCV</em>, <em>133</em>(9), 6088-6107. (<a href='https://doi.org/10.1007/s11263-025-02454-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The under-display camera (UDC) technology furnishes users with an uninterrupted full-screen viewing experience, eliminating the need for notches or punch holes. However, the translucent properties of the display lead to substantial degradation in UDC images. This work addresses the challenge of restoring UDC images by specifically targeting the scattering effect induced by the display. We explicitly model this scattering phenomenon by treating the display as a homogeneous scattering medium. Leveraging this physical model, the image formation pipeline is enhanced to synthesize more realistic UDC images alongside corresponding ground-truth images, thereby constructing a more accurate UDC dataset. To counteract the scattering effect in the restoration process, we propose a dual-branch network. The scattering branch employs channel-wise self-attention to estimate the scattering parameters, while the image branch capitalizes on the local feature representation capabilities of CNNs to restore the degraded UDC images. Additionally, we introduce a novel channel-wise cross-attention fusion block that integrates global scattering information into the image branch, facilitating improved restoration. To further refine the model, we design a dark channel regularization loss during training to reduce the gap between the dark channel distributions of the restored and ground-truth images. Comprehensive experiments conducted on both synthetic and real-world datasets demonstrate the superiority of our approach over current state-of-the-art UDC restoration methods. Our source code is publicly available at: https://github.com/NamecantbeNULL/SRUDC_pp .},
  archive      = {J_IJCV},
  author       = {Song, Binbin and Zhou, Jiantao and Chen, Xiangyu and Xu, Shuning},
  doi          = {10.1007/s11263-025-02454-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6088-6107},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Modeling scattering effect for under-display camera image restoration},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIM4D: Masked modeling with multi-view video for autonomous driving representation learning. <em>IJCV</em>, <em>133</em>(9), 6074-6087. (<a href='https://doi.org/10.1007/s11263-025-02464-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning robust and scalable visual representations from massive multi-view video data remains a challenge in computer vision and autonomous driving. Existing pre-training methods either rely on expensive supervised learning with 3D annotations, limiting the scalability, or focus on single-frame or monocular inputs, neglecting the temporal information, which is fundamental for the ultimate application, i.e., end-to-end planning. We propose MIM4D, a novel pre-training paradigm based on dual masked image modeling (MIM). MIM4D leverages both spatial and temporal relations by training on masked multi-view video inputs. It constructs pseudo-3D features using continuous scene flow and projects them onto 2D plane for supervision. To address the lack of dense 3D supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable rendering to learn geometric representations. We demonstrate that MIM4D achieves state-of-the-art performance on the nuScenes dataset for visual representation learning in autonomous driving. It significantly improves existing methods on multiple downstream tasks, including end-to-end planning( $$9\%$$ collision decrease), BEV segmentation ( $$8.7\%$$ IoU), 3D object detection ( $$3.5\%$$ mAP), and HD map construction ( $$1.4\%$$ mAP). Our work offers a new choice for learning representation at scale in autonomous driving. Code and models are released at https://github.com/hustvl/MIM4D .},
  archive      = {J_IJCV},
  author       = {Zou, Jialv and Liao, Bencheng and Zhang, Qian and Liu, Wenyu and Wang, Xinggang},
  doi          = {10.1007/s11263-025-02464-w},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6074-6087},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MIM4D: Masked modeling with multi-view video for autonomous driving representation learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RigNet++: Semantic assisted repetitive image guided network for depth completion. <em>IJCV</em>, <em>133</em>(9), 6051-6073. (<a href='https://doi.org/10.1007/s11263-025-02470-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion aims to recover dense depth maps from sparse ones, where color images are often used to facilitate this task. Recent depth methods primarily focus on image guided learning frameworks. However, blurry guidance in the image and unclear structure in the depth still impede their performance. To tackle these challenges, we explore a repetitive design in our image guided network to gradually and sufficiently recover depth values. Specifically, the repetition is embodied in both the image guidance branch and depth generation branch. In the former branch, we design a dense repetitive hourglass network (DRHN) to extract discriminative image features of complex environments, which can provide powerful contextual instruction for depth prediction. In the latter branch, we present a repetitive guidance (RG) module based on dynamic convolution, in which an efficient convolution factorization is proposed to reduce the complexity while modeling high-frequency structures progressively. Furthermore, in the semantic guidance branch, we utilize the well-known large vision model, i.e., segment anything (SAM), to supply RG with semantic prior. In addition, we propose a region-aware spatial propagation network (RASPN) for further depth refinement based on the semantic prior constraint. Finally, we collect a new dataset termed TOFDC for the depth completion task, which is acquired by the time-of-flight (TOF) sensor and the color camera on smartphones. Extensive experiments demonstrate that our method achieves state-of-the-art performance on KITTI, NYUv2, Matterport3D, 3D60, VKITTI, and our TOFDC.},
  archive      = {J_IJCV},
  author       = {Yan, Zhiqiang and Li, Xiang and Hui, Le and Zhang, Zhenyu and Li, Jun and Yang, Jian},
  doi          = {10.1007/s11263-025-02470-y},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6051-6073},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RigNet++: Semantic assisted repetitive image guided network for depth completion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized contour vibration model for building extraction. <em>IJCV</em>, <em>133</em>(9), 6025-6050. (<a href='https://doi.org/10.1007/s11263-025-02468-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classic active contour models (ACMs) are becoming a great promising solution to the contour-based object extraction with the progress of deep learning recently. Inspired by the wave vibration theory in physics, we propose a Generalized Contour Vibration Model (G-CVM) by inheriting the force and motion principle of contour wave for automatically estimating building contours. The contour estimation problems, conventionally solved by snake and level-set based ACMs, are unified to formulate as second-order partial differential equation to model the contour evolution. In parallel with the current ACM methods, we propose two types of evolution paradigms: curve-CVM and surface-CVM, from the perspective of the vibration spaces of contour waves. To tailor personalization contours for specific targets, we parameterize the constant coefficient wave differential equation through a convolutional network, and hereby integrate them into a unified learnable model for contour extraction. Through adopting finite difference optimization, we can progressively perform the contour evolution from an initial state through a recursive computation on the contour vibration model. Both the building contour evolution and the model optimization are modulated to form a close-looping end-to-end network. Besides, we make a discussion of ours vs the conventional ACMs, all which can be interpreted uniformly from the view of differential equation in different evolution domains. Comprehensive evaluations on several building datasets demonstrate the effectiveness and superiority of our proposed G-CVM when compared with other state-of-the-art building extraction networks and deep active contour solutions.},
  archive      = {J_IJCV},
  author       = {Xu, Chunyan and Yao, Shuaizhen and Xu, Ziqiang and Cui, Zhen and Yang, Jian},
  doi          = {10.1007/s11263-025-02468-6},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6025-6050},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A generalized contour vibration model for building extraction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal transformer for single RGB-D camera synchronous tracking and reconstruction of non-rigid dynamic objects. <em>IJCV</em>, <em>133</em>(9), 6015-6024. (<a href='https://doi.org/10.1007/s11263-025-02469-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple and effective method that views the problem of single RGB-D camera synchronous tracking and reconstruction of non-rigid dynamic objects as an aligned sequential point cloud prediction problem. Our method does not require additional data transformations (truncated signed distance function or deformation graphs, etc.), alignment constraints (handcrafted features or optical flow, etc.), and prior regularities (as-rigid-as-possible or embedded deformation, etc.). We propose an end-to-end model architecture that is TRansformer for synchronous Tracking and Reconstruction of non-rigid dynamic target based on RGB-D images from a monocular camera, called TR4TR. We use a spatial-temporal combined 2D image encoder that directly encodes features from RGB-D sequence images, and a 3D point decoder to generate aligned sequential point cloud containing tracking and reconstruction results. The TR4TR model outperforms the baselines on the DeepDeform non-rigid dataset, and outperforms the state-of-the-art method by 8.82% on the deformation error evaluation metric. In addition, TR4TR is more robust when the target undergoes large inter-frame deformation. The code is available at https://github.com/xfliu1998/tr4tr-main .},
  archive      = {J_IJCV},
  author       = {Liu, Xiaofei and Yi, Zhengkun and Wu, Xinyu and Shang, Wanfeng},
  doi          = {10.1007/s11263-025-02469-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {6015-6024},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Spatial-temporal transformer for single RGB-D camera synchronous tracking and reconstruction of non-rigid dynamic objects},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-aligned learning with collaborative refinement for unsupervised VI-ReID. <em>IJCV</em>, <em>133</em>(9), 5992-6014. (<a href='https://doi.org/10.1007/s11263-025-02461-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to match pedestrian images of the same individual across different modalities without human annotations for model learning. Previous methods unify pseudo-labels of cross-modality images through label association algorithms and then design contrastive learning framework for global feature learning. However, these methods overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. This insight results in insufficient modality-shared learning when only global features are optimized. To address this issue, we propose a Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework, which builds up optimization objective for specific fine-grained patterns emphasized by each modality, thereby achieving complementary alignment between the label distributions of different modalities. Specifically, we first introduce a Dual Association with Global Learning (DAGI) module to unify the pseudo-labels of cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances. Optimization objective is then formulated based on the semantic-aligned features and their corresponding label space. To alleviate the side-effects arising from noisy pseudo-labels, we propose a Global-Part Collaborative Refinement (GPCR) module to mine reliable positive sample sets for the global and part features dynamically and optimize the inter-instance relationships. Extensive experiments demonstrate the effectiveness of the proposed method, which achieves superior performances to state-of-the-art methods. Our code is available at https://github.com/FranklinLingfeng/code-for-SALCR .},
  archive      = {J_IJCV},
  author       = {Cheng, De and He, Lingfeng and Wang, Nannan and Zhang, Dingwen and Gao, Xinbo},
  doi          = {10.1007/s11263-025-02461-z},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5992-6014},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantic-aligned learning with collaborative refinement for unsupervised VI-ReID},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to deblur polarized images. <em>IJCV</em>, <em>133</em>(9), 5976-5991. (<a href='https://doi.org/10.1007/s11263-025-02459-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A polarization camera can capture four linear polarized images with different polarizer angles in a single shot, which is useful in polarization-based vision applications since the degree of linear polarization (DoLP) and the angle of linear polarization (AoLP) can be directly computed from the captured polarized images. However, since the on-chip micro-polarizers block part of the light so that the sensor often requires a longer exposure time, the captured polarized images are prone to motion blur caused by camera shakes, leading to noticeable degradation in the computed DoLP and AoLP. Deblurring methods for conventional images often show degraded performance when handling the polarized images since they only focus on deblurring without considering the polarization constraints. In this paper, we propose a polarized image deblurring pipeline to solve the problem in a polarization-aware manner by adopting a divide-and-conquer strategy to explicitly decompose the problem into two less ill-posed sub-problems, and design a two-stage neural network to handle the two sub-problems respectively. Experimental results show that our method achieves state-of-the-art performance on both synthetic and real-world images, and can improve the performance of polarization-based vision applications such as image dehazing and reflection removal.},
  archive      = {J_IJCV},
  author       = {Zhou, Chu and Teng, Minggui and Zhou, Xinyu and Xu, Chao and Sato, Imari and Shi, Boxin},
  doi          = {10.1007/s11263-025-02459-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5976-5991},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to deblur polarized images},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized closed-form formulae for feature-based subpixel alignment in patch-based matching. <em>IJCV</em>, <em>133</em>(9), 5958-5975. (<a href='https://doi.org/10.1007/s11263-025-02457-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patch-based matching is a technique meant to measure the disparity between pixels in a source and target image and is at the core of various methods in computer vision. When the subpixel disparity between the source and target images is required, the cost function or the target image has to be interpolated. While cost-based interpolation is easier to implement, multiple works have shown that image-based interpolation can increase the accuracy of the disparity estimate. In this paper we review closed-form formulae for subpixel disparity computation for one dimensional matching, e.g., rectified stereo matching, for the standard cost functions used in patch-based matching. We then propose new formulae to generalize to high-dimensional search spaces, which is necessary for unrectified stereo matching and optical flow. We also compare the image-based interpolation formulae with traditional cost-based formulae, and show that image-based interpolation brings a significant improvement over the cost-based interpolation methods for two dimensional search spaces, and small improvement in the case of one dimensional search spaces. The zero-mean normalized cross correlation cost function is found to be preferable for subpixel alignment. A new error model, based on very broad assumptions is outlined in the Supplementary Material to demonstrate why these image-based interpolation formulae outperform their cost-based counterparts and why the zero-mean normalized cross correlation function is preferable for subpixel alignement.},
  archive      = {J_IJCV},
  author       = {Jospin, Laurent Valentin and Laga, Hamid and Boussaid, Farid and Bennamoun, Mohammed},
  doi          = {10.1007/s11263-025-02457-9},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5958-5975},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generalized closed-form formulae for feature-based subpixel alignment in patch-based matching},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HumanLiff: Layer-wise 3D human diffusion model. <em>IJCV</em>, <em>133</em>(9), 5938-5957. (<a href='https://doi.org/10.1007/s11263-025-02477-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human generation from 2D images has achieved remarkable progress through the synergistic utilization of neural rendering and generative models. Existing 3D human generative models mainly generate a clothed 3D human as an inseparable 3D model in a single pass, while rarely considering the layer-wise nature of a clothed human body, which often consists of the human body and various clothes such as underwear, outerwear, trousers, shoes, etc. In this work, we propose HumanLiff, the first layer-wise 3D human generative model with a unified diffusion process. Specifically, HumanLiff firstly generates minimal-clothed humans, represented by tri-plane features, in a canonical space, and then progressively generates clothes in a layer-wise manner. In this way, the 3D human generation is thus formulated as a sequence of diffusion-based 3D conditional generation. To reconstruct more fine-grained 3D humans with tri-plane representation, we propose a tri-plane shift operation that splits each tri-plane into three sub-planes and shifts these sub-planes to enable feature grid subdivision. To further enhance the controllability of 3D generation with 3D layered conditions, HumanLiff hierarchically fuses tri-plane features and 3D layered conditions to facilitate the 3D diffusion model learning. Extensive experiments on two layer-wise 3D human datasets, SynBody (synthetic) and TightCap (real-world), validate that HumanLiff significantly outperforms state-of-the-art methods in layer-wise 3D human generation. Our code and datasets are available at https://skhu101.github.io/HumanLiff .},
  archive      = {J_IJCV},
  author       = {Hu, Shoukang and Hong, Fangzhou and Hu, Tao and Pan, Liang and Mei, Haiyi and Xiao, Weiye and Yang, Lei and Liu, Ziwei},
  doi          = {10.1007/s11263-025-02477-5},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5938-5957},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HumanLiff: Layer-wise 3D human diffusion model},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending against adversarial examples via modeling adversarial noise. <em>IJCV</em>, <em>133</em>(9), 5920-5937. (<a href='https://doi.org/10.1007/s11263-025-02467-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples have become a major threat to the reliable application of deep learning models. Meanwhile, this issue promotes the development of adversarial defenses. Adversarial noise contains well-generalizing and misleading features, which can manipulate predicted labels to be flipped maliciously. Motivated by this, we study modeling adversarial noise for defending against adversarial examples by learning the transition relationship between adversarial labels (i.e., flipped labels caused by adversarial noise) and natural labels (i.e., real labels of natural samples). In this work, we propose an adversarial defense method from the perspective of modeling adversarial noise. Specifically, we construct an instance-dependent label transition matrix to represent the label transition relationship for explicitly modeling adversarial noise. The label transition matrix is obtained from the input sample by leveraging a label transition network. By exploiting the label transition matrix, we can infer the natural label from the adversarial label and thus correct wrong predictions misled by adversarial noise. Additionally, to enhance the robustness of the label transition network, we design an adversarial robustness constraint at the transition matrix level. Experimental results demonstrate that our method effectively improves the robust accuracy against multiple attacks and exhibits great performance in detecting adversarial input samples.},
  archive      = {J_IJCV},
  author       = {Zhou, Dawei and Wang, Nannan and Han, Bo and Liu, Tongliang and Gao, Xinbo},
  doi          = {10.1007/s11263-025-02467-7},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5920-5937},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Defending against adversarial examples via modeling adversarial noise},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring bidirectional bounds for minimax-training of energy-based models. <em>IJCV</em>, <em>133</em>(9), 5898-5919. (<a href='https://doi.org/10.1007/s11263-025-02460-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy-based models (EBMs) estimate unnormalized densities in an elegant framework, but they are generally difficult to train. Recent work has linked EBMs to generative adversarial networks, by noting that they can be trained through a minimax game using a variational lower bound. To avoid the instabilities caused by minimizing a lower bound, we propose to instead work with bidirectional bounds, meaning that we maximize a lower bound and minimize an upper bound when training the EBM. We investigate four different bounds on the log-likelihood derived from different perspectives. We derive lower bounds based on the singular values of the generator Jacobian and on mutual information. To upper bound the negative log-likelihood, we consider a gradient penalty-like bound, as well as one based on diffusion processes. In all cases, we provide algorithms for evaluating the bounds. We compare the different bounds to investigate, the pros and cons of the different approaches. Finally, we demonstrate that the use of bidirectional bounds stabilizes EBM training and yields high-quality density estimation and sample generation.},
  archive      = {J_IJCV},
  author       = {Geng, Cong and Wang, Jia and Chen, Li and Gao, Zhiyong and Frellsen, Jes and Hauberg, Søren},
  doi          = {10.1007/s11263-025-02460-0},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5898-5919},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploring bidirectional bounds for minimax-training of energy-based models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A norm regularization training strategy for robust image quality assessment models. <em>IJCV</em>, <em>133</em>(9), 5883-5897. (<a href='https://doi.org/10.1007/s11263-025-02458-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Quality Assessment (IQA) models predict the quality score of input images. They can be categorized into Full-Reference (FR-) and No-Reference (NR-) IQA models based on the availability of reference images. These models are essential for performance evaluation and optimization guidance in the media industry. However, researchers have observed that introducing imperceptible perturbations to input images can notably influence the predicted scores of both FR- and NR-IQA models, resulting in inaccurate assessments of image quality. This phenomenon is known as adversarial attacks. In this paper, we initially define attacks targeted at both FR-IQA and NR-IQA models. Subsequently, we introduce a defense approach applicable to both types of models, aimed at enhancing the stability of predicted scores and boosting the adversarial robustness of IQA models. To be specific, we present theoretical evidence showing that the magnitude of score changes is related to the $$\ell _1$$ norm of the model’s gradient with respect to the input image. Building upon this theoretical foundation, we propose a norm regularization training strategy aimed at reducing the $$\ell _1$$ norm of the gradient, thereby boosting the robustness of IQA models. Experiments conducted on three FR-IQA and four NR-IQA models demonstrate the effectiveness of our strategy in reducing score changes in the presence of adversarial attacks. To the best of our knowledge, this work marks the first attempt to defend against adversarial attacks on both FR- and NR-IQA models. Our study offers valuable insights into the adversarial robustness of IQA models and provides a foundation for future research in this area.},
  archive      = {J_IJCV},
  author       = {Liu, Yujia and Yang, Chenxi and Li, Dingquan and Jiang, Tingting and Huang, Tiejun},
  doi          = {10.1007/s11263-025-02458-8},
  journal      = {International Journal of Computer Vision},
  month        = {9},
  number       = {9},
  pages        = {5883-5897},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A norm regularization training strategy for robust image quality assessment models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Consistent prompt tuning for generalized category discovery. <em>IJCV</em>, <em>133</em>(8), 5872-5881. (<a href='https://doi.org/10.1007/s11263-025-02449-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Yang, Muli and Yin, Jie and Gu, Yanan and Deng, Cheng and Zhang, Hanwang and Zhu, Hongyuan},
  doi          = {10.1007/s11263-025-02449-9},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5872-5881},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Consistent prompt tuning for generalized category discovery},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simplified concrete dropout - Improving the generation of attribution masks for fine-grained classification. <em>IJCV</em>, <em>133</em>(8), 5857-5871. (<a href='https://doi.org/10.1007/s11263-025-02453-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In fine-grained classification, which is classifying images into subcategories within a common broader category, it is crucial to have precise visual explanations of the classification model’s decision. While commonly used attention- or gradient-based methods deliver either too coarse or too noisy explanations unsuitable for highlighting subtle visual differences reliably, perturbation-based methods can precisely locate pixels causally responsible for the predicted category. The fill-in of the dropout (FIDO) algorithm is one of those methods, which utilizes concrete dropout (CD) to sample a set of attribution masks and updates the sampling parameters based on the output of the classification model. In this paper, we present a solution against the high variance in the gradient estimates, a known problem of the FIDO algorithm that has been mitigated until now by large mini-batch updates of the sampling parameters. First, our solution allows for estimating the parameters with smaller mini-batch sizes without losing the quality of the estimates but with a reduced computational effort. Next, our method produces finer and more coherent attribution masks. Finally, we use the resulting attribution masks to improve the classification performance on three fine-grained datasets without additional fine-tuning steps and achieve results that are otherwise only achieved if ground truth bounding boxes are used.},
  archive      = {J_IJCV},
  author       = {Korsch, Dimitri and Shadaydeh, Maha and Denzler, Joachim},
  doi          = {10.1007/s11263-025-02453-z},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5857-5871},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Simplified concrete dropout - Improving the generation of attribution masks for fine-grained classification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized relative pose and scale from affine correspondences. <em>IJCV</em>, <em>133</em>(8), 5840-5856. (<a href='https://doi.org/10.1007/s11263-025-02452-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aligning extrinsically calibrated view sets is essential for merging 3D reconstructions from different agents or localizing them within a large existing map of the environment. In such scenarios, we not only have to account for the 3D rotation and translation but also need to estimate the unknown scaling factor between the reconstructions. In this paper, we propose the first closed-form solvers for image-only data to the general problem, leveraging either 26 point or 9 affine correspondences (AC) to obtain the scale, 3D orientation, and translation. Considering that modern image-capturing tools like smartphones and mixed reality devices typically come with Inertial Measurement Units and return the gravity direction by default, we also propose a solver that requires just 2 ACs along with gravity measurements. The proposed methods have been rigorously tested on both synthetic data and extensive publicly available real-world datasets. The results demonstrate that our approach achieves state-of-the-art accuracy and permits robust estimation in real-time owing to small sample sizes. The code is available at https://github.com/gowanting/GRPS-Affine .},
  archive      = {J_IJCV},
  author       = {Xu, Wanting and Zhang, Xinyue and Pollefeys, Marc and Barath, Daniel and Kneip, Laurent},
  doi          = {10.1007/s11263-025-02452-0},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5840-5856},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generalized relative pose and scale from affine correspondences},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supplementary prompt learning for vision-language models. <em>IJCV</em>, <em>133</em>(8), 5822-5839. (<a href='https://doi.org/10.1007/s11263-025-02451-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained vision-language models like CLIP have shown remarkable capabilities across various downstream tasks with well-tuned prompts. Advanced methods tune prompts by optimizing context while keeping the class name fixed, implicitly assuming that the class names in prompts are accurate and not missing. However, this assumption may be violated in numerous real-world scenarios, leading to potential performance degeneration or even failure of existing prompt learning methods. For example, an accurate class name for an image containing “Transformers” might be inaccurate because selecting a precise class name among numerous candidates is challenging. Moreover, assigning class names to some images may require specialized knowledge, resulting in indexing rather than semantic labels, e.g., Group 3 and Group 4 subtypes of medulloblastoma. To cope with the class-name missing issue, we propose a simple yet effective prompt learning approach, called Supplementary Optimization (SOp) for supplementing the missing class-related information. Specifically, SOp models the class names as learnable vectors while keeping the context fixed to learn prompts for downstream tasks. Extensive experiments across 18 public datasets demonstrate the efficacy of SOp when class names are missing. SOp can achieve performance comparable to that of the context optimization approach, even without using the prior information in the class names.},
  archive      = {J_IJCV},
  author       = {Zeng, Rongfei and Yang, Zhipeng and Yu, Ruiyun and Zhang, Yonggang},
  doi          = {10.1007/s11263-025-02451-1},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5822-5839},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Supplementary prompt learning for vision-language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bamboo: Building mega-scale vision dataset continually with Human–Machine synergy. <em>IJCV</em>, <em>133</em>(8), 5806-5821. (<a href='https://doi.org/10.1007/s11263-025-02450-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale datasets play a vital role in computer vision. But current datasets are annotated blindly without differentiation to samples, making the data collection inefficient and unscalable. The open question is how to build a mega-scale dataset actively. Although advanced active learning algorithms might be the answer, we experimentally found that they are lame in the realistic annotation scenario where out-of-distribution data is extensive. This work thus proposes a novel active learning framework for realistic dataset annotation. Equipped with this framework, we build a high-quality vision dataset—Bamboo, which consists of 69M image classification annotations with 119K categories and 28M object bounding box annotations with 809 categories. We organize these categories by a hierarchical taxonomy integrated from several knowledge bases. The classification annotations are four times larger than ImageNet22K, and that of detection is three times larger than Object365. Compared to ImageNet22K and Objects365, models pre-trained on Bamboo achieve superior performance among various downstream tasks (6.2% gains on classification and 2.1% gains on detection). We believe our active learning framework and Bamboo are essential for future work. Code and dataset are available at https://github.com/ZhangYuanhan-AI/Bamboo .},
  archive      = {J_IJCV},
  author       = {Zhang, Yuanhan and Sun, Qinghong and Zhou, Yichun and He, Zexin and Yin, Zhenfei and Wang, Kun and Sheng, Lu and Qiao, Yu and Shao, Jing and Liu, Ziwei},
  doi          = {10.1007/s11263-025-02450-2},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5806-5821},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bamboo: Building mega-scale vision dataset continually with Human–Machine synergy},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-fidelity image inpainting with multimodal guided GAN inversion. <em>IJCV</em>, <em>133</em>(8), 5788-5805. (<a href='https://doi.org/10.1007/s11263-025-02448-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Network (GAN) inversion have demonstrated excellent performance in image inpainting that aims to restore lost or damaged image texture using its unmasked content. Previous GAN inversion-based methods usually utilize well-trained GAN models as effective priors to generate the realistic regions for missing holes. Despite excellence, they ignore a hard constraint that the unmasked regions in the input and the output should be the same, resulting in a gap between GAN inversion and image inpainting and thus degrading the performance. Besides, existing GAN inversion approaches often consider a single modality of the input image, neglecting other auxiliary cues in images for improvements. Addressing these problems, we propose a novel GAN inversion approach, dubbed MMInvertFill, for image inpainting. MMInvertFill contains primarily a multimodal guided encoder with a pre-modulation and a GAN generator with $$ \mathcal {F} \& \mathcal {W}^+$$ latent space. Specifically, the multimodal encoder aims to enhance the multi-scale structures with additional semantic segmentation edge texture modalities through a gated mask-aware attention module. Afterwards, a pre-modulation is presented to encode these structures into style vectors. To mitigate issues of conspicuous color discrepancy and semantic inconsistency, we introduce the $$ \mathcal {F} \& \mathcal {W}^+$$ latent space to bridge the gap between GAN inversion and image inpainting. Furthermore, in order to reconstruct faithful and photorealistic images, we devise a simple yet effective Soft-update Mean Latent module to capture more diversified in-domain patterns for generating high-fidelity textures for massive corruptions. In our extensive experiments on six challenging datasets, including CelebA-HQ, Places2, OST, CityScapes, MetFaces and Scenery, we show that our MMInvertFill qualitatively and quantitatively outperforms other state-of-the-arts and it supports the completion of out-of-domain images effectively. Our project webpage including code and results will be available at https://yeates.github.io/mm-invertfill .},
  archive      = {J_IJCV},
  author       = {Zhang, Libo and Yu, Yongsheng and Yao, Jiali and Fan, Heng},
  doi          = {10.1007/s11263-025-02448-w},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5788-5805},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {High-fidelity image inpainting with multimodal guided GAN inversion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BackdoorBench: A comprehensive benchmark and analysis of backdoor learning. <em>IJCV</em>, <em>133</em>(8), 5700-5787. (<a href='https://doi.org/10.1007/s11263-025-02447-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, backdoor learning has attracted increasing attention due to its effectiveness on investigating the adversarial vulnerability of artificial intelligence (AI) systems. Several seminal backdoor attack and defense algorithms have been developed, forming an increasingly fierce arms race. However, since backdoor learning involves various factors in different stages of an AI system (e.g., data preprocessing, model training algorithm, model activation), there have been diverse settings in existing works, causing unfair comparisons or unreliable conclusions (e.g., misleading, biased, or even false conclusions). Hence, it is urgent to build a unified and standardized benchmark of backdoor learning, such that we can track real progress and design a roadmap for the future development of this literature. To that end, we construct a comprehensive benchmark of backdoor learning, dubbed BackdoorBench. Our benchmark makes three valuable contributions to the research community. (1) We provide an integrated implementation of representative backdoor learning algorithms (currently including 20 attack algorithms and 32 defense algorithms), based on an extensible modular-based codebase. (2) We conduct comprehensive evaluations of the implemented algorithms on 4 models and 4 datasets, leading to 11,492 pairs of attack-against-defense evaluations in total. (3) Based on above evaluations, we present abundant analysis from 10 perspectives via 23 analysis tools, and reveal several inspiring insights about backdoor learning. We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore the intrinsic mechanism of backdoor learning. Finally, we have created a user-friendly website at https://backdoorbench.github.io/ , which collects all the important information of BackdoorBench, including the link to Codebase, Docs, Leaderboard, and Model Zoo.},
  archive      = {J_IJCV},
  author       = {Wu, Baoyuan and Chen, Hongrui and Zhang, Mingda and Zhu, Zihao and Wei, Shaokui and Yuan, Danni and Zhu, Mingli and Wang, Ruotong and Liu, Li and Shen, Chao},
  doi          = {10.1007/s11263-025-02447-x},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5700-5787},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {BackdoorBench: A comprehensive benchmark and analysis of backdoor learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local concept embeddings for analysis of concept distributions in vision DNN feature spaces. <em>IJCV</em>, <em>133</em>(8), 5649-5699. (<a href='https://doi.org/10.1007/s11263-025-02446-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insights into the learned latent representations are imperative for verifying deep neural networks (DNNs) in critical computer vision (CV) tasks. Therefore, state-of-the-art supervised Concept-based eXplainable Artificial Intelligence (C-XAI) methods associate user-defined concepts like “car” each with a single vector in the DNN latent space (concept embedding vector). In the case of concept segmentation, these linearly separate between activation map pixels belonging to a concept and those belonging to background. Existing methods for concept segmentation, however, fall short of capturing implicitly learned sub-concepts (e.g., the DNN might split car into “proximate car” and “distant car”), and overlap of user-defined concepts (e.g., between “bus” and “truck”). In other words, they do not capture the full distribution of concept representatives in latent space. For the first time, this work shows that these simplifications are frequently broken and that distribution information can be particularly useful for understanding DNN-learned notions of sub-concepts, concept confusion, and concept outliers. To allow exploration of learned concept distributions, we propose a novel local concept analysis framework. Instead of optimizing a single global concept vector on the complete dataset, it generates a local concept embedding (LoCE) vector for each individual sample. We use the distribution formed by LoCEs to explore the latent concept distribution by fitting Gaussian mixture models (GMMs), hierarchical clustering, and concept-level information retrieval and outlier detection. Despite its context sensitivity, our method’s concept segmentation performance is competitive to global baselines. Analysis results are obtained on three datasets and six diverse vision DNN architectures, including vision transformers (ViTs). The code is available at https://github.com/continental/local-concept-embeddings .},
  archive      = {J_IJCV},
  author       = {Mikriukov, Georgii and Schwalbe, Gesina and Bade, Korinna},
  doi          = {10.1007/s11263-025-02446-y},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5649-5699},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Local concept embeddings for analysis of concept distributions in vision DNN feature spaces},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interaction confidence attention for Human–Object interaction detection. <em>IJCV</em>, <em>133</em>(8), 5629-5648. (<a href='https://doi.org/10.1007/s11263-025-02445-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human–object interaction (HOI) detection task, ensuring that interactive pairs receive higher attention weights while reducing the weight of non-interaction pairs is imperative for enhancing HOI detection accuracy. Guiding attention learning is also a key aspect of existing transformer-based algorithms. To tackle this challenge, this study proposes a novel approach termed Interaction Confidence Score Learning Attention (ICSLA), which introduces weakening and augmentation operations into the original attention weight calculation and feature extraction processes. In ICSLA, feature learning is coupled with confidence score learning, simultaneously. Leveraging ICSLA, a new and universal decoder is devised, establishing a transformer-based one-stage HOI detection architecture. Experimental results demonstrate the effectiveness of the proposed method in improving HOI detection accuracy, offering valuable insights for further optimization of attention mechanisms.},
  archive      = {J_IJCV},
  author       = {Zhang, Hong-Bo and Lin, Wang-Kai and Su, Hang and Lei, Qing and Liu, Jing-Hua and Du, Ji-Xiang},
  doi          = {10.1007/s11263-025-02445-z},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5629-5648},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Interaction confidence attention for Human–Object interaction detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot referring video single- and multi-object segmentation via cross-modal affinity with instance sequence matching. <em>IJCV</em>, <em>133</em>(8), 5610-5628. (<a href='https://doi.org/10.1007/s11263-025-02444-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring Video Object Segmentation (RVOS) aims to segment specific objects in videos based on the provided natural language descriptions. As a new supervised visual learning task, achieving RVOS for a given scene requires a substantial amount of annotated data. However, only minimal annotations are usually available for new scenes in realistic scenarios. Another practical problem is that, apart from a single object, multiple objects of the same category coexist in the same scene. Both of these issues may significantly reduce the performance of existing RVOS methods in handling real-world applications. In this paper, we propose a simple yet effective model to address these issues by incorporating a newly designed cross-modal affinity (CMA) module based on a Transformer architecture. The CMA module facilitates the establishment of multi-modal affinity over a limited number of samples, allowing the rapid acquisition of new semantic information while fostering the model’s adaptability to diverse scenarios. Furthermore, we extend our FS-RVOS approach to multiple objects through a new instance sequence matching module over CMA, which filters out all object trajectories with similarity to language features that exceed a matching threshold, thereby achieving few-shot referring multi-object segmentation (FS-RVMOS). To foster research in this field, we establish a new dataset based on currently available datasets, which covers many scenarios in terms of single-object and multi-object data, hence effectively simulating real-world scenes. Extensive experiments and comparative analyses underscore the exceptional performance of our proposed FS-RVOS and FS-RVMOS methods. Our method consistently outperforms existing related approaches through practical performance evaluations and robustness studies, achieving optimal performance on metrics across diverse benchmark tests.},
  archive      = {J_IJCV},
  author       = {Liu, Heng and Li, Guanghui and Gao, Mingqi and Zhen, Xiantong and Zheng, Feng and Wang, Yang},
  doi          = {10.1007/s11263-025-02444-0},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5610-5628},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Few-shot referring video single- and multi-object segmentation via cross-modal affinity with instance sequence matching},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IPAD: Iterative, parallel, and diffusion-based network for scene text recognition. <em>IJCV</em>, <em>133</em>(8), 5589-5609. (<a href='https://doi.org/10.1007/s11263-025-02443-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, scene text recognition has attracted more and more attention due to its diverse applications. Most state-of-the-art methods adopt an encoder-decoder framework with the attention mechanism, autoregressively generating text from left to right. Despite the convincing performance, this sequential decoding strategy constrains the inference speed. Conversely, non-autoregressive models provide faster, simultaneous predictions but often sacrifice accuracy. Although utilizing an explicit language model can improve performance, it burdens the computational load. Besides, separating linguistic knowledge from vision information may harm the final prediction. In this paper, we propose an alternative solution that uses a parallel and iterative decoder that adopts an easy-first decoding strategy. Furthermore, we regard text recognition as an image-based conditional text generation task and utilize the discrete diffusion strategy, ensuring exhaustive exploration of bidirectional contextual information. Extensive experiments demonstrate that the proposed approach achieves superior results on the benchmark datasets, including both Chinese and English text images.},
  archive      = {J_IJCV},
  author       = {Yang, Xiaomeng and Qiao, Zhi and Zhou, Yu},
  doi          = {10.1007/s11263-025-02443-1},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5589-5609},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {IPAD: Iterative, parallel, and diffusion-based network for scene text recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLIMS++: Cross language image matching with automatic context discovery for weakly supervised semantic segmentation. <em>IJCV</em>, <em>133</em>(8), 5569-5588. (<a href='https://doi.org/10.1007/s11263-025-02442-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While promising results have been achieved in weakly-supervised semantic segmentation (WSSS), limited supervision from image-level tags inevitably induces discriminative reliance and spurious relations between target classes and background regions. Thus, Class Activation Map (CAM) usually tends to activate discriminative object regions and falsely includes lots of class-related backgrounds. Without pixel-level supervisions, it could be very difficult to enlarge the foreground activation and suppress those false activation of background regions. In this paper, we propose a novel framework of Cross Language Image Matching with Automatic Context Discovery (CLIMS++), based on the recently introduced Contrastive Language-Image Pre-training (CLIP) model, for WSSS. The core idea of our framework is to introduce natural language supervision to activate more complete object regions and suppress class-related background regions in CAM. In particular, we design object, background region, and text label matching losses to guide the model to excite more reasonable object regions of each category. In addition, we propose to automatically find spurious relations between foreground categories and backgrounds, through which a background suppression loss is designed to suppress the activation of class-related backgrounds. The above designs enable the proposed CLIMS++ to generate a more complete and compact activation map for the target objects. Extensive experiments on PASCAL VOC 2012 and MS COCO 2014 datasets show that our CLIMS++ significantly outperforms the previous state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Xie, Jinheng and Deng, Songhe and Hou, Xianxu and Luo, Zhaochuan and Shen, Linlin and Huang, Yawen and Zheng, Yefeng and Shou, Mike Zheng},
  doi          = {10.1007/s11263-025-02442-2},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5569-5588},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CLIMS++: Cross language image matching with automatic context discovery for weakly supervised semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P2Object: Single point supervised object detection and instance segmentation. <em>IJCV</em>, <em>133</em>(8), 5544-5568. (<a href='https://doi.org/10.1007/s11263-025-02441-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition using single-point supervision has attracted increasing attention recently. However, the performance gap compared with fully-supervised algorithms remains large. Previous works generated class-agnostic proposals in an image offline and then treated mixed candidates as a single bag, putting a huge burden on multiple instance learning (MIL). In this paper, we introduce Point-to-Box Network (P2BNet), which constructs balanced instance-level proposal bags by generating proposals in an anchor-like way and refining the proposals in a coarse-to-fine paradigm. Through further research, we find that the bag of proposals, either at the image level or the instance level, is established on discrete box sampling. This leads the pseudo box estimation into a sub-optimal solution, resulting in the truncation of object boundaries or the excessive inclusion of background. Hence, we conduct a series exploration of discrete-to-continuous optimization, yielding P2BNet++ and Point-to-Mask Network (P2MNet). P2BNet++ conducts an approximately continuous proposal sampling strategy by better utilizing spatial clues. P2MNet further introduces low-level image information to assist in pixel prediction, and a boundary self-prediction is designed to relieve the limitation of the estimated boxes. Benefiting from the continuous object-aware pixel-level perception, P2MNet can generate more precise bounding boxes and generalize to segmentation tasks. Our method largely surpasses the previous methods in terms of the mean average precision on COCO, VOC, SBD, and Cityscapes, demonstrating great potential to bridge the performance gap compared with fully supervised tasks.},
  archive      = {J_IJCV},
  author       = {Chen, Pengfei and Yu, Xuehui and Han, Xumeng and Wang, Kuiran and Li, Guorong and Xie, Lingxi and Han, Zhenjun and Jiao, Jianbin},
  doi          = {10.1007/s11263-025-02441-3},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5544-5568},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {P2Object: Single point supervised object detection and instance segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADEM-VL: Adaptive and embedded fusion for efficient vision-language tuning. <em>IJCV</em>, <em>133</em>(8), 5527-5543. (<a href='https://doi.org/10.1007/s11263-025-02440-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in multimodal fusion have witnessed the remarkable success of vision-language (VL) models, which excel in various multimodal applications such as image captioning and visual question answering. However, building VL models requires substantial hardware resources, where efficiency is restricted by two key factors: the extended input sequence of the language model with vision features demands more computational operations, and a large number of additional learnable parameters increase memory complexity. These challenges significantly restrict the broader applicability of such models. To bridge this gap, we propose ADEM-VL, an efficient vision-language method that tunes VL models based on pretrained large language models (LLMs) by adopting a parameter-free cross-attention mechanism for similarity measurements in multimodal fusion. This approach only requires embedding vision features into the language space, significantly reducing the number of trainable parameters and accelerating both training and inference speeds. To enhance representation learning in fusion module, we introduce an efficient multiscale feature generation scheme that requires only a single forward pass through the vision encoder. Moreover, we propose an adaptive fusion scheme that dynamically discards less relevant visual information for each text token based on its attention score. This ensures that the fusion process prioritizes the most pertinent visual features. With experiments on various tasks including visual question answering, image captioning, and instruction-following, we demonstrate that our framework outperforms existing approaches. Specifically, our method surpasses existing methods by an average accuracy of 0.77% on ScienceQA dataset, with reduced training and inference latency, demonstrating the superiority of our framework.},
  archive      = {J_IJCV},
  author       = {Hao, Zhiwei and Guo, Jianyuan and Shen, Li and Luo, Yong and Hu, Han and Wen, Yonggang},
  doi          = {10.1007/s11263-025-02440-4},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5527-5543},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ADEM-VL: Adaptive and embedded fusion for efficient vision-language tuning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autoregressive temporal modeling for advanced tracking-by-diffusion. <em>IJCV</em>, <em>133</em>(8), 5505-5526. (<a href='https://doi.org/10.1007/s11263-025-02439-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking is a widely studied computer vision task with video and instance analysis applications. While paradigms such as tracking-by-regression,-detection,-attention have advanced the field, generative modeling offers new potential. Although some studies explore the generative process in instance-based understanding tasks, they rely on prediction refinement in the coordinate space rather than the visual domain. Instead, this paper presents Tracking-by-Diffusion, a novel paradigm for object tracking in video, leveraging visual generative models via the perspective of autoregressive models. This paradigm demonstrates broad applicability across point, box, and mask modalities while uniquely enabling textual guidance. We present DIFTracker, a framework that utilizes iterative latent variable diffusion models to redefine tracking as a next-frame reconstruction task. Our approach uniquely combines spatial and temporal dependencies in video data, offering a unified solution that encompasses existing tracking paradigms within a single Inversion-Reconstruction process. DIFTracker operates online and auto-regressively, enabling flexible instance-based video understanding. It allows us to overcome difficulties in variable-length video understanding encountered by video-inflated models and perform superior performance on seven benchmarks across five modalities. This paper not only introduces a new perspective on visual autoregressive modeling in understanding sequential visual data, specifically videos, but also provides robust theoretical validations and demonstrates broader applications in visual tracking and computer vision.},
  archive      = {J_IJCV},
  author       = {Nguyen, Pha and Madhok, Rishi and Raj, Bhiksha and Luu, Khoa},
  doi          = {10.1007/s11263-025-02439-x},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5505-5526},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Autoregressive temporal modeling for advanced tracking-by-diffusion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RGB-D visual perception for occluded scenes via event camera. <em>IJCV</em>, <em>133</em>(8), 5483-5504. (<a href='https://doi.org/10.1007/s11263-025-02438-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the first RGB-D visual perception method and dataset for densely occluded scenes. Under such dense occlusion scenarios, existing synthetic aperture imaging methods could only recover the 2D appearance of the target scene. In contrast, our proposed method could see through dense foreground occlusions and recover both the 2D appearance and 3D structure of the target scene, which is more beneficial for downstream applications. To achieve this, our proposed method takes the occluded frames and event stream captured by a moving event camera as inputs, which could provide sufficient visual information about the densely occluded scene due to the high temporal resolution of the event camera. To tackle the noise interference caused by dense foreground occlusions, an occlusion segmentation module with the guidance of event epipolar-plane images is proposed to predict occlusion masks of input occluded frames and event stream. Then, invalid occlusions are excluded according to the predicted masks, and valid visual features are extracted to simultaneously predict the appearance and structure of the target scene. A lightweight high-order conditional random fields module is proposed to model multi-pixel higher-order correlations, making pixels with similar color and structure have smoother features. A cross-modal edge consistency mechanism is proposed to achieve consistent RGB-D visual perception. In addition, we construct a hybrid vision acquisition system and collect the first Event-enhanced Occluded scene RGB-D Visual Perception dataset, named $$\hbox {THU}^\text {E-OccVP}$$ , which will be released as the first RGB-D visual perception benchmark for densely occluded scenes. Experimental results show that our proposed framework achieves significantly superior results over other baseline solutions, and the ablation experiments further demonstrate the effectiveness of each proposed module.},
  archive      = {J_IJCV},
  author       = {Li, Siqi and Wu, Zongze and Li, Yipeng and Xue, Zhou and Liu, Yu-Shen and Gao, Yue},
  doi          = {10.1007/s11263-025-02438-y},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5483-5504},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RGB-D visual perception for occluded scenes via event camera},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An information theory-inspired strategy for automated network pruning. <em>IJCV</em>, <em>133</em>(8), 5455-5482. (<a href='https://doi.org/10.1007/s11263-025-02437-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite superior performance achieved on many computer vision tasks, deep neural networks demand high computing power and memory footprint. Most existing network pruning methods require laborious human efforts and prohibitive computation resources, especially when the constraints are changed. This practically limits the application of model compression when the model needs to be deployed on a wide range of devices. Besides, existing methods are still challenged by the missing theoretical guidance, which lacks influence on the generalization error. In this paper we propose an information theory-inspired strategy for automated network pruning. The principle behind our method is the information bottleneck theory. Concretely, we introduce a new theorem to illustrate that the hidden representation should compress information with each other to achieve a better generalization. In this way, we further introduce the normalized Hilbert-Schmidt Independence Criterion on network activations as a stable and generalized indicator to construct layer importance. When a certain resource constraint is given, we integrate the HSIC indicator with the constraint to transform the architecture search problem into a linear programming problem with quadratic constraints. Such a problem is easily solved by a convex optimization method within a few seconds. We also provide rigorous proof to reveal that optimizing the normalized HSIC simultaneously minimizes the mutual information between different layers. Without any search process, our method achieves better compression trade-offs compared to the state-of-the-art compression algorithms. For instance, on ResNet-50, we achieve a 45.3%-FLOPs reduction, with a 75.75 top-1 accuracy on ImageNet. Codes are available at https://github.com/MAC-AutoML/ITPruner .},
  archive      = {J_IJCV},
  author       = {Zheng, Xiawu and Ma, Yuexiao and Xi, Teng and Zhang, Gang and Ding, Errui and Li, Yuchao and Chen, Jie and Tian, Yonghong and Ji, Rongrong},
  doi          = {10.1007/s11263-025-02437-z},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5455-5482},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An information theory-inspired strategy for automated network pruning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image captions are natural prompts for training data synthesis. <em>IJCV</em>, <em>133</em>(8), 5435-5454. (<a href='https://doi.org/10.1007/s11263-025-02436-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Artificial Intelligence Generated Content, it has become a common practice to train models on synthetic data due to data-scarcity and privacy leakage problems. Owing to massive and diverse information conveyed in real images, it is challenging for text-to-image generative models to synthesize informative training data with hand-crafted prompts. Considering the impressive ability of large generative models, could such models directly synthesize good training images for prediction tasks with proper prompts? We offer an affirmative response to this question by proposing a simple yet effective method, validated through ImageNet classification. Specifically, we caption each real image with the advanced captioning model to obtain informative and faithful prompts that extract class-relevant information and clarify the polysemy of class names. The image captions and class names are concatenated to prompt generative models for training image synthesis. We show that this simple caption incorporation significantly boosts the informativeness of synthetic data therefore enhancing downstream model generalization. More importantly, besides improvements in data augmentation and privacy preservation, our experiments demonstrate that synthesized images can exceed real data in terms of out-of-distribution robustness.},
  archive      = {J_IJCV},
  author       = {Lei, Shiye and Chen, Hao and Zhang, Sen and Zhao, Bo and Tao, Dacheng},
  doi          = {10.1007/s11263-025-02436-0},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5435-5454},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Image captions are natural prompts for training data synthesis},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Paragraph-to-image generation with information-enriched diffusion model. <em>IJCV</em>, <em>133</em>(8), 5413-5434. (<a href='https://doi.org/10.1007/s11263-025-02435-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image models have recently experienced rapid development, achieving astonishing performance in terms of fidelity and textual alignment capabilities. However, given a long paragraph (up to 512 words), these generation models still struggle to achieve strong alignment and are unable to generate images depicting complex scenes. In this paper, we introduce an information-enriched diffusion model for paragraph-to-image generation task, termed ParaDiffusion, which delves into the transference of the extensive semantic comprehension capabilities of large language models to the task of image generation. At its core is using a large language model (e.g., Llama V2) to encode long-form text, followed by fine-tuning with LoRA to align the text-image feature spaces in the generation task. To facilitate the training of long-text semantic alignment, we also curated a high-quality paragraph-image pair dataset, namely ParaImage. This dataset contains a small amount of high-quality, meticulously annotated data, and a large-scale synthetic dataset with long text descriptions being generated using a vision-language model. Experiments demonstrate that ParaDiffusion outperforms state-of-the-art models (SD XL, DeepFloyd IF) on ViLG-300 and ParaPrompts, achieving up to $$45\%$$ human voting rate improvements for text faithfulness. Code and data can be found at: https://github.com/weijiawu/ParaDiffusion .},
  archive      = {J_IJCV},
  author       = {Wu, Weijia and Li, Zhuang and He, Yefei and Shou, Mike Zheng and Shen, Chunhua and Cheng, Lele and Li, Yan and Gao, Tingting and Zhang, Di},
  doi          = {10.1007/s11263-025-02435-1},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5413-5434},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Paragraph-to-image generation with information-enriched diffusion model},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-adaptive weight-ensembling for multi-task model fusion. <em>IJCV</em>, <em>133</em>(8), 5396-5412. (<a href='https://doi.org/10.1007/s11263-025-02434-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating a multi-task model by merging models for distinct tasks has proven to be an economical and scalable approach. Recent research, like task arithmetic, demonstrates that a static solution for multi-task model fusion can be located within the vector space spanned by task vectors. However, the static nature of these methods limits their ability to adapt to the intricacies of individual instances, thereby hindering their performance in complex scenarios. To overcome this limitation, we propose a data-adaptive weight-ensembling approach that generates model weights in time. Specifically, we first feed the input samples into a hypernetwork to generate instance-specific weights for the primary model. Subsequently, we perform a functional call on the primary large model with the instance-specific weights. By generating model weights in time, the unified model gains increased flexibility and can resolve potential weight conflicts between tasks. Building upon this adaptability, our method necessitates solely the model checkpoints and unlabeled test samples using test-time adaptation training. We primarily conduct extensive experiments on vision Transformers and Flan-T5 models, demonstrating superior performance and satisfactory zero-shot transferability.},
  archive      = {J_IJCV},
  author       = {Tang, Anke and Shen, Li and Luo, Yong and Liu, Shiwei and Hu, Han and Du, Bo and Tao, Dacheng},
  doi          = {10.1007/s11263-025-02434-2},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5396-5412},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Data-adaptive weight-ensembling for multi-task model fusion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HiLM-D: Enhancing MLLMs with multi-scale high-resolution details for autonomous driving. <em>IJCV</em>, <em>133</em>(8), 5379-5395. (<a href='https://doi.org/10.1007/s11263-025-02433-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent efforts to use natural language for interpretable driving focus mainly on planning, neglecting perception tasks. In this paper, we address this gap by introducing ROLISP (Risk Object Localization and Intention and Suggestion Prediction), which towards interpretable risk object detection and suggestion for ego car motions. Accurate ROLISP implementation requires extensive reasoning to identify critical traffic objects and infer their intentions, prompting us to explore the capabilities of multimodal large language models (MLLMs). However, the limited perception performance of CLIP-ViT vision encoders in existing MLLMs struggles with capturing essential visual perception information, e.g., high-resolution, multi-scale and visual-related inductive biases, which are important for autonomous driving. Addressing these challenges, we introduce HiLM-D, a resource-efficient framework that enhances visual information processing in MLLMs for ROLISP. Our method is motivated by the fact that the primary variations in autonomous driving scenarios are the motion trajectories rather than the semantic or appearance information (e.g., the shapes and colors) of objects. Hence, the visual process of HiLM-D is a two-stream framework: (i) a temporal reasoning stream, receiving low-resolution dynamic video content, to capture temporal semantics, and (ii) a spatial perception stream, receiving a single high-resolution frame, to capture holistic visual perception-related information. The spatial perception stream can be made very lightweight by a well-designed P-Adapter, which is lightweight, training-efficient, and easily integrated into existing MLLMs. Experiments on the DRAMA-ROLISP dataset show HiLM-D’s significant improvements over current MLLMs, with a $$3.7\%$$ in BLEU-4 for captioning and $$8.7\%$$ in mIoU for detection. Further tests on the Shikra-RD dataset confirm our method’s generalization capabilities. The DRAMA-ROLISP is available at https://github.com/xmed-lab/HiLM-D .},
  archive      = {J_IJCV},
  author       = {Ding, Xinpeng and Han, Jianhua and Xu, Hang and Zhang, Wei and Li, Xiaomeng},
  doi          = {10.1007/s11263-025-02433-3},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5379-5395},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HiLM-D: Enhancing MLLMs with multi-scale high-resolution details for autonomous driving},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $$\text {A}^2\text {M}^2$$ -net: Adaptively aligned multi-scale moment for few-shot action recognition. <em>IJCV</em>, <em>133</em>(8), 5363-5378. (<a href='https://doi.org/10.1007/s11263-025-02432-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to capability to alleviate the cost of large-scale annotation, few-shot action recognition (FSAR) has attracted increased attention of researchers in recent years. Existing FSAR approaches typically neglect the role of individual motion pattern in comparison, and under-explore the feature statistics for video dynamics. Thereby, they struggle to handle the challenging temporal misalignment in video dynamics, particularly by using 2D backbones. To overcome these limitations, this work proposes an adaptively aligned multi-scale second-order moment network, namely $$\text {A}^2\text {M}^2$$ -Net, to describe the latent video dynamics with a collection of powerful representation candidates and adaptively align them in an instance-guided manner. To this end, our $$\text {A}^2\text {M}^2$$ -Net involves two core components, namely, adaptive alignment ( $$\text {A}^2$$ module) for matching, and multi-scale second-order moment ( $$\text {M}^2$$ block) for strong representation. Specifically, $$\text {M}^2$$ block develops a collection of semantic second-order descriptors at multiple spatio-temporal scales. Furthermore, $$\text {A}^2$$ module aims to adaptively select informative candidate descriptors while considering the individual motion pattern. By such means, our $$\text {A}^2\text {M}^2$$ -Net is able to handle the challenging temporal misalignment problem by establishing an adaptive alignment protocol for strong representation. Notably, our proposed method generalizes well to various few-shot settings and diverse metrics. The experiments are conducted on five widely used FSAR benchmarks, and the results show our $$\text {A}^2\text {M}^2$$ -Net achieves very competitive performance compared to state-of-the-arts, demonstrating its effectiveness and generalization.},
  archive      = {J_IJCV},
  author       = {Gao, Zilin and Wang, Qilong and Zhang, Bingbing and Hu, Qinghua and Li, Peihua},
  doi          = {10.1007/s11263-025-02432-4},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5363-5378},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {$$\text {A}^2\text {M}^2$$ -net: Adaptively aligned multi-scale moment for few-shot action recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DocScanner: Robust document image rectification with progressive learning. <em>IJCV</em>, <em>133</em>(8), 5343-5362. (<a href='https://doi.org/10.1007/s11263-025-02431-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with flatbed scanners, portable smartphones provide more convenience for physical document digitization. However, such digitized documents are often distorted due to uncontrolled physical deformations, camera positions, and illumination variations. To this end, we present DocScanner, a novel framework for document image rectification. Different from existing solutions, DocScanner addresses this issue by introducing a progressive learning mechanism. Specifically, DocScanner maintains a single estimate of the rectified image, which is progressively corrected with a recurrent architecture. The iterative refinements make DocScanner converge to a robust and superior rectification performance, while the lightweight recurrent architecture ensures the running efficiency. To further improve the rectification quality, based on the geometric priori between the distorted and the rectified images, a geometric constraint is introduced during training to further improve the performance. Extensive experiments are conducted on the Doc3D dataset and the DocUNet Benchmark dataset, and the quantitative and qualitative evaluation results verify the effectiveness of DocScanner, which outperforms previous methods on OCR accuracy, image similarity, and our proposed distortion metric by a considerable margin. Furthermore, our DocScanner shows superior efficiency in runtime latency and model size. The codes and pre-trained models are available at https://github.com/fh2019ustc/DocScanner .},
  archive      = {J_IJCV},
  author       = {Feng, Hao and Zhou, Wengang and Deng, Jiajun and Tian, Qi and Li, Houqiang},
  doi          = {10.1007/s11263-025-02431-5},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5343-5362},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DocScanner: Robust document image rectification with progressive learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P2P: Part-to-part motion cues guide a strong tracking framework for LiDAR point clouds. <em>IJCV</em>, <em>133</em>(8), 5326-5342. (<a href='https://doi.org/10.1007/s11263-025-02430-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D single object tracking (SOT) methods based on appearance matching has long suffered from insufficient appearance information incurred by incomplete, textureless and semantically deficient LiDAR point clouds. While motion paradigm exploits motion cues instead of appearance matching for tracking, it incurs complex multi-stage processing and segmentation module. In this paper, we first provide in-depth explorations on motion paradigm, which proves that (i) it is feasible to directly infer target relative motion from point clouds across consecutive frames; (ii) fine-grained information comparison between consecutive point clouds facilitates target motion modeling. We thereby propose to perform part-to-part motion modeling for consecutive point clouds and introduce a novel tracking framework, termed P2P. The novel framework fuses each corresponding part information between consecutive point clouds, effectively exploring detailed information changes and thus modeling accurate target-related motion cues. Following this framework, we present P2P-point and P2P-voxel models, incorporating implicit and explicit part-to-part motion modeling by point- and voxel-based representation, respectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art performance ( $$\sim $$ 89%, 72% and 63% precision on KITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same point-based representation, P2P-point outperforms the previous motion tracker M $$^2$$ Track by 3.3% and 6.7% on the KITTI and NuScenes, while running at a considerably high speed of 107 Fps on a single RTX3090 GPU. The source code and pre-trained models are available at https://github.com/haooozi/P2P .},
  archive      = {J_IJCV},
  author       = {Nie, Jiahao and Xie, Fei and Zhou, Sifan and Zhou, Xueyi and Chae, Dong-Kyu and He, Zhiwei},
  doi          = {10.1007/s11263-025-02430-6},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5326-5342},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {P2P: Part-to-part motion cues guide a strong tracking framework for LiDAR point clouds},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effectively leveraging CLIP for generating situational summaries of images and videos. <em>IJCV</em>, <em>133</em>(8), 5302-5325. (<a href='https://doi.org/10.1007/s11263-025-02429-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Situation recognition refers to the ability of an agent to identify and understand various situations or contexts based on available information and sensory inputs. It involves the cognitive process of interpreting data from the environment to determine what is happening, what factors are involved, and what actions caused those situations. This interpretation of situations is formulated as a semantic role labeling problem in computer vision-based situation recognition. Situations depicted in images and videos hold pivotal information, essential for various applications like image and video captioning, multimedia retrieval, autonomous systems and event monitoring. However, existing methods often struggle with ambiguity and lack of context in generating meaningful and accurate predictions. Leveraging multimodal models such as CLIP, we propose ClipSitu, which sidesteps the need for full fine-tuning and achieves state-of-the-art results in situation recognition and localization tasks. ClipSitu harnesses CLIP-based image, verb, and role embeddings to predict nouns fulfilling all the roles associated with a verb, providing a comprehensive understanding of depicted scenarios. Through a cross-attention transformer, ClipSitu XTF enhances the connection between semantic role queries and visual token representations, leading to superior performance in situation recognition. We also propose a verb-wise role prediction model with near-perfect accuracy to create an end-to-end framework for producing situational summaries for out-of-domain images. We show that situational summaries empower our ClipSitu models to produce structured descriptions with reduced ambiguity compared to generic captions. Finally, we extend ClipSitu to video situation recognition to showcase its versatility and produce comparable performance to state-of-the-art methods. In summary, ClipSitu offers a robust solution to the challenge of semantic role labeling providing a way for structured understanding of visual media. ClipSitu advances the state-of-the-art in situation recognition, paving the way for a more nuanced and contextually relevant understanding of visual content that potentially could derive meaningful insights about the environment that agents observe. Code is available at https://github.com/LUNAProject22/CLIPSitu .},
  archive      = {J_IJCV},
  author       = {Verma, Dhruv and Roy, Debaditya and Fernando, Basura},
  doi          = {10.1007/s11263-025-02429-z},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5302-5325},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Effectively leveraging CLIP for generating situational summaries of images and videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SwinTextSpotter v2: Towards better synergy for scene text spotting. <em>IJCV</em>, <em>133</em>(8), 5281-5301. (<a href='https://doi.org/10.1007/s11263-025-02428-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end scene text spotting, which aims to read the text in natural images, has garnered significant attention in recent years. However, recent state-of-the-art methods usually incorporate detection and recognition simply by sharing the backbone, which does not directly take advantage of the feature interaction between the two tasks. In this paper, we propose a new end-to-end scene text spotting framework termed SwinTextSpotter v2, which seeks to find a better synergy between text detection and recognition. Specifically, we enhance the relationship between two tasks using novel Recognition Conversion and Recognition Alignment modules. Recognition Conversion explicitly guides text localization through recognition loss, while Recognition Alignment dynamically extracts text features for recognition through the detection predictions. This simple yet effective design results in a concise framework that requires neither an additional rectification module nor character-level annotations for the arbitrarily-shaped text. Furthermore, the parameters of the detector are greatly reduced without performance degradation by introducing a Box Selection Schedule. Qualitative and quantitative experiments demonstrate that SwinTextSpotter v2 achieves state-of-the-art performance on various multilingual (English, Chinese, and Vietnamese) benchmarks. The code will be available at https://github.com/mxin262/SwinTextSpotterv2 .},
  archive      = {J_IJCV},
  author       = {Huang, Mingxin and Peng, Dezhi and Li, Hongliang and Peng, Zhenghao and Liu, Chongyu and Lin, Dahua and Liu, Yuliang and Bai, Xiang and Jin, Lianwen},
  doi          = {10.1007/s11263-025-02428-0},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5281-5301},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SwinTextSpotter v2: Towards better synergy for scene text spotting},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). C2RF: Bridging multi-modal image registration and fusion via commonality mining and contrastive learning. <em>IJCV</em>, <em>133</em>(8), 5262-5280. (<a href='https://doi.org/10.1007/s11263-025-02427-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image fusion methods are typically only applicable to strictly aligned source images, and they introduce undesirable artifacts when source images are misaligned, compromising visual perception and downstream applications. In this work, we propose a mutually promoting multi-modal image registration and fusion framework based on commonality mining and contrastive learning, named C2RF. We adaptively decompose multi-modal images into modality-invariant common features and modality-specific unique features. Effective disentanglement not only reduces the difficulty of cross-modal registration but also facilitates purposeful information aggregation. Moreover, C2RF incorporates fusion-based contrastive learning to explicitly model the requirements of fusion on registration, which breaks the dilemma that registration and fusion are independent of each other. The aligned and misaligned fusion results act as positive and negative samples to guide registration optimization. Particularly, negative samples generated with hard negative sample mining enable our fusion results away from artifacts. Extensive experiments demonstrate that C2RF outperforms other competitors in both multi-modal image registration and fusion, notably in bolstering the robustness of image fusion to misalignment. The source code has been released at https://github.com/QinglongYan-hub/C2RF .},
  archive      = {J_IJCV},
  author       = {Tang, Linfeng and Yan, Qinglong and Xiang, Xinyu and Fang, Leyuan and Ma, Jiayi},
  doi          = {10.1007/s11263-025-02427-1},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5262-5280},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {C2RF: Bridging multi-modal image registration and fusion via commonality mining and contrastive learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D3T: Dual-domain diffusion transformer in triplanar latent space for 3D incomplete-view CT reconstruction. <em>IJCV</em>, <em>133</em>(8), 5238-5261. (<a href='https://doi.org/10.1007/s11263-025-02426-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) is a cornerstone of clinical imaging, yet its accessibility in certain scenarios is constrained by radiation exposure concerns and operational limitations within surgical environments. CT reconstruction from incomplete views has attracted increasing research attention due to its great potential in medical applications. However, it is inherently an ill-posed problem, which, coupled with the complex, high-dimensional characteristics of 3D medical data, poses great challenges such as artifact mitigation, global incoherence, and high computational costs. To tackle those challenges, this paper introduces D3T, a new 3D conditional diffusion transformer that models 3D CT distributions in the low-dimensional 2D latent space for incomplete-view CT reconstruction. Our approach comprises two primary components: a triplanar vector quantized auto-encoder (TriVQAE) and a latent dual-domain diffusion transformer (LD3T). TriVQAE encodes high-resolution 3D CT images into compact 2D latent triplane codes which effectively factorize the intricate CT structures, further enabling compute-friendly diffusion model architecture design. Operating in the latent triplane space, LD3T significantly reduces the complexity of capturing the intricate structures in CT images. Its improved diffusion transformer architecture efficiently understands the global correlations across the three planes, ensuring high-fidelity 3D reconstructions. LD3T presents a new dual-domain conditional generation pipeline that incorporates both image and projection conditions, facilitating controllable reconstruction to produce 3D structures consistent with the given conditions. Moreover, LD3T introduces a new Dual-Space Consistency Loss that integrates image-level supervision beyond standard supervision in the latent space to enhance consistency in the 3D image space. Extensive experiments on four datasets with three inverse settings demonstrate the effectiveness of our proposal.},
  archive      = {J_IJCV},
  author       = {Liu, Xuhui and Li, Hong and Qiao, Zhi and Huang, Yawen and Liu, Xi and Zhang, Juan and Qian, Zhen and Zhen, Xiantong and Zhang, Baochang},
  doi          = {10.1007/s11263-025-02426-2},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5238-5261},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {D3T: Dual-domain diffusion transformer in triplanar latent space for 3D incomplete-view CT reconstruction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast and lightweight 3D keypoint detector. <em>IJCV</em>, <em>133</em>(8), 5216-5237. (<a href='https://doi.org/10.1007/s11263-025-02425-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keypoint detection is crucial in many visual tasks, such as object recognition, shape retrieval, and 3D reconstruction, as labeling point data is labor-intensive or sometimes implausible. Nevertheless, it is challenging to quickly and accurately locate keypoints unsupervised from point clouds. This work proposes a fast and lightweight 3D keypoint detector that can efficiently and accurately detect keypoints from point clouds. Our method does not require a complex model learning process and generalizes well to new scenes. Specifically, we consider detecting keypoints a saliency detection problem for a point cloud. First, we propose a simple and effective distance measure to characterize the saliency of points in a point cloud. This distance describes geometrically essential points in the point cloud. Next, we present a regional saliency based on relative centroid distance representation that can globally characterize keypoints with regional visual information. Third, we combine geometric and semantic cues to generate a saliency map of the point cloud for determining stable 3D keypoints. We evaluate our method against existing approaches on four benchmark keypoint datasets to demonstrate its state-of-the-art performance.},
  archive      = {J_IJCV},
  author       = {Yang, Chengzhuan and Yu, Qian and Wei, Hui and Wu, Fei and Jiang, Yunliang and Zheng, Zhonglong and Yang, Ming-Hsuan},
  doi          = {10.1007/s11263-025-02425-3},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5216-5237},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A fast and lightweight 3D keypoint detector},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Creatively upscaling images with global-regional priors. <em>IJCV</em>, <em>133</em>(8), 5197-5215. (<a href='https://doi.org/10.1007/s11263-025-02424-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary diffusion models show remarkable capability in text-to-image generation, while still being limited to restricted resolutions (e.g., $$1024\times 1024$$ ). Recent advances enable tuning-free higher-resolution image generation by recycling pre-trained diffusion models and extending them via regional denoising or dilated sampling/convolutions. However, these models struggle to simultaneously preserve global semantic structure and produce creative regional details in higher-resolution images. To address this, we present C-Upscale, a new recipe of tuning-free image upscaling that pivots on global-regional priors derived from given global prompt and estimated regional prompts via Multimodal LLM. Technically, the low-frequency component of low-resolution image is recognized as global structure prior to encourage global semantic consistency in high-resolution generation. Next, we perform regional attention control to screen cross-attention between global prompt and each region during regional denoising, leading to regional attention prior that alleviates object repetition issue. The estimated regional prompts containing rich descriptive details further act as regional semantic prior to fuel the creativity of regional detail generation. Both quantitative and qualitative evaluations demonstrate that our C-Upscale manages to generate ultra-high-resolution images (e.g., $$4096\times 4096 \,{\text {and}}\, 8192\times 8192$$ ) with higher visual fidelity and more creative regional details.},
  archive      = {J_IJCV},
  author       = {Qian, Yurui and Cai, Qi and Pan, Yingwei and Yao, Ting and Mei, Tao},
  doi          = {10.1007/s11263-025-02424-4},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5197-5215},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Creatively upscaling images with global-regional priors},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AvatarStudio: High-fidelity and animatable 3D avatar creation from text. <em>IJCV</em>, <em>133</em>(8), 5178-5196. (<a href='https://doi.org/10.1007/s11263-025-02423-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of creating high-fidelity and animatable 3D avatars from only textual descriptions. Existing text-to-avatar methods are either limited to static avatars which cannot be animated or struggle to generate animatable avatars with promising quality and precise pose control. To address these limitations, we propose AvatarStudio, a generative model that yields explicit textured 3D meshes for animatable human avatars. Specifically, AvatarStudio proposes to incorporate articulation modeling into the explicit mesh representation to support high-resolution rendering and avatar animation. To ensure view consistency and pose controllability of the resulting avatars, we introduce a simple-yet-effective 2D diffusion model conditioned on DensePose for Score Distillation Sampling supervision. By effectively leveraging the synergy between the articulated mesh representation and DensePose-conditional diffusion model, AvatarStudio can create high-quality avatars from text ready for animation. Furthermore, it is competent for many applications, e.g., multimodal avatar animations and style-guided avatar creation. Please refer to our project page for more results.},
  archive      = {J_IJCV},
  author       = {Zhang, Xuanmeng and Zhang, Jianfeng and Zhang, Chenxu and Liew, Jun Hao and Zhang, Huichao and Yang, Yi and Feng, Jiashi},
  doi          = {10.1007/s11263-025-02423-5},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5178-5196},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AvatarStudio: High-fidelity and animatable 3D avatar creation from text},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SimZSL: Zero-shot learning beyond a pre-defined semantic embedding space. <em>IJCV</em>, <em>133</em>(8), 5161-5177. (<a href='https://doi.org/10.1007/s11263-025-02422-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot recognition is centered around learning representations to transfer knowledge from seen to unseen classes. Where foundational approaches perform the transfer with semantic embedding spaces, e.g., from attributes or word vectors, the current state-of-the-art relies on prompting pre-trained vision-language models to obtain class embeddings. Whether zero-shot learning is performed with attributes, CLIP, or something else, current approaches de facto assume that there is a pre-defined embedding space in which seen and unseen classes can be positioned. Our work is concerned with real-world zero-shot settings where a pre-defined embedding space can no longer be assumed. This is natural in domains such as biology and medicine, where class names are not common English words, rendering vision-language models useless; or neuroscience, where class relations are only given with non-semantic human comparison scores. We find that there is one data structure enabling zero-shot learning in both standard and non-standard settings: a similarity matrix spanning the seen and unseen classes. We introduce four similarity-based zero-shot learning challenges, tackling open-ended scenarios such as learning with uncommon class names, learning from multiple partial sources, and learning with missing knowledge. As the first step for zero-shot learning beyond a pre-defined semantic embedding space, we propose $$\kappa $$ -MDS, a general approach that obtains a prototype for each class on any manifold from similarities alone, even when part of the similarities are missing. Our approach can be plugged into any standard, hyperspherical, or hyperbolic zero-shot learner. Experiments on existing datasets and the new benchmarks show the promise and challenges of similarity-based zero-shot learning.},
  archive      = {J_IJCV},
  author       = {Atigh, Mina Ghadimi and Nargang, Stephanie and Keller-Ressel, Martin and Mettes, Pascal},
  doi          = {10.1007/s11263-025-02422-6},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5161-5177},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SimZSL: Zero-shot learning beyond a pre-defined semantic embedding space},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segment anything in 3D with radiance fields. <em>IJCV</em>, <em>133</em>(8), 5138-5160. (<a href='https://doi.org/10.1007/s11263-025-02421-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Segment Anything Model (SAM) emerges as a powerful vision foundation model to generate high-quality 2D segmentation results. This paper aims to generalize SAM to segment 3D objects. Rather than replicating the data acquisition and annotation procedure which is costly in 3D, we design an efficient solution, leveraging the radiance field as a cheap and off-the-shelf prior that connects multi-view 2D images to the 3D space. We refer to the proposed solution as SA3D, short for Segment Anything in 3D. With SA3D, the user is only required to provide a 2D segmentation prompt (e.g., rough points) for the target object in a single view, which is used to generate its corresponding 2D mask with SAM. Next, SA3D alternately performs mask inverse rendering and cross-view self-prompting across various views to iteratively refine the 3D mask of the target object. For one view, mask inverse rendering projects the 2D mask obtained by SAM into the 3D space with guidance of the density distribution learned by the radiance field for 3D mask refinement. Then, cross-view self-prompting extracts reliable prompts automatically as the input to SAM from the rendered 2D mask of the inaccurate 3D mask for a new view. We show in experiments that SA3D adapts to various scenes and achieves 3D segmentation within seconds. Our research reveals a potential methodology to lift the ability of a 2D segmentation model to 3D. Our code is available at https://github.com/Jumpat/SegmentAnythingin3D .},
  archive      = {J_IJCV},
  author       = {Cen, Jiazhong and Fang, Jiemin and Zhou, Zanwei and Yang, Chen and Xie, Lingxi and Zhang, Xiaopeng and Shen, Wei and Tian, Qi},
  doi          = {10.1007/s11263-025-02421-7},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5138-5160},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Segment anything in 3D with radiance fields},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Free lunch to meet the gap: Intermediate domain reconstruction for cross-domain few-shot learning. <em>IJCV</em>, <em>133</em>(8), 5118-5137. (<a href='https://doi.org/10.1007/s11263-025-02419-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain few-shot learning (CDFSL) endeavors to transfer generalized knowledge from the source domain to target domains using only a minimal amount of training data, which faces a triplet of learning challenges in the meantime, i.e., semantic disjoint, large domain discrepancy, and data scarcity. Different from predominant CDFSL works focused on generalized representations, we make novel attempts to construct intermediate domain proxies (IDP) with source feature embeddings as the codebook and reconstruct the target domain feature with this learned codebook. We then conduct an empirical study to explore the intrinsic attributes from perspectives of visual styles and semantic contents in intermediate domain proxies. Reaping benefits from these attributes of intermediate domains, we develop a fast domain alignment method to use these proxies as learning guidance for target domain feature transformation. With the collaborative learning of intermediate domain reconstruction and target feature transformation, our proposed model is able to surpass the state-of-the-art models by a margin on 8 cross-domain few-shot learning benchmarks. Our code and models will be publicly available.},
  archive      = {J_IJCV},
  author       = {Zhang, Tong and Zhao, Yifan and Wang, Liangyu and Li, Jia},
  doi          = {10.1007/s11263-025-02419-1},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5118-5137},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Free lunch to meet the gap: Intermediate domain reconstruction for cross-domain few-shot learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NU-AIR: A neuromorphic urban aerial dataset for detection and localization of pedestrians and vehicles. <em>IJCV</em>, <em>133</em>(8), 5099-5117. (<a href='https://doi.org/10.1007/s11263-025-02418-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an open-source aerial neuromorphic dataset that captures pedestrians and vehicles moving in an urban environment. The dataset, titled NU-AIR, features over 70 min of event footage acquired with a 640 $$\times $$ 480 resolution neuromorphic sensor mounted on a quadrotor operating in an urban environment. Crowds of pedestrians, different types of vehicles, and street scenes featuring busy urban environments are captured at different elevations and illumination conditions. Manual bounding box annotations of vehicles and pedestrians contained in the recordings are provided at a frequency of 30 Hz, yielding more than 93,000 labels in total. A baseline evaluation for this dataset was performed using three Spiking Neural Networks (SNNs) and ten Deep Neural Networks (DNNs). All data and Python code to voxelize the data and subsequently train SNNs/DNNs has been open-sourced.},
  archive      = {J_IJCV},
  author       = {Iaboni, Craig and Kelly, Thomas and Abichandani, Pramod},
  doi          = {10.1007/s11263-025-02418-2},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5099-5117},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {NU-AIR: A neuromorphic urban aerial dataset for detection and localization of pedestrians and vehicles},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion-enhanced test-time adaptation with text and image augmentation. <em>IJCV</em>, <em>133</em>(8), 5083-5098. (<a href='https://doi.org/10.1007/s11263-025-02412-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing test-time prompt tuning (TPT) methods focus on single-modality data, primarily enhancing images and using confidence ratings to filter out inaccurate images. However, while image generation models can produce visually diverse images, single-modality data enhancement techniques still fail to capture the comprehensive knowledge provided by different modalities. Additionally, we note that the performance of TPT-based methods drops significantly when the number of augmented images is limited, which is not unusual given the computational expense of generative augmentation. To address these issues, we introduce $$\text {IT}^{3}\text {A}$$ , a novel test-time adaptation method that utilizes a pre-trained generative model for multi-modal augmentation of each test sample from unknown new domains. By combining augmented data from pre-trained vision and language models, we enhance the ability of the model to adapt to unknown new test data. Additionally, to ensure that key semantics are accurately retained when generating various visual and text enhancements, we employ cosine similarity filtering between the logits of the enhanced images and text with the original test data. This process allows us to filter out some spurious augmentation and inadequate combinations. To leverage the diverse enhancements provided by the generation model across different modals, we have replaced prompt tuning with an adapter for greater flexibility in utilizing text templates. Our experiments on the test datasets with distribution shifts and domain gaps show that in a zero-shot setting, $$\text {IT}^{3}\text {A}$$ outperforms state-of-the-art test-time prompt tuning methods with a 5.50% increase in accuracy.},
  archive      = {J_IJCV},
  author       = {Feng, Chun-Mei and He, Yuanyang and Zou, Jian and Khan, Salman and Xiong, Huan and Li, Zhen and Zuo, Wangmeng and Goh, Rick Siow Mong and Liu, Yong},
  doi          = {10.1007/s11263-025-02412-8},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5083-5098},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Diffusion-enhanced test-time adaptation with text and image augmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Animal-CLIP: A dual-prompt enhanced vision-language model for animal action recognition. <em>IJCV</em>, <em>133</em>(8), 5062-5082. (<a href='https://doi.org/10.1007/s11263-025-02408-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animal action recognition has a wide range of applications. With the rise of visual-language pretraining models (VLMs), new possibilities have emerged for action recognition. However, while current VLMs perform well on human-centric videos, they still struggle with animal videos. This is primarily due to the lack of domain-specific knowledge during model training and more pronounced intra-class variations compared to humans. To address these issues, we introduce Animal-CLIP, a specialized and efficient animal action recognition framework built upon existing VLMs. To address the lack of domain-specific knowledge in animal actions, we leverage the extensive expertise of large language models (LLMs) to automatically generate external prompts, thereby expanding the semantic scope of labels and enhancing the model’s generalization capability. To effectively integrate external knowledge into the model, we propose a knowledge-enhanced internal prompt fine-tuning approach. We design a text feature refinement module to reduce potential recognition inconsistencies. Furthermore, to address the high intra-class variation in animal actions, a novel category-specific prompting method is introduced to generate adaptive prompts to optimize the alignment between text and video features, facilitating more precise partitioning of the action space. Experimental results demonstrate that our method outperforms six previous action recognition methods across three large-scale multi-species, multi-action datasets and exhibits strong generalization capability on unseen animals.},
  archive      = {J_IJCV},
  author       = {Jing, Yinuo and Liang, Kongming and Zhang, Ruxu and Sun, Hao and Li, Yongxiang and He, Zhongjiang and Ma, Zhanyu},
  doi          = {10.1007/s11263-025-02408-4},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5062-5082},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Animal-CLIP: A dual-prompt enhanced vision-language model for animal action recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in 3D neural stylization: A survey. <em>IJCV</em>, <em>133</em>(8), 5026-5061. (<a href='https://doi.org/10.1007/s11263-025-02403-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern artificial intelligence offers a novel and transformative approach to creating digital art across diverse styles and modalities like images, videos and 3D data, unleashing the power of creativity and revolutionizing the way that we perceive and interact with visual content. This paper reports on recent advances in stylized 3D asset creation and manipulation with the expressive power of neural networks. We establish a taxonomy for neural stylization, considering crucial design choices such as scene representation, guidance data, optimization strategies, and output styles. Building on such taxonomy, our survey first revisits the background of neural stylization on 2D images, and then presents in-depth discussions on recent neural stylization methods for 3D data, accompanied by a benchmark evaluating selected mesh and neural field stylization methods. Based on the insights gained from the survey, we highlight the practical significance, open challenges, future research, and potential impacts of neural stylization, which facilitates researchers and practitioners to navigate the rapidly evolving landscape of 3D content creation using modern artificial intelligence.},
  archive      = {J_IJCV},
  author       = {Chen, Yingshu and Shao, Guocheng and Shum, Ka Chun and Hua, Binh-Son and Yeung, Sai-Kit},
  doi          = {10.1007/s11263-025-02403-9},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5026-5061},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Advances in 3D neural stylization: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A closer look at benchmarking self-supervised pre-training with image classification. <em>IJCV</em>, <em>133</em>(8), 5013-5025. (<a href='https://doi.org/10.1007/s11263-025-02402-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) is a machine learning approach where the data itself provides supervision, eliminating the need for external labels. The model is forced to learn about the data’s inherent structure or context by solving a pretext task. With SSL, models can learn from abundant and cheap unlabeled data, significantly reducing the cost of training models where labels are expensive or inaccessible. In Computer Vision, SSL is widely used as pre-training followed by a downstream task, such as supervised transfer, few-shot learning on smaller labeled data sets, and/or unsupervised clustering. Unfortunately, it is infeasible to evaluate SSL methods on all possible downstream tasks and objectively measure the quality of the learned representation. Instead, SSL methods are evaluated using in-domain evaluation protocols, such as fine-tuning, linear probing, and k-nearest neighbors (kNN). However, it is not well understood how well these evaluation protocols estimate the representation quality of a pre-trained model for different downstream tasks under different conditions, such as dataset, metric, and model architecture. In this work, we study how classification-based evaluation protocols for SSL correlate and how well they predict downstream performance on different dataset types. Our study includes eleven common image datasets and 26 models that were pre-trained with different SSL methods or have different model backbones. We find that in-domain linear/kNN probing protocols are, on average, the best general predictors for out-of-domain performance. We further investigate the importance of batch normalization for the various protocols and evaluate how robust correlations are for different kinds of dataset domain shifts. In addition, we challenge assumptions about the relationship between discriminative and generative self-supervised methods, finding that most of their performance differences can be explained by changes to model backbones.},
  archive      = {J_IJCV},
  author       = {Marks, Markus and Knott, Manuel and Kondapaneni, Neehar and Cole, Elijah and Defraeye, Thijs and Perez-Cruz, Fernando and Perona, Pietro},
  doi          = {10.1007/s11263-025-02402-w},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {5013-5025},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A closer look at benchmarking self-supervised pre-training with image classification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of representation learning, optimization strategies, and applications for omnidirectional vision. <em>IJCV</em>, <em>133</em>(8), 4973-5012. (<a href='https://doi.org/10.1007/s11263-025-02391-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional image (ODI) data is captured with a field-of-view of $$360^\circ \times 180^\circ $$ , which is much wider than the pinhole cameras and captures richer surrounding environment details than the conventional perspective images. In recent years, the availability of customer-level $$360^\circ $$ cameras has made omnidirectional vision more popular, and the advance of deep learning (DL) has significantly sparked its research and applications. This paper presents a systematic and comprehensive review and analysis of the recent progress of DL for omnidirectional vision. It delineates the distinct challenges and complexities encountered in applying DL to omnidirectional images as opposed to traditional perspective imagery. Our work covers four main contents: (i) A thorough introduction to the principles of omnidirectional imaging and commonly explored projections of ODI; (ii) A methodical review of varied representation learning approaches tailored for ODI; (iii) An in-depth investigation of optimization strategies specific to omnidirectional vision; (iv) A structural and hierarchical taxonomy of the DL methods for the representative omnidirectional vision tasks, from visual enhancement (e.g., image generation and super-resolution) to 3D geometry and motion estimation (e.g., depth and optical flow estimation), alongside the discussions on emergent research directions; (v) An overview of cutting-edge applications (e.g., autonomous driving and virtual reality), coupled with a critical discussion on prevailing challenges and open questions, to trigger more research in the community.},
  archive      = {J_IJCV},
  author       = {Ai, Hao and Cao, Zidong and Wang, Lin},
  doi          = {10.1007/s11263-025-02391-w},
  journal      = {International Journal of Computer Vision},
  month        = {8},
  number       = {8},
  pages        = {4973-5012},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A survey of representation learning, optimization strategies, and applications for omnidirectional vision},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preconditioned score-based generative models. <em>IJCV</em>, <em>133</em>(7), 4837-4863. (<a href='https://doi.org/10.1007/s11263-025-02410-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Score-based generative models (SGMs) have recently emerged as a promising class of generative models. However, a fundamental limitation is that their sampling process is slow due to a need for many (e.g., 2000) iterations of sequential computations. An intuitive acceleration method is to reduce the sampling iterations which however causes severe performance degradation. We assault this problem to the ill-conditioned issues of the Langevin dynamics and reverse diffusion in the sampling process. Under this insight, we propose a novel preconditioned diffusion sampling (PDS) method that leverages matrix preconditioning to alleviate the aforementioned problem. PDS alters the sampling process of a vanilla SGM at marginal extra computation cost and without model retraining. Theoretically, we prove that PDS preserves the output distribution of the SGM, with no risk of inducing systematical bias to the original sampling process. We further theoretically reveal a relation between the parameter of PDS and the sampling iterations, easing the parameter estimation under varying sampling iterations. Extensive experiments on various image datasets with a variety of resolutions and diversity validate that our PDS consistently accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In particular, PDS can accelerate by up to $$28\times $$ on more challenging high-resolution (1024 $$\times $$ 1024) image generation. Compared with the latest generative models (e.g., CLD-SGM, DDIM, and Analytic-DDIM), PDS can achieve the best sampling quality on CIFAR-10 at an FID score of 1.99. Our code is publicly available to foster any further research https://github.com/fudan-zvg/PDS .},
  archive      = {J_IJCV},
  author       = {Ma, Hengyuan and Zhu, Xiatian and Feng, Jianfeng and Zhang, Li},
  doi          = {10.1007/s11263-025-02410-w},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4837-4863},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Preconditioned score-based generative models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CT3D++: Improving 3D object detection with keypoint-induced channel-wise transformer. <em>IJCV</em>, <em>133</em>(7), 4817-4836. (<a href='https://doi.org/10.1007/s11263-025-02404-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of 3D object detection from point clouds is rapidly advancing in computer vision, aiming to accurately and efficiently detect and localize objects in three-dimensional space. Current 3D detectors commonly fall short in terms of flexibility and scalability, with ample room for advancements in performance. In this paper, our objective is to address these limitations by introducing two frameworks for 3D object detection. Firstly, we propose CT3D, which sequentially performs raw-point-based embedding, a standard Transformer encoder, and a channel-wise decoder for point features within each proposal. Secondly, we present an enhanced network called CT3D++, which incorporates geometric and semantic fusion-based embedding to extract more valuable and comprehensive proposal-aware information. Additionally, CT3D++ utilizes a point-to-key bidirectional encoder for more efficient feature encoding with reduced computational cost. By replacing the corresponding components of CT3D with these novel modules, CT3D++ achieves state-of-the-art performance on both the KITTI dataset and the large-scale Waymo Open Dataset. The source code for our frameworks will be made accessible at https://github.com/hlsheng1/CT3Dplusplus.},
  archive      = {J_IJCV},
  author       = {Sheng, Hualian and Cai, Sijia and Zhao, Na and Deng, Bing and Liang, Qiao and Zhao, Min-Jian and Ye, Jieping},
  doi          = {10.1007/s11263-025-02404-8},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4817-4836},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CT3D++: Improving 3D object detection with keypoint-induced channel-wise transformer},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully decoupled end-to-end person search: An approach without conflicting objectives. <em>IJCV</em>, <em>133</em>(7), 4795-4816. (<a href='https://doi.org/10.1007/s11263-025-02407-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end person search aims to jointly detect and re-identify a target person in raw scene images with a unified model. The detection sub-task learns to identify all persons as one category while the re-identification (re-id) sub-task aims to discriminate persons of different identities, resulting in conflicting optimal objectives. Existing works proposed to decouple end-to-end person search to alleviate such conflict. Yet these methods are still sub-optimal on the sub-tasks due to their partially decoupled models, which limits the overall person search performance. To further eliminate the last coupled part in decoupled models without sacrificing the efficiency of end-to-end person search, we propose a fully decoupled person search framework in this work. Specifically, we design a task-incremental network to construct an end-to-end model in a task-incremental learning procedure. Given that the detection subtask is easier, we start by training a lightweight detection sub-network and expand it with a re-id sub-network trained in another stage. On top of the fully decoupled design, we also enable one-stage training for the task-incremental network. The fully decoupled framework further allows an Online Representation Distillation to mitigate the representation gap between the end-to-end model and two-step models for learning robust representations. Without requiring an offline teacher re-id model, this transfers structured representational knowledge learned from cropped images to the person search model. The learned person representations thus focus more on discriminative clues of foreground persons and suppress the distractive background information. To understand the effectiveness and efficiency of the proposed method, we conduct comprehensive experimental evaluations on two popular person search datasets PRW and CUHK-SYSU. The experimental results demonstrate that the fully decoupled model achieves superior performance than previous decoupled methods. The inference of the model is also shown to be efficient among recent end-to-end methods. The source code is available at https://github.com/PatrickZad/fdps .},
  archive      = {J_IJCV},
  author       = {Zhang, Pengcheng and Yu, Xiaohan and Bai, Xiao and Zheng, Jin and Ning, Xin and Hancock, Edwin R.},
  doi          = {10.1007/s11263-025-02407-5},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4795-4816},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fully decoupled end-to-end person search: An approach without conflicting objectives},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointSea: Point cloud completion via self-structure augmentation. <em>IJCV</em>, <em>133</em>(7), 4770-4794. (<a href='https://doi.org/10.1007/s11263-025-02400-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion is a fundamental yet not well-solved problem in 3D vision. Current approaches often rely on 3D coordinate information and/or additional data (e.g., images and scanning viewpoints) to fill in missing parts. Unlike these methods, we explore self-structure augmentation and propose PointSea for global-to-local point cloud completion. In the global stage, consider how we inspect a defective region of a physical object, we may observe it from various perspectives for a better understanding. Inspired by this, PointSea augments data representation by leveraging self-projected depth images from multiple views. To reconstruct a compact global shape from the cross-modal input, we incorporate a feature fusion module to fuse features at both intra-view and inter-view levels. In the local stage, to reveal highly detailed structures, we introduce a point generator called the self-structure dual-generator. This generator integrates both learned shape priors and geometric self-similarities for shape refinement. Unlike existing efforts that apply a unified strategy for all points, our dual-path design adapts refinement strategies conditioned on the structural type of each point, addressing the specific incompleteness of each point. Comprehensive experiments on widely-used benchmarks demonstrate that PointSea effectively understands global shapes and generates local details from incomplete input, showing clear improvements over existing methods. Our code is available at https://github.com/czvvd/SVDFormer_PointSea .},
  archive      = {J_IJCV},
  author       = {Zhu, Zhe and Chen, Honghua and He, Xing and Wei, Mingqiang},
  doi          = {10.1007/s11263-025-02400-y},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4770-4794},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PointSea: Point cloud completion via self-structure augmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LR-ASD: Lightweight and robust network for active speaker detection. <em>IJCV</em>, <em>133</em>(7), 4749-4769. (<a href='https://doi.org/10.1007/s11263-025-02399-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active speaker detection is a challenging task aimed at identifying who is speaking. Due to the critical importance of this task in numerous applications, it has received considerable attention. Existing studies endeavor to enhance performance at any cost by inputting information from multiple candidates and designing complex models. While these methods have achieved excellent performance, their substantial memory and computational demands pose challenges for their application to resource-limited scenarios. Therefore, in this study, a lightweight and robust network for active speaker detection, named LR-ASD, is constructed by reducing the number of input candidates, splitting 2D and 3D convolutions for audio-visual feature extraction, using a simple channel attention module for multi-modal feature fusion, and applying gated recurrent unit (GRU) with low computational complexity for temporal modeling. Results on the AVA-ActiveSpeaker dataset reveal that LR-ASD achieves competitive mean Average Precision (mAP) performance (94.5% vs. 95.2%), while the resource costs are significantly lower than the state-of-the-art method, particularly in terms of model parameters (0.84 M vs. 34.33 M, approximately 41 times) and floating point operations (FLOPs) (0.51 G vs. 4.86 G, approximately 10 times). Additionally, LR-ASD demonstrates excellent robustness by achieving state-of-the-art performance on the Talkies, Columbia, and RealVAD datasets in cross-dataset testing without fine-tuning. The project is available at https://github.com/Junhua-Liao/LR-ASD .},
  archive      = {J_IJCV},
  author       = {Liao, Junhua and Duan, Haihan and Feng, Kanghui and Zhao, Wanbing and Yang, Yanbing and Chen, Liangyin and Chen, Yanru},
  doi          = {10.1007/s11263-025-02399-2},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4749-4769},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LR-ASD: Lightweight and robust network for active speaker detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to generalize heterogeneous representation for cross-modality image synthesis via multiple domain interventions. <em>IJCV</em>, <em>133</em>(7), 4727-4748. (<a href='https://doi.org/10.1007/s11263-025-02381-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging with modality diversity substantially increases productivity in routine diagnosis and advanced research. However, high inter-equipment variability and expensive examination cost remain as key challenges in acquiring and utilizing multi-modal images. Missing modalities often can be synthesized from existing ones. While the rapid growth in image style transfer with deep models overwhelms the above endeavor, such image synthesis may not always be achievable and even impractical when applied to medical data. The proposed method addresses this issue by a convolutional sparse coding (CSC) adaptation network to handle the lacking of generalizing medical image representation learning. We reduce both inter-domain and intra-domain divergences by the domain-adaptation and domain-standardization modules, respectively. On the basis of CSC features, we penalize their subspace mismatching to reduce the generalization error. The overall framework is cast in a minimax setting, and the extensive experiments show that the proposed method yields state-of-the-art results on multiple datasets.},
  archive      = {J_IJCV},
  author       = {Huang, Yawen and Huang, Huimin and Zheng, Hao and Li, Yuexiang and Zheng, Feng and Zhen, Xiantong and Zheng, Yefeng},
  doi          = {10.1007/s11263-025-02381-y},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4727-4748},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to generalize heterogeneous representation for cross-modality image synthesis via multiple domain interventions},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A solution to co-occurrence bias in pedestrian attribute recognition: Theory, algorithms, and improvements. <em>IJCV</em>, <em>133</em>(7), 4712-4726. (<a href='https://doi.org/10.1007/s11263-025-02405-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the pedestrian attributes recognition, we demonstrate that deep models can memorize the pattern of attributes co-occurrences inherent to dataset, whether through explicit or implicit means. However, since the attributes interdependency is highly variable and unpredictable across different scenarios, the modeled attributes co-occurrences de facto serve as a data selection bias that hardly generalizes onto out-of-distribution samples. To address this thorny issue, we formulate a novel concept of attributes-disentangled feature learning, by which the mutual information among features of different attributes is minimized, ensuring the recognition of an attribute independent to the presence of others. Stemming from it, practical approaches are developed to effectively decouple attributes by suppressing the shared feature factors among attributes-specific features. As compelling merits, our method is exercised with minimal test-time computation, and is also highly extendable. With slight modifications on it, further improvements regarding better exploration of the feature space, softening the issue of imbalanced attributes distribution in dataset and flexibility in term of preserving certain causal attributes interdependencies can be achieved. Comprehensive experiments on various realistic datasets, such as PA100k, PETAzs and RAPzs, validate the efficacy and a spectrum of superiorities of our method.},
  archive      = {J_IJCV},
  author       = {Zhou, Yibo and Hu, Hai-Miao and Yu, Jinzuo and Wu, Haotian and Pu, Shiliang and Wang, Hanzi},
  doi          = {10.1007/s11263-025-02405-7},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4712-4726},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A solution to co-occurrence bias in pedestrian attribute recognition: Theory, algorithms, and improvements},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusion for visual-infrared person ReID in real-world surveillance using corrupted multimodal data. <em>IJCV</em>, <em>133</em>(7), 4690-4711. (<a href='https://doi.org/10.1007/s11263-025-02396-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (V-I ReID) seeks to match images of individuals captured over a distributed network of RGB and IR cameras. The task is challenging due to the significant differences between V and I modalities, especially under real-world conditions, where images face corruptions such as blur, noise, and weather. Despite their practical relevance, deep learning models for multimodal V-I ReID remain far less investigated than for single and cross-modal V to I settings. Moreover, state-of-art V-I ReID models cannot leverage corrupted modality information to sustain a high level of accuracy. In this paper, we propose an efficient model for multimodal V-I ReID – named Multimodal Middle Stream Fusion (MMSF) – that preserves modality-specific knowledge for improved robustness to corrupted multimodal images. In addition, three state-of-art attention-based multimodal fusion models are adapted to address corrupted multimodal data in V-I ReID, allowing for dynamic balancing of the importance of each modality. The literature typically reports ReID performance using clean datasets, but more recently, evaluation protocols have been proposed to assess the robustness of ReID models under challenging real-world scenarios, using data with realistic corruptions. However, these protocols are limited to unimodal V settings. For realistic evaluation of multimodal (and cross-modal) V-I person ReID models, we propose new challenging corrupted datasets for scenarios where V and I cameras are co-located (CL) and not co-located (NCL). Finally, the benefits of our Masking and Local Multimodal Data Augmentation (ML-MDA) strategy are explored to improve the robustness of ReID models to multimodal corruption. Our experiments on clean and corrupted versions of the SYSU-MM01, RegDB, and ThermalWORLD datasets indicate the multimodal V-I ReID models that are more likely to perform well in real-world operational conditions. In particular, the proposed ML-MDA is shown as essential for a V-I person ReID system to sustain high accuracy and robustness in face of corrupted multimodal images. Our multimodal ReID models attains the best accuracy and complexity trade-off under both CL and NCL settings and compared to state-of-art unimodal ReID systems, except for the ThermalWORLD dataset due to its low-quality I. Our MMSF model outperforms every method under CL and NCL camera scenarios. GitHub code: https://github.com/art2611/MREiD-UCD-CCD.git .},
  archive      = {J_IJCV},
  author       = {Josi, Arthur and Alehdaghi, Mahdi and Cruz, Rafael M. O. and Granger, Eric},
  doi          = {10.1007/s11263-025-02396-5},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4690-4711},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fusion for visual-infrared person ReID in real-world surveillance using corrupted multimodal data},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Not all pixels are equal: Learning pixel hardness for semantic segmentation. <em>IJCV</em>, <em>133</em>(7), 4669-4689. (<a href='https://doi.org/10.1007/s11263-025-02416-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has witnessed great progress. Despite the impressive overall results, the segmentation performance in some hard areas (e.g., small objects or thin parts) is still not promising. A straightforward solution is hard sample mining. Yet, most existing hard pixel mining strategies for semantic segmentation often rely on pixel’s loss value, which tends to decrease during training. Intuitively, the pixel hardness for segmentation mainly depends on image structure and is expected to be stable. In this paper, we propose to learn pixel hardness for semantic segmentation by leveraging hardness information contained in global and historical loss values. More precisely, we add a gradient-independent branch for learning a hardness level (HL) map by maximizing hardness-weighted segmentation loss, which is minimized for the segmentation head. This encourages large hardness values in difficult areas, leading to appropriate and stable HL map. Despite its simplicity, the proposed method can be applied to most segmentation methods with no and marginal extra cost during inference and training, respectively. Without bells and whistles, the proposed method achieves consistent improvement (1.37% mIoU on average) over most popular semantic segmentation methods on the Cityscapes dataset, and demonstrates good generalization ability across domains. The source codes are available at this link .},
  archive      = {J_IJCV},
  author       = {Xiao, Xin and Zhou, Daiguo and Hu, Jiagao and Hu, Yi and Xu, Yongchao},
  doi          = {10.1007/s11263-025-02416-4},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4669-4689},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Not all pixels are equal: Learning pixel hardness for semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-text guidance is important: Multi-modality image fusion via large generative vision-language model. <em>IJCV</em>, <em>133</em>(7), 4646-4668. (<a href='https://doi.org/10.1007/s11263-025-02409-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modality image fusion aims to extract complementary features from multiple source images of different modalities, generating a fused image that inherits their advantages. To address challenges in cross-modality shared feature (CMSF) extraction, single-modality specific feature (SMSF) fusion, and the absence of ground truth (GT) images, we propose MTG-Fusion, a multi-text guided model. We leverage the capabilities of large vision-language models to generate text descriptions tailored to the input images, providing novel insights for these challenges. Our model introduces a text-guided CMSF extractor (TGCE) and a text-guided SMSF fusion module (TGSF). TGCE transforms visual features into the text domain using manifold-isometric domain transform techniques and provides effective visual-text interaction based on text-vision and text-text distances. TGSF fuses each dimension of visual features with corresponding text features, creating a weight matrix utilized for SMSF fusion. We also incorporate the constructed textual GT into the loss function for collaborative training. Extensive experiments demonstrate that MTG-Fusion achieves state-of-the-art performance on infrared and visible image fusion and medical image fusion tasks. The code is available at: https://github.com/zhaolb4080/MTG-Fusion .},
  archive      = {J_IJCV},
  author       = {Wang, Zeyu and Zhao, Libo and Zhang, Jizheng and Song, Rui and Song, Haiyu and Meng, Jiana and Wang, Shidong},
  doi          = {10.1007/s11263-025-02409-3},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4646-4668},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-text guidance is important: Multi-modality image fusion via large generative vision-language model},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source domain adaptation by causal-guided adaptive multimodal diffusion networks. <em>IJCV</em>, <em>133</em>(7), 4623-4645. (<a href='https://doi.org/10.1007/s11263-025-02401-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source domain adaptation (MSDA) strives to adapt the models trained on multimodal labelled source domains to an unlabelled target domain. Recent GANs based MSDA methods implicitly characterize the image distribution, which may result in limited sample fidelity, causing misalignment of pixel-level information among sources and the target. Furthermore, when samples from different sources interfere during the learning process, significant misalignment across different source domains may arise. In this paper, we propose a novel MSDA framework, called Causal-guided Adaptive Multimodal Diffusion Networks (C-AMDN), to tackle these challenges. C-AMDN incorporates a diffusive adversarial generation model for high-fidelity, efficient adaptation among source and target domains, along with deep causal inference re-weighting mechanism for the decision-making process that the conditional distributions of outcomes remain consistent across different domains, even as the input distributions change. In addition, we propose an efficient way to further adapt the input image to another domain: we preserve important semantic information by a density constraint regularization in the generation model. Experimental results demonstrate that C-AMDN significantly outperforms existing methods across several real-world domain adaptation benchmarks.},
  archive      = {J_IJCV},
  author       = {Cai, Ziyun and Huang, Yawen and Zhang, Tengfei and Zheng, Yefeng and Yue, Dong},
  doi          = {10.1007/s11263-025-02401-x},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4623-4645},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Multi-source domain adaptation by causal-guided adaptive multimodal diffusion networks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expressive image generation and editing with rich text. <em>IJCV</em>, <em>133</em>(7), 4604-4622. (<a href='https://doi.org/10.1007/s11263-025-02361-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plain text has become a prevalent interface for text-based image synthesis and editing. Its limited customization options, however, hinder users from accurately describing desired outputs. For example, plain text makes it hard to specify continuous quantities, such as the precise RGB color value or importance of each word. Creating detailed text prompts for complex scenes is tedious for humans to write and challenging for text encoders to interpret. Furthermore, describing a reference concept or texture in plain text is non-trivial. To address these challenges, we propose using a rich-text editor supporting formats such as font style, size, color, texture fill, footnote, and embedded image. We extract each word’s attributes from rich text to enable local style control, explicit token reweighting, precise color rendering, and detailed region synthesis with reference concepts or texture. We achieve these capabilities through a region-based diffusion process. We first obtain each word’s mask that characterizes the region guided by the word. For each region, we enforce its text attributes by creating customized prompts, applying guidance within the region, and maintaining its fidelity against plain-text generations or input images through region-based injections. We present various examples of image generation and editing from rich text and demonstrate that our method outperforms strong baselines with quantitative evaluations.},
  archive      = {J_IJCV},
  author       = {Ge, Songwei and Park, Taesung and Zhu, Jun-Yan and Huang, Jia-Bin},
  doi          = {10.1007/s11263-025-02361-2},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4604-4622},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Expressive image generation and editing with rich text},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter efficient fine-tuning for multi-modal generative vision models with möbius-inspired transformation. <em>IJCV</em>, <em>133</em>(7), 4590-4603. (<a href='https://doi.org/10.1007/s11263-025-02398-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of multimodal generative vision models has drawn scientific curiosity. Notable advancements, such as OpenAI’s ChatGPT and Stable Diffusion, demonstrate the potential of combining multimodal data for generative content. Nonetheless, customising these models to specific domains or tasks is challenging due to computational costs and data requirements. Conventional fine-tuning methods take redundant processing resources, motivating the development of parameter-efficient fine-tuning technologies such as adapter module, low-rank factorization and orthogonal fine-tuning. These solutions selectively change a subset of model parameters, reducing learning needs while maintaining high-quality results. Orthogonal fine-tuning, regarded as a reliable technique, preserves semantic linkages in weight space but has limitations in its expressive powers. To better overcome these constraints, we provide a simple but innovative and effective transformation method inspired by Möbius geometry, which replaces conventional orthogonal transformations in parameter-efficient fine-tuning. This strategy improved fine-tuning’s adaptability and expressiveness, allowing it to capture more data patterns. Our strategy, which is supported by theoretical understanding and empirical validation, outperforms existing approaches, demonstrating competitive improvements in generation quality for key generative tasks.},
  archive      = {J_IJCV},
  author       = {Duan, Haoran and Shao, Shuai and Zhai, Bing and Shah, Tejal and Han, Jungong and Ranjan, Rajiv},
  doi          = {10.1007/s11263-025-02398-3},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4590-4603},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Parameter efficient fine-tuning for multi-modal generative vision models with möbius-inspired transformation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exemplar-free continual learning of vision transformers via gated class-attention and cascaded feature drift compensation. <em>IJCV</em>, <em>133</em>(7), 4571-4589. (<a href='https://doi.org/10.1007/s11263-025-02374-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformers (ViTs) have achieved remarkable successes across a broad range of computer vision applications. As a consequence, there has been increasing interest in extending continual learning theory and techniques to ViT architectures. We propose a new method for exemplar-free class incremental training of ViTs. The main challenge of exemplar-free continual learning is maintaining plasticity of the learner without causing catastrophic forgetting of previously learned tasks. This is often achieved via exemplar replay which can help recalibrate previous task classifiers to the feature drift which occurs when learning new tasks. Exemplar replay, however, comes at the cost of retaining samples from previous tasks which for many applications may not be possible. To address the problem of continual ViT training, we first propose gated class-attention to minimize the drift in the final ViT transformer block. This mask-based gating is applied to class-attention mechanism of the last transformer block and strongly regulates the weights crucial for previous tasks. Importantly, gated class-attention does not require the task-ID during inference, which distinguishes it from other parameter isolation methods. Secondly, we propose a new method of feature drift compensation that accommodates feature drift in the backbone when learning new tasks. The combination of gated class-attention and cascaded feature drift compensation allows for plasticity towards new tasks while limiting forgetting of previous ones. Extensive experiments performed on CIFAR-100, Tiny-ImageNet and ImageNet100 demonstrate that our exemplar-free method obtains competitive results when compared to rehearsal based ViT methods.(Code: https://github.com/OcraM17/GCAB-CFDC )},
  archive      = {J_IJCV},
  author       = {Cotogni, Marco and Yang, Fei and Cusano, Claudio and Bagdanov, Andrew D. and van de Weijer, Joost},
  doi          = {10.1007/s11263-025-02374-x},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4571-4589},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exemplar-free continual learning of vision transformers via gated class-attention and cascaded feature drift compensation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute-centric compositional text-to-image generation. <em>IJCV</em>, <em>133</em>(7), 4555-4570. (<a href='https://doi.org/10.1007/s11263-025-02371-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent impressive breakthroughs in text-to-image generation, generative models have difficulty in capturing the data distribution of underrepresented attribute compositions while over-memorizing overrepresented attribute compositions, which raises public concerns about their robustness and fairness. To tackle this challenge, we propose ACTIG, an attribute-centric compositional text-to-image generation framework. We present an attribute-centric feature augmentation and a novel image-free training scheme, which greatly improves model’s ability to generate images with underrepresented attributes. We further propose an attribute-centric contrastive loss to avoid overfitting to overrepresented attribute compositions. We validate our framework on the CelebA-HQ and CUB datasets. Extensive experiments show that the compositional generalization of ACTIG is outstanding, and our framework outperforms previous works in terms of image quality and text-image consistency. The source code and trained models are publicly available at https://github.com/yrcong/ACTIG .},
  archive      = {J_IJCV},
  author       = {Cong, Yuren and Min, Martin Renqiang and Li, Li Erran and Rosenhahn, Bodo and Yang, Michael Ying},
  doi          = {10.1007/s11263-025-02371-0},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4555-4570},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Attribute-centric compositional text-to-image generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniFace++: Revisiting a unified framework for face reenactment and swapping via 3D priors. <em>IJCV</em>, <em>133</em>(7), 4538-4554. (<a href='https://doi.org/10.1007/s11263-025-02395-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face reenactment and swapping share a similar pattern of identity and attribute manipulation. Our previous work UniFace has preliminarily explored establishing a unification between the two at the feature level, but it heavily relies on the accuracy of feature disentanglement, and GANs are also unstable during training. In this work, we delve into the intrinsic connections between the two from a more general training paradigm perspective, introducing a novel diffusion-based unified method UniFace++. Specifically, this work combines the advantages of each, i.e., stability of reconstruction training from reenactment, simplicity and effectiveness of the target-oriented processing from swapping, and redefining both as target-oriented reconstruction tasks. In this way, face reenactment avoids complex source feature deformation and face swapping mitigates the unstable seesaw-style optimization. The core of our approach is the rendered face obtained from reassembled 3D facial priors serving as the target pivot, which contains precise geometry and coarse identity textures. We further incorporate it with the proposed Texture-Geometry-aware Diffusion Model (TGDM) to perform texture transfer under the reconstruction supervision for high-fidelity face synthesis. Extensive quantitative and qualitative experiments demonstrate the superiority of our method for both tasks.},
  archive      = {J_IJCV},
  author       = {Xu, Chao and Qian, Yijie and Zhu, Shaoting and Sun, Baigui and Zhao, Jian and Liu, Yong and Li, Xuelong},
  doi          = {10.1007/s11263-025-02395-6},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4538-4554},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {UniFace++: Revisiting a unified framework for face reenactment and swapping via 3D priors},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating self-supervised methods for label-efficient learning. <em>IJCV</em>, <em>133</em>(7), 4522-4537. (<a href='https://doi.org/10.1007/s11263-025-02397-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformers combined with self-supervised learning have enabled the development of models which scale across large datasets for several downstream tasks, including classification, segmentation, and detection. However, the potential of these models for low-shot learning across several downstream tasks remains largely under explored. In this work, we conduct a systematic examination of different self-supervised pretext tasks, namely contrastive learning, clustering, and masked image modelling, to assess their low-shot capabilities by comparing different pretrained models. In addition, we explore the impact of various collapse avoidance techniques, such as centring, ME-MAX, and sinkhorn, on these downstream tasks. Based on our detailed analysis, we introduce a framework that combines mask image modelling and clustering as pretext tasks. This framework demonstrates superior performance across all examined low-shot downstream tasks, including multi-class classification, multi-label classification and semantic segmentation. Furthermore, when testing the model on large-scale datasets, we show performance gains in various tasks.},
  archive      = {J_IJCV},
  author       = {Nandam, Srinivasa Rao and Atito, Sara and Feng, Zhenhua and Kittler, Josef and Awais, Muhammad},
  doi          = {10.1007/s11263-025-02397-4},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4522-4537},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Investigating self-supervised methods for label-efficient learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantics-conditioned generative zero-shot learning via feature refinement. <em>IJCV</em>, <em>133</em>(7), 4504-4521. (<a href='https://doi.org/10.1007/s11263-025-02394-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative zero-shot learning (ZSL) recognizes novel categories by employing a cross-modal generative model conditioned on semantic factors (such as attributes) to transfer knowledge from seen classes to unseen ones. Many existing generative ZSL methods rely solely on feature extraction models pre-trained on ImageNet, disregarding the cross-dataset bias between ImageNet and ZSL benchmarks. This bias inevitably leads to suboptimal visual features that lack semantic relevance to the predefined attributes, constraining the generator’s ability to synthesize semantically meaningful visual features for generative ZSL. In this paper, we introduce a visual feature refinement method (ViFR) to mitigate cross-dataset bias and advance generative ZSL. Given a generative ZSL model, ViFR incorporates both pre-feature refinement (Pre-FR) and post-feature refinement (Post-FR) modules to simultaneously enhance visual features. In Pre-FR, ViFR aims to learn attribute localization for discriminative visual feature representations using an attribute-guided attention mechanism optimized with attribute-based cross-entropy loss. In Post-FR, ViFR learns an effective visual $$\rightarrow $$ semantic mapping by integrating the semantic-conditioned generator into a unified generative model to enhance visual features. Additionally, we propose a self-adaptive margin center loss (SAMC-loss) that collaborates with semantic cycle-consistency loss to guide Post-FR in learning class- and semantically-relevant representations. The features in Post-FR are concatenated to form fully refined visual features for ZSL classification. Extensive experiments on benchmark datasets (i.e., CUB, SUN, and AWA2) demonstrate that ViFR outperforms state-of-the-art ZSL approaches. Our implementation is publicly available at https://github.com/shiming-chen/ViFR .},
  archive      = {J_IJCV},
  author       = {Chen, Shiming and Hong, Ziming and You, Xinge and Shao, Ling},
  doi          = {10.1007/s11263-025-02394-7},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4504-4521},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Semantics-conditioned generative zero-shot learning via feature refinement},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Part-whole relational fusion towards multi-modal scene understanding. <em>IJCV</em>, <em>133</em>(7), 4483-4503. (<a href='https://doi.org/10.1007/s11263-025-02393-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal fusion has played a vital role in multi-modal scene understanding. Most existing methods focus on cross-modal fusion involving two modalities, often overlooking more complex multi-modal fusion, which is essential for real-world applications like autonomous driving, where visible, depth, event, LiDAR, etc., are used. Besides, few attempts for multi-modal fusion, e.g., simple concatenation, cross-modal attention, and token selection, cannot well dig into the intrinsic shared and specific details of multiple modalities. To tackle the challenge, in this paper, we propose a Part-Whole Relational Fusion (PWRF) framework. For the first time, this framework treats multi-modal fusion as part-whole relational fusion. It routes multiple individual part-level modalities to a fused whole-level modality using the part-whole relational routing ability of Capsule Networks (CapsNets). Through this part-whole routing, our PWRF generates modal-shared and modal-specific semantics from the whole-level modal capsules and the routing coefficients, respectively. On top of that, modal-shared and modal-specific details can be employed to solve the issue of multi-modal scene understanding, including synthetic multi-modal segmentation and visible-depth-thermal salient object detection in this paper. Experiments on several datasets demonstrate the superiority of the proposed PWRF framework for multi-modal scene understanding. The source code has been released on https://github.com/liuyi1989/PWRF .},
  archive      = {J_IJCV},
  author       = {Liu, Yi and Li, Chengxin and Xu, Shoukun and Han, Jungong},
  doi          = {10.1007/s11263-025-02393-8},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4483-4503},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Part-whole relational fusion towards multi-modal scene understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal transductive inference for few-shot video object segmentation. <em>IJCV</em>, <em>133</em>(7), 4465-4482. (<a href='https://doi.org/10.1007/s11263-025-02390-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot video object segmentation (FS-VOS) aims at segmenting video frames using a few labelled examples of classes not seen during initial training. In this paper, we present a simple but effective temporal transductive inference (TTI) approach that leverages temporal consistency in the unlabelled video frames during few-shot inference without episodic training. Key to our approach is the use of a video-level temporal constraint that augments frame-level constraints. The objective of the video-level constraint is to learn consistent linear classifiers for novel classes across the image sequence. It acts as a spatiotemporal regularizer during the transductive inference to increase temporal coherence and reduce overfitting on the few-shot support set. Empirically, our approach outperforms state-of-the-art meta-learning approaches in terms of mean intersection over union on YouTube-VIS by 2.5%. In addition, we introduce an improved benchmark dataset that is exhaustively labelled (i.e., all object occurrences are labelled, unlike the currently available). Our empirical results and temporal consistency analysis confirm the added benefits of the proposed spatiotemporal regularizer to improve temporal coherence. Our code and benchmark dataset is publicly available at, https://github.com/MSiam/tti_fsvos/ .},
  archive      = {J_IJCV},
  author       = {Siam, Mennatullah},
  doi          = {10.1007/s11263-025-02390-x},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4465-4482},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Temporal transductive inference for few-shot video object segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMSCS: A novel unpaired multimodal image segmentation method via cross-modality generative and semi-supervised learning. <em>IJCV</em>, <em>133</em>(7), 4442-4464. (<a href='https://doi.org/10.1007/s11263-025-02389-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal medical image segmentation is crucial for enhancing diagnostic accuracy in various clinical settings. However, due to the difficulty of obtaining complete data in real clinical settings, the use of unpaired and unlabeled multimodal data is severely limited. This results in unpaired data being unusable as simultaneous input for models due to spatial misalignments and morphological differences, and unlabeled data failing to provide effective supervisory signals for models. To alleviate these issues, we propose a semi-supervised multimodal segmentation method based on cross-modal generative that seamlessly integrates image translation and segmentation stages. In the cross-modalities generative stage, we employ adversarial learning to discern the latent anatomical correlations across various modalities, followed by maintaining a balance between semantic consistency and structural consistency in image translation through region-aware constraints and cross-modal structural information contrastive learning with dynamic weight adjustment. In the segmentation stage, we employ a teacher-student semi-supervised learning (SSL) framework where the student network distills multimodal knowledge from the teacher network and utilizes unlabeled source data to enhance the supervisory signal. Experimental results demonstrate that our proposed method achieves state-of-the-art performance in extensive experiments on the segmentation tasks of cardiac substructures and multi-organs abdominal, outperforming other competitive methods.},
  archive      = {J_IJCV},
  author       = {Yang, Feiyang and Li, Xiongfei and Wang, Bo and Teng, Peihong and Liu, Guifeng},
  doi          = {10.1007/s11263-025-02389-4},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4442-4464},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {UMSCS: A novel unpaired multimodal image segmentation method via cross-modality generative and semi-supervised learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep hierarchical learning for 3D semantic segmentation. <em>IJCV</em>, <em>133</em>(7), 4420-4441. (<a href='https://doi.org/10.1007/s11263-025-02387-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent structure of human cognition facilitates the hierarchical organization of semantic categories for three-dimensional objects, simplifying the visual world into distinct and manageable layers. A vivid example is observed in the animal-taxonomy domain, where distinctions are not only made between broader categories like birds and mammals but also within subcategories such as different bird species, illustrating the depth of human hierarchical processing. This observation bridges to the computational realm as this paper presents deep hierarchical learning (DHL) on 3D data. By formulating a probabilistic representation, our proposed DHL lays a pioneering theoretical foundation for hierarchical learning (HL) in visual tasks. Addressing the primary challenges in effectiveness and generality of DHL for 3D data, we 1) introduce a hierarchical regularization term to connect hierarchical coherence across the predictions with the classification loss; 2) develop a general deep learning framework with a hierarchical embedding fusion module for enhanced hierarchical embedding learning; and 3) devise a novel method for constructing class hierarchies in datasets with non-hierarchical labels, leveraging recent vision language models. A novel hierarchy quality indicator, CH-MOS, supported by questionnaire-based surveys, is developed to evaluate the semantic explainability of the generated class hierarchy for human understanding. Our methodology’s validity is confirmed through extensive experiments on multiple datasets for 3D object and scene point cloud semantic segmentation tasks, demonstrating DHL’s capability in parsing 3D data across various hierarchical levels. This evidence suggests DHL’s potential for broader applicability to a wide range of tasks.},
  archive      = {J_IJCV},
  author       = {Li, Chongshou and Liu, Yuheng and Li, Xinke and Zhang, Yuning and Li, Tianrui and Yuan, Junsong},
  doi          = {10.1007/s11263-025-02387-6},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4420-4441},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep hierarchical learning for 3D semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). METS: Motion-encoded time-surface for event-based high-speed pose tracking. <em>IJCV</em>, <em>133</em>(7), 4401-4419. (<a href='https://doi.org/10.1007/s11263-025-02379-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel event-based representation, named Motion-Encoded Time-Surface (METS), and how it can be used to address the challenge of pose tracking under high-speed scenarios with an event camera. The core concept is dynamically encoding the pixel-wise decay rate of the Time-Surface to account for localized spatio-temporal scene dynamics captured by events, rendering remarkable adaptability with respect to motion dynamics. The consistency between METS and the scene in highly dynamic conditions establishes a reliable foundation for robust pose estimation. Building upon this, we employ a semi-dense 3D-2D alignment pipeline to fully unlock the potential of the event camera for high-speed tracking applications. Given the intrinsic characteristics of METS, we further develop specialized lightweight operations aimed at minimizing the per-event computational cost. The proposed algorithm is successfully evaluated on public datasets and our high-speed motion datasets covering various scenes and motion complexities. It shows that our approach outperforms state-of-the-art pose tracking methods, especially in highly dynamic scenarios, and is capable of tracking accurately under incredibly fast motions that are inaccessible for other event- or frame-based counterparts. Due to its simplicity, our algorithm exhibits outstanding practicality, running at over 70 Hz on a standard CPU.},
  archive      = {J_IJCV},
  author       = {Xu, Ninghui and Wang, Lihui and Yao, Zhiting and Okatani, Takayuki},
  doi          = {10.1007/s11263-025-02379-6},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4401-4419},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {METS: Motion-encoded time-surface for event-based high-speed pose tracking},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LaMD: Latent motion diffusion for image-conditional video generation. <em>IJCV</em>, <em>133</em>(7), 4384-4400. (<a href='https://doi.org/10.1007/s11263-025-02386-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The video generation field has witnessed rapid improvements with the introduction of recent diffusion models. While these models have successfully enhanced appearance quality, they still face challenges in generating coherent and natural movements while efficiently sampling videos. In this paper, we propose to condense video generation into a problem of motion generation, to improve the expressiveness of motion and make video generation more manageable. This can be achieved by breaking down the video generation process into latent motion generation and video reconstruction. Specifically, we present a latent motion diffusion (LaMD) framework, which consists of a motion-decomposed video autoencoder and a diffusion-based motion generator, to implement this idea. Through careful design, the motion-decomposed video autoencoder can compress patterns in movement into a concise latent motion representation. Consequently, the diffusion-based motion generator is able to efficiently generate realistic motion on a continuous latent space under multi-modal conditions, at a cost that is similar to that of image diffusion models. Results show that LaMD generates high-quality videos on various benchmark datasets, including BAIR, Landscape, NATOPS, MUG and CATER-GEN, that encompass a variety of stochastic dynamics and highly controllable movements on multiple image-conditional video generation tasks, while significantly decreases sampling time.},
  archive      = {J_IJCV},
  author       = {Hu, Yaosi and Chen, Zhenzhong and Luo, Chong},
  doi          = {10.1007/s11263-025-02386-7},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4384-4400},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LaMD: Latent motion diffusion for image-conditional video generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unknown support prototype set for open set recognition. <em>IJCV</em>, <em>133</em>(7), 4366-4383. (<a href='https://doi.org/10.1007/s11263-025-02384-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, visual recognition systems inevitably encounter unknown classes which are not present in the training set. Open set recognition aims to classify samples from known classes and detect unknowns, simultaneously. One promising solution is to inject unknowns into training sets, and significant progress has been made on how to build an unknowns generator. However, what unknowns exhibit strong generalization is rarely explored. This work presents a new concept called Unknown Support Prototypes, which serve as good representatives for potential unknown classes. Two novel metrics coined Support and Diversity are introduced to construct Unknown Support Prototype Set. In the algorithm, we further propose to construct Unknown Support Prototypes in the semantic subspace of the feature space, which can largely reduce the cardinality of Unknown Support Prototype Set and enhance the reliability of unknowns generation. Extensive experiments on several benchmark datasets demonstrate the proposed algorithm offers effective generalization for unknowns.},
  archive      = {J_IJCV},
  author       = {Jiang, Guosong and Zhu, Pengfei and Cao, Bing and Chen, Dongyue and Hu, Qinghua},
  doi          = {10.1007/s11263-025-02384-9},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4366-4383},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unknown support prototype set for open set recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LMD: Light-weight prediction quality estimation for object detection in lidar point clouds. <em>IJCV</em>, <em>133</em>(7), 4349-4365. (<a href='https://doi.org/10.1007/s11263-025-02377-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection on Lidar point cloud data is a promising technology for autonomous driving and robotics which has seen a significant rise in performance and accuracy during recent years. Particularly uncertainty estimation is a crucial component for down-stream tasks and deep neural networks remain error-prone even for predictions with high confidence. Previously proposed methods for quantifying prediction uncertainty tend to alter the training scheme of the detector or rely on prediction sampling which results in vastly increased inference time. In order to address these two issues, we propose LidarMetaDetect (LMD), a light-weight post-processing scheme for prediction quality estimation. Our method can easily be added to any pre-trained Lidar object detector without altering anything about the base model and is purely based on post-processing, therefore, only leading to a negligible computational overhead. Our experiments show a significant increase of statistical reliability in separating true from false predictions. We show that this improvement carries over to object detection performance when replacing the objectness score native to the object detector. We propose and evaluate an additional application of our method leading to the detection of annotation errors. Explicit samples and a conservative count of annotation error proposals indicates the viability of our method for large-scale datasets like KITTI and nuScenes. On the widely-used nuScenes test dataset, 43 out of the top 100 proposals of our method indicate, in fact, erroneous annotations.},
  archive      = {J_IJCV},
  author       = {Riedlinger, Tobias and Schubert, Marius and Penquitt, Sarina and Kezmann, Jan-Marcel and Colling, Pascal and Kahl, Karsten and Roese-Koerner, Lutz and Arnold, Michael and Zimmermann, Urs and Rottmann, Matthias},
  doi          = {10.1007/s11263-025-02377-8},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4349-4365},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LMD: Light-weight prediction quality estimation for object detection in lidar point clouds},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the trustworthiness landscape of state-of-the-art generative models: A survey and outlook. <em>IJCV</em>, <em>133</em>(7), 4317-4348. (<a href='https://doi.org/10.1007/s11263-025-02375-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models and large language models have emerged as leading-edge generative models, revolutionizing various aspects of human life. However, their practical implementation has also exposed inherent risks, bringing to light their potential downsides and sparking concerns about their trustworthiness. Despite the wealth of literature on this subject, a comprehensive survey that specifically delves into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, this paper investigates both long-standing and emerging threats associated with these models across four fundamental dimensions: 1) privacy, 2) security, 3) fairness, and 4) responsibility. Based on our investigation results, we develop an extensive survey that outlines the trustworthiness of large generative models. Following that, we provide practical recommendations and identify promising research directions for generative AI, ultimately promoting the trustworthiness of these models and benefiting society as a whole.},
  archive      = {J_IJCV},
  author       = {Fan, Mingyuan and Wang, Chengyu and Chen, Cen and Liu, Yang and Huang, Jun},
  doi          = {10.1007/s11263-025-02375-w},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4317-4348},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {On the trustworthiness landscape of state-of-the-art generative models: A survey and outlook},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Realistic evaluation of deep active learning for image classification and semantic segmentation. <em>IJCV</em>, <em>133</em>(7), 4294-4316. (<a href='https://doi.org/10.1007/s11263-025-02372-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning aims to reduce the high labeling cost involved in training machine learning models on large datasets by efficiently labeling only the most informative samples. Recently, deep active learning has shown success on various tasks. However, the conventional evaluation schemes are either incomplete or below par. This study critically assesses various active learning approaches, identifying key factors essential for choosing the most effective active learning method. It includes a comprehensive guide to obtain the best performance for each case, in image classification and semantic segmentation. For image classification, the AL methods improve by a large-margin when integrated with data augmentation and semi-supervised learning, but barely perform better than the random baseline. In this work, we evaluate them under more realistic settings and propose a more suitable evaluation protocol. For semantic segmentation, previous academic studies focused on diverse datasets with substantial annotation resources. In contrast, data collected in many driving scenarios is highly redundant, and most medical applications are subject to very constrained annotation budgets. The study evaluates active learning techniques under various conditions including data redundancy, the use of semi-supervised learning, and differing annotation budgets. As an outcome of our study, we provide a comprehensive usage guide to obtain the best performance for each case.},
  archive      = {J_IJCV},
  author       = {Mittal, Sudhanshu and Niemeijer, Joshua and Çiçek, Özgün and Tatarchenko, Maxim and Ehrhardt, Jan and Schäfer, Jörg P. and Handels, Heinz and Brox, Thomas},
  doi          = {10.1007/s11263-025-02372-z},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4294-4316},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Realistic evaluation of deep active learning for image classification and semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fg-T2M++: LLMs-augmented fine-grained text driven human motion generation. <em>IJCV</em>, <em>133</em>(7), 4277-4293. (<a href='https://doi.org/10.1007/s11263-025-02392-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the challenging problem of fine-grained text-driven human motion generation. Existing works generate imprecise motions that fail to accurately capture relationships specified in text due to: (1) lack of effective text parsing for detailed semantic cues regarding body parts, (2) not fully modeling linguistic structures between words to comprehend text comprehensively. To tackle these limitations, we propose a novel fine-grained framework Fg-T2M++ that consists of: (1) an LLMs semantic parsing module to extract body part descriptions and semantics from text, (2) a hyperbolic text representation module to encode relational information between text units by embedding the syntactic dependency graph into hyperbolic space, and (3) a multi-modal fusion module to hierarchically fuse text and motion features. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that Fg-T2M++ outperforms SOTA methods, validating its ability to accurately generate motions adhering to comprehensive text semantics.},
  archive      = {J_IJCV},
  author       = {Wang, Yin and Li, Mu and Liu, Jiapeng and Leng, Zhiying and Li, Frederick W. B. and Zhang, Ziyao and Liang, Xiaohui},
  doi          = {10.1007/s11263-025-02392-9},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4277-4293},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fg-T2M++: LLMs-augmented fine-grained text driven human motion generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on deep stereo matching in the twenties. <em>IJCV</em>, <em>133</em>(7), 4245-4276. (<a href='https://doi.org/10.1007/s11263-024-02331-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching is close to hitting a half-century of history, yet witnessed a rapid evolution in the last decade thanks to deep learning. While previous surveys in the late 2010s covered the first stage of this revolution, the last five years of research brought further ground-breaking advancements to the field. This paper aims to fill this gap in a two-fold manner: first, we offer an in-depth examination of the latest developments in deep stereo matching, focusing on the pioneering architectural designs and groundbreaking paradigms that have redefined the field in the 2020s; second, we present a thorough analysis of the critical challenges that have emerged alongside these advances, providing a comprehensive taxonomy of these issues and exploring the state-of-the-art techniques proposed to address them. By reviewing both the architectural innovations and the key challenges, we offer a holistic view of deep stereo matching and highlight the specific areas that require further investigation. To accompany this survey, we maintain a regularly updated project page that catalogs papers on deep stereo matching in our Awesome-Deep-Stereo-Matching repository.},
  archive      = {J_IJCV},
  author       = {Tosi, Fabio and Bartolomei, Luca and Poggi, Matteo},
  doi          = {10.1007/s11263-024-02331-0},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4245-4276},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A survey on deep stereo matching in the twenties},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DustNet++: Deep learning-based visual regression for dust density estimation. <em>IJCV</em>, <em>133</em>(7), 4220-4244. (<a href='https://doi.org/10.1007/s11263-025-02376-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting airborne dust in standard RGB images presents significant challenges. Nevertheless, the monitoring of airborne dust holds substantial potential benefits for climate protection, environmentally sustainable construction, scientific research, and various other fields. To develop an efficient and robust algorithm for airborne dust monitoring, several hurdles have to be addressed. Airborne dust can be opaque or translucent, exhibit considerable variation in density, and possess indistinct boundaries. Moreover, distinguishing dust from other atmospheric phenomena, such as fog or clouds, can be particularly challenging. To meet the demand for a high-performing and reliable method for monitoring airborne dust, we introduce DustNet++, a neural network designed for dust density estimation. DustNet++ leverages feature maps from multiple resolution scales and semantic levels through window and grid attention mechanisms to maintain a sparse, globally effective receptive field with linear complexity. To validate our approach, we benchmark the performance of DustNet++ against existing methods from the domains of crowd counting and monocular depth estimation using the Meteodata airborne dust dataset and the URDE binary dust segmentation dataset. Our findings demonstrate that DustNet++ surpasses comparative methodologies in terms of regression and localization capabilities.},
  archive      = {J_IJCV},
  author       = {Michel, Andreas and Weinmann, Martin and Kuester, Jannick and AlNasser, Faisal and Gomez, Tomas and Falvey, Mark and Schmitz, Rainer and Middelmann, Wolfgang and Hinz, Stefan},
  doi          = {10.1007/s11263-025-02376-9},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4220-4244},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DustNet++: Deep learning-based visual regression for dust density estimation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Informative scene graph generation via debiasing. <em>IJCV</em>, <em>133</em>(7), 4196-4219. (<a href='https://doi.org/10.1007/s11263-025-02365-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation aims to detect visual relationship triplets, (subject, predicate, object). Due to biases in data, current models tend to predict common predicates, e.g., “on” and “at”, instead of informative ones, e.g., “standing on” and “looking at”. This tendency results in the loss of precise information and overall performance. If a model only uses “stone on road” rather than “stone blocking road” to describe an image, it may be a grave misunderstanding. We argue that this phenomenon is caused by two imbalances: semantic space level imbalance and training sample level imbalance. For this problem, we propose DB-SGG, an effective framework based on debiasing but not the conventional distribution fitting. It integrates two components: Semantic Debiasing (SD) and Balanced Predicate Learning (BPL), for these imbalances. SD utilizes a confusion matrix and a bipartite graph to construct predicate relationships. BPL adopts a random undersampling strategy and an ambiguity removing strategy to focus on informative predicates. Benefiting from the model-agnostic process, our method can be easily applied to SGG models and outperforms Transformer by $$136.3\%$$ , $$119.5\%$$ , and $$122.6\%$$ on mR@20 at three SGG sub-tasks on the SGG-VG dataset. Our method is further verified on another complex SGG dataset (SGG-GQA) and two downstream tasks (sentence-to-graph retrieval and image captioning).},
  archive      = {J_IJCV},
  author       = {Gao, Lianli and Lyu, Xinyu and Guo, Yuyu and Hu, Yuxuan and Li, Yuan-Fang and Xu, Lu and Shen, Heng Tao and Song, Jingkuan},
  doi          = {10.1007/s11263-025-02365-y},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4196-4219},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Informative scene graph generation via debiasing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swap attention in spatiotemporal diffusions for text-to-video generation. <em>IJCV</em>, <em>133</em>(7), 4177-4195. (<a href='https://doi.org/10.1007/s11263-025-02349-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive popularity of AI-generated content (AIGC), video generation has recently received a lot of attention. Generating videos guided by text instructions poses significant challenges, such as modeling the complex relationship between space and time, and the lack of large-scale text-video paired data. Existing text-video datasets suffer from limitations in both content quality and scale, or they are not open-source, rendering them inaccessible for study and use. For model design, previous approaches extend pretrained text-to-image generation models by adding temporal 1D convolution/attention modules for video generation. However, these approaches overlook the importance of jointly modeling space and time, inevitably leading to temporal distortions and misalignment between texts and videos. In this paper, we propose a novel approach that strengthens the interaction between spatial and temporal perceptions. In particular, we utilize a swapped cross-attention mechanism in 3D windows that alternates the “query” role between spatial and temporal blocks, enabling mutual reinforcement for each other. Moreover, to fully unlock model capabilities for high-quality video generation and promote the development of the field, we curate a large-scale and open-source video dataset called HD-VG-130M. This dataset comprises 130 million text-video pairs from the open-domain, ensuring high-definition, widescreen and watermark-free characters. A smaller-scale yet more meticulously cleaned subset further enhances the data quality, aiding models in achieving superior performance. Experimental quantitative and qualitative results demonstrate the superiority of our approach in terms of per-frame quality, temporal correlation, and text-video alignment, with clear margins.},
  archive      = {J_IJCV},
  author       = {Wang, Wenjing and Yang, Huan and Tuo, Zixi and He, Huiguo and Zhu, Junchen and Fu, Jianlong and Liu, Jiaying},
  doi          = {10.1007/s11263-025-02349-y},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4177-4195},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Swap attention in spatiotemporal diffusions for text-to-video generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep convolutional neural network enhanced non-uniform fast fourier transform for undersampled MRI reconstruction. <em>IJCV</em>, <em>133</em>(7), 4158-4176. (<a href='https://doi.org/10.1007/s11263-025-02378-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NUFFT is widely used in MRI reconstruction, offering a balance of efficiency and accuracy. However, it struggles with uneven or sparse sampling, leading to unacceptable under sampling errors. To address this, we introduced DCNUFFT, a novel method that enhances NUFFT with deep convolutional neural network. The interpolation kernel and density compensation in inverse NUFFT were replaced with trainable neural network layers and incorporated a new global correlation prior in the spatial-frequency domain to better recover high-frequency information, enhancing reconstruction quality. DCNUFFT outperformed inverse NUFFT, iterative methods, and other deep learning approaches in terms of normalized root mean square error (NRMSE) and structural similarity index (SSIM) across various anatomies and sampling trajectories. Importantly, DCNUFFT also excelled in reconstructing under sampled PET and CT data, showing strong generalization capabilities. In subjective evaluations by radiologists, DCNUFFT scored highest in visual quality (VQ) and lesion distinguishing ability (LD), highlighting its clinical potential.},
  archive      = {J_IJCV},
  author       = {Li, Yuze and Qi, Haikun and Hu, Zhangxuan and Sun, Haozhong and Li, Guangqi and Zhang, Zhe and Liu, Yilong and Guo, Hua and Chen, Huijun},
  doi          = {10.1007/s11263-025-02378-7},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4158-4176},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep convolutional neural network enhanced non-uniform fast fourier transform for undersampled MRI reconstruction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual test-time adaptation for single image defocus deblurring via causal siamese networks. <em>IJCV</em>, <em>133</em>(7), 4134-4157. (<a href='https://doi.org/10.1007/s11263-025-02363-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image defocus deblurring (SIDD) aims to restore an all-in-focus image from a defocused one. Distribution shifts in defocused images generally lead to performance degradation of existing methods during out-of-distribution inferences. In this work, we gauge the intrinsic reason behind the performance degradation, which is identified as the heterogeneity of lens-specific point spread functions. Empirical evidence supports this finding, motivating us to employ a continual test-time adaptation (CTTA) paradigm for SIDD. However, traditional CTTA methods, which primarily rely on entropy minimization, cannot sufficiently explore task-dependent information for pixel-level regression tasks like SIDD. To address this issue, we propose a novel Siamese networks-based continual test-time adaptation framework, which adapts source models to continuously changing target domains only requiring unlabeled target data in an online manner. To further mitigate semantically erroneous textures introduced by source SIDD models under severe degradation, we revisit the learning paradigm through a structural causal model and propose Causal Siamese networks (CauSiam). Our method leverages large-scale pre-trained vision-language models to derive discriminative universal semantic priors and integrates these priors into Siamese networks, ensuring causal identifiability between blurry inputs and restored images. Extensive experiments demonstrate that CauSiam effectively improves the generalization performance of existing SIDD methods in continuously changing domains.},
  archive      = {J_IJCV},
  author       = {Cui, Shuang and Li, Yi and Li, Jiangmeng and Tang, Xiongxin and Su, Bing and Xu, Fanjiang and Xiong, Hui},
  doi          = {10.1007/s11263-025-02363-0},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4134-4157},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Continual test-time adaptation for single image defocus deblurring via causal siamese networks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bootstrapping vision-language models for frequency-centric self-supervised remote physiological measurement. <em>IJCV</em>, <em>133</em>(7), 4112-4133. (<a href='https://doi.org/10.1007/s11263-025-02388-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial video-based remote physiological measurement is a promising research area for detecting human vital signs (e.g., heart rate, respiration frequency) in a non-contact way. Conventional approaches are mostly supervised learning, requiring extensive collections of facial videos and synchronously recorded photoplethysmography (PPG) signals. To tackle it, self-supervised learning has recently gained attentions; due to the lack of ground truth PPG signals, its performance is however limited. In this paper, we propose a novel frequency-centric self-supervised framework that successfully integrates the popular vision-language models (VLMs) into the remote physiological measurement task. Given a facial video, we first augment its positive and negative video samples with varying rPPG signal frequencies. Next, we introduce a frequency-oriented vision-text pair generation method by carefully creating contrastive spatio-temporal maps from positive and negative samples and designing proper text prompts to describe their relative ratios of signal frequencies. A pre-trained VLM is employed to extract features for these formed vision-text pairs and estimate rPPG signals thereafter. We develop a series of frequency-related generative and contrastive learning mechanisms to optimize the VLM, including the text-guided visual reconstruction task, the vision-text contrastive learning task, and the frequency contrastive and ranking task. Overall, our method for the first time adapts VLMs to digest and align the frequency-related knowledge in vision and text modalities. Extensive experiments on four benchmark datasets demonstrate that it significantly outperforms state of the art self-supervised methods. Our codes will be available at https://github.com/yuezijie/Bootstrapping-VLM-for-Frequency-centric-Self-supervised-Remote-Physiological-Measurement .},
  archive      = {J_IJCV},
  author       = {Yue, Zijie and Shi, Miaojing and Wang, Hanli and Ding, Shuai and Chen, Qijun and Yang, Shanlin},
  doi          = {10.1007/s11263-025-02388-5},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4112-4133},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bootstrapping vision-language models for frequency-centric self-supervised remote physiological measurement},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image matting and 3D reconstruction in one loop. <em>IJCV</em>, <em>133</em>(7), 4091-4111. (<a href='https://doi.org/10.1007/s11263-024-02341-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent 3D object reconstruction methods rely on user-input alpha mattes to remove the background and reconstruct the object, because automatically predicted alpha mattes are not accurate enough. To realize automatic 3D object reconstruction, we propose a Joint framework for image Matting and 3D object Reconstruction (JointMR). It iteratively integrates information from all images into object hint maps to help image matting models predict better alpha mattes for each image and, in turn, improves 3D object reconstruction performance. The convergence of our framework is theoretically guaranteed. We further propose a method to convert an arbitrary image matting model into its hint-based counterpart. We conduct experiments on 3D object reconstruction from multi-view images and 3D dynamic object reconstruction from monocular videos. Different combinations of 3D object reconstruction models and image matting models are also tested. Experimental results show that our framework only slightly increases the computation cost but significantly improves the performance of all model combinations, demonstrating its compatibility and efficiency. Our code, models, and data are available at https://github.com/XinshuangL/JointMR .},
  archive      = {J_IJCV},
  author       = {Liu, Xinshuang and Li, Siqi and Gao, Yue},
  doi          = {10.1007/s11263-024-02341-y},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4091-4111},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Image matting and 3D reconstruction in one loop},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imbuing, enrichment and calibration: Leveraging language for unseen domain extension. <em>IJCV</em>, <em>133</em>(7), 4064-4090. (<a href='https://doi.org/10.1007/s11263-025-02382-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incorporation of language to enable model extension into unseen domains has gained significant interest in recent years. Previous methods commonly utilize semantically guided distributional shifts in training features to achieve this. Nevertheless, the intrinsic modal disparities between language and pixel-level images frequently result in a divergence within the feature manifold when employing semantic guidelines to augment features. This paper presents the IMbuing, Enrichment, and Calibration (IMEC) strategy as a concise solution for these issues. Unlike previous approaches, IMEC reverses the target domain style mining process to ensure the retention of semantic content within a more structured framework. Guided by global semantics, we conditionally generate style vectors for imbuing into visual features. After which IMEC introduces minor perturbations to disperse these vectors using local semantics and selectively calibrates semantic content in features through a dimensional activation strategy. IMEC integrates semantic abstract knowledge with detail image content, bridging the gap between synthetic and real samples in the target domain and mitigating content collapse resulting from semantic-visual disparities. Our model is evaluated on semantic segmentation, object detection, and image classification tasks across challenging datasets, demonstrating superior performance over existing methods in both the target and source domains. The code for IMEC is available at https://github.com/LanchJL/IMEC-ZSDE .},
  archive      = {J_IJCV},
  author       = {Jiang, Chenyi and Zhao, Jianqin and Deng, Jingjing and Li, Zechao and Zhang, Haofeng},
  doi          = {10.1007/s11263-025-02382-x},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4064-4090},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Imbuing, enrichment and calibration: Leveraging language for unseen domain extension},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instance-level moving object segmentation from a single image with events. <em>IJCV</em>, <em>133</em>(7), 4042-4063. (<a href='https://doi.org/10.1007/s11263-025-02380-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving object segmentation plays a crucial role in understanding dynamic scenes involving multiple moving objects, while the difficulties lie in taking into account both spatial texture structures and temporal motion cues. Existing methods based on video frames encounter difficulties in distinguishing whether pixel displacements of an object are caused by camera motion or object motion due to the complexities of accurate image-based motion modeling. Recent advances exploit the motion sensitivity of novel event cameras to counter conventional images’ inadequate motion modeling capabilities, but instead lead to challenges in segmenting pixel-level object masks due to the lack of dense texture structures in events. To address these two limitations imposed by unimodal settings, we propose the first instance-level moving object segmentation framework that integrates complementary texture and motion cues. Our model incorporates implicit cross-modal masked attention augmentation, explicit contrastive feature learning, and flow-guided motion enhancement to exploit dense texture information from a single image and rich motion information from events, respectively. By leveraging the augmented texture and motion features, we separate mask segmentation from motion classification to handle varying numbers of independently moving objects. Through extensive evaluations on multiple datasets, as well as ablation experiments with different input settings and real-time efficiency analysis of the proposed framework, we believe that our first attempt to incorporate image and event data for practical deployment can provide new insights for future work in event-based motion related works. The source code with model training and pre-trained weights is released at https://npucvr.github.io/EvInsMOS .},
  archive      = {J_IJCV},
  author       = {Wan, Zhexiong and Fan, Bin and Hui, Le and Dai, Yuchao and Lee, Gim Hee},
  doi          = {10.1007/s11263-025-02380-z},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4042-4063},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Instance-level moving object segmentation from a single image with events},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent prompt tuning for generalized category discovery. <em>IJCV</em>, <em>133</em>(7), 4014-4041. (<a href='https://doi.org/10.1007/s11263-024-02343-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Category Discovery (GCD) aims at discovering both known and unknown classes in unlabeled data, using the knowledge learned from a limited set of labeled data. Despite today’s foundation models being trained with Internet-scale multi-modal corpus, we find that they still struggle in GCD due to the ambiguity in class definitions. In this paper, we present Consistent Prompt Tuning (CPT) to disambiguate the classes for large vision-language models (e.g., CLIP). To this end, CPT learns a set of “task + class” prompts for labeled and unlabeled data of both known and unknown classes, with the “task” tokens globally shared across classes, which contain a unified class definition pattern, e.g., “the foreground is an animal named” or “the background scene is”. These prompts are optimized with two efficient regularization techniques that encourage consistent global and local relationships between any two matched inputs. CPT is evaluated on various existing GCD benchmarks, as well as in new practical scenarios with fewer annotations and customized class definitions, demonstrating clear superiority and broad versatility over existing state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Yang, Muli and Yin, Jie and Gu, Yanan and Deng, Cheng and Zhang, Hanwang and Zhu, Hongyuan},
  doi          = {10.1007/s11263-024-02343-w},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {4014-4041},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Consistent prompt tuning for generalized category discovery},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VL-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models. <em>IJCV</em>, <em>133</em>(7), 3994-4013. (<a href='https://doi.org/10.1007/s11263-025-02368-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoregressive Visual Language Models (VLMs) demonstrate remarkable few-shot learning capabilities within a multimodal context. Recently, multimodal instruction tuning has emerged as a technique to further refine instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by inserting poisoned samples with triggers embedded in instructions or images to datasets, enabling malicious manipulation of the victim model’s predictions with predefined triggers. However, the frozen visual encoder in autoregressive VLMs imposes constraints on learning conventional image triggers. Additionally, adversaries may lack access to the parameters and architectures of the victim model. To overcome these challenges, we introduce a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through active reshaping of poisoned features and enhances black-box attack efficacy through an iterative character-level text trigger generation method. Our attack successfully induces target output during inference, significantly outperforming baselines (+15.68%) in ASR. Furthermore, our attack demonstrates robustness across various model scales, architectures and few-shot in-context reasoning scenarios. Our codes are available at https://github.com/JWLiang007/VL-Trojan .},
  archive      = {J_IJCV},
  author       = {Liang, Jiawei and Liang, Siyuan and Liu, Aishan and Cao, Xiaochun},
  doi          = {10.1007/s11263-025-02368-9},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3994-4013},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {VL-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VideoQA in the era of LLMs: An empirical study. <em>IJCV</em>, <em>133</em>(7), 3970-3993. (<a href='https://doi.org/10.1007/s11263-025-02385-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Large Language Models (Video-LLMs) are flourishing and has advanced many video-language tasks. As a golden testbed, Video Question Answering (VideoQA) plays pivotal role in Video-LLM developing. This work conducts a timely and comprehensive study of Video-LLMs’ behavior in VideoQA, aiming to elucidate their success and failure modes, and provide insights towards more human-like video understanding and question answering. Our analyses demonstrate that Video-LLMs excel in VideoQA; they can correlate contextual cues and generate plausible responses to questions about varied video contents. However, models falter in handling video temporality, both in reasoning about temporal content ordering and grounding QA-relevant temporal moments. Moreover, the models behave unintuitively - they are unresponsive to adversarial video perturbations while being sensitive to simple variations of candidate answers and questions. Also, they do not necessarily generalize better. The findings demonstrate Video-LLMs’ QA capability in standard condition yet highlight their severe deficiency in robustness and interpretability, suggesting the urgent need on rationales in Video-LLM developing.},
  archive      = {J_IJCV},
  author       = {Xiao, Junbin and Huang, Nanxin and Qin, Hangyu and Li, Dongyang and Li, Yicong and Zhu, Fengbin and Tao, Zhulin and Yu, Jianxing and Lin, Liang and Chua, Tat-Seng and Yao, Angela},
  doi          = {10.1007/s11263-025-02385-8},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3970-3993},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {VideoQA in the era of LLMs: An empirical study},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusion4DAL: Offline multi-modal 3D object detection for 4D auto-labeling. <em>IJCV</em>, <em>133</em>(7), 3951-3969. (<a href='https://doi.org/10.1007/s11263-025-02370-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating LiDAR and camera information has been a widely adopted approach for 3D object detection in autonomous driving. Nevertheless, the unexplored potential of multi-modal fusion remains in the realm of offline 4D detection. We experimentally find that the root lies in two reasons: (1) the sparsity of point clouds poses a challenge in extracting long-term image features and thereby results in information loss. (2) some of the LiDAR points may be obstructed in the image, leading to incorrect image features. To tackle these problems, we first propose a simple yet effective offline multi-modal 3D object detection method, named Fusion4DAL, for 4D auto-labeling with long-term multi-modal sequences. Specifically, in order to address the sparsity of points within objects, we propose a multi-modal mixed feature fusion module (MMFF). In the MMFF module, we introduce virtual points based on a dense 3D grid and combine them with real LiDAR points. The mixed points are then utilized to extract dense point-level image features, thereby enhancing multi-modal feature fusion without being constrained by the sparse real LiDAR points. As to the obstructed LiDAR points, we leverage the occlusion relationship among objects to ensure depth consistency between LiDAR points and their corresponding depth feature maps, thus filtering out erroneous image features. In addition, we define a virtual point loss (VP Loss) to distinguish different types of mixed points and preserve the geometric shape of objects. Furthermore, in order to promote long-term receptive field and capture finer-grained features, we propose a global point attention decoder with a box-level self-attention module and a global point attention module. Finally, comprehensive experiments show that Fusion4DAL outperforms state-of-the-art offline 3D detection methods on nuScenes and Waymo dataset.},
  archive      = {J_IJCV},
  author       = {Yang, Zhiyuan and Wang, Xuekuan and Zhang, Wei and Tan, Xiao and Lu, Jincheng and Wang, Jingdong and Ding, Errui and Zhao, Cairong},
  doi          = {10.1007/s11263-025-02370-1},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3951-3969},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Fusion4DAL: Offline multi-modal 3D object detection for 4D auto-labeling},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An experimental study on exploring strong lightweight vision transformers via masked image modeling pre-training. <em>IJCV</em>, <em>133</em>(7), 3918-3950. (<a href='https://doi.org/10.1007/s11263-024-02327-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Masked image modeling (MIM) pre-training for large-scale vision transformers (ViTs) has enabled promising downstream performance on top of the learned self-supervised ViT features. In this paper, we question if the extremely simple lightweight ViTs’ fine-tuning performance can also benefit from this pre-training paradigm, which is considerably less studied yet in contrast to the well-established lightweight architecture design methodology. We use an observation-analysis-solution flow for our study. We first systematically observe different behaviors among the evaluated pre-training methods with respect to the downstream fine-tuning data scales. Furthermore, we analyze the layer representation similarities and attention maps across the obtained models, which clearly show the inferior learning of MIM pre-training on higher layers, leading to unsatisfactory transfer performance on data-insufficient downstream tasks. This finding is naturally a guide to designing our distillation strategies during pre-training to solve the above deterioration problem. Extensive experiments have demonstrated the effectiveness of our approach. Our pre-training with distillation on pure lightweight ViTs with vanilla/hierarchical design (5.7M/6.5M) can achieve $$79.4\%$$ / $$78.9\%$$ top-1 accuracy on ImageNet-1K. It also enables SOTA performance on the ADE20K segmentation task ( $$42.8\%$$ mIoU) and LaSOT tracking task ( $$66.1\%$$ AUC) in the lightweight regime. The latter even surpasses all the current SOTA lightweight CPU-realtime trackers.},
  archive      = {J_IJCV},
  author       = {Gao, Jin and Lin, Shubo and Wang, Shaoru and Kou, Yutong and Li, Zeming and Li, Liang and Zhang, Congxuan and Zhang, Xiaoqin and Wang, Yizheng and Hu, Weiming},
  doi          = {10.1007/s11263-024-02327-w},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3918-3950},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An experimental study on exploring strong lightweight vision transformers via masked image modeling pre-training},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smaller but better: Unifying layout generation with smaller large language models. <em>IJCV</em>, <em>133</em>(7), 3891-3917. (<a href='https://doi.org/10.1007/s11263-025-02353-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose LGGPT, an LLM-based model tailored for unified layout generation. First, we propose Arbitrary Layout Instruction (ALI) and Universal Layout Response (ULR) as the uniform I/O template. ALI accommodates arbitrary layout generation task inputs across multiple layout domains, enabling LGGPT to unify both task-generic and domain-generic layout generation hitherto unexplored. Collectively, ALI and ULR boast a succinct structure that forgoes superfluous tokens typically found in existing HTML-based formats, facilitating efficient instruction tuning and boosting unified generation performance. In addition, we propose an Interval Quantization Encoding (IQE) strategy that compresses ALI into a more condensed structure. IQE precisely preserves valid layout clues while eliminating the less informative placeholders, facilitating LGGPT to capture complex and variable layout generation conditions during the unified training process. Experimental results demonstrate that LGGPT achieves superior or on par performance compared to existing methods. Notably, LGGPT strikes a prominent balance between proficiency and efficiency with a compact 1.5B parameter LLM, which beats prior 7B or 175B models even in the most extensive and challenging unified scenario. Furthermore, we underscore the necessity of employing LLMs for unified layout generation and suggest that 1.5B could be an optimal parameter size by comparing LLMs of varying scales. Code is available at https://github.com/NiceRingNode/LGGPT .},
  archive      = {J_IJCV},
  author       = {Zhang, Peirong and Zhang, Jiaxin and Cao, Jiahuan and Li, Hongliang and Jin, Lianwen},
  doi          = {10.1007/s11263-025-02353-2},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3891-3917},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Smaller but better: Unifying layout generation with smaller large language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiDAR-guided geometric pretraining for vision-centric 3D object detection. <em>IJCV</em>, <em>133</em>(7), 3877-3890. (<a href='https://doi.org/10.1007/s11263-025-02351-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-camera 3D object detection for autonomous driving is a challenging problem that has garnered notable attention from both academia and industry. An obstacle encountered in vision-based techniques involves the precise extraction of geometry-conscious features from RGB images. Recent approaches have utilized geometric-aware image backbones pretrained on depth-relevant tasks to acquire spatial information. However, these approaches overlook the critical aspect of view transformation, resulting in inadequate performance due to the misalignment of spatial knowledge between the image backbone and view transformation. To address this issue, we propose a novel geometric-aware pretraining framework called GAPretrain. Our approach incorporates spatial and structural cues to camera networks by employing the geometric-rich modality as guidance during the pretraining phase. The transference of modal-specific attributes across different modalities is non-trivial, but we bridge this gap by using a unified bird’s-eye-view (BEV) representation and structural hints derived from LiDAR point clouds to facilitate the pretraining process. GAPretrain serves as a plug-and-play solution that can be flexibly applied to multiple state-of-the-art detectors. Our experiments demonstrate the effectiveness and generalization ability of the proposed method. We achieve 46.2 mAP and 55.5 NDS on the nuScenes val set using the BEVFormer method, with a gain of 2.7 and 2.1 points, respectively. We also conduct experiments on various image backbones and view transformations to validate the efficacy of our approach. Code will be released at https://github.com/OpenDriveLab/BEVPerception-Survey-Recipe .},
  archive      = {J_IJCV},
  author       = {Huang, Linyan and Wang, Huijie and Zeng, Jia and Zhang, Shengchuan and Cao, Liujuan and Yan, Junchi and Li, Hongyang},
  doi          = {10.1007/s11263-025-02351-4},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3877-3890},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LiDAR-guided geometric pretraining for vision-centric 3D object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning structure-supporting dependencies via keypoint interactive transformer for general mammal pose estimation. <em>IJCV</em>, <em>133</em>(7), 3858-3876. (<a href='https://doi.org/10.1007/s11263-025-02355-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General mammal pose estimation is an important and challenging task in computer vision, which is essential for understanding mammal behaviour in real-world applications. However, existing studies are at their preliminary research stage, which focus on addressing the problem for only a few specific mammal species. In principle, from specific to general mammal pose estimation, the biggest issue is how to address the huge appearance and pose variances for different species. We argue that given appearance context, instance-level prior and the structural relation among keypoints can serve as complementary evidence. To this end, we propose a Keypoint Interactive Transformer (KIT) to learn instance-level structure-supporting dependencies for general mammal pose estimation. Specifically, our KITPose consists of two coupled components. The first component is to extract keypoint features and generate body part prompts. The features are supervised by a dedicated generalised heatmap regression loss (GHRL). Instead of introducing external visual/text prompts, we devise keypoints clustering to generate body part biases, aligning them with image context to generate corresponding instance-level prompts. Second, we propose a novel interactive transformer that takes feature slices as input tokens without performing spatial splitting. In addition, to enhance the capability of the KIT model, we design an adaptive weight strategy to address the imbalance issue among different keypoints. Extensive experimental results obtained on the widely used animal datasets, AP10K and AnimalKingdom, demonstrate the superiority of the proposed method over the state-of-the-art approaches. It achieves 77.9 AP on the AP10K val set, outperforming HRFormer by 2.2. Besides, our KITPose can be directly transferred to human pose estimation with promising results, as evaluated on COCO, reflecting the merits of constructing structure-supporting architectures for general mammal pose estimation.},
  archive      = {J_IJCV},
  author       = {Xu, Tianyang and Rao, Jiyong and Song, Xiaoning and Feng, Zhenhua and Wu, Xiao-Jun},
  doi          = {10.1007/s11263-025-02355-0},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3858-3876},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning structure-supporting dependencies via keypoint interactive transformer for general mammal pose estimation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards boosting out-of-distribution detection from a spatial feature importance perspective. <em>IJCV</em>, <em>133</em>(7), 3839-3857. (<a href='https://doi.org/10.1007/s11263-025-02347-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ensuring the reliable and secure operation of models, Out-of-Distribution (OOD) detection has gained widespread attention in recent years. Researchers have proposed various promising detection criteria to construct the rejection region of the model, treating samples falling into this region as out-of-distribution. However, these detection criteria are computed using all dense features of the model (before the pooling layer), overlooking the fact that different features may exhibit varying importance in the decision-making process. To this end, we first propose quantifying the contribution of different spatial positions in the dense features to the decision result with the Shapley Value, thereby obtaining the feature importance map. This spatial-oriented feature attribution method, compared to the classical channel-oriented feature attribution method that linearly combines weights with each activation map, achieves superior visual performance and fidelity in interpreting the decision-making process. Subsequently, we introduce a spatial feature purification method that removes spatial features with low importance in dense features and advocates using these purified features to compute detection criteria. Extensive experiments demonstrate the effectiveness of spatial feature purification in enhancing the performance of various existing detection methods. Notably, spatial feature purification boosts Energy Score and NNGuide on the ImageNet benchmark by 18.39 $$\%$$ and 26.45 $$\%$$ in average FPR95, respectively.},
  archive      = {J_IJCV},
  author       = {Zhu, Yao and Yan, Xiu and Xie, Chuanlong},
  doi          = {10.1007/s11263-025-02347-0},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3839-3857},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards boosting out-of-distribution detection from a spatial feature importance perspective},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive decoupled representation learning and regularization for speech-preserving facial expression manipulation. <em>IJCV</em>, <em>133</em>(7), 3822-3838. (<a href='https://doi.org/10.1007/s11263-025-02358-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech-preserving facial expression manipulation (SPFEM) aims to modify a talking head to display a specific reference emotion while preserving the mouth animation of source spoken contents. Thus, emotion and content information existing in reference and source inputs can provide direct and accurate supervision signals for SPFEM models. However, the intrinsic intertwining of these elements during the talking process poses challenges to their effectiveness as supervisory signals. In this work, we propose to learn content and emotion priors as guidance augmented with contrastive learning to learn decoupled content and emotion representation via an innovative Contrastive Decoupled Representation Learning (CDRL) algorithm. Specifically, a Contrastive Content Representation Learning (CCRL) module is designed to learn audio feature, which primarily contains content information, as content priors to guide learning content representation from the source input. Meanwhile, a Contrastive Emotion Representation Learning (CERL) module is proposed to make use of a pre-trained visual-language model to learn emotion prior, which is then used to guide learning emotion representation from the reference input. We further introduce emotion-aware and emotion-augmented contrastive learning to train CCRL and CERL modules, respectively, ensuring learning emotion-independent content representation and content-independent emotion representation. During SPFEM model training, the decoupled content and emotion representations are used to supervise the generation process, ensuring more accurate emotion manipulation together with audio-lip synchronization. Extensive experiments and evaluations on various benchmarks show the effectiveness of the proposed algorithm.},
  archive      = {J_IJCV},
  author       = {Chen, Tianshui and Lin, Jianman and Yang, Zhijing and Qing, Chumei and Shi, Yukai and Lin, Liang},
  doi          = {10.1007/s11263-025-02358-x},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3822-3838},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Contrastive decoupled representation learning and regularization for speech-preserving facial expression manipulation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffuVolume: Diffusion model for volume based stereo matching. <em>IJCV</em>, <em>133</em>(7), 3807-3821. (<a href='https://doi.org/10.1007/s11263-025-02362-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching is a significant part in many computer vision tasks and driving-based applications. Recently cost volume-based methods have achieved great success benefiting from the rich geometry information in paired images. However, the redundancy of cost volume also interferes with the model training and limits the performance. To construct a more precise cost volume, we pioneeringly apply the diffusion model to stereo matching. Our method, termed DiffuVolume, considers the diffusion model as a cost volume filter, which will recurrently remove the redundant information from the cost volume. Two main designs make our method not trivial. Firstly, to make the diffusion model more adaptive to stereo matching, we eschew the traditional manner of directly adding noise into the image but embed the diffusion model into a task-specific module. In this way, we outperform the traditional diffusion stereo matching method by 27 $$\%$$ EPE improvement and 7 times parameters reduction. Secondly, DiffuVolume can be easily embedded into any volume-based stereo matching network, boosting performance with only a slight increase in parameters (approximately 2 $$\%$$ ). By adding the DiffuVolume into well-performed methods, we outperform all the published methods on Scene Flow, KITTI2012, KITTI2015 benchmarks and zero-shot generalization setting. It is worth mentioning that the proposed model ranks 1st on KITTI 2012 leader board, 2nd on KITTI 2015 leader board since 15, July 2023.},
  archive      = {J_IJCV},
  author       = {Zheng, Dian and Wu, Xiao-Ming and Liu, Zuhao and Meng, Jingke and Zheng, Wei-Shi},
  doi          = {10.1007/s11263-025-02362-1},
  journal      = {International Journal of Computer Vision},
  month        = {7},
  number       = {7},
  pages        = {3807-3821},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DiffuVolume: Diffusion model for volume based stereo matching},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Few annotated pixels and point cloud based weakly supervised semantic segmentation of driving scenes. <em>IJCV</em>, <em>133</em>(6), 3805. (<a href='https://doi.org/10.1007/s11263-025-02366-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Ma, Huimin and Yi, Sheng and Chen, Shijie and Chen, Jiansheng and Wang, Yu},
  doi          = {10.1007/s11263-025-02366-x},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3805},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Few annotated pixels and point cloud based weakly supervised semantic segmentation of driving scenes},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Deep attention learning for pre-operative lymph node metastasis prediction in pancreatic cancer via multi-object relationship modeling. <em>IJCV</em>, <em>133</em>(6), 3804. (<a href='https://doi.org/10.1007/s11263-025-02360-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zheng, Zhilin and Fang, Xu and Yao, Jiawen and Zhu, Mengmeng and Lu, Le and Shi, Yu and Lu, Hong and Lu, Jianping and Zhang, Ling and Shao, Chengwei and Bian, Yun},
  doi          = {10.1007/s11263-025-02360-3},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3804},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Deep attention learning for pre-operative lymph node metastasis prediction in pancreatic cancer via multi-object relationship modeling},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: CMAE-3D: Contrastive masked AutoEncoders for self-supervised 3D object detection. <em>IJCV</em>, <em>133</em>(6), 3803. (<a href='https://doi.org/10.1007/s11263-025-02359-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zhang, Yanan and Chen, Jiaxin and Huang, Di},
  doi          = {10.1007/s11263-025-02359-w},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3803},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: CMAE-3D: Contrastive masked AutoEncoders for self-supervised 3D object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TryOn-adapter: Efficient fine-grained clothing identity adaptation for high-fidelity virtual try-on. <em>IJCV</em>, <em>133</em>(6), 3781-3802. (<a href='https://doi.org/10.1007/s11263-025-02352-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual try-on focuses on adjusting the given clothes to fit a specific person seamlessly while avoiding any distortion of the patterns and textures of the garment. However, the clothing identity uncontrollability and training inefficiency of existing diffusion-based methods, which struggle to maintain the identity even with full parameter training, are significant limitations that hinder the widespread applications. In this work, we propose an effective and efficient framework, termed TryOn-Adapter. Specifically, we first decouple clothing identity into fine-grained factors: style for color and category information, texture for high-frequency details, and structure for smooth spatial adaptive transformation. Our approach utilizes a pre-trained exemplar-based diffusion model as the fundamental network, whose parameters are frozen except for the attention layers. We then customize three lightweight modules (Style Preserving, Texture Highlighting, and Structure Adapting) incorporated with fine-tuning techniques to enable precise and efficient identity control. Meanwhile, we introduce the training-free T-RePaint strategy to further enhance clothing identity preservation while maintaining the realistic try-on effect during the inference. Our experiments demonstrate that our approach achieves state-of-the-art performance on two widely-used benchmarks. Additionally, compared with recent full-tuning diffusion-based methods, we only use about half of their tunable parameters during training. The code will be made publicly available at https://github.com/jiazheng-xing/TryOn-Adapter .},
  archive      = {J_IJCV},
  author       = {Xing, Jiazheng and Xu, Chao and Qian, Yijie and Liu, Yang and Dai, Guang and Sun, Baigui and Liu, Yong and Wang, Jingdong},
  doi          = {10.1007/s11263-025-02352-3},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3781-3802},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {TryOn-adapter: Efficient fine-grained clothing identity adaptation for high-fidelity virtual try-on},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised shutter unrolling with events. <em>IJCV</em>, <em>133</em>(6), 3762-3780. (<a href='https://doi.org/10.1007/s11263-025-02364-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous-time Global Shutter Video Recovery (CGVR) faces a substantial challenge in recovering undistorted high frame-rate Global Shutter (GS) videos from distorted Rolling Shutter (RS) images. This problem is severely ill-posed due to the absence of temporal dynamic information within RS intra-frame scanlines and inter-frame exposures, particularly when prior knowledge about camera/object motions is unavailable. Commonly used artificial assumptions on scenes/motions and data-specific characteristics are prone to producing sub-optimal solutions in real-world scenarios. To address this challenge, we propose an event-based CGVR network within a self-supervised learning paradigm, i.e., SelfUnroll, and leverage the extremely high temporal resolution of event cameras to provide accurate inter/intra-frame dynamic information. Specifically, an Event-based Inter/intra-frame Compensator (E-IC) is proposed to predict the per-pixel dynamic between arbitrary time intervals, including the temporal transition and spatial translation. Exploring connections in terms of RS-RS, RS-GS, and GS-RS, we explicitly formulate mutual constraints with the proposed E-IC, resulting in supervisions without ground-truth GS images. Extensive evaluations over synthetic and real datasets demonstrate that the proposed method achieves state-of-the-art methods and shows remarkable performance for event-based RS2GS inversion in real-world scenarios. The dataset and code are available at https://w3un.github.io/selfunroll/ .},
  archive      = {J_IJCV},
  author       = {Lin, Mingyuan and Wang, Yangguang and Zhang, Xiang and Shi, Boxin and Yang, Wen and He, Chu and Xia, Gui-song and Yu, Lei},
  doi          = {10.1007/s11263-025-02364-z},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3762-3780},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Self-supervised shutter unrolling with events},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with enriched inductive biases for vision-language models. <em>IJCV</em>, <em>133</em>(6), 3746-3761. (<a href='https://doi.org/10.1007/s11263-025-02354-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-Language Models, pre-trained on large-scale image-text pairs, serve as strong foundation models for transfer learning across a variety of downstream tasks. For few-shot generalization tasks, i.e., when the model is trained on few-shot samples and then tested on unseen categories or datasets, there is a balance to be struck between generalization and discrimination when tweaking these models. Existing approaches typically rely on one or two strategies during training to learn task-specific knowledge, while preserving as much task-agnostic representation as possible. However, these methods overlook the importance of other useful inductive biases, thereby limiting their generalization capabilities. In this work, we propose a method – Learning with Enriched Inductive Biases (LwEIB) – to explore multiple inductive biases at the text, model, and optimization levels. Specifically, we first propose to enrich the handcrafted text prompt with Large Language Model generated descriptions for each category. To better capture structural cues in both linguistics and vision, we design two new adapters for text and image encoders, respectively. Additionally, we propose a slow-fast optimization method to explore different degrees of adaptation more efficiently, learning task-specific representations while maintaining task-agnostic ones. We empirically validate the effectiveness of LwEIB on three widely used benchmarks. Remarkably, our LwEIB outperforms numerous state-of-the-art methods across all evaluation metrics, demonstrating its efficacy and versatility. Our code is available at https://github.com/ZjjConan/VLM-LwEIB .},
  archive      = {J_IJCV},
  author       = {Yang, Lingxiao and Zhang, Ru-Yuan and Chen, Qi and Xie, Xiaohua},
  doi          = {10.1007/s11263-025-02354-1},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3746-3761},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning with enriched inductive biases for vision-language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample-cohesive pose-aware contrastive facial representation learning. <em>IJCV</em>, <em>133</em>(6), 3727-3745. (<a href='https://doi.org/10.1007/s11263-025-02348-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised facial representation learning (SFRL) methods, especially contrastive learning (CL) methods, have been increasingly popular due to their ability to perform face understanding without heavily relying on large-scale well-annotated datasets. However, analytically, current CL-based SFRL methods still perform unsatisfactorily in learning facial representations due to their tendency to learn pose-insensitive features, resulting in the loss of some useful pose details. This could be due to the inappropriate positive/negative pair selection within CL. To conquer this challenge, we propose a Pose-disentangled Contrastive Facial Representation Learning (PCFRL) framework to enhance pose awareness for SFRL. We achieve this by explicitly disentangling the pose-aware features from non-pose face-aware features and introducing appropriate sample calibration schemes for better CL with the disentangled features. In PCFRL, we first devise a pose-disentangled decoder with a delicately designed orthogonalizing regulation to perform the disentanglement; therefore, the learning on the pose-aware and non-pose face-aware features would not affect each other. Then, we introduce a false-negative pair calibration module to overcome the issue that the two types of disentangled features may not share the same negative pairs for CL. Our calibration employs a novel neighborhood-cohesive pair alignment method to identify pose and face false-negative pairs, respectively, and further help calibrate them to appropriate positive pairs. Lastly, we devise two calibrated CL losses, namely calibrated pose-aware and face-aware CL losses, for adaptively learning the calibrated pairs more effectively, ultimately enhancing the learning with the disentangled features and providing robust facial representations for various downstream tasks. In the experiments, we perform linear evaluations on four challenging downstream facial tasks with SFRL using our method, including facial expression recognition, face recognition, facial action unit detection, and head pose estimation. Experimental results show that PCFRL outperforms existing state-of-the-art methods by a substantial margin, demonstrating the importance of improving pose awareness for SFRL. Our evaluation code and model will be available at https://github.com/fulaoze/CV/tree/main .},
  archive      = {J_IJCV},
  author       = {Liu, Yuanyuan and Feng, Shaoze and Liu, Shuyang and Zhan, Yibing and Tao, Dapeng and Chen, Zijing and Chen, Zhe},
  doi          = {10.1007/s11263-025-02348-z},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3727-3745},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Sample-cohesive pose-aware contrastive facial representation learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image synthesis under limited data: A survey and taxonomy. <em>IJCV</em>, <em>133</em>(6), 3689-3726. (<a href='https://doi.org/10.1007/s11263-025-02357-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models, which target reproducing the data distribution to produce novel images, have made unprecedented advancements in recent years. However, one critical prerequisite for their tremendous success is the availability of a sufficient number of training samples, which requires massive computation resources. When trained on limited data, generative models tend to suffer from severe performance deterioration due to overfitting and memorization. Accordingly, researchers have devoted considerable attention to develop novel models that are capable of generating plausible and diverse images from limited training data recently. Despite numerous efforts to enhance training stability and synthesis quality in the limited data scenarios, there is a lack of a systematic survey that provides (1) a clear problem definition, challenges, and taxonomy of various tasks; (2) an in-depth analysis on the pros, cons, and limitations of existing literature; and (3) a thorough discussion on the potential applications and future directions in this field. To fill this gap and provide an informative introduction to researchers who are new to this topic, this survey offers a comprehensive review and a novel taxonomy on the development of image synthesis under limited data. In particular, it covers the problem definition, requirements, main solutions, popular benchmarks, and remaining challenges in a comprehensive and all-around manner. We hope this survey can provide an informative overview and a valuable resource for researchers and practitioners. Apart from the relevant references, we aim to constantly maintain a timely up-to-date repository to track the latest advances at awesome-few-shot-generation .},
  archive      = {J_IJCV},
  author       = {Yang, Mengping and Wang, Zhe},
  doi          = {10.1007/s11263-025-02357-y},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3689-3726},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Image synthesis under limited data: A survey and taxonomy},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-space video person re-identification. <em>IJCV</em>, <em>133</em>(6), 3667-3688. (<a href='https://doi.org/10.1007/s11263-025-02350-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video person re-identification (VReID) aims to recognize individuals across video sequences. Existing methods primarily use Euclidean space for representation learning but struggle to capture complex hierarchical structures, especially in scenarios with occlusions and background clutter. In contrast, hyperbolic space, with its negatively curved geometry, excels at preserving hierarchical relationships and enhancing discrimination between similar appearances. Inspired by these, we propose Dual-Space Video Person Re-Identification (DS-VReID) to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features while also exploring the intrinsic hierarchical relations, thereby enhancing the discriminative capacity of the features. Specifically, we design the Dynamic Prompt Graph Construction (DPGC) module, which uses a pre-trained CLIP model with learnable dynamic prompts to construct 3D graphs that capture subtle changes and dynamic information in video sequences. Building upon this, we introduce the Hyperbolic Disentangled Aggregation (HDA) module, which addresses long-range dependency modeling by decoupling node distances and integrating adjacency matrices, capturing detailed spatial-temporal hierarchical relationships. Extensive experiments on benchmark datasets demonstrate the superiority of DS-VReID over state-of-the-art methods, showcasing its potential in complex VReID scenarios.},
  archive      = {J_IJCV},
  author       = {Leng, Jiaxu and Kuang, Changjiang and Li, Shuang and Gan, Ji and Chen, Haosheng and Gao, Xinbo},
  doi          = {10.1007/s11263-025-02350-5},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3667-3688},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dual-space video person re-identification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SeaFormer++: Squeeze-enhanced axial transformer for mobile visual recognition. <em>IJCV</em>, <em>133</em>(6), 3645-3666. (<a href='https://doi.org/10.1007/s11263-025-02345-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the introduction of Vision Transformers, the landscape of many computer vision tasks (e.g., semantic segmentation), which has been overwhelmingly dominated by CNNs, recently has significantly revolutionized. However, the computational cost and memory requirement renders these methods unsuitable on the mobile device. In this paper, we introduce a new method squeeze-enhanced Axial Transformer (SeaFormer) for mobile visual recognition. Specifically, we design a generic attention block characterized by the formulation of squeeze Axial and detail enhancement. It can be further used to create a family of backbone architectures with superior cost-effectiveness. Coupled with a light segmentation head, we achieve the best trade-off between segmentation accuracy and latency on the ARM-based mobile devices on the ADE20K, Cityscapes Pascal Context and COCO-Stuff datasets. Critically, we beat both the mobile-friendly rivals and Transformer-based counterparts with better performance and lower latency without bells and whistles. Furthermore, we incorporate a feature upsampling-based multi-resolution distillation technique, further reducing the inference latency of the proposed framework. Beyond semantic segmentation, we further apply the proposed SeaFormer architecture to image classification and object detection problems, demonstrating the potential of serving as a versatile mobile-friendly backbone. Our code and models are made publicly available at https://github.com/fudan-zvg/SeaFormer .},
  archive      = {J_IJCV},
  author       = {Wan, Qiang and Huang, Zilong and Lu, Jiachen and Yu, Gang and Zhang, Li},
  doi          = {10.1007/s11263-025-02345-2},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3645-3666},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SeaFormer++: Squeeze-enhanced axial transformer for mobile visual recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MoonShot: Towards controllable video generation and editing with motion-aware multimodal conditions. <em>IJCV</em>, <em>133</em>(6), 3629-3644. (<a href='https://doi.org/10.1007/s11263-025-02346-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current video diffusion models (VDMs) mostly rely on text conditions, limiting control over video appearance and geometry. This study introduces a new model, MoonShot, conditioning on both image and text for enhanced control. It features the Multimodal Video Block (MVB), integrating the motion-aware dual cross-attention layer for precise appearance and motion alignment with provided prompts, and the spatiotemporal attention layer for large motion dynamics. It can also incorporate pre-trained Image ControlNet modules for geometry conditioning without extra video training. Experiments show our model significantly improves visual quality and motion fidelity, and its versatility allows for applications in personalized video generation, animation, and editing, making it a foundational tool for controllable video creation. More video results can be found here .},
  archive      = {J_IJCV},
  author       = {Zhang, David Junhao and Li, Dongxu and Le, Hung and Shou, Mike Zheng and Xiong, Caiming and Sahoo, Doyen},
  doi          = {10.1007/s11263-025-02346-1},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3629-3644},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MoonShot: Towards controllable video generation and editing with motion-aware multimodal conditions},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepFake-adapter: Dual-level adapter for DeepFake detection. <em>IJCV</em>, <em>133</em>(6), 3613-3628. (<a href='https://doi.org/10.1007/s11263-024-02274-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deepfake detection methods fail to generalize well to unseen or degraded samples, which can be attributed to the over-fitting of low-level forgery patterns. Here we argue that high-level semantics are also indispensable recipes for generalizable forgery detection. Recently, large pre-trained Vision Transformers (ViTs) have shown promising generalization capability. In this paper, we propose the first parameter-efficient tuning approach for deepfake detection, namely DeepFake-Adapter, to effectively and efficiently adapt the generalizable high-level semantics from large pre-trained ViTs to aid deepfake detection. Given large pre-trained models but limited deepfake data, DeepFake-Adapter introduces lightweight yet dedicated dual-level adapter modules to a ViT while keeping the model backbone frozen. Specifically, to guide the adaptation process to be aware of both global and local forgery cues of deepfake data, 1) we not only insert Globally-aware Bottleneck Adapters in parallel to MLP layers of ViT, 2) but also actively cross-attend Locally-aware Spatial Adapters with features from ViT. Unlike existing deepfake detection methods merely focusing on low-level forgery patterns, the forgery detection process of our model can be regularized by generalizable high-level semantics from a pre-trained ViT and adapted by global and local low-level forgeries of deepfake data. Extensive experiments on several standard deepfake detection benchmarks validate the effectiveness of our approach. Notably, DeepFake-Adapter demonstrates a convincing advantage under cross-dataset and cross-manipulation settings.},
  archive      = {J_IJCV},
  author       = {Shao, Rui and Wu, Tianxing and Nie, Liqiang and Liu, Ziwei},
  doi          = {10.1007/s11263-024-02274-6},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3613-3628},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DeepFake-adapter: Dual-level adapter for DeepFake detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mutual supervision framework for referring expression segmentation and generation. <em>IJCV</em>, <em>133</em>(6), 3597-3612. (<a href='https://doi.org/10.1007/s11263-024-02325-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reference Expression Segmentation (RES) and Reference Expression Generation (REG) are mutually inverse tasks that can be naturally jointly trained. Though recent work has explored such joint training, the mechanism of how RES and REG can benefit each other is still unclear. In this paper, we propose a mutual supervision framework that enables two tasks to improve each other. Our mutual supervision contains two directions. On the one hand, Disambiguation Supervision leverages the expression unambiguity measurement provided by RES to enhance the language generation of REG. On the other hand, Generation Supervision uses expressions automatically generated by REG to scale up the training of RES. Such mutual supervision effectively improves two tasks by solving their bottleneck problems. Extensive experiments show that our approach significantly outperforms all existing methods on REG and RES tasks under the same setting, and detailed ablation studies demonstrate the effectiveness of all components in our framework.},
  archive      = {J_IJCV},
  author       = {Huang, Shijia and Li, Feng and Zhang, Hao and Liu, Shilong and Zhang, Lei and Wang, Liwei},
  doi          = {10.1007/s11263-024-02325-y},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3597-3612},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A mutual supervision framework for referring expression segmentation and generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GL-MCM: Global and local maximum concept matching for zero-shot out-of-distribution detection. <em>IJCV</em>, <em>133</em>(6), 3586-3596. (<a href='https://doi.org/10.1007/s11263-025-02356-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot OOD detection is a task that detects OOD images during inference with only in-distribution (ID) class names. Existing methods assume ID images contain a single, centered object, and do not consider the more realistic multi-object scenarios, where both ID and OOD objects are present. To meet the needs of many users, the detection method must have the flexibility to adapt the type of ID images. To this end, we present Global-Local Maximum Concept Matching (GL-MCM), which incorporates local image scores as an auxiliary score to enhance the separability of global and local visual features. Due to the simple ensemble score function design, GL-MCM can control the type of ID images with a single weight parameter. Experiments on ImageNet and multi-object benchmarks demonstrate that GL-MCM outperforms baseline zero-shot methods and is comparable to fully supervised methods. Furthermore, GL-MCM offers strong flexibility in adjusting the target type of ID images. The code is available via https://github.com/AtsuMiyai/GL-MCM .},
  archive      = {J_IJCV},
  author       = {Miyai, Atsuyuki and Yu, Qing and Irie, Go and Aizawa, Kiyoharu},
  doi          = {10.1007/s11263-025-02356-z},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3586-3596},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {GL-MCM: Global and local maximum concept matching for zero-shot out-of-distribution detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pre-trained trojan attacks for visual recognition. <em>IJCV</em>, <em>133</em>(6), 3568-3585. (<a href='https://doi.org/10.1007/s11263-024-02333-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained vision models (PVMs) have become a dominant component due to their exceptional performance when fine-tuned for downstream tasks. However, the presence of backdoors within PVMs poses significant threats. Unfortunately, existing studies primarily focus on backdooring PVMs for the classification task, neglecting potential inherited backdoors in downstream tasks such as detection and segmentation. In this paper, we propose the Pre-trained Trojan attack, which embeds backdoors into a PVM, enabling attacks across various downstream vision tasks. We highlight the challenges posed by cross-task activation and shortcut connections in successful backdoor attacks. To achieve effective trigger activation in diverse tasks, we stylize the backdoor trigger patterns with class-specific textures, enhancing the recognition of task-irrelevant low-level features associated with the target class in the trigger pattern. Moreover, we address the issue of shortcut connections by introducing a context-free learning pipeline for poison training. In this approach, triggers without contextual backgrounds are directly utilized as training data, diverging from the conventional use of clean images. Consequently, we establish a direct shortcut from the trigger to the target class, mitigating the shortcut connection issue. We conducted extensive experiments to thoroughly validate the effectiveness of our attacks on downstream detection and segmentation tasks. Additionally, we showcase the potential of our approach in more practical scenarios, including large vision models and 3D object detection in autonomous driving. This paper aims to raise awareness of the potential threats associated with applying PVMs in practical scenarios. Our codes are available at https://github.com/Veee9/Pre-trained-Trojan .},
  archive      = {J_IJCV},
  author       = {Liu, Aishan and Liu, Xianglong and Zhang, Xinwei and Xiao, Yisong and Zhou, Yuguang and Liang, Siyuan and Wang, Jiakai and Cao, Xiaochun and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02333-y},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3568-3585},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pre-trained trojan attacks for visual recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking generalizability and discriminability of self-supervised learning from evolutionary game theory perspective. <em>IJCV</em>, <em>133</em>(6), 3542-3567. (<a href='https://doi.org/10.1007/s11263-024-02321-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representations learned by self-supervised approaches are generally considered to possess sufficient generalizability and discriminability. However, we disclose a nontrivial mutual-exclusion relationship between these critical representation properties through an exploratory demonstration on self-supervised learning. State-of-the-art self-supervised methods tend to enhance either generalizability or discriminability but not both simultaneously. Thus, learning representations jointly possessing strong generalizability and discriminability presents a specific challenge for self-supervised learning. To this end, we revisit the learning paradigm of self-supervised learning from the perspective of evolutionary game theory (EGT) and outline the theoretical roadmap to achieve a desired trade-off between these representation properties. EGT performs well in analyzing the trade-off point in a two-player game by utilizing dynamic system modeling. However, the EGT analysis requires sufficient annotated data, which contradicts the principle of self-supervised learning, i.e., the EGT analysis cannot be conducted without the annotations of the specific target domain for self-supervised learning. Thus, to enhance the methodological generalization, we propose a novel self-supervised learning method that leverages advancements in reinforcement learning to jointly benefit from the general guidance of EGT and sequentially optimize the model to chase the consistent improvement of generalizability and discriminability for specific target domains during pre-training. On top of this, we provide a benchmark to evaluate the generalizability and discriminability of learned representations comprehensively. Theoretically, we establish that the proposed method tightens the generalization error upper bound of self-supervised learning. Empirically, our method achieves state-of-the-art performance on various benchmarks. Our implementation is available at https://github.com/ZangZehua/essl .},
  archive      = {J_IJCV},
  author       = {Li, Jiangmeng and Zang, Zehua and Ji, Qirui and Sun, Chuxiong and Qiang, Wenwen and Zhang, Junge and Zheng, Changwen and Sun, Fuchun and Xiong, Hui},
  doi          = {10.1007/s11263-024-02321-2},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3542-3567},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rethinking generalizability and discriminability of self-supervised learning from evolutionary game theory perspective},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised semantic segmentation of urban scenes via cross-modal distillation. <em>IJCV</em>, <em>133</em>(6), 3519-3541. (<a href='https://doi.org/10.1007/s11263-024-02320-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic image segmentation models typically require extensive pixel-wise annotations, which are costly to obtain and prone to biases. Our work investigates learning semantic segmentation in urban scenes without any manual annotation. We propose a novel method for learning pixel-wise semantic segmentation using raw, uncurated data from vehicle-mounted cameras and LiDAR sensors, thus eliminating the need for manual labeling. Our contributions are as follows. First, we develop a novel approach for cross-modal unsupervised learning of semantic segmentation by leveraging synchronized LiDAR and image data. A crucial element of our method is the integration of an object proposal module that examines the LiDAR point cloud to generate proposals for spatially consistent objects. Second, we demonstrate that these 3D object proposals can be aligned with corresponding images and effectively grouped into semantically meaningful pseudo-classes. Third, we introduce a cross-modal distillation technique that utilizes image data partially annotated with the learnt pseudo-classes to train a transformer-based model for semantic image segmentation. Fourth, we demonstrate further significant improvements of our approach by extending the proposed model using a teacher-student distillation with an exponential moving average and incorporating soft targets from the teacher. We show the generalization capabilities of our method by testing on four different testing datasets (Cityscapes, Dark Zurich, Nighttime Driving, and ACDC) without any fine-tuning. We present an in-depth experimental analysis of the proposed model including results when using another pre-training dataset, per-class and pixel accuracy results, confusion matrices, PCA visualization, k-NN evaluation, ablations of the number of clusters and LiDAR’s density, supervised finetuning as well as additional qualitative results and their analysis.},
  archive      = {J_IJCV},
  author       = {Vobecky, Antonin and Hurych, David and Siméoni, Oriane and Gidaris, Spyros and Bursuc, Andrei and Pérez, Patrick and Sivic, Josef},
  doi          = {10.1007/s11263-024-02320-3},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3519-3541},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unsupervised semantic segmentation of urban scenes via cross-modal distillation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized robot vision-language model via linguistic foreground-aware contrast. <em>IJCV</em>, <em>133</em>(6), 3481-3518. (<a href='https://doi.org/10.1007/s11263-024-02340-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has recently demonstrated great potential for unsupervised pre-training in 3D scene understanding tasks. However, most existing work randomly selects point features as anchors while building contrast, leading to a clear bias toward background points that often dominate in 3D scenes. Also, object awareness and foreground-to-background discrimination are neglected, making contrastive learning less effective. To tackle these issues, we propose a general foreground-aware feature contrast FAC++ framework to learn more effective point cloud representations in pre-training. FAC++ consists of two novel contrast designs to construct more effective and informative contrast pairs. The first is building positive pairs within the same foreground segment where points tend to have the same semantics. The second is that we prevent over-discrimination between 3D segments/objects and encourage grouped foreground-to-background distinctions at the segment level with adaptive feature learning in a Siamese correspondence network, which adaptively learns feature correlations within and across point cloud views effectively. Our proposed approach enhances both the local coherence as well as the overall feature discrimination. Moreover, we have designed the linguistic foreground-aware regional point sampling to enhance more balanced foreground-aware learning, which is termed FAC++. Visualization with point activation maps shows that our contrast pairs capture clear correspondences among foreground regions during pre-training. Quantitative experiments also show that FAC++ achieves superior knowledge transfer and data efficiency in various downstream 3D semantic segmentation, instance segmentation as well as object detection tasks. All codes, data, and models are available at: ( https://github.com/KangchengLiu/FAC_Foreground_Aware_Contrast ).},
  archive      = {J_IJCV},
  author       = {Liu, Kangcheng and Wang, Chaoqun and Han, Xiaodong and Liu, Yong-Jin and Chen, Baoquan},
  doi          = {10.1007/s11263-024-02340-z},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3481-3518},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Generalized robot vision-language model via linguistic foreground-aware contrast},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniCanvas: Affordance-aware unified real image editing via customized text-to-image generation. <em>IJCV</em>, <em>133</em>(6), 3456-3480. (<a href='https://doi.org/10.1007/s11263-024-02334-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for assorted conditional edits on a single real image is becoming increasingly prevalent. We focus on two dominant editing tasks that respectively condition on image and text input, namely subject-driven editing and semantic editing. Previous studies typically tackle these two editing tasks separately, thereby demanding multiple editing processes to achieve versatile edits on a single image. However, fragmented and sequential editing processes not only require more user effort but also further degrade the editing quality. In this paper, we propose UniCanvas, an affordance-aware unified framework that can achieve high-quality parallel subject-driven and semantic editing on a single real image within one inference process. UniCanvas innovatively unifies the multimodal inputs of the editing task into the textual condition space using tailored customization strategies. Building upon the unified representations, we propose a novel inference pipeline that performs parallel editing by selectively blending and manipulating two collaborative text-to-image generative branches. Customization enables the editing process to harness the strong visual understanding and reasoning capability of pre-trained generative models for affordance perception, and a unified inference space further facilitates more effective affordance interaction and alignment for compelling editing. Extensive experiments on diverse real images demonstrate that UniCanvas exhibits powerful scene affordance perception in unified image editing, achieving seamless subject-driven editing and precise semantic editing for various target subjects and query prompts ( https://jinjianrick.github.io/unicanvas/ ).},
  archive      = {J_IJCV},
  author       = {Jin, Jian and Shen, Yang and Zhao, Xinyang and Fu, Zhenyong and Yang, Jian},
  doi          = {10.1007/s11263-024-02334-x},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3456-3480},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {UniCanvas: Affordance-aware unified real image editing via customized text-to-image generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RIGID: Recurrent GAN inversion and editing of real face videos and beyond. <em>IJCV</em>, <em>133</em>(6), 3437-3455. (<a href='https://doi.org/10.1007/s11263-024-02329-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GAN inversion is essential for harnessing the editability of GANs in real images, yet existing methods that invert video frames individually often yield temporally inconsistent results. To address this issue, we present a unified recurrent framework, Recurrent vIdeo GAN Inversion and eDiting (RIGID), designed to enforce temporally coherent GAN inversion and facial editing in real videos explicitly and simultaneously. Our approach models temporal relations between current and previous frames in three ways: (1) by maximizing inversion fidelity and consistency through learning a temporally compensated latent code and spatial features, (2) by disentangling high-frequency incoherent noises from the latent space, and (3) by introducing an in-between frame composition constraint to eliminate inconsistency after attribute manipulation, ensuring that each frame is a direct composite of its neighbors. Compared to existing video- and attribute-specific works, RIGID eliminates the need for expensive re-training of the model, resulting in approximately 60 $$\times $$ faster performance. Furthermore, RIGID can be easily extended to other face domains, showcasing its versatility and adaptability. Extensive experiments demonstrate that RIGID outperforms state-of-the-art methods in inversion and editing tasks both qualitatively and quantitatively.},
  archive      = {J_IJCV},
  author       = {Xu, Yangyang and He, Shengfeng and Wong, Kwan-Yee K. and Luo, Ping},
  doi          = {10.1007/s11263-024-02329-8},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3437-3455},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RIGID: Recurrent GAN inversion and editing of real face videos and beyond},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning meshing from delaunay triangulation for 3D shape representation. <em>IJCV</em>, <em>133</em>(6), 3413-3436. (<a href='https://doi.org/10.1007/s11263-024-02344-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been a growing interest in learning-based explicit methods due to their ability to respect the original input and preserve details. However, the connectivity on complex structures is still difficult to infer due to the limited local shape perception, resulting in artifacts and non-watertight triangles. In this paper, we present a novel learning-based method with Delaunay triangulation to achieve high-precision reconstruction. We model the Delaunay triangulation as a dual graph, extract multi-scale geometric information from the points, and embed it into the structural representation of Delaunay triangulation in an organic way, benefiting fine-grained details reconstruction. To encourage neighborhood information interaction of edges and nodes in the graph, we introduce a Local Graph Iteration algorithm, serving as a variant of graph neural network. Benefiting from its robust local processing for dual graph, a scaling strategy is designed to enable large-scale reconstruction. Moreover, due to the complicated spatial relations between tetrahedrons and the ground truth surface, it is hard to directly generate ground truth labels of tetrahedrons for supervision. Therefore, we propose a multi-label supervision strategy, which is integrated in the loss we design for this task and allows our method to obtain robust labeling without visibility information. Experiments show that our method yields watertight and high-quality meshes. Especially for some thin structures and sharp edges, our method shows better performance than the current state-of-the-art methods. Furthermore, it has a strong adaptability to point clouds of different densities.},
  archive      = {J_IJCV},
  author       = {Zhang, Chen and Tao, Wenbing},
  doi          = {10.1007/s11263-024-02344-9},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3413-3436},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning meshing from delaunay triangulation for 3D shape representation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LDTrack: Dynamic people tracking by service robots using diffusion models. <em>IJCV</em>, <em>133</em>(6), 3392-3412. (<a href='https://doi.org/10.1007/s11263-024-02336-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking of dynamic people in cluttered and crowded human-centered environments is a challenging robotics problem due to the presence of intraclass variations including occlusions, pose deformations, and lighting variations. This paper introduces a novel deep learning architecture, using conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for tracking multiple dynamic people under intraclass variations. By uniquely utilizing conditional latent diffusion models to capture temporal person embeddings, our architecture can adapt to appearance changes of people over time. We incorporated a latent feature encoder network which enables the diffusion process to operate within a high-dimensional latent space to allow for the extraction and spatial–temporal refinement of such rich features as person appearance, motion, location, identity, and contextual information. Extensive experiments demonstrate the effectiveness of LDTrack over other state-of-the-art tracking methods in cluttered and crowded human-centered environments under intraclass variations. Namely, the results show our method outperforms existing deep learning robotic people tracking methods in both tracking accuracy and tracking precision with statistical significance. Additionally, a comprehensive multi-object tracking comparison study was performed against the state-of-the-art methods in urban environments, demonstrating the generalizability of LDTrack. An ablation study was performed to validate the design choices of LDTrack.},
  archive      = {J_IJCV},
  author       = {Fung, Angus and Benhabib, Beno and Nejat, Goldie},
  doi          = {10.1007/s11263-024-02336-9},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3392-3412},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LDTrack: Dynamic people tracking by service robots using diffusion models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-aware multi-view stereo network for efficient edge-preserving depth estimation. <em>IJCV</em>, <em>133</em>(6), 3367-3391. (<a href='https://doi.org/10.1007/s11263-024-02337-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based multi-view stereo methods have achieved great progress in recent years by employing the coarse-to-fine depth estimation framework. However, existing methods still encounter difficulties in recovering depth in featureless areas, object boundaries, and thin structures which mainly due to the poor distinguishability of matching clues in low-textured regions, the inherently smooth properties of 3D convolution neural networks used for cost volume regularization, and information loss of the coarsest scale features. To address these issues, we propose a Context-Aware multi-view stereo Network (CANet) that leverages contextual cues in images to achieve efficient edge-preserving depth estimation. The structural self-similarity information in the reference view is exploited by the introduced self-similarity attended cost aggregation module to perform long-range dependencies modeling in the cost volume, which can boost the matchability of featureless regions. The context information in the reference view is subsequently utilized to progressively refine multi-scale depth estimation through the proposed hierarchical edge-preserving residual learning module, resulting in delicate depth estimation at edges. To enrich features at the coarsest scale by making it focus more on delicate areas, a focal selection module is presented which can enhance the recovery of initial depth with finer details such as thin structure. By integrating the strategies above into the well-designed lightweight cascade framework, CANet achieves superior performance and efficiency trade-offs. Extensive experiments show that the proposed method achieves state-of-the-art performance with fast inference speed and low memory usage. Notably, CANet ranks first on challenging Tanks and Temples advanced dataset and ETH3D high-res benchmark among all published learning-based methods.},
  archive      = {J_IJCV},
  author       = {Su, Wanjuan and Tao, Wenbing},
  doi          = {10.1007/s11263-024-02337-8},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3367-3391},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Context-aware multi-view stereo network for efficient edge-preserving depth estimation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delving deep into simplicity bias for long-tailed image recognition. <em>IJCV</em>, <em>133</em>(6), 3349-3366. (<a href='https://doi.org/10.1007/s11263-024-02342-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simplicity Bias (SB) is a phenomenon that deep neural networks tend to rely favorably on simpler predictive patterns but ignore some complex features when applied to supervised discriminative tasks. In this work, we investigate SB in long-tailed image recognition and find the tail classes suffer more severely from SB, which harms the generalization performance of such underrepresented classes. We empirically report that self-supervised learning (SSL) can mitigate SB and perform in complementary to the supervised counterpart by enriching the features extracted from tail samples and consequently taking better advantage of such rare samples. However, standard SSL methods are designed without explicitly considering the inherent data distribution in terms of classes and may not be optimal for long-tailed distributed data. To address this limitation, we propose a novel SSL method tailored to imbalanced data. It leverages SSL by triple diverse levels, i.e., holistic-, partial-, and augmented-level, to enhance the learning of predictive complex patterns, which provides the potential to overcome the severe SB on tail data. Both quantitative and qualitative experimental results on five long-tailed benchmark datasets show our method can effectively mitigate SB and significantly outperform the competing state-of-the-arts.},
  archive      = {J_IJCV},
  author       = {Wei, Xiu-Shen and Sun, Xuhao and Shen, Yang and Wang, Peng},
  doi          = {10.1007/s11263-024-02342-x},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3349-3366},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Delving deep into simplicity bias for long-tailed image recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). General class-balanced multicentric dynamic prototype pseudo-labeling for source-free domain adaptation. <em>IJCV</em>, <em>133</em>(6), 3327-3348. (<a href='https://doi.org/10.1007/s11263-024-02335-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free Domain Adaptation aims to adapt a pre-trained source model to an unlabeled target domain while circumventing access to well-labeled source data. To compensate for the absence of source data, most existing approaches employ prototype-based pseudo-labeling strategies to facilitate self-training model adaptation. Nevertheless, these methods commonly rely on instance-level predictions for direct monocentric prototype construction, leading to category bias and noisy labels. This is primarily due to the inherent visual domain gaps that often differ across categories. Besides, the monocentric prototype design is ineffective and may introduce negative transfer for those ambiguous data. To tackle these challenges, we propose a general class-Balanced Multicentric Dynamic (BMD) prototype strategy. Specifically, we first introduce a global inter-class balanced sampling strategy for each target category to mitigate category bias. Subsequently, we design an intra-class multicentric clustering strategy to generate robust and representative prototypes. In contrast to existing approaches that only update pseudo-labels at fixed intervals, e.g., one epoch, we employ a dynamic pseudo-labeling strategy that incorporates network update information throughout the model adaptation. We refer to the vanilla implementation of these three sub-strategies as BMD-v1. Furthermore, we promote the BMD-v1 to BMD-v2 by incorporating a consistency-guided reweighting strategy to improve inter-class balanced sampling, and leveraging the silhouettes metric to realize adaptive intra-class multicentric clustering. Extensive experiments conducted on both 2D images and 3D point cloud recognition demonstrate that our proposed BMD strategy significantly improves existing representative methods. Remarkably, BMD-v2 improves NRC from 52.6 to 59.2% in accuracy on the PointDA-10 benchmark. The code will be available at https://github.com/ispc-lab/BMD .},
  archive      = {J_IJCV},
  author       = {Qu, Sanqing and Chen, Guang and Zhang, Jing and Li, Zhijun and He, Wei and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02335-w},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3327-3348},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {General class-balanced multicentric dynamic prototype pseudo-labeling for source-free domain adaptation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relation-guided versatile regularization for federated semi-supervised learning. <em>IJCV</em>, <em>133</em>(6), 3312-3326. (<a href='https://doi.org/10.1007/s11263-024-02330-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated semi-supervised learning (FSSL) target to address the increasing privacy concerns for the practical scenarios, where data holders are limited in labeling capability. Latest FSSL approaches leverage the prediction consistency between the local model and global model to exploit knowledge from partially labeled or completely unlabeled clients. However, they merely utilize data-level augmentation for prediction consistency and simply aggregate model parameters through the weighted average at the server, which leads to biased classifiers and suffers from skewed unlabeled clients. To remedy these issues, we present a novel FSSL framework, Relation-guided Versatile Regularization (FedRVR), consisting of versatile regularization at clients and relation-guided directional aggregation strategy at the server. In versatile regularization, we propose the model-guided regularization together with the data-guided one, and encourage the prediction of the local model invariant to two extreme global models with different abilities, which provides richer consistency supervision for local training. Moreover, we devise a relation-guided directional aggregation at the server, in which a parametric relation predictor is introduced to yield pairwise model relation and obtain a model ranking. In this manner, the server can provide a superior global model by aggregating relative dependable client models, and further produce an inferior global model via reverse aggregation to promote the versatile regularization at clients. Extensive experiments on three FSSL benchmarks verify the superiority of FedRVR over state-of-the-art counterparts across various federated learning settings.},
  archive      = {J_IJCV},
  author       = {Yang, Qiushi and Chen, Zhen and Peng, Zhe and Yuan, Yixuan},
  doi          = {10.1007/s11263-024-02330-1},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3312-3326},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Relation-guided versatile regularization for federated semi-supervised learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PICK: Predict and mask for semi-supervised medical image segmentation. <em>IJCV</em>, <em>133</em>(6), 3296-3311. (<a href='https://doi.org/10.1007/s11263-024-02328-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo-labeling and consistency-based co-training are established paradigms in semi-supervised learning. Pseudo-labeling focuses on selecting reliable pseudo-labels, while co-training emphasizes sub-network diversity for complementary information extraction. However, both paradigms struggle with the inevitable erroneous predictions from unlabeled data, which poses a risk to task-specific decoders and ultimately impact model performance. To address this challenge, we propose a PredICt-and-masK (PICK) model for semi-supervised medical image segmentation. PICK operates by masking and predicting pseudo-label-guided attentive regions to exploit unlabeled data. It features a shared encoder and three task-specific decoders. Specifically, PICK employs a primary decoder supervised solely by labeled data to generate pseudo-labels, identifying potential targets in unlabeled data. The model then masks these regions and reconstructs them using a masked image modeling (MIM) decoder, optimizing through a reconstruction task. To reconcile segmentation and reconstruction, an auxiliary decoder is further developed to learn from the reconstructed images, whose predictions are constrained by the primary decoder. We evaluate PICK on five medical benchmarks, including single organ/tumor segmentation, multi-organ segmentation, and domain-generalized tasks. Our results indicate that PICK outperforms state-of-the-art methods. The code is available at https://github.com/maxwell0027/PICK .},
  archive      = {J_IJCV},
  author       = {Zeng, Qingjie and Lu, Zilin and Xie, Yutong and Xia, Yong},
  doi          = {10.1007/s11263-024-02328-9},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3296-3311},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {PICK: Predict and mask for semi-supervised medical image segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust sequential DeepFake detection. <em>IJCV</em>, <em>133</em>(6), 3278-3295. (<a href='https://doi.org/10.1007/s11263-024-02339-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since photorealistic faces can be readily generated by facial manipulation technologies nowadays, potential malicious abuse of these technologies has drawn great concerns. Numerous deepfake detection methods are thus proposed. However, existing methods only focus on detecting one-step facial manipulation. As the emergence of easy-accessible facial editing applications, people can easily manipulate facial components using multi-step operations in a sequential manner. This new threat requires us to detect a sequence of facial manipulations, which is vital for both detecting deepfake media and recovering original faces afterwards. Motivated by this observation, we emphasize the need and propose a novel research problem called Detecting Sequential DeepFake Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only demanding a binary label prediction, detecting Seq-DeepFake manipulation requires correctly predicting a sequential vector of facial manipulation operations. To support a large-scale investigation, we construct the first Seq-DeepFake dataset, where face images are manipulated sequentially with corresponding annotations of sequential facial manipulation vectors. Based on this new dataset, we cast detecting Seq-DeepFake manipulation as a specific image-to-sequence (e.g., image captioning) task and propose a concise yet effective Seq-DeepFake Transformer (SeqFakeFormer). To better reflect real-world deepfake data distributions, we further apply various perturbations on the original Seq-DeepFake dataset and construct the more challenging Sequential DeepFake dataset with perturbations (Seq-DeepFake-P). To exploit deeper correlation between images and sequences when facing Seq-DeepFake-P, a dedicated Seq-DeepFake Transformer with Image-Sequence Reasoning (SeqFakeFormer++) is devised, which builds stronger correspondence between image-sequence pairs for more robust Seq-DeepFake detection. Moreover, we build a comprehensive benchmark and set up rigorous evaluation protocols and metrics for this new research problem. Extensive quantitative and qualitative experiments demonstrate the effectiveness of SeqFakeFormer and SeqFakeFormer++. Several valuable observations are also revealed to facilitate future research in broader deepfake detection problems. The code has been released at https://github.com/rshaojimmy/SeqDeepFake/ .},
  archive      = {J_IJCV},
  author       = {Shao, Rui and Wu, Tianxing and Liu, Ziwei},
  doi          = {10.1007/s11263-024-02339-6},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3278-3295},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust sequential DeepFake detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HUPE: Heuristic underwater perceptual enhancement with semantic collaborative learning. <em>IJCV</em>, <em>133</em>(6), 3259-3277. (<a href='https://doi.org/10.1007/s11263-024-02318-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images are often affected by light refraction and absorption, reducing visibility and interfering with subsequent applications. Existing underwater image enhancement methods primarily focus on improving visual quality while overlooking practical implications. To strike a balance between visual quality and application, we propose a heuristic invertible network for underwater perception enhancement, dubbed HUPE, which enhances visual quality and demonstrates flexibility in handling other downstream tasks. Specifically, we introduced a information-preserving reversible transformation with embedded Fourier transform to establish a bidirectional mapping between underwater images and their clear images. Additionally, a heuristic prior is incorporated into the enhancement process to better capture scene information. To further bridges the feature gap between vision-based enhancement images and application-oriented images, a semantic collaborative learning module is applied in the joint optimization process of the visual enhancement task and the downstream task, which guides the proposed enhancement model to extract more task-oriented semantic features while obtaining visually pleasing images. Extensive experiments, both quantitative and qualitative, demonstrate the superiority of our HUPE over state-of-the-art methods. The source code is available at https://github.com/ZengxiZhang/HUPE .},
  archive      = {J_IJCV},
  author       = {Zhang, Zengxi and Jiang, Zhiying and Ma, Long and Liu, Jinyuan and Fan, Xin and Liu, Risheng},
  doi          = {10.1007/s11263-024-02318-x},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3259-3277},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {HUPE: Heuristic underwater perceptual enhancement with semantic collaborative learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind image quality assessment: Exploring content fidelity perceptibility via quality adversarial learning. <em>IJCV</em>, <em>133</em>(6), 3242-3258. (<a href='https://doi.org/10.1007/s11263-024-02338-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep learning-based no-reference image quality assessment (NR-IQA) methods, the absence of reference images limits their ability to assess content fidelity, making it difficult to distinguish between original content and distortions that degrade quality. To address this issue, we propose a quality adversarial learning framework emphasizing both content fidelity and prediction accuracy. The main contributions of this study are as follows: First, we investigate the importance of content fidelity, especially in no-reference scenarios. Second, we propose a quality adversarial learning framework that dynamically adapts and refines the image quality assessment process on the basis of the quality optimization results. The framework generates adversarial samples for the quality prediction model, and simultaneously, the quality prediction model optimizes the quality prediction model by using these adversarial samples to maintain fidelity and improve accuracy. Finally, we demonstrate that by employing the quality prediction model as a loss function for image quality optimization, our framework effectively reduces the generation of artifacts, highlighting its superior ability to preserve content fidelity. The experimental results demonstrate the validity of our method compared with state-of-the-art NR-IQA methods. The code is publicly available at the following website: https://github.com/Land5cape/QAL-IQA.},
  archive      = {J_IJCV},
  author       = {Zhou, Mingliang and Shen, Wenhao and Wei, Xuekai and Luo, Jun and Jia, Fan and Zhuang, Xu and Jia, Weijia},
  doi          = {10.1007/s11263-024-02338-7},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3242-3258},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Blind image quality assessment: Exploring content fidelity perceptibility via quality adversarial learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RepSNet: A nucleus instance segmentation model based on boundary regression and structural re-parameterization. <em>IJCV</em>, <em>133</em>(6), 3222-3241. (<a href='https://doi.org/10.1007/s11263-024-02332-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus instance segmentation is a key step in digital pathology analysis and pathological diagnosis. However, the computational efficiency of the model and the treatment of overlapping targets are the major challenges in the studies of this problem. To this end, a neural network model RepSNet was designed based on a nucleus boundary regression and a structural re-parameterization scheme for segmenting and classifying the nuclei in H&E-stained histopathological images. First, RepSNet estimates the boundary position information (BPI) of the parent nucleus for each pixel. The BPI estimation incorporates the local information of the pixel and the contextual information of the parent nucleus. Then, the nucleus boundary is estimated by aggregating the BPIs from a series of pixels using a proposed boundary voting mechanism (BVM), and the instance segmentation results are computed from the estimated nucleus boundary using a connected component analysis procedure. The BVM intrinsically achieves a kind of synergistic belief enhancement among the BPIs from various pixels. Therefore, different from the methods available in literature that obtain nucleus boundaries based on a direct pixel recognition scheme, RepSNet computes its boundary decisions based on some guidances from macroscopic information using an integration mechanism. In addition, RepSNet employs a re-parametrizable encoder-decoder structure. This model can not only aggregate features from some receptive fields with various scales which helps segmentation accuracy improvement, but also reduce the parameter amount and computational burdens in the model inference phase through the structural re-parameterization technique. In the experimental comparisons and evaluations on the Lizard dataset, RepSNet demonstrated superior segmentation accuracy and inference speed compared to several typical benchmark models. The experimental code, dataset splitting configuration and the pre-trained model were released at https://github.com/luckyrz0/RepSNet .},
  archive      = {J_IJCV},
  author       = {Xiong, Shengchun and Li, Xiangru and Zhong, Yunpeng and Peng, Wanfen},
  doi          = {10.1007/s11263-024-02332-z},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3222-3241},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {RepSNet: A nucleus instance segmentation model based on boundary regression and structural re-parameterization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudo-plane regularized signed distance field for neural indoor scene reconstruction. <em>IJCV</em>, <em>133</em>(6), 3203-3221. (<a href='https://doi.org/10.1007/s11263-024-02319-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given only a set of images, neural implicit surface representation has shown its capability in 3D surface reconstruction. However, as the nature of per-scene optimization is based on the volumetric rendering of color, previous neural implicit surface reconstruction methods usually fail in the low-textured regions, including floors, walls, etc., which commonly exist for indoor scenes. Being aware of the fact that these low-textured regions usually correspond to planes, without introducing additional ground-truth supervisory signals or making additional assumptions about the room layout, we propose to leverage a novel Pseudo-plane regularized Signed Distance Field (PPlaneSDF) for indoor scene reconstruction. Specifically, we consider adjacent pixels with similar colors to be on the same pseudo-planes. The plane parameters are then estimated on the fly during training by an efficient and effective two-step scheme. Then the signed distances of the points on the planes are regularized by the estimated plane parameters in the training phase. As the unsupervised plane segments are usually noisy and inaccurate, we propose to assign different weights to the sampled points on the plane in plane estimation as well as the regularization loss. The weights come by fusing the plane segments from different views. As the sampled rays in the planar regions are redundant, leading to inefficient training, we further propose a keypoint-guided rays sampling strategy that attends to the informative textured regions with large color variations, and the implicit network gets a better reconstruction, compared with the original uniform ray sampling strategy. Experiments show that our PPlaneSDF achieves competitive reconstruction performance in Manhattan scenes. Further, as we do not introduce any additional room layout assumption, our PPlaneSDF generalizes well to the reconstruction of non-Manhattan scenes.},
  archive      = {J_IJCV},
  author       = {Li, Jing and Yu, Jinpeng and Wang, Ruoyu and Gao, Shenghua},
  doi          = {10.1007/s11263-024-02319-w},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3203-3221},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pseudo-plane regularized signed distance field for neural indoor scene reconstruction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSFRNet: Integrating clothing status awareness for long-term person re-identification. <em>IJCV</em>, <em>133</em>(6), 3180-3202. (<a href='https://doi.org/10.1007/s11263-024-02315-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the dynamic nature of long-term person re-identification (LT-reID) amid varying clothing conditions necessitates a departure from conventional methods. Traditional LT-reID strategies, mainly biometrics-based and data adaptation-based, each have their pitfalls. The former falters in environments lacking high-quality biometric data, while the latter loses efficacy with minimal or subtle clothing changes. To overcome these obstacles, we propose the clothing status-aware feature regularization network (CSFRNet). This novel approach seamlessly incorporates clothing status awareness into the feature learning process, significantly enhancing the adaptability and accuracy of LT-reID systems where clothing can either change completely, partially, or not at all over time, without the need for explicit clothing labels. The versatility of our CSFRNet is showcased on diverse LT-reID benchmarks, including Celeb-reID, Celeb-reID-light, PRCC, DeepChange, and LTCC, marking a significant advancement in the field by addressing the real-world variability of clothing in LT-reID scenarios.},
  archive      = {J_IJCV},
  author       = {Huang, Yan and Zhang, Zhang and Wu, Qiang and Zhong, Yi and Wang, Liang},
  doi          = {10.1007/s11263-024-02315-0},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3180-3202},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CSFRNet: Integrating clothing status awareness for long-term person re-identification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combating label noise with a general surrogate model for sample selection. <em>IJCV</em>, <em>133</em>(6), 3166-3179. (<a href='https://doi.org/10.1007/s11263-024-02324-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep learning systems are data-hungry. Learning with web data is one of the feasible solutions, but will introduce label noise inevitably, which can hinder the performance of deep neural networks. Sample selection is an effective way to deal with label noise. The key is to separate clean samples based on some criterion. Previous methods pay more attention to the small loss criterion where small-loss samples are regarded as clean ones. Nevertheless, such a strategy relies on the learning dynamics of each data instance. Some noisy samples are still memorized due to frequently occurring corrupted learning patterns. To tackle this problem, a training-free surrogate model is preferred, freeing from the effect of memorization. In this work, we propose to leverage the vision-language surrogate model CLIP to filter noisy samples automatically. CLIP brings external knowledge to facilitate the selection of clean samples with its ability of text-image alignment. Furthermore, a margin adaptive loss is designed to regularize the selection bias introduced by CLIP, providing robustness to label noise. We validate the effectiveness of our proposed method on both real-world and synthetic noisy datasets. Our method achieves significant improvement without CLIP involved during the inference stage.},
  archive      = {J_IJCV},
  author       = {Liang, Chao and Zhu, Linchao and Shi, Humphrey and Yang, Yi},
  doi          = {10.1007/s11263-024-02324-z},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3166-3179},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Combating label noise with a general surrogate model for sample selection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AniClipart: Clipart animation with text-to-video priors. <em>IJCV</em>, <em>133</em>(6), 3149-3165. (<a href='https://doi.org/10.1007/s11263-024-02306-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clipart, a pre-made graphic art form, offers a convenient and efficient way of illustrating visual content. Traditional workflows to convert static clipart images into motion sequences are laborious and time-consuming, involving numerous intricate steps like rigging, key animation and in-betweening. Recent advancements in text-to-video generation hold great potential in resolving this problem. Nevertheless, direct application of text-to-video generation models often struggles to retain the visual identity of clipart images or generate cartoon-style motions, resulting in unsatisfactory animation outcomes. In this paper, we introduce AniClipart, a system that transforms static clipart images into high-quality motion sequences guided by text-to-video priors. To generate cartoon-style and smooth motion, we first define Bézier curves over keypoints of the clipart image as a form of motion regularization. We then align the motion trajectories of the keypoints with the provided text prompt by optimizing the Video Score Distillation Sampling (VSDS) loss, which encodes adequate knowledge of natural motion within a pretrained text-to-video diffusion model. With a differentiable As-Rigid-As-Possible shape deformation algorithm, our method can be end-to-end optimized while maintaining deformation rigidity. Experimental results show that the proposed AniClipart consistently outperforms existing image-to-video generation models, in terms of text-video alignment, visual identity preservation, and motion consistency. Furthermore, we showcase the versatility of AniClipart by adapting it to generate a broader array of animation formats, such as layered animation, which allows topological changes.},
  archive      = {J_IJCV},
  author       = {Wu, Ronghuan and Su, Wanchao and Ma, Kede and Liao, Jing},
  doi          = {10.1007/s11263-024-02306-1},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3149-3165},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AniClipart: Clipart animation with text-to-video priors},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring homogeneous and heterogeneous consistent label associations for unsupervised visible-infrared person ReID. <em>IJCV</em>, <em>133</em>(6), 3129-3148. (<a href='https://doi.org/10.1007/s11263-024-02322-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised visible-infrared person re-identification (USL-VI-ReID) endeavors to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency between the feature space and the pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to quantify the inconsistency between the pseudo-label space and the feature space, subsequently minimizing it. The proposed MULT ensures that the generated pseudo-labels maintain alignment across modalities while upholding structural consistency within intra-modality. Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the side effects of noisy pseudo-labels while simultaneously aligning different modalities, coupled with an Alternative Modality-Invariant Representation Learning (AMIRL) framework. Experiments demonstrate that our proposed method outperforms existing state-of-the-art USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods. Code is available at https://github.com/FranklinLingfeng/code_for_MULT .},
  archive      = {J_IJCV},
  author       = {He, Lingfeng and Cheng, De and Wang, Nannan and Gao, Xinbo},
  doi          = {10.1007/s11263-024-02322-1},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3129-3148},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Exploring homogeneous and heterogeneous consistent label associations for unsupervised visible-infrared person ReID},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLIDE: A unified mesh and texture generation framework with enhanced geometric control and multi-view consistency. <em>IJCV</em>, <em>133</em>(6), 3105-3128. (<a href='https://doi.org/10.1007/s11263-024-02326-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generation of textured mesh is crucial for computer graphics and virtual content creation. However, current generative models often struggle with challenges such as irregular mesh structures and inconsistencies in multi-view textures. In this study, we present a unified framework for both geometry generation and texture generation, utilizing a novel sparse latent point diffusion model that specifically addresses the geometric aspects of models. Our approach employs point clouds as an efficient intermediate representation, encoding them into sparse latent points with semantically meaningful features for precise geometric control. While the sparse latent points facilitate a high-level control over the geometry, shaping the overall structure and fine details of the meshes, this control does not extend to textures. To address this, we propose a separate texture generation process that integrates multi-view priors post-geometry generation, effectively resolving the issue of multi-view texture inconsistency. This process ensures the production of coherent and high-quality textures that complement the precisely generated meshes, thereby creating visually appealing and detailed models. Our framework distinctively separates the control mechanisms for geometry and texture, leading to significant improvements in the generation of complex, textured 3D content. Evaluations on the ShapeNet dataset for geometry and the Objaverse dataset for textures demonstrate that our model surpasses existing methods in terms of geometric quality, control, and the generation of coherent, high-quality textures.},
  archive      = {J_IJCV},
  author       = {Wang, Jinyi and Lyu, Zhaoyang and Fei, Ben and Yao, Jiangchao and Zhang, Ya and Dai, Bo and Lin, Dahua and He, Ying and Wang, Yanfeng},
  doi          = {10.1007/s11263-024-02326-x},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3105-3128},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SLIDE: A unified mesh and texture generation framework with enhanced geometric control and multi-view consistency},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoStory: Generating diverse storytelling images with minimal human efforts. <em>IJCV</em>, <em>133</em>(6), 3083-3104. (<a href='https://doi.org/10.1007/s11263-024-02309-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Story visualization aims to generate a series of images that match the story described in texts, and it requires the generated images to satisfy high quality, alignment with the text description, and consistency in character identities. Given the complexity of story visualization, existing methods drastically simplify the problem by considering only a few specific characters and scenarios, or requiring the users to provide per-image control conditions such as sketches. However, these simplifications render these methods incompetent for real applications. To this end, we propose an automated story visualization system that can effectively generate diverse, high-quality, and consistent sets of story images, with minimal human interactions. Specifically, we utilize the comprehension and planning capabilities of large language models for layout planning, and then leverage large-scale text-to-image models to generate sophisticated story images based on the layout. We empirically find that sparse control conditions, such as bounding boxes, are suitable for layout planning, while dense control conditions, e.g., sketches, and keypoints, are suitable for generating high-quality image content. To obtain the best of both worlds, we devise a dense condition generation module to transform simple bounding box layouts into sketch or keypoint control conditions for final image generation, which not only improves the image quality but also allows easy and intuitive user interactions. In addition, we propose a simple yet effective method to generate multi-view consistent character images, eliminating the reliance on human labor to collect or draw character images. This allows our method to obtain consistent story visualization even when only texts are provided as input. Both qualitative and quantitative experiments demonstrate the superiority of our method.},
  archive      = {J_IJCV},
  author       = {Wang, Wen and Zhao, Canyu and Chen, Hao and Chen, Zhekai and Zheng, Kecheng and Shen, Chunhua},
  doi          = {10.1007/s11263-024-02309-y},
  journal      = {International Journal of Computer Vision},
  month        = {6},
  number       = {6},
  pages        = {3083-3104},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AutoStory: Generating diverse storytelling images with minimal human efforts},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Special issue on multimodal learning. <em>IJCV</em>, <em>133</em>(5), 3079-3081. (<a href='https://doi.org/10.1007/s11263-024-02312-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Yang, Michael Ying and Rota, Paolo and Mancini, Massimiliano and Morerio, Pietro and Rosenhahn, Bodo and Murino, Vittorio},
  doi          = {10.1007/s11263-024-02312-3},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {3079-3081},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on multimodal learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LaVie: High-quality video generation with cascaded latent diffusion models. <em>IJCV</em>, <em>133</em>(5), 3059-3078. (<a href='https://doi.org/10.1007/s11263-024-02295-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously (a) accomplish the synthesis of visually realistic and temporally coherent videos while (b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: (1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. (2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications. Project page: https://github.com/Vchitect/LaVie/ .},
  archive      = {J_IJCV},
  author       = {Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and Guo, Yuwei and Wu, Tianxing and Si, Chenyang and Jiang, Yuming and Chen, Cunjian and Loy, Chen Change and Dai, Bo and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
  doi          = {10.1007/s11263-024-02295-1},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {3059-3078},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LaVie: High-quality video generation with cascaded latent diffusion models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FusionBooster: A unified image fusion boosting paradigm. <em>IJCV</em>, <em>133</em>(5), 3041-3058. (<a href='https://doi.org/10.1007/s11263-024-02266-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, numerous ideas have emerged for designing a mutually reinforcing mechanism or extra stages for the image fusion task, ignoring the inevitable gaps between different vision tasks and the computational burden. We argue that there is a scope to improve the fusion performance with the help of the FusionBooster, a model specifically designed for fusion tasks. In particular, our booster is based on the divide-and-conquer strategy controlled by an information probe. The booster is composed of three building blocks: the probe units, the booster layer, and the assembling module. Given the result produced by a backbone method, the probe units assess the fused image and divide the results according to their information content. This is instrumental in identifying missing information, as a step to its recovery. The recovery of the degraded components along with the fusion guidance are the role of the booster layer. Lastly, the assembling module is responsible for piecing these advanced components together to deliver the output. We use concise reconstruction loss functions in conjunction with lightweight autoencoder models to formulate the learning task, with marginal computational complexity increase. The experimental results obtained in various fusion missions, as well as downstream detection tasks, consistently demonstrate that the proposed FusionBooster significantly improves the performance. Our code will be publicly available at https://github.com/AWCXV/FusionBooster .},
  archive      = {J_IJCV},
  author       = {Cheng, Chunyang and Xu, Tianyang and Wu, Xiao-Jun and Li, Hui and Li, Xi and Kittler, Josef},
  doi          = {10.1007/s11263-024-02266-6},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {3041-3058},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FusionBooster: A unified image fusion boosting paradigm},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise-resistant multimodal transformer for emotion recognition. <em>IJCV</em>, <em>133</em>(5), 3020-3040. (<a href='https://doi.org/10.1007/s11263-024-02304-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition identifies human emotions from various data modalities like video, text, and audio. However, we found that this task can be easily affected by noisy information that does not contain useful semantics and may occur at different locations of a multimodal input sequence. To this end, we present a novel paradigm that attempts to extract noise-resistant features in its pipeline and introduces a noise-aware learning scheme to effectively improve the robustness of multimodal emotion understanding against noisy information. Our new pipeline, namely Noise-Resistant Multimodal Transformer (NORM-TR), mainly introduces a Noise-Resistant Generic Feature (NRGF) extractor and a multimodal fusion Transformer for the multimodal emotion recognition task. In particular, we make the NRGF extractor learn to provide a generic and disturbance-insensitive representation so that consistent and meaningful semantics can be obtained. Furthermore, we apply a multimodal fusion Transformer to incorporate Multimodal Features (MFs) of multimodal inputs (serving as the key and value) based on their relations to the NRGF (serving as the query). Therefore, the possible insensitive but useful information of NRGF could be complemented by MFs that contain more details, achieving more accurate emotion understanding while maintaining robustness against noises. To train the NORM-TR properly, our proposed noise-aware learning scheme complements normal emotion recognition losses by enhancing the learning against noises. Our learning scheme explicitly adds noises to either all the modalities or a specific modality at random locations of a multimodal input sequence. We correspondingly introduce two adversarial losses to encourage the NRGF extractor to learn to extract the NRGFs invariant to the added noises, thus facilitating the NORM-TR to achieve more favorable multimodal emotion recognition performance. In practice, extensive experiments can demonstrate the effectiveness of the NORM-TR and the noise-aware learning scheme for dealing with both explicitly added noisy information and the normal multimodal sequence with implicit noises. On several popular multimodal datasets (e.g., MOSI, MOSEI, IEMOCAP, and RML), our NORM-TR achieves state-of-the-art performance and outperforms existing methods by a large margin, which demonstrates that the ability to resist noisy information in multimodal input is important for effective emotion recognition.},
  archive      = {J_IJCV},
  author       = {Liu, Yuanyuan and Zhang, Haoyu and Zhan, Yibing and Chen, Zijing and Yin, Guanghao and Wei, Lin and Chen, Zhe},
  doi          = {10.1007/s11263-024-02304-3},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {3020-3040},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Noise-resistant multimodal transformer for emotion recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep attention learning for pre-operative lymph node metastasis prediction in pancreatic cancer via multi-object relationship modeling. <em>IJCV</em>, <em>133</em>(5), 2996-3019. (<a href='https://doi.org/10.1007/s11263-024-02314-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lymph node (LN) metastasis status is one of the most critical prognostic and cancer staging clinical factors for patients with resectable pancreatic ductal adenocarcinoma (PDAC, generally for any types of solid malignant tumors). Pre-operative prediction of LN metastasis from non-invasive CT imaging is highly desired, as it might be directly and conveniently used to guide the follow-up neoadjuvant treatment decision and surgical planning. Most previous studies only use the tumor characteristics in CT imaging alone to implicitly infer LN metastasis. To the best of our knowledge, this is the first work to propose a fully-automated LN segmentation and identification network to directly facilitate the LN metastasis status prediction task for patients with PDAC. Specially, (1) we explore the anatomical spatial context priors of pancreatic LN locations by generating a guiding attention map from related organs and vessels to assist segmentation and infer LN status. As such, LN segmentation is impelled to focus on regions that are anatomically adjacent or plausible with respect to the specific organs and vessels. (2) The metastasized LN identification network is trained to classify the segmented LN instances into positives or negatives by reusing the segmentation network as a pre-trained backbone and padding a new classification head. (3) Importantly, we develop a LN metastasis status prediction network that combines and aggregates the holistic patient-wise diagnosis information of both LN segmentation/identification and deep imaging characteristics by the PDAC tumor region. Extensive quantitative nested five-fold cross-validation is conducted on a discovery dataset of 749 patients with PDAC. External multi-center clinical evaluation is further performed on two other hospitals of 191 total patients. Our multi-staged LN metastasis status prediction network statistically significantly outperforms strong baselines of nnUNet and several other compared methods, including CT-reported LN status, radiomics, and deep learning models.},
  archive      = {J_IJCV},
  author       = {Zheng, Zhilin and Fang, Xu and Yao, Jiawen and Zhu, Mengmeng and Lu, Le and Shi, Yu and Lu, Hong and Lu, Jianping and Zhang, Ling and Shao, Chengwei and Bian, Yun},
  doi          = {10.1007/s11263-024-02314-1},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2996-3019},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep attention learning for pre-operative lymph node metastasis prediction in pancreatic cancer via multi-object relationship modeling},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polynomial implicit neural framework for promoting shape awareness in generative models. <em>IJCV</em>, <em>133</em>(5), 2967-2995. (<a href='https://doi.org/10.1007/s11263-024-02270-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polynomial functions have been employed to represent shape-related information in 2D and 3D computer vision, even from the very early days of the field. In this paper, we present a framework using polynomial-type basis functions to promote shape awareness in contemporary generative architectures. The benefits of using a learnable form of polynomial basis functions as drop-in modules into generative architectures are several—including promoting shape awareness, a noticeable disentanglement of shape from texture, and high quality generation. To enable the architectures to have a small number of parameters, we further use implicit neural representations (INR) as the base architecture. Most INR architectures rely on sinusoidal positional encoding, which accounts for high-frequency information in data. However, the finite encoding size restricts the model’s representational power. Higher representational power is critically needed to transition from representing a single given image to effectively representing large and diverse datasets. Our approach addresses this gap by representing an image with a polynomial function and eliminates the need for positional encodings. Therefore, to achieve a progressively higher degree of polynomial representation, we use element-wise multiplications between features and affine-transformed coordinate locations after every ReLU layer. The proposed method is evaluated qualitatively and quantitatively on large datasets such as ImageNet. The proposed Poly-INR model performs comparably to state-of-the-art generative models without any convolution, normalization, or self-attention layers, and with significantly fewer trainable parameters. With substantially fewer training parameters and higher representative power, our approach paves the way for broader adoption of INR models for generative modeling tasks in complex domains. The code is publicly available at https://github.com/Rajhans0/Poly_INR .},
  archive      = {J_IJCV},
  author       = {Nath, Utkarsh and Singh, Rajhans and Shukla, Ankita and Kulkarni, Kuldeep and Turaga, Pavan},
  doi          = {10.1007/s11263-024-02270-w},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2967-2995},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Polynomial implicit neural framework for promoting shape awareness in generative models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning discriminative features for visual tracking via scenario decoupling. <em>IJCV</em>, <em>133</em>(5), 2950-2966. (<a href='https://doi.org/10.1007/s11263-024-02307-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking aims to estimate object state automatically in a video sequence, which is challenging especially in complex scenarios. Recent Transformer-based trackers enable the interaction between the target template and search region in the feature extraction phase for target-aware feature learning, which have achieved superior performance. However, visual tracking is essentially a task to discriminate the specified target from the backgrounds. These trackers commonly ignore the role of background in feature learning, which may cause backgrounds to be mistakenly enhanced in complex scenarios, affecting temporal robustness and spatial discriminability. To address the above limitations, we propose a scenario-aware tracker (SATrack) based on a specifically designed scenario-aware Vision Transformer, which integrates a scenario knowledge extractor and a scenario knowledge modulator. The proposed SATrack enjoys several merits. Firstly, we design a novel scenario-aware Vision Transformer for visual tracking, which can decouple historic scenarios into explicit target and background knowledge to guide discriminative feature learning. Secondly, a scenario knowledge extractor is designed to dynamically acquire decoupled and compact scenario knowledge from video contexts, and a scenario knowledge modulator is designed to embed scenario knowledge into attention mechanisms for scenario-aware feature learning. Extensive experimental results on nine tracking benchmarks demonstrate that SATrack achieves new state-of-the-art performance with high FPS.},
  archive      = {J_IJCV},
  author       = {Ma, Yinchao and Yu, Qianjin and Yang, Wenfei and Zhang, Tianzhu and Zhang, Jinpeng},
  doi          = {10.1007/s11263-024-02307-0},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2950-2966},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning discriminative features for visual tracking via scenario decoupling},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard-normal example-aware template mutual matching for industrial anomaly detection. <em>IJCV</em>, <em>133</em>(5), 2927-2949. (<a href='https://doi.org/10.1007/s11263-024-02323-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detectors are widely used in industrial manufacturing to detect and localize unknown defects in query images. These detectors are trained on anomaly-free samples and have successfully distinguished anomalies from most normal samples. However, hard-normal examples are scattered and far apart from most normal samples, and thus they are often mistaken for anomalies by existing methods. To address this issue, we propose Hard-normal Example-aware Template Mutual Matching (HETMM), an efficient framework to build a robust prototype-based decision boundary. Specifically, HETMM employs the proposed Affine-invariant Template Mutual Matching (ATMM) to mitigate the affection brought by the affine transformations and easy-normal examples. By mutually matching the pixel-level prototypes within the patch-level search spaces between query and template set, ATMM can accurately distinguish between hard-normal examples and anomalies, achieving low false-positive and missed-detection rates. In addition, we also propose PTS to compress the original template set for speed-up. PTS selects cluster centres and hard-normal examples to preserve the original decision boundary, allowing this tiny set to achieve comparable performance to the original one. Extensive experiments demonstrate that HETMM outperforms state-of-the-art methods, while using a 60-sheet tiny set can achieve competitive performance and real-time inference speed (around 26.1 FPS) on a Quadro 8000 RTX GPU. HETMM is training-free and can be hot-updated by directly inserting novel samples into the template set, which can promptly address some incremental learning issues in industrial manufacturing.},
  archive      = {J_IJCV},
  author       = {Chen, Zixuan and Xie, Xiaohua and Yang, Lingxiao and Lai, Jian-Huang},
  doi          = {10.1007/s11263-024-02323-0},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2927-2949},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hard-normal example-aware template mutual matching for industrial anomaly detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond talking – Generating holistic 3D human dyadic motion for communication. <em>IJCV</em>, <em>133</em>(5), 2910-2926. (<a href='https://doi.org/10.1007/s11263-024-02300-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions for both speakers and listeners. Central to our approach is the incorporation of factorization to decouple audio features and the combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements. We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener. We consider the real-time mutual influence between the speaker and the listener and propose a novel chain-like transformer-based auto-regressive model specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both the speaker and the listener simultaneously. These designs ensure that the results we generate are both coordinated and diverse. Our approach demonstrates state-of-the-art performance on two benchmark datasets. Furthermore, we introduce the HoCo holistic communication dataset, which is a valuable resource for future research. Our HoCo dataset and code will be released for research purposes upon acceptance.},
  archive      = {J_IJCV},
  author       = {Sun, Mingze and Xu, Chao and Jiang, Xinyu and Liu, Yang and Sun, Baigui and Huang, Ruqi},
  doi          = {10.1007/s11263-024-02300-7},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2910-2926},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Beyond talking – Generating holistic 3D human dyadic motion for communication},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyper-3DG: Text-to-3D gaussian generation via hypergraph. <em>IJCV</em>, <em>133</em>(5), 2886-2909. (<a href='https://doi.org/10.1007/s11263-024-02298-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named “3D Gaussian Generation via Hypergraph (Hyper-3DG)”, designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named “Geometry and Texture Hypergraph Refiner (HGRefiner)”. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework. (Project code: https://github.com/yjhboy/Hyper3DG ).},
  archive      = {J_IJCV},
  author       = {Di, Donglin and Yang, Jiahui and Luo, Chaofan and Xue, Zhou and Chen, Wei and Yang, Xun and Gao, Yue},
  doi          = {10.1007/s11263-024-02298-y},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2886-2909},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hyper-3DG: Text-to-3D gaussian generation via hypergraph},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relation-guided adversarial learning for data-free knowledge transfer. <em>IJCV</em>, <em>133</em>(5), 2868-2885. (<a href='https://doi.org/10.1007/s11263-024-02303-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-free knowledge distillation transfers knowledge by recovering training data from a pre-trained model. Despite the recent success of seeking global data diversity, the diversity within each class and the similarity among different classes are largely overlooked, resulting in data homogeneity and limited performance. In this paper, we introduce a novel Relation-Guided Adversarial Learning method with triplet losses, which solves the homogeneity problem from two aspects. To be specific, our method aims to promote both intra-class diversity and inter-class confusion of the generated samples. To this end, we design two phases, an image synthesis phase and a student training phase. In the image synthesis phase, we construct an optimization process to push away samples with the same labels and pull close samples with different labels, leading to intra-class diversity and inter-class confusion, respectively. Then, in the student training phase, we perform an opposite optimization, which adversarially attempts to reduce the distance of samples of the same classes and enlarge the distance of samples of different classes. To mitigate the conflict of seeking high global diversity and keeping inter-class confusing, we propose a focal weighted sampling strategy by selecting the negative in the triplets unevenly within a finite range of distance. RGAL shows significant improvement over previous state-of-the-art methods in accuracy and data efficiency. Besides, RGAL can be inserted into state-of-the-art methods on various data-free knowledge transfer applications. Experiments on various benchmarks demonstrate the effectiveness and generalizability of our proposed method on various tasks, specially data-free knowledge distillation, data-free quantization, and non-exemplar incremental learning. Our code will be publicly available to the community.},
  archive      = {J_IJCV},
  author       = {Liang, Yingping and Fu, Ying},
  doi          = {10.1007/s11263-024-02303-4},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2868-2885},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Relation-guided adversarial learning for data-free knowledge transfer},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structured generative models for scene understanding. <em>IJCV</em>, <em>133</em>(5), 2845-2867. (<a href='https://doi.org/10.1007/s11263-024-02316-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This position paper argues for the use of structured generative models (SGMs) for the understanding of static scenes. This requires the reconstruction of a 3D scene from an input image (or a set of multi-view images), whereby the contents of the image(s) are causally explained in terms of models of instantiated objects, each with their own type, shape, appearance and pose, along with global variables like scene lighting and camera parameters. This approach also requires scene models which account for the co-occurrences and inter-relationships of objects in a scene. The SGM approach has the merits that it is compositional and generative, which lead to interpretability and editability. To pursue the SGM agenda, we need models for objects and scenes, and approaches to carry out inference. We first review models for objects, which include “things” (object categories that have a well defined shape), and “stuff” (categories which have amorphous spatial extent). We then move on to review scene models which describe the inter-relationships of objects. Perhaps the most challenging problem for SGMs is inference of the objects, lighting and camera parameters, and scene inter-relationships from input consisting of a single or multiple images. We conclude with a discussion of issues that need addressing to advance the SGM agenda.},
  archive      = {J_IJCV},
  author       = {Williams, Christopher K. I.},
  doi          = {10.1007/s11263-024-02316-z},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2845-2867},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Structured generative models for scene understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MoDA: Modeling deformable 3D objects from casual videos. <em>IJCV</em>, <em>133</em>(5), 2825-2844. (<a href='https://doi.org/10.1007/s11263-024-02310-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the challenges of modeling deformable 3D objects from casual videos. With the popularity of NeRF, many works extend it to dynamic scenes with a canonical NeRF and a deformation model that achieves 3D point transformation between the observation space and the canonical space. Recent works rely on linear blend skinning (LBS) to achieve the canonical-observation transformation. However, the linearly weighted combination of rigid transformation matrices is not guaranteed to be rigid. As a matter of fact, unexpected scale and shear factors often appear. In practice, using LBS as the deformation model can always lead to skin-collapsing artifacts for bending or twisting motions. To solve this problem, we propose neural dual quaternion blend skinning (NeuDBS) to achieve 3D point deformation, which can perform rigid transformation without skin-collapsing artifacts. To register 2D pixels across different frames, we establish a correspondence between canonical feature embeddings that encodes 3D points within the canonical space, and 2D image features by solving an optimal transport problem. Besides, we introduce a texture filtering approach for texture rendering that effectively minimizes the impact of noisy colors outside target deformable objects.},
  archive      = {J_IJCV},
  author       = {Song, Chaoyue and Wei, Jiacheng and Chen, Tianyi and Chen, Yiwen and Foo, Chuan-Sheng and Liu, Fayao and Lin, Guosheng},
  doi          = {10.1007/s11263-024-02310-5},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2825-2844},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MoDA: Modeling deformable 3D objects from casual videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MaskDiffusion: Boosting text-to-image consistency with conditional mask. <em>IJCV</em>, <em>133</em>(5), 2805-2824. (<a href='https://doi.org/10.1007/s11263-024-02294-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in diffusion models have showcased their impressive capacity to generate visually striking images. However, ensuring a close match between the generated image and the given prompt remains a persistent challenge. In this work, we identify that a crucial factor leading to the erroneous generation of objects and their attributes is the inadequate cross-modality relation learning between the prompt and the generated images. To better align the prompt and image content, we advance the cross-attention with an adaptive mask, which is conditioned on the attention maps and the prompt embeddings, to dynamically adjust the contribution of each text token to the image features. This mechanism explicitly diminishes the ambiguity in the semantic information embedding of the text encoder, leading to a boost of text-to-image consistency in the synthesized images. Our method, termed MaskDiffusion, is training-free and hot-pluggable for popular pre-trained diffusion models. When applied to the latent diffusion models, our MaskDiffusion can largely enhance their capability to correctly generate objects and their attributes, with negligible computation overhead compared to the original diffusion models. Our project page is https://github.com/HVision-NKU/MaskDiffusion .},
  archive      = {J_IJCV},
  author       = {Zhou, Yupeng and Zhou, Daquan and Wang, Yaxing and Feng, Jiashi and Hou, Qibin},
  doi          = {10.1007/s11263-024-02294-2},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2805-2824},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MaskDiffusion: Boosting text-to-image consistency with conditional mask},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMAE-3D: Contrastive masked AutoEncoders for self-supervised 3D object detection. <em>IJCV</em>, <em>133</em>(5), 2783-2804. (<a href='https://doi.org/10.1007/s11263-024-02313-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LiDAR-based 3D object detection is a crucial task for autonomous driving, owing to its accurate object recognition and localization capabilities in the 3D real-world space. However, existing methods heavily rely on time-consuming and laborious large-scale labeled LiDAR data, posing a bottleneck for both performance improvement and practical applications. In this paper, we propose Contrastive Masked AutoEncoders for self-supervised 3D object detection, dubbed as CMAE-3D, which is a promising solution to effectively alleviate label dependency in 3D perception. Specifically, we integrate Contrastive Learning (CL) and Masked AutoEncoders (MAE) into one unified framework to fully utilize the complementary characteristics of global semantic representation and local spatial perception. Furthermore, from the perspective of MAE, we develop the Geometric-Semantic Hybrid Masking (GSHM) to selectively mask representative regions in point clouds with imbalanced foreground-background and uneven density distribution, and design the Multi-scale Latent Feature Reconstruction (MLFR) to capture high-level semantic features while mitigating the redundant reconstruction of low-level details. From the perspective of CL, we present Hierarchical Relational Contrastive Learning (HRCL) to mine rich semantic similarity information while alleviating the issue of negative sample mismatch from both the voxel-level and frame-level. Extensive experiments demonstrate the effectiveness of our pre-training method when applied to multiple mainstream 3D object detectors (SECOND, CenterPoint and PV-RCNN) on three popular datasets (KITTI, Waymo and nuScenes).},
  archive      = {J_IJCV},
  author       = {Zhang, Yanan and Chen, Jiaxin and Huang, Di},
  doi          = {10.1007/s11263-024-02313-2},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2783-2804},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CMAE-3D: Contrastive masked AutoEncoders for self-supervised 3D object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InfoPro: Locally supervised deep learning by maximizing information propagation. <em>IJCV</em>, <em>133</em>(5), 2752-2782. (<a href='https://doi.org/10.1007/s11263-024-02296-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end (E2E) training has become the de-facto standard for training modern deep networks, e.g., ConvNets and vision Transformers (ViTs). Typically, a global error signal is generated at the end of a model and back-propagated layer-by-layer to update the parameters. This paper shows that the reliance on back-propagating global errors may not be necessary for deep learning. More precisely, deep networks with a competitive or even better performance can be obtained by purely leveraging locally supervised learning, i.e., splitting a network into gradient-isolated modules and training them with local supervision signals. However, such an extension is non-trivial. Our experimental and theoretical analysis demonstrates that simply training local modules with an E2E objective tends to be short-sighted, collapsing task-relevant information at early layers, and hurting the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discarding task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. We evaluate InfoPro extensively with ConvNets and ViTs, based on twelve computer vision benchmarks organized into five tasks (i.e., image/video recognition, semantic/instance segmentation, and object detection). InfoPro exhibits superior efficiency over E2E training in terms of GPU memory footprints, convergence speed, and training data scale. Moreover, InfoPro enables the effective training of more parameter- and computation-efficient models (e.g., much deeper networks), which suffer from inferior performance when trained in E2E. Code: https://github.com/blackfeather-wang/InfoPro-Pytorch .},
  archive      = {J_IJCV},
  author       = {Wang, Yulin and Ni, Zanlin and Pu, Yifan and Zhou, Cai and Ying, Jixuan and Song, Shiji and Huang, Gao},
  doi          = {10.1007/s11263-024-02296-0},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2752-2782},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {InfoPro: Locally supervised deep learning by maximizing information propagation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On mitigating stability-plasticity dilemma in CLIP-guided image morphing via geodesic distillation loss. <em>IJCV</em>, <em>133</em>(5), 2721-2751. (<a href='https://doi.org/10.1007/s11263-024-02308-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale language-vision pre-training models, such as CLIP, have achieved remarkable results in text-guided image morphing by leveraging several unconditional generative models. However, existing CLIP-guided methods face challenges in achieving photorealistic morphing when adapting the generator from the source to the target domain. Specifically, current guidance methods fail to provide detailed explanations of the morphing regions within the image, leading to misguidance and catastrophic forgetting of the original image’s fidelity. In this paper, we propose a novel approach considering proper regularization losses to overcome these difficulties by addressing the SP dilemma in CLIP guidance. Our approach consists of two key components: (1) a geodesic cosine similarity loss that minimizes inter-modality features (i.e., image and text) in a projected subspace of CLIP space, and (2) a latent regularization loss that minimizes intra-modality features (i.e., image and image) on the image manifold. By replacing the naive directional CLIP loss in a drop-in replacement manner, our method achieves superior morphing results for both images and videos across various benchmarks, including CLIP-inversion.},
  archive      = {J_IJCV},
  author       = {Oh, Yeongtak and Lee, Saehyung and Hwang, Uiwon and Yoon, Sungroh},
  doi          = {10.1007/s11263-024-02308-z},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2721-2751},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {On mitigating stability-plasticity dilemma in CLIP-guided image morphing via geodesic distillation loss},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image-based virtual try-on: A survey. <em>IJCV</em>, <em>133</em>(5), 2692-2720. (<a href='https://doi.org/10.1007/s11263-024-02305-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based virtual try-on aims to synthesize a naturally dressed person image with a clothing image, which revolutionizes online shopping and inspires related topics within image generation, showing both research significance and commercial potential. However, there is a gap between current research progress and commercial applications and an absence of comprehensive overview of this field to accelerate the development. In this survey, we provide a comprehensive analysis of the state-of-the-art techniques and methodologies in aspects of pipeline architecture, person representation and key modules such as try-on indication, clothing warping and try-on stage. We additionally apply CLIP to assess the semantic alignment of try-on results, and evaluate representative methods with uniformly implemented evaluation metrics on the same dataset. In addition to quantitative and qualitative evaluation of current open-source methods, unresolved issues are highlighted and future research directions are prospected to identify key trends and inspire further exploration. The uniformly implemented evaluation metrics, dataset and collected methods will be made public available at https://github.com/little-misfit/Survey-Of-Virtual-Try-On.},
  archive      = {J_IJCV},
  author       = {Song, Dan and Zhang, Xuanpu and Zhou, Juan and Nie, Weizhi and Tong, Ruofeng and Kankanhalli, Mohan and Liu, An-An},
  doi          = {10.1007/s11263-024-02305-2},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2692-2720},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Image-based virtual try-on: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Language-guided hierarchical fine-grained image forgery detection and localization. <em>IJCV</em>, <em>133</em>(5), 2670-2691. (<a href='https://doi.org/10.1007/s11263-024-02255-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differences in forgery attributes of images generated in CNN-synthesized and image-editing domains are large, and such differences make a unified image forgery detection and localization (IFDL) challenging. To this end, we present a hierarchical fine-grained formulation for IFDL representation learning. Specifically, we first represent forgery attributes of a manipulated image with multiple labels at different levels. Then, we perform fine-grained classification at these levels using the hierarchical dependency between them. As a result, the algorithm is encouraged to learn both comprehensive features and the inherent hierarchical nature of different forgery attributes, thereby improving the IFDL representation. In this work, we propose a Language-guided Hierarchical Fine-grained IFDL, denoted as HiFi-Net++. Specifically, HiFi-Net++ contains four components: multi-branch feature extractor, language-guided forgery localization enhancer, as well as classification and localization modules. Each branch of the multi-branch feature extractor learns to classify forgery attributes at one level, while localization and classification modules segment the pixel-level forgery region and detect image-level forgery, respectively. In addition, the language-guided forgery localization enhancer (LFLE), containing image and text encoders learned by contrastive language-image pre-training (CLIP), is used to further enrich the IFDL representation. LFLE takes specifically designed texts and the given image as multi-modal inputs and then generates the visual embedding and manipulation score maps, which are used to further improve HiFi-Net++ manipulation localization performance. Lastly, we construct a hierarchical fine-grained dataset to facilitate our study. We demonstrate the effectiveness of our method on 8 different benchmarks for both tasks of IFDL and forgery attribute classification. Our source code and dataset can be found: github.com/CHELSEA234/HiFi-IFDL .},
  archive      = {J_IJCV},
  author       = {Guo, Xiao and Liu, Xiaohong and Masi, Iacopo and Liu, Xiaoming},
  doi          = {10.1007/s11263-024-02255-9},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2670-2691},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Language-guided hierarchical fine-grained image forgery detection and localization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Occlusion-preserved surveillance video synopsis with flexible object graph. <em>IJCV</em>, <em>133</em>(5), 2653-2669. (<a href='https://doi.org/10.1007/s11263-024-02302-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video synopsis is a technique that condenses a long surveillance video to a short summary. It faces challenges to process objects originally occluding each other in the source video. Previous approaches either treat occlusion objects as a single object, which however reduce compression ratio; or have to separate occlusion objects individually, but destroy interactions between them and yield visual artifacts. This paper presents a novel data structure called Flexible Object Graph (FOG) to handle original occlusions. Our FOG-based video synopsis approach can manipulate each object flexibly while preserving the original occlusions between them, achieving high synopsis ratio while maintaining interactions of objects. A challenging issue that comes with the introduction of FOG is that FOG may contain circulations that yield conflicts. We solve this problem by proposing a circulation conflict resolving algorithm. Furthermore, video synopsis methods usually minimize a multi-objective energy function. Previous approaches optimize the multiple objectives simultaneously which needs to strike a balance between them. Instead, we propose a stepwise optimization strategy consuming less running time while producing higher quality. Experiments demonstrate the effectiveness of our method.},
  archive      = {J_IJCV},
  author       = {Nie, Yongwei and Ge, Wei and Zeng, Siming and Zhang, Qing and Li, Guiqing and Li, Ping and Cai, Hongmin},
  doi          = {10.1007/s11263-024-02302-5},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2653-2669},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Occlusion-preserved surveillance video synopsis with flexible object graph},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evaluation of zero-cost proxies - From neural architecture performance prediction to model robustness. <em>IJCV</em>, <em>133</em>(5), 2635-2652. (<a href='https://doi.org/10.1007/s11263-024-02265-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-cost proxies are nowadays frequently studied and used to search for neural architectures. They show an impressive ability to predict the performance of architectures by making use of their untrained weights. These techniques allow for immense search speed-ups. So far the joint search for well performing and robust architectures has received much less attention in the field of NAS. Therefore, the main focus of zero-cost proxies is the clean accuracy of architectures, whereas the model robustness should play an evenly important part. In this paper, we analyze the ability of common zero-cost proxies to serve as performance predictors for robustness in the popular NAS-Bench-201 search space. We are interested in the single prediction task for robustness and the joint multi-objective of clean and robust accuracy. We further analyze the feature importance of the proxies and show that predicting the robustness makes the prediction task from existing zero-cost proxies more challenging. As a result, the joint consideration of several proxies becomes necessary to predict a model’s robustness while the clean accuracy can be regressed from a single such feature. Our code is available at https://github.com/jovitalukasik/zcp_eval .},
  archive      = {J_IJCV},
  author       = {Lukasik, Jovita and Moeller, Michael and Keuper, Margret},
  doi          = {10.1007/s11263-024-02265-7},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2635-2652},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {An evaluation of zero-cost proxies - From neural architecture performance prediction to model robustness},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object pose estimation based on multi-precision vectors and seg-driven PnP. <em>IJCV</em>, <em>133</em>(5), 2620-2634. (<a href='https://doi.org/10.1007/s11263-024-02317-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object pose estimation based on a single RGB image has wide application potential but is difficult to achieve. Existing pose estimation involves various inference pipelines. One popular pipeline is to first use Convolutional Neural Networks (CNN) to predict 2D projections of 3D keypoints in a single RGB image and then calculate the 6D pose via a Perspective-n-Point (PnP) solver. Due to the gap between synthetic data and real data, the model trained on synthetic data has difficulty predicting the 6D pose accurately when applied to real data. To address the acute problem, we propose a two-stage pipeline of object pose estimation based upon multi-precision vectors and segmentation-driven (Seg-Driven) PnP. In keypoint localization stage, we first develop a CNN-based three-branch network to predict multi-precision 2D vectors pointing to 2D keypoints. Then we introduce an accurate and fast Keypoint Voting scheme of Multi-precision vectors (KVM), which computes low-precision 2D keypoints using low-precision vectors and refines 2D keypoints on mid- and high-precision vectors. In the pose calculation stage, we propose Seg-Driven PnP to refine the 3D Translation of poses and get the optimal pose by minimizing the non-overlapping area between segmented and rendered masks. The Seg-Driven PnP leverages 2D segmentation trained on real images to improve the accuracy of pose estimation trained on synthetic data, thereby reducing the synthetic-to-real gap. Extensive experiments show our approach materially outperforms state-of-the-art methods on LM and HB datasets. Importantly, our proposed method works reasonably well for weakly textured and occluded objects in diverse scenes.},
  archive      = {J_IJCV},
  author       = {Wang, Yulin and Li, Hongli and Luo, Chen},
  doi          = {10.1007/s11263-024-02317-y},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2620-2634},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Object pose estimation based on multi-precision vectors and seg-driven PnP},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modality-missing RGBT tracking: Invertible prompt learning and high-quality benchmarks. <em>IJCV</em>, <em>133</em>(5), 2599-2619. (<a href='https://doi.org/10.1007/s11263-024-02311-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current RGBT tracking research relies on the complete multi-modality input, but modal information might miss due to some factors such as thermal sensor self-calibration and data transmission error, called modality-missing challenge in this work. To address this challenge, we propose a novel invertible prompt learning approach, which integrates the content-preserving prompts into a well-trained tracking model to adapt to various modality-missing scenarios, for robust RGBT tracking. Given one modality-missing scenario, we propose to utilize the available modality to generate the prompt of the missing modality to adapt to RGBT tracking model. However, the cross-modality gap between available and missing modalities usually causes semantic distortion and information loss in prompt generation. To handle this issue, we design the invertible prompter by incorporating the full reconstruction of the input available modality from the generated prompt. To provide a comprehensive evaluation platform, we construct several high-quality benchmark datasets, in which various modality-missing scenarios are considered to simulate real-world challenges. Extensive experiments on three modality-missing benchmark datasets show that our method achieves significant performance improvements compared with state-of-the-art methods. We have released the code and simulation datasets at: https://github.com/mmic-lcl .},
  archive      = {J_IJCV},
  author       = {Lu, Andong and Li, Chenglong and Zhao, Jiacong and Tang, Jin and Luo, Bin},
  doi          = {10.1007/s11263-024-02311-4},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2599-2619},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Modality-missing RGBT tracking: Invertible prompt learning and high-quality benchmarks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLIP-powered TASS: Target-aware single-stream network for audio-visual question answering. <em>IJCV</em>, <em>133</em>(5), 2581-2598. (<a href='https://doi.org/10.1007/s11263-024-02289-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While vision-language pretrained models (VLMs) excel in various multimodal understanding tasks, their potential in fine-grained audio-visual reasoning, particularly for audio-visual question answering (AVQA), remains largely unexplored. AVQA presents specific challenges for VLMs due to the requirement of visual understanding at the region level and seamless integration with audio modality. Previous VLM-based AVQA methods merely used CLIP as a feature encoder but underutilized its knowledge, and mistreated audio and video as separate entities in a dual-stream framework as most AVQA methods. This paper proposes a new CLIP-powered target-aware single-stream (TASS) network for AVQA using the pretrained knowledge of the CLIP model through the audio-visual matching characteristic of nature. It consists of two key components: the target-aware spatial grounding module (TSG+) and the single-stream joint temporal grounding module (JTG). Specifically, TSG+ module transfers the image-text matching knowledge from CLIP models to the required region-text matching process without corresponding ground-truth labels. Moreover, unlike previous separate dual-stream networks that still required an additional audio-visual fusion module, JTG unifies audio-visual fusion and question-aware temporal grounding in a simplified single-stream architecture. It treats audio and video as a cohesive entity and further extends the image-text matching knowledge to audio-text matching by preserving their temporal correlation with our proposed cross-modal synchrony (CMS) loss. Besides, we propose a simple yet effective preprocessing strategy to optimize accuracy-efficiency trade-offs. Extensive experiments conducted on the MUSIC-AVQA benchmark verified the effectiveness of our proposed method over existing state-of-the-art methods. The code is available at https://github.com/Bravo5542/CLIP-TASS.},
  archive      = {J_IJCV},
  author       = {Jiang, Yuanyuan and Yin, Jianqin},
  doi          = {10.1007/s11263-024-02289-z},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2581-2598},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CLIP-powered TASS: Target-aware single-stream network for audio-visual question answering},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instance-dependent label distribution estimation for learning with label noise. <em>IJCV</em>, <em>133</em>(5), 2568-2580. (<a href='https://doi.org/10.1007/s11263-024-02299-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise transition matrix estimation is a promising approach for learning with label noise. It can infer clean posterior probabilities, known as Label Distribution (LD), based on noisy ones and reduce the impact of noisy labels. However, this estimation is challenging, since the ground truth labels are not always available. Most existing methods estimate a global noise transition matrix using either correctly labeled samples (anchor points) or detected reliable samples (pseudo anchor points). These methods heavily rely on the existence of anchor points or the quality of pseudo ones, and the global noise transition matrix can hardly provide accurate label transition information for each sample, since the label noise in real applications is mostly instance-dependent. To address these challenges, we propose an Instance-dependent Label Distribution Estimation (ILDE) method to learn from noisy labels for image classification. The method’s workflow has three major steps. First, we estimate each sample’s noisy posterior probability, supervised by noisy labels. Second, since mislabeling probability closely correlates with inter-class correlation, we compute the inter-class correlation matrix to estimate the noise transition matrix, bypassing the need for (pseudo) anchor points. Moreover, for a precise approximation of the instance-dependent noise transition matrix, we calculate the inter-class correlation matrix using only mini-batch samples rather than the entire training dataset. Third, we transform the noisy posterior probability into instance-dependent LD by multiplying it with the estimated noise transition matrix, using the resulting LD for enhanced supervision to prevent DCNNs from memorizing noisy labels. The proposed ILDE method has been evaluated against several state-of-the-art methods on two synthetic and three real-world noisy datasets. Our results indicate that the proposed ILDE method outperforms all competing methods, no matter whether the noise is synthetic or real noise.},
  archive      = {J_IJCV},
  author       = {Liao, Zehui and Hu, Shishuai and Xie, Yutong and Xia, Yong},
  doi          = {10.1007/s11263-024-02299-x},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2568-2580},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Instance-dependent label distribution estimation for learning with label noise},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReFusion: Learning image fusion from reconstruction with learnable loss via meta-learning. <em>IJCV</em>, <em>133</em>(5), 2547-2567. (<a href='https://doi.org/10.1007/s11263-024-02256-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion aims to combine information from multiple source images into a single one with more comprehensive informational content. Deep learning-based image fusion algorithms face significant challenges, including the lack of a definitive ground truth and the corresponding distance measurement. Additionally, current manually defined loss functions limit the model’s flexibility and generalizability for various fusion tasks. To address these limitations, we propose ReFusion, a unified meta-learning based image fusion framework that dynamically optimizes the fusion loss for various tasks through source image reconstruction. Compared to existing methods, ReFusion employs a parameterized loss function, that allows the training framework to be dynamically adapted according to the specific fusion scenario and task. ReFusion consists of three key components: a fusion module, a source reconstruction module, and a loss proposal module. We employ a meta-learning strategy to train the loss proposal module using the reconstruction loss. This strategy forces the fused image to be more conducive to reconstruct source images, allowing the loss proposal module to generate a adaptive fusion loss that preserves the optimal information from the source images. The update of the fusion module relies on the learnable fusion loss proposed by the loss proposal module. The three modules update alternately, enhancing each other to optimize the fusion loss for different tasks and consistently achieve satisfactory results. Extensive experiments demonstrate that ReFusion is capable of adapting to various tasks, including infrared-visible, medical, multi-focus, and multi-exposure image fusion. The code is available at https://github.com/HaowenBai/ReFusion .},
  archive      = {J_IJCV},
  author       = {Bai, Haowen and Zhao, Zixiang and Zhang, Jiangshe and Wu, Yichen and Deng, Lilun and Cui, Yukun and Jiang, Baisong and Xu, Shuang},
  doi          = {10.1007/s11263-024-02256-8},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2547-2567},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ReFusion: Learning image fusion from reconstruction with learnable loss via meta-learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffLLE: Diffusion-based domain calibration for weak supervised low-light image enhancement. <em>IJCV</em>, <em>133</em>(5), 2527-2546. (<a href='https://doi.org/10.1007/s11263-024-02292-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing weak supervised low-light image enhancement methods lack enough effectiveness and generalization in practical applications. We suppose this is because of the absence of explicit supervision and the inherent gap between real-world low-light domain and the training low-light domain. For example, low-light datasets are well-designed, but real-world night scenes are plagued with sophisticated interference such as noise, artifacts, and extreme lighting conditions. In this paper, we develop Diffusion-based domain calibration to realize more robust and effective weak supervised Low-Light Enhancement, called DiffLLE. Since the diffusion model performs impressive denoising capability and has been trained on massive clean images, we adopt it to bridge the gap between the real low-light domain and training degradation domain, while providing efficient priors of real-world content for weak supervised models. Specifically, we adopt a naive weak supervised enhancement algorithm to realize preliminary restoration and design two zero-shot plug-and-play modules based on diffusion model to improve generalization and effectiveness. The Diffusion-guided Degradation Calibration (DDC) module narrows the gap between real-world and training low-light degradation through diffusion-based domain calibration and a lightness enhancement curve, which makes the enhancement model perform robustly even in sophisticated wild degradation. Due to the limited enhancement effect of the weak supervised model, we further develop the Fine-grained Target domain Distillation (FTD) module to find a more visual-friendly solution space. It exploits the priors of the pre-trained diffusion model to generate pseudo-references, which shrinks the preliminary restored results from a coarse normal-light domain to a finer high-quality clean field, addressing the lack of strong explicit supervision for weak supervised methods. Benefiting from these, our approach even outperforms some supervised methods by using only a simple weak supervised baseline. Extensive experiments demonstrate the superior effectiveness of the proposed DiffLLE, especially in real-world dark scenes.},
  archive      = {J_IJCV},
  author       = {Yang, Shuzhou and Zhang, Xuanyu and Wang, Yinhuai and Yu, Jiwen and Wang, Yuhan and Zhang, Jian},
  doi          = {10.1007/s11263-024-02292-4},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2527-2546},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DiffLLE: Diffusion-based domain calibration for weak supervised low-light image enhancement},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Draw sketch, draw flesh: Whole-body computed tomography from any X-ray views. <em>IJCV</em>, <em>133</em>(5), 2505-2526. (<a href='https://doi.org/10.1007/s11263-024-02286-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereoscopic observation is a common foundation of medical image analysis and is generally achieved by 3D medical imaging based on settled scanners, such as CT and MRI, that are not as convenient as X-ray machines in some flexible scenarios. However, X-ray images can only provide perspective 2D observation and lack view in the third dimension. If 3D information can be deduced from X-ray images, it would broaden the application of X-ray machines. Focus on the above objective, this paper dedicates to the generation of pseudo 3D CT scans from non-parallel 2D perspective X-ray (PXR) views and proposes the Draw Sketch and Draw Flesh (DSDF) framework to first roughly predict the tissue distribution (Sketch) from PXR views and then render the tissue details (Flesh) from the tissue distribution and PXR views. Different from previous studies that focus only on partial locations, e.g., chest or neck, this study theoretically investigates the feasibility of head-to-leg reconstruction, i.e., generally applicable to any body parts. Experiments on 559 whole-body samples from 4 cohorts suggest that our DSDF can reconstruct more reasonable pseudo CT images than state-of-the-art methods and achieve promising results in both visualization and various downstream tasks. The source code and well-trained models are available a https://github.com/YongshengPan/WholeBodyXraytoCT .},
  archive      = {J_IJCV},
  author       = {Pan, Yongsheng and Ye, Yiwen and Zhang, Yanning and Xia, Yong and Shen, Dinggang},
  doi          = {10.1007/s11263-024-02286-2},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2505-2526},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Draw sketch, draw flesh: Whole-body computed tomography from any X-ray views},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ICEv2: Interpretability, comprehensiveness, and explainability in vision transformer. <em>IJCV</em>, <em>133</em>(5), 2487-2504. (<a href='https://doi.org/10.1007/s11263-024-02290-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformers use [CLS] token to predict image classes. Their explainability visualization has been studied using relevant information from the [CLS] token or focusing on attention scores during self-attention. However, such visualization is challenging because of the dependence of the interpretability of a vision transformer on skip connections and attention operators, the instability of non-linearities in the learning process, and the limited reflection of self-attention scores on relevance. We argue that the output patch embeddings in a vision transformer preserve the image information of each patch location, which can facilitate the prediction of an image class. In this paper, we propose ICEv2 (ICEv2: $${{{\underline{\varvec{I}}}}}$$ nterpretability, $${{{\underline{\varvec{C}}}}}$$ omprehensiveness, and $${{{\underline{\varvec{E}}}}}$$ xplainability in Vision Transformer), an explainability visualization method that addresses the limitations of ICE (i.e., high dependence of hyperparameters on performance and the inability to preserve the model’s properties) by minimizing the number of training encoder layers, redesigning the MLP layer, and optimizing hyperparameters along with various model size. Overall, ICEv2 shows higher efficiency, performance, robustness, and scalability than ICE. On the ImageNet-Segmentation dataset, ICEv2 outperformed all explainability visualization methods in all cases depending on the model size. On the Pascal VOC dataset, ICEv2 outperformed both self-supervised and supervised methods on Jaccard similarity. In the unsupervised single object discovery, where untrained classes are present in the images, ICEv2 effectively distinguished between foreground and background, showing performance comparable to the previous state-of-the-art. Lastly, ICEv2 can be trained with significantly lower training computational complexity.},
  archive      = {J_IJCV},
  author       = {Choi, Hoyoung and Jin, Seungwan and Han, Kyungsik},
  doi          = {10.1007/s11263-024-02290-6},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2487-2504},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ICEv2: Interpretability, comprehensiveness, and explainability in vision transformer},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IEBins: Iterative elastic bins for monocular depth estimation and completion. <em>IJCV</em>, <em>133</em>(5), 2463-2486. (<a href='https://doi.org/10.1007/s11263-024-02293-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation and completion are fundamental aspects of geometric computer vision, serving as essential techniques for various downstream applications. In recent developments, several methods have reformulated these two tasks as a classification-regression problem, deriving depth with a linear combination of predicted probabilistic distribution and bin centers. In this paper, we introduce an innovative concept termed iterative elastic bins (IEBins) for the classification-regression-based monocular depth estimation and completion. The IEBins involves the idea of iterative division of bins. In the initialization stage, a coarse and uniform discretization is applied to the entire depth range. Subsequent update stages then iteratively identify and uniformly discretize the target bin, by leveraging it as the new depth range for further refinement. To mitigate the risk of error accumulation during iterations, we propose a novel elastic target bin, replacing the original one. The width of this elastic bin is dynamically adapted according to the depth uncertainty. Furthermore, we develop dedicated frameworks to instantiate the IEBins. Extensive experiments on the KITTI, NYU-Depth-v2, SUN RGB-D, ScanNet and DIODE datasets indicate that our method outperforms prior state-of-the-art monocular depth estimation and completion competitors.},
  archive      = {J_IJCV},
  author       = {Shao, Shuwei and Pei, Zhongcai and Chen, Weihai and Chen, Peter C. Y. and Li, Zhengguo},
  doi          = {10.1007/s11263-024-02293-3},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2463-2486},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {IEBins: Iterative elastic bins for monocular depth estimation and completion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Globally correlation-aware hard negative generation. <em>IJCV</em>, <em>133</em>(5), 2441-2462. (<a href='https://doi.org/10.1007/s11263-024-02288-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hard negative generation aims to generate informative negative samples that help to determine the decision boundaries and thus facilitate advancing deep metric learning. Current works select pair/triplet samples, learn their correlations, and fuse them to generate hard negatives. However, these works merely consider the local correlations of selected samples, ignoring global sample correlations that would provide more significant information to generate more informative negatives. In this work, we propose a globally correlation-aware hard negative generation (GCA-HNG) framework, which first learns sample correlations from a global perspective and exploits these correlations to guide generating hardness-adaptive and diverse negatives. Specifically, this approach begins by constructing a structured graph to model sample correlations, where each node represents a specific sample and each edge represents the correlations between corresponding samples. Then, we introduce an iterative graph message propagation to propagate the messages of node and edge through the whole graph and thus learn the sample correlations globally. Finally, with the guidance of the learned global correlations, we propose a channel-adaptive manner to combine an anchor and multiple negatives for HNG. Compared to current methods, GCA-HNG allows perceiving sample correlations with numerous negatives from a global and comprehensive perspective and generates the negatives with better hardness and diversity. Extensive experiment results demonstrate that the proposed GCA-HNG is superior to related methods on four image retrieval benchmark datasets.},
  archive      = {J_IJCV},
  author       = {Peng, Wenjie and Huang, Hongxiang and Chen, Tianshui and Ke, Quhui and Dai, Gang and Huang, Shuangping},
  doi          = {10.1007/s11263-024-02288-0},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2441-2462},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Globally correlation-aware hard negative generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer for object re-identification: A survey. <em>IJCV</em>, <em>133</em>(5), 2410-2440. (<a href='https://doi.org/10.1007/s11263-024-02284-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object Re-identification (Re-ID) aims to identify specific objects across different times and scenes, which is a widely researched task in computer vision. For a prolonged period, this field has been predominantly driven by deep learning technology based on convolutional neural networks. In recent years, the emergence of Vision Transformers has spurred a growing number of studies delving deeper into Transformer-based Re-ID, continuously breaking performance records and witnessing significant progress in the Re-ID field. Offering a powerful, flexible, and unified solution, Transformers cater to a wide array of Re-ID tasks with unparalleled efficacy. This paper provides a comprehensive review and in-depth analysis of the Transformer-based Re-ID. In categorizing existing works into Image/Video-Based Re-ID, Re-ID with limited data/annotations, Cross-Modal Re-ID, and Special Re-ID Scenarios, we thoroughly elucidate the advantages demonstrated by the Transformer in addressing a multitude of challenges across these domains. Considering the trending unsupervised Re-ID, we propose a new Transformer baseline, UntransReID, achieving state-of-the-art performance on both single/cross modal tasks. For the under-explored animal Re-ID, we devise a standardized experimental benchmark and conduct extensive experiments to explore the applicability of Transformer for this task and facilitate future research. Finally, we discuss some important yet under-investigated open issues in the large foundation model era, we believe it will serve as a new handbook for researchers in this field. A periodically updated website will be available at https://github.com/mangye16/ReID-Survey .},
  archive      = {J_IJCV},
  author       = {Ye, Mang and Chen, Shuoyi and Li, Chenyue and Zheng, Wei-Shi and Crandall, David and Du, Bo},
  doi          = {10.1007/s11263-024-02284-4},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2410-2440},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Transformer for object re-identification: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable evaluation of attribution maps in CNNs: A perturbation-based approach. <em>IJCV</em>, <em>133</em>(5), 2392-2409. (<a href='https://doi.org/10.1007/s11263-024-02282-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an approach for evaluating attribution maps, which play a central role in interpreting the predictions of convolutional neural networks (CNNs). We show that the widely used insertion/deletion metrics are susceptible to distribution shifts that affect the reliability of the ranking. Our method proposes to replace pixel modifications with adversarial perturbations, which provides a more robust evaluation framework. By using smoothness and monotonicity measures, we illustrate the effectiveness of our approach in correcting distribution shifts. In addition, we conduct the most comprehensive quantitative and qualitative assessment of attribution maps to date. Introducing baseline attribution maps as sanity checks, we find that our metric is the only contender to pass all checks. Using Kendall’s $$\tau $$ rank correlation coefficient, we show the increased consistency of our metric across 15 dataset-architecture combinations. Of the 16 attribution maps tested, our results clearly show SmoothGrad to be the best map currently available. This research makes an important contribution to the development of attribution maps by providing a reliable and consistent evaluation framework. To ensure reproducibility, we will provide the code along with our results.},
  archive      = {J_IJCV},
  author       = {Nieradzik, Lars and Stephani, Henrike and Keuper, Janis},
  doi          = {10.1007/s11263-024-02282-6},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2392-2409},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Reliable evaluation of attribution maps in CNNs: A perturbation-based approach},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-shot generative domain adaptation in 3D GANs. <em>IJCV</em>, <em>133</em>(5), 2371-2391. (<a href='https://doi.org/10.1007/s11263-024-02268-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D-aware image generation necessitates extensive training data to ensure stable training and mitigate the risk of overfitting. This paper first consider a novel task known as One-shot 3D Generative Domain Adaptation (GDA), aimed at transferring a pre-trained 3D generator from one domain to a new one, relying solely on a single reference image. One-shot 3D GDA is characterized by the pursuit of specific attributes, namely, high fidelity, large diversity, cross-domain consistency, and multi-view consistency. Within this paper, we introduce 3D-Adapter, the first one-shot 3D GDA method, for diverse and faithful generation. Our approach begins by judiciously selecting a restricted weight set for fine-tuning, and subsequently leverages four advanced loss functions to facilitate adaptation. An efficient progressive fine-tuning strategy is also implemented to enhance the adaptation process. The synergy of these three technological components empowers 3D-Adapter to achieve remarkable performance, substantiated both quantitatively and qualitatively, across all desired properties of 3D GDA. Furthermore, 3D-Adapter seamlessly extends its capabilities to zero-shot scenarios, and preserves the potential for crucial tasks such as interpolation, reconstruction, and editing within the latent space of the pre-trained generator. Code will be available at https://github.com/iceli1007/3D-Adapter .},
  archive      = {J_IJCV},
  author       = {Li, Ziqiang and Wu, Yi and Wang, Chaoyue and Rui, Xue and Li, Bin},
  doi          = {10.1007/s11263-024-02268-4},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2371-2391},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {One-shot generative domain adaptation in 3D GANs},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NAFT and SynthStab: A RAFT-based network and a synthetic dataset for digital video stabilization. <em>IJCV</em>, <em>133</em>(5), 2345-2370. (<a href='https://doi.org/10.1007/s11263-024-02264-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple deep learning-based stabilization methods have been proposed recently. Some of them directly predict the optical flow to warp each unstable frame into its stabilized version, which we called direct warping. These methods primarily perform online or semi-online stabilization, prioritizing lower computational cost while achieving satisfactory results in certain scenarios. However, they fail to smooth intense instabilities and have considerably inferior results in comparison to other approaches. To improve their quality and reduce this difference, we propose: (a) NAFT, a new direct warping semi-online stabilization method, which adapts RAFT to videos by including a neighborhood-aware update mechanism, called IUNO. By using our training approach along with IUNO, we can learn the characteristics that contribute to video stability from the data patterns, rather than requiring an explicit stability definition. Furthermore, we demonstrate how leveraging an off-the-shelf video inpainting method to achieve full-frame stabilization; (b) SynthStab, a new synthetic dataset consisting of paired videos that allows supervision by camera motion instead of pixel similarities. To build SynthStab, we modeled camera motion using kinematic concepts. In addition, the unstable motion respects scene constraints, such as depth variation. We performed several experiments on SynthStab to develop and validate NAFT. We compared our results with five other methods from the literature with publicly available code. Our experimental results show that we were able to stabilize intense camera motion, outperforming other direct warping methods and bringing its performance closer to state-of-the-art methods. In terms of computational resources, our smallest network has only about 7% of model size and trainable parameters than the smallest values among the competing methods.},
  archive      = {J_IJCV},
  author       = {e Souza, Marcos Roberto and Maia, Helena de Almeida and Pedrini, Helio},
  doi          = {10.1007/s11263-024-02264-8},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2345-2370},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {NAFT and SynthStab: A RAFT-based network and a synthetic dataset for digital video stabilization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CS-CoLBP: Cross-scale co-occurrence local binary pattern for image classification. <em>IJCV</em>, <em>133</em>(5), 2327-2344. (<a href='https://doi.org/10.1007/s11263-024-02297-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The local binary pattern (LBP) is an effective feature, describing the size relationship between the neighboring pixels and the current pixel. While individual LBP-based methods yield good results, co-occurrence LBP-based methods exhibit a better ability to extract structural information. However, most of the co-occurrence LBP-based methods excel mainly in dealing with rotated images, exhibiting limitations in preserving performance for scaled images. To address the issue, a cross-scale co-occurrence LBP (CS-CoLBP) is proposed. Initially, we construct an LBP co-occurrence space to capture robust structural features by simulating scale transformation. Subsequently, we use Cross-Scale Co-occurrence pairs (CS-Co pairs) to extract the structural features, keeping robust descriptions even in the presence of scaling. Finally, we refine these CS-Co pairs through Rotation Consistency Adjustment (RCA) to bolster their rotation invariance, thereby making the proposed CS-CoLBP as powerful as existing co-occurrence LBP-based methods for rotated image description. While keeping the desired geometric invariance, the proposed CS-CoLBP maintains a modest feature dimension. Empirical evaluations across several datasets demonstrate that CS-CoLBP outperforms the existing state-of-the-art LBP-based methods even in the presence of geometric transformations and image manipulations.},
  archive      = {J_IJCV},
  author       = {Xiao, Bin and Shi, Danyu and Bi, Xiuli and Li, Weisheng and Gao, Xinbo},
  doi          = {10.1007/s11263-024-02297-z},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2327-2344},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CS-CoLBP: Cross-scale co-occurrence local binary pattern for image classification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Warping the residuals for image editing with StyleGAN. <em>IJCV</em>, <em>133</em>(5), 2311-2326. (<a href='https://doi.org/10.1007/s11263-024-02301-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {StyleGAN models show editing capabilities via their semantically interpretable latent organizations which require successful GAN inversion methods to edit real images. Many works have been proposed for inverting images into StyleGAN’s latent space. However, their results either suffer from low fidelity to the input image or poor editing qualities, especially for edits that require large transformations. That is because low bit rate latent spaces lose many image details due to the information bottleneck even though it provides an editable space. On the other hand, higher bit rate latent spaces can pass all the image details to StyleGAN for perfect reconstruction of images but suffer from low editing qualities. In this work, we present a novel image inversion architecture that extracts high-rate latent features and includes a flow estimation module to warp these features to adapt them to edits. This is because edits often involve spatial changes in the image, such as adjustments to pose or smile. Thus, high-rate latent features must be accurately repositioned to match their new locations in the edited image space. We achieve this by employing flow estimation to determine the necessary spatial adjustments, followed by warping the features to align them correctly in the edited image. Specifically, we estimate the flows from StyleGAN features of edited and unedited latent codes. By estimating the high-rate features and warping them for edits, we achieve both high-fidelity to the input image and high-quality edits. We run extensive experiments and compare our method with state-of-the-art inversion methods. Qualitative metrics and visual comparisons show significant improvements.},
  archive      = {J_IJCV},
  author       = {Yildirim, Ahmet Burak and Pehlivan, Hamza and Dundar, Aysegul},
  doi          = {10.1007/s11263-024-02301-6},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2311-2326},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Warping the residuals for image editing with StyleGAN},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pulling target to source: A new perspective on domain adaptive semantic segmentation. <em>IJCV</em>, <em>133</em>(5), 2287-2310. (<a href='https://doi.org/10.1007/s11263-024-02285-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain-adaptive semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. However, existing methods primarily focus on directly learning categorically discriminative target features for segmenting target images, which is challenging in the absence of target labels. This work provides a new perspective. We ob serve that the features learned with source data manage to keep categorically discriminative during training, thereby enabling us to implicitly learn adequate target representations by simply pulling target features close to source features for each category. To this end, we propose T2S-DA, which encourages the model to learn similar cross-domain features. Also, considering the pixel categories are heavily imbalanced for segmentation datasets, we come up with a dynamic re-weighting strategy to help the model concentrate on those underperforming classes. Extensive experiments confirm that T2S-DA learns a more discriminative and generalizable representation, significantly surpassing the state-of-the-art. We further show that T2S-DA is quite qualified for the domain generalization task, verifying its domain-invariant property.},
  archive      = {J_IJCV},
  author       = {Wang, Haochen and Shen, Yujun and Fei, Jingjing and Li, Wei and Wu, Liwei and Wang, Yuxi and Zhang, Zhaoxiang},
  doi          = {10.1007/s11263-024-02285-3},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2287-2310},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Pulling target to source: A new perspective on domain adaptive semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature matching via graph clustering with local affine consensus. <em>IJCV</em>, <em>133</em>(5), 2259-2286. (<a href='https://doi.org/10.1007/s11263-024-02291-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies graph clustering with application to feature matching and proposes an effective method, termed as GC-LAC, that can establish reliable feature correspondences and simultaneously discover all potential visual patterns. In particular, we regard each putative match as a node and encode the geometric relationships into edges where a visual pattern sharing similar motion behaviors corresponds to a strongly connected subgraph. In this setting, it is natural to formulate the feature matching task as a graph clustering problem. To construct a geometric meaningful graph, based on the best practices, we adopt a local affine strategy. By investigating the motion coherence prior, we further propose an efficient and deterministic geometric solver (MCDG) to extract the local geometric information that helps construct the graph. The graph is sparse and general for various image transformations. Subsequently, a novel robust graph clustering algorithm (D2SCAN) is introduced, which defines the notion of density-reachable on the graph by replicator dynamics optimization. Extensive experiments focusing on both the local and the whole of our GC-LAC with various practical vision tasks including relative pose estimation, homography and fundamental matrix estimation, loop-closure detection, and multimodel fitting, demonstrate that our GC-LAC is more competitive than current state-of-the-art methods, in terms of generality, efficiency, and effectiveness. The source code for this work is publicly available at: https://github.com/YifanLu2000/GCLAC.},
  archive      = {J_IJCV},
  author       = {Lu, Yifan and Ma, Jiayi},
  doi          = {10.1007/s11263-024-02291-5},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2259-2286},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Feature matching via graph clustering with local affine consensus},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to detect novel species with SAM in the wild. <em>IJCV</em>, <em>133</em>(5), 2247-2258. (<a href='https://doi.org/10.1007/s11263-024-02234-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the limitation of a closed-world object detection model that was trained on one species. The expectation for this model is that it will not generalize well to recognize the instances of new species if they were present in the incoming data stream. We propose a novel object detection framework for this open-world setting that is suitable for applications that monitor wildlife, ocean life, livestock, plant phenotype and crops that typically feature one species in the image. Our method leverages labeled samples from one species in combination with a novelty detection method and Segment Anything Model, a vision foundation model, to (1) identify the presence of new species in unlabeled images, (2) localize their instances, and (3) retrain the initial model with the localized novel class instances. The resulting integrated system assimilates and learns from unlabeled samples of the new classes while not “forgetting” the original species the model was trained on. We demonstrate our findings on two different domains, (1) wildlife detection and (2) plant detection. Our method achieves an AP of 56.2 (for 4 novel species) to 61.6 (for 1 novel species) for wildlife domain, without relying on any ground truth data in the background.},
  archive      = {J_IJCV},
  author       = {Allabadi, Garvita and Lucic, Ana and Wang, Yu-Xiong and Adve, Vikram},
  doi          = {10.1007/s11263-024-02234-0},
  journal      = {International Journal of Computer Vision},
  month        = {5},
  number       = {5},
  pages        = {2247-2258},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning to detect novel species with SAM in the wild},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Continual face forgery detection via historical distribution preserving. <em>IJCV</em>, <em>133</em>(4), 2246. (<a href='https://doi.org/10.1007/s11263-024-02287-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Sun, Ke and Chen, Shen and Yao, Taiping and Sun, Xiaoshuai and Ding, Shouhong and Ji, Rongrong},
  doi          = {10.1007/s11263-024-02287-1},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2246},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Continual face forgery detection via historical distribution preserving},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Multi-source-free domain adaptive object detection. <em>IJCV</em>, <em>133</em>(4), 2245. (<a href='https://doi.org/10.1007/s11263-024-02257-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zhao, Sicheng and Yao, Huizai and Lin, Chuang and Gao, Yue and Ding, Guiguang},
  doi          = {10.1007/s11263-024-02257-7},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2245},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Multi-source-free domain adaptive object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diagnosing human-object interaction detectors. <em>IJCV</em>, <em>133</em>(4), 2227-2244. (<a href='https://doi.org/10.1007/s11263-025-02369-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have witnessed significant progress in human-object interaction (HOI) detection. However, relying solely on mAP (mean Average Precision) scores as a summary metric does not provide sufficient insight into the nuances of model performance (e.g., why one model outperforms another), which can hinder further innovation in this field. To address this issue, we introduce a diagnosis toolbox in this paper to offer a detailed quantitative breakdown of HOI detection models, inspired by the success of object detection diagnosis tools. We first conduct a holistic investigation into the HOI detection pipeline. By defining a set of errors and using oracles to fix each one, we quantitatively analyze the significance of different errors based on the mAP improvement gained from fixing them. Next, we explore the two key sub-tasks of HOI detection: human-object pair localization and interaction classification. For the pair localization task, we compute the coverage of ground-truth human-object pairs and assess the noisiness of the localization results. For the classification task, we measure a model’s ability to distinguish between positive and negative detection results and to classify actual interactions when human-object pairs are correctly localized. We analyze eight state-of-the-art HOI detection models, providing valuable diagnostic insights to guide future research. For instance, our diagnosis reveals that the state-of-the-art model RLIPv2 outperforms others primarily due to its significant improvement in multi-label interaction classification accuracy. Our toolbox is applicable across various methods and datasets and is available at https://neu-vi.github.io/Diag-HOI/ .},
  archive      = {J_IJCV},
  author       = {Zhu, Fangrui and Xie, Yiming and Xie, Weidi and Jiang, Huaizu},
  doi          = {10.1007/s11263-025-02369-8},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2227-2244},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Diagnosing human-object interaction detectors},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MVTN: Learning multi-view transformations for 3D understanding. <em>IJCV</em>, <em>133</em>(4), 2197-2226. (<a href='https://doi.org/10.1007/s11263-024-02283-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view projection techniques have shown themselves to be highly effective in achieving top-performing results in the recognition of 3D shapes. These methods involve learning how to combine information from multiple view-points. However, the camera view-points from which these views are obtained are often fixed for all shapes. To overcome the static nature of current multi-view techniques, we propose learning these view-points. Specifically, we introduce the Multi-View Transformation Network (MVTN), which uses differentiable rendering to determine optimal view-points for 3D shape recognition. As a result, MVTN can be trained end-to-end with any multi-view network for 3D shape classification. We integrate MVTN into a novel adaptive multi-view pipeline that is capable of rendering both 3D meshes and point clouds. Our approach demonstrates state-of-the-art performance in 3D classification and shape retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55). Further analysis indicates that our approach exhibits improved robustness to occlusion compared to other methods. We also investigate additional aspects of MVTN, such as 2D pretraining and its use for segmentation. To support further research in this area, we have released MVTorch, a PyTorch library for 3D understanding and generation using multi-view projections.},
  archive      = {J_IJCV},
  author       = {Hamdi, Abdullah and AlZahrani, Faisal and Giancola, Silvio and Ghanem, Bernard},
  doi          = {10.1007/s11263-024-02283-5},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2197-2226},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MVTN: Learning multi-view transformations for 3D understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive middle modality alignment learning for visible-infrared person re-identification. <em>IJCV</em>, <em>133</em>(4), 2176-2196. (<a href='https://doi.org/10.1007/s11263-024-02276-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VIReID) has attracted increasing attention due to the requirements for 24-hour intelligent surveillance systems. In this task, one of the major challenges is the modality discrepancy between the visible (VIS) and infrared (NIR) images. Most conventional methods try to design complex networks or generative models to mitigate the cross-modality discrepancy while ignoring the fact that the modality gaps differ between the different VIS and NIR images. Different from existing methods, in this paper, we propose an Adaptive Middle-modality Alignment Learning (AMML) method, which can effectively reduce the modality discrepancy via an adaptive middle modality learning strategy at both image level and feature level. The proposed AMML method enjoys several merits. First, we propose an Adaptive Middle-modality Generator (AMG) module to reduce the modality discrepancy between the VIS and NIR images from the image level, which can effectively project the VIS and NIR images into a unified middle modality image (UMMI) space to adaptively generate middle-modality (M-modality) images. Second, we propose a feature-level Adaptive Distribution Alignment (ADA) loss to force the distribution of the VIS features and NIR features adaptively align with the distribution of M-modality features. Moreover, we also propose a novel Center-based Diverse Distribution Learning (CDDL) loss, which can effectively learn diverse cross-modality knowledge from different modalities while reducing the modality discrepancy between the VIS and NIR modalities. Extensive experiments on three challenging VIReID datasets show the superiority of the proposed AMML method over the other state-of-the-art methods. More remarkably, our method achieves 77.8% in terms of Rank-1 and 74.8% in terms of mAP on the SYSU-MM01 dataset for all search mode, and 86.6% in terms of Rank-1 and 88.3% in terms of mAP on the SYSU-MM01 dataset for indoor search mode. The code is released at: https://github.com/ZYK100/MMN .},
  archive      = {J_IJCV},
  author       = {Zhang, Yukang and Yan, Yan and Lu, Yang and Wang, Hanzi},
  doi          = {10.1007/s11263-024-02276-4},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2176-2196},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive middle modality alignment learning for visible-infrared person re-identification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking contemporary deep learning techniques for error correction in biometric data. <em>IJCV</em>, <em>133</em>(4), 2158-2175. (<a href='https://doi.org/10.1007/s11263-024-02280-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of cryptography, the implementation of error correction in biometric data offers many benefits, including secure data storage and key derivation. Deep learning-based decoders have emerged as a catalyst for improved error correction when decoding noisy biometric data. Although these decoders exhibit competence in approximating precise solutions, we expose the potential inadequacy of their security assurances through a minimum entropy analysis. This limitation curtails their applicability in secure biometric contexts, as the inherent complexities of their non-linear neural network architectures pose challenges in modeling the solution distribution precisely. To address this limitation, we introduce U-Sketch, a universal approach for error correction in biometrics, which converts arbitrary input random biometric source distributions into independent and identically distributed (i.i.d.) data while maintaining the pairwise distance of the data post-transformation. This method ensures interpretability within the decoder, facilitating transparent entropy analysis and a substantiated security claim. Moreover, U-Sketch employs Maximum Likelihood Decoding, which provides optimal error tolerance and a precise security guarantee.},
  archive      = {J_IJCV},
  author       = {Lai, YenLung and Dong, XingBo and Jin, Zhe and Jia, Wei and Tistarelli, Massimo and Li, XueJun},
  doi          = {10.1007/s11263-024-02280-8},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2158-2175},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rethinking contemporary deep learning techniques for error correction in biometric data},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Day2Dark: Pseudo-supervised activity recognition beyond silent daylight. <em>IJCV</em>, <em>133</em>(4), 2136-2157. (<a href='https://doi.org/10.1007/s11263-024-02273-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper strives to recognize activities in the dark, as well as in the day. We first establish that state-of-the-art activity recognizers are effective during the day, but not trustworthy in the dark. The main causes are the limited availability of labeled dark videos to learn from, as well as the distribution shift towards the lower color contrast at test-time. To compensate for the lack of labeled dark videos, we introduce a pseudo-supervised learning scheme, which utilizes easy to obtain unlabeled and task-irrelevant dark videos to improve an activity recognizer in low light. As the lower color contrast results in visual information loss, we further propose to incorporate the complementary activity information within audio, which is invariant to illumination. Since the usefulness of audio and visual features differs depending on the amount of illumination, we introduce our ‘darkness-adaptive’ audio-visual recognizer. Experiments on EPIC-Kitchens, Kinetics-Sound, and Charades demonstrate our proposals are superior to image enhancement, domain adaptation and alternative audio-visual fusion methods, and can even improve robustness to local darkness caused by occlusions. Project page: https://xiaobai1217.github.io/Day2Dark/ .},
  archive      = {J_IJCV},
  author       = {Zhang, Yunhua and Doughty, Hazel and Snoek, Cees G. M.},
  doi          = {10.1007/s11263-024-02273-7},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2136-2157},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Day2Dark: Pseudo-supervised activity recognition beyond silent daylight},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EfficientDeRain+: Learning uncertainty-aware filtering via RainMix augmentation for high-efficiency deraining. <em>IJCV</em>, <em>133</em>(4), 2111-2135. (<a href='https://doi.org/10.1007/s11263-024-02281-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deraining is a significant and fundamental computer vision task, aiming to remove the rain streaks and accumulations in an image or video. Existing deraining methods usually make heuristic assumptions of the rain model, which compels them to employ complex optimization or iterative refinement for high recovery quality. However, this leads to time-consuming methods and affects the effectiveness of addressing rain patterns, deviating from the assumptions. This paper proposes a simple yet efficient deraining method by formulating deraining as a predictive filtering problem without complex rain model assumptions. Specifically, we identify spatially-variant predictive filtering (SPFilt) that adaptively predicts proper kernels via a deep network to filter different individual pixels. Since the filtering can be implemented via well-accelerated convolution, our method can be significantly efficient. We further propose the EfDeRain+ that contains three main contributions to address residual rain traces, multi-scale, and diverse rain patterns without harming efficiency. First, we propose the uncertainty-aware cascaded predictive filtering (UC-PFilt) that can identify the difficulties of reconstructing clean pixels via predicted kernels and remove the residual rain traces effectively. Second, we design the weight-sharing multi-scale dilated filtering (WS-MS-DFilt) to handle multi-scale rain streaks without harming the efficiency. Third, to eliminate the gap across diverse rain patterns, we propose a novel data augmentation method (i.e., RainMix) to train our deep models. By combining all contributions with sophisticated analysis on different variants, our final method outperforms baseline methods on six single-image deraining datasets and one video-deraining dataset in terms of both recovery quality and speed. In particular, EfDeRain+ can derain within about 6.3 ms on a $$481\times 321$$ image and is over 74 times faster than the top baseline method with even better recovery quality. We release code in https://github.com/tsingqguo/efficientderainplus .},
  archive      = {J_IJCV},
  author       = {Guo, Qing and Qi, Hua and Sun, Jingyang and Juefei-Xu, Felix and Ma, Lei and Lin, Di and Feng, Wei and Wang, Song},
  doi          = {10.1007/s11263-024-02281-7},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2111-2135},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {EfficientDeRain+: Learning uncertainty-aware filtering via RainMix augmentation for high-efficiency deraining},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few annotated pixels and point cloud based weakly supervised semantic segmentation of driving scenes. <em>IJCV</em>, <em>133</em>(4), 2096-2110. (<a href='https://doi.org/10.1007/s11263-024-02275-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous weakly supervised semantic segmentation (WSSS) methods mainly begin with the segmentation seeds from the CAM method. Because of the high complexity of driving scene images, their framework performs not well on driving scene datasets. In this paper, we propose a new kind of WSSS annotations on the complex driving scene dataset, with only one or several labeled points per category. This annotation is more lightweight than image-level annotation and provides critical localization information for prototypes. We propose a framework to address the WSSS task under this annotation, which generates prototype feature vectors from labeled points and then produces 2D pseudo labels. Besides, we found the point cloud data is useful for distinguishing different objects. Our framework could extract rich semantic information from unlabeled point cloud data and generate instance masks, which does not require extra annotation resources. We combine the pseudo labels and the instance masks to modify the incorrect regions and thus obtain more accurate supervision for training the semantic segmentation network. We evaluated this framework on the KITTI dataset. Experiments show that the proposed method achieves state-of-the-art performance.},
  archive      = {J_IJCV},
  author       = {Ma, Huimin and Yi, Sheng and Chen, Shijie and Chen, Jiansheng and Wang, Yu},
  doi          = {10.1007/s11263-024-02275-5},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2096-2110},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Few annotated pixels and point cloud based weakly supervised semantic segmentation of driving scenes},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving procedure-aware instructional video correlation learning under weak supervision from a collaborative perspective. <em>IJCV</em>, <em>133</em>(4), 2070-2095. (<a href='https://doi.org/10.1007/s11263-024-02272-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Correlation Learning (VCL) delineates a high-level research domain that centers on analyzing the semantic and temporal correspondences between videos through a comparative paradigm. Recently, instructional video-related tasks have drawn increasing attention due to their promising potential. Compared with general videos, instructional videos possess more complex procedure information, making correlation learning quite challenging. To obtain procedural knowledge, current methods rely heavily on fine-grained step-level annotations, which are costly and non-scalable. To improve VCL on instructional videos, we introduce a weakly supervised framework named Collaborative Procedure Alignment (CPA). To be specific, our framework comprises two core components: the collaborative step mining (CSM) module and the frame-to-step alignment (FSA) module. Free of the necessity for step-level annotations, the CSM module can properly conduct temporal step segmentation and pseudo-step learning by exploring the inner procedure correspondences between paired videos. Subsequently, the FSA module efficiently yields the probability of aligning one video’s frame-level features with another video’s pseudo-step labels, which can act as a reliable correlation degree for paired videos. The two modules are inherently interconnected and can mutually enhance each other to extract the step-level knowledge and measure the video correlation distances accurately. Our framework provides an effective tool for instructional video correlation learning. We instantiate our framework on four representative tasks, including sequence verification, few-shot action recognition, temporal action segmentation, and action quality assessment. Furthermore, we extend our framework to more innovative functions to further exhibit its potential. Extensive and in-depth experiments validate CPA’s strong correlation learning capability on instructional videos. The implementation can be found at https://github.com/hotelll/Collaborative_Procedure_Alignment .},
  archive      = {J_IJCV},
  author       = {He, Tianyao and Liu, Huabin and Ni, Zelin and Li, Yuxi and Ma, Xiao and Zhong, Cheng and Zhang, Yang and Wang, Yingxue and Lin, Weiyao},
  doi          = {10.1007/s11263-024-02272-8},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2070-2095},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Achieving procedure-aware instructional video correlation learning under weak supervision from a collaborative perspective},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). APPTracker+: Displacement uncertainty for occlusion handling in low-frame-rate multiple object tracking. <em>IJCV</em>, <em>133</em>(4), 2044-2069. (<a href='https://doi.org/10.1007/s11263-024-02237-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) in the scenario of low-frame-rate videos is a promising solution to better meet the computing, storage, and transmitting bandwidth resource constraints of edge devices. Tracking with a low frame rate poses particular challenges in the association stage as objects in two successive frames typically exhibit much quicker variations in locations, velocities, appearances, and visibilities than those in normal frame rates. In this paper, we observe severe performance degeneration of many existing association strategies caused by such variations. Though optical-flow-based methods like CenterTrack can handle the large displacement to some extent due to their large receptive field, the temporally local nature makes them fail to give reliable displacement estimations of objects that newly appear in the current frame (i.e., not visible in the previous frame). To overcome the local nature of optical-flow-based methods, we propose an online tracking method by extending the CenterTrack architecture with a new head, named APP, to recognize unreliable displacement estimations. Further, to capture the fine-grained and private unreliability of each displacement estimation, we extend the binary APP predictions to displacement uncertainties. To this end, we reformulate the displacement estimation task via Bayesian deep learning tools. With APP predictions, we propose to conduct association in a multi-stage manner where vision cues or historical motion cues are leveraged in the corresponding stage. By rethinking the commonly used bipartite matching algorithms, we equip the proposed multi-stage association policy with a hybrid matching strategy conditioned on displacement uncertainties. Our method shows robustness in preserving identities in low-frame-rate video sequences. Experimental results on public datasets in various low-frame-rate settings demonstrate the advantages of the proposed method.},
  archive      = {J_IJCV},
  author       = {Zhou, Tao and Ye, Qi and Luo, Wenhan and Ran, Haizhou and Shi, Zhiguo and Chen, Jiming},
  doi          = {10.1007/s11263-024-02237-x},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2044-2069},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {APPTracker+: Displacement uncertainty for occlusion handling in low-frame-rate multiple object tracking},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anti-fake vaccine: Safeguarding privacy against face swapping via visual-semantic dual degradation. <em>IJCV</em>, <em>133</em>(4), 2025-2043. (<a href='https://doi.org/10.1007/s11263-024-02259-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake techniques pose a significant threat to personal privacy and social security. To mitigate these risks, various defensive techniques have been introduced, including passive methods through fake detection and proactive methods through adding invisible perturbations. Recent proactive methods mainly focus on face manipulation but perform poorly against face swapping, as face swapping involves the more complex process of identity information transfer. To address this issue, we develop a novel privacy-preserving framework, named Anti-Fake Vaccine, to protect the facial images against the malicious face swapping. This new proactive technique dynamically fuses visual corruption and content misdirection, significantly enhancing protection performance. Specifically, we first formulate constraints from two distinct perspectives: visual quality and identity semantics. The visual perceptual constraint targets image quality degradation in the visual space, while the identity similarity constraint induces erroneous alterations in the semantic space. We then introduce a multi-objective optimization solution to effectively balance the allocation of adversarial perturbations generated according to these constraints. To further improving performance, we develop an additive perturbation strategy to discover the shared adversarial perturbations across diverse face swapping models. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that our method exhibits superior generalization capabilities across diverse face swapping models, including commercial ones.},
  archive      = {J_IJCV},
  author       = {Li, Jingzhi and Luo, Changjiang and Zhang, Hua and Cao, Yang and Liao, Xin and Cao, Xiaochun},
  doi          = {10.1007/s11263-024-02259-5},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {2025-2043},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Anti-fake vaccine: Safeguarding privacy against face swapping via visual-semantic dual degradation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Basis restricted elastic shape analysis on the space of unregistered surfaces. <em>IJCV</em>, <em>133</em>(4), 1999-2024. (<a href='https://doi.org/10.1007/s11263-024-02269-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new framework for surface analysis derived from the general setting of elastic Riemannian metrics on shape spaces. Traditionally, those metrics are defined over the infinite dimensional manifold of immersed surfaces and satisfy specific invariance properties enabling the comparison of surfaces modulo shape preserving transformations such as reparametrizations. The specificity of our approach is to restrict the space of allowable transformations to predefined finite dimensional bases of deformation fields. These are estimated in a data-driven way so as to emulate specific types of surface transformations. This allows us to simplify the representation of the corresponding shape space to a finite dimensional latent space. However, in sharp contrast with methods involving e.g. mesh autoencoders, the latent space is equipped with a non-Euclidean Riemannian metric inherited from the family of elastic metrics. We demonstrate how this model can be effectively implemented to perform a variety of tasks on surface meshes which, importantly, does not assume these to be pre-registered or to even have a consistent mesh structure. We specifically validate our approach on human body shape and pose data as well as human face and hand scans for problems such as shape registration, interpolation, motion transfer or random pose generation.},
  archive      = {J_IJCV},
  author       = {Hartman, Emmanuel and Pierson, Emery and Bauer, Martin and Daoudi, Mohamed and Charon, Nicolas},
  doi          = {10.1007/s11263-024-02269-3},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1999-2024},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Basis restricted elastic shape analysis on the space of unregistered surfaces},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving 3D finger traits recognition via generalizable neural rendering. <em>IJCV</em>, <em>133</em>(4), 1964-1998. (<a href='https://doi.org/10.1007/s11263-024-02248-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D biometric techniques on finger traits have become a new trend and have demonstrated a powerful ability for recognition and anti-counterfeiting. Existing methods follow an explicit 3D pipeline that reconstructs the models first and then extracts features from 3D models. However, these explicit 3D methods suffer from the following problems: 1) Inevitable information dropping during 3D reconstruction; 2) Tight coupling between specific hardware and algorithm for 3D reconstruction. It leads us to a question: Is it indispensable to reconstruct 3D information explicitly in recognition tasks? Hence, we consider this problem in an implicit manner, leaving the nerve-wracking 3D reconstruction problem for learnable neural networks with the help of neural radiance fields (NeRFs). We propose FingerNeRF, a novel generalizable NeRF for 3D finger biometrics. To handle the shape-radiance ambiguity problem that may result in incorrect 3D geometry, we aim to involve extra geometric priors based on the correspondence of binary finger traits like fingerprints or finger veins. First, we propose a novel Trait Guided Transformer (TGT) module to enhance the feature correspondence with the guidance of finger traits. Second, we involve extra geometric constraints on the volume rendering loss with the proposed Depth Distillation Loss and Trait Guided Rendering Loss. To evaluate the performance of the proposed method on different modalities, we collect two new datasets: SCUT-Finger-3D with finger images and SCUT-FingerVein-3D with finger vein images. Moreover, we also utilize the UNSW-3D dataset with fingerprint images for evaluation. In experiments, our FingerNeRF can achieve 4.37% EER on SCUT-Finger-3D dataset, 8.12% EER on SCUT-FingerVein-3D dataset, and 2.90% EER on UNSW-3D dataset, showing the superiority of the proposed implicit method in 3D finger biometrics.},
  archive      = {J_IJCV},
  author       = {Xu, Hongbin and Huang, Junduan and Ma, Yuer and Li, Zifeng and Kang, Wenxiong},
  doi          = {10.1007/s11263-024-02248-8},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1964-1998},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Improving 3D finger traits recognition via generalizable neural rendering},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A memory-assisted knowledge transferring framework with curriculum anticipation for weakly supervised online activity detection. <em>IJCV</em>, <em>133</em>(4), 1940-1963. (<a href='https://doi.org/10.1007/s11263-024-02279-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial topic of high-level video understanding, weakly supervised online activity detection (WS-OAD) involves identifying the ongoing behaviors moment-to-moment in streaming videos, trained with solely cheap video-level annotations. It is essentially a challenging task, which requires addressing the entangled issues of the weakly supervised settings and online constraints. In this paper, we tackle the WS-OAD task from the knowledge-distillation (KD) perspective, which trains an online student detector to distill dual-level knowledge from a weakly supervised offline teacher model. To guarantee the completeness of knowledge transfer, we improve the vanilla KD framework from two aspects. First, we introduce an external memory bank to maintain the long-term activity prototypes, which serves as a bridge to align the activity semantics learned from the offline teacher and online student models. Second, to compensate the missing contexts of unseen near future, we leverage a curriculum learning paradigm to gradually train the online student detector to anticipate the future activity semantics. By dynamically scheduling the provided auxiliary future states, the online detector progressively distills contextual information from the offline model in an easy-to-hard course. Extensive experimental results on three public data sets demonstrate the superiority of our proposed method over the competing methods.},
  archive      = {J_IJCV},
  author       = {Liu, Tianshan and Lam, Kin-Man and Bao, Bing-Kun},
  doi          = {10.1007/s11263-024-02279-1},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1940-1963},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A memory-assisted knowledge transferring framework with curriculum anticipation for weakly supervised online activity detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic attention vision-language transformer network for person re-identification. <em>IJCV</em>, <em>133</em>(4), 1927-1939. (<a href='https://doi.org/10.1007/s11263-024-02277-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal based person re-identification (ReID) has garnered increasing attention in recent years. However, the integration of visual and textual information encounters significant challenges. Biases in feature integration are frequently observed in existing methods, resulting in suboptimal performance and restricted generalization across a spectrum of ReID tasks. At the same time, since there is a domain gap between the datasets used by the pretraining model and the ReID datasets, it has a certain impact on the performance. In response to these challenges, we proposed a dynamic attention vision-language transformer network for the ReID task. In this network, a novel image-text dynamic attention module (ITDA) is designed to promote unbiased feature integration by dynamically assigning the importance of image and text representations. Additionally, an adapter module is adopted to address the domain gap between pretraining datasets and ReID datasets. Our network can capture complex connections between visual and textual information and achieve satisfactory performance. We conducted numerous experiments on ReID benchmarks to demonstrate the efficacy of our proposed method. The experimental results show that our method achieves state-of-the-art performance, surpassing existing integration strategies. These findings underscore the critical role of unbiased feature dynamic integration in enhancing the capabilities of multimodal based ReID models.},
  archive      = {J_IJCV},
  author       = {Zhang, Guifang and Tan, Shijun and Ji, Zhe and Fang, Yuming},
  doi          = {10.1007/s11263-024-02277-3},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1927-1939},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dynamic attention vision-language transformer network for person re-identification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample correlation for fingerprinting deep face recognition. <em>IJCV</em>, <em>133</em>(4), 1912-1926. (<a href='https://doi.org/10.1007/s11263-024-02254-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has witnessed remarkable advancements in recent years, thanks to the development of deep learning techniques. However, an off-the-shelf face recognition model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting, as a model stealing detection method, aims to verify whether a suspect model is stolen from the victim model, gaining more and more attention nowadays. Previous methods always utilize transferable adversarial examples as the model fingerprint, but this method is known to be sensitive to adversarial defense and transfer learning techniques. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-JC that selects JPEG compressed samples as model inputs and calculates the correlation matrix among their model outputs. Extensive results validate that SAC successfully defends against various model stealing attacks in deep face recognition, encompassing face verification and face emotion recognition, exhibiting the highest performance in terms of AUC, p-value and F1 score. Furthermore, we extend our evaluation of SAC-JC to object recognition datasets including Tiny-ImageNet and CIFAR10, which also demonstrates the superior performance of SAC-JC to previous methods. The code will be available at https://github.com/guanjiyang/SAC_JC .},
  archive      = {J_IJCV},
  author       = {Guan, Jiyang and Liang, Jian and Wang, Yanbo and He, Ran},
  doi          = {10.1007/s11263-024-02254-w},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1912-1926},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Sample correlation for fingerprinting deep face recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StyleAdapter: A unified stylized image generation model. <em>IJCV</em>, <em>133</em>(4), 1894-1911. (<a href='https://doi.org/10.1007/s11263-024-02253-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on generating high-quality images with specific style of reference images and content of provided textual descriptions. Current leading algorithms, i.e., DreamBooth and LoRA, require fine-tuning for each style, leading to time-consuming and computationally expensive processes. In this work, we propose StyleAdapter, a unified stylized image generation model capable of producing a variety of stylized images that match both the content of a given prompt and the style of reference images, without the need for per-style fine-tuning. It introduces a two-path cross-attention (TPCA) module to separately process style information and textual prompt, which cooperate with a semantic suppressing vision model (SSVM) to suppress the semantic content of style images. In this way, it can ensure that the prompt maintains control over the content of the generated images, while also mitigating the negative impact of semantic information in style references. This results in the content of the generated image adhering to the prompt, and its style aligning with the style references. Besides, our StyleAdapter can be integrated with existing controllable synthesis methods, such as T2I-adapter and ControlNet, to attain a more controllable and stable generation process. Extensive experiments demonstrate the superiority of our method over previous works.},
  archive      = {J_IJCV},
  author       = {Wang, Zhouxia and Wang, Xintao and Xie, Liangbin and Qi, Zhongang and Shan, Ying and Wang, Wenping and Luo, Ping},
  doi          = {10.1007/s11263-024-02253-x},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1894-1911},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {StyleAdapter: A unified stylized image generation model},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Show-1: Marrying pixel and latent diffusion models for text-to-video generation. <em>IJCV</em>, <em>133</em>(4), 1879-1893. (<a href='https://doi.org/10.1007/s11263-024-02271-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution, which can also remove potential artifacts and corruptions from low-resolution videos. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15 G vs. 72 G). Furthermore, our Show-1 model can be readily adapted for motion customization and video stylization applications through simple temporal attention layer finetuning. Our model achieves state-of-the-art performance on standard video generation benchmarks. Code of Show-1 is publicly available and more videos can be found here .},
  archive      = {J_IJCV},
  author       = {Zhang, David Junhao and Wu, Jay Zhangjie and Liu, Jia-Wei and Zhao, Rui and Ran, Lingmin and Gu, Yuchao and Gao, Difei and Shou, Mike Zheng},
  doi          = {10.1007/s11263-024-02271-9},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1879-1893},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Show-1: Marrying pixel and latent diffusion models for text-to-video generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural vector fields for implicit surface representation and inference. <em>IJCV</em>, <em>133</em>(4), 1855-1878. (<a href='https://doi.org/10.1007/s11263-024-02251-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural implicit fields have recently shown increasing success in representing, learning and analysis of 3D shapes. Signed distance fields and occupancy fields are still the preferred choice of implicit representations with well-studied properties, despite their restriction to closed surfaces. With neural networks, unsigned distance fields as well as several other variations and training principles have been proposed with the goal to represent all classes of shapes. In this paper, we develop a novel and yet a fundamental representation considering unit vectors in 3D space and call it Vector Field (VF). At each point in $$\mathbb {R}^3$$ , VF is directed to the closest point on the surface. We theoretically demonstrate that VF can be easily transformed to surface density by computing the flux density. Unlike other standard representations, VF directly encodes an important physical property of the surface, its normal. We further show the advantages of VF representation, in learning open, closed, or multi-layered surfaces. We show that, thanks to the continuity property of the neural optimization with VF, a separate distance field becomes unnecessary for extracting surfaces from the implicit field via Marching Cubes. We compare our method on several datasets including ShapeNet where the proposed new neural implicit field shows superior accuracy in representing any type of shape, outperforming other standard methods. Codes are available at https://github.com/edomel/ImplicitVF .},
  archive      = {J_IJCV},
  author       = {Mello Rella, Edoardo and Chhatkuli, Ajad and Konukoglu, Ender and Van Gool, Luc},
  doi          = {10.1007/s11263-024-02251-z},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1855-1878},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Neural vector fields for implicit surface representation and inference},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning text-to-video retrieval from image captioning. <em>IJCV</em>, <em>133</em>(4), 1834-1854. (<a href='https://doi.org/10.1007/s11263-024-02202-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a protocol to study text-to-video retrieval training with unlabeled videos, where we assume (i) no access to labels for any videos, i.e., no access to the set of ground-truth captions, but (ii) access to labeled images in the form of text. Using image expert models is a realistic scenario given that annotating images is cheaper therefore scalable, in contrast to expensive video labeling schemes. Recently, zero-shot image experts such as CLIP have established a new strong baseline for video understanding tasks. In this paper, we make use of this progress and instantiate the image experts from two types of models: a text-to-image retrieval model to provide an initial backbone, and image captioning models to provide supervision signal into unlabeled videos. We show that automatically labeling video frames with image captioning allows text-to-video retrieval training. This process adapts the features to the target domain at no manual annotation cost, consequently outperforming the strong zero-shot CLIP baseline. During training, we sample captions from multiple video frames that best match the visual content, and perform a temporal pooling over frame representations by scoring frames according to their relevance to each caption. We conduct extensive ablations to provide insights and demonstrate the effectiveness of this simple framework by outperforming the CLIP zero-shot baselines on text-to-video retrieval on three standard datasets, namely ActivityNet, MSR-VTT, and MSVD. Code and models will be made publicly available.},
  archive      = {J_IJCV},
  author       = {Ventura, Lucas and Schmid, Cordelia and Varol, Gül},
  doi          = {10.1007/s11263-024-02202-8},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1834-1854},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning text-to-video retrieval from image captioning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CogCartoon: Towards practical story visualization. <em>IJCV</em>, <em>133</em>(4), 1808-1833. (<a href='https://doi.org/10.1007/s11263-024-02267-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art methods for story visualization demonstrate a significant demand for training data and storage, as well as limited flexibility in story presentation, thereby rendering them impractical for real-world applications. We introduce CogCartoon, a practical story visualization method based on pre-trained diffusion models. To alleviate dependence on data and storage, we propose an innovative strategy of character-plugin generation that can represent a specific character as a compact 316 KB plugin by using a few training samples. To facilitate enhanced flexibility, we employ a strategy of plugin-guided and layout-guided inference, enabling users to seamlessly incorporate new characters and custom layouts into the generated image results at their convenience. We have conducted comprehensive qualitative and quantitative studies, providing compelling evidence for the superiority of CogCartoon over existing methodologies. Moreover, CogCartoon demonstrates its power in tackling challenging tasks, including long story visualization and realistic style story visualization.},
  archive      = {J_IJCV},
  author       = {Zhu, Zhongyang and Tang, Jie},
  doi          = {10.1007/s11263-024-02267-5},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1808-1833},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {CogCartoon: Towards practical story visualization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AgMTR: Agent mining transformer for few-shot segmentation in remote sensing. <em>IJCV</em>, <em>133</em>(4), 1780-1807. (<a href='https://doi.org/10.1007/s11263-024-02252-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Segmentation aims to segment the interested objects in the query image with just a handful of labeled samples (i.e., support images). Previous schemes would leverage the similarity between support-query pixel pairs to construct the pixel-level semantic correlation. However, in remote sensing scenarios with extreme intra-class variations and cluttered backgrounds, such pixel-level correlations may produce tremendous mismatches, resulting in semantic ambiguity between the query foreground (FG) and background (BG) pixels. To tackle this problem, we propose a novel Agent Mining Transformer, which adaptively mines a set of local-aware agents to construct agent-level semantic correlation. Compared with pixel-level semantics, the given agents are equipped with local-contextual information and possess a broader receptive field. At this point, different query pixels can selectively aggregate the fine-grained local semantics of different agents, thereby enhancing the semantic clarity between query FG and BG pixels. Concretely, the Agent Learning Encoder is first proposed to erect the optimal transport plan that arranges different agents to aggregate support semantics under different local regions. Then, for further optimizing the agents, the Agent Aggregation Decoder and the Semantic Alignment Decoder are constructed to break through the limited support set for mining valuable class-specific semantics from unlabeled data sources and the query image itself, respectively. Extensive experiments on the remote sensing benchmark iSAID indicate that the proposed method achieves state-of-the-art performance. Surprisingly, our method remains quite competitive when extended to more common natural scenarios, i.e., PASCAL- $$5^i$$ and COCO- $$20^{i}$$ .},
  archive      = {J_IJCV},
  author       = {Bi, Hanbo and Feng, Yingchao and Mao, Yongqiang and Pei, Jianning and Diao, Wenhui and Wang, Hongqi and Sun, Xian},
  doi          = {10.1007/s11263-024-02252-y},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1780-1807},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AgMTR: Agent mining transformer for few-shot segmentation in remote sensing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interweaving insights: High-order feature interaction for fine-grained visual recognition. <em>IJCV</em>, <em>133</em>(4), 1755-1779. (<a href='https://doi.org/10.1007/s11263-024-02260-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach for Fine-Grained Visual Classification (FGVC) by exploring Graph Neural Networks (GNNs) to facilitate high-order feature interactions, with a specific focus on constructing both inter- and intra-region graphs. Unlike previous FGVC techniques that often isolate global and local features, our method combines both features seamlessly during learning via graphs. Inter-region graphs capture long-range dependencies to recognize global patterns, while intra-region graphs delve into finer details within specific regions of an object by exploring high-dimensional convolutional features. A key innovation is the use of shared GNNs with an attention mechanism coupled with the Approximate Personalized Propagation of Neural Predictions (APPNP) message-passing algorithm, enhancing information propagation efficiency for better discriminability and simplifying the model architecture for computational efficiency. Additionally, the introduction of residual connections improves performance and training stability. Comprehensive experiments showcase state-of-the-art results on benchmark FGVC datasets, affirming the efficacy of our approach. This work underscores the potential of GNN in modeling high-level feature interactions, distinguishing it from previous FGVC methods that typically focus on singular aspects of feature representation. Our source code is available at https://github.com/Arindam-1991/I2-HOFI .},
  archive      = {J_IJCV},
  author       = {Sikdar, Arindam and Liu, Yonghuai and Kedarisetty, Siddhardha and Zhao, Yitian and Ahmed, Amr and Behera, Ardhendu},
  doi          = {10.1007/s11263-024-02260-y},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1755-1779},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Interweaving insights: High-order feature interaction for fine-grained visual recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the generalization and causal explanation in self-supervised learning. <em>IJCV</em>, <em>133</em>(4), 1727-1754. (<a href='https://doi.org/10.1007/s11263-024-02263-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) methods learn from unlabeled data and achieve high generalization performance on downstream tasks. However, they may also suffer from overfitting to their training data and lose the ability to adapt to new tasks. To investigate this phenomenon, we conduct experiments on various SSL methods and datasets and make two observations: (1) Overfitting occurs abruptly in later layers and epochs, while generalizing features are learned in early layers for all epochs; (2) Coding rate reduction can be used as an indicator to measure the degree of overfitting in SSL models. Based on these observations, we propose Undoing Memorization Mechanism (UMM), a plug-and-play method that mitigates overfitting of the pre-trained feature extractor by aligning the feature distributions of the early and the last layers to maximize the coding rate reduction of the last layer output. The learning process of UMM is a bi-level optimization process. We provide a causal analysis of UMM to explain how UMM can help the pre-trained feature extractor overcome overfitting and recover generalization. We also demonstrate that UMM significantly improves the generalization performance of SSL methods on various downstream tasks. The source code is to be released at https://github.com/ZeenSong/UMM .},
  archive      = {J_IJCV},
  author       = {Qiang, Wenwen and Song, Zeen and Gu, Ziyin and Li, Jiangmeng and Zheng, Changwen and Sun, Fuchun and Xiong, Hui},
  doi          = {10.1007/s11263-024-02263-9},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1727-1754},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {On the generalization and causal explanation in self-supervised learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial action unit detection by adaptively constraining self-attention and causally deconfounding sample. <em>IJCV</em>, <em>133</em>(4), 1711-1726. (<a href='https://doi.org/10.1007/s11263-024-02258-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial action unit (AU) detection remains a challenging task, due to the subtlety, dynamics, and diversity of AUs. Recently, the prevailing techniques of self-attention and causal inference have been introduced to AU detection. However, most existing methods directly learn self-attention guided by AU detection, or employ common patterns for all AUs during causal intervention. The former often captures irrelevant information in a global range, and the latter ignores the specific causal characteristic of each AU. In this paper, we propose a novel AU detection framework called $$\textrm{AC}^{2}$$ D by adaptively constraining self-attention weight distribution and causally deconfounding the sample confounder. Specifically, we explore the mechanism of self-attention weight distribution, in which the self-attention weight distribution of each AU is regarded as spatial distribution and is adaptively learned under the constraint of location-predefined attention and the guidance of AU detection. Moreover, we propose a causal intervention module for each AU, in which the bias caused by training samples and the interference from irrelevant AUs are both suppressed. Extensive experiments show that our method achieves competitive performance compared to state-of-the-art AU detection approaches on challenging benchmarks, including BP4D, DISFA, GFT, and BP4D+ in constrained scenarios and Aff-Wild2 in unconstrained scenarios.},
  archive      = {J_IJCV},
  author       = {Shao, Zhiwen and Zhu, Hancheng and Zhou, Yong and Xiang, Xiang and Liu, Bing and Yao, Rui and Ma, Lizhuang},
  doi          = {10.1007/s11263-024-02258-6},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1711-1726},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Facial action unit detection by adaptively constraining self-attention and causally deconfounding sample},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards data-centric face anti-spoofing: Improving cross-domain generalization via physics-based data synthesis. <em>IJCV</em>, <em>133</em>(4), 1689-1710. (<a href='https://doi.org/10.1007/s11263-024-02240-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face Anti-Spoofing (FAS) research is challenged by the cross-domain problem, where there is a domain gap between the training and testing data. While recent FAS works are mainly model-centric, focusing on developing domain generalization algorithms for improving cross-domain performance, data-centric research for face anti-spoofing, improving generalization from data quality and quantity, is largely ignored. Therefore, our work starts with data-centric FAS by conducting a comprehensive investigation from the data perspective for improving cross-domain generalization of FAS models. More specifically, at first, based on physical procedures of capturing and recapturing, we propose task-specific FAS data augmentation (FAS-Aug), which increases data diversity by synthesizing data of artifacts, such as printing noise, color distortion, moiré pattern, etc. Our experiments show that using our FAS augmentation can surpass traditional image augmentation in training FAS models to achieve better cross-domain performance. Nevertheless, we observe that models may rely on the augmented artifacts, which are not environment-invariant, and using FAS-Aug may have a negative effect. As such, we propose Spoofing Attack Risk Equalization (SARE) to prevent models from relying on certain types of artifacts and improve the generalization performance. Last but not least, our proposed FAS-Aug and SARE with recent Vision Transformer backbones can achieve state-of-the-art performance on the FAS cross-domain generalization protocols. The implementation is available at https://github.com/RizhaoCai/FAS-Aug .},
  archive      = {J_IJCV},
  author       = {Cai, Rizhao and Soh, Cecelia and Yu, Zitong and Li, Haoliang and Yang, Wenhan and Kot, Alex C.},
  doi          = {10.1007/s11263-024-02240-2},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1689-1710},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards data-centric face anti-spoofing: Improving cross-domain generalization via physics-based data synthesis},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind multimodal quality assessment of low-light images. <em>IJCV</em>, <em>133</em>(4), 1665-1688. (<a href='https://doi.org/10.1007/s11263-024-02239-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image quality assessment (BIQA) aims at automatically and accurately forecasting objective scores for visual signals, which has been widely used to monitor product and service quality in low-light applications, covering smartphone photography, video surveillance, autonomous driving, etc. Recent developments in this field are dominated by unimodal solutions inconsistent with human subjective rating patterns, where human visual perception is simultaneously reflected by multiple sensory information. In this article, we present a unique blind multimodal quality assessment (BMQA) of low-light images from subjective evaluation to objective score. To investigate the multimodal mechanism, we first establish a multimodal low-light image quality (MLIQ) database with authentic low-light distortions, containing image-text modality pairs. Further, we specially design the key modules of BMQA, considering multimodal quality representation, latent feature alignment and fusion, and hybrid self-supervised and supervised learning. Extensive experiments show that our BMQA yields state-of-the-art accuracy on the proposed MLIQ benchmark database. In particular, we also build an independent single-image modality Dark-4K database, which is used to verify its applicability and generalization performance in mainstream unimodal applications. Qualitative and quantitative results on Dark-4K show that BMQA achieves superior performance to existing BIQA approaches as long as a pre-trained model is provided to generate text descriptions. The proposed framework and two databases as well as the collected BIQA methods and evaluation metrics are made publicly available on https://charwill.github.io/bmqa.html .},
  archive      = {J_IJCV},
  author       = {Wang, Miaohui and Xu, Zhuowei and Xu, Mai and Lin, Weisi},
  doi          = {10.1007/s11263-024-02239-9},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1665-1688},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Blind multimodal quality assessment of low-light images},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual segmentation with semantics. <em>IJCV</em>, <em>133</em>(4), 1644-1664. (<a href='https://doi.org/10.1007/s11263-024-02261-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new problem called audio-visual segmentation (AVS), in which the goal is to output a pixel-level map of the object(s) that produce sound at the time of the image frame. To facilitate this research, we construct the first audio-visual segmentation benchmark, i.e., AVSBench, providing pixel-wise annotations for sounding objects in audible videos. It contains three subsets: AVSBench-object (Single-source subset, Multi-sources subset) and AVSBench-semantic (Semantic-labels subset). Accordingly, three settings are studied: 1) semi-supervised audio-visual segmentation with a single sound source; 2) fully-supervised audio-visual segmentation with multiple sound sources, and 3) fully-supervised audio-visual semantic segmentation. The first two settings need to generate binary masks of sounding objects indicating pixels corresponding to the audio, while the third setting further requires to generate semantic maps indicating the object category. To deal with these problems, we propose a new baseline method that uses a temporal pixel-wise audio-visual interaction module to inject audio semantics as guidance for the visual segmentation process. We also design a regularization loss to encourage audio-visual mapping during training. Quantitative and qualitative experiments on the AVSBench dataset compare our approach to several existing methods for related tasks, demonstrating that the proposed method is promising for building a bridge between the audio and pixel-wise visual semantics. Code can be found at https://github.com/OpenNLPLab/AVSBench .},
  archive      = {J_IJCV},
  author       = {Zhou, Jinxing and Shen, Xuyang and Wang, Jianyuan and Zhang, Jiayi and Sun, Weixuan and Zhang, Jing and Birchfield, Stan and Guo, Dan and Kong, Lingpeng and Wang, Meng and Zhong, Yiran},
  doi          = {10.1007/s11263-024-02261-x},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1644-1664},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Audio-visual segmentation with semantics},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning accurate low-bit quantization towards efficient computational imaging. <em>IJCV</em>, <em>133</em>(4), 1611-1643. (<a href='https://doi.org/10.1007/s11263-024-02250-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances of deep neural networks (DNNs) promote low-level vision applications in real-world scenarios, e.g., image enhancement, dehazing. Nevertheless, DNN-based methods encounter challenges in terms of high computational and memory requirements, especially when deployed on real-world devices with limited resources. Quantization is one of effective compression techniques that significantly reduces computational and memory requirements by employing low-bit parameters and bit-wise operations. However, low-bit quantization for computational imaging (Q-Imaging) remains largely unexplored and usually suffer from a significant performance drop compared with the real-valued counterparts. In this work, through empirical analysis, we identify the main factor responsible for such significant performance drop underlies in the large gradient estimation error from non-differentiable weight quantization methods, and the activation information degeneration along with the activation quantization. To address these issues, we introduce a differentiable quantization search (DQS) method to learn the quantized weights and an information boosting module (IBM) for network activation quantization. Our DQS method allows us to treat the discrete weights in a quantized neural network as variables that can be searched. We achieve this end by using a differential approach to accurately search for these weights. In specific, each weight is represented as a probability distribution across a set of discrete values. During training, these probabilities are optimized, and the values with the highest probabilities are chosen to construct the desired quantized network. Moreover, our IBM module can rectify the activation distribution before quantization to maximize the self-information entropy, which retains the maximum information during the quantization process. Extensive experiments across a range of image processing tasks, including enhancement, super-resolution, denoising and dehazing, validate the effectiveness of our Q-Imaging along with superior performances compared to a variety of state-of-the-art quantization methods. In particular, the method in Q-Imaging also achieves a strong generalization performance when composing a detection network for the dark object detection task.},
  archive      = {J_IJCV},
  author       = {Xu, Sheng and Li, Yanjing and Liu, Chuanjian and Zhang, Baochang},
  doi          = {10.1007/s11263-024-02250-0},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1611-1643},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning accurate low-bit quantization towards efficient computational imaging},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards ultra high-speed hyperspectral imaging by integrating compressive and neuromorphic sampling. <em>IJCV</em>, <em>133</em>(4), 1587-1610. (<a href='https://doi.org/10.1007/s11263-024-02236-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral and high-speed imaging are both important for scene representation and understanding. However, simultaneously capturing both hyperspectral and high-speed data is still under-explored. In this work, we propose a high-speed hyperspectral imaging system by integrating compressive sensing sampling with bioinspired neuromorphic sampling. Our system includes a coded aperture snapshot spectral imager capturing moderate-speed hyperspectral measurement frames and a spike camera capturing high-speed grayscale dense spike streams. The two cameras provide complementary dual-modality data for reconstructing high-speed hyperspectral videos (HSV). To effectively synergize the two sampling mechanisms and obtain high-quality HSV, we propose a unified multi-modal reconstruction framework. The framework consists of a Spike Spectral Prior Network for spike-based information extraction and prior regularization, coupled with a dual-modality iterative optimization algorithm for reliable reconstruction. We finally build a hardware prototype to verify the effectiveness of our system and algorithm design. Experiments on both simulated and real data demonstrate the superiority of the proposed approach, where for the first time to our knowledge, high-speed HSV with 30 spectral bands can be captured at a frame rate of up to 20,000 FPS.},
  archive      = {J_IJCV},
  author       = {Geng, Mengyue and Wang, Lizhi and Zhu, Lin and Zhang, Wei and Xiong, Ruiqin and Tian, Yonghong},
  doi          = {10.1007/s11263-024-02236-y},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1587-1610},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards ultra high-speed hyperspectral imaging by integrating compressive and neuromorphic sampling},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 4Seasons: Benchmarking visual SLAM and long-term localization for autonomous driving in challenging conditions. <em>IJCV</em>, <em>133</em>(4), 1564-1586. (<a href='https://doi.org/10.1007/s11263-024-02230-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel visual SLAM and long-term localization benchmark for autonomous driving in challenging conditions based on the large-scale 4Seasons dataset. The proposed benchmark provides drastic appearance variations caused by seasonal changes and diverse weather and illumination conditions. While significant progress has been made in advancing visual SLAM on small-scale datasets with similar conditions, there is still a lack of unified benchmarks representative of real-world scenarios for autonomous driving. We introduce a new unified benchmark for jointly evaluating visual odometry, global place recognition, and map-based visual localization performance which is crucial to successfully enable autonomous driving in any condition. The data has been collected for more than one year, resulting in more than 300 km of recordings in nine different environments ranging from a multi-level parking garage to urban (including tunnels) to countryside and highway. We provide globally consistent reference poses with up to centimeter-level accuracy obtained from the fusion of direct stereo-inertial odometry with RTK GNSS. We evaluate the performance of several state-of-the-art visual odometry and visual localization baseline approaches on the benchmark and analyze their properties. The experimental results provide new insights into current approaches and show promising potential for future research. Our benchmark and evaluation protocols will be available at https://go.vision.in.tum.de/4seasons .},
  archive      = {J_IJCV},
  author       = {Wenzel, Patrick and Yang, Nan and Wang, Rui and Zeller, Niclas and Cremers, Daniel},
  doi          = {10.1007/s11263-024-02230-4},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1564-1586},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {4Seasons: Benchmarking visual SLAM and long-term localization for autonomous driving in challenging conditions},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-oriented adversarial attack for deep gait recognition. <em>IJCV</em>, <em>133</em>(4), 1549-1563. (<a href='https://doi.org/10.1007/s11263-024-02225-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition is a non-intrusive method that captures unique walking patterns without subject cooperation, which has emerged as a promising technique across various fields. Recent studies based on Deep Neural Networks (DNNs) have notably improved the performance, however, the potential vulnerability inherent in DNNs and their resistance to interference in practical gait recognition systems remain under-explored. To fill the gap, in this paper, we focus on imperceptible adversarial attack for deep gait recognition and propose an edge-oriented attack strategy tailored for silhouette-based approaches. Specifically, we make a pioneering attempt to explore the intrinsic characteristics of binary silhouettes, with a primary focus on injecting noise perturbations into the edge area. This simple yet effective solution enables sparse attack in both the spatial and temporal dimensions, which largely ensures imperceptibility and simultaneously achieves high success rate. In particular, our solution is built on a unified framework, allowing seamless switching between untargeted and targeted attack modes. Extensive experiments conducted on in-the-lab and in-the-wild benchmarks validate the effectiveness of our attack strategy and emphasize the necessity to study adversarial attack and defense strategy in the near future.},
  archive      = {J_IJCV},
  author       = {Hou, Saihui and Wang, Zengbin and Zhang, Man and Cao, Chunshui and Liu, Xu and Huang, Yongzhen},
  doi          = {10.1007/s11263-024-02225-1},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1549-1563},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Edge-oriented adversarial attack for deep gait recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining generalized multi-timescale inconsistency for detecting deepfake videos. <em>IJCV</em>, <em>133</em>(4), 1532-1548. (<a href='https://doi.org/10.1007/s11263-024-02249-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in face forgery techniques have continuously evolved, leading to emergent security concerns in society. Existing detection methods have poor generalization ability due to the insufficient extraction of dynamic inconsistency cues on the one hand, and their inability to deal well with the gaps between forgery techniques on the other hand. To develop a new generalized framework that emphasizes extracting generalizable multi-timescale inconsistency cues. Firstly, we capture subtle dynamic inconsistency via magnifying the multipath dynamic inconsistency from the local-consecutive short-term temporal view. Secondly, the inter-group graph learning is conducted to establish the sufficient-interactive long-term temporal view for capturing dynamic inconsistency comprehensively. Finally, we design the domain alignment module to directly reduce the distribution gaps via simultaneously disarranging inter- and intra-domain feature distributions for obtaining a more generalized framework. Extensive experiments on six large-scale datasets and the designed generalization evaluation protocols show that our framework outperforms state-of-the-art deepfake video detection methods.},
  archive      = {J_IJCV},
  author       = {Yu, Yang and Ni, Rongrong and Yang, Siyuan and Ni, Yu and Zhao, Yao and Kot, Alex C.},
  doi          = {10.1007/s11263-024-02249-7},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1532-1548},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mining generalized multi-timescale inconsistency for detecting deepfake videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DLRA-net: Deep local residual attention network with contextual refinement for spectral super-resolution. <em>IJCV</em>, <em>133</em>(4), 1499-1531. (<a href='https://doi.org/10.1007/s11263-024-02238-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral Images (HSIs) provide detailed scene insights using extensive spectral bands, crucial for material discrimination and earth observation with substantial costs and low spatial resolution. Recently, Convolutional Neural Networks (CNNs) are common choice for Spectral Super-Resolution (SSR) from Multispectral Images (MSIs). However, they often fail to simultaneously exploit pixel-level noise degradation of MSIs and complex contextual spatial-spectral characteristics of HSIs. In this paper, a Deep Local Residual Attention Network with Contextual Refinement Network (DLRA-Net) is proposed to integrate local low-rank spectral and global contextual priors for improved SSR. Specifically, SSR is unfolded into Contextual-attention Refinement Module (CRM) and Dual Local Residual Attention Module (DLRAM). CRM is proposed to adaptively learn complex contextual priors to guide the convolution layer weights for improved spatial restorations. While DLRAM captures deep refined texture details to enhance contextual priors representations for recovering HSIs. Moreover, lateral fusion strategy is designed to integrate the obtained priors among DLRAMs for faster network convergence. Experimental results on natural-scene datasets with practical noise patterns confirm exceptional DLRA-Net performance with relatively small model size. DLRA-Net demonstrates Maximum Relative Improvements (MRI) between 9.71 and 58.58% in Mean Relative Absolute Error (MRAE) with reduced parameters between 52.18 and 85.85%. Besides, a practical RS-HSI dataset is generated for evaluations showing MRI between 8.64 and 50.56% in MRAE. Furthermore, experiments with HSI classifiers indicate improved performance of reconstructed RS-HSIs compared to RS-MSIs, with MRI in Overall Accuracy (OA) between 7.10 and 15.27%. Lastly, a detailed ablation study assesses model complexity and runtime.},
  archive      = {J_IJCV},
  author       = {El-gabri, Ahmed R. and Aly, Hussein A. and Ghoniemy, Tarek S. and Elshafey, Mohamed A.},
  doi          = {10.1007/s11263-024-02238-w},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1499-1531},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {DLRA-net: Deep local residual attention network with contextual refinement for spectral super-resolution},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using unreliable pseudo-labels for label-efficient semantic segmentation. <em>IJCV</em>, <em>133</em>(4), 1476-1498. (<a href='https://doi.org/10.1007/s11263-024-02229-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crux of label-efficient semantic segmentation is to produce high-quality pseudo-labels to leverage a large amount of unlabeled or weakly labeled data. A common practice is to select the highly confident predictions as the pseudo-ground-truths for each pixel, but it leads to a problem that most pixels may be left unused due to their unreliability. However, we argue that every pixel matters to the model training, even those unreliable and ambiguous pixels. Intuitively, an unreliable prediction may get confused among the top classes, however, it should be confident about the pixel not belonging to the remaining classes. Hence, such a pixel can be convincingly treated as a negative key to those most unlikely categories. Therefore, we develop an effective pipeline to make sufficient use of unlabeled data. Concretely, we separate reliable and unreliable pixels via the entropy of predictions, push each unreliable pixel to a category-wise queue that consists of negative keys, and manage to train the model with all candidate pixels. Considering the training evolution, we adaptively adjust the threshold for the reliable-unreliable partition. Experimental results on various benchmarks and training settings demonstrate the superiority of our approach over the state-of-the-art alternatives.},
  archive      = {J_IJCV},
  author       = {Wang, Haochen and Wang, Yuchao and Shen, Yujun and Fan, Junsong and Wang, Yuxi and Zhang, Zhaoxiang},
  doi          = {10.1007/s11263-024-02229-x},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1476-1498},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Using unreliable pseudo-labels for label-efficient semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MosaicFusion: Diffusion models as data augmenters for large vocabulary instance segmentation. <em>IJCV</em>, <em>133</em>(4), 1456-1475. (<a href='https://doi.org/10.1007/s11263-024-02223-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchmarks demonstrate that MosaicFusion can significantly improve the performance of existing instance segmentation models, especially for rare and novel categories. Code: https://github.com/Jiahao000/MosaicFusion .},
  archive      = {J_IJCV},
  author       = {Xie, Jiahao and Li, Wei and Li, Xiangtai and Liu, Ziwei and Ong, Yew Soon and Loy, Chen Change},
  doi          = {10.1007/s11263-024-02223-3},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1456-1475},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MosaicFusion: Diffusion models as data augmenters for large vocabulary instance segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group-based distinctive image captioning with memory difference encoding and attention. <em>IJCV</em>, <em>133</em>(4), 1435-1455. (<a href='https://doi.org/10.1007/s11263-024-02220-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in image captioning have focused on enhancing accuracy by substantially increasing the dataset and model size. While conventional captioning models exhibit high performance on established metrics such as BLEU, CIDEr, and SPICE, the capability of captions to distinguish the target image from other similar images is under-explored. To generate distinctive captions, a few pioneers employed contrastive learning or re-weighted the ground-truth captions. However, these approaches often overlook the relationships among objects in a similar image group (e.g., items or properties within the same album or fine-grained events). In this paper, we introduce a novel approach to enhance the distinctiveness of image captions, namely Group-based Differential Distinctive Captioning Method, which visually compares each image with other images in one similar group and highlights the uniqueness of each image. In particular, we introduce a Group-based Differential Memory Attention (GDMA) module, designed to identify and emphasize object features in an image that are uniquely distinguishable within its image group, i.e., those exhibiting low similarity with objects in other images. This mechanism ensures that such unique object features are prioritized during caption generation for the image, thereby enhancing the distinctiveness of the resulting captions. To further refine this process, we select distinctive words from the ground-truth captions to guide both the language decoder and the GDMA module. Additionally, we propose a new evaluation metric, the Distinctive Word Rate (DisWordRate), to quantitatively assess caption distinctiveness. Quantitative results indicate that the proposed method significantly improves the distinctiveness of several baseline models, and achieves state-of-the-art performance on distinctiveness while not excessively sacrificing accuracy. Moreover, the results of our user study are consistent with the quantitative evaluation and demonstrate the rationality of the new metric DisWordRate.},
  archive      = {J_IJCV},
  author       = {Wang, Jiuniu and Xu, Wenjia and Wang, Qingzhong and Chan, Antoni B.},
  doi          = {10.1007/s11263-024-02220-6},
  journal      = {International Journal of Computer Vision},
  month        = {4},
  number       = {4},
  pages        = {1435-1455},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Group-based distinctive image captioning with memory difference encoding and attention},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Variational rectification inference for learning with noisy labels. <em>IJCV</em>, <em>133</em>(3), 1434. (<a href='https://doi.org/10.1007/s11263-024-02242-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Sun, Haoliang and Wei, Qi and Feng, Lei and Hu, Yupeng and Liu, Fan and Fan, Hehe and Yin, Yilong},
  doi          = {10.1007/s11263-024-02242-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1434},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Variational rectification inference for learning with noisy labels},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on computer vision approaches for animal tracking and modeling 2023. <em>IJCV</em>, <em>133</em>(3), 1433. (<a href='https://doi.org/10.1007/s11263-024-02241-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  doi          = {10.1007/s11263-024-02241-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1433},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on computer vision approaches for animal tracking and modeling 2023},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on german conference on pattern recognition (DAGM GCPR). <em>IJCV</em>, <em>133</em>(3), 1432. (<a href='https://doi.org/10.1007/s11263-024-02212-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Goldluecke, Bastian},
  doi          = {10.1007/s11263-024-02212-6},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1432},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Editor’s note: Special issue on german conference on pattern recognition (DAGM GCPR)},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSKNet: A foundation lightweight backbone for remote sensing. <em>IJCV</em>, <em>133</em>(3), 1410-1431. (<a href='https://doi.org/10.1007/s11263-024-02247-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection, semantic segmentation and change detection, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightweight LSKNet backbone network sets new state-of-the-art scores on standard remote sensing classification, object detection, semantic segmentation and change detection benchmarks. Our comprehensive analysis further validated the significance of the identified priors and the effectiveness of LSKNet. The code is available at https://github.com/zcablii/LSKNet .},
  archive      = {J_IJCV},
  author       = {Li, Yuxuan and Li, Xiang and Dai, Yimain and Hou, Qibin and Liu, Li and Liu, Yongxiang and Cheng, Ming-Ming and Yang, Jian},
  doi          = {10.1007/s11263-024-02247-9},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1410-1431},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LSKNet: A foundation lightweight backbone for remote sensing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified frequency-assisted transformer framework for detecting and grounding multi-modal manipulation. <em>IJCV</em>, <em>133</em>(3), 1392-1409. (<a href='https://doi.org/10.1007/s11263-024-02245-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and grounding multi-modal media manipulation ( $$\hbox {DGM}^4$$ ) has become increasingly crucial due to the widespread dissemination of face forgery and text misinformation. In this paper, we present the Unified Frequency-Assisted transFormer framework, named UFAFormer, to address the $$\hbox {DGM}^4$$ problem. Unlike previous state-of-the-art methods that solely focus on the image (RGB) domain to describe visual forgery features, we additionally introduce the frequency domain as a complementary viewpoint. By leveraging the discrete wavelet transform, we decompose images into several frequency sub-bands, capturing rich face forgery artifacts. Then, our proposed frequency encoder, incorporating intra-band and inter-band self-attentions, explicitly aggregates forgery features within and across diverse sub-bands. Moreover, to address the semantic conflicts between image and frequency domains, the forgery-aware mutual module is developed to further enable the effective interaction of disparate image and frequency features, resulting in aligned and comprehensive visual forgery representations. Finally, based on visual and textual forgery features, we propose a unified decoder that comprises two symmetric cross-modal interaction modules responsible for gathering modality-specific forgery information, along with a fusing interaction module for aggregation of both modalities. The proposed unified decoder formulates our UFAFormer as a unified framework, ultimately simplifying the overall architecture and facilitating the optimization process. Experimental results on the $$\hbox {DGM}^4$$ dataset, containing several perturbations, demonstrate the superior performance of our framework compared to previous methods, setting a new benchmark in the field.},
  archive      = {J_IJCV},
  author       = {Liu, Huan and Tan, Zichang and Chen, Qiang and Wei, Yunchao and Zhao, Yao and Wang, Jingdong},
  doi          = {10.1007/s11263-024-02245-x},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1392-1409},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unified frequency-assisted transformer framework for detecting and grounding multi-modal manipulation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-VLGM: Bi-level class-severity-aware vision-language graph matching for text guided medical image segmentation. <em>IJCV</em>, <em>133</em>(3), 1375-1391. (<a href='https://doi.org/10.1007/s11263-024-02246-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical reports containing specific diagnostic results and additional information not present in medical images can be effectively employed to assist image understanding tasks, and the modality gap between vision and language can be bridged by vision-language matching (VLM). However, current vision-language models distort the intra-model relation and only include class information in reports that is insufficient for segmentation task. In this paper, we introduce a novel Bi-level class-severity-aware Vision-Language Graph Matching (Bi-VLGM) for text guided medical image segmentation, composed of a word-level VLGM module and a sentence-level VLGM module, to exploit the class-severity-aware relation among visual-textual features. In word-level VLGM, to mitigate the distorted intra-modal relation during VLM, we reformulate VLM as graph matching problem and introduce a vision-language graph matching (VLGM) to exploit the high-order relation among visual-textual features. Then, we perform VLGM between the local features for each class region and class-aware prompts to bridge their gap. In sentence-level VLGM, to provide disease severity information for segmentation task, we introduce a severity-aware prompting to quantify the severity level of disease lesion, and perform VLGM between the global features and the severity-aware prompts. By exploiting the relation between the local (global) and class (severity) features, the segmentation model can include the class-aware and severity-aware information to promote segmentation performance. Extensive experiments proved the effectiveness of our method and its superiority to existing methods. The source code will be released.},
  archive      = {J_IJCV},
  author       = {Chen, Wenting and Liu, Jie and Liu, Tianming and Yuan, Yixuan},
  doi          = {10.1007/s11263-024-02246-w},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1375-1391},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bi-VLGM: Bi-level class-severity-aware vision-language graph matching for text guided medical image segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MapTRv2: An end-to-end framework for online vectorized HD map construction. <em>IJCV</em>, <em>133</em>(3), 1352-1374. (<a href='https://doi.org/10.1007/s11263-024-02235-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-definition (HD) map provides abundant and precise static environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. In this paper, we present Map TRansformer, an end-to-end framework for online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. To speed up convergence, we further introduce auxiliary one-to-many matching and dense supervision. The proposed method well copes with various map elements with arbitrary shapes. It runs at real-time inference speed and achieves state-of-the-art performance on both nuScenes and Argoverse2 datasets. Abundant qualitative results show stable and robust map construction quality in complex and various driving scenes. Code and more demos are available at https://github.com/hustvl/MapTR for facilitating further studies and applications.},
  archive      = {J_IJCV},
  author       = {Liao, Bencheng and Chen, Shaoyu and Zhang, Yunchi and Jiang, Bo and Zhang, Qian and Liu, Wenyu and Huang, Chang and Wang, Xinggang},
  doi          = {10.1007/s11263-024-02235-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1352-1374},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {MapTRv2: An end-to-end framework for online vectorized HD map construction},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dissecting out-of-distribution detection and open-set recognition: A critical analysis of methods and benchmarks. <em>IJCV</em>, <em>133</em>(3), 1326-1351. (<a href='https://doi.org/10.1007/s11263-024-02222-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting test-time distribution shift has emerged as a key capability for safely deployed machine learning models, with the question being tackled under various guises in recent years. In this paper, we aim to provide a consolidated view of the two largest sub-fields within the community: out-of-distribution (OOD) detection and open-set recognition (OSR). In particular, we aim to provide rigorous empirical analysis of different methods across settings and provide actionable takeaways for practitioners and researchers. Concretely, we make the following contributions: (i) We perform rigorous cross-evaluation between state-of-the-art methods in the OOD detection and OSR settings and identify a strong correlation between the performances of methods for them; (ii) We propose a new, large-scale benchmark setting which we suggest better disentangles the problem tackled by OOD detection and OSR, re-evaluating state-of-the-art OOD detection and OSR methods in this setting; (iii) We surprisingly find that the best performing method on standard benchmarks (Outlier Exposure) struggles when tested at scale, while scoring rules which are sensitive to the deep feature magnitude consistently show promise; and (iv) We conduct empirical analysis to explain these phenomena and highlight directions for future research. Code: https://github.com/Visual-AI/Dissect-OOD-OSR},
  archive      = {J_IJCV},
  author       = {Wang, Hongjun and Vaze, Sagar and Han, Kai},
  doi          = {10.1007/s11263-024-02222-4},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1326-1351},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Dissecting out-of-distribution detection and open-set recognition: A critical analysis of methods and benchmarks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2D semantic-guided semantic scene completion. <em>IJCV</em>, <em>133</em>(3), 1306-1325. (<a href='https://doi.org/10.1007/s11263-024-02244-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic scene completion (SSC) aims to simultaneously perform scene completion (SC) and predict semantic categories of a 3D scene from a single depth and/or RGB image. Most existing SSC methods struggle to handle complex regions with multiple objects close to each other, especially for objects with reflective or dark surfaces. This primarily stems from two challenges: (1) the loss of geometric information due to the unreliability of depth values from sensors, and (2) the potential for semantic confusion when simultaneously predicting 3D shapes and semantic labels. To address these problems, we propose a Semantic-guided Semantic Scene Completion framework, dubbed SG-SSC, which involves Semantic-guided Fusion (SGF) and Volume-guided Semantic Predictor (VGSP). Guided by 2D semantic segmentation maps, SGF adaptively fuses RGB and depth features to compensate for the missing geometric information caused by the missing values in depth images, thus performing more robustly to unreliable depth information. VGSP exploits the mutual benefit between SC and SSC tasks, making SSC more focused on predicting the categories of voxels with high occupancy probabilities and also allowing SC to utilize semantic priors to better predict voxel occupancy. Experimental results show that SG-SSC outperforms existing state-of-the-art methods on the NYU, NYUCAD, and SemanticKITTI datasets. Models and code are available at https://github.com/aipixel/SG-SSC .},
  archive      = {J_IJCV},
  author       = {Liu, Xianzhu and Xie, Haozhe and Zhang, Shengping and Yao, Hongxun and Ji, Rongrong and Nie, Liqiang and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02244-y},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1306-1325},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {2D semantic-guided semantic scene completion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From gaze jitter to domain adaptation: Generalizing gaze estimation by manipulating high-frequency components. <em>IJCV</em>, <em>133</em>(3), 1290-1305. (<a href='https://doi.org/10.1007/s11263-024-02233-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaze, as a pivotal indicator of human emotion, plays a crucial role in various computer vision tasks. However, the accuracy of gaze estimation often significantly deteriorates when applied to unseen environments, thereby limiting its practical value. Therefore, enhancing the generalizability of gaze estimators to new domains emerges as a critical challenge. A common limitation in existing domain adaptation research is the inability to identify and leverage truly influential factors during the adaptation process. This shortcoming often results in issues such as limited accuracy and unstable adaptation. To address this issue, this article discovers a truly influential factor in the cross-domain problem, i.e., high-frequency components (HFC). This discovery stems from an analysis of gaze jitter-a frequently overlooked but impactful issue where predictions can deviate drastically even for visually similar input images. Inspired by this discovery, we propose an “embed-then-suppress" HFC manipulation strategy to adapt gaze estimation to new domains. Our method first embeds additive HFC to the input images, then performs domain adaptation by suppressing the impact of HFC. Specifically, the suppression is carried out in a contrasive manner. Each original image is paired with its HFC-embedded version, thereby enabling our method to suppress the HFC impact through contrasting the representations within the pairs. The proposed method is evaluated across four cross-domain gaze estimation tasks. The experimental results show that it not only enhances gaze estimation accuracy but also significantly reduces gaze jitter in the target domain. Compared with previous studies, our method offers higher accuracy, reduced gaze jitter, and improved adaptation stability, marking the potential for practical deployment.},
  archive      = {J_IJCV},
  author       = {Liu, Ruicong and Wang, Haofei and Lu, Feng},
  doi          = {10.1007/s11263-024-02233-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1290-1305},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {From gaze jitter to domain adaptation: Generalizing gaze estimation by manipulating high-frequency components},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEO: Generative latent image animator for human video synthesis. <em>IJCV</em>, <em>133</em>(3), 1277-1289. (<a href='https://doi.org/10.1007/s11263-024-02231-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing. Project page: https://wyhsirius.github.io/LEO-project/ .},
  archive      = {J_IJCV},
  author       = {Wang, Yaohui and Ma, Xin and Chen, Xinyuan and Chen, Cunjian and Dantcheva, Antitza and Dai, Bo and Qiao, Yu},
  doi          = {10.1007/s11263-024-02231-3},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1277-1289},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LEO: Generative latent image animator for human video synthesis},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mutual prompt leaning for vision language models. <em>IJCV</em>, <em>133</em>(3), 1258-1276. (<a href='https://doi.org/10.1007/s11263-024-02243-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pre-trained vision language models (VLMs) have demonstrated impressive representation learning capabilities, but their transferability across various downstream tasks heavily relies on prompt learning. Since VLMs consist of text and visual sub-branches, existing prompt approaches are mainly divided into text and visual prompts. Recent text prompt methods have achieved great performance by designing input-condition prompts that encompass both text and image domain knowledge. However, roughly incorporating the same image feature into each learnable text token may be unjustifiable, as it could result in learnable text prompts being concentrated on one or a subset of characteristics. In light of this, we propose a fine-grained text prompt (FTP) that decomposes the single global image features into several finer-grained semantics and incorporates them into corresponding text prompt tokens. On the other hand, current methods neglect valuable text semantic information when building the visual prompt. Furthermore, text information contains redundant and negative category semantics. To address this, we propose a text-reorganized visual prompt (TVP) that reorganizes the text descriptions of the current image to construct the visual prompt, guiding the image branch to attend to class-related representations. By leveraging both FTP and TVP, we enable mutual prompting between the text and visual modalities, unleashing their potential to tap into the representation capabilities of VLMs. Extensive experiments on 11 classification benchmarks show that our method surpasses existing methods by a large margin. In particular, our approach improves recent state-of-the-art CoCoOp by 4.79% on new classes and 3.88% on harmonic mean over eleven classification benchmarks.},
  archive      = {J_IJCV},
  author       = {Long, Sifan and Zhao, Zhen and Yuan, Junkun and Tan, Zichang and Liu, Jiangjiang and Feng, Jingyuan and Wang, Shengsheng and Wang, Jingdong},
  doi          = {10.1007/s11263-024-02243-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1258-1276},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Mutual prompt leaning for vision language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust deep object tracking against adversarial attacks. <em>IJCV</em>, <em>133</em>(3), 1238-1257. (<a href='https://doi.org/10.1007/s11263-024-02226-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the vulnerability of deep neural networks (DNNs) has attracted significant attention in recent years. While recent studies on adversarial attack and defense mainly reside in a single image, few efforts have been made to perform temporal attacks against video sequences. As the temporal consistency between frames is not considered, existing adversarial attack approaches designed for static images do not perform well for deep object tracking. In this work, we generate adversarial examples on top of video sequences to improve the tracking robustness against adversarial attacks under white-box and black-box settings. To this end, we consider motion signals when generating lightweight perturbations over the estimated tracking results frame-by-frame. For the white-box attack, we generate temporal perturbations via known trackers to degrade significantly the tracking performance. We transfer the generated perturbations into unknown targeted trackers for the black-box attack to achieve transferring attacks. Furthermore, we train universal adversarial perturbations and directly add them into all frames of videos, improving the attack effectiveness with minor computational costs. On the other hand, we sequentially learn to estimate and remove the perturbations from input sequences to restore the tracking performance. We apply the proposed adversarial attack and defense approaches to state-of-the-art tracking algorithms. Extensive evaluations on large-scale benchmark datasets, including OTB, VOT, UAV123, and LaSOT, demonstrate that our attack method degrades the tracking performance significantly with favorable transferability to other backbones and trackers. Notably, the proposed defense method restores the original tracking performance to some extent and achieves additional performance gains when not under adversarial attacks.},
  archive      = {J_IJCV},
  author       = {Jia, Shuai and Ma, Chao and Song, Yibing and Yang, Xiaokang and Yang, Ming-Hsuan},
  doi          = {10.1007/s11263-024-02226-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1238-1257},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Robust deep object tracking against adversarial attacks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Slimmable networks for contrastive self-supervised learning. <em>IJCV</em>, <em>133</em>(3), 1222-1237. (<a href='https://doi.org/10.1007/s11263-024-02211-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning makes significant progress in pre-training large models, but struggles with small models. Mainstream solutions to this problem rely mainly on knowledge distillation, which involves a two-stage procedure: first training a large teacher model and then distilling it to improve the generalization ability of smaller ones. In this work, we introduce another one-stage solution to obtain pre-trained small models without the need for extra teachers, namely, slimmable networks for contrastive self-supervised learning (SlimCLR). A slimmable network consists of a full network and several weight-sharing sub-networks, which can be pre-trained once to obtain various networks, including small ones with low computation costs. However, interference between weight-sharing networks leads to severe performance degradation in self-supervised cases, as evidenced by gradient magnitude imbalance and gradient direction divergence. The former indicates that a small proportion of parameters produce dominant gradients during backpropagation, while the main parameters may not be fully optimized. The latter shows that the gradient direction is disordered, and the optimization process is unstable. To address these issues, we introduce three techniques to make the main parameters produce dominant gradients and sub-networks have consistent outputs. These techniques include slow start training of sub-networks, online distillation, and loss re-weighting according to model sizes. Furthermore, theoretical results are presented to demonstrate that a single slimmable linear layer is sub-optimal during linear evaluation. Thus a switchable linear probe layer is applied during linear evaluation. We instantiate SlimCLR with typical contrastive learning frameworks and achieve better performance than previous arts with fewer parameters and FLOPs. The code is available at https://github.com/mzhaoshuai/SlimCLR .},
  archive      = {J_IJCV},
  author       = {Zhao, Shuai and Zhu, Linchao and Wang, Xiaohan and Yang, Yi},
  doi          = {10.1007/s11263-024-02211-7},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1222-1237},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Slimmable networks for contrastive self-supervised learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breaking the limits of reliable prediction via generated data. <em>IJCV</em>, <em>133</em>(3), 1195-1221. (<a href='https://doi.org/10.1007/s11263-024-02221-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In open-world recognition of safety-critical applications, providing reliable prediction for deep neural networks has become a critical requirement. Many methods have been proposed for reliable prediction related tasks such as confidence calibration, misclassification detection, and out-of-distribution detection. Recently, pre-training has been shown to be one of the most effective methods for improving reliable prediction, particularly for modern networks like ViT, which require a large amount of training data. However, collecting data manually is time-consuming. In this paper, taking advantage of the breakthrough of generative models, we investigate whether and how expanding the training set using generated data can improve reliable prediction. Our experiments reveal that training with a large quantity of generated data can eliminate overfitting in reliable prediction, leading to significantly improved performance. Surprisingly, classical networks like ResNet-18, when trained on a notably extensive volume of generated data, can sometimes exhibit performance competitive to pre-training ViT with a substantial real dataset.},
  archive      = {J_IJCV},
  author       = {Cheng, Zhen and Zhu, Fei and Zhang, Xu-Yao and Liu, Cheng-Lin},
  doi          = {10.1007/s11263-024-02221-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1195-1221},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Breaking the limits of reliable prediction via generated data},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FastComposer: Tuning-free multi-subject image generation with localized attention. <em>IJCV</em>, <em>133</em>(3), 1175-1194. (<a href='https://doi.org/10.1007/s11263-024-02227-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. We present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300 $$\times $$ –2500 $$\times $$ speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation. Code, model, and dataset are available here ( https://github.com/mit-han-lab/fastcomposer ).},
  archive      = {J_IJCV},
  author       = {Xiao, Guangxuan and Yin, Tianwei and Freeman, William T. and Durand, Frédo and Han, Song},
  doi          = {10.1007/s11263-024-02227-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1175-1194},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FastComposer: Tuning-free multi-subject image generation with localized attention},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lidar panoptic segmentation in an open world. <em>IJCV</em>, <em>133</em>(3), 1153-1174. (<a href='https://doi.org/10.1007/s11263-024-02166-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing Lidar Panoptic Segmentation (LPS) is crucial for safe deployment of autnomous vehicles. LPS aims to recognize and segment lidar points w.r.t. a pre-defined vocabulary of semantic classes, including thing classes of countable objects (e.g., pedestrians and vehicles) and stuff classes of amorphous regions (e.g., vegetation and road). Importantly, LPS requires segmenting individual thing instances (e.g., every single vehicle). Current LPS methods make an unrealistic assumption that the semantic class vocabulary is fixed in the real open world, but in fact, class ontologies usually evolve over time as robots encounter instances of novel classes that are considered to be unknowns w.r.t. thepre-defined class vocabulary. To address this unrealistic assumption, we study LPS in the Open World (LiPSOW): we train models on a dataset with a pre-defined semantic class vocabulary and study their generalization to a larger dataset where novel instances of thing and stuff classes can appear. This experimental setting leads to interesting conclusions. While prior art train class-specific instance segmentation methods and obtain state-of-the-art results on known classes, methods based on class-agnostic bottom-up grouping perform favorably on classes outside of the initial class vocabulary (i.e., unknown classes). Unfortunately, these methods do not perform on-par with fully data-driven methods on known classes. Our work suggests a middle ground: we perform class-agnostic point clustering and over-segment the input cloud in a hierarchical fashion, followed by binary point segment classification, akin to Region Proposal Network (Ren et al. NeurIPS, 2015). We obtain the final point cloud segmentation by computing a cut in the weighted hierarchical tree of point segments, independently of semantic classification. Remarkably, this unified approach leads to strong performance on both known and unknown classes.},
  archive      = {J_IJCV},
  author       = {Chakravarthy, Anirudh S. and Ganesina, Meghana Reddy and Hu, Peiyun and Leal-Taixé, Laura and Kong, Shu and Ramanan, Deva and Osep, Aljosa},
  doi          = {10.1007/s11263-024-02166-9},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1153-1174},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lidar panoptic segmentation in an open world},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical active learning for low-altitude drone-view object detection. <em>IJCV</em>, <em>133</em>(3), 1140-1152. (<a href='https://doi.org/10.1007/s11263-024-02228-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various object detection techniques are employed on drone platforms. However, the task of annotating drone-view samples is both time-consuming and laborious. This is primarily due to the presence of numerous small-sized instances to be labeled in the drone-view image. To tackle this issue, we propose HALD, a hierarchical active learning approach for low-altitude drone-view object detection. HALD extracts unlabeled image information sequentially from different levels, including point, box, image, and class, aiming to obtain a reliable indicator of image information. The point-level module is utilized to ascertain the valid count and location of instances, while the box-level module screens out reliable predictions. The image-level module selects candidate samples by calculating the consistency of valid boxes within an image, and the class-level module selects the final selected samples based on the distribution of candidate and labeled samples across different classes. Extensive experiments conducted on the VisDrone and CityPersons datasets demonstrate that HALD outperforms several other baseline methods. Additionally, we provide an in-depth analysis of each proposed module. The results show that the performance of evaluating the informativeness of samples can be effectively improved by the four hierarchical levels.},
  archive      = {J_IJCV},
  author       = {Hu, Haohao and Han, Tianyu and Wang, Yuerong and Zhong, Wanjun and Yue, Jingwei and Zan, Peng},
  doi          = {10.1007/s11263-024-02228-y},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1140-1152},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Hierarchical active learning for low-altitude drone-view object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In search of lost online test-time adaptation: A survey. <em>IJCV</em>, <em>133</em>(3), 1106-1139. (<a href='https://doi.org/10.1007/s11263-024-02213-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a comprehensive survey of online test-time adaptation (OTTA), focusing on effectively adapting machine learning models to distributionally different target data upon batch arrival. Despite the recent proliferation of OTTA methods, conclusions from previous studies are inconsistent due to ambiguous settings, outdated backbones, and inconsistent hyperparameter tuning, which obscure core challenges and hinder reproducibility. To enhance clarity and enable rigorous comparison, we classify OTTA techniques into three primary categories and benchmark them using a modern backbone, the Vision Transformer. Our benchmarks cover conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C, as well as real-world shifts represented by CIFAR-10.1, OfficeHome, and CIFAR-10-Warehouse. The CIFAR-10-Warehouse dataset includes a variety of variations from different search engines and synthesized data generated through diffusion models. To measure efficiency in online scenarios, we introduce novel evaluation metrics, including GFLOPs, wall clock time, and GPU memory usage, providing a clearer picture of the trade-offs between adaptation accuracy and computational overhead. Our findings diverge from existing literature, revealing that (1) transformers demonstrate heightened resilience to diverse domain shifts, (2) the efficacy of many OTTA methods relies on large batch sizes, and (3) stability in optimization and resistance to perturbations are crucial during adaptation, particularly when the batch size is 1. Based on these insights, we highlight promising directions for future research. Our benchmarking toolkit and source code are available at https://github.com/Jo-wang/OTTA_ViT_survey .},
  archive      = {J_IJCV},
  author       = {Wang, Zixin and Luo, Yadan and Zheng, Liang and Chen, Zhuoxiao and Wang, Sen and Huang, Zi},
  doi          = {10.1007/s11263-024-02213-5},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1106-1139},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {In search of lost online test-time adaptation: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WeakCLIP: Adapting CLIP for weakly-supervised semantic segmentation. <em>IJCV</em>, <em>133</em>(3), 1085-1105. (<a href='https://doi.org/10.1007/s11263-024-02224-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive language and image pre-training (CLIP) achieves great success in various computer vision tasks and also presents an opportune avenue for enhancing weakly-supervised image understanding with its large-scale pre-trained knowledge. As an effective way to reduce the reliance on pixel-level human-annotated labels, weakly-supervised semantic segmentation (WSSS) aims to refine the class activation map (CAM) and produce high-quality pseudo masks. Weakly-supervised semantic segmentation (WSSS) aims to refine the class activation map (CAM) as pseudo masks, but heavily relies on inductive biases like hand-crafted priors and digital image processing methods. For the vision-language pre-trained model, i.e. CLIP, we propose a novel text-to-pixel matching paradigm for WSSS. However, directly applying CLIP to WSSS is challenging due to three critical problems: (1) the task gap between contrastive pre-training and WSSS CAM refinement, (2) lacking text-to-pixel modeling to fully utilize the pre-trained knowledge, and (3) the insufficient details owning to the $$\frac{1}{16}$$ down-sampling resolution of ViT. Thus, we propose WeakCLIP to address the problems and leverage the pre-trained knowledge from CLIP to WSSS. Specifically, we first address the task gap by proposing a pyramid adapter and learnable prompts to extract WSSS-specific representation. We then design a co-attention matching module to model text-to-pixel relationships. Finally, the pyramid adapter and text-guided decoder are introduced to gather multi-level information and integrate it with text guidance hierarchically. WeakCLIP provides an effective and parameter-efficient way to transfer CLIP knowledge to refine CAM. Extensive experiments demonstrate that WeakCLIP achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 74.0% mIoU on the val set of PASCAL VOC 2012 and 46.1% mIoU on the val set of COCO 2014. The source code and model checkpoints are released at https://github.com/hustvl/WeakCLIP .},
  archive      = {J_IJCV},
  author       = {Zhu, Lianghui and Wang, Xinggang and Feng, Jiapei and Cheng, Tianheng and Li, Yingyue and Jiang, Bo and Zhang, Dingwen and Han, Junwei},
  doi          = {10.1007/s11263-024-02224-2},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1085-1105},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {WeakCLIP: Adapting CLIP for weakly-supervised semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual face forgery detection via historical distribution preserving. <em>IJCV</em>, <em>133</em>(3), 1067-1084. (<a href='https://doi.org/10.1007/s11263-024-02160-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery techniques have advanced rapidly and pose serious security threats. Existing face forgery detection methods try to learn generalizable features, but they still fall short of practical application. Additionally, finetuning these methods on historical training data is resource-intensive in terms of time and storage. In this paper, we focus on a novel and challenging problem: Continual Face Forgery Detection (CFFD), which aims to efficiently learn from new forgery attacks without forgetting previous ones. Specifically, we propose a Historical Distribution Preserving (HDP) framework that reserves and preserves the distributions of historical faces. To achieve this, we use universal adversarial perturbation (UAP) to simulate historical forgery distribution, and knowledge distillation to maintain the distribution variation of real faces across different models. We also construct a new benchmark for CFFD with three evaluation protocols. Our extensive experiments on the benchmarks show that our method outperforms the state-of-the-art competitors. Our code is available at https://github.com/skJack/HDP .},
  archive      = {J_IJCV},
  author       = {Sun, Ke and Chen, Shen and Yao, Taiping and Sun, Xiaoshuai and Ding, Shouhong and Ji, Rongrong},
  doi          = {10.1007/s11263-024-02160-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1067-1084},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Continual face forgery detection via historical distribution preserving},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive fuzzy positive learning for annotation-scarce semantic segmentation. <em>IJCV</em>, <em>133</em>(3), 1048-1066. (<a href='https://doi.org/10.1007/s11263-024-02217-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotation-scarce semantic segmentation aims to obtain meaningful pixel-level discrimination with scarce or even no manual annotations, of which the crux is how to utilize unlabeled data by pseudo-label learning. Typical works focus on ameliorating the error-prone pseudo-labeling, e.g., only utilizing high-confidence pseudo labels and filtering low-confidence ones out. But we think differently and resort to exhausting informative semantics from multiple probably correct candidate labels. This brings our method the ability to learn more accurately even though pseudo labels are unreliable. In this paper, we propose Adaptive Fuzzy Positive Learning (A-FPL) for correctly learning unlabeled data in a plug-and-play fashion, targeting adaptively encouraging fuzzy positive predictions and suppressing highly probable negatives. Specifically, A-FPL comprises two main components: (1) Fuzzy positive assignment (FPA) that adaptively assigns fuzzy positive labels to each pixel, while ensuring their quality through a T-value adaption algorithm (2) Fuzzy positive regularization (FPR) that restricts the predictions of fuzzy positive categories to be larger than those of negative categories. Being conceptually simple yet practically effective, A-FPL remarkably alleviates interference from wrong pseudo labels, progressively refining semantic discrimination. Theoretical analysis and extensive experiments on various training settings with consistent performance gain justify the superiority of our approach. Codes are at A-FPL .},
  archive      = {J_IJCV},
  author       = {Qiao, Pengchong and Wang, Yu and Liu, Chang and Shang, Lei and Sun, Baigui and Wang, Zhennan and Zheng, Xiawu and Ji, Rongrong and Chen, Jie},
  doi          = {10.1007/s11263-024-02217-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1048-1066},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Adaptive fuzzy positive learning for annotation-scarce semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic evaluation of uncertainty calibration in pretrained object detectors. <em>IJCV</em>, <em>133</em>(3), 1033-1047. (<a href='https://doi.org/10.1007/s11263-024-02219-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of deep learning based computer vision, the development of deep object detection has led to unique paradigms (e.g., two-stage or set-based) and architectures (e.g., Faster-RCNN or DETR) which enable outstanding performance on challenging benchmark datasets. Despite this, the trained object detectors typically do not reliably assess uncertainty regarding their own knowledge, and the quality of their probabilistic predictions is usually poor. As these are often used to make subsequent decisions, such inaccurate probabilistic predictions must be avoided. In this work, we investigate the uncertainty calibration properties of different pretrained object detection architectures in a multi-class setting. We propose a framework to ensure a fair, unbiased, and repeatable evaluation and conduct detailed analyses assessing the calibration under distributional changes (e.g., distributional shift and application to out-of-distribution data). Furthermore, by investigating the influence of different detector paradigms, post-processing steps, and suitable choices of metrics, we deliver novel insights into why poor detector calibration emerges. Based on these insights, we are able to improve the calibration of a detector by simply finetuning its last layer.},
  archive      = {J_IJCV},
  author       = {Huseljic, Denis and Herde, Marek and Hahn, Paul and Müjde, Mehmet and Sick, Bernhard},
  doi          = {10.1007/s11263-024-02219-z},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1033-1047},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Systematic evaluation of uncertainty calibration in pretrained object detectors},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need. <em>IJCV</em>, <em>133</em>(3), 1012-1032. (<a href='https://doi.org/10.1007/s11263-024-02218-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (CIL) aims to adapt to emerging new classes without forgetting old ones. Traditional CIL models are trained from scratch to continually acquire knowledge as data evolves. Recently, pre-training has achieved substantial progress, making vast pre-trained models (PTMs) accessible for CIL. Contrary to traditional methods, PTMs possess generalizable embeddings, which can be easily transferred for CIL. In this work, we revisit CIL with PTMs and argue that the core factors in CIL are adaptivity for model updating and generalizability for knowledge transferring. (1) We first reveal that frozen PTM can already provide generalizable embeddings for CIL. Surprisingly, a simple baseline (SimpleCIL) which continually sets the classifiers of PTM to prototype features can beat state-of-the-art even without training on the downstream task. (2) Due to the distribution gap between pre-trained and downstream datasets, PTM can be further cultivated with adaptivity via model adaptation. We propose AdaPt and mERge (Aper), which aggregates the embeddings of PTM and adapted models for classifier construction. Aper is a general framework that can be orthogonally combined with any parameter-efficient tuning method, which holds the advantages of PTM’s generalizability and adapted model’s adaptivity. (3) Additionally, considering previous ImageNet-based benchmarks are unsuitable in the era of PTM due to data overlapping, we propose four new benchmarks for assessment, namely ImageNet-A, ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the effectiveness of Aper with a unified and concise framework. Code is available at https://github.com/zhoudw-zdw/RevisitingCIL .},
  archive      = {J_IJCV},
  author       = {Zhou, Da-Wei and Cai, Zi-Wen and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
  doi          = {10.1007/s11263-024-02218-0},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {1012-1032},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight high-speed photography built on coded exposure and implicit neural representation of videos. <em>IJCV</em>, <em>133</em>(3), 991-1011. (<a href='https://doi.org/10.1007/s11263-024-02198-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for compact cameras capable of recording high-speed scenes with high resolution is steadily increasing. However, achieving such capabilities often entails high bandwidth requirements, resulting in bulky, heavy systems unsuitable for low-capacity platforms. To address this challenge, leveraging a coded exposure setup to encode a frame sequence into a blurry snapshot and subsequently retrieve the latent sharp video presents a lightweight solution. Nevertheless, restoring motion from blur remains a formidable challenge due to the inherent ill-posedness of motion blur decomposition, the intrinsic ambiguity in motion direction, and the diverse motions present in natural videos. In this study, we propose a novel approach to address these challenges by combining the classical coded exposure imaging technique with the emerging implicit neural representation for videos. We strategically embed motion direction cues into the blurry image during the imaging process. Additionally, we develop a novel implicit neural representation based blur decomposition network to sequentially extract the latent video frames from the blurry image, leveraging the embedded motion direction cues. To validate the effectiveness and efficiency of our proposed framework, we conduct extensive experiments using benchmark datasets and real-captured blurry images. The results demonstrate that our approach significantly outperforms existing methods in terms of both quality and flexibility. The code for our work is available at https://github.com/zhihongz/BDINR .},
  archive      = {J_IJCV},
  author       = {Zhang, Zhihong and Yang, Runzhao and Suo, Jinli and Cheng, Yuxiao and Dai, Qionghai},
  doi          = {10.1007/s11263-024-02198-1},
  journal      = {International Journal of Computer Vision},
  month        = {3},
  number       = {3},
  pages        = {991-1011},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Lightweight high-speed photography built on coded exposure and implicit neural representation of videos},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Open-vocabulary text-driven human image generation. <em>IJCV</em>, <em>133</em>(2), 989. (<a href='https://doi.org/10.1007/s11263-024-02200-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zhang, Kaiduo and Sun, Muyi and Sun, Jianxin and Zhang, Kunbo and Sun, Zhenan and Tan, Tieniu},
  doi          = {10.1007/s11263-024-02200-w},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {989},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Open-vocabulary text-driven human image generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Special issue on open-world visual recognition. <em>IJCV</em>, <em>133</em>(2), 985-988. (<a href='https://doi.org/10.1007/s11263-024-02232-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Zhong, Zhun and Liu, Hong and Cui, Yin and Satoh, Shin’ichi and Sebe, Nicu and Yang, Ming-Hsuan},
  doi          = {10.1007/s11263-024-02232-2},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {985-988},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Guest editorial: Special issue on open-world visual recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning general and specific embedding with transformer for few-shot object detection. <em>IJCV</em>, <em>133</em>(2), 968-984. (<a href='https://doi.org/10.1007/s11263-024-02199-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot object detection (FSOD) studies how to detect novel objects with few annotated examples effectively. Recently, it has been demonstrated that decent feature embeddings, including the general feature embeddings that are more invariant to visual changes and the specific feature embeddings that are more discriminative for different object classes, are both important for FSOD. However, current methods lack appropriate mechanisms to sensibly cooperate both types of feature embeddings based on their importance to detecting objects of novel classes, which may result in sub-optimal performance. In this paper, to achieve more effective FSOD, we attempt to explicitly encode both general and specific feature embeddings using learnable tensors and apply a Transformer to help better incorporate them in FSOD according to their relations to the input object features. We thus propose a Transformer-based general and specific embedding learning (T-GSEL) method for FSOD. In T-GSEL, learnable tensors are employed in a three-stage pipeline, encoding feature embeddings in general level, intermediate level, and specific level, respectively. In each stage, we apply a Transformer to first model the relations of the corresponding embedding to input object features and then apply the estimated relations to refine the input features. Meanwhile, we further introduce cross-stage connections between embeddings of different stages to make them complement and cooperate with each other, delivering general, intermediate, and specific feature embeddings stage by stage and utilizing them together for feature refinement in FSOD. In practice, a T-GSEL module is easy to inject. Extensive empirical results further show that our proposed T-GSEL method achieves compelling FSOD performance on both PASCAL VOC and MS COCO datasets compared with other state-of-the-art approaches.},
  archive      = {J_IJCV},
  author       = {Zhang, Xu and Chen, Zhe and Zhang, Jing and Liu, Tongliang and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02199-0},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {968-984},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning general and specific embedding with transformer for few-shot object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning box regression and mask segmentation under long-tailed distribution with gradient transfusing. <em>IJCV</em>, <em>133</em>(2), 951-967. (<a href='https://doi.org/10.1007/s11263-024-02104-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning object detectors under long-tailed data distribution is challenging and has been widely studied recently, the prior works mainly focus on balancing the learning signal of classification task such that samples from tail object classes are effectively recognized. However, the learning difficulty of other class-wise tasks including bounding box regression and mask segmentation are not explored before. In this work, we investigate how long-tailed distribution affects the optimization of box regression and mask segmentation tasks. We find that although the standard class-wise box regression and mask segmentation offer strong class-specific prediction, they suffer from limited training signal and instability on the tail object classes. Aiming to address the limitation, our insight is that the knowledge of box regression and object segmentation is naturally shared across classes. We thus develop a cross class gradient transfusing (CRAT) approach to transfer the abundant training signal from head classes to help the training of sample-scarce tail classes. The transferring process is guided by the Fisher information to aggregate useful signals. CRAT can be seamlessly integrated into existing end-to-end or decoupled long-tailed object detection pipelines to robustly learn class-wise box regression and mask segmentation under long-tailed distribution. Our method improves the state-of-the-art long-tailed object detection and instance segmentation models with an average of 3.0 tail AP on the LVIS benchmark. The code implementation will be available at https://github.com/twangnh/CRAT},
  archive      = {J_IJCV},
  author       = {Wang, Tao and Yuan, Li and Wang, Xinchao and Feng, Jiashi},
  doi          = {10.1007/s11263-024-02104-9},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {951-967},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning box regression and mask segmentation under long-tailed distribution with gradient transfusing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AROID: Improving adversarial robustness through online instance-wise data augmentation. <em>IJCV</em>, <em>133</em>(2), 929-950. (<a href='https://doi.org/10.1007/s11263-024-02206-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks are vulnerable to adversarial examples. Adversarial training (AT) is an effective defense against adversarial examples. However, AT is prone to overfitting which degrades robustness substantially. Recently, data augmentation (DA) was shown to be effective in mitigating robust overfitting if appropriately designed and optimized for AT. This work proposes a new method to automatically learn online, instance-wise, DA policies to improve robust generalization for AT. This is the first automated DA method specific for robustness. A novel policy learning objective, consisting of Vulnerability, Affinity and Diversity, is proposed and shown to be sufficiently effective and efficient to be practical for automatic DA generation during AT. Importantly, our method dramatically reduces the cost of policy search from the 5000 h of AutoAugment and the 412 h of IDBH to 9 h, making automated DA more practical to use for adversarial robustness. This allows our method to efficiently explore a large search space for a more effective DA policy and evolve the policy as training progresses. Empirically, our method is shown to outperform all competitive DA methods across various model architectures and datasets. Our DA policy reinforced vanilla AT to surpass several state-of-the-art AT methods regarding both accuracy and robustness. It can also be combined with those advanced AT methods to further boost robustness. Code and pre-trained models are available at: https://github.com/TreeLLi/AROID .},
  archive      = {J_IJCV},
  author       = {Li, Lin and Qiu, Jianing and Spratling, Michael},
  doi          = {10.1007/s11263-024-02206-4},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {929-950},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {AROID: Improving adversarial robustness through online instance-wise data augmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). R $$^{2}$$ S100K: Road-region segmentation dataset for semi-supervised autonomous driving in the wild. <em>IJCV</em>, <em>133</em>(2), 910-928. (<a href='https://doi.org/10.1007/s11263-024-02207-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic understanding of roadways is a key enabling factor for safe autonomous driving. However, existing autonomous driving datasets provide well-structured urban roads while ignoring unstructured roadways containing distress, potholes, water puddles, and various kinds of road patches i.e., earthen, gravel etc. To this end, we introduce Road Region Segmentation dataset (R2S100K)—a large-scale dataset and benchmark for training and evaluation of road segmentation in aforementioned challenging unstructured roadways. R2S100K comprises 100K images extracted from a large and diverse set of video sequences covering more than 1000 km of roadways. Out of these 100K privacy respecting images, 14,000 images have fine pixel-labeling of road regions, with 86,000 unlabeled images that can be leveraged through semi-supervised learning methods. Alongside, we present an Efficient Data Sampling based self-training framework to improve learning by leveraging unlabeled data. Our experimental results demonstrate that the proposed method significantly improves learning methods in generalizability and reduces the labeling cost for semantic segmentation tasks. Our benchmark will be publicly available to facilitate future research at https://r2s100k.github.io/ .},
  archive      = {J_IJCV},
  author       = {Butt, Muhammad Atif and Ali, Hassan and Qayyum, Adnan and Sultani, Waqas and Al-Fuqaha, Ala and Qadir, Junaid},
  doi          = {10.1007/s11263-024-02207-3},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {910-928},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {R $$^{2}$$ S100K: Road-region segmentation dataset for semi-supervised autonomous driving in the wild},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMC-det: Intra–Inter modality contrastive learning for video object detection. <em>IJCV</em>, <em>133</em>(2), 890-909. (<a href='https://doi.org/10.1007/s11263-024-02201-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection is an important yet challenging task in the computer vision field. One limitation of off-the-shelf video object detection methods is that they only explore information from the visual modality, without considering the semantic knowledge of the textual modality due to the large inter-modality discrepancies, resulting in limited detection performance. In this paper, we propose a novel intra–inter modality contrastive learning network for high-performance video object detection (IMC-Det), which includes three substantial improvements over existing methods. First, we design an intra-modality contrastive learning module to pull close similar features while pushing apart dissimilar ones, enabling our IMC-Det to learn more discriminative feature representations. Second, we develop a graph relational feature aggregation module to effectively model the structural relations between features by leveraging cross-graph learning and residual graph convolution, which is conducive to performing more effective feature aggregation in the spatio-temporal domain. Third, we present an inter-modality contrastive learning module to enforce the visual features belonging to same classes to be compactly gathered around the corresponding textual semantic representations, endowing our IMC-Det with better object classification capability. We conduct extensive experiments on the challenging ImageNet VID dataset, and the experimental results demonstrate that our IMC-Det performs favorably against existing state-of-the-art methods. More remarkably, our IMC-Det achieves 85.5% mAP and 86.7% mAP with ResNet-101 and ResNeXt-101, respectively.},
  archive      = {J_IJCV},
  author       = {Qi, Qiang and Qiu, Zhenyu and Yan, Yan and Lu, Yang and Wang, Hanzi},
  doi          = {10.1007/s11263-024-02201-9},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {890-909},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {IMC-det: Intra–Inter modality contrastive learning for video object detection},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward accurate and robust pedestrian detection via variational inference. <em>IJCV</em>, <em>133</em>(2), 867-889. (<a href='https://doi.org/10.1007/s11263-024-02216-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection is notoriously considered a challenging task due to the frequent occlusion between humans. Unlike generic object detection, pedestrian detection involves a single category but dense instances, making it crucial to achieve accurate and robust object localization. By analogizing instance-level localization to a variational autoencoder and regarding the dense proposals as the latent variables, we establish a unique perspective of formulating pedestrian detection as a variational inference problem. From this vantage, we propose the Variational Pedestrian Detector (VPD), which uses a probabilistic model to estimate the true posterior of inferred proposals and applies a reparameterization trick to approximate the expected detection likelihood. In order to adapt the variational inference problem to the case of pedestrian detection, we propose a series of customized designs to cope with the issue of occlusion and spatial vibration. Specifically, we propose the Normal Gaussian and its variant of the Mixture model to parameterize the posterior in complicated scenarios. The inferred posterior is regularized by a conditional prior related to the ground-truth distribution, thus directly coupling the latent variables to specific target objects. Based on the posterior distribution, maximum detection likelihood estimation is applied to optimize the pedestrian detector, where a lightweight statistic decoder is designed to cast the detection likelihood into a parameterized form and enhance the confidence score estimation. With this variational inference process, VPD endows each proposal with the discriminative ability from its adjacent distractor due to the disentangling nature of the latent variable in variational inference, achieving accurate and robust detection in crowded scenes. Experiments conducted on CrowdHuman, CityPersons, and MS COCO demonstrate that our method is not only plug-and-play for numerous popular single-stage methods and two-stage methods but also can achieve a remarkable performance gain in highly occluded scenarios. The code for this project can be found at https://github.com/hhy-ee/VPD .},
  archive      = {J_IJCV},
  author       = {He, Huanyu and Lin, Weiyao and Zhang, Yuang and He, Tianyao and Li, Yuxi and Li, Jianguo},
  doi          = {10.1007/s11263-024-02216-2},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {867-889},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Toward accurate and robust pedestrian detection via variational inference},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free domain adaptation guided by vision and vision-language pre-training. <em>IJCV</em>, <em>133</em>(2), 844-866. (<a href='https://doi.org/10.1007/s11263-024-02215-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) aims to adapt a source model trained on a fully-labeled source domain to a related but unlabeled target domain. While the source model is a key avenue for acquiring target pseudolabels, the generated pseudolabels may exhibit source bias. In the conventional SFDA pipeline, a large data (e.g. ImageNet) pre-trained feature extractor is used to initialize the source model at the start of source training, and subsequently discarded. Despite having diverse features important for generalization, the pre-trained feature extractor can overfit to the source data distribution during source training and forget relevant target domain knowledge. Rather than discarding this valuable knowledge, we introduce an integrated framework to incorporate pre-trained networks into the target adaptation process. The proposed framework is flexible and allows us to plug modern pre-trained networks into the adaptation process to leverage their stronger representation learning capabilities. For adaptation, we propose the Co-learn algorithm to improve target pseudolabel quality collaboratively through the source model and a pre-trained feature extractor. Building on the recent success of the vision-language model CLIP in zero-shot image recognition, we present an extension Co-learn++ to further incorporate CLIP’s zero-shot classification decisions. We evaluate on 4 benchmark datasets and include more challenging scenarios such as open-set, partial-set and open-partial SFDA. Experimental results demonstrate that our proposed strategy improves adaptation performance and can be successfully integrated with existing SFDA methods.},
  archive      = {J_IJCV},
  author       = {Zhang, Wenyu and Shen, Li and Foo, Chuan-Sheng},
  doi          = {10.1007/s11263-024-02215-3},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {844-866},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Source-free domain adaptation guided by vision and vision-language pre-training},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual object detection with multimodal large language models. <em>IJCV</em>, <em>133</em>(2), 825-843. (<a href='https://doi.org/10.1007/s11263-024-02214-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection—understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation.},
  archive      = {J_IJCV},
  author       = {Zang, Yuhang and Li, Wei and Han, Jun and Zhou, Kaiyang and Loy, Chen Change},
  doi          = {10.1007/s11263-024-02214-4},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {825-843},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Contextual object detection with multimodal large language models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-rank transformer for high-resolution hyperspectral computational imaging. <em>IJCV</em>, <em>133</em>(2), 809-824. (<a href='https://doi.org/10.1007/s11263-024-02203-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial-spectral fusion aims to obtain high-resolution hyperspectral image (HR-HSI) by fusing low-resolution hyperspectral image (LR-HSI) and high-resolution multispectral image (MSI). Recently, many convolutional neural network (CNN)-based methods have achieved excellent results. However, these methods only consider local contextual information, which limits the fusion performance. Although some Transformer-based methods overcome this problem, they ignore some intrinsic characteristics of HR-HSI, such as spatial low-rank characteristics, resulting in large parameters and high computational cost. To address this problem, we propose a low-rank Transformer network (LRTN) for spatial-spectral fusion. LRTN can make full use of the spatial prior of MSI and the spectral prior of LR-HSI, thereby achieving outstanding fusion performance. Specifically, in the feature extraction stage, we utilize the cross-attention mechanism to force the model to focus on spatial information that is not available in LR-HSI and spectral information that is not available in MSI. In the feature fusion stage, we carefully design a self-attention mechanism guided by spatial and spectral priors to improve spatial and spectral fidelity. Moreover, we present a novel spatial low-rank cross-attention module, which can better capture global spatial information compared to other Transformer structures. In this module, we combine the matrix factorization theorem to fully exploit the spatial low-rank characteristics of HSI, which reduces parameters and computational cost while ensuring fusion quality. Experiments on several datasets demonstrate that our method outperforms the current state-of-the-art spatial-spectral fusion methods.},
  archive      = {J_IJCV},
  author       = {Liu, Yuanye and Dian, Renwei and Li, Shutao},
  doi          = {10.1007/s11263-024-02203-7},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {809-824},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Low-rank transformer for high-resolution hyperspectral computational imaging},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised object localization in the era of self-supervised ViTs: A survey. <em>IJCV</em>, <em>133</em>(2), 781-808. (<a href='https://doi.org/10.1007/s11263-024-02167-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent enthusiasm for open-world vision systems show the high interest of the community to perform perception tasks outside of the closed-vocabulary benchmark setups which have been so popular until now. Being able to discover objects in images/videos without knowing in advance what objects populate the dataset is an exciting prospect. But how to find objects without knowing anything about them? Recent works show that it is possible to perform class-agnostic unsupervised object localization by exploiting self-supervised pre-trained features. We propose here a survey of unsupervised object localization methods that discover objects in images without requiring any manual annotation in the era of self-supervised ViTs.},
  archive      = {J_IJCV},
  author       = {Siméoni, Oriane and Zablocki, Éloi and Gidaris, Spyros and Puy, Gilles and Pérez, Patrick},
  doi          = {10.1007/s11263-024-02167-8},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {781-808},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Unsupervised object localization in the era of self-supervised ViTs: A survey},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked channel modeling for bootstrapping visual pre-training. <em>IJCV</em>, <em>133</em>(2), 760-780. (<a href='https://doi.org/10.1007/s11263-024-02204-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large vision models have achieved great success in computer vision recently, e.g., CLIP for large-scale image-text contrastive learning. They have prominent potential in representation learning and show strong transfer ability in various downstream tasks. However, directly training a larger CLIP model from scratch is difficult because of the enormous training cost, unstable training, and difficulty in collecting a large amount of training data. In this work, we aim to scale the sizes of CLIP models and extend their strong capabilities with self-supervised representation learning. We introduce masked channel modeling (MCM), a new self-supervised learning framework that randomly masks the input feature maps extracted by a CLIP model and reconstructs the missing features. Unlike masked image modeling (MIM) which takes raw pixels as the input and output, MCM performs masked modeling at a high-dimensional semantic space by masking random channels of the visual features and reconstructing the corrupted channels. We show that channel maps are a great fit for masked modeling, as the visual features are semantically structured across channels. We demonstrate that our method can easily scale up the CLIP model at a low training cost, and extend its capabilities on zero-shot learning, few-shot learning, and end-to-end fine-tuning. Based on CLIP ViT-L, MCM improves the zero-shot image classification accuracy by 0.5% averaged over 8 benchmarks. With a few samples, e.g., 1-shot or 2-shot, MCM achieves significant improvements when adapting to 11 image classification benchmarks. In addition, MCM shows strong performance when end-to-end fine-tuned on different downstream tasks, e.g., improving CLIP ViT-B by 0.9% top-1 accuracy on ImageNet-1K classification and 2.5% mIoU on ADE20K semantic segmentation.},
  archive      = {J_IJCV},
  author       = {Liu, Yang and Wang, Xinlong and Zhu, Muzhi and Cao, Yue and Huang, Tiejun and Shen, Chunhua},
  doi          = {10.1007/s11263-024-02204-6},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {760-780},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Masked channel modeling for bootstrapping visual pre-training},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLMFormer: Large language model for open-vocabulary semantic segmentation. <em>IJCV</em>, <em>133</em>(2), 742-759. (<a href='https://doi.org/10.1007/s11263-024-02171-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary (OV) semantic segmentation has attracted increasing attention in recent years, which aims to recognize objects in an open class set for real-world applications. While prior OV semantic segmentation approaches have relied on additional semantic knowledge derived from vision-language (VL) pre-training, such as the popular CLIP model, this paper introduces a novel paradigm by harnessing the unprecedented capabilities of large language models (LLMs). Inspired by recent breakthroughs in LLMs that provide a richer knowledge base compared to traditional vision-language pre-training, our proposed methodology capitalizes on the vast knowledge embedded within LLMs for OV semantic segmentation. Particularly, we partition LLM knowledge into object, attribute, and relation priors, and propose three novel attention modules-semantic, scaled visual, and relation attentions, to utilize the LLM priors. Extensive experiments are conducted on common benchmarks including ADE20K (847 classes) and Pascal Context (459 classes). The results show that our model outperforms previous state-of-the-art (SoTA) methods by up to 7.2% absolute. Moreover, unlike previous VL-pre-training-based works, our method can even predict OV segmentation results without target candidate classes.},
  archive      = {J_IJCV},
  author       = {Shi, Hengcan and Dao, Son Duy and Cai, Jianfei},
  doi          = {10.1007/s11263-024-02171-y},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {742-759},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {LLMFormer: Large language model for open-vocabulary semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Out-of-distribution detection with virtual outlier smoothing. <em>IJCV</em>, <em>133</em>(2), 724-741. (<a href='https://doi.org/10.1007/s11263-024-02210-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting out-of-distribution (OOD) inputs plays a crucial role in guaranteeing the reliability of deep neural networks (DNNs) when deployed in real-world scenarios. However, DNNs typically exhibit overconfidence in OOD samples, which is attributed to the similarity in patterns between OOD and in-distribution (ID) samples. To mitigate this overconfidence, advanced approaches suggest the incorporation of auxiliary OOD samples during model training, where the outliers are assigned with an equal likelihood of belonging to any category. However, identifying outliers that share patterns with ID samples poses a significant challenge. To address the challenge, we propose a novel method, Virtual Outlier Smoothing (VOSo), which constructs auxiliary outliers using ID samples, thereby eliminating the need to search for OOD samples. Specifically, VOSo creates these virtual outliers by perturbing the semantic regions of ID samples and infusing patterns from other ID samples. For instance, a virtual outlier might consist of a cat’s face with a dog’s nose, where the cat’s face serves as the semantic feature for model prediction. Meanwhile, VOSo adjusts the labels of virtual OOD samples based on the extent of semantic region perturbation, aligning with the notion that virtual outliers may contain ID patterns. Extensive experiments are conducted on diverse OOD detection benchmarks, demonstrating the effectiveness of the proposed VOSo. Our code will be available at https://github.com/junz-debug/VOSo .},
  archive      = {J_IJCV},
  author       = {Nie, Jun and Luo, Yadan and Ye, Shanshan and Zhang, Yonggang and Tian, Xinmei and Fang, Zhen},
  doi          = {10.1007/s11263-024-02210-8},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {724-741},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Out-of-distribution detection with virtual outlier smoothing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised scalable deep compressed sensing. <em>IJCV</em>, <em>133</em>(2), 688-723. (<a href='https://doi.org/10.1007/s11263-024-02209-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed sensing (CS) is a promising tool for reducing sampling costs. Current deep neural network (NN)-based CS approaches face the challenges of collecting labeled measurement-ground truth (GT) data and generalizing to real applications. This paper proposes a novel Self-supervised sCalable deep CS method, comprising a deep Learning scheme called SCL and a family of Networks named SCNet, which does not require GT and can handle arbitrary sampling ratios and matrices once trained on a partial measurement set. Our SCL contains a dual-domain loss and a four-stage recovery strategy. The former encourages a cross-consistency on two measurement parts and a sampling-reconstruction cycle-consistency regarding arbitrary ratios and matrices to maximize data utilization. The latter can progressively leverage the common signal prior in external measurements and internal characteristics of test samples and learned NNs to improve accuracy. SCNet combines both the explicit guidance from optimization algorithms and the implicit regularization from advanced NN blocks to learn a collaborative signal representation. Our theoretical analyses and experiments on simulated and real captured data, covering 1-/2-/3-D natural and scientific signals, demonstrate the effectiveness, superior performance, flexibility, and generalization ability of our method over existing self-supervised methods and its significant potential in competing against many state-of-the-art supervised methods. Code is available at https://github.com/Guaishou74851/SCNet .},
  archive      = {J_IJCV},
  author       = {Chen, Bin and Zhang, Xuanyu and Liu, Shuai and Zhang, Yongbing and Zhang, Jian},
  doi          = {10.1007/s11263-024-02209-1},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {688-723},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Self-supervised scalable deep compressed sensing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test-time forgery detection with spatial-frequency prompt learning. <em>IJCV</em>, <em>133</em>(2), 672-687. (<a href='https://doi.org/10.1007/s11263-024-02208-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significance of face forgery detection has grown substantially due to the emergence of facial manipulation technologies. Recent methods have turned to face detection forgery in the spatial-frequency domain, resulting in improved overall performance. Nonetheless, these methods are still not guaranteed to cover various forgery technologies, and the networks trained on public datasets struggle to accurately quantify their uncertainty levels. In this work, we design a Dynamic Dual-spectrum Interaction Network that allows test-time training with uncertainty guidance and spatial-frequency prompt learning. RGB and frequency features are first interacted in multi-level by using a Frequency-guided Attention Module. Then these multi-modal features are merged with a Dynamic Fusion Module. As a bias in the fusion weight of uncertain data during dynamic fusion, we further exploit uncertain perturbation as guidance during the test-time training phase. Furthermore, we propose a spatial-frequency prompt learning method to effectively enhance the generalization of the forgery detection model. Finally, we curate a novel, extensive dataset containing images synthesized by various diffusion and non-diffusion methods. Comprehensive evaluations of experiments show that our method achieves more appealing results for face forgery detection than recent state-of-the-art methods.},
  archive      = {J_IJCV},
  author       = {Duan, Junxian and Ai, Yuang and Liu, Jipeng and Huang, Shenyuan and Huang, Huaibo and Cao, Jie and He, Ran},
  doi          = {10.1007/s11263-024-02208-2},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {672-687},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Test-time forgery detection with spatial-frequency prompt learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational rectification inference for learning with noisy labels. <em>IJCV</em>, <em>133</em>(2), 652-671. (<a href='https://doi.org/10.1007/s11263-024-02205-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label noise has been broadly observed in real-world datasets. To mitigate the negative impact of overfitting to label noise for deep models, effective strategies (e.g., re-weighting, or loss rectification) have been broadly applied in prevailing approaches, which have been generally learned under the meta-learning scenario. Despite the robustness of noise achieved by the probabilistic meta-learning models, they usually suffer from model collapse that degenerates generalization performance. In this paper, we propose variational rectification inference (VRI) to formulate the adaptive rectification for loss functions as an amortized variational inference problem and derive the evidence lower bound under the meta-learning framework. Specifically, VRI is constructed as a hierarchical Bayes by treating the rectifying vector as a latent variable, which can rectify the loss of the noisy sample with the extra randomness regularization and is, therefore, more robust to label noise. To achieve the inference of the rectifying vector, we approximate its conditional posterior with an amortization meta-network. By introducing the variational term in VRI, the conditional posterior is estimated accurately and avoids collapsing to a Dirac delta function, which can significantly improve the generalization performance. The elaborated meta-network and prior network adhere to the smoothness assumption, enabling the generation of reliable rectification vectors. Given a set of clean meta-data, VRI can be efficiently meta-learned within the bi-level optimization programming. Besides, theoretical analysis guarantees that the meta-network can be efficiently learned with our algorithm. Comprehensive comparison experiments and analyses validate its effectiveness for robust learning with noisy labels, particularly in the presence of open-set noise.},
  archive      = {J_IJCV},
  author       = {Sun, Haoliang and Wei, Qi and Feng, Lei and Hu, Yupeng and Liu, Fan and Fan, Hehe and Yin, Yilong},
  doi          = {10.1007/s11263-024-02205-5},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {652-671},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Variational rectification inference for learning with noisy labels},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking open-world DeepFake attribution with multi-perspective sensory learning. <em>IJCV</em>, <em>133</em>(2), 628-651. (<a href='https://doi.org/10.1007/s11263-024-02184-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge in sourcing attribution for forgery faces has gained widespread attention due to the rapid development of generative techniques. While many recent works have taken essential steps on GAN-generated faces, more threatening attacks related to identity swapping or diffusion models are still overlooked. And the forgery traces hidden in unknown attacks from the open-world unlabeled faces remain under-explored. To push the related frontier research, we introduce a novel task named Open-World DeepFake Attribution, and the corresponding benchmark OW-DFA++, which aims to evaluate attribution performance against various types of fake faces in open-world scenarios. Meanwhile, we propose a Multi-Perspective Sensory Learning (MPSL) framework that aims to address the challenge of OW-DFA++. Since different forged faces have different tampering regions and frequency artifacts, we introduce the Multi-Perception Voting (MPV) module, which aligns inter-sample features based on global, multi-scale local, and frequency relations. The MPV module effectively filters and groups together samples belonging to the same attack type. Pseudo-labeling is another common and effective strategy in semi-supervised learning tasks, and we propose the Confidence-Adaptive Pseudo-labeling (CAP) module, using soft pseudo-labeling to enhance the class compactness and mitigate pseudo-noise induced by similar novel attack methods. The CAP module imposes strong constraints and adaptively filters samples with high uncertainty to improve the accuracy of the pseudo-labeling. In addition, we extend the MPSL framework with a multi-stage paradigm that leverages pre-train technique and iterative learning to further enhance traceability performance. Extensive experiments and visualizations verify the superiority of our proposed method on the OW-DFA++ and demonstrate the interpretability of the deepfake attribution task and its impact on improving the security of the deepfake detection area.},
  archive      = {J_IJCV},
  author       = {Sun, Zhimin and Chen, Shen and Yao, Taiping and Yi, Ran and Ding, Shouhong and Ma, Lizhuang},
  doi          = {10.1007/s11263-024-02184-7},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {628-651},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Rethinking open-world DeepFake attribution with multi-perspective sensory learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FMGS: Foundation model embedded 3D gaussian splatting for holistic 3D scene understanding. <em>IJCV</em>, <em>133</em>(2), 611-627. (<a href='https://doi.org/10.1007/s11263-024-02183-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present Foundation Model Embedded Gaussian Splatting (FMGS), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by $${10.2}$$ object detection, despite that we are $${851\times }$$ faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code on the [project page] .},
  archive      = {J_IJCV},
  author       = {Zuo, Xingxing and Samangouei, Pouya and Zhou, Yunwen and Di, Yan and Li, Mingyang},
  doi          = {10.1007/s11263-024-02183-8},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {611-627},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FMGS: Foundation model embedded 3D gaussian splatting for holistic 3D scene understanding},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted joint distribution optimal transport based domain adaptation for cross-scenario face anti-spoofing. <em>IJCV</em>, <em>133</em>(2), 590-610. (<a href='https://doi.org/10.1007/s11263-024-02178-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation-based face anti-spoofing methods have attracted more and more attention due to their promising generalization abilities. To mitigate domain bias, existing methods generally attempt to align the marginal distributions of samples from source and target domains. However, the label and pseudo-label information of the samples from source and target domains are ignored. To solve this problem, this paper proposes a Weighted Joint Distribution Optimal Transport unsupervised multi-source domain adaptation method for cross-scenario face anti-spoofing (WJDOT-FAS). WJDOT-FAS consists of three modules: joint distribution estimation, joint distribution optimal transport, and domain weight optimization. Specifically, the joint distributions of the features and pseudo labels of multi-source and target domains are firstly estimated based on a pre-trained feature extractor and a randomly initialized classifier. Then, we compute the cost matrices and the optimal transportation mappings from the joint distributions related to each source domain and the target domain by solving Lp-L1 optimal transport problems. Finally, based on the loss functions of different source domains, the target domain, and the optimal transportation losses from each source domain to the target domain, we can estimate the weights of each source domain, and meanwhile, the parameters of the feature extractor and classifier are also updated. All the learnable parameters and the computations of the three modules are updated alternatively. Extensive experimental results on four widely used 2D attack datasets and three recently published 3D attack datasets under both single- and multi-source domain adaptation settings (including both close-set and open-set) show the advantages of our proposed method for cross-scenario face anti-spoofing.},
  archive      = {J_IJCV},
  author       = {Mao, Shiyun and Chen, Ruolin and Li, Huibin},
  doi          = {10.1007/s11263-024-02178-5},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {590-610},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Weighted joint distribution optimal transport based domain adaptation for cross-scenario face anti-spoofing},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive study on robustness of image classification models: Benchmarking and rethinking. <em>IJCV</em>, <em>133</em>(2), 567-589. (<a href='https://doi.org/10.1007/s11263-024-02196-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of deep neural networks is frequently compromised when faced with adversarial examples, common corruptions, and distribution shifts, posing a significant research challenge in the advancement of deep learning. Although new deep learning methods and robustness improvement techniques have been constantly proposed, the robustness evaluations of existing methods are often inadequate due to their rapid development, diverse noise patterns, and simple evaluation metrics. Without thorough robustness evaluations, it is hard to understand the advances in the field and identify the effective methods. In this paper, we establish a comprehensive robustness benchmark called ARES-Bench on the image classification task. In our benchmark, we evaluate the robustness of 61 typical deep learning models on ImageNet with diverse architectures (e.g., CNNs, Transformers) and learning algorithms (e.g., normal supervised training, pre-training, adversarial training) under numerous adversarial attacks and out-of-distribution (OOD) datasets. Using robustness curves as the major evaluation criteria, we conduct large-scale experiments and draw several important findings, including: (1) there exists an intrinsic trade-off between the adversarial and natural robustness of specific noise types for the same model architecture; (2) adversarial training effectively improves adversarial robustness, especially when performed on Transformer architectures; (3) pre-training significantly enhances natural robustness by leveraging larger training datasets, incorporating multi-modal data, or employing self-supervised learning techniques. Based on ARES-Bench, we further analyze the training tricks in large-scale adversarial training on ImageNet. Through tailored training settings, we achieve a new state-of-the-art in adversarial robustness. We have made the benchmarking results and code platform publicly available.},
  archive      = {J_IJCV},
  author       = {Liu, Chang and Dong, Yinpeng and Xiang, Wenzhao and Yang, Xiao and Su, Hang and Zhu, Jun and Chen, Yuefeng and He, Yuan and Xue, Hui and Zheng, Shibao},
  doi          = {10.1007/s11263-024-02196-3},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {567-589},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A comprehensive study on robustness of image classification models: Benchmarking and rethinking},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SplitNet: Learnable clean-noisy label splitting for learning with noisy labels. <em>IJCV</em>, <em>133</em>(2), 549-566. (<a href='https://doi.org/10.1007/s11263-024-02187-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotating the dataset with high-quality labels is crucial for deep networks’ performance, but in real-world scenarios, the labels are often contaminated by noise. To address this, some methods were recently proposed to automatically split clean and noisy labels among training data, and learn a semi-supervised learner in a Learning with Noisy Labels (LNL) framework. However, they leverage a handcrafted module for clean-noisy label splitting, which induces a confirmation bias in the semi-supervised learning phase and limits the performance. In this paper, for the first time, we present a learnable module for clean-noisy label splitting, dubbed SplitNet, and a novel LNL framework which complementarily trains the SplitNet and main network for the LNL task. We also propose to use a dynamic threshold based on split confidence by SplitNet to optimize the semi-supervised learner better. To enhance SplitNet training, we further present a risk hedging method. Our proposed method performs at a state-of-the-art level, especially in high noise ratio settings on various LNL benchmarks.},
  archive      = {J_IJCV},
  author       = {Kim, Daehwan and Ryoo, Kwangrok and Cho, Hansang and Kim, Seungryong},
  doi          = {10.1007/s11263-024-02187-4},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {549-566},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SplitNet: Learnable clean-noisy label splitting for learning with noisy labels},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel class discovery meets foundation models for 3D semantic segmentation. <em>IJCV</em>, <em>133</em>(2), 527-548. (<a href='https://doi.org/10.1007/s11263-024-02180-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of Novel Class Discovery (NCD) in semantic segmentation involves training a model to accurately segment unlabelled (novel) classes, using the supervision available from annotated (base) classes. The NCD task within the 3D point cloud domain is novel, and it is characterised by assumptions and challenges absent in its 2D counterpart. This paper advances the analysis of point cloud data in four directions. Firstly, it introduces the novel task of NCD for point cloud semantic segmentation. Secondly, it demonstrates that directly applying an existing NCD method for 2D image semantic segmentation to 3D data yields limited results. Thirdly, it presents a new NCD approach based on online clustering, uncertainty estimation, and semantic distillation. Lastly, it proposes a novel evaluation protocol to rigorously assess the performance of NCD in point cloud semantic segmentation. Through comprehensive evaluations on the SemanticKITTI, SemanticPOSS, and S3DIS datasets, our approach show superior performance compared to the considered baselines.},
  archive      = {J_IJCV},
  author       = {Riz, Luigi and Saltori, Cristiano and Wang, Yiming and Ricci, Elisa and Poiesi, Fabio},
  doi          = {10.1007/s11263-024-02180-x},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {527-548},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Novel class discovery meets foundation models for 3D semantic segmentation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive visual prompt learning with contrastive feature re-formation. <em>IJCV</em>, <em>133</em>(2), 511-526. (<a href='https://doi.org/10.1007/s11263-024-02172-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt learning has recently emerged as a compelling alternative to the traditional fine-tuning paradigm for adapting the pre-trained Vision-Language (V-L) models to downstream tasks. Drawing inspiration from the success of prompt learning in Natural Language Processing, pioneering research efforts have been predominantly concentrated on text-based prompting strategies. By contrast, the visual prompting within V-L models remains underexploited. The straightforward transposition of existing visual prompt methods, tailored for Vision Transformers (ViT), into the V-L models often leads to suboptimal performance or training instability. To mitigate these challenges, in this paper, we propose a novel structure called Progressive Visual Prompt (ProVP). This design aims to strengthen the interaction among prompts from adjacent layers, thereby enabling more effective propagation of image embeddings to deeper layers in a manner akin to an instance-specific manner. Additionally, to address the common issue of generalization deterioration in the training period of learnable prompts, we further introduce a contrastive feature re-formation technique for visual prompt learning. This method prevents significant deviations of prompted visual features from the fixed CLIP visual feature distribution, ensuring its better generalization capability. Combining the ProVP and the contrastive feature re-formation technique, our proposed method, ProVP-Ref, significantly stabilizes the training process and enhances both the adaptation and generalization capabilities of visual prompt learning in V-L models. To demonstrate the efficacy of our approach, we evaluate ProVP-Ref across 11 image datasets, achieving the state-of-the-art results on 7 of these datasets in both few-shot learning and base-to-new generalization settings. To the best of our knowledge, this is the first study to showcase the exceptional performance of visual prompts in V-L models compared to previous text prompting methods in this area.},
  archive      = {J_IJCV},
  author       = {Xu, Chen and Zhu, Yuhan and Shen, Haocheng and Chen, Boheng and Liao, Yixuan and Chen, Xiaoxin and Wang, Limin},
  doi          = {10.1007/s11263-024-02172-x},
  journal      = {International Journal of Computer Vision},
  month        = {2},
  number       = {2},
  pages        = {511-526},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Progressive visual prompt learning with contrastive feature re-formation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Instant3D: Instant text-to-3D generation. <em>IJCV</em>, <em>133</em>(1), 509. (<a href='https://doi.org/10.1007/s11263-024-02193-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJCV},
  author       = {Li, Ming and Zhou, Pan and Liu, Jia-Wei and Keppo, Jussi and Lin, Min and Yan, Shuicheng and Xu, Xiangyu},
  doi          = {10.1007/s11263-024-02193-6},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {509},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Correction: Instant3D: Instant text-to-3D generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From easy to hard: Learning curricular shape-aware features for robust panoptic scene graph generation. <em>IJCV</em>, <em>133</em>(1), 489-508. (<a href='https://doi.org/10.1007/s11263-024-02190-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panoptic Scene Graph Generation (PSG) aims to generate a comprehensive graph-structure representation based on panoptic segmentation masks. Despite remarkable progress in PSG, almost all existing methods neglect the importance of shape-aware features, which inherently focus on the contours and boundaries of objects. To bridge this gap, we propose a model-agnostic Curricular shApe-aware FEature (CAFE) learning strategy for PSG. Specifically, we incorporate shape-aware features (i.e., mask features and boundary features) into PSG, moving beyond reliance solely on bbox features. Furthermore, drawing inspiration from human cognition, we propose to integrate shape-aware features in an easy-to-hard manner. To achieve this, we categorize the predicates into three groups based on cognition learning difficulty and correspondingly divide the training process into three stages. Each stage utilizes a specialized relation classifier to distinguish specific groups of predicates. As the learning difficulty of predicates increases, these classifiers are equipped with features of ascending complexity. We also incorporate knowledge distillation to retain knowledge acquired in earlier stages. Due to its model-agnostic nature, CAFE can be seamlessly incorporated into any PSG model. Extensive experiments and ablations on two PSG tasks under both robust and zero-shot PSG have attested to the superiority and robustness of our proposed CAFE, which outperforms existing state-of-the-art methods by a large margin.},
  archive      = {J_IJCV},
  author       = {Shi, Hanrong and Li, Lin and Xiao, Jun and Zhuang, Yueting and Chen, Long},
  doi          = {10.1007/s11263-024-02190-9},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {489-508},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {From easy to hard: Learning curricular shape-aware features for robust panoptic scene graph generation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Triplane-smoothed video dehazing with CLIP-enhanced generalization. <em>IJCV</em>, <em>133</em>(1), 475-488. (<a href='https://doi.org/10.1007/s11263-024-02161-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video dehazing is a critical research area in computer vision that aims to enhance the quality of hazy frames, which benefits many downstream tasks, e.g. semantic segmentation. Recent work devise CNN-based structure or attention mechanism to fuse temporal information, while some others utilize offset between frames to align frames explicitly. Another significant line of video dehazing research focuses on constructing paired datasets by synthesizing foggy effect on clear video or generating real haze effect on indoor scenes. Despite the significant contributions of these dehazing networks and datasets to the advancement of video dehazing, current methods still suffer from spatial–temporal inconsistency and poor generalization ability. We address the aforementioned issues by proposing a triplane smoothing module to explicitly benefit from spatial–temporal smooth prior of the input video and generate temporally coherent dehazing results. We further devise a query base decoder to extract haze-relevant information while also aggregate temporal clues implicitly. To increase the generalization ability of our dehazing model we utilize CLIP guidance with a rich and high-level understanding of hazy effect. We conduct extensive experiments to verify the effectiveness of our model to generate spatial–temporally consistent dehazing results and produce pleasing dehazing results of real-world data.},
  archive      = {J_IJCV},
  author       = {Ren, Jingjing and Chen, Haoyu and Ye, Tian and Wu, Hongtao and Zhu, Lei},
  doi          = {10.1007/s11263-024-02161-0},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {475-488},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Triplane-smoothed video dehazing with CLIP-enhanced generalization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Winning prize comes from losing tickets: Improve invariant learning by exploring variant parameters for out-of-distribution generalization. <em>IJCV</em>, <em>133</em>(1), 456-474. (<a href='https://doi.org/10.1007/s11263-024-02075-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-Distribution (OOD) Generalization aims to learn robust models that generalize well to various environments without fitting to distribution-specific features. Recent studies based on Lottery Ticket Hypothesis (LTH) address this problem by minimizing the learning target to find some of the parameters that are critical to the task. However, in open-world visual recognition problems, such solutions are suboptimal as the learning task contains severe distribution noises, which can mislead the optimization process. Therefore, apart from finding the task-related parameters (i.e., invariant parameters), we propose Exploring Variant parameters for Invariant Learning (EVIL) which also leverages the distribution knowledge to find the parameters that are sensitive to distribution shift (i.e., variant parameters). Once the variant parameters are left out of invariant learning, a robust subnetwork that is resistant to distribution shift can be found. Additionally, the parameters that are relatively stable across distributions can be considered invariant ones to improve invariant learning. By fully exploring both variant and invariant parameters, our EVIL can effectively identify a robust subnetwork to improve OOD generalization. In extensive experiments on integrated testbed: DomainBed, EVIL can effectively and efficiently enhance many popular methods, such as ERM, IRM, SAM, etc. Our code is available at https://github.com/tmllab/EVIL .},
  archive      = {J_IJCV},
  author       = {Huang, Zhuo and Li, Muyang and Shen, Li and Yu, Jun and Gong, Chen and Han, Bo and Liu, Tongliang},
  doi          = {10.1007/s11263-024-02075-x},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {456-474},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Winning prize comes from losing tickets: Improve invariant learning by exploring variant parameters for out-of-distribution generalization},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compressed event sensing (CES) volumes for event cameras. <em>IJCV</em>, <em>133</em>(1), 435-455. (<a href='https://doi.org/10.1007/s11263-024-02197-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has made significant progress in event-driven applications. But to match standard vision networks, most approaches rely on aggregating events into grid-like representations, which obscure crucial temporal information and limit overall performance. To address this issue, we propose a novel event representation called compressed event sensing (CES) volumes. CES volumes preserve the high temporal resolution of event streams by leveraging the sparsity property of events and the principles of compressed sensing theory. They effectively capture the frequency characteristics of events in low-dimensional representations, which can be accurately decoded to raw high-dimensional event signals. In addition, our theoretical analysis show that, when integrated with a neural network, CES volumes demonstrates greater expressive power under the neural tangent kernel approximation. Through synthetic phantom validation on dense frame regression and two downstream applications involving intensity-image reconstruction and object recognition tasks, we demonstrate the superior performance of CES volumes compared to state-of-the-art event representations.},
  archive      = {J_IJCV},
  author       = {Lin, Songnan and Ma, Ye and Chen, Jing and Wen, Bihan},
  doi          = {10.1007/s11263-024-02197-2},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {435-455},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Compressed event sensing (CES) volumes for event cameras},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging the source-to-target gap for cross-domain person re-identification with intermediate domains. <em>IJCV</em>, <em>133</em>(1), 410-434. (<a href='https://doi.org/10.1007/s11263-024-02169-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain person re-identification (re-ID), such as unsupervised domain adaptive re-ID (UDA re-ID), aims to transfer the identity-discriminative knowledge from the source to the target domain. Existing methods commonly consider the source and target domains are isolated from each other, i.e., no intermediate status is modeled between the source and target domains. Directly transferring the knowledge between two isolated domains can be very difficult, especially when the domain gap is large. This paper, from a novel perspective, assumes these two domains are not completely isolated, but can be connected through a series of intermediate domains. Instead of directly aligning the source and target domains against each other, we propose to align the source and target domains against their intermediate domains so as to facilitate a smooth knowledge transfer. To discover and utilize these intermediate domains, this paper proposes an Intermediate Domain Module (IDM) and a Mirrors Generation Module (MGM). IDM has two functions: (1) it generates multiple intermediate domains by mixing the hidden-layer features from source and target domains and (2) it dynamically reduces the domain gap between the source/target domain features and the intermediate domain features. While IDM achieves good domain alignment effect, it introduces a side effect, i.e., the mix-up operation may mix the identities into a new identity and lose the original identities. Accordingly, MGM is introduced to compensate the loss of the original identity by mapping the features into the IDM-generated intermediate domains without changing their original identity. It allows to focus on minimizing domain variations to further promote the alignment between the source/target domain and intermediate domains, which reinforces IDM into IDM++. We extensively evaluate our method under both the UDA and domain generalization (DG) scenarios and observe that IDM++ yields consistent (and usually significant) performance improvement for cross-domain re-ID, achieving new state of the art. For example, on the challenging MSMT17 benchmark, IDM++ surpasses the prior state of the art by a large margin (e.g., up to 9.9% and 7.8% rank-1 accuracy) for UDA and DG scenarios, respectively. Code is available at https://github.com/SikaStar/IDM .},
  archive      = {J_IJCV},
  author       = {Dai, Yongxing and Sun, Yifan and Liu, Jun and Tong, Zekun and Duan, Ling-Yu},
  doi          = {10.1007/s11263-024-02169-6},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {410-434},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Bridging the source-to-target gap for cross-domain person re-identification with intermediate domains},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video instance segmentation in an open-world. <em>IJCV</em>, <em>133</em>(1), 398-409. (<a href='https://doi.org/10.1007/s11263-024-02195-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing video instance segmentation (VIS) approaches generally follow a closed-world assumption, where only seen category instances are identified and spatio-temporally segmented at inference. Open-world formulation relaxes the close-world static-learning assumption as follows: (a) first, it distinguishes a set of known categories as well as labels an unknown object as ‘unknown’ and then (b) it incrementally learns the class of an unknown as and when the corresponding semantic labels become available. We propose the first open-world VIS approach, named OW-VISFormer, that introduces a novel feature enrichment mechanism and a spatio-temporal objectness (STO) module. The feature enrichment mechanism based on a light-weight auxiliary network aims at accurate pixel-level (unknown) object delineation from the background as well as distinguishing category-specific known semantic classes. The STO module strives to generate instance-level pseudo-labels by enhancing the foreground activations through a contrastive loss. Moreover, we also introduce an extensive experimental protocol to measure the characteristics of OW-VIS. Our OW-VISFormer performs favorably against a solid baseline in OW-VIS setting. Further, we evaluate our contributions in the standard fully-supervised VIS setting by integrating them into the recent SeqFormer, achieving an absolute gain of 1.6% AP on Youtube-VIS 2019 val. set. Lastly, we show the generalizability of our contributions for the open-world detection (OWOD) setting, outperforming the best existing OWOD method in the literature. Code, models along with OW-VIS splits are available at https://github.com/OmkarThawakar/OWVISFormer .},
  archive      = {J_IJCV},
  author       = {Thawakar, Omkar and Narayan, Sanath and Cholakkal, Hisham and Anwer, Rao Muhammad and Khan, Salman and Laaksonen, Jorma and Shah, Mubarak and Khan, Fahad Shahbaz},
  doi          = {10.1007/s11263-024-02195-4},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {398-409},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Video instance segmentation in an open-world},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equiangular basis vectors: A novel paradigm for classification tasks. <em>IJCV</em>, <em>133</em>(1), 372-397. (<a href='https://doi.org/10.1007/s11263-024-02189-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose Equiangular Basis Vectors (EBVs) as a novel training paradigm of deep learning for image classification tasks. Differing from prominent training paradigms, e.g., k-way classification layers (mapping the learned representations to the label space) and deep metric learning (quantifying sample similarity), our method generates normalized vector embeddings as "predefined classifiers", which act as the fixed learning targets corresponding to different categories. By minimizing the spherical distance of the embedding of an input between its categorical EBV in training, the predictions can be obtained by identifying the categorical EBV with the smallest distance during inference. More importantly, by directly adding EBVs corresponding to newly added categories of equal status on the basis of existing EBVs, our method exhibits strong scalability to deal with the large increase of training categories in open-environment machine learning. In experiments, we evaluate EBVs on diverse computer vision tasks with large-scale real-world datasets, including classification on ImageNet-1K, object detection on COCO, semantic segmentation on ADE20K, etc. We further collected a dataset consisting of 100,000 categories to validate the superior performance of EBVs when handling a large number of categories. Comprehensive experiments validate both the effectiveness and scalability of our EBVs. Our method won the first place in the 2022 DIGIX Global AI Challenge, code along with all associated logs are open-source and available at https://github.com/aassxun/Equiangular-Basis-Vectors .},
  archive      = {J_IJCV},
  author       = {Shen, Yang and Sun, Xuhao and Wei, Xiu-Shen and Xu, Anqi and Gao, Lingyan},
  doi          = {10.1007/s11263-024-02189-2},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {372-397},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Equiangular basis vectors: A novel paradigm for classification tasks},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Face3DAdv: Exploiting robust adversarial 3D patches on physical face recognition. <em>IJCV</em>, <em>133</em>(1), 353-371. (<a href='https://doi.org/10.1007/s11263-024-02177-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has elucidated the susceptibility of face recognition models to physical adversarial patches, thus provoking security concerns about the deployed face recognition systems. Most existing 2D and 3D physical attacks on face recognition, however, produce adversarial examples using a single-state face image of an attacker. This point-wise attack paradigm tends to yield inferior results when countering numerous complicated states in physical environments, such as diverse pose variations. In this paper, by reassessing the intrinsic relationship between an attacker’s face and its variations, we propose a practical pipeline that simulates complex facial transformations in the physical world through 3D face modeling. This adaptive simulation serves as a digital counterpart of physical faces and empowers us to regulate various facial variations and physical conditions. With this digital simulator, we present the Face3DAdv method to craft 3D adversarial patches, which account for 3D facial transformations and realistic physical variations. Moreover, by optimizing the latent space on 3D modeling and involving importance sampling on various transformations, we demonstrate that Face3DAdv can significantly improve the effectiveness and naturalness of a wide range of physically feasible adversarial patches. Furthermore, the physically 3D-printed adversarial patches by Face3DAdv can achieve an effective evaluation of adversarial robustness on multiple popular commercial services, including four recognition APIs, three anti-spoofing APIs and one automated access control system.},
  archive      = {J_IJCV},
  author       = {Yang, Xiao and Xu, Longlong and Pang, Tianyu and Dong, Yinpeng and Wang, Yikai and Su, Hang and Zhu, Jun},
  doi          = {10.1007/s11263-024-02177-6},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {353-371},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Face3DAdv: Exploiting robust adversarial 3D patches on physical face recognition},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kill two birds with one stone: Domain generalization for semantic segmentation via network pruning. <em>IJCV</em>, <em>133</em>(1), 335-352. (<a href='https://doi.org/10.1007/s11263-024-02194-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep models are notoriously known to perform poorly when encountering new domains with different statistics. To alleviate this issue, we present a new domain generalization method based on network pruning, dubbed NPDG. Our core idea is to prune the filters or attention heads that are more sensitive to domain shift while preserving those domain-invariant ones. To this end, we propose a new pruning policy tailored to improve generalization ability, which identifies the filter and head sensibility of domain shift by judging its activation variance among different domains (unary manner) and its correlation to other filters (binary manner). To better reveal those potentially sensitive filters and heads, we present a differentiable style perturbation scheme to imitate the domain variance dynamically. NPDG is trained on a single source domain and can be applied to both CNN- and Transformer-based backbones. To our knowledge, we are among the pioneers in tackling domain generalization in segmentation via network pruning. NPDG not only improves the generalization ability of a segmentation model but also decreases its computation cost. Extensive experiments demonstrate the state-of-the-art generalization performance of NPDG with a lighter-weight structure.},
  archive      = {J_IJCV},
  author       = {Luo, Yawei and Liu, Ping and Yang, Yi},
  doi          = {10.1007/s11263-024-02194-5},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {335-352},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Kill two birds with one stone: Domain generalization for semantic segmentation via network pruning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge distillation meets open-set semi-supervised learning. <em>IJCV</em>, <em>133</em>(1), 315-334. (<a href='https://doi.org/10.1007/s11263-024-02192-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing knowledge distillation methods mostly focus on distillation of teacher’s prediction and intermediate activation. However, the structured representation, which arguably is one of the most critical ingredients of deep models, is largely overlooked. In this work, we propose a novel semantic representational distillation (SRD) method dedicated for distilling representational knowledge semantically from a pretrained teacher to a target student. The key idea is that we leverage the teacher’s classifier as a semantic critic for evaluating the representations of both teacher and student and distilling the semantic knowledge with high-order structured information over all feature dimensions. This is accomplished by introducing a notion of cross-network logit computed through passing student’s representation into teacher’s classifier. Further, considering the set of seen classes as a basis for the semantic space in a combinatorial perspective, we scale SRD to unseen classes for enabling effective exploitation of largely available, arbitrary unlabeled training data. At the problem level, this establishes an interesting connection between knowledge distillation with open-set semi-supervised learning (SSL). Extensive experiments show that our SRD outperforms significantly previous state-of-the-art knowledge distillation methods on both coarse object classification and fine face recognition tasks, as well as less studied yet practically crucial binary network distillation. Under more realistic open-set SSL settings we introduce, we reveal that knowledge distillation is generally more effective than existing out-of-distribution sample detection, and our proposed SRD is superior over both previous distillation and SSL competitors. The source code is available at https://github.com/jingyang2017/SRD_ossl .},
  archive      = {J_IJCV},
  author       = {Yang, Jing and Zhu, Xiatian and Bulat, Adrian and Martinez, Brais and Tzimiropoulos, Georgios},
  doi          = {10.1007/s11263-024-02192-7},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {315-334},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Knowledge distillation meets open-set semi-supervised learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning rate curriculum. <em>IJCV</em>, <em>133</em>(1), 291-314. (<a href='https://doi.org/10.1007/s11263-024-02186-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most curriculum learning methods require an approach to sort the data samples by difficulty, which is often cumbersome to perform. In this work, we propose a novel curriculum learning approach termed Learning Rate Curriculum (LeRaC), which leverages the use of a different learning rate for each layer of a neural network to create a data-agnostic curriculum during the initial training epochs. More specifically, LeRaC assigns higher learning rates to neural layers closer to the input, gradually decreasing the learning rates as the layers are placed farther away from the input. The learning rates increase at various paces during the first training iterations, until they all reach the same value. From this point on, the neural model is trained as usual. This creates a model-level curriculum learning strategy that does not require sorting the examples by difficulty and is compatible with any neural network, generating higher performance levels regardless of the architecture. We conduct comprehensive experiments on 12 data sets from the computer vision (CIFAR-10, CIFAR-100, Tiny ImageNet, ImageNet-1K, Food-101, UTKFace, PASCAL VOC), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D) domains, considering various convolutional (ResNet-18, Wide-ResNet-50, DenseNet-121, YOLOv5), recurrent (LSTM) and transformer (CvT, BERT, SepTr) architectures. We compare our approach with the conventional training regime, as well as with Curriculum by Smoothing (CBS), a state-of-the-art data-agnostic curriculum learning approach. Unlike CBS, our performance improvements over the standard training regime are consistent across all data sets and models. Furthermore, we significantly surpass CBS in terms of training time (there is no additional cost over the standard training regime for LeRaC). Our code is freely available at: https://github.com/CroitoruAlin/LeRaC .},
  archive      = {J_IJCV},
  author       = {Croitoru, Florinel-Alin and Ristea, Nicolae-Cătălin and Ionescu, Radu Tudor and Sebe, Nicu},
  doi          = {10.1007/s11263-024-02186-5},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {291-314},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning rate curriculum},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Position-guided point cloud panoptic segmentation transformer. <em>IJCV</em>, <em>133</em>(1), 275-290. (<a href='https://doi.org/10.1007/s11263-024-02162-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DEtection TRansformer (DETR) started a trend that uses a group of learnable queries for unified visual perception. This work begins by applying this appealing paradigm to LiDAR-based point cloud segmentation and obtains a simple yet effective baseline. Although the naive adaptation obtains fair results, the instance segmentation performance is noticeably inferior to previous works. By diving into the details, we observe that instances in the sparse point clouds are relatively small to the whole scene and often have similar geometry but lack distinctive appearance for segmentation, which are rare in the image domain. Considering instances in 3D are more featured by their positional information, we emphasize their roles during the modeling and design a robust Mixed-parameterized Positional Embedding (MPE) to guide the segmentation process. It is embedded into backbone features and later guides the mask prediction and query update processes iteratively, leading to Position-Aware Segmentation (PA-Seg) and Masked Focal Attention (MFA). All these designs impel the queries to attend to specific regions and identify various instances. The method, named Position-guided Point cloud Panoptic segmentation transFormer (P3Former), outperforms previous state-of-the-art methods by 2.7% and 1.2% PQ on SemanticKITTI and nuScenes datasets, respectively. The source code and models are available at https://github.com/OpenRobotLab/P3Former .},
  archive      = {J_IJCV},
  author       = {Xiao, Zeqi and Zhang, Wenwei and Wang, Tai and Loy, Chen Change and Lin, Dahua and Pang, Jiangmiao},
  doi          = {10.1007/s11263-024-02162-z},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {275-290},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Position-guided point cloud panoptic segmentation transformer},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ESceme: Vision-and-language navigation with episodic scene memory. <em>IJCV</em>, <em>133</em>(1), 254-274. (<a href='https://doi.org/10.1007/s11263-024-02159-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-and-language navigation (VLN) simulates a visual agent that follows natural-language navigation instructions in real-world scenes. Existing approaches have made enormous progress in navigation in new environments, such as beam search, pre-exploration, and dynamic or hierarchical history encoding. To balance generalization and efficiency, we resort to memorizing visited scenarios apart from the ongoing route while navigating. In this work, we introduce a mechanism of Episodic Scene memory (ESceme) for VLN that wakes an agent’s memories of past visits when it enters the current scene. The episodic scene memory allows the agent to envision a bigger picture of the next prediction. This way, the agent learns to utilize dynamically updated information instead of merely adapting to the current observations. We provide a simple yet effective implementation of ESceme by enhancing the accessible views at each location and progressively completing the memory while navigating. We verify the superiority of ESceme on short-horizon (R2R), long-horizon (R4R), and vision-and-dialog (CVDN) VLN tasks. Our ESceme also wins first place on the CVDN leaderboard. Code is available: https://github.com/qizhust/esceme .},
  archive      = {J_IJCV},
  author       = {Zheng, Qi and Liu, Daqing and Wang, Chaoyue and Zhang, Jing and Wang, Dadong and Tao, Dacheng},
  doi          = {10.1007/s11263-024-02159-8},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {254-274},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {ESceme: Vision-and-language navigation with episodic scene memory},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Top-K pairwise ranking: Bridging the gap among ranking-based measures for multi-label classification. <em>IJCV</em>, <em>133</em>(1), 211-253. (<a href='https://doi.org/10.1007/s11263-024-02157-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label ranking, which returns multiple top-ranked labels for each instance, has a wide range of applications for visual tasks. Due to its complicated setting, prior arts have proposed various measures to evaluate model performances. However, both theoretical analysis and empirical observations show that a model might perform inconsistently on different measures. To bridge this gap, this paper proposes a novel measure named Top-K Pairwise Ranking (TKPR), and a series of analyses show that TKPR is compatible with existing ranking-based measures. In light of this, we further establish an empirical surrogate risk minimization framework for TKPR. On one hand, the proposed framework enjoys convex surrogate losses with the theoretical support of Fisher consistency. On the other hand, we establish a sharp generalization bound for the proposed framework based on a novel technique named data-dependent contraction. Finally, empirical results on benchmark datasets validate the effectiveness of the proposed framework.},
  archive      = {J_IJCV},
  author       = {Wang, Zitai and Xu, Qianqian and Yang, Zhiyong and Wen, Peisong and He, Yuan and Cao, Xiaochun and Huang, Qingming},
  doi          = {10.1007/s11263-024-02157-w},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {211-253},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Top-K pairwise ranking: Bridging the gap among ranking-based measures for multi-label classification},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep unrolled weighted graph laplacian regularization for depth completion. <em>IJCV</em>, <em>133</em>(1), 190-210. (<a href='https://doi.org/10.1007/s11263-024-02188-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion aims to estimate dense depth images from sparse depth measurements with RGB image guidance. However, previous approaches have not fully considered sparse input fidelity, resulting in inconsistency with sparse input and poor robustness to input corruption. In this paper, we propose the deep unrolled Weighted Graph Laplacian Regularization (WGLR) for depth completion which enhances input fidelity and noise robustness by enforcing input constraints in the network design. Specifically, we assume graph Laplacian regularization as the prior for depth completion optimization and derive the WGLR solution by interpreting the depth map as the discrete counterpart of continuous manifold, enabling analysis in continuous domain and enforcing input consistency. Based on its anisotropic diffusion interpretation, we unroll the WGLR solution into iterative filtering for efficient implementation. Furthermore, we integrate the unrolled WGLR into deep learning framework to develop high-performance yet interpretable network, which diffuses the depth in a hierarchical manner to ensure global smoothness while preserving visually salient details. Experimental results demonstrate that the proposed scheme improves consistency with depth measurements and robustness to input corruption for depth completion, outperforming competing schemes on the NYUv2, KITTI-DC and TetrasRGBD datasets.},
  archive      = {J_IJCV},
  author       = {Zeng, Jin and Zhu, Qingpeng and Tian, Tongxuan and Sun, Wenxiu and Zhang, Lin and Zhao, Shengjie},
  doi          = {10.1007/s11263-024-02188-3},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {190-210},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Deep unrolled weighted graph laplacian regularization for depth completion},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRConvNet: A transformer-style ConvNet for lightweight image super-resolution. <em>IJCV</em>, <em>133</em>(1), 173-189. (<a href='https://doi.org/10.1007/s11263-024-02147-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, vision transformers have demonstrated their superiority against convolutional neural networks (ConvNet) in various tasks including single-image super-resolution (SISR). The success of transformers can be attributed to the indispensable multi-head self-attention (MHSA) mechanism, which enables to effectively model global connectivity with fewer parameters. However, the quadratic complexity of MHSA usually encounters vast computation costs and memory resource occupation, limiting their efficient deployment on mobile devices compared to widely used lightweight ConvNets. In this work, we thoroughly explore the key differences between ConvNet- and transformer-based SR models, thus presenting SRConvNet that absorbs both the merits for lightweight SISR. Our SRConvNet is accomplished by two primary designs: (1) the Fourier modulated attention (FMA), an MHSA-like but more computationally and parametrically efficient operator that performs regional frequency-spatial modulation and aggregation to ensure long-term and short-term dependencies modeling; (2) the dynamic mixing layer (DML) utilizing mixed-scale depthwise dynamic convolution with channel splitting and shuffling to explore multi-scale contextualized information for model locality and adaptability enhancement. Combining FMA and DFN, we can build a pure transformer-style ConvNet to compete with the best lightweight SISR models in the trade-off between efficiency and accuracy. Extensive experiments demonstrate that SRConvNet can achieve more efficient SR reconstruction than recent state-of-the-art lightweight SISR methods on both computation and parameters while preserving comparable performance. Code is available at https://github.com/lifengcs/SRConvNet .},
  archive      = {J_IJCV},
  author       = {Li, Feng and Cong, Runmin and Wu, Jingjing and Bai, Huihui and Wang, Meng and Zhao, Yao},
  doi          = {10.1007/s11263-024-02147-y},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {173-189},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {SRConvNet: A transformer-style ConvNet for lightweight image super-resolution},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FADE: A task-agnostic upsampling operator for Encoder–Decoder architectures. <em>IJCV</em>, <em>133</em>(1), 151-172. (<a href='https://doi.org/10.1007/s11263-024-02191-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this work is to develop a task-agnostic feature upsampling operator for dense prediction where the operator is required to facilitate not only region-sensitive tasks like semantic segmentation but also detail-sensitive tasks such as image matting. Prior upsampling operators often can work well in either type of the tasks, but not both. We argue that task-agnostic upsampling should dynamically trade off between semantic preservation and detail delineation, instead of having a bias between the two properties. In this paper, we present FADE, a novel, plug-and-play, lightweight, and task-agnostic upsampling operator by fusing the assets of decoder and encoder features at three levels: (i) considering both the encoder and decoder feature in upsampling kernel generation; (ii) controlling the per-point contribution of the encoder/decoder feature in upsampling kernels with an efficient semi-shift convolutional operator; and (iii) enabling the selective pass of encoder features with a decoder-dependent gating mechanism for compensating details. To improve the practicality of FADE, we additionally study parameter- and memory-efficient implementations of semi-shift convolution. We analyze the upsampling behavior of FADE on toy data and show through large-scale experiments that FADE is task-agnostic with consistent performance improvement on a number of dense prediction tasks with little extra cost. For the first time, we demonstrate robust feature upsampling on both region- and detail-sensitive tasks successfully. Code is made available at: https://github.com/poppinace/fade},
  archive      = {J_IJCV},
  author       = {Lu, Hao and Liu, Wenze and Fu, Hongtao and Cao, Zhiguo},
  doi          = {10.1007/s11263-024-02191-8},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {151-172},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {FADE: A task-agnostic upsampling operator for Encoder–Decoder architectures},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning combinatorial prompts for universal controllable image captioning. <em>IJCV</em>, <em>133</em>(1), 129-150. (<a href='https://doi.org/10.1007/s11263-024-02179-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllable Image Captioning (CIC)—generating natural language descriptions about images under the guidance of given control signals—is one of the most promising directions toward next-generation captioning systems. Till now, various kinds of control signals for CIC have been proposed, ranging from content-related control to structure-related control. However, due to the format and target gaps of different control signals, all existing CIC works (or architectures) only focus on one certain control signal, and overlook the human-like combinatorial ability. By “combinatorial", we mean that our humans can easily meet multiple needs (or constraints) simultaneously when generating descriptions. To this end, we propose a novel prompt-based framework for CIC by learning Combinatorial Prompts, dubbed as ComPro. Specifically, we directly utilize a pretrained language model GPT-2 Radford et al. (OpenAI blog 1:9, 2019) as our language model, which can help to bridge the gap between different signal-specific CIC architectures. Then, we reformulate the CIC as a prompt-guide sentence generation problem, and propose a new lightweight prompt generation network to generate the combinatorial prompts for different kinds of control signals. For different control signals, we further design a new mask attention mechanism to realize the prompt-based CIC. Due to its simplicity, our ComPro can be further extended to more kinds of combined control signals by concatenating these prompts. Extensive experiments on two prevalent CIC benchmarks have verified the effectiveness and efficiency of our ComPro on both single and combined control signals.},
  archive      = {J_IJCV},
  author       = {Wang, Zhen and Xiao, Jun and Zhuang, Yueting and Gao, Fei and Shao, Jian and Chen, Long},
  doi          = {10.1007/s11263-024-02179-4},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {129-150},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning combinatorial prompts for universal controllable image captioning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous spatial-spectral reconstruction via implicit neural representation. <em>IJCV</em>, <em>133</em>(1), 106-128. (<a href='https://doi.org/10.1007/s11263-024-02150-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for spectral image reconstruction from low spatial/spectral resolution inputs are typically in discrete manners, only producing results with fixed spatial/spectral resolutions. However, these discrete methods neglect the continuous nature of three-dimensional spectral signals, limiting their applicability and performance. To address this limitation, we propose a novel method leveraging implicit neural representation, which allows for spectral image reconstruction with arbitrary resolutions in both spatial and spectral dimensions for the first time. Specifically, we design neural spatial-spectral representation (NeSSR), which projects the deep features extracted from low-resolution inputs to the corresponding intensity values under target 3D coordinates (including 2D spatial positions and 1D spectral wavelengths). To achieve continuous reconstruction, within NeSSR we devise: a spectral profile interpolation module, which efficiently interpolates features to the desired resolution, and a coordinate-aware neural attention mapping module, which aggregates the coordinate and content information for the final reconstruction. Before NeSSR, we design the spatial-spectral encoder leveraging large-kernel 3D attention, which effectively captures the spatial-spectral correlation in the form of deep features for subsequent high-fidelity representation. Extensive experiments demonstrate the superiority of our method over existing methods across three representative spatial-spectral reconstruction tasks, showcasing its ability to reconstruct spectral images with arbitrary and even extreme spatial/spectral resolutions beyond the training scale.},
  archive      = {J_IJCV},
  author       = {Xu, Ruikang and Yao, Mingde and Chen, Chang and Wang, Lizhi and Xiong, Zhiwei},
  doi          = {10.1007/s11263-024-02150-3},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {106-128},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Continuous spatial-spectral reconstruction via implicit neural representation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning dynamic batch-graph representation for deep representation learning. <em>IJCV</em>, <em>133</em>(1), 84-105. (<a href='https://doi.org/10.1007/s11263-024-02175-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, batch-based image data representation has been demonstrated to be effective for context-enhanced image representation. The core issue for this task is capturing the dependences of image samples within each mini-batch and conducting message communication among different samples. Existing approaches mainly adopt self-attention or local self-attention models (on patch dimension) for this task which fail to fully exploit the intrinsic relationships of samples within mini-batch and also be sensitive to noises and outliers. To address this issue, in this paper, we propose a flexible Dynamic Batch-Graph Representation (DyBGR) model, to automatically explore the intrinsic relationship of samples for contextual sample representation. Specifically, DyBGR first represents the mini-batch with a graph (termed batch-graph) in which nodes represent image samples and edges encode the dependences of images. This graph is dynamically learned with the constraint of similarity, sparseness and semantic correlation. Upon this, DyBGR exchanges the sample (node) information on the batch-graph to update each node representation. Note that, both batch-graph learning and information propagation are jointly optimized to boost their respective performance. Furthermore, in practical, DyBGR model can be implemented via a simple plug-and-play block (named DyBGR block) which thus can be potentially integrated into any mini-batch based deep representation learning schemes. Extensive experiments on deep metric learning tasks demonstrate the effectiveness of DyBGR. We will release the code at https://github.com/SissiW/DyBGR .},
  archive      = {J_IJCV},
  author       = {Wang, Xixi and Jiang, Bo and Wang, Xiao and Luo, Bin},
  doi          = {10.1007/s11263-024-02175-8},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {84-105},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Learning dynamic batch-graph representation for deep representation learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental model enhancement via memory-based contrastive learning. <em>IJCV</em>, <em>133</em>(1), 65-83. (<a href='https://doi.org/10.1007/s11263-024-02138-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training data of many vision tasks may be sequentially arrived in practice, e.g., the vision tasks in autonomous driving or video surveillance applications. This raises a fundamental challenge that, how to keep improving the performance on a specific task by learning from sequentially available training splits. This paper investigates this task as Incremental Model Enhancement (IME). IME is distinct from the conventional Incremental Learning (IL), where each training split typically corresponds to a set of independent classes, domains, or tasks. In IME, each training split may only cover part of the entire data distribution for the target vision task. Consequently, the IME model should be optimized towards the joint distribution of all available training splits, instead of optimizing towards each newly arrived one like IL methods. To deal with above issues, our method stores feature vectors of previously observed training data in the memory bank, which preserves compressed knowledge of the previous training data. We hence adopt the memorized features and each newly arrived training split for training via Memory-based Contrastive Learning (MCL). A new Contrastive Relation Preserving (CRP) scheme updates the memory bank to prevent obsoleteness of the preserved features and works with MCL simultaneously to boost the model performance. Experiments on several large-scale image classification benchmarks demonstrate the effectiveness of our method. Our method also works well on semantic segmentation, showing strong generalization ability on diverse vision tasks.},
  archive      = {J_IJCV},
  author       = {Xuan, Shiyu and Yang, Ming and Zhang, Shiliang},
  doi          = {10.1007/s11263-024-02138-z},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {65-83},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Incremental model enhancement via memory-based contrastive learning},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on test-time adaptation under distribution shifts. <em>IJCV</em>, <em>133</em>(1), 31-64. (<a href='https://doi.org/10.1007/s11263-024-02181-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning methods strive to acquire a robust model during the training process that can effectively generalize to test samples, even in the presence of distribution shifts. However, these methods often suffer from performance degradation due to unknown test distributions. Test-time adaptation (TTA), an emerging paradigm, has the potential to adapt a pre-trained model to unlabeled data during testing, before making predictions. Recent progress in this paradigm has highlighted the significant benefits of using unlabeled data to train self-adapted models prior to inference. In this survey, we categorize TTA into several distinct groups based on the form of test data, namely, test-time domain adaptation, test-time batch adaptation, and online test-time adaptation. For each category, we provide a comprehensive taxonomy of advanced algorithms and discuss various learning scenarios. Furthermore, we analyze relevant applications of TTA and discuss open challenges and promising areas for future research. For a comprehensive list of TTA methods, kindly refer to https://github.com/tim-learn/awesome-test-time-adaptation .},
  archive      = {J_IJCV},
  author       = {Liang, Jian and He, Ran and Tan, Tieniu},
  doi          = {10.1007/s11263-024-02181-w},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {31-64},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {A comprehensive survey on test-time adaptation under distribution shifts},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UPR-net: A unified pyramid recurrent network for video frame interpolation. <em>IJCV</em>, <em>133</em>(1), 16-30. (<a href='https://doi.org/10.1007/s11263-024-02164-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow-guided synthesis provides a popular framework for video frame interpolation, where optical flow is firstly estimated to warp input frames, and then the intermediate frame is synthesized from warped representations. Within this framework, optical flow is typically estimated from coarse-to-fine by a pyramid network, but the intermediate frame is commonly synthesized in a single pass, missing the opportunity of refining possible imperfect synthesis for high-resolution and large motion cases. While cascading several synthesis networks is a natural idea, it is nontrivial to unify iterative estimation of both optical flow and intermediate frame into a compact, flexible, and general framework. In this paper, we present UPR-Net, a novel Unified Pyramid Recurrent Network for frame interpolation. Cast in a flexible pyramid framework, UPR-Net exploits lightweight recurrent modules for both bi-directional flow estimation and intermediate frame synthesis. At each pyramid level, it leverages estimated bi-directional flow to generate forward-warped representations for frame synthesis; across pyramid levels, it enables iterative refinement for both optical flow and intermediate frame. We show that our iterative synthesis significantly improves the interpolation robustness on large motion cases, and the recurrent module design enables flexible resolution-aware adaptation in testing. When trained on low-resolution data, UPR-Net can achieve excellent performance on both low- and high-resolution benchmarks. Despite being extremely lightweight (1.7M parameters), the base version of UPR-Net competes favorably with many methods that rely on much heavier architectures. Code and trained models are publicly available at: https://github.com/srcn-ivl/UPR-Net .},
  archive      = {J_IJCV},
  author       = {Jin, Xin and Wu, Longhai and Chen, Jie and Chen, Youxin and Koo, Jayoon and Hahm, Cheul-Hee and Chen, Zhao-Min},
  doi          = {10.1007/s11263-024-02164-x},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {16-30},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {UPR-net: A unified pyramid recurrent network for video frame interpolation},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards training-free open-world segmentation via image prompt foundation models. <em>IJCV</em>, <em>133</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11263-024-02185-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The realm of computer vision has witnessed a paradigm shift with the advent of foundational models, mirroring the transformative influence of large language models in the domain of natural language processing. This paper delves into the exploration of open-world segmentation, presenting a novel approach called Image Prompt Segmentation (IPSeg) that harnesses the power of vision foundational models. IPSeg lies the principle of a training-free paradigm, which capitalizes on image prompt techniques. Specifically, IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. Our approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image. The generated point prompts are further utilized to guide the Segment Anything Model to segment the target object in the input image. The proposed method stands out by eliminating the need for exhaustive training sessions, thereby offering a more efficient and scalable solution. Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg’s efficacy for flexible open-world segmentation using intuitive image prompts. This work pioneers tapping foundation models for open-world understanding through visual concepts conveyed in images.},
  archive      = {J_IJCV},
  author       = {Tang, Lv and Jiang, Peng-Tao and Xiao, Haoke and Li, Bo},
  doi          = {10.1007/s11263-024-02185-6},
  journal      = {International Journal of Computer Vision},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Int. J. Comput. Vis.},
  title        = {Towards training-free open-world segmentation via image prompt foundation models},
  volume       = {133},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
